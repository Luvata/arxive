<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.01502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17041" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.17065">
<title>Efficient Deep Speech Understanding at the Edge. (arXiv:2311.17065v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2311.17065</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary Speech Understanding (SU) involves a sophisticated pipeline:
capturing real-time voice input, the pipeline encompasses a deep neural network
with an encoder-decoder architecture enhanced by beam search. This network
periodically assesses attention and Connectionist Temporal Classification (CTC)
scores in its autoregressive output.
&lt;/p&gt;
&lt;p&gt;This paper aims to enhance SU performance on edge devices with limited
resources. It pursues two intertwined goals: accelerating on-device execution
and efficiently handling inputs that surpass the on-device model&apos;s capacity.
While these objectives are well-established, we introduce innovative solutions
that specifically address SU&apos;s distinctive challenges: 1. Late
contextualization: Enables the parallel execution of a model&apos;s attentive
encoder during input ingestion. 2. Pilot decoding: Alleviates temporal load
imbalances. 3. Autoregression offramps: Facilitate offloading decisions based
on partial output sequences.
&lt;/p&gt;
&lt;p&gt;Our techniques seamlessly integrate with existing SU models, pipelines, and
frameworks, allowing for independent or combined application. Together, they
constitute a hybrid solution for edge SU, exemplified by our prototype, XYZ.
Evaluated on platforms equipped with 6-8 Arm cores, our system achieves
State-of-the-Art (SOTA) accuracy, reducing end-to-end latency by 2x and halving
offloading requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rongxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Felix Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17076">
<title>Compositional Chain-of-Thought Prompting for Large Multimodal Models. (arXiv:2311.17076v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17076</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of strong visual backbones and Large Language Model (LLM)
reasoning has led to Large Multimodal Models (LMMs) becoming the current
standard for a wide range of vision and language (VL) tasks. However, recent
research has shown that even the most advanced LMMs still struggle to capture
aspects of compositional visual reasoning, such as attributes and relationships
between objects. One solution is to utilize scene graphs (SGs)--a formalization
of objects and their relations and attributes that has been extensively used as
a bridge between the visual and textual domains. Yet, scene graph data requires
scene graph annotations, which are expensive to collect and thus not easily
scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic
forgetting of the pretraining objective. To overcome this, inspired by
chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a
novel zero-shot Chain-of-Thought prompting method that utilizes SG
representations in order to extract compositional knowledge from an LMM.
Specifically, we first generate an SG using the LMM, and then use that SG in
the prompt to produce a response. Through extensive experiments, we find that
the proposed CCoT approach not only improves LMM performance on several vision
and language VL compositional benchmarks but also improves the performance of
several popular LMMs on general multimodal benchmarks, without the need for
fine-tuning or annotated ground-truth SGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1&quot;&gt;Chancharik Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Brandon Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1&quot;&gt;Roei Herzig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17086">
<title>PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation. (arXiv:2311.17086v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17086</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models are well-known for their ability to generate
realistic images based on textual prompts. However, the existing works have
predominantly focused on English, lacking support for non-English text-to-image
models. The most commonly used translation methods cannot solve the generation
problem related to language culture, while training from scratch on a specific
language dataset is prohibitively expensive. In this paper, we are inspired to
propose a simple plug-and-play language transfer method based on knowledge
distillation. All we need to do is train a lightweight MLP-like
parameter-efficient adapter (PEA) with only 6M parameters under teacher
knowledge distillation along with a small parallel data corpus. We are
surprised to find that freezing the parameters of UNet can still achieve
remarkable performance on the language-specific prompt evaluation set,
demonstrating that PEA can stimulate the potential generation ability of the
original UNet. Additionally, it closely approaches the performance of the
English text-to-image model on a general prompt evaluation set. Furthermore,
our adapter can be used as a plugin to achieve significant results in
downstream tasks in cross-lingual text-to-image generation. Code will be
available at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qingsong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haonan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17107">
<title>ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?. (arXiv:2311.17107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17107</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the accuracy of outputs generated by Large Language Models (LLMs)
is especially important in the climate science and policy domain. We introduce
the Expert Confidence in Climate Statements (ClimateX) dataset, a novel,
curated, expert-labeled dataset consisting of 8094 climate statements collected
from the latest Intergovernmental Panel on Climate Change (IPCC) reports,
labeled with their associated confidence levels. Using this dataset, we show
that recent LLMs can classify human expert confidence in climate-related
statements, especially in a few-shot learning setting, but with limited (up to
47%) accuracy. Overall, models exhibit consistent and significant
over-confidence on low and medium confidence statements. We highlight
implications of our results for climate communication, LLMs evaluation
strategies, and the use of LLMs in information retrieval systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacombe_R/0/1/0/all/0/1&quot;&gt;Romain Lacombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kerrie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilworth_E/0/1/0/all/0/1&quot;&gt;Eddie Dilworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17126">
<title>Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis. (arXiv:2311.17126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17126</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in text-to-image (T2I) generative models have shown
remarkable capabilities in producing diverse and imaginative visuals based on
text prompts. Despite the advancement, these diffusion models sometimes
struggle to translate the semantic content from the text into images entirely.
While conditioning on the layout has shown to be effective in improving the
compositional ability of T2I diffusion models, they typically require manual
layout input. In this work, we introduce a novel approach to improving T2I
diffusion models using Large Language Models (LLMs) as layout generators. Our
method leverages the Chain-of-Thought prompting of LLMs to interpret text and
generate spatially reasonable object layouts. The generated layout is then used
to enhance the generated images&apos; composition and spatial accuracy. Moreover, we
propose an efficient adapter based on a cross-attention mechanism, which
explicitly integrates the layout information into the stable diffusion models.
Our experiments demonstrate significant improvements in image quality and
layout accuracy, showcasing the potential of LLMs in augmenting generative
image models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yingxiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jianbo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1&quot;&gt;Quanzeng You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li-Ping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17136">
<title>UniIR: Training and Benchmarking Universal Multimodal Information Retrievers. (arXiv:2311.17136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17136</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing information retrieval (IR) models often assume a homogeneous format,
limiting their applicability to diverse user needs, such as searching for
images with text descriptions, searching for a news article with a headline
image, or finding a similar photo with a query image. To approach such
different information-seeking demands, we introduce UniIR, a unified
instruction-guided multimodal retriever capable of handling eight distinct
retrieval tasks across modalities. UniIR, a single retrieval system jointly
trained on ten diverse multimodal-IR datasets, interprets user instructions to
execute various retrieval tasks, demonstrating robust performance across
existing datasets and zero-shot generalization to new tasks. Our experiments
highlight that multi-task training and instruction tuning are keys to UniIR&apos;s
generalization ability. Additionally, we construct the M-BEIR, a multimodal
retrieval benchmark with comprehensive results, to standardize the evaluation
of universal multimodal information retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Cong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haonan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hexiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1&quot;&gt;Alan Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17154">
<title>Pragmatic Radiology Report Generation. (arXiv:2311.17154v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17154</link>
<description rdf:parseType="Literal">&lt;p&gt;When pneumonia is not found on a chest X-ray, should the report describe this
negative observation or omit it? We argue that this question cannot be answered
from the X-ray alone and requires a pragmatic perspective, which captures the
communicative goal that radiology reports serve between radiologists and
patients. However, the standard image-to-text formulation for radiology report
generation fails to incorporate such pragmatic intents. Following this
pragmatic perspective, we demonstrate that the indication, which describes why
a patient comes for an X-ray, drives the mentions of negative observations and
introduce indications as additional input to report generation. With respect to
the output, we develop a framework to identify uninferable information from the
image as a source of model hallucinations, and limit them by cleaning
groundtruth reports. Finally, we use indications and cleaned groundtruth
reports to develop pragmatic models, and show that they outperform existing
methods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also
in standard metrics (+6.3 Positive F1 and +11.0 BLEU-2).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chacha Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chenhao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17213">
<title>General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports. (arXiv:2311.17213v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17213</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiologists produce unstructured data that could be valuable for clinical
care when consumed by information systems. However, variability in style limits
usage. Study compares performance of system using domain-adapted language model
(RadLing) and general-purpose large language model (GPT-4) in extracting common
data elements (CDE) from thoracic radiology reports. Three radiologists
annotated a retrospective dataset of 1300 thoracic reports (900 training, 400
test) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate
embeddings for sentences and identify CDEs using cosine-similarity, which were
mapped to values using light-weight mapper. GPT-4 system used OpenAI&apos;s
general-purpose embeddings to identify relevant CDEs and used GPT-4 to map to
values. The output CDE:value pairs were compared to the reference standard; an
identical match was considered true positive. Precision (positive predictive
value) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall
(sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4;
the difference was statistically significant (P&amp;lt;.001). RadLing&apos;s domain-adapted
embeddings were more sensitive in CDE identification (95% vs 71%) and its
light-weight mapper had comparable precision in value assignment (95.4% vs
95.0%). RadLing system exhibited higher performance than GPT-4 system in
extracting CDEs from radiology reports. RadLing system&apos;s domain-adapted
embeddings outperform general-purpose embeddings from OpenAI in CDE
identification and its light-weight value mapper achieves comparable precision
to large GPT-4. RadLing system offers operational advantages including local
deployment and reduced runtime costs. Domain-adapted RadLing system surpasses
GPT-4 system in extracting common data elements from radiology reports, while
providing benefits of local deployment and lower costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhanaliwala_A/0/1/0/all/0/1&quot;&gt;Ali H. Dhanaliwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1&quot;&gt;Rikhiya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1&quot;&gt;Sanjeev Kumar Karn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullaskrishnan_P/0/1/0/all/0/1&quot;&gt;Poikavila Ullaskrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farri_O/0/1/0/all/0/1&quot;&gt;Oladimeji Farri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1&quot;&gt;Dorin Comaniciu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1&quot;&gt;Charles E. Kahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17227">
<title>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars. (arXiv:2311.17227v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17227</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems&apos; abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lizhou Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1&quot;&gt;Kai Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1&quot;&gt;Libby Hemphill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17233">
<title>Quantifying the redundancy between prosody and text. (arXiv:2311.17233v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17233</link>
<description rdf:parseType="Literal">&lt;p&gt;Prosody -- the suprasegmental component of speech, including pitch, loudness,
and tempo -- carries critical aspects of meaning. However, the relationship
between the information conveyed by prosody vs. by the words themselves remains
poorly understood. We use large language models (LLMs) to estimate how much
information is redundant between prosody and the words themselves. Using a
large spoken corpus of English audiobooks, we extract prosodic features aligned
to individual words and test how well they can be predicted from LLM
embeddings, compared to non-contextual word embeddings. We find a high degree
of redundancy between the information carried by the words and prosodic
information across several prosodic features, including intensity, duration,
pauses, and pitch contours. Furthermore, a word&apos;s prosodic information is
redundant with both the word itself and the context preceding as well as
following it. Still, we observe that prosodic features can not be fully
predicted from text, suggesting that prosody carries information above and
beyond the words. Along with this paper, we release a general-purpose data
processing pipeline for quantifying the relationship between linguistic
information and extra-linguistic features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lukas Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1&quot;&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1&quot;&gt;Alex Warstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1&quot;&gt;Ethan Wilcox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regev_T/0/1/0/all/0/1&quot;&gt;Tamar Regev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17264">
<title>RETSim: Resilient and Efficient Text Similarity. (arXiv:2311.17264v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17264</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces RETSim (Resilient and Efficient Text Similarity), a
lightweight, multilingual deep learning model trained to produce robust metric
embeddings for near-duplicate text retrieval, clustering, and dataset
deduplication tasks. We demonstrate that RETSim is significantly more robust
and accurate than MinHash and neural text embeddings, achieving new
state-of-the-art performance on dataset deduplication, adversarial text
retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D
benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual,
near-duplicate text retrieval capabilities under adversarial settings. RETSim
and the W4NT3D benchmark are open-sourced under the MIT License at
https://github.com/google/unisim.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Marina Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vallis_O/0/1/0/all/0/1&quot;&gt;Owen Vallis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bumin_A/0/1/0/all/0/1&quot;&gt;Aysegul Bumin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakharia_T/0/1/0/all/0/1&quot;&gt;Tanay Vakharia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursztein_E/0/1/0/all/0/1&quot;&gt;Elie Bursztein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17280">
<title>Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17280</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation via back-translation is common when pretraining
Vision-and-Language Navigation (VLN) models, even though the generated
instructions are noisy. But: does that noise matter? We find that nonsensical
or irrelevant language instructions during pretraining can have little effect
on downstream performance for both HAMT and VLN-BERT on R2R, and is still
better than only using clean, human data. To underscore these results, we
concoct an efficient augmentation method, Unigram + Object, which generates
nonsensical instructions that nonetheless improve downstream performance. Our
findings suggest that what matters for VLN R2R pretraining is the quantity of
visual trajectories, not the quality of instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1&quot;&gt;Ishika Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Robin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17295">
<title>Elo Uncovered: Robustness and Best Practices in Language Model Evaluation. (arXiv:2311.17295v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17295</link>
<description rdf:parseType="Literal">&lt;p&gt;In Natural Language Processing (NLP), the Elo rating system, originally
designed for ranking players in dynamic games such as chess, is increasingly
being used to evaluate Large Language Models (LLMs) through &quot;A vs B&quot; paired
comparisons. However, while popular, the system&apos;s suitability for assessing
entities with constant skill levels, such as LLMs, remains relatively
unexplored. We study two fundamental axioms that evaluation methods should
adhere to: reliability and transitivity. We conduct extensive evaluation of Elo
behaviour, illustrating that individual Elo computations exhibit volatility and
delving into the impact of varying the Elo rating system&apos;s hyperparameters. We
show that these axioms are not always satisfied raising questions about the
reliability of current comparative evaluations of LLMs. If the current use of
Elo scores is intended to substitute the costly head-to-head comparison of
LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by
the axioms, our findings offer concrete guidelines for enhancing the
reliability of LLM evaluation methods, suggesting a need for reassessment of
existing comparative approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boubdir_M/0/1/0/all/0/1&quot;&gt;Meriem Boubdir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Edward Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermis_B/0/1/0/all/0/1&quot;&gt;Beyza Ermis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1&quot;&gt;Sara Hooker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1&quot;&gt;Marzieh Fadaee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17301">
<title>Language Models: A Guide for the Perplexed. (arXiv:2311.17301v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17301</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the growing importance of AI literacy, we decided to write this
tutorial to help narrow the gap between the discourse among those who study
language models -- the core technology underlying ChatGPT and similar products
-- and those who are intrigued and want to learn more about them. In short, we
believe the perspective of researchers and educators can add some clarity to
the public&apos;s understanding of the technologies beyond what&apos;s currently
available, which tends to be either extremely technical or promotional material
generated about products by their purveyors.
&lt;/p&gt;
&lt;p&gt;Our approach teases apart the concept of a language model from products built
on them, from the behaviors attributed to or desired from those products, and
from claims about similarity to human cognition. As a starting point, we (1)
offer a scientific viewpoint that focuses on questions amenable to study
through experimentation; (2) situate language models as they are today in the
context of the research that led to their development; and (3) describe the
boundaries of what is known about the models at this writing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serrano_S/0/1/0/all/0/1&quot;&gt;Sofia Serrano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brumbaugh_Z/0/1/0/all/0/1&quot;&gt;Zander Brumbaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17307">
<title>RoKEPG: RoBERTa and Knowledge Enhancement for Prescription Generation of Traditional Chinese Medicine. (arXiv:2311.17307v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17307</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional Chinese medicine (TCM) prescription is the most critical form of
TCM treatment, and uncovering the complex nonlinear relationship between
symptoms and TCM is of great significance for clinical practice and assisting
physicians in diagnosis and treatment. Although there have been some studies on
TCM prescription generation, these studies consider a single factor and
directly model the symptom-prescription generation problem mainly based on
symptom descriptions, lacking guidance from TCM knowledge. To this end, we
propose a RoBERTa and Knowledge Enhancement model for Prescription Generation
of Traditional Chinese Medicine (RoKEPG). RoKEPG is firstly pre-trained by our
constructed TCM corpus, followed by fine-tuning the pre-trained model, and the
model is guided to generate TCM prescriptions by introducing four classes of
knowledge of TCM through the attention mask matrix. Experimental results on the
publicly available TCM prescription dataset show that RoKEPG improves the F1
metric by about 2% over the baseline model with the best results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1&quot;&gt;Hua Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_J/0/1/0/all/0/1&quot;&gt;Jiacong Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jieyue He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17311">
<title>Universal Self-Consistency for Large Language Model Generation. (arXiv:2311.17311v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17311</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-consistency with chain-of-thought prompting (CoT) has demonstrated
remarkable performance gains on various challenging tasks, by utilizing
multiple reasoning paths sampled from large language models (LLMs). However,
self-consistency relies on the answer extraction process to aggregate multiple
solutions, which is not applicable to free-form answers. In this work, we
propose Universal Self-Consistency (USC), which leverages LLMs themselves to
select the most consistent answer among multiple candidates. We evaluate USC on
a variety of benchmarks, including mathematical reasoning, code generation,
long-context summarization, and open-ended question answering. On open-ended
generation tasks where the original self-consistency method is not applicable,
USC effectively utilizes multiple samples and improves the performance. For
mathematical reasoning, USC matches the standard self-consistency performance
without requiring the answer formats to be similar. Finally, without access to
execution results, USC also matches the execution-based voting performance on
code generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1&quot;&gt;Renat Aksitov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1&quot;&gt;Uri Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1&quot;&gt;Kefan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Pengcheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1&quot;&gt;Sushant Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1&quot;&gt;Charles Sutton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Denny Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17330">
<title>Biomedical knowledge graph-enhanced prompt generation for large language models. (arXiv:2311.17330v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17330</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have been driving progress in AI at an
unprecedented rate, yet still face challenges in knowledge-intensive domains
like biomedicine. Solutions such as pre-training and domain-specific
fine-tuning add substantial computational overhead, and the latter require
domain-expertise. External knowledge infusion is task-specific and requires
model training. Here, we introduce a task-agnostic Knowledge Graph-based
Retrieval Augmented Generation (KG-RAG) framework by leveraging the massive
biomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to
generate meaningful biomedical text rooted in established knowledge. KG-RAG
consistently enhanced the performance of LLMs across various prompt types,
including one-hop and two-hop prompts, drug repurposing queries, biomedical
true/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG
provides a remarkable 71% boost in the performance of the Llama-2 model on the
challenging MCQ dataset, demonstrating the framework&apos;s capacity to empower
open-source models with fewer parameters for domain-specific questions.
Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as
GPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ
data. Our approach was also able to address drug repurposing questions,
returning meaningful repurposing suggestions. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM, respectively,
in an optimized fashion, thus enhancing the adaptability of general-purpose
LLMs to tackle domain-specific questions in a unified framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soman_K/0/1/0/all/0/1&quot;&gt;Karthik Soman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rose_P/0/1/0/all/0/1&quot;&gt;Peter W Rose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1&quot;&gt;John H Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbas_R/0/1/0/all/0/1&quot;&gt;Rabia E Akbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1&quot;&gt;Brett Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peetoom_B/0/1/0/all/0/1&quot;&gt;Braian Peetoom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villouta_Reyes_C/0/1/0/all/0/1&quot;&gt;Catalina Villouta-Reyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerono_G/0/1/0/all/0/1&quot;&gt;Gabriel Cerono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yongmei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizk_Jackson_A/0/1/0/all/0/1&quot;&gt;Angela Rizk-Jackson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Israni_S/0/1/0/all/0/1&quot;&gt;Sharat Israni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nelson_C/0/1/0/all/0/1&quot;&gt;Charlotte A Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baranzini_S/0/1/0/all/0/1&quot;&gt;Sergio E Baranzini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17351">
<title>Exploring Large Language Models for Human Mobility Prediction under Public Events. (arXiv:2311.17351v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17351</link>
<description rdf:parseType="Literal">&lt;p&gt;Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuebing Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yichao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17352">
<title>Efficient Stitchable Task Adaptation. (arXiv:2311.17352v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17352</link>
<description rdf:parseType="Literal">&lt;p&gt;The paradigm of pre-training and fine-tuning has laid the foundation for
deploying deep learning models. However, most fine-tuning methods are designed
to meet a specific resource budget. Recently, considering diverse deployment
scenarios with various resource budgets, stitchable neural network (SN-Net) is
introduced to quickly obtain numerous new networks (stitches) from the
pre-trained models (anchors) in a model family via model stitching. Although
promising, SN-Net confronts new challenges when adapting it to new target
domains, including huge memory and storage requirements and a long and
sub-optimal multistage adaptation process. In this work, we present a novel
framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce
a palette of fine-tuned models that adhere to diverse resource constraints.
Specifically, we first tailor parameter-efficient fine-tuning to share low-rank
updates among the stitches while maintaining independent bias terms. In this
way, we largely reduce fine-tuning memory burdens and mitigate the interference
among stitches that arises in task adaptation. Furthermore, we streamline a
simple yet effective one-stage deployment pipeline, which estimates the
important stitches to deploy with training-time gradient statistics. By
assigning higher sampling probabilities to important stitches, we also get a
boosted Pareto frontier. Extensive experiments on 25 downstream visual
recognition tasks demonstrate that our ESTA is capable of generating stitches
with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net
adaptation by remarkable margins with significantly lower training time and
fewer trainable parameters. Furthermore, we demonstrate the flexibility and
scalability of our ESTA framework by stitching LLMs from LLaMA family,
obtaining chatbot stitches of assorted sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haoyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zizheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17355">
<title>Are Large Language Models Good Fact Checkers: A Preliminary Study. (arXiv:2311.17355v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17355</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Large Language Models (LLMs) have drawn significant attention due
to their outstanding reasoning capabilities and extensive knowledge repository,
positioning them as superior in handling various natural language processing
tasks compared to other language models. In this paper, we present a
preliminary investigation into the potential of LLMs in fact-checking. This
study aims to comprehensively evaluate various LLMs in tackling specific
fact-checking subtasks, systematically evaluating their capabilities, and
conducting a comparative analysis of their performance against pre-trained and
state-of-the-art low-parameter models. Experiments demonstrate that LLMs
achieve competitive performance compared to other small models in most
scenarios. However, they encounter challenges in effectively handling Chinese
fact verification and the entirety of the fact-checking pipeline due to
language inconsistencies and hallucinations. These findings underscore the need
for further exploration and research to enhance the proficiency of LLMs as
reliable fact-checkers, unveiling the potential capability of LLMs and the
possible challenges in fact-checking tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Han Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lingwei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mengyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Songlin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17371">
<title>Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&amp;A. (arXiv:2311.17371v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17371</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) underscore their
potential for responding to medical inquiries. However, ensuring that
generative agents provide accurate and reliable answers remains an ongoing
challenge. In this context, multi-agent debate (MAD) has emerged as a prominent
strategy for enhancing the truthfulness of LLMs. In this work, we provide a
comprehensive benchmark of MAD strategies for medical Q&amp;amp;A, along with
open-source implementations. This explores the effective utilization of various
strategies including the trade-offs between cost, time, and accuracy. We build
upon these insights to provide a novel debate-prompting strategy based on agent
agreement that outperforms previously published strategies on medical Q&amp;amp;A
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smit_A/0/1/0/all/0/1&quot;&gt;Andries Smit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duckworth_P/0/1/0/all/0/1&quot;&gt;Paul Duckworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1&quot;&gt;Nathan Grinsztajn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1&quot;&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_T/0/1/0/all/0/1&quot;&gt;Thomas D. Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17376">
<title>CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs. (arXiv:2311.17376v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17376</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction-based multitasking has played a critical role in the success of
large language models (LLMs) in multi-turn dialog applications. While publicly
available LLMs have shown promising performance, when exposed to complex
instructions with multiple constraints, they lag against state-of-the-art
models like ChatGPT. In this work, we hypothesize that the availability of
large-scale complex demonstrations is crucial in bridging this gap. Focusing on
dialog applications, we propose a novel framework, CESAR, that unifies a large
number of dialog tasks in the same format and allows programmatic induction of
complex instructions without any manual effort.
&lt;/p&gt;
&lt;p&gt;We apply CESAR on InstructDial, a benchmark for instruction-based dialog
tasks. We further enhance InstructDial with new datasets and tasks and utilize
CESAR to induce complex tasks with compositional instructions. This results in
a new benchmark called InstructDial++, which includes 63 datasets with 86 basic
tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the
scalability of CESAR in providing rich instructions. Models trained on
InstructDial++ can follow compositional prompts, such as prompts that ask for
multiple stylistic constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksu_T/0/1/0/all/0/1&quot;&gt;Taha Aksu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1&quot;&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1&quot;&gt;Shikib Mehri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seokhwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1&quot;&gt;Dilek Hakkani-T&amp;#xfc;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1&quot;&gt;Mahdi Namazifar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17391">
<title>Unveiling the Implicit Toxicity in Large Language Models. (arXiv:2311.17391v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17391</link>
<description rdf:parseType="Literal">&lt;p&gt;The open-endedness of large language models (LLMs) combined with their
impressive capabilities may lead to new safety issues when being exploited for
malicious use. While recent studies primarily focus on probing toxic outputs
that can be easily detected with existing toxicity classifiers, we show that
LLMs can generate diverse implicit toxic outputs that are exceptionally
difficult to detect via simply zero-shot prompting. Moreover, we propose a
reinforcement learning (RL) based attacking method to further induce the
implicit toxicity in LLMs. Specifically, we optimize the language model with a
reward that prefers implicit toxic outputs to explicit toxic and non-toxic
ones. Experiments on five widely-adopted toxicity classifiers demonstrate that
the attack success rate can be significantly improved through RL fine-tuning.
For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate
of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose
a significant threat in generating undetectable implicit toxic outputs. We
further show that fine-tuning toxicity classifiers on the annotated examples
from our attacking method can effectively enhance their ability to detect
LLM-generated implicit toxic language. The code is publicly available at
https://github.com/thu-coai/Implicit-Toxicity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jiaxin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1&quot;&gt;Pei Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhexin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinfeng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Minlie Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17400">
<title>Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention. (arXiv:2311.17400v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17400</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based models, such as BERT and GPT, have been widely adopted in
natural language processing (NLP) due to their exceptional performance.
However, recent studies show their vulnerability to textual adversarial attacks
where the model&apos;s output can be misled by intentionally manipulating the text
inputs. Despite various methods that have been proposed to enhance the model&apos;s
robustness and mitigate this vulnerability, many require heavy consumption
resources (e.g., adversarial training) or only provide limited protection
(e.g., defensive dropout). In this paper, we propose a novel method called
dynamic attention, tailored for the transformer architecture, to enhance the
inherent robustness of the model itself against various adversarial attacks.
Our method requires no downstream task knowledge and does not incur additional
costs. The proposed dynamic attention consists of two modules: (I) attention
rectification, which masks or weakens the attention value of the chosen tokens,
and (ii) dynamic modeling, which dynamically builds the set of candidate
tokens. Extensive experiments demonstrate that dynamic attention significantly
mitigates the impact of adversarial attacks, improving up to 33\% better
performance than previous methods against widely-used adversarial attacks. The
model-level design of dynamic attention enables it to be easily combined with
other defense methods (e.g., adversarial training) to further enhance the
model&apos;s robustness. Furthermore, we demonstrate that dynamic attention
preserves the state-of-the-art robustness space of the original model compared
to other dynamic modeling methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lujia Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yuwen Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shouling Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changjiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1&quot;&gt;Chunpeng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17404">
<title>VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models. (arXiv:2311.17404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17404</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to perceive how objects change over time is a crucial ingredient
in human intelligence. However, current benchmarks cannot faithfully reflect
the temporal understanding abilities of video-language models (VidLMs) due to
the existence of static visual shortcuts. To remedy this issue, we present
VITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal
Concept underStanding. Specifically, we first introduce a fine-grained taxonomy
of temporal concepts in natural language in order to diagnose the capability of
VidLMs to comprehend different temporal aspects. Furthermore, to disentangle
the correlation between static and temporal information, we generate
counterfactual video descriptions that differ from the original one only in the
specified temporal aspect. We employ a semi-automatic data collection framework
using large language models and human-in-the-loop annotation to obtain
high-quality counterfactual descriptions efficiently. Evaluation of
representative video-language understanding models confirms their deficiency in
temporal understanding, revealing the need for greater emphasis on the temporal
elements in video-language research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rundong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lu Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17429">
<title>TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4. (arXiv:2311.17429v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17429</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based learning has been widely applied in many low-resource NLP tasks
such as few-shot scenarios. However, this paradigm has been shown to be
vulnerable to backdoor attacks. Most of the existing attack methods focus on
inserting manually predefined templates as triggers in the pre-training phase
to train the victim model and utilize the same triggers in the downstream task
to perform inference, which tends to ignore the transferability and
stealthiness of the templates. In this work, we propose a novel approach of
TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models
via GPT4), which is a data-independent attack method. Specifically, we first
utilize GPT4 to reformulate manual templates to generate tone-strong and normal
templates, and the former are injected into the model as a backdoor trigger in
the pre-training phase. Then, we not only directly employ the above templates
in the downstream task, but also use GPT4 to generate templates with similar
tone to the above templates to carry out transferable attacks. Finally we have
conducted extensive experiments on five NLP datasets and three BERT series
models, with experimental results justifying that our TARGET method has better
attack performance and stealthiness compared to the two-external baseline
methods on direct attacks, and in addition achieves satisfactory attack
capability in the unseen tone-similar templates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zihao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongjian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17438">
<title>CLOMO: Counterfactual Logical Modification with Large Language Models. (arXiv:2311.17438v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17438</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we delve into the realm of counterfactual reasoning
capabilities of large language models (LLMs). Our primary objective is to
cultivate the counterfactual thought processes within LLMs and rigorously
assess these processes for their validity. Specifically, we introduce a novel
task, Counterfactual Logical Modification (CLOMO), and a high-quality
human-annotated benchmark. In this task, LLMs must adeptly alter a given
argumentative text to uphold a predetermined logical relationship. To
effectively evaluate a generation model&apos;s counterfactual capabilities, we
propose an innovative evaluation metric, the LogicAware Counterfactual Score to
directly evaluate the natural language output of LLMs instead of modeling the
task as a multiple-choice problem. Analysis shows that the proposed automatic
metric aligns well with human preference. Our experimental results show that
while LLMs demonstrate a notable capacity for logical counterfactual thinking,
there remains a discernible gap between their current abilities and human
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yinya Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Ruixin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linqi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17487">
<title>Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model. (arXiv:2311.17487v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17487</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of language models, the nuanced linguistic and cultural
intricacies of Traditional Chinese, as spoken in Taiwan, have been largely
overlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model
that specifically caters to the Traditional Chinese language, with a focus on
the variant used in Taiwan. Leveraging a comprehensive pretraining corpus and
instruction-finetuning datasets, we have developed a model that not only
understands the complexities of Traditional Chinese but also embodies the
cultural context of Taiwan. Taiwan LLM represents the first of its kind, a
model that is not only linguistically accurate but also culturally resonant
with its user base. Our evaluations demonstrate that Taiwan LLM achieves
superior performance in understanding and generating Traditional Chinese text,
outperforming existing models that are predominantly trained on Simplified
Chinese or English. The open-source release of Taiwan LLM invites collaboration
and further innovation, ensuring that the linguistic diversity of Chinese
speakers is embraced and well-served. The model, datasets, and further
resources are made publicly available to foster ongoing research and
development in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen-Ting Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun-Nung Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17492">
<title>Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17492</link>
<description rdf:parseType="Literal">&lt;p&gt;The Manchu language, with its roots in the historical Manchurian region of
Northeast China, is now facing a critical threat of extinction, as there are
very few speakers left. In our efforts to safeguard the Manchu language, we
introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation
(MT) model. To develop this model, we utilize valuable resources such as the
Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the
scarcity of a Manchu-Korean parallel dataset, we expand our data by employing
word replacement guided by GloVe embeddings, trained on both monolingual and
parallel texts. Our approach is built around an encoder-decoder neural machine
translation model, incorporating a bi-directional Gated Recurrent Unit (GRU)
layer. The experiments have yielded promising results, showcasing a significant
enhancement in Manchu-Korean translation, with a remarkable 20-30 point
increase in the BLEU score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Jean Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1&quot;&gt;Sungjoo Byun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minha Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangah Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17502">
<title>Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models. (arXiv:2311.17502v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17502</link>
<description rdf:parseType="Literal">&lt;p&gt;Community Question Answering (CQA) becomes increasingly prevalent in recent
years. However, there are a large number of answers, which is difficult for
users to select the relevant answers. Therefore, answer selection is a very
significant subtask of CQA. In this paper, we first propose the Question-Answer
cross attention networks (QAN) with pre-trained models for answer selection and
utilize large language model (LLM) to perform answer selection with knowledge
augmentation. Specifically, we apply the BERT model as the encoder layer to do
pre-training for question subjects, question bodies and answers, respectively,
then the cross attention mechanism selects the most relevant answer for
different questions. Experiments show that the QAN model achieves
state-of-the-art performance on two datasets, SemEval2015 and SemEval2017.
Moreover, we use the LLM to generate external knowledge from questions and
correct answers to achieve knowledge augmentation for the answer selection task
by LLM, while optimizing the prompt of LLM in different aspects. The results
show that the introduction of external knowledge can improve the correct answer
selection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM
can also select the correct answer on more questions by optimized prompt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinghang Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17514">
<title>Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning. (arXiv:2311.17514v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17514</link>
<description rdf:parseType="Literal">&lt;p&gt;Query-focused Summarization (QfS) deals with systems that generate summaries
from document(s) based on a query. Motivated by the insight that Reinforcement
Learning (RL) provides a generalization to Supervised Learning (SL) for Natural
Language Generation, and thereby performs better (empirically) than SL, we use
an RL-based approach for this task of QfS. Additionally, we also resolve the
conflict of employing RL in Transformers with Teacher Forcing. We develop
multiple Policy Gradient networks, trained on various reward signals: ROUGE,
BLEU, and Semantic Similarity, which lead to a 10-point improvement over the
State-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5).
We also show performance of our approach in zero-shot setting for another
benchmark dataset (DebatePedia) -- our approach leads to results comparable to
baselines, which were specifically trained on DebatePedia. To aid the RL
training, we propose a better semantic similarity reward, enabled by a novel
Passage Embedding scheme developed using Cluster Hypothesis. Lastly, we
contribute a gold-standard test dataset to further research in QfS and
Long-form Question Answering (LfQA).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1&quot;&gt;Swaroop Nath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1&quot;&gt;Harshad Khadilkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1&quot;&gt;Pushpak Bhattacharyya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17593">
<title>LanGWM: Language Grounded World Model. (arXiv:2311.17593v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17593</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep reinforcement learning have showcased its potential
in tackling complex tasks. However, experiments on visual control tasks have
revealed that state-of-the-art reinforcement learning models struggle with
out-of-distribution generalization. Conversely, expressing higher-level
concepts and global contexts is relatively easy using language.
&lt;/p&gt;
&lt;p&gt;Building upon recent success of the large language models, our main objective
is to improve the state abstraction technique in reinforcement learning by
leveraging language for robust action selection. Specifically, we focus on
learning language-grounded visual features to enhance the world model learning,
a model-based reinforcement learning technique.
&lt;/p&gt;
&lt;p&gt;To enforce our hypothesis explicitly, we mask out the bounding boxes of a few
objects in the image observation and provide the text prompt as descriptions
for these masked objects. Subsequently, we predict the masked objects along
with the surrounding regions as pixel reconstruction, similar to the
transformer-based masked autoencoder approach.
&lt;/p&gt;
&lt;p&gt;Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art
performance in out-of-distribution test at the 100K interaction steps
benchmarks of iGibson point navigation tasks. Furthermore, our proposed
technique of explicit language-grounded visual representation learning has the
potential to improve models for human-robot interaction because our extracted
visual features are language grounded.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poudel_R/0/1/0/all/0/1&quot;&gt;Rudra P.K. Poudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandya_H/0/1/0/all/0/1&quot;&gt;Harit Pandya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1&quot;&gt;Roberto Cipolla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17633">
<title>Introduction to Transformers: an NLP Perspective. (arXiv:2311.17633v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17633</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have dominated empirical machine learning models of natural
language processing. In this paper, we introduce basic concepts of Transformers
and present key techniques that form the recent advances of these models. This
includes a description of the standard Transformer architecture, a series of
model refinements, and common applications. Given that Transformers and related
deep learning techniques might be evolving in ways we have never seen, we
cannot dive into all the model details or cover all the technical areas.
Instead, we focus on just those concepts that are helpful for gaining a good
understanding of Transformers and their variants. We also summarize the key
ideas that impact this field, thereby yielding some insights into the strengths
and limitations of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17647">
<title>VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following. (arXiv:2311.17647v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17647</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to
evaluate the visual instruction following capability of Multimodal Large
Language Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs
by embedding the instructions into the visual scenes, demanding strong visual
interpretative skills for instruction following. We adapt VIM to various
benchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM
bench, and probe diverse MLLMs across three distinct in-context learning
settings: Zero Shot, One Shot, and Pair Shot. We observe that there is a
significant performance disparity between the open-source MLLMs and GPT-4V,
implying that their proficiency in visual instruction comprehension is not up
to par. Our results highlight a promising direction for the enhancement of
MLLMs capabilities on instruction following. We aim VIM to serve as a useful
norm for advancing the state of the art and driving further progress in the
field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17667">
<title>TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models. (arXiv:2311.17667v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17667</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding time is a pivotal aspect of human cognition, crucial in the
broader framework of grasping the intricacies of the world. Previous studies
typically focus on specific aspects of time, lacking a comprehensive temporal
reasoning benchmark. To address this issue, we propose TimeBench, a
comprehensive hierarchical temporal reasoning benchmark that covers a broad
spectrum of temporal reasoning phenomena, which provides a thorough evaluation
for investigating the temporal reasoning capabilities of large language models.
We conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and
Mistral, incorporating chain-of-thought prompting. Our experimental results
indicate a significant performance gap between the state-of-the-art LLMs and
humans, highlighting that there is still a considerable distance to cover in
temporal reasoning. We aspire for TimeBench to serve as a comprehensive
benchmark, fostering research in temporal reasoning for LLMs. Our resource is
available at https://github.com/zchuz/TimeBench
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zheng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingchang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qianglong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weijiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haotian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17676">
<title>Improving Minority Stress Detection with Emotions. (arXiv:2311.17676v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17676</link>
<description rdf:parseType="Literal">&lt;p&gt;Psychological stress detection is an important task for mental healthcare
research, but there has been little prior work investigating the effectiveness
of psychological stress models on minority individuals, who are especially
vulnerable to poor mental health outcomes. In this work, we use the related
task of minority stress detection to evaluate the ability of psychological
stress models to understand the language of sexual and gender minorities. We
find that traditional psychological stress models underperform on minority
stress detection, and we propose using emotion-infused models to reduce that
performance disparity. We further demonstrate that multi-task psychological
stress models outperform the current state-of-the-art for minority stress
detection without directly training on minority stress data. We provide
explanatory analysis showing that minority communities have different
distributions of emotions than the general population and that emotion-infused
models improve the performance of stress models on underrepresented groups
because of their effectiveness in low-data environments, and we propose that
integrating emotions may benefit underrepresented groups in other mental health
detection tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivey_J/0/1/0/all/0/1&quot;&gt;Jonathan Ivey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_S/0/1/0/all/0/1&quot;&gt;Susan Gauch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17686">
<title>AviationGPT: A Large Language Model for the Aviation Domain. (arXiv:2311.17686v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17686</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of ChatGPT and GPT-4 has captivated the world with large language
models (LLMs), demonstrating exceptional performance in question-answering,
summarization, and content generation. The aviation industry is characterized
by an abundance of complex, unstructured text data, replete with technical
jargon and specialized terminology. Moreover, labeled data for model building
are scarce in this domain, resulting in low usage of aviation text data. The
emergence of LLMs presents an opportunity to transform this situation, but
there is a lack of LLMs specifically designed for the aviation domain. To
address this gap, we propose AviationGPT, which is built on open-source LLaMA-2
and Mistral architectures and continuously trained on a wealth of carefully
curated aviation datasets. Experimental results reveal that AviationGPT offers
users multiple advantages, including the versatility to tackle diverse natural
language processing (NLP) problems (e.g., question-answering, summarization,
document writing, information extraction, report querying, data cleaning, and
interactive data exploration). It also provides accurate and contextually
relevant responses within the aviation domain and significantly improves
performance (e.g., over a 40% performance gain in tested cases). With
AviationGPT, the aviation industry is better equipped to address more complex
research problems and enhance the efficiency and safety of National Airspace
System (NAS) operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liya Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1&quot;&gt;Jason Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tien_A/0/1/0/all/0/1&quot;&gt;Alex Tien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumgartner_D/0/1/0/all/0/1&quot;&gt;Diane M Baumgartner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17696">
<title>How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation. (arXiv:2311.17696v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17696</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence is transforming education through data-driven,
personalized learning solutions. This paper introduces AI Tutor, an innovative
web application that provides personalized tutoring in any subject using
state-of-the-art Large Language Model (LLM). AI Tutor ingests course materials
to construct an adaptive knowledge base tailored to the course. When students
pose questions, it retrieves the most relevant information and generates
detailed, conversational responses citing supporting evidence. The system is
powered by advanced large language models and Retrieval-Augmented Generation
(RAG) techniques for accurate, natural question answering. We present a
fully-functional web interface and video demonstration that showcase AI Tutor&apos;s
versatility across diverse subjects and its ability to produce pedagogically
cogent responses. While an initial prototype, this work represents a pioneering
step toward AI-enabled tutoring systems that can democratize access to
high-quality, customized educational support.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chenxi Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17722">
<title>SenTest: Evaluating Robustness of Sentence Encoders. (arXiv:2311.17722v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17722</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1&quot;&gt;Tanmay Chavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shantanu Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1&quot;&gt;Aditya Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1&quot;&gt;Omkar Gokhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_G/0/1/0/all/0/1&quot;&gt;Geetanjali Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1&quot;&gt;Raviraj Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17741">
<title>End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data. (arXiv:2311.17741v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17741</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint rich and normalized automatic speech recognition (ASR), that produces
transcriptions both with and without punctuation and capitalization, remains a
challenge. End-to-end (E2E) ASR models offer both convenience and the ability
to perform such joint transcription of speech. Training such models requires
paired speech and rich text data, which is not widely available. In this paper,
we compare two different approaches to train a stateless Transducer-based E2E
joint rich and normalized ASR system, ready for streaming applications, with a
limited amount of rich labeled data. The first approach uses a language model
to generate pseudo-rich transcriptions of normalized training data. The second
approach uses a single decoder conditioned on the type of the output. The first
approach leads to E2E rich ASR which perform better on out-of-domain data, with
up to 9% relative reduction in errors. The second approach demonstrates the
feasibility of an E2E joint rich and normalized ASR system using as low as 5%
rich training data with moderate (2.42% absolute) increase in errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Can Cui&lt;/a&gt; (MULTISPEECH), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikh_I/0/1/0/all/0/1&quot;&gt;Imran Ahamad Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1&quot;&gt;Mostafa Sadeghi&lt;/a&gt; (MULTISPEECH), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1&quot;&gt;Emmanuel Vincent&lt;/a&gt; (MULTISPEECH)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17743">
<title>Mukhyansh: A Headline Generation Dataset for Indic Languages. (arXiv:2311.17743v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17743</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of headline generation within the realm of Natural Language
Processing (NLP) holds immense significance, as it strives to distill the true
essence of textual content into concise and attention-grabbing summaries. While
noteworthy progress has been made in headline generation for widely spoken
languages like English, there persist numerous challenges when it comes to
generating headlines in low-resource languages, such as the rich and diverse
Indian languages. A prominent obstacle that specifically hinders headline
generation in Indian languages is the scarcity of high-quality annotated data.
To address this crucial gap, we proudly present Mukhyansh, an extensive
multilingual dataset, tailored for Indian language headline generation.
Comprising an impressive collection of over 3.39 million article-headline
pairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu,
Tamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present a
comprehensive evaluation of several state-of-the-art baseline models.
Additionally, through an empirical analysis of existing works, we demonstrate
that Mukhyansh outperforms all other models, achieving an impressive average
ROUGE-L score of 31.43 across all 8 languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madasu_L/0/1/0/all/0/1&quot;&gt;Lokesh Madasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanumolu_G/0/1/0/all/0/1&quot;&gt;Gopichand Kanumolu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surange_N/0/1/0/all/0/1&quot;&gt;Nirmal Surange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1&quot;&gt;Manish Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17771">
<title>Supervising the Centroid Baseline for Extractive Multi-Document Summarization. (arXiv:2311.17771v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17771</link>
<description rdf:parseType="Literal">&lt;p&gt;The centroid method is a simple approach for extractive multi-document
summarization and many improvements to its pipeline have been proposed. We
further refine it by adding a beam search process to the sentence selection and
also a centroid estimation attention model that leads to improved results. We
demonstrate this in several multi-document summarization datasets, including in
a multilingual scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_S/0/1/0/all/0/1&quot;&gt;Sim&amp;#xe3;o Gon&amp;#xe7;alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_G/0/1/0/all/0/1&quot;&gt;Gon&amp;#xe7;alo Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pernes_D/0/1/0/all/0/1&quot;&gt;Diogo Pernes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendes_A/0/1/0/all/0/1&quot;&gt;Afonso Mendes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17786">
<title>DSS: Synthesizing long Digital Ink using Data augmentation, Style encoding and Split generation. (arXiv:2311.17786v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.17786</link>
<description rdf:parseType="Literal">&lt;p&gt;As text generative models can give increasingly long answers, we tackle the
problem of synthesizing long text in digital ink. We show that the commonly
used models for this task fail to generalize to long-form data and how this
problem can be solved by augmenting the training data, changing the model
architecture and the inference procedure. These methods use contrastive
learning technique and are tailored specifically for the handwriting domain.
They can be applied to any encoder-decoder model that works with digital ink.
We demonstrate that our method reduces the character error rate on long-form
English data by half compared to baseline RNN and by 16% compared to the
previous approach that aims at addressing the same problem. We show that all
three parts of the method improve recognizability of generated inks. In
addition, we evaluate synthesized data in a human study and find that people
perceive most of generated data as real.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofeev_A/0/1/0/all/0/1&quot;&gt;Aleksandr Timofeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadeeva_A/0/1/0/all/0/1&quot;&gt;Anastasiia Fadeeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afonin_A/0/1/0/all/0/1&quot;&gt;Andrei Afonin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1&quot;&gt;Claudiu Musat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maksai_A/0/1/0/all/0/1&quot;&gt;Andrii Maksai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17813">
<title>Higher-Order DisCoCat (Peirce-Lambek-Montague semantics). (arXiv:2311.17813v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17813</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new definition of higher-order DisCoCat (categorical
compositional distributional) models where the meaning of a word is not a
diagram, but a diagram-valued higher-order function. Our models can be seen as
a variant of Montague semantics based on a lambda calculus where the primitives
act on string diagrams rather than logical formulae. As a special case, we show
how to translate from the Lambek calculus into Peirce&apos;s system beta for
first-order logic. This allows us to give a purely diagrammatic treatment of
higher-order and non-linear processes in natural language semantics: adverbs,
prepositions, negation and quantifiers. The theoretical definition presented in
this article comes with a proof-of-concept implementation in DisCoPy, the
Python library for string diagrams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toumi_A/0/1/0/all/0/1&quot;&gt;Alexis Toumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1&quot;&gt;Giovanni de Felice&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17842">
<title>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.17842</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa&apos;s superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yingdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fanqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.01502">
<title>Multi-turn Response Selection using Dialogue Dependency Relations. (arXiv:2010.01502v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2010.01502</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-turn response selection is a task designed for developing dialogue
agents. The performance on this task has a remarkable improvement with
pre-trained language models. However, these models simply concatenate the turns
in dialogue history as the input and largely ignore the dependencies between
the turns. In this paper, we propose a dialogue extraction algorithm to
transform a dialogue history into threads based on their dependency relations.
Each thread can be regarded as a self-contained sub-dialogue. We also propose
Thread-Encoder model to encode threads and candidates into compact
representations by pre-trained Transformers and finally get the matching score
through an attention layer. The experiments show that dependency relations are
helpful for dialogue context understanding, and our model outperforms the
state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results
on UbuntuV2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1&quot;&gt;Qi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yizhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Siyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kenny Q. Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haifeng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10240">
<title>Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning. (arXiv:2212.10240v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10240</link>
<description rdf:parseType="Literal">&lt;p&gt;Previously, non-autoregressive models were widely perceived as being superior
in generation efficiency but inferior in generation quality due to the
difficulties of modeling multiple target modalities. To enhance the
multi-modality modeling ability, we propose the diffusion glancing transformer,
which employs a modality diffusion process and residual glancing sampling. The
modality diffusion process is a discrete process that interpolates the
multi-modal distribution along the decoding steps, and the residual glancing
sampling approach guides the model to continuously learn the remaining
modalities across the layers. Experimental results on various machine
translation and text generation benchmarks demonstrate that DIFFGLAT achieves
better generation accuracy while maintaining fast decoding speed compared with
both autoregressive and non-autoregressive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Lihua Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04118">
<title>Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04118</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated impressive capabilities in
general scenarios, exhibiting a level of aptitude that approaches, in some
aspects even surpasses, human-level intelligence. Among their numerous skills,
the translation abilities of LLMs have received considerable attention.
Compared to typical machine translation that focuses solely on source-to-target
mapping, LLM-based translation can potentially mimic the human translation
process which might take preparatory steps to ensure high-quality translation.
This work explores this possibility by proposing the MAPS framework, which
stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs
first to analyze the given source sentence and induce three aspects of
translation-related knowledge: keywords, topics, and relevant demonstrations to
guide the final translation process. Moreover, we employ a selection mechanism
based on quality estimation to filter out noisy and unhelpful knowledge. Both
automatic (3 LLMs x 11 directions x 2 automatic metrics) and human evaluation
(preference study and MQM) demonstrate the effectiveness of MAPS. Further
analysis shows that by mimicking the human translation process, MAPS reduces
various translation errors such as hallucination, ambiguity, mistranslation,
awkward style, untranslated text, and omission. Source code is available at
https://github.com/zwhe99/MAPS-mt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiwei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1&quot;&gt;Tian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1&quot;&gt;Wenxiang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05189">
<title>SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05189</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models, which have emerged to become popular text-to-image
generation models, can produce high-quality and content-rich images guided by
textual prompts. However, there are limitations to semantic understanding and
commonsense reasoning in existing models when the input prompts are concise
narrative, resulting in low-quality image generation. To improve the capacities
for narrative prompts, we propose a simple-yet-effective parameter-efficient
fine-tuning approach called the Semantic Understanding and Reasoning adapter
(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first
collect and annotate a new dataset SURD which consists of more than 57,000
semantically corrected multi-modal samples. Each sample contains a simple
narrative prompt, a complex keyword-based prompt, and a high-quality image.
Then, we align the semantic representation of narrative prompts to the complex
prompts and transfer knowledge of large language models (LLMs) to our
SUR-adapter via knowledge distillation so that it can acquire the powerful
semantic understanding and reasoning capabilities to build a high-quality
textual semantic representation for text-to-image generation. We conduct
experiments by integrating multiple LLMs and popular pre-trained diffusion
models to show the effectiveness of our approach in enabling diffusion models
to understand and reason concise natural language without image quality
degradation. Our approach can make text-to-image diffusion models easier to use
with better user experience, which demonstrates our approach has the potential
for further advancing the development of user-friendly text-to-image generation
models by bridging the semantic gap between simple narrative prompts and
complex keyword-based prompts. The code is released at
https://github.com/Qrange-group/SUR-adapter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wushao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jinghui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09556">
<title>Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09556</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning effective sentence representations is crucial for many Natural
Language Processing (NLP) tasks, including semantic search, semantic textual
similarity (STS), and clustering. While multiple transformer models have been
developed for sentence embedding learning, these models may not perform
optimally when dealing with specialized domains like aviation, which has unique
characteristics such as technical jargon, abbreviations, and unconventional
grammar. Furthermore, the absence of labeled datasets makes it difficult to
train models specifically for the aviation domain. To address these challenges,
we propose a novel approach for adapting sentence transformers for the aviation
domain. Our method is a two-stage process consisting of pre-training followed
by fine-tuning. During pre-training, we use Transformers and Sequential
Denoising AutoEncoder (TSDAE) with aviation text data as input to improve the
initial model performance. Subsequently, we fine-tune our models using a
Natural Language Inference (NLI) dataset in the Sentence Bidirectional Encoder
Representations from Transformers (SBERT) architecture to mitigate overfitting
issues. Experimental results on several downstream tasks show that our adapted
sentence transformers significantly outperform general-purpose transformers,
demonstrating the effectiveness of our approach in capturing the nuances of the
aviation domain. Overall, our work highlights the importance of domain-specific
adaptation in developing high-quality NLP solutions for specialized industries
like aviation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liya Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1&quot;&gt;Jason Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouck_D/0/1/0/all/0/1&quot;&gt;Dave Rouck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tien_A/0/1/0/all/0/1&quot;&gt;Alex Tien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumgartner_D/0/1/0/all/0/1&quot;&gt;Diane M Baumgartner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14991">
<title>MuLER: Detailed and Scalable Reference-based Evaluation. (arXiv:2305.14991v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14991</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel methodology (namely, MuLER) that transforms any
reference-based evaluation metric for text generation, such as machine
translation (MT) into a fine-grained analysis tool. Given a system and a
metric, MuLER quantifies how much the chosen metric penalizes specific error
types (e.g., errors in translating names of locations). MuLER thus enables a
detailed error analysis which can lead to targeted improvement efforts for
specific phenomena. We perform experiments in both synthetic and naturalistic
settings to support MuLER&apos;s validity and showcase its usability in MT
evaluation, and other tasks, such as summarization. Analyzing all submissions
to WMT in 2014-2020, we find consistent trends. For example, nouns and verbs
are among the most frequent POS tags. However, they are among the hardest to
translate. Performance on most POS tags improves with overall system
performance, but a few are not thus correlated (their identity changes from
language to language). Preliminary experiments with summarization reveal
similar trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1&quot;&gt;Taelin Karidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1&quot;&gt;Leshem Choshen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1&quot;&gt;Gal Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1&quot;&gt;Omri Abend&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19103">
<title>Does Conceptual Representation Require Embodiment? Insights From Large Language Models. (arXiv:2305.19103v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19103</link>
<description rdf:parseType="Literal">&lt;p&gt;To what extent can language alone give rise to complex concepts, or is
embodied experience essential? Recent advancements in large language models
(LLMs) offer fresh perspectives on this question. Although LLMs are trained on
restricted modalities, they exhibit human-like performance in diverse
psychological tasks. Our study compared representations of 4,442 lexical
concepts between humans and ChatGPTs (GPT-3.5 and GPT-4) across multiple
dimensions, including five key domains: emotion, salience, mental
visualization, sensory, and motor experience. We identify two main findings: 1)
Both models strongly align with human representations in non-sensorimotor
domains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5;
2) GPT-4&apos;s gains are associated with its additional visual learning, which also
appears to benefit related dimensions like haptics and imageability. These
results highlight the limitations of language in isolation, and that the
integration of diverse modalities of inputs leads to a more human-like
conceptual representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qihui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yingying Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Minghua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Feng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chodorow_M/0/1/0/all/0/1&quot;&gt;Martin Chodorow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05052">
<title>Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05052</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it&apos;s more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Paiheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyemi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01029">
<title>Explainability for Large Language Models: A Survey. (arXiv:2309.01029v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01029</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated impressive capabilities in
natural language processing. However, their internal mechanisms are still
unclear and this lack of transparency poses unwanted risks for downstream
applications. Therefore, understanding and explaining these models is crucial
for elucidating their behaviors, limitations, and social impacts. In this
paper, we introduce a taxonomy of explainability techniques and provide a
structured overview of methods for explaining Transformer-based language
models. We categorize techniques based on the training paradigms of LLMs:
traditional fine-tuning-based paradigm and prompting-based paradigm. For each
paradigm, we summarize the goals and dominant approaches for generating local
explanations of individual predictions and global explanations of overall model
knowledge. We also discuss metrics for evaluating generated explanations, and
discuss how explanations can be leveraged to debug models and improve
performance. Lastly, we examine key challenges and emerging opportunities for
explanation techniques in the era of LLMs in comparison to conventional machine
learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Huiqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hengyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12931">
<title>On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12931</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised training methods for transformers have demonstrated
remarkable performance across various domains. Previous transformer-based
models, such as masked autoencoders (MAE), typically utilize a single
normalization layer for both the [CLS] symbol and the tokens. We propose in
this paper a simple modification that employs separate normalization layers for
the tokens and the [CLS] symbol to better capture their distinct
characteristics and enhance downstream task performance. Our method aims to
alleviate the potential negative effects of using the same normalization
statistics for both token types, which may not be optimally aligned with their
individual roles. We empirically show that by utilizing a separate
normalization layer, the [CLS] embeddings can better encode the global
contextual information and are distributed more uniformly in its anisotropic
space. When replacing the conventional normalization layer with the two
separate layers, we observe an average 2.7% performance improvement over the
image, natural language, and graph domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinkai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuanqi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1&quot;&gt;Soha Hassoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li-Ping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13731">
<title>Arabic Sentiment Analysis with Noisy Deep Explainable Model. (arXiv:2309.13731v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13731</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentiment Analysis (SA) is an indispensable task for many real-world
applications. Compared to limited resourced languages (i.e., Arabic, Bengali),
most of the research on SA are conducted for high resourced languages (i.e.,
English, Chinese). Moreover, the reasons behind any prediction of the Arabic
sentiment analysis methods exploiting advanced artificial intelligence
(AI)-based approaches are like black-box - quite difficult to understand. This
paper proposes an explainable sentiment classification framework for the Arabic
language by introducing a noise layer on Bi-Directional Long Short-Term Memory
(BiLSTM) and Convolutional Neural Networks (CNN)-BiLSTM models that overcome
over-fitting problem. The proposed framework can explain specific predictions
by training a local surrogate explainable model to understand why a particular
sentiment (positive or negative) is being predicted. We carried out experiments
on public benchmark Arabic SA datasets. The results concluded that adding noise
layers improves the performance in sentiment analysis for the Arabic language
by reducing overfitting and our method outperformed some known state-of-the-art
methods. In addition, the introduced explainability with noise layer could make
the model more transparent and accountable and hence help adopting AI-enabled
system in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atabuzzaman_M/0/1/0/all/0/1&quot;&gt;Md. Atabuzzaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1&quot;&gt;Md Shajalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baby_M/0/1/0/all/0/1&quot;&gt;Maksuda Bilkis Baby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boden_A/0/1/0/all/0/1&quot;&gt;Alexander Boden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14327">
<title>DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14327</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the existing multi-modal models, hindered by their incapacity to
adeptly manage interleaved image-and-text inputs in multi-image, multi-round
dialogues, face substantial constraints in resource allocation for training and
data accessibility, impacting their adaptability and scalability across varied
interaction realms. To address this, we present the DeepSpeed-VisualChat
framework, designed to optimize Large Language Models (LLMs) by incorporating
multi-modal capabilities, with a focus on enhancing the proficiency of Large
Vision and Language Models in handling interleaved inputs. Our framework is
notable for (1) its open-source support for multi-round and multi-image
dialogues, (2) introducing an innovative multi-modal causal attention
mechanism, and (3) utilizing data blending techniques on existing datasets to
assure seamless interactions in multi-round, multi-image conversations.
Compared to existing frameworks, DeepSpeed-VisualChat shows superior
scalability up to 70B parameter language model size, representing a significant
advancement in multi-modal language models and setting a solid foundation for
future explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Conglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Heyang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1&quot;&gt;Olatunji Ruwase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1&quot;&gt;Ammar Ahmad Awan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1&quot;&gt;Samyam Rajbhandari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01929">
<title>Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01929</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have
demonstrated remarkable prompt-based image generation capabilities.
Multilingual encoders may have a substantial impact on the cultural agency of
these models, as language is a conduit of culture. In this study, we explore
the cultural perception embedded in TTI models by characterizing culture across
three hierarchical tiers: cultural dimensions, cultural domains, and cultural
concepts. Based on this ontology, we derive prompt templates to unlock the
cultural knowledge in TTI models, and propose a comprehensive suite of
evaluation techniques, including intrinsic evaluations using the CLIP space,
extrinsic evaluations with a Visual-Question-Answer (VQA) model and human
assessments, to evaluate the cultural content of TTI-generated images. To
bolster our research, we introduce the CulText2I dataset, derived from four
diverse TTI models and spanning ten languages. Our experiments provide insights
regarding Do, What, Which and How research questions about the nature of
cultural encoding in TTI models, paving the way for cross-cultural applications
of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventura_M/0/1/0/all/0/1&quot;&gt;Mor Ventura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1&quot;&gt;Eyal Ben-David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1&quot;&gt;Roi Reichart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05199">
<title>Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05199</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from human feedback serves as a crucial bridge,
aligning large language models with human and societal values. This alignment
requires a vast corpus of human feedback to learn a reward model, which is
subsequently used to finetune language models. However, we have identified that
the reward model often finds shortcuts to bypass its intended objectives,
misleadingly assuming that humans prefer longer responses. The emergence of
length bias often induces the model to favor longer outputs, yet it doesn&apos;t
equate to an increase in helpful information within these outputs. In this
paper, we propose an innovative solution, applying the Product-of-Experts (PoE)
technique to separate reward modeling from the influence of sequence length. In
our framework, the main expert concentrates on understanding human intents,
while the biased expert targets the identification and capture of length bias.
To further enhance the learning of bias, we introduce perturbations into the
bias-focused expert, disrupting the flow of semantic information. Experimental
results validate the effectiveness of our approach, indicating that language
model performance is improved, irrespective of sequence length.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wenyu Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05703">
<title>An Attribution Method for Siamese Encoders. (arXiv:2310.05703v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05703</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of Siamese encoder models such as sentence transformers
(ST), little is known about the aspects of inputs they pay attention to. A
barrier is that their predictions cannot be attributed to individual features,
as they compare two inputs rather than processing a single one. This paper
derives a local attribution method for Siamese encoders by generalizing the
principle of integrated gradients to models with multiple inputs. The solution
takes the form of feature-pair attributions, and can be reduced to a
token-token matrix for STs. Our method involves the introduction of integrated
Jacobians and inherits the advantageous formal properties of integrated
gradients: it accounts for the model&apos;s full computation graph and is guaranteed
to converge to the actual prediction. A pilot study shows that in an ST few
token-pairs can often explain large fractions of predictions, and it focuses on
nouns and verbs. For accurate predictions, it however needs to attend to the
majority of tokens and parts of speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_L/0/1/0/all/0/1&quot;&gt;Lucas M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1&quot;&gt;Dmitry Nikolaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1&quot;&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09949">
<title>Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models. (arXiv:2310.09949v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09949</link>
<description rdf:parseType="Literal">&lt;p&gt;A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1&quot;&gt;Marco Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1&quot;&gt;Roger Waleffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1&quot;&gt;Gustavo Alonso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14566">
<title>HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination &amp; Visual Illusion in Large Vision-Language Models. (arXiv:2310.14566v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14566</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce HallusionBench, a comprehensive benchmark designed for the
evaluation of image-context reasoning. This benchmark presents significant
challenges to advanced large visual-language models (LVLMs), such as
GPT-4V(Vision) and LLaVA-1.5, by emphasizing nuanced understanding and
interpretation of visual data. The benchmark comprises 346 images paired with
1129 questions, all meticulously crafted by human experts. We introduce a novel
structure for these visual questions designed to establish control groups. This
structure enables us to conduct a quantitative analysis of the models&apos; response
tendencies, logical consistency, and various failure modes. In our evaluation
on HallusionBench, we benchmarked 13 different models, highlighting a 31.42%
question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all
other evaluated models achieve accuracy below 16%. Moreover, our analysis not
only highlights the observed failure modes, including language hallucination
and visual illusion, but also deepens an understanding of these pitfalls. Our
comprehensive case studies within HallusionBench shed light on the challenges
of hallucination and illusion in LVLMs. Based on these insights, we suggest
potential pathways for their future improvement. The benchmark and codebase can
be accessed at https://github.com/tianyi-lab/HallusionBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1&quot;&gt;Tianrui Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1&quot;&gt;Ruiqi Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lichang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yacoob_Y/0/1/0/all/0/1&quot;&gt;Yaser Yacoob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18023">
<title>SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis. (arXiv:2310.18023v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18023</link>
<description rdf:parseType="Literal">&lt;p&gt;Code-mixing is a well-studied linguistic phenomenon when two or more
languages are mixed in text or speech. Several datasets have been build with
the goal of training computational models for code-mixing. Although it is very
common to observe code-mixing with multiple languages, most datasets available
contain code-mixed between only two languages. In this paper, we introduce
SentMix-3L, a novel dataset for sentiment analysis containing code-mixed data
between three languages Bangla, English, and Hindi. We carry out a
comprehensive evaluation using SentMix-3L. We show that zero-shot prompting
with GPT-3.5 outperforms all transformer-based models on SentMix-3L.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1&quot;&gt;Md Nishat Raihan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1&quot;&gt;Dhiman Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmud_A/0/1/0/all/0/1&quot;&gt;Antara Mahmud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1&quot;&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1&quot;&gt;Marcos Zampieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18348">
<title>Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18348</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to extract meaning representations from autoregressive language
models by considering the distribution of all possible trajectories extending
an input text. This strategy is prompt-free, does not require fine-tuning, and
is applicable to any pre-trained autoregressive model. Moreover, unlike
vector-based representations, distribution-based representations can also model
asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym
relations) by using algebraic operations between likelihood functions. These
ideas are grounded in distributional perspectives on semantics and are
connected to standard constructions in automata theory, but to our knowledge
they have not been applied to modern language models. We empirically show that
the representations obtained from large models align well with human
annotations, outperform other zero-shot and prompt-free methods on semantic
similarity tasks, and can be used to solve more complex entailment and
containment tasks that standard embeddings cannot handle. Finally, we extend
our method to represent data from different modalities (e.g., image and text)
using multimodal autoregressive models. Our code is available at:
https://github.com/tianyu139/meaning-as-trajectories
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1&quot;&gt;Matthew Trager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1&quot;&gt;Pramuditha Perera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1&quot;&gt;Luca Zancato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10642">
<title>Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers. (arXiv:2311.10642v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10642</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents an analysis of the effectiveness of using standard shallow
feed-forward networks to mimic the behavior of the attention mechanism in the
original Transformer model, a state-of-the-art architecture for
sequence-to-sequence tasks. We substitute key elements of the attention
mechanism in the Transformer with simple feed-forward networks, trained using
the original components via knowledge distillation. Our experiments, conducted
on the IWSLT2017 dataset, reveal the capacity of these &quot;attentionless
Transformers&quot; to rival the performance of the original architecture. Through
rigorous ablation studies, and experimenting with various replacement network
types and sizes, we offer insights that support the viability of our approach.
This not only sheds light on the adaptability of shallow feed-forward networks
in emulating attention mechanisms but also underscores their potential to
streamline complex architectures for sequence-to-sequence tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozic_V/0/1/0/all/0/1&quot;&gt;Vukasin Bozic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dordevic_D/0/1/0/all/0/1&quot;&gt;Danilo Dordevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coppola_D/0/1/0/all/0/1&quot;&gt;Daniele Coppola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thommes_J/0/1/0/all/0/1&quot;&gt;Joseph Thommes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sidak Pal Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10813">
<title>A Language Agent for Autonomous Driving. (arXiv:2311.10813v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiageng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuxi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13534">
<title>LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13534</link>
<description rdf:parseType="Literal">&lt;p&gt;The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose LM-Cocktail which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging, where the fine-tuned language model is merged with the
pre-trained base model or the peer models from other domains through weighted
average. Despite simplicity, LM-Cocktail is surprisingly effective: the
resulted model is able to achieve a strong empirical performance in the whole
scope of general tasks while preserving a superior capacity in its targeted
domain. We conduct comprehensive experiments with LLama and BGE model on
popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shitao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peitian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xingrun Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14743">
<title>A Baseline Analysis of Reward Models&apos; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14743</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models, specifically Large Language Models (LLM&apos;s), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align an LLM. These reward models are additionally used
at inference-time to estimate how well LLM responses adhere to those desired
behaviors. However, there is little work measuring how robust these reward
models are to distribution shifts. In this work, we evaluate how reward model
performance - measured via accuracy and calibration (i.e. alignment between
accuracy and confidence) - is affected by distribution shift. We show novel
calibration patterns and accuracy drops due to OOD prompts and responses, and
that the reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting in order to detect these
distribution shifts in prompts and responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1&quot;&gt;Ben Pikus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1&quot;&gt;Will LeVine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tony Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1&quot;&gt;Sean Hendryx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16203">
<title>ChatTraffic: Text-to-Traffic Generation via Diffusion Model. (arXiv:2311.16203v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16203</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction is one of the most significant foundations in Intelligent
Transportation Systems (ITS). Traditional traffic prediction methods rely only
on historical traffic data to predict traffic trends and face two main
challenges. 1) insensitivity to unusual events. 2) poor performance in
long-term prediction. In this work, we explore how generative models combined
with text describing the traffic system can be applied for traffic generation
and name the task Text-to-Traffic Generation (TTG). The key challenge of the
TTG task is how to associate text with the spatial structure of the road
network and traffic data for generating traffic situations. To this end, we
propose ChatTraffic, the first diffusion model for text-to-traffic generation.
To guarantee the consistency between synthetic and real data, we augment a
diffusion model with the Graph Convolutional Network (GCN) to extract spatial
correlations of traffic data. In addition, we construct a large dataset
containing text-traffic pairs for the TTG task. We benchmarked our model
qualitatively and quantitatively on the released dataset. The experimental
results indicate that ChatTraffic can generate realistic traffic situations
from the text. Our code and dataset are available at
https://github.com/ChyaZhang/ChatTraffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1&quot;&gt;Qitan Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yisheng Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piao_X/0/1/0/all/0/1&quot;&gt;Xinglin Piao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Baocai Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16444">
<title>Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos. (arXiv:2311.16444v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16444</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel benchmark for cross-view knowledge transfer of dense video
captioning, adapting models from web instructional videos with exocentric views
to an egocentric view. While dense video captioning (predicting time segments
and their captions) is primarily studied with exocentric videos (e.g.,
YouCook2), benchmarks with egocentric videos are restricted due to data
scarcity. To overcome the limited video availability, transferring knowledge
from abundant exocentric web videos is demanded as a practical approach.
However, learning the correspondence between exocentric and egocentric views is
difficult due to their dynamic view changes. The web videos contain mixed views
focusing on either human body actions or close-up hand-object interactions,
while the egocentric view is constantly shifting as the camera wearer moves.
This necessitates the in-depth study of cross-view transfer under complex view
changes. In this work, we first create a real-life egocentric dataset (EgoYC2)
whose captions are shared with YouCook2, enabling transfer learning between
these datasets assuming their ground-truth is accessible. To bridge the view
gaps, we propose a view-invariant learning method using adversarial training in
both the pre-training and fine-tuning stages. While the pre-training is
designed to learn invariant features against the mixed views in the web videos,
the view-invariant fine-tuning further mitigates the view gaps between both
datasets. We validate our proposed method by studying how effectively it
overcomes the view change problem and efficiently transfers the knowledge to
the egocentric domain. Our benchmark pushes the study of the cross-view
transfer into a new task domain of dense video captioning and will envision
methodologies to describe egocentric videos in natural language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1&quot;&gt;Takehiko Ohkawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1&quot;&gt;Takuma Yagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_T/0/1/0/all/0/1&quot;&gt;Taichi Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1&quot;&gt;Ryosuke Furuta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1&quot;&gt;Atsushi Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yoichi Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16989">
<title>ChatGPT&apos;s One-year Anniversary: Are Open-Source Large Language Models Catching up?. (arXiv:2311.16989v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16989</link>
<description rdf:parseType="Literal">&lt;p&gt;Upon its release in late 2022, ChatGPT has brought a seismic shift in the
entire landscape of AI, both in research and commerce. Through
instruction-tuning a large language model (LLM) with supervised fine-tuning and
reinforcement learning from human feedback, it showed that a model could answer
human questions and follow instructions on a broad panel of tasks. Following
this success, interests in LLMs have intensified, with new LLMs flourishing at
frequent interval across academia and industry, including many start-ups
focused on LLMs. While closed-source LLMs (e.g., OpenAI&apos;s GPT, Anthropic&apos;s
Claude) generally outperform their open-source counterparts, the progress on
the latter has been rapid with claims of achieving parity or even better on
certain tasks. This has crucial implications not only on research but also on
business. In this work, on the first anniversary of ChatGPT, we provide an
exhaustive overview of this success, surveying all tasks where an open-source
LLM has claimed to be on par or better than ChatGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hailin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1&quot;&gt;Fangkai Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chengwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1&quot;&gt;Mathieu Ravaut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ruochen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17041">
<title>Efficient In-Context Learning in Vision-Language Models for Egocentric Videos. (arXiv:2311.17041v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17041</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in text-only large language models (LLMs) have
highlighted the benefit of in-context learning for adapting to new tasks with a
few demonstrations. However, extending in-context learning to large
vision-language models (VLMs) using a huge amount of naturalistic
vision-language data has shown limited success, particularly for egocentric
videos, due to high data collection costs. We propose a novel training method
$\mathbb{E}$fficient $\mathbb{I}$n-context $\mathbb{L}$earning on
$\mathbb{E}$gocentric $\mathbb{V}$ideos ($\mathbb{EILEV}$), which elicits
in-context learning in VLMs for egocentric videos without requiring massive,
naturalistic egocentric video datasets. $\mathbb{EILEV}$ involves architectural
and training data adaptations to allow the model to process contexts
interleaved with video clips and narrations, sampling of in-context examples
with clusters of similar verbs and nouns, use of data with skewed marginal
distributions with a long tail of infrequent verbs and nouns, as well as
homonyms and synonyms. Our evaluations show that $\mathbb{EILEV}$-trained
models outperform larger VLMs trained on a huge amount of naturalistic data in
in-context learning. Furthermore, they can generalize to not only
out-of-distribution, but also novel, rare egocentric videos and texts via
in-context learning, demonstrating potential for applications requiring
cost-effective training, and rapid post-deployment adaptability. Our code and
demo are available at \url{https://github.com/yukw777/EILEV}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Keunwoo Peter Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1&quot;&gt;Fengyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Joyce Chai&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>