<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.06467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02111" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.02167">
<title>Uncertainty Quantification in Machine Learning Based Segmentation: A Post-Hoc Approach for Left Ventricle Volume Estimation in MRI. (arXiv:2312.02167v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02167</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have confirmed cardiovascular diseases remain responsible for
highest death toll amongst non-communicable diseases. Accurate left ventricular
(LV) volume estimation is critical for valid diagnosis and management of
various cardiovascular conditions, but poses significant challenge due to
inherent uncertainties associated with segmentation algorithms in magnetic
resonance imaging (MRI). Recent machine learning advancements, particularly
U-Net-like convolutional networks, have facilitated automated segmentation for
medical images, but struggles under certain pathologies and/or different
scanner vendors and imaging protocols. This study proposes a novel methodology
for post-hoc uncertainty estimation in LV volume prediction using It\^{o}
stochastic differential equations (SDEs) to model path-wise behavior for the
prediction error. The model describes the area of the left ventricle along the
heart&apos;s long axis. The method is agnostic to the underlying segmentation
algorithm, facilitating its use with various existing and future segmentation
technologies. The proposed approach provides a mechanism for quantifying
uncertainty, enabling medical professionals to intervene for unreliable
predictions. This is of utmost importance in critical applications such as
medical diagnosis, where prediction accuracy and reliability can directly
impact patient outcomes. The method is also robust to dataset changes, enabling
application for medical centers with limited access to labeled data. Our
findings highlight the proposed uncertainty estimation methodology&apos;s potential
to enhance automated segmentation robustness and generalizability, paving the
way for more reliable and accurate LV volume estimation in clinical settings as
well as opening new avenues for uncertainty quantification in biomedical image
segmentation, providing promising directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terhag_F/0/1/0/all/0/1&quot;&gt;F. Terhag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knechtges_P/0/1/0/all/0/1&quot;&gt;P. Knechtges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basermann_A/0/1/0/all/0/1&quot;&gt;A. Basermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tempone_R/0/1/0/all/0/1&quot;&gt;R. Tempone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02168">
<title>The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch. (arXiv:2312.02168v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02168</link>
<description rdf:parseType="Literal">&lt;p&gt;The Street View House Numbers (SVHN) dataset is a popular benchmark dataset
in deep learning. Originally designed for digit classification tasks, the SVHN
dataset has been widely used as a benchmark for various other tasks including
generative modeling. However, with this work, we aim to warn the community
about an issue of the SVHN dataset as a benchmark for generative modeling
tasks: we discover that the official split into training set and test set of
the SVHN dataset are not drawn from the same distribution. We empirically show
that this distribution mismatch has little impact on the classification task
(which may explain why this issue has not been detected before), but it
severely affects the evaluation of probabilistic generative models, such as
Variational Autoencoders and diffusion models. As a workaround, we propose to
mix and re-split the official training and test set when SVHN is used for tasks
other than classification. We publish a new split and the indices we used to
create it at https://jzenn.github.io/svhn-remix/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tim Z. Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenn_J/0/1/0/all/0/1&quot;&gt;Johannes Zenn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1&quot;&gt;Robert Bamler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02173">
<title>TailorMe: Self-Supervised Learning of an Anatomically Constrained Volumetric Human Shape Model. (arXiv:2312.02173v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02173</link>
<description rdf:parseType="Literal">&lt;p&gt;Human shape spaces have been extensively studied, as they are a core element
of human shape and pose inference tasks. Classic methods for creating a human
shape model register a surface template mesh to a database of 3D scans and use
dimensionality reduction techniques, such as Principal Component Analysis, to
learn a compact representation. While these shape models enable global shape
modifications by correlating anthropometric measurements with the learned
subspace, they only provide limited localized shape control. We instead
register a volumetric anatomical template, consisting of skeleton bones and
soft tissue, to the surface scans of the CAESAR database. We further enlarge
our training data to the full Cartesian product of all skeletons and all soft
tissues using physically plausible volumetric deformation transfer. This data
is then used to learn an anatomically constrained volumetric human shape model
in a self-supervised fashion. The resulting TailorMe model enables shape
sampling, localized shape manipulation, and fast inference from given surface
scans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenninger_S/0/1/0/all/0/1&quot;&gt;Stephan Wenninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemper_F/0/1/0/all/0/1&quot;&gt;Fabian Kemper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwanecke_U/0/1/0/all/0/1&quot;&gt;Ulrich Schwanecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1&quot;&gt;Mario Botsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02185">
<title>Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition. (arXiv:2312.02185v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.02185</link>
<description rdf:parseType="Literal">&lt;p&gt;Various types of sensors can be used for Human Activity Recognition (HAR),
and each of them has different strengths and weaknesses. Sometimes a single
sensor cannot fully observe the user&apos;s motions from its perspective, which
causes wrong predictions. While sensor fusion provides more information for
HAR, it comes with many inherent drawbacks like user privacy and acceptance,
costly set-up, operation, and maintenance. To deal with this problem, we
propose Virtual Fusion - a new method that takes advantage of unlabeled data
from multiple time-synchronized sensors during training, but only needs one
sensor for inference. Contrastive learning is adopted to exploit the
correlation among sensors. Virtual Fusion gives significantly better accuracy
than training with the same single sensor, and in some cases, it even surpasses
actual fusion using multiple sensors at test time. We also extend this method
to a more general version called Actual Fusion within Virtual Fusion (AFVF),
which uses a subset of training sensors during inference. Our method achieves
state-of-the-art accuracy and F1-score on UCI-HAR and PAMAP2 benchmark
datasets. Implementation is available upon request.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc-Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1&quot;&gt;Cuong Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Khac_N/0/1/0/all/0/1&quot;&gt;Nhien-An Le-Khac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02186">
<title>Identifying Spurious Correlations using Counterfactual Alignment. (arXiv:2312.02186v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02186</link>
<description rdf:parseType="Literal">&lt;p&gt;Models driven by spurious correlations often yield poor generalization
performance. We propose the counterfactual alignment method to detect and
explore spurious correlations of black box classifiers. Counterfactual images
generated with respect to one classifier can be input into other classifiers to
see if they also induce changes in the outputs of these classifiers. The
relationship between these responses can be quantified and used to identify
specific instances where a spurious correlation exists as well as compute
aggregate statistics over a dataset. Our work demonstrates the ability to
detect spurious correlations in face attribute classifiers. This is validated
by observing intuitive trends in a face attribute classifier as well as
fabricating spurious correlations and detecting their presence, both visually
and quantitatively. Further, utilizing the CF alignment method, we demonstrate
that we can rectify spurious correlations identified in classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Joseph Paul Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blankemeier_L/0/1/0/all/0/1&quot;&gt;Louis Blankemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1&quot;&gt;Akshay Chaudhari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02188">
<title>Video Summarization: Towards Entity-Aware Captions. (arXiv:2312.02188v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02188</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing popular video captioning benchmarks and models deal with generic
captions devoid of specific person, place or organization named entities. In
contrast, news videos present a challenging setting where the caption requires
such named entities for meaningful summarization. As such, we propose the task
of summarizing news video directly to entity-aware captions. We also release a
large-scale dataset, VIEWS (VIdeo NEWS), to support research on this task.
Further, we propose a method that augments visual information from videos with
context retrieved from external world knowledge to generate entity-aware
captions. We demonstrate the effectiveness of our approach on three video
captioning models. We also show that our approach generalizes to existing news
image captions dataset. With all the extensive experiments and insights, we
believe we establish a solid basis for future research on this challenging
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1&quot;&gt;Hammad A. Ayyubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1&quot;&gt;Arsha Nagrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xudong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1&quot;&gt;Anurag Arnab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1&quot;&gt;Feng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yukun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jialu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Fu Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02189">
<title>StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D. (arXiv:2312.02189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02189</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of text-to-3D generation, utilizing 2D diffusion models through
score distillation sampling (SDS) frequently leads to issues such as blurred
appearances and multi-faced geometry, primarily due to the intrinsically noisy
nature of the SDS loss. Our analysis identifies the core of these challenges as
the interaction among noise levels in the 2D diffusion process, the
architecture of the diffusion network, and the 3D model representation. To
overcome these limitations, we present StableDreamer, a methodology
incorporating three advances. First, inspired by InstructNeRF2NeRF, we
formalize the equivalence of the SDS generative prior and a simple supervised
L2 reconstruction loss. This finding provides a novel tool to debug SDS, which
we use to show the impact of time-annealing noise levels on reducing
multi-faced geometries. Second, our analysis shows that while image-space
diffusion contributes to geometric precision, latent-space diffusion is crucial
for vivid color rendition. Based on this observation, StableDreamer introduces
a two-stage training strategy that effectively combines these aspects,
resulting in high-fidelity 3D models. Third, we adopt an anisotropic 3D
Gaussians representation, replacing Neural Radiance Fields (NeRFs), to enhance
the overall quality, reduce memory usage during training, and accelerate
rendering speeds, and better capture semi-transparent objects. StableDreamer
reduces multi-face geometries, generates fine details, and converges stably.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Pengsheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1&quot;&gt;Hans Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caccavale_A/0/1/0/all/0/1&quot;&gt;Adam Caccavale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhongzheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Edward Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1&quot;&gt;Qi Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_A/0/1/0/all/0/1&quot;&gt;Aditya Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1&quot;&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1&quot;&gt;Alex Colburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fangchang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02190">
<title>Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D. (arXiv:2312.02190v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02190</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Handles is a novel approach to enabling 3D object edits on
diffusion images. We accomplish these edits using existing pre-trained
diffusion models, and 2D image depth estimation, without any fine-tuning or 3D
object retrieval. The edited results remain plausible, photo-real, and preserve
object identity. Diffusion Handles address a critically missing facet of
generative image based creative design, and significantly advance the
state-of-the-art in generative image editing. Our key insight is to lift
diffusion activations for an object to 3D using a proxy depth, 3D-transform the
depth and associated activations, and project them back to image space. The
diffusion process applied to the manipulated activations with identity control,
produces plausible edited images showing complex 3D occlusion and lighting
effects. We evaluate Diffusion Handles: quantitatively, on a large synthetic
data benchmark; and qualitatively by a user study, showing our output to be
more plausible, and better than prior art at both, 3D editing and identity
control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1&quot;&gt;Karran Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1&quot;&gt;Paul Guerrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1&quot;&gt;Matheus Gadelha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hold_Geoffroy_Y/0/1/0/all/0/1&quot;&gt;Yannick Hold-Geoffroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1&quot;&gt;Niloy Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02191">
<title>Prompt Tuning for Zero-shot Compositional Learning. (arXiv:2312.02191v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02191</link>
<description rdf:parseType="Literal">&lt;p&gt;Open World Compositional Zero-Shot Learning (OW-CZSL) is known to be an
extremely challenging task, which aims to recognize unseen compositions formed
from seen attributes and objects without any prior assumption of the output
space. In order to achieve this goal, a model has to be &quot;smart&quot; and
&quot;knowledgeable&quot;. To be smart, a model should be good at reasoning the
interactions between attributes and objects from the seen compositions. While
&quot;knowledgeable&quot; means the model owns &quot;common sense&quot; to the open world that can
&quot;foresee&quot; some features of the unseen compositions. Most previous work focuses
on the &quot;smart&quot; part, while few of them provided an effective solution to
achieve the &quot;knowledgeable&quot; goal. In this paper, we proposed a framework named
Multi-Modal Prompt Tuning (MMPT) to inherit the &quot;knowledgeable&quot; property from
the large pre-trained vision-language model. Extensive experiments show that
our proposed MMPT obtains new state-of-the-art results in OW-CZSL task. On the
UT-Zappos dataset, MMPT pushes the AUC score to $29.8$, while the previous best
score is $26.5$. On the more challenging MIT-States dataset, the AUC score of
MMPT is 1.5 times better than the current state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lingyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1&quot;&gt;Ting Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yilin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongxia Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02192">
<title>DiverseDream: Diverse Text-to-3D Synthesis with Augmented Text Embedding. (arXiv:2312.02192v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02192</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-3D synthesis has recently emerged as a new approach to sampling 3D
models by adopting pretrained text-to-image models as guiding visual priors. An
intriguing but underexplored problem with existing text-to-3D methods is that
3D models obtained from the sampling-by-optimization procedure tend to have
mode collapses, and hence poor diversity in their results. In this paper, we
provide an analysis and identify potential causes of such a limited diversity,
and then devise a new method that considers the joint generation of different
3D models from the same text prompt, where we propose to use augmented text
prompts via textual inversion of reference images to diversify the joint
generation. We show that our method leads to improved diversity in text-to-3D
synthesis qualitatively and quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_U/0/1/0/all/0/1&quot;&gt;Uy Dieu Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_M/0/1/0/all/0/1&quot;&gt;Minh Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1&quot;&gt;Janne Heikkila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khoi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1&quot;&gt;Binh-Son Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02194">
<title>Local Masking Meets Progressive Freezing: Crafting Efficient Vision Transformers for Self-Supervised Learning. (arXiv:2312.02194v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02194</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an innovative approach to self-supervised learning
for Vision Transformers (ViTs), integrating local masked image modeling with
progressive layer freezing. This method focuses on enhancing the efficiency and
speed of initial layer training in ViTs. By systematically freezing specific
layers at strategic points during training, we reduce computational demands
while maintaining or improving learning capabilities. Our approach employs a
novel multi-scale reconstruction process that fosters efficient learning in
initial layers and enhances semantic comprehension across scales. The results
demonstrate a substantial reduction in training time (~12.5\%) with a minimal
impact on model accuracy (decrease in top-1 accuracy by 0.6\%). Our method
achieves top-1 and top-5 accuracies of 82.6\% and 96.2\%, respectively,
underscoring its potential in scenarios where computational resources and time
are critical. This work marks an advancement in the field of self-supervised
learning for computer vision. The implementation of our approach is available
at our project&apos;s GitHub repository: github.com/utkutpcgl/ViTFreeze.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topcuoglu_U/0/1/0/all/0/1&quot;&gt;Utku Mert Topcuoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akagunduz_E/0/1/0/all/0/1&quot;&gt;Erdem Akag&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02196">
<title>Dynamic Inertial Poser (DynaIP): Part-Based Motion Dynamics Learning for Enhanced Human Pose Estimation with Sparse Inertial Sensors. (arXiv:2312.02196v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02196</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel human pose estimation approach using sparse
inertial sensors, addressing the shortcomings of previous methods reliant on
synthetic data. It leverages a diverse array of real inertial motion capture
data from different skeleton formats to improve motion diversity and model
generalization. This method features two innovative components: a
pseudo-velocity regression model for dynamic motion capture with inertial
sensors, and a part-based model dividing the body and sensor data into three
regions, each focusing on their unique characteristics. The approach
demonstrates superior performance over state-of-the-art models across five
public datasets, notably reducing pose error by 19\% on the DIP-IMU dataset,
thus representing a significant improvement in inertial sensor-based human pose
estimation. We will make the implementation of our model available for public
use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Songpengcheng Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1&quot;&gt;Lei Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiarui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_L/0/1/0/all/0/1&quot;&gt;Ling Pei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02197">
<title>Exploiting Diffusion Priors for All-in-One Image Restoration. (arXiv:2312.02197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02197</link>
<description rdf:parseType="Literal">&lt;p&gt;All-in-one aims to solve various tasks of image restoration in a single
model. To this end, we present a feasible way of exploiting the image priors
captured by the pretrained diffusion model, through addressing the two
challenges, i.e., degradation modeling and diffusion guidance. The former aims
to simulate the process of the clean image degenerated by certain degradations,
and the latter aims at guiding the diffusion model to generate the
corresponding clean image. With the motivations, we propose a zero-shot
framework for all-in-one image restoration, termed ZeroAIR, which alternatively
performs the test-time degradation modeling (TDM) and the three-stage diffusion
guidance (TDG) at each timestep of the reverse sampling. To be specific, TDM
exploits the diffusion priors to learn a degradation model from a given
degraded image, and TDG divides the timesteps into three stages for taking full
advantage of the varying diffusion priors. Thanks to their degradation-agnostic
property, the all-in-one image restoration could be achieved in a zero-shot way
by ZeroAIR. Through extensive experiments, we show that our ZeroAIR achieves
comparable even better performance than those task-specific methods. The code
will be available on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1&quot;&gt;Yuanbiao Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xinyan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02199">
<title>USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery. (arXiv:2312.02199v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02199</link>
<description rdf:parseType="Literal">&lt;p&gt;Large, self-supervised vision models have led to substantial advancements for
automatically interpreting natural images. Recent works have begun tailoring
these methods to remote sensing data which has rich structure with
multi-sensor, multi-spectral, and temporal information providing massive
amounts of self-labeled data that can be used for self-supervised pre-training.
In this work, we develop a new encoder architecture called USat that can input
multi-spectral data from multiple sensors for self-supervised pre-training.
USat is a vision transformer with modified patch projection layers and
positional encodings to model spectral bands with varying spatial scales from
multiple sensors. We integrate USat into a Masked Autoencoder (MAE)
self-supervised pre-training procedure and find that a pre-trained USat
outperforms state-of-the-art self-supervised MAE models trained on remote
sensing data on multiple remote sensing benchmark datasets (up to 8%) and leads
to improvements in low data regimes (up to 7%). Code and pre-trained weights
are available at https://github.com/stanfordmlgroup/USat .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1&quot;&gt;Jeremy Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Lucas Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joanne Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuntao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nashold_L/0/1/0/all/0/1&quot;&gt;Langston Nashold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Benjamin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Y. Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02200">
<title>An Empirical Study of Automated Mislabel Detection in Real World Vision Datasets. (arXiv:2312.02200v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02200</link>
<description rdf:parseType="Literal">&lt;p&gt;Major advancements in computer vision can primarily be attributed to the use
of labeled datasets. However, acquiring labels for datasets often results in
errors which can harm model performance. Recent works have proposed methods to
automatically identify mislabeled images, but developing strategies to
effectively implement them in real world datasets has been sparsely explored.
Towards improved data-centric methods for cleaning real world vision datasets,
we first conduct more than 200 experiments carefully benchmarking recently
developed automated mislabel detection methods on multiple datasets under a
variety of synthetic and real noise settings with varying noise levels. We
compare these methods to a Simple and Efficient Mislabel Detector (SEMD) that
we craft, and find that SEMD performs similarly to or outperforms prior
mislabel detection approaches. We then apply SEMD to multiple real world
computer vision datasets and test how dataset size, mislabel removal strategy,
and mislabel removal amount further affect model performance after retraining
on the cleaned data. With careful design of the approach, we find that mislabel
removal leads per-class performance improvements of up to 8% of a retrained
classifier in smaller data regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srikanth_M/0/1/0/all/0/1&quot;&gt;Maya Srikanth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1&quot;&gt;Jeremy Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_B/0/1/0/all/0/1&quot;&gt;Brian Wesley Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godoy_F/0/1/0/all/0/1&quot;&gt;Felipe Godoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabane_I/0/1/0/all/0/1&quot;&gt;Ishan Sabane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Y. Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02201">
<title>ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation. (arXiv:2312.02201v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02201</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce &quot;ImageDream,&quot; an innovative image-prompt, multi-view diffusion
model for 3D object generation. ImageDream stands out for its ability to
produce 3D models of higher quality compared to existing state-of-the-art,
image-conditioned methods. Our approach utilizes a canonical camera
coordination for the objects in images, improving visual geometry accuracy. The
model is designed with various levels of control at each block inside the
diffusion model based on the input image, where global control shapes the
overall object layout and local control fine-tunes the image details. The
effectiveness of ImageDream is demonstrated through extensive evaluations using
a standard prompt list. For more information, visit our project page at
https://Image-Dream.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yichun Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02202">
<title>Volumetric Rendering with Baked Quadrature Fields. (arXiv:2312.02202v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2312.02202</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel Neural Radiance Field (NeRF) representation for non-opaque
scenes that allows fast inference by utilizing textured polygons. Despite the
high-quality novel view rendering that NeRF provides, a critical limitation is
that it relies on volume rendering that can be computationally expensive and
does not utilize the advancements in modern graphics hardware. Existing methods
for this problem fall short when it comes to modelling volumetric effects as
they rely purely on surface rendering. We thus propose to model the scene with
polygons, which can then be used to obtain the quadrature points required to
model volumetric effects, and also their opacity and colour from the texture.
To obtain such polygonal mesh, we train a specialized field whose
zero-crossings would correspond to the quadrature points when volume rendering,
and perform marching cubes on this field. We then rasterize the polygons and
utilize the fragment shaders to obtain the final colour image. Our method
allows rendering on various devices and easy integration with existing graphics
frameworks while keeping the benefits of volume rendering alive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gopal Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebain_D/0/1/0/all/0/1&quot;&gt;Daniel Rebain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02205">
<title>Disentangling the Effects of Data Augmentation and Format Transform in Self-Supervised Learning of Image Representations. (arXiv:2312.02205v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02205</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-Supervised Learning (SSL) enables training performant models using
limited labeled data. One of the pillars underlying vision SSL is the use of
data augmentations/perturbations of the input which do not significantly alter
its semantic content. For audio and other temporal signals, augmentations are
commonly used alongside format transforms such as Fourier transforms or wavelet
transforms. Unlike augmentations, format transforms do not change the
information contained in the data; rather, they express the same information in
different coordinates. In this paper, we study the effects of format transforms
and augmentations both separately and together on vision SSL. We define
augmentations in frequency space called Fourier Domain Augmentations (FDA) and
show that training SSL models on a combination of these and image augmentations
can improve the downstream classification accuracy by up to 1.3% on
ImageNet-1K. We also show improvements against SSL baselines in few-shot and
transfer learning setups using FDA. Surprisingly, we also observe that format
transforms can improve the quality of learned representations even without
augmentations; however, the combination of the two techniques yields better
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1&quot;&gt;Neha Kalibhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morningstar_W/0/1/0/all/0/1&quot;&gt;Warren Morningstar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bijamov_A/0/1/0/all/0/1&quot;&gt;Alex Bijamov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Luyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1&quot;&gt;Karan Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1&quot;&gt;Philip Mansfield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02207">
<title>TranSegPGD: Improving Transferability of Adversarial Examples on Semantic Segmentation. (arXiv:2312.02207v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02207</link>
<description rdf:parseType="Literal">&lt;p&gt;Transferability of adversarial examples on image classification has been
systematically explored, which generates adversarial examples in black-box
mode. However, the transferability of adversarial examples on semantic
segmentation has been largely overlooked. In this paper, we propose an
effective two-stage adversarial attack strategy to improve the transferability
of adversarial examples on semantic segmentation, dubbed TranSegPGD.
Specifically, at the first stage, every pixel in an input image is divided into
different branches based on its adversarial property. Different branches are
assigned different weights for optimization to improve the adversarial
performance of all pixels.We assign high weights to the loss of the
hard-to-attack pixels to misclassify all pixels. At the second stage, the
pixels are divided into different branches based on their transferable property
which is dependent on Kullback-Leibler divergence. Different branches are
assigned different weights for optimization to improve the transferability of
the adversarial examples. We assign high weights to the loss of the
high-transferability pixels to improve the transferability of adversarial
examples. Extensive experiments with various segmentation models are conducted
on PASCAL VOC 2012 and Cityscapes datasets to demonstrate the effectiveness of
the proposed method. The proposed adversarial attack method can achieve
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaojun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yihao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Simeng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02208">
<title>A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing. (arXiv:2312.02208v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02208</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing state-of-the-art 3D point clouds understanding methods only perform
well in a fully supervised manner. To the best of our knowledge, there exists
no unified framework which simultaneously solves the downstream high-level
understanding tasks, especially when labels are extremely limited. This work
presents a general and simple framework to tackle point clouds understanding
when labels are limited. We propose a novel unsupervised region expansion based
clustering method for generating clusters. More importantly, we innovatively
propose to learn to merge the over-divided clusters based on the local
low-level geometric property similarities and the learned high-level feature
similarities supervised by weak labels. Hence, the true weak labels guide
pseudo labels merging taking both geometric and semantic feature correlations
into consideration. Finally, the self-supervised reconstruction and data
augmentation optimization modules are proposed to guide the propagation of
labels among semantically similar points within a scene. Experimental Results
demonstrate that our framework has the best performance among the three most
important weakly supervised point clouds understanding tasks including semantic
segmentation, instance segmentation, and object detection even when limited
points are labeled, under the data-efficient settings for the large-scale 3D
semantic scene parsing. The developed techniques have postentials to be applied
to downstream tasks for better representations in robotic manipulation and
robotic autonomous navigation. Codes and models are publicly available at:
https://github.com/KangchengLiu.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangcheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02209">
<title>AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing. (arXiv:2312.02209v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02209</link>
<description rdf:parseType="Literal">&lt;p&gt;Editable 3D-aware generation, which supports user-interacted editing, has
witnessed rapid development recently. However, existing editable 3D GANs either
fail to achieve high-accuracy local editing or suffer from huge computational
costs. We propose AttriHuman-3D, an editable 3D human generation model, which
address the aforementioned problems with attribute decomposition and indexing.
The core idea of the proposed model is to generate all attributes (e.g. human
body, hair, clothes and so on) in an overall attribute space with six feature
planes, which are then decomposed and manipulated with different attribute
indexes. To precisely extract features of different attributes from the
generated feature planes, we propose a novel attribute indexing method as well
as an orthogonal projection regularization to enhance the disentanglement. We
also introduce a hyper-latent training strategy and an attribute-specific
sampling strategy to avoid style entanglement and misleading punishment from
the discriminator. Our method allows users to interactively edit selected
attributes in the generated 3D human avatars while keeping others fixed. Both
qualitative and quantitative experiments demonstrate that our model provides a
strong disentanglement between different attributes, allows fine-grained image
editing and generates high-quality 3D human avatars.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaosheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Si Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02211">
<title>Cycle-consistent Generative Adversarial Network Synthetic CT for MR-only Adaptive Radiation Therapy on MR-Linac. (arXiv:2312.02211v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2312.02211</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: This study assesses the effectiveness of Deep Learning (DL) for
creating synthetic CT (sCT) images in MR-guided adaptive radiation therapy
(MRgART).
&lt;/p&gt;
&lt;p&gt;Methods: A Cycle-GAN model was trained with MRI and CT scan slices from
MR-LINAC treatments, generating sCT volumes. The analysis involved
retrospective treatment plan data from patients with various tumors. sCT images
were compared with standard CT scans using mean absolute error in Hounsfield
Units (HU) and image similarity metrics (SSIM, PSNR, NCC). sCT volumes were
integrated into a clinical treatment system for dosimetric re-evaluation.
&lt;/p&gt;
&lt;p&gt;Results: The model, trained on 8405 frames from 57 patients and tested on 357
sCT frames from 17 patients, showed sCTs comparable to dCTs in electron density
and structural similarity with MRI scans. The MAE between sCT and dCT was 49.2
+/- 13.2 HU, with sCT NCC exceeding dCT by 0.06, and SSIM and PSNR at 0.97 +/-
0.01 and 19.9 +/- 1.6 respectively. Dosimetric evaluations indicated minimal
differences between sCTs and dCTs, with sCTs showing better air-bubble
reconstruction.
&lt;/p&gt;
&lt;p&gt;Conclusions: DL-based sCT generation on MR-Linacs is accurate for dose
calculation and optimization in MRgART. This could facilitate MR-only treatment
planning, enhancing simulation and adaptive planning efficiency on MR-Linacs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Asher_G/0/1/0/all/0/1&quot;&gt;Gabriel L. Asher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zaki_B/0/1/0/all/0/1&quot;&gt;Bassem I. Zaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Russo_G/0/1/0/all/0/1&quot;&gt;Gregory A. Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gill_G/0/1/0/all/0/1&quot;&gt;Gobind S. Gill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thomas_C/0/1/0/all/0/1&quot;&gt;Charles R. Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Prioleau_T/0/1/0/all/0/1&quot;&gt;Temiloluwa O. Prioleau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hunt_B/0/1/0/all/0/1&quot;&gt;Brady Hunt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02212">
<title>Portrait Diffusion: Training-free Face Stylization with Chain-of-Painting. (arXiv:2312.02212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02212</link>
<description rdf:parseType="Literal">&lt;p&gt;Face stylization refers to the transformation of a face into a specific
portrait style. However, current methods require the use of example-based
adaptation approaches to fine-tune pre-trained generative models so that they
demand lots of time and storage space and fail to achieve detailed style
transformation. This paper proposes a training-free face stylization framework,
named Portrait Diffusion. This framework leverages off-the-shelf text-to-image
diffusion models, eliminating the need for fine-tuning specific examples.
Specifically, the content and style images are first inverted into latent
codes. Then, during image reconstruction using the corresponding latent code,
the content and style features in the attention space are delicately blended
through a modified self-attention operation called Style Attention Control.
Additionally, a Chain-of-Painting method is proposed for the gradual redrawing
of unsatisfactory areas from rough adjustments to fine-tuning. Extensive
experiments validate the effectiveness of our Portrait Diffusion method and
demonstrate the superiority of Chain-of-Painting in achieving precise face
stylization. Code will be released at
\url{https://github.com/liujin112/PortraitDiffusion}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02214">
<title>FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS. (arXiv:2312.02214v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02214</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose FlashAvatar, a novel and lightweight 3D animatable avatar
representation that could reconstruct a digital avatar from a short monocular
video sequence in minutes and render high-fidelity photo-realistic images at
300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D
Gaussian field embedded in the surface of a parametric face model and learn
extra spatial offset to model non-surface regions and subtle facial details.
While full use of geometric priors can capture high-frequency facial details
and preserve exaggerated expressions, proper initialization can help reduce the
number of Gaussians, thus enabling super-fast rendering speed. Extensive
experimental results demonstrate that FlashAvatar outperforms existing works
regarding visual quality and personalized details and is almost an order of
magnitude faster in rendering speed. Project page:
https://ustc3dv.github.io/FlashAvatar/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1&quot;&gt;Jun Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yudong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juyong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02216">
<title>DragVideo: Interactive Drag-style Video Editing. (arXiv:2312.02216v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2312.02216</link>
<description rdf:parseType="Literal">&lt;p&gt;Editing visual content on videos remains a formidable challenge with two main
issues: 1) direct and easy user control to produce 2) natural editing results
without unsightly distortion and artifacts after changing shape, expression and
layout. Inspired by DragGAN, a recent image-based drag-style editing technique,
we address above issues by proposing DragVideo, where a similar drag-style user
interaction is adopted to edit video content while maintaining temporal
consistency. Empowered by recent diffusion models as in DragDiffusion,
DragVideo contains the novel Drag-on-Video U-Net (DoVe) editing method, which
optimizes diffused video latents generated by video U-Net to achieve the
desired control. Specifically, we use Sample-specific LoRA fine-tuning and
Mutual Self-Attention control to ensure faithful reconstruction of video from
the DoVe method. We also present a series of testing examples for drag-style
video editing and conduct extensive experiments across a wide array of
challenging editing tasks, such as motion editing, skeleton editing, etc,
underscoring DragVideo&apos;s versatility and generality. Our codes including the
DragVideo web user interface will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yufan Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02218">
<title>WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance Fields. (arXiv:2312.02218v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02218</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic Neural Radiance Fields (Dynamic NeRF) enhance NeRF technology to
model moving scenes. However, they are resource intensive and challenging to
compress. To address this issue, this paper presents WavePlanes, a fast and
more compact explicit model. We propose a multi-scale space and space-time
feature plane representation using N-level 2-D wavelet coefficients. The
inverse discrete wavelet transform reconstructs N feature signals at varying
detail, which are linearly decoded to approximate the color and density of
volumes in a 4-D grid. Exploiting the sparsity of wavelet coefficients, we
compress a Hash Map containing only non-zero coefficients and their locations
on each plane. This results in a compressed model size of ~12 MB. Compared with
state-of-the-art plane-based models, WavePlanes is up to 15x smaller, less
computationally demanding and achieves comparable results in as little as one
hour of training - without requiring custom CUDA code or high performance
computing resources. Additionally, we propose new feature fusion schemes that
work as well as previously proposed schemes while providing greater
interpretability. Our code is available at:
https://github.com/azzarelli/waveplanes/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azzarelli_A/0/1/0/all/0/1&quot;&gt;Adrian Azzarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1&quot;&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David R Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02219">
<title>Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models. (arXiv:2312.02219v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02219</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Vision and Language Models have enabled significant advances in fully
supervised and zero-shot vision tasks. These large pre-trained architectures
serve as the baseline to what is currently known as Instruction Tuning Large
Vision and Language models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal
assistants whose responses are modulated by natural language instructions and
arbitrary visual data. Despite this versatility, IT-LVLM effectiveness in
fundamental computer vision problems remains unclear, primarily due to the
absence of a standardized evaluation benchmark. This paper introduces a
Multi-modal Evaluation Benchmark named MERLIM, a scalable test-bed to assess
the performance of IT-LVLMs on fundamental computer vision tasks. MERLIM
contains over 279K image-question pairs, and has a strong focus on detecting
cross-modal &quot;hallucination&quot; events in IT-LVLMs, where the language output
refers to visual concepts that lack any effective grounding in the image. Our
results show that state-of-the-art IT-LVMLs are still limited at identifying
fine-grained visual concepts, object hallucinations are common across tasks,
and their results are strongly biased by small variations in the input query,
even if the queries have the very same semantics. Our findings also suggest
that these models have weak visual groundings but they can still make adequate
guesses by global visual patterns or textual biases contained in the LLM
component.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villa_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Villa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1&quot;&gt;Juan Carlos Le&amp;#xf3;n Alc&amp;#xe1;zar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1&quot;&gt;Alvaro Soto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02220">
<title>QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers. (arXiv:2312.02220v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02220</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been a significant trend in deep neural networks
(DNNs), particularly transformer-based models, of developing ever-larger and
more capable models. While they demonstrate state-of-the-art performance, their
growing scale requires increased computational resources (e.g., GPUs with
greater memory capacity). To address this problem, quantization techniques
(i.e., low-bit-precision representation and matrix multiplication) have been
proposed. Most quantization techniques employ a static strategy in which the
model parameters are quantized, either during training or inference, without
considering the test-time sample. In contrast, dynamic quantization techniques,
which have become increasingly popular, adapt during inference based on the
input provided, while maintaining full-precision performance. However, their
dynamic behavior and average-case performance assumption makes them vulnerable
to a novel threat vector -- adversarial attacks that target the model&apos;s
efficiency and availability. In this paper, we present QuantAttack, a novel
attack that targets the availability of quantized models, slowing down the
inference, and increasing memory usage and energy consumption. We show that
carefully crafted adversarial examples, which are designed to exhaust the
resources of the operating system, can trigger worst-case performance. In our
experiments, we demonstrate the effectiveness of our attack on vision
transformers on a wide range of tasks, both uni-modal and multi-modal. We also
examine the effect of different attack variants (e.g., a universal
perturbation) and the transferability between different models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baras_A/0/1/0/all/0/1&quot;&gt;Amit Baras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zolfi_A/0/1/0/all/0/1&quot;&gt;Alon Zolfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1&quot;&gt;Asaf Shabtai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02221">
<title>Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction. (arXiv:2312.02221v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02221</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce multi-slice reasoning, a new notion for single-view 3D
reconstruction which challenges the current and prevailing belief that
multi-view synthesis is the most natural conduit between single-view and 3D.
Our key observation is that object slicing is more advantageous than altering
views to reveal occluded structures. Specifically, slicing is more
occlusion-revealing since it can peel through any occluders without
obstruction. In the limit, i.e., with infinitely many slices, it is guaranteed
to unveil all hidden object parts. We realize our idea by developing Slice3D, a
novel method for single-view 3D reconstruction which first predicts multi-slice
images from a single RGB image and then integrates the slices into a 3D model
using a coordinate-based transformer network for signed distance prediction.
The slice images can be regressed or generated, both through a U-Net based
network. For the former, we inject a learnable slice indicator code to
designate each decoded image into a spatial slice location, while the slice
generator is a denoising diffusion model operating on the entirety of slice
images stacked on the input channels. We conduct extensive evaluation against
state-of-the-art alternatives to demonstrate superiority of our method,
especially in recovering complex and severely occluded shape structures, amid
ambiguities. All Slice3D results were produced by networks trained on a single
Nvidia A40 GPU, with an inference time less than 20 seconds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lira_W/0/1/0/all/0/1&quot;&gt;Wallace Lira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Mahdavi-Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02222">
<title>InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars. (arXiv:2312.02222v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02222</link>
<description rdf:parseType="Literal">&lt;p&gt;While high fidelity and efficiency are central to the creation of digital
head avatars, recent methods relying on 2D or 3D generative models often
experience limitations such as shape distortion, expression inaccuracy, and
identity flickering. Additionally, existing one-shot inversion techniques fail
to fully leverage multiple input images for detailed feature extraction. We
propose a novel framework, \textbf{Incremental 3D GAN Inversion}, that enhances
avatar reconstruction performance using an algorithm designed to increase the
fidelity from multiple frames, resulting in improved reconstruction quality
proportional to frame count. Our method introduces a unique animatable 3D GAN
prior with two crucial modifications for enhanced expression controllability
alongside an innovative neural texture encoder that categorizes texture feature
spaces based on UV parameterization. Differentiating from traditional
techniques, our architecture emphasizes pixel-aligned image-to-image
translation, mitigating the need to learn correspondences between observation
and canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent
networks for temporal data aggregation from multiple frames, boosting geometry
and texture detail reconstruction. The proposed paradigm demonstrates
state-of-the-art performance on one-shot and few-shot avatar animation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaochen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yebin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02224">
<title>Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network. (arXiv:2312.02224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02224</link>
<description rdf:parseType="Literal">&lt;p&gt;Model Parsing defines the research task of predicting hyperparameters of the
generative model (GM), given a generated image as input. Since a diverse set of
hyperparameters is jointly employed by the generative model, and dependencies
often exist among them, it is crucial to learn these hyperparameter
dependencies for the improved model parsing performance. To explore such
important dependencies, we propose a novel model parsing method called
Learnable Graph Pooling Network (LGPN). Specifically, we transform model
parsing into a graph node classification task, using graph nodes and edges to
represent hyperparameters and their dependencies, respectively. Furthermore,
LGPN incorporates a learnable pooling-unpooling mechanism tailored to model
parsing, which adaptively learns hyperparameter dependencies of GMs used to
generate the input image. We also extend our proposed method to CNN-generated
image detection and coordinate attacks detection. Empirically, we achieve
state-of-the-art results in model parsing and its extended applications,
showing the effectiveness of our method. Our source code are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1&quot;&gt;Vishal Asnani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02225">
<title>Digital Histopathology with Graph Neural Networks: Concepts and Explanations for Clinicians. (arXiv:2312.02225v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2312.02225</link>
<description rdf:parseType="Literal">&lt;p&gt;To address the challenge of the ``black-box&quot; nature of deep learning in
medical settings, we combine GCExplainer - an automated concept discovery
solution - along with Logic Explained Networks to provide global explanations
for Graph Neural Networks. We demonstrate this using a generally applicable
graph construction and classification pipeline, involving panoptic segmentation
with HoVer-Net and cancer prediction with Graph Convolution Networks. By
training on H&amp;amp;E slides of breast cancer, we show promising results in offering
explainable and trustworthy AI tools for clinicians.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Villaforesta_A/0/1/0/all/0/1&quot;&gt;Alessandro Farace di Villaforesta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Magister_L/0/1/0/all/0/1&quot;&gt;Lucie Charlotte Magister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Barbiero_P/0/1/0/all/0/1&quot;&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02226">
<title>Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition. (arXiv:2312.02226v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02226</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploring open-vocabulary video action recognition is a promising venture,
which aims to recognize previously unseen actions within any arbitrary set of
categories. Existing methods typically adapt pretrained image-text models to
the video domain, capitalizing on their inherent strengths in generalization. A
common thread among such methods is the augmentation of visual embeddings with
temporal information to improve the recognition of seen actions. Yet, they
compromise with standard less-informative action descriptions, thus faltering
when confronted with novel actions. Drawing inspiration from human cognitive
processes, we argue that augmenting text embeddings with human prior knowledge
is pivotal for open-vocabulary video action recognition. To realize this, we
innovatively blend video models with Large Language Models (LLMs) to devise
Action-conditioned Prompts. Specifically, we harness the knowledge in LLMs to
produce a set of descriptive sentences that contain distinctive features for
identifying given actions. Building upon this foundation, we further introduce
a multi-modal action knowledge alignment mechanism to align concepts in video
and textual knowledge encapsulated within the prompts. Extensive experiments on
various video benchmarks, including zero-shot, few-shot, and base-to-novel
generalization settings, demonstrate that our method not only sets new SOTA
performance but also possesses excellent interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chengyou Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Minnan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Z/0/1/0/all/0/1&quot;&gt;Zhuohang Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mingfei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1&quot;&gt;Guang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_S/0/1/0/all/0/1&quot;&gt;Sizhe Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02228">
<title>PixelLM: Pixel Reasoning with Large Multimodal Model. (arXiv:2312.02228v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02228</link>
<description rdf:parseType="Literal">&lt;p&gt;While large multimodal models (LMMs) have achieved remarkable progress,
generating pixel-level masks for image reasoning tasks involving multiple
open-world targets remains a challenge. To bridge this gap, we introduce
PixelLM, an effective and efficient LMM for pixel-level reasoning and
understanding. Central to PixelLM is a novel, lightweight pixel decoder and a
comprehensive segmentation codebook. The decoder efficiently produces masks
from the hidden embeddings of the codebook tokens, which encode detailed
target-relevant information. With this design, PixelLM harmonizes with the
structure of popular LMMs and avoids the need for additional costly
segmentation models. Furthermore, we propose a target refinement loss to
enhance the model&apos;s ability to differentiate between multiple targets, leading
to substantially improved mask quality. To advance research in this area, we
construct MUSE, a high-quality multi-target reasoning segmentation benchmark.
PixelLM excels across various pixel-level image reasoning and understanding
tasks, outperforming well-established methods in multiple benchmarks, including
MUSE, single- and multi-referring segmentation. Comprehensive ablations confirm
the efficacy of each proposed component. All code, models, and datasets will be
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhongwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Dongmei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaojie Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02232">
<title>HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with Diverse Poses. (arXiv:2312.02232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02232</link>
<description rdf:parseType="Literal">&lt;p&gt;We present HumanNeRF-SE, which can synthesize diverse novel pose images with
simple input. Previous HumanNeRF studies require large neural networks to fit
the human appearance and prior knowledge. Subsequent methods build upon this
approach with some improvements. Instead, we reconstruct this approach,
combining explicit and implicit human representations with both general and
specific mapping processes. Our key insight is that explicit shape can filter
the information used to fit implicit representation, and frozen general mapping
combined with point-specific mapping can effectively avoid overfitting and
improve pose generalization performance. Our explicit and implicit human
represent combination architecture is extremely effective. This is reflected in
our model&apos;s ability to synthesize images under arbitrary poses with few-shot
input and increase the speed of synthesizing images by 15 times through a
reduction in computational complexity without using any existing acceleration
modules. Compared to the state-of-the-art HumanNeRF studies, HumanNeRF-SE
achieves better performance with fewer learnable parameters and less training
time (see Figure 1).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Caoyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu-Lun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02233">
<title>MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model. (arXiv:2312.02233v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02233</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of Large Language Models (LLMs) in general image tasks, a
gap persists in the medical field for a multimodal large model adept at
handling the nuanced diversity of medical images. Addressing this, we propose
MedXChat, a unified multimodal large model designed for seamless interactions
between medical assistants and users. MedXChat encompasses three key
functionalities: CXR(Chest X-ray)-to-Report generation, CXR-based visual
question-answering (VQA), and Text-to-CXR synthesis. Our contributions are as
follows. Firstly, our model showcases exceptional cross-task adaptability,
displaying adeptness across all three defined tasks and outperforming the
benchmark models on the MIMIC dataset in medical multimodal applications.
Secondly, we introduce an innovative Text-to-CXR synthesis approach that
utilizes instruction-following capabilities within the Stable Diffusion (SD)
architecture. This technique integrates smoothly with the existing model
framework, requiring no extra parameters, thereby maintaining the SD&apos;s
generative strength while also bestowing upon it the capacity to render
fine-grained medical images with high fidelity. Comprehensive experiments
validate MedXChat&apos;s synergistic enhancement across all tasks. Our instruction
data and model will be open-sourced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Ling Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02235">
<title>GenEM: Physics-Informed Generative Cryo-Electron Microscopy. (arXiv:2312.02235v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02235</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decade, deep conditional generative models have revolutionized
the generation of realistic images, extending their application from
entertainment to scientific domains. Single-particle cryo-electron microscopy
(cryo-EM) is crucial in resolving near-atomic resolution 3D structures of
proteins, such as the SARS-COV-2 spike protein. To achieve high-resolution
reconstruction, AI models for particle picking and pose estimation have been
adopted. However, their performance is still limited as they lack high-quality
annotated datasets. To address this, we introduce physics-informed generative
cryo-electron microscopy (GenEM), which for the first time integrates
physical-based cryo-EM simulation with a generative unpaired noise translation
to generate physically correct synthetic cryo-EM datasets with realistic
noises. Initially, GenEM simulates the cryo-EM imaging process based on a
virtual specimen. To generate realistic noises, we leverage an unpaired noise
translation via contrastive learning with a novel mask-guided sampling scheme.
Extensive experiments show that GenEM is capable of generating realistic
cryo-EM images. The generated dataset can further enhance particle picking and
pose estimation models, eventually improving the reconstruction resolution. We
will release our code and annotated synthetic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiakai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qihe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wenyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02237">
<title>Singular Regularization with Information Bottleneck Improves Model&apos;s Adversarial Robustness. (arXiv:2312.02237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02237</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples are one of the most severe threats to deep learning
models. Numerous works have been proposed to study and defend adversarial
examples. However, these works lack analysis of adversarial information or
perturbation, which cannot reveal the mystery of adversarial examples and lose
proper interpretation. In this paper, we aim to fill this gap by studying
adversarial information as unstructured noise, which does not have a clear
pattern. Specifically, we provide some empirical studies with singular value
decomposition, by decomposing images into several matrices, to analyze
adversarial information for different attacks. Based on the analysis, we
propose a new module to regularize adversarial information and combine
information bottleneck theory, which is proposed to theoretically restrict
intermediate representations. Therefore, our method is interpretable. Moreover,
the fashion of our design is a novel principle that is general and unified.
Equipped with our new module, we evaluate two popular model structures on two
mainstream datasets with various adversarial attacks. The results indicate that
the improvement in robust accuracy is significant. On the other hand, we prove
that our method is efficient with only a few additional parameters and able to
be explained under regional faithfulness analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Naishan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Man Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02238">
<title>X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model. (arXiv:2312.02238v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02238</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce X-Adapter, a universal upgrader to enable the pretrained
plug-and-play modules (e.g., ControlNet, LoRA) to work directly with the
upgraded text-to-image diffusion model (e.g., SDXL) without further retraining.
We achieve this goal by training an additional network to control the frozen
upgraded model with the new text-image data pairs. In detail, X-Adapter keeps a
frozen copy of the old model to preserve the connectors of different plugins.
Additionally, X-Adapter adds trainable mapping layers that bridge the decoders
from models of different versions for feature remapping. The remapped features
will be used as guidance for the upgraded model. To enhance the guidance
ability of X-Adapter, we employ a null-text training strategy for the upgraded
model. After training, we also introduce a two-stage denoising strategy to
align the initial latents of X-Adapter and the upgraded model. Thanks to our
strategies, X-Adapter demonstrates universal compatibility with various plugins
and also enables plugins of different versions to work together, thereby
expanding the functionalities of diffusion community. To verify the
effectiveness of the proposed method, we conduct extensive experiments and the
results show that X-Adapter may facilitate wider application in the upgraded
foundational diffusion model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_L/0/1/0/all/0/1&quot;&gt;Lingmin Ran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1&quot;&gt;Xiaodong Cun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;JiaWei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zijie_S/0/1/0/all/0/1&quot;&gt;Song Zijie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keppo_J/0/1/0/all/0/1&quot;&gt;Jussi Keppo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02240">
<title>Contrastive Learning-Based Spectral Knowledge Distillation for Multi-Modality and Missing Modality Scenarios in Semantic Segmentation. (arXiv:2312.02240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02240</link>
<description rdf:parseType="Literal">&lt;p&gt;Improving the performance of semantic segmentation models using multispectral
information is crucial, especially for environments with low-light and adverse
conditions. Multi-modal fusion techniques pursue either the learning of
cross-modality features to generate a fused image or engage in knowledge
distillation but address multimodal and missing modality scenarios as distinct
issues, which is not an optimal approach for multi-sensor models. To address
this, a novel multi-modal fusion approach called CSK-Net is proposed, which
uses a contrastive learning-based spectral knowledge distillation technique
along with an automatic mixed feature exchange mechanism for semantic
segmentation in optical (EO) and infrared (IR) images. The distillation scheme
extracts detailed textures from the optical images and distills them into the
optical branch of CSK-Net. The model encoder consists of shared convolution
weights with separate batch norm (BN) layers for both modalities, to capture
the multi-spectral information from different modalities of the same objects. A
Novel Gated Spectral Unit (GSU) and mixed feature exchange strategy are
proposed to increase the correlation of modality-shared information and
decrease the modality-specific information during the distillation process.
Comprehensive experiments show that CSK-Net surpasses state-of-the-art models
in multi-modal tasks and for missing modalities when exclusively utilizing IR
data for inference across three public benchmarking datasets. For missing
modality scenarios, the performance increase is achieved without additional
computational costs compared to the baseline segmentation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikdar_A/0/1/0/all/0/1&quot;&gt;Aniruddh Sikdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teotia_J/0/1/0/all/0/1&quot;&gt;Jayant Teotia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1&quot;&gt;Suresh Sundaram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02244">
<title>Geometrically-driven Aggregation for Zero-shot 3D Point Cloud Understanding. (arXiv:2312.02244v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02244</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language
Models (VLMs). Existing strategies directly map Vision-Language Models from 2D
pixels of rendered or captured views to 3D points, overlooking the inherent and
expressible point cloud geometric structure. Geometrically similar or close
regions can be exploited for bolstering point cloud understanding as they are
likely to share semantic information. To this end, we introduce the first
training-free aggregation technique that leverages the point cloud&apos;s 3D
geometric structure to improve the quality of the transferred Vision-Language
Models. Our approach operates iteratively, performing local-to-global
aggregation based on geometric and semantic point-level reasoning. We benchmark
our approach on three downstream tasks, including classification, part
segmentation, and semantic segmentation, with a variety of datasets
representing both synthetic/real-world, and indoor/outdoor scenarios. Our
approach achieves new state-of-the-art results in all benchmarks. We will
release the source code publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1&quot;&gt;Guofeng Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riz_L/0/1/0/all/0/1&quot;&gt;Luigi Riz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1&quot;&gt;Fabio Poiesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02246">
<title>Conditional Variational Diffusion Models. (arXiv:2312.02246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02246</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggiora_G/0/1/0/all/0/1&quot;&gt;Gabriel della Maggiora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Croquevielle_L/0/1/0/all/0/1&quot;&gt;Luis Alberto Croquevielle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desphande_N/0/1/0/all/0/1&quot;&gt;Nikita Desphande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horsley_H/0/1/0/all/0/1&quot;&gt;Harry Horsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinis_T/0/1/0/all/0/1&quot;&gt;Thomas Heinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakimovich_A/0/1/0/all/0/1&quot;&gt;Artur Yakimovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02247">
<title>Federated Active Learning for Target Domain Generalisation. (arXiv:2312.02247v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.02247</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Active Learning framework in Federated Learning
for Target Domain Generalisation, harnessing the strength from both learning
paradigms. Our framework, FEDALV, composed of Active Learning (AL) and
Federated Domain Generalisation (FDG), enables generalisation of an image
classification model trained from limited source domain client&apos;s data without
sharing images to an unseen target domain. To this end, our FDG, FEDA, consists
of two optimisation updates during training, one at the client and another at
the server level. For the client, the introduced losses aim to reduce feature
complexity and condition alignment, while in the server, the regularisation
limits free energy biases between source and target obtained by the global
model. The remaining component of FEDAL is AL with variable budgets, which
queries the server to retrieve and sample the most informative local data for
the targeted client. We performed multiple experiments on FDG w/ and w/o AL and
compared with both conventional FDG baselines and Federated Active Learning
baselines. Our extensive quantitative experiments demonstrate the superiority
of our method in accuracy and efficiency compared to the multiple contemporary
methods. FEDALV manages to obtain the performance of the full training target
accuracy while sampling as little as 5% of the source client&apos;s data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caramalau_R/0/1/0/all/0/1&quot;&gt;Razvan Caramalau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1&quot;&gt;Binod Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02249">
<title>Recursive Visual Programming. (arXiv:2312.02249v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02249</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Programming (VP) has emerged as a powerful framework for Visual
Question Answering (VQA). By generating and executing bespoke code for each
question, these methods demonstrate impressive compositional and reasoning
capabilities, especially in few-shot and zero-shot scenarios. However, existing
VP methods generate all code in a single function, resulting in code that is
suboptimal in terms of both accuracy and interpretability. Inspired by human
coding practices, we propose Recursive Visual Programming (RVP), which
simplifies generated routines, provides more efficient problem solving, and can
manage more complex data structures. RVP is inspired by human coding practices
and approaches VQA tasks with an iterative recursive code generation approach,
allowing decomposition of complicated problems into smaller parts. Notably, RVP
is capable of dynamic type assignment, i.e., as the system recursively
generates a new piece of code, it autonomously determines the appropriate
return type and crafts the requisite code to generate that output. We show
RVP&apos;s efficacy through extensive experiments on benchmarks including VSR, COVR,
GQA, and NextQA, underscoring the value of adopting human-like recursive and
modular programming techniques for solving VQA tasks through coding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jiaxin Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1&quot;&gt;Sanjay Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Baifeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1&quot;&gt;Roei Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02252">
<title>Large Language Models as Consistent Story Visualizers. (arXiv:2312.02252v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02252</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent generative models have demonstrated impressive capabilities in
generating realistic and visually pleasing images grounded on textual prompts.
Nevertheless, a significant challenge remains in applying these models for the
more intricate task of story visualization. Since it requires resolving
pronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,
and ensuring consistent characters and background synthesis across frames. Yet,
the emerging Large Language Model (LLM) showcases robust reasoning abilities to
navigate through ambiguous references and process extensive sequences.
Therefore, we introduce \textbf{StoryGPT-V}, which leverages the merits of the
latent diffusion (LDM) and LLM to produce images with consistent and
high-quality characters grounded on given story descriptions. First, we train a
character-aware LDM, which takes character-augmented semantic embedding as
input and includes the supervision of the cross-attention map using character
segmentation masks, aiming to enhance character generation accuracy and
faithfulness. In the second stage, we enable an alignment between the output of
LLM and the character-augmented embedding residing in the input space of the
first-stage model. This harnesses the reasoning ability of LLM to address
ambiguous references and the comprehension capability to memorize the context.
We conduct comprehensive experiments on two visual story visualization
benchmarks. Our model reports superior quantitative results and consistently
generates accurate characters of remarkable quality with low memory
consumption. Our code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02253">
<title>Diversify, Don&apos;t Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images. (arXiv:2312.02253v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02253</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative deep learning have enabled the creation of
high-quality synthetic images in text-to-image generation. Prior work shows
that fine-tuning a pretrained diffusion model on ImageNet and generating
synthetic training images from the finetuned model can enhance an ImageNet
classifier&apos;s performance. However, performance degrades as synthetic images
outnumber real ones. In this paper, we explore whether generative fine-tuning
is essential for this improvement and whether it is possible to further scale
up training using more synthetic data. We present a new framework leveraging
off-the-shelf generative models to generate synthetic training images,
addressing multiple challenges: class name ambiguity, lack of diversity in
naive prompts, and domain shifts. Specifically, we leverage large language
models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we
propose contextualized diversification (CD) and stylized diversification (SD)
methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage
domain adaptation techniques with auxiliary batch normalization for synthetic
images. Our framework consistently enhances recognition model performance with
more synthetic data, up to 6x of original ImageNet size showcasing the
potential of synthetic data for improved recognition models and strong
out-of-domain generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenchen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1&quot;&gt;Sean Culatana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Fanyi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02255">
<title>Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields through Novel Views Synthesis. (arXiv:2312.02255v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02255</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis
capabilities even in large-scale, unbounded scenes, albeit requiring hundreds
of views or introducing artifacts in sparser settings. Their optimization
suffers from shape-radiance ambiguities wherever only a small visual overlap is
available. This leads to erroneous scene geometry and artifacts. In this paper,
we propose Re-Nerfing, a simple and general multi-stage approach that leverages
NeRF&apos;s own view synthesis to address these limitations. With Re-Nerfing, we
increase the scene&apos;s coverage and enhance the geometric consistency of novel
views as follows: First, we train a NeRF with the available views. Then, we use
the optimized NeRF to synthesize pseudo-views next to the original ones to
simulate a stereo or trifocal setup. Finally, we train a second NeRF with both
original and pseudo views while enforcing structural, epipolar constraints via
the newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset
show the effectiveness of Re-Nerfing across denser and sparser input scenarios,
bringing improvements to the state-of-the-art Zip-NeRF, even when trained with
all views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tristram_F/0/1/0/all/0/1&quot;&gt;Felix Tristram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1&quot;&gt;Stefano Gasperini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1&quot;&gt;Benjamin Busam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02256">
<title>EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion Generation. (arXiv:2312.02256v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02256</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Efficient Motion Diffusion Model (EMDM) for fast and
high-quality human motion generation. Although previous motion diffusion models
have shown impressive results, they struggle to achieve fast generation while
maintaining high-quality human motions. Motion latent diffusion has been
proposed for efficient motion generation. However, effectively learning a
latent space can be non-trivial in such a two-stage manner. Meanwhile,
accelerating motion sampling by increasing the step size, e.g., DDIM, typically
leads to a decline in motion quality due to the inapproximation of complex data
distributions when naively increasing the step size. In this paper, we propose
EMDM that allows for much fewer sample steps for fast motion generation by
modeling the complex denoising distribution during multiple sampling steps.
Specifically, we develop a Conditional Denoising Diffusion GAN to capture
multimodal data distributions conditioned on both control signals, i.e.,
textual description and denoising time step. By modeling the complex data
distribution, a larger sampling step size and fewer steps are achieved during
motion synthesis, significantly accelerating the generation process. To
effectively capture the human dynamics and reduce undesired artifacts, we
employ motion geometric loss during network training, which improves the motion
quality and training efficiency. As a result, EMDM achieves a remarkable
speed-up at the generation stage while maintaining high-quality motion
generation in terms of fidelity and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zeyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Zhouyingcheng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02284">
<title>PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation. (arXiv:2312.02284v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02284</link>
<description rdf:parseType="Literal">&lt;p&gt;Single image depth estimation is a foundational task in computer vision and
generative modeling. However, prevailing depth estimation models grapple with
accommodating the increasing resolutions commonplace in today&apos;s consumer
cameras and devices. Existing high-resolution strategies show promise, but they
often face limitations, ranging from error propagation to the loss of
high-frequency details. We present PatchFusion, a novel tile-based framework
with three key components to improve the current state of the art: (1) A
patch-wise fusion network that fuses a globally-consistent coarse prediction
with finer, inconsistent tiled predictions via high-level feature guidance, (2)
A Global-to-Local (G2L) module that adds vital context to the fusion network,
discarding the need for patch selection heuristics, and (3) A Consistency-Aware
Training (CAT) and Inference (CAI) approach, emphasizing patch overlap
consistency and thereby eradicating the necessity for post-processing.
Experiments on UnrealStereo4K, MVS-Synth, and Middleburry 2014 demonstrate that
our framework can generate high-resolution depth maps with intricate details.
PatchFusion is independent of the base model for depth estimation. Notably, our
framework built on top of SOTA ZoeDepth brings improvements for a total of
17.3% and 29.4% in terms of the root mean squared error (RMSE) on
UnrealStereo4K and MVS-Synth, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1&quot;&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1&quot;&gt;Peter Wonka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02290">
<title>You Can Run but not Hide: Improving Gait Recognition with Intrinsic Occlusion Type Awareness. (arXiv:2312.02290v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02290</link>
<description rdf:parseType="Literal">&lt;p&gt;While gait recognition has seen many advances in recent years, the occlusion
problem has largely been ignored. This problem is especially important for gait
recognition from uncontrolled outdoor sequences at range - since any small
obstruction can affect the recognition system. Most current methods assume the
availability of complete body information while extracting the gait features.
When parts of the body are occluded, these methods may hallucinate and output a
corrupted gait signature as they try to look for body parts which are not
present in the input at all. To address this, we exploit the learned occlusion
type while extracting identity features from videos. Thus, in this work, we
propose an occlusion aware gait recognition method which can be used to model
intrinsic occlusion awareness into potentially any state-of-the-art gait
recognition method. Our experiments on the challenging GREW and BRIAR datasets
show that networks enhanced with this occlusion awareness perform better at
recognition tasks than their counterparts trained on similar occlusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Ayush Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02298">
<title>MoE-AMC: Enhancing Automatic Modulation Classification Performance Using Mixture-of-Experts. (arXiv:2312.02298v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.02298</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Modulation Classification (AMC) plays a vital role in time series
analysis, such as signal classification and identification within wireless
communications. Deep learning-based AMC models have demonstrated significant
potential in this domain. However, current AMC models inadequately consider the
disparities in handling signals under conditions of low and high
Signal-to-Noise Ratio (SNR), resulting in an unevenness in their performance.
In this study, we propose MoE-AMC, a novel Mixture-of-Experts (MoE) based model
specifically crafted to address AMC in a well-balanced manner across varying
SNR conditions. Utilizing the MoE framework, MoE-AMC seamlessly combines the
strengths of LSRM (a Transformer-based model) for handling low SNR signals and
HSRM (a ResNet-based model) for high SNR signals. This integration empowers
MoE-AMC to achieve leading performance in modulation classification, showcasing
its efficacy in capturing distinctive signal features under diverse SNR
scenarios. We conducted experiments using the RML2018.01a dataset, where
MoE-AMC achieved an average classification accuracy of 71.76% across different
SNR levels, surpassing the performance of previous SOTA models by nearly 10%.
This study represents a pioneering application of MoE techniques in the realm
of AMC, offering a promising avenue for elevating signal classification
accuracy within wireless communication systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiaxin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qinglong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02310">
<title>VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding. (arXiv:2312.02310v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02310</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in language-model-based video understanding have been
progressing at a remarkable pace, spurred by the introduction of Large Language
Models (LLMs). However, the focus of prior research has been predominantly on
devising a projection layer that maps video features to tokens, an approach
that is both rudimentary and inefficient. In our study, we introduce a
cutting-edge framework, VaQuitA, designed to refine the synergy between video
and textual information. At the data level, instead of sampling frames
uniformly, we implement a sampling method guided by CLIP-score rankings, which
enables a more aligned selection of frames with the given question. At the
feature level, we integrate a trainable Video Perceiver alongside a
Visual-Query Transformer (abbreviated as VQ-Former), which bolsters the
interplay between the input question and the video features. We also discover
that incorporating a simple prompt, &quot;Please be critical&quot;, into the LLM input
can substantially enhance its video comprehension capabilities. Our
experimental results indicate that VaQuitA consistently sets a new benchmark
for zero-shot video question-answering tasks and is adept at producing
high-quality, multi-turn video dialogues with users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoliang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1&quot;&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02312">
<title>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games. (arXiv:2312.02312v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.02312</link>
<description rdf:parseType="Literal">&lt;p&gt;Video games have served as useful benchmarks for the decision making
community, but going beyond Atari games towards training agents in modern games
has been prohibitively expensive for the vast majority of the research
community. Recent progress in the research, development and open release of
large vision models has the potential to amortize some of these costs across
the community. However, it is currently unclear which of these models have
learnt representations that retain information critical for sequential decision
making. Towards enabling wider participation in the research of gameplaying
agents in modern games, we present a systematic study of imitation learning
with publicly available visual encoders compared to the typical, task-specific,
end-to-end training approach in Minecraft, Minecraft Dungeons and
Counter-Strike: Global Offensive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1&quot;&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1&quot;&gt;Logan Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1&quot;&gt;Anssi Kanervisto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuhan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1&quot;&gt;Tabish Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1&quot;&gt;Raluca Georgescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bignell_D/0/1/0/all/0/1&quot;&gt;Dave Bignell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1&quot;&gt;Siddhartha Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavito_A/0/1/0/all/0/1&quot;&gt;Andrea Trevi&amp;#xf1;o Gavito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1&quot;&gt;Sam Devlin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02320">
<title>Cable Slack Detection for Arresting Gear Application using Machine Vision. (arXiv:2312.02320v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02320</link>
<description rdf:parseType="Literal">&lt;p&gt;The cable-based arrestment systems are integral to the launch and recovery of
aircraft onboard carriers and on expeditionary land-based installations. These
modern arrestment systems rely on various mechanisms to absorb energy from an
aircraft during an arrestment cycle to bring the aircraft to a full stop. One
of the primary components of this system is the cable interface to the engine.
The formation of slack in the cable at this interface can result in reduced
efficiency and drives maintenance efforts to remove the slack prior to
continued operations. In this paper, a machine vision based slack detection
system is presented. A situational awareness camera is utilized to collect
video data of the cable interface region, machine vision algorithms are applied
to reduce noise, remove background clutter, focus on regions of interest, and
detect changes in the image representative of slack formations. Some algorithms
employed in this system include bilateral image filters, least squares
polynomial fit, Canny Edge Detection, K-Means clustering, Gaussian
Mixture-based Background/Foreground Segmentation for background subtraction,
Hough Circle Transforms, and Hough line Transforms. The resulting detections
are filtered and highlighted to create an indication to the shipboard operator
of the presence of slack and a need for a maintenance action. A user interface
was designed to provide operators with an easy method to redefine regions of
interest and adjust the methods to specific locations. The algorithms were
validated on shipboard footage and were able to accurately identify slack with
minimal false positives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_A/0/1/0/all/0/1&quot;&gt;Ari Goodman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shevach_G/0/1/0/all/0/1&quot;&gt;Glenn Shevach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabriskie_S/0/1/0/all/0/1&quot;&gt;Sean Zabriskie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thajudeen_D/0/1/0/all/0/1&quot;&gt;Dr. Chris Thajudeen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02338">
<title>A Contrastive Compositional Benchmark for Text-to-Image Synthesis: A Study with Unified Text-to-Image Fidelity Metrics. (arXiv:2312.02338v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02338</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image (T2I) synthesis has recently achieved significant advancements.
However, challenges remain in the model&apos;s compositionality, which is the
ability to create new combinations from known components. We introduce
Winoground-T2I, a benchmark designed to evaluate the compositionality of T2I
models. This benchmark includes 11K complex, high-quality contrastive sentence
pairs spanning 20 categories. These contrastive sentence pairs with subtle
differences enable fine-grained evaluations of T2I synthesis models.
Additionally, to address the inconsistency across different metrics, we propose
a strategy that evaluates the reliability of various metrics by using
comparative sentence pairs. We use Winoground-T2I with a dual objective: to
evaluate the performance of T2I models and the metrics used for their
evaluation. Finally, we provide insights into the strengths and weaknesses of
these metrics and the capabilities of current T2I models in tackling challenges
across a range of complex compositional categories. Our benchmark is publicly
available at https://github.com/zhuxiangru/Winoground-T2I .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangru Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Penglei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhixu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yanghua Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02344">
<title>STEREOFOG -- Computational DeFogging via Image-to-Image Translation on a real-world Dataset. (arXiv:2312.02344v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02344</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-to-Image translation (I2I) is a subtype of Machine Learning (ML) that
has tremendous potential in applications where two domains of images and the
need for translation between the two exist, such as the removal of fog. For
example, this could be useful for autonomous vehicles, which currently struggle
with adverse weather conditions like fog. However, datasets for I2I tasks are
not abundant and typically hard to acquire. Here, we introduce STEREOFOG, a
dataset comprised of $10,067$ paired fogged and clear images, captured using a
custom-built device, with the purpose of exploring I2I&apos;s potential in this
domain. It is the only real-world dataset of this kind to the best of our
knowledge. Furthermore, we apply and optimize the pix2pix I2I ML framework to
this dataset. With the final model achieving an average Complex
Wavelet-Structural Similarity (CW-SSIM) score of $0.76$, we prove the
technique&apos;s suitability for the problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollak_A/0/1/0/all/0/1&quot;&gt;Anton Pollak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1&quot;&gt;Rajesh Menon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02345">
<title>CLIPDrawX: Primitive-based Explanations for Text Guided Sketch Synthesis. (arXiv:2312.02345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02345</link>
<description rdf:parseType="Literal">&lt;p&gt;With the goal of understanding the visual concepts that CLIP associates with
text prompts, we show that the latent space of CLIP can be visualized solely in
terms of linear transformations on simple geometric primitives like circles and
straight lines. Although existing approaches achieve this by
sketch-synthesis-through-optimization, they do so on the space of B\&apos;ezier
curves, which exhibit a wastefully large set of structures that they can evolve
into, as most of them are non-essential for generating meaningful sketches. We
present CLIPDrawX, an algorithm that provides significantly better
visualizations for CLIP text embeddings, using only simple primitive shapes
like straight lines and circles. This constrains the set of possible outputs to
linear transformations on these primitives, thereby exhibiting an inherently
simpler mathematical form. The synthesis process of CLIPDrawX can be tracked
end-to-end, with each visual concept being explained exclusively in terms of
primitives. Implementation will be released upon acceptance. Project Page:
$\href{https://clipdrawx.github.io/}{\text{https://clipdrawx.github.io/}}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathur_N/0/1/0/all/0/1&quot;&gt;Nityanand Mathur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marjit_S/0/1/0/all/0/1&quot;&gt;Shyam Marjit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1&quot;&gt;Abhra Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1&quot;&gt;Anjan Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02350">
<title>Calibrated Uncertainties for Neural Radiance Fields. (arXiv:2312.02350v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02350</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields have achieved remarkable results for novel view
synthesis but still lack a crucial component: precise measurement of
uncertainty in their predictions. Probabilistic NeRF methods have tried to
address this, but their output probabilities are not typically accurately
calibrated, and therefore do not capture the true confidence levels of the
model. Calibration is a particularly challenging problem in the sparse-view
setting, where additional held-out data is unavailable for fitting a calibrator
that generalizes to the test distribution. In this paper, we introduce the
first method for obtaining calibrated uncertainties from NeRF models. Our
method is based on a robust and efficient metric to calculate per-pixel
uncertainties from the predictive posterior distribution. We propose two
techniques that eliminate the need for held-out data. The first, based on patch
sampling, involves training two NeRF models for each scene. The second is a
novel meta-calibrator that only requires the training of one NeRF model. Our
proposed approach for obtaining calibrated uncertainties achieves
state-of-the-art uncertainty in the sparse-view setting while maintaining image
quality. We further demonstrate our method&apos;s effectiveness in applications such
as view enhancement and next-best view selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_Naieni_N/0/1/0/all/0/1&quot;&gt;Niki Amini-Naieni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakab_T/0/1/0/all/0/1&quot;&gt;Tomas Jakab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_R/0/1/0/all/0/1&quot;&gt;Ronald Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02362">
<title>PointNeRF++: A multi-scale, point-based Neural Radiance Field. (arXiv:2312.02362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02362</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds offer an attractive source of information to complement images
in neural scene representations, especially when few images are available.
Neural rendering methods based on point clouds do exist, but they do not
perform well when the point cloud quality is low -- e.g., sparse or incomplete,
which is often the case with real-world data. We overcome these problems with a
simple representation that aggregates point clouds at multiple scale levels
with sparse voxel grids at different resolutions. To deal with point cloud
sparsity, we average across multiple scale levels -- but only among those that
are valid, i.e., that have enough neighboring points in proximity to the ray of
a pixel. To help model areas without points, we add a global voxel at the
coarsest scale, thus unifying &quot;classical&quot; and point-based NeRF formulations. We
validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,
outperforming the state of the art by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weiwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trulls_E/0/1/0/all/0/1&quot;&gt;Eduard Trulls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1&quot;&gt;Yang-Che Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sambandam_S/0/1/0/all/0/1&quot;&gt;Sneha Sambandam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gopal Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02364">
<title>Class-Discriminative Attention Maps for Vision Transformers. (arXiv:2312.02364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02364</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability methods are critical components for examining and exploring
deep neural networks (DNN), as well as increasing our understanding of and
trust in them. Vision transformers (ViT), which can be trained to
state-of-the-art performance with a self-supervised learning (SSL) training
method, provide built-in attention maps (AM). While AMs can provide
high-quality semantic segmentation of input images, they do not account for any
signal coming from a downstream classifier. We introduce class-discriminative
attention maps (CDAM), a novel post-hoc explanation method that is highly
sensitive to the target class. Our method essentially scales attention scores
by how relevant the corresponding tokens are for the predictions of a
classifier head. Alternative to classifier outputs, CDAM can also explain a
user-defined concept by targeting similarity measures in the latent space of
the ViT. This allows for explanations of arbitrary concepts, defined by the
user through a few sample images. We investigate the operating characteristics
of CDAM in comparison with relevance propagation (RP) and token ablation maps
(TAM), an alternative to pixel occlusion methods. CDAM is highly
class-discriminative and semantically relevant, while providing implicit
regularization of relevance scores.
&lt;/p&gt;
&lt;p&gt;PyTorch implementation: \url{https://github.com/lenbrocki/CDAM}
&lt;/p&gt;
&lt;p&gt;Web live demo: \url{https://cdam.informatism.com/}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brocki_L/0/1/0/all/0/1&quot;&gt;Lennart Brocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1&quot;&gt;Neo Christopher Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02365">
<title>MEDPSeg: End-to-end segmentation of pulmonary structures and lesions in computed tomography. (arXiv:2312.02365v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.02365</link>
<description rdf:parseType="Literal">&lt;p&gt;The COVID-19 pandemic response highlighted the potential of deep learning
methods in facilitating the diagnosis and prognosis of lung diseases through
automated segmentation of normal and abnormal tissue in computed tomography
(CT). Such methods not only have the potential to aid in clinical
decision-making but also contribute to the comprehension of novel diseases. In
light of the labor-intensive nature of manual segmentation for large chest CT
cohorts, there is a pressing need for reliable automated approaches that enable
efficient analysis of chest CT anatomy in vast research databases, especially
in more scarcely annotated targets such as pneumonia consolidations. A limiting
factor for the development of such methods is that most current models optimize
a fixed annotation format per network output. To tackle this problem,
polymorphic training is used to optimize a network with a fixed number of
output channels to represent multiple hierarchical anatomic structures,
indirectly optimizing more complex labels with simpler annotations. We combined
over 6000 volumetric CT scans containing varying formats of manual and
automated labels from different sources, and used polymorphic training along
with multitask learning to develop MEDPSeg, an end-to-end method for the
segmentation of lungs, airways, pulmonary artery, and lung lesions with
separation of ground glass opacities, and parenchymal consolidations, all in a
single forward prediction. We achieve state-of-the-art performance in multiple
targets, particularly in the segmentation of ground glass opacities and
consolidations, a challenging problem with limited manual annotation
availability. In addition, we provide an open-source implementation with a
graphical user interface at https://github.com/MICLab-Unicamp/medpseg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Carmo_D/0/1/0/all/0/1&quot;&gt;Diedre S. Carmo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ribeiro_J/0/1/0/all/0/1&quot;&gt;Jean Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Comellas_A/0/1/0/all/0/1&quot;&gt;Alejandro P. Comellas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reinhardt_J/0/1/0/all/0/1&quot;&gt;Joseph M. Reinhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gerard_S/0/1/0/all/0/1&quot;&gt;Sarah E. Gerard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rittner_L/0/1/0/all/0/1&quot;&gt;Let&amp;#xed;cia Rittner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lotufo_R/0/1/0/all/0/1&quot;&gt;Roberto A. Lotufo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02366">
<title>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks. (arXiv:2312.02366v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02366</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of deep learning systems into the medical domain has been
hindered by the resource-intensive process of data annotation and the inability
of these systems to generalize to different data distributions. Foundation
models, which are models pre-trained on large datasets, have emerged as a
solution to reduce reliance on annotated data and enhance model
generalizability and robustness. DINOv2, an open-source foundation model
pre-trained with self-supervised learning on 142 million curated natural
images, excels in extracting general-purpose visual representations, exhibiting
promising capabilities across various vision tasks. Nevertheless, a critical
question remains unanswered regarding DINOv2&apos;s adaptability to radiological
imaging, and the clarity on whether its features are sufficiently general to
benefit radiology image analysis is yet to be established. Therefore, this
study comprehensively evaluates DINOv2 for radiology, conducting over 100
experiments across diverse modalities (X-ray, CT, and MRI). Tasks include
disease classification and organ segmentation on both 2D and 3D images,
evaluated under different settings like kNN, few-shot learning, linear-probing,
end-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the
effectiveness and generalizability of the DINOv2 feature embeddings.
Comparative analyses with established medical image analysis models, U-Net and
TransUnet for segmentation, and CNN and ViT models pre-trained via supervised,
weakly supervised, and self-supervised learning for classification, reveal
DINOv2&apos;s superior performance in segmentation tasks and competitive results in
disease classification. The findings contribute insights to potential avenues
for optimizing pre-training strategies for medical imaging and enhancing the
broader understanding of DINOv2&apos;s role in bridging the gap between natural and
radiological image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baharoon_M/0/1/0/all/0/1&quot;&gt;Mohammed Baharoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_W/0/1/0/all/0/1&quot;&gt;Waseem Qureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1&quot;&gt;Jiahong Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phol_K/0/1/0/all/0/1&quot;&gt;Kilian Phol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljouie_A/0/1/0/all/0/1&quot;&gt;Abdulrhman Aljouie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wei Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02396">
<title>Unsupervised Change Detection for Space Habitats Using 3D Point Clouds. (arXiv:2312.02396v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.02396</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents an algorithm for scene change detection from point clouds
to enable autonomous robotic caretaking in future space habitats. Autonomous
robotic systems will help maintain future deep-space habitats, such as the
Gateway space station, which will be uncrewed for extended periods. Existing
scene analysis software used on the International Space Station (ISS) relies on
manually-labeled images for detecting changes. In contrast, the algorithm
presented in this work uses raw, unlabeled point clouds as inputs. The
algorithm first applies modified Expectation-Maximization Gaussian Mixture
Model (GMM) clustering to two input point clouds. It then performs change
detection by comparing the GMMs using the Earth Mover&apos;s Distance. The algorithm
is validated quantitatively and qualitatively using a test dataset collected by
an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth
images taken directly by Astrobee and full-scene reconstructed maps built with
RGB-D and pose data from Astrobee. The runtimes of the approach are also
analyzed in depth. The source code is publicly released to promote further
development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Jamie Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinkel_H/0/1/0/all/0/1&quot;&gt;Holly Dinkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_J/0/1/0/all/0/1&quot;&gt;Julia Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1&quot;&gt;Paulo V.K. Borges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_M/0/1/0/all/0/1&quot;&gt;Marina Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandrov_O/0/1/0/all/0/1&quot;&gt;Oleg Alexandrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coltin_B/0/1/0/all/0/1&quot;&gt;Brian Coltin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1&quot;&gt;Trey Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02409">
<title>MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR. (arXiv:2312.02409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02409</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion prediction has been an essential component of autonomous driving
systems since it handles highly uncertain and complex scenarios involving
moving agents of different types. In this paper, we propose a Multi-Granular
TRansformer (MGTR) framework, an encoder-decoder network that exploits context
features in different granularities for different kinds of traffic agents. To
further enhance MGTR&apos;s capabilities, we leverage LiDAR point cloud data by
incorporating LiDAR semantic features from an off-the-shelf LiDAR feature
extractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark
and show that the proposed method achieved state-of-the-art performance,
ranking 1st on its leaderboard
(https://waymo.com/open/challenges/2023/motion-prediction/).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yiqian Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Hao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yizhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Ethan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1&quot;&gt;Lingting Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02420">
<title>Towards Granularity-adjusted Pixel-level Semantic Annotation. (arXiv:2312.02420v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02420</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in computer vision predominantly rely on learning-based
systems, leveraging annotations as the driving force to develop specialized
models. However, annotating pixel-level information, particularly in semantic
segmentation, presents a challenging and labor-intensive task, prompting the
need for autonomous processes. In this work, we propose GranSAM which
distinguishes itself by providing semantic segmentation at the user-defined
granularity level on unlabeled data without the need for any manual
supervision, offering a unique contribution in the realm of semantic mask
annotation method. Specifically, we propose an approach to enable the Segment
Anything Model (SAM) with semantic recognition capability to generate
pixel-level annotations for images without any manual supervision. For this, we
accumulate semantic information from synthetic images generated by the Stable
Diffusion model or web crawled images and employ this data to learn a mapping
function between SAM mask embeddings and object class labels. As a result, SAM,
enabled with granularity-adjusted mask recognition, can be used for pixel-level
semantic annotation purposes. We conducted experiments on the PASCAL VOC 2012
and COCO-80 datasets and observed a +17.95% and +5.17% increase in mIoU,
respectively, compared to existing state-of-the-art methods when evaluated
under our problem setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_R/0/1/0/all/0/1&quot;&gt;Rohit Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Sudipta Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_R/0/1/0/all/0/1&quot;&gt;Rohit Lal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1&quot;&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02428">
<title>FreestyleRet: Retrieving Images from Style-Diversified Queries. (arXiv:2312.02428v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02428</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Retrieval aims to retrieve corresponding images based on a given query.
In application scenarios, users intend to express their retrieval intent
through various query styles. However, current retrieval tasks predominantly
focus on text-query retrieval exploration, leading to limited retrieval query
options and potential ambiguity or bias in user intention. In this paper, we
propose the Style-Diversified Query-Based Image Retrieval task, which enables
retrieval based on various query styles. To facilitate the novel setting, we
propose the first Diverse-Style Retrieval dataset, encompassing diverse query
styles including text, sketch, low-resolution, and art. We also propose a
light-weighted style-diversified retrieval framework. For various query style
inputs, we apply the Gram Matrix to extract the query&apos;s textural features and
cluster them into a style space with style-specific bases. Then we employ the
style-init prompt tuning module to enable the visual encoder to comprehend the
texture and style information of the query. Experiments demonstrate that our
model, employing the style-init prompt tuning strategy, outperforms existing
retrieval models on the style-diversified retrieval task. Moreover,
style-diversified queries~(sketch+text, art+text, etc) can be simultaneously
retrieved in our model. The auxiliary information from other queries enhances
the retrieval performance within the respective query.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Curise Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zesen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kehan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_J/0/1/0/all/0/1&quot;&gt;Jialu Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02432">
<title>Orthogonal Adaptation for Modular Customization of Diffusion Models. (arXiv:2312.02432v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02432</link>
<description rdf:parseType="Literal">&lt;p&gt;Customization techniques for text-to-image models have paved the way for a
wide range of previously unattainable applications, enabling the generation of
specific concepts across diverse contexts and styles. While existing methods
facilitate high-fidelity customization for individual concepts or a limited,
pre-defined set of them, they fall short of achieving scalability, where a
single model can seamlessly render countless concepts. In this paper, we
address a new problem called Modular Customization, with the goal of
efficiently merging customized models that were fine-tuned independently for
individual concepts. This allows the merged model to jointly synthesize
concepts in one image without compromising fidelity or incurring any additional
computational costs.
&lt;/p&gt;
&lt;p&gt;To address this problem, we introduce Orthogonal Adaptation, a method
designed to encourage the customized models, which do not have access to each
other during fine-tuning, to have orthogonal residual weights. This ensures
that during inference time, the customized models can be summed with minimal
interference.
&lt;/p&gt;
&lt;p&gt;Our proposed method is both simple and versatile, applicable to nearly all
optimizable weights in the model architecture. Through an extensive set of
quantitative and qualitative evaluations, our method consistently outperforms
relevant baselines in terms of efficiency and identity preservation,
demonstrating a significant leap toward scalable customization of diffusion
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Po_R/0/1/0/all/0/1&quot;&gt;Ryan Po&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guandao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02433">
<title>Lenna: Language Enhanced Reasoning Detection Assistant. (arXiv:2312.02433v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02433</link>
<description rdf:parseType="Literal">&lt;p&gt;With the fast-paced development of multimodal large language models (MLLMs),
we can now converse with AI systems in natural languages to understand images.
However, the reasoning power and world knowledge embedded in the large language
models have been much less investigated and exploited for image perception
tasks. In this paper, we propose Lenna, a language-enhanced reasoning detection
assistant, which utilizes the robust multimodal feature representation of
MLLMs, while preserving location information for detection. This is achieved by
incorporating an additional &amp;lt;DET&amp;gt; token in the MLLM vocabulary that is free of
explicit semantic context but serves as a prompt for the detector to identify
the corresponding position. To evaluate the reasoning capability of Lenna, we
construct a ReasonDet dataset to measure its performance on reasoning-based
detection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet
and comes with significantly low training costs. It also incurs minimal
transferring overhead when extended to other tasks. Our code and model will be
available at https://git.io/Lenna.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Fei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ailing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02434">
<title>FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions. (arXiv:2312.02434v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02434</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit Neural Representation (INR), which utilizes a neural network to map
coordinate inputs to corresponding attributes, is causing a revolution in the
field of signal processing. However, current INR techniques suffer from a
restricted capability to tune their supported frequency set, resulting in
imperfect performance when representing complex signals with multiple
frequencies. We have identified that this frequency-related problem can be
greatly alleviated by introducing variable-periodic activation functions, for
which we propose FINER. By initializing the bias of the neural network within
different ranges, sub-functions with various frequencies in the
variable-periodic function are selected for activation. Consequently, the
supported frequency set of FINER can be flexibly tuned, leading to improved
performance in signal representation. We demonstrate the capabilities of FINER
in the contexts of 2D image fitting, 3D signed distance field representation,
and 5D neural radiance fields optimization, and we show that it outperforms
existing INRs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jingde Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Weibing Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yanwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02437">
<title>GDN: A Stacking Network Used for Skin Cancer Diagnosis. (arXiv:2312.02437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02437</link>
<description rdf:parseType="Literal">&lt;p&gt;Skin cancer, the primary type of cancer that can be identified by visual
recognition, requires an automatic identification system that can accurately
classify different types of lesions. This paper presents GoogLe-Dense Network
(GDN), which is an image-classification model to identify two types of skin
cancer, Basal Cell Carcinoma, and Melanoma. GDN uses stacking of different
networks to enhance the model performance. Specifically, GDN consists of two
sequential levels in its structure. The first level performs basic
classification tasks accomplished by GoogLeNet and DenseNet, which are trained
in parallel to enhance efficiency. To avoid low accuracy and long training
time, the second level takes the output of the GoogLeNet and DenseNet as the
input for a logistic regression model. We compare our method with four baseline
networks including ResNet, VGGNet, DenseNet, and GoogLeNet on the dataset, in
which GoogLeNet and DenseNet significantly outperform ResNet and VGGNet. In the
second level, different stacking methods such as perceptron, logistic
regression, SVM, decision trees and K-neighbor are studied in which Logistic
Regression shows the best prediction result among all. The results prove that
GDN, compared to a single network structure, has higher accuracy in optimizing
skin cancer detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jingmin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haoyang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziqian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02439">
<title>Let&apos;s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation. (arXiv:2312.02439v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.02439</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain-of-Thought (CoT) guides large language models (LLMs) to reason
step-by-step, and can motivate their logical reasoning ability. While effective
for logical tasks, CoT is not conducive to creative problem-solving which often
requires out-of-box thoughts and is crucial for innovation advancements. In
this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a
non-sequential, creative paradigm involving strong associations and knowledge
leaps. To this end, we study LLMs on the popular Oogiri game which needs
participants to have good creativity and strong associative thinking for
responding unexpectedly and humorously to the given image, text, or both, and
thus is suitable for LoT study. Then to investigate LLMs&apos; LoT ability in the
Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset
which contains over 130,000 samples from the Oogiri game, and observe the
insufficient LoT ability or failures of most existing LLMs on the Oogiri game.
Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve
LLM&apos;s LoT ability. CLoT first formulates the Oogiri-GO dataset into
LoT-oriented instruction tuning data to train pretrained LLM for achieving
certain LoT humor generation and discrimination abilities. Then CLoT designs an
explorative self-refinement that encourages the LLM to generate more creative
LoT data via exploring parallels between seemingly unrelated concepts and
selects high-quality data to train itself for self-refinement. CLoT not only
excels in humor generation in the Oogiri game but also boosts creative
abilities in various tasks like cloud guessing game and divergent association
task. These findings advance our understanding and offer a pathway to improve
LLMs&apos; creative capacities for innovative applications across domains. The
dataset, code, and models will be released online.
https://github.com/sail-sg/CLoT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shanghua Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wushao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02464">
<title>SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints. (arXiv:2312.02464v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02464</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation of remote sensing imagery plays a pivotal role in
extracting precise information for diverse down-stream applications. Recent
development of the Segment Anything Model (SAM), an advanced general-purpose
segmentation model, has revolutionized this field, presenting new avenues for
accurate and efficient segmentation. However, SAM is limited to generating
segmentation results without class information. Consequently, the utilization
of such a powerful general vision model for semantic segmentation in remote
sensing images has become a focal point of research. In this paper, we present
a streamlined framework aimed at leveraging the raw output of SAM by exploiting
two novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary
(SGB). More specifically, we propose a novel object loss and further introduce
a boundary loss as augmentative components to aid in model optimization in a
general semantic segmentation framework. Taking into account the content
characteristics of SGO, we introduce the concept of object consistency to
leverage segmented regions lacking semantic information. By imposing
constraints on the consistency of predicted values within objects, the object
loss aims to enhance semantic segmentation performance. Furthermore, the
boundary loss capitalizes on the distinctive features of SGB by directing the
model&apos;s attention to the boundary information of the object. Experimental
results on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban,
demonstrate the effectiveness of our proposed method. The source code for this
work will be accessible at https://github.com/sstary/SSRS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianping Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qianqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1&quot;&gt;Man-On Pun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bo Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02469">
<title>Learning Energy-based Model via Dual-MCMC Teaching. (arXiv:2312.02469v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.02469</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the fundamental learning problem of the energy-based model
(EBM). Learning the EBM can be achieved using the maximum likelihood estimation
(MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling,
such as the Langevin dynamics. However, the noise-initialized Langevin dynamics
can be challenging in practice and hard to mix. This motivates the exploration
of joint training with the generator model where the generator model serves as
a complementary model to bypass MCMC sampling. However, such a method can be
less accurate than the MCMC and result in biased EBM learning. While the
generator can also serve as an initializer model for better MCMC sampling, its
learning can be biased since it only matches the EBM and has no access to
empirical training examples. Such biased generator learning may limit the
potential of learning the EBM. To address this issue, we present a joint
learning framework that interweaves the maximum likelihood learning algorithm
for both the EBM and the complementary generator model. In particular, the
generator model is learned by MLE to match both the EBM and the empirical data
distribution, making it a more informative initializer for MCMC sampling of
EBM. Learning generator with observed examples typically requires inference of
the generator posterior. To ensure accurate and efficient inference, we adopt
the MCMC posterior sampling and introduce a complementary inference model to
initialize such latent MCMC sampling. We show that three separate models can be
seamlessly integrated into our joint framework through two (dual-) MCMC
teaching, enabling effective and efficient EBM learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiali Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tian Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02470">
<title>Generator Born from Classifier. (arXiv:2312.02470v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.02470</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we make a bold attempt toward an ambitious task: given a
pre-trained classifier, we aim to reconstruct an image generator, without
relying on any data samples. From a black-box perspective, this challenge seems
intractable, since it inevitably involves identifying the inverse function for
a classifier, which is, by nature, an information extraction process. As such,
we resort to leveraging the knowledge encapsulated within the parameters of the
neural network. Grounded on the theory of Maximum-Margin Bias of gradient
descent, we propose a novel learning paradigm, in which the generator is
trained to ensure that the convergence conditions of the network parameters are
satisfied over the generated distribution of the samples. Empirical validation
from various image generation tasks substantiates the efficacy of our strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Runpeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02480">
<title>Differentiable Point-based Inverse Rendering. (arXiv:2312.02480v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02480</link>
<description rdf:parseType="Literal">&lt;p&gt;We present differentiable point-based inverse rendering, DPIR, an
analysis-by-synthesis method that processes images captured under diverse
illuminations to estimate shape and spatially-varying BRDF. To this end, we
adopt point-based rendering, eliminating the need for multiple samplings per
ray, typical of volumetric rendering, thus significantly enhancing the speed of
inverse rendering. To realize this idea, we devise a hybrid point-volumetric
representation for geometry and a regularized basis-BRDF representation for
reflectance. The hybrid geometric representation enables fast rendering through
point-based splatting while retaining the geometric details and stability
inherent to SDF-based representations. The regularized basis-BRDF mitigates the
ill-posedness of inverse rendering stemming from limited light-view angular
samples. We also propose an efficient shadow detection method using point-based
shadow map rendering. Our extensive evaluations demonstrate that DPIR
outperforms prior works in terms of reconstruction accuracy, computational
efficiency, and memory footprint. Furthermore, our explicit point-based
representation and rendering enables intuitive geometry and reflectance
editing. The code will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hoon-Gyu Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seokjun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seung-Hwan Baek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02481">
<title>Learning to Holistically Detect Bridges from Large-Size VHR Remote Sensing Imagery. (arXiv:2312.02481v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02481</link>
<description rdf:parseType="Literal">&lt;p&gt;Bridge detection in remote sensing images (RSIs) plays a crucial role in
various applications, but it poses unique challenges compared to the detection
of other objects. In RSIs, bridges exhibit considerable variations in terms of
their spatial scales and aspect ratios. Therefore, to ensure the visibility and
integrity of bridges, it is essential to perform holistic bridge detection in
large-size very-high-resolution (VHR) RSIs. However, the lack of datasets with
large-size VHR RSIs limits the deep learning algorithms&apos; performance on bridge
detection. Due to the limitation of GPU memory in tackling large-size images,
deep learning-based object detection methods commonly adopt the cropping
strategy, which inevitably results in label fragmentation and discontinuous
prediction. To ameliorate the scarcity of datasets, this paper proposes a
large-scale dataset named GLH-Bridge comprising 6,000 VHR RSIs sampled from
diverse geographic locations across the globe. These images encompass a wide
range of sizes, varying from 2,048*2,048 to 16,38*16,384 pixels, and
collectively feature 59,737 bridges. Furthermore, we present an efficient
network for holistic bridge detection (HBD-Net) in large-size RSIs. The HBD-Net
presents a separate detector-based feature fusion (SDFF) architecture and is
optimized via a shape-sensitive sample re-weighting (SSRW) strategy. Based on
the proposed GLH-Bridge dataset, we establish a bridge detection benchmark
including the OBB and HBB tasks, and validate the effectiveness of the proposed
HBD-Net. Additionally, cross-dataset generalization experiments on two publicly
available datasets illustrate the strong generalization capability of the
GLH-Bridge dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yansheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Junwei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yihua Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jin-Gang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Song Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02483">
<title>EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model. (arXiv:2312.02483v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02483</link>
<description rdf:parseType="Literal">&lt;p&gt;Early weakly supervised video grounding (WSVG) methods often struggle with
incomplete boundary detection due to the absence of temporal boundary
annotations. To bridge the gap between video-level and boundary-level
annotation, explicit-supervision methods, i.e., generating pseudo-temporal
boundaries for training, have achieved great success. However, data
augmentations in these methods might disrupt critical temporal information,
yielding poor pseudo boundaries. In this paper, we propose a new perspective
that maintains the integrity of the original temporal content while introducing
more valuable information for expanding the incomplete boundaries. To this end,
we propose EtC (Expand then Clarify), first use the additional information to
expand the initial incomplete pseudo boundaries, and subsequently refine these
expanded ones to achieve precise boundaries. Motivated by video continuity,
i.e., visual similarity across adjacent frames, we use powerful multimodal
large language models (MLLMs) to annotate each frame within initial pseudo
boundaries, yielding more comprehensive descriptions for expanded boundaries.
To further clarify the noise of expanded boundaries, we combine mutual learning
with a tailored proposal-level contrastive objective to use a learnable
approach to harmonize a balance between incomplete yet clean (initial) and
comprehensive yet noisy (expanded) boundaries for more precise ones.
Experiments demonstrate the superiority of our method on two challenging WSVG
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guozhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xinpeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;De Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02493">
<title>Flexible Communication for Optimal Distributed Learning over Unpredictable Networks. (arXiv:2312.02493v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2312.02493</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient compression alleviates expensive communication in distributed deep
learning by sending fewer values and its corresponding indices, typically via
Allgather (AG). Training with high compression ratio (CR) achieves high
accuracy like DenseSGD, but has lower parallel scaling due to high
communication cost (i.e., parallel efficiency). Using lower CRs improves
parallel efficiency by lowering synchronization cost, but degrades model
accuracy as well (statistical efficiency). Further, speedup attained with
different models and CRs also varies with network latency, effective bandwidth
and collective op used for aggregation. In many cases, collectives like
Allreduce (AR) have lower cost than AG to exchange the same amount of data. In
this paper, we propose an AR-compatible Topk compressor that is
bandwidth-optimal and thus performs better than AG in certain network
configurations. We develop a flexible communication strategy that switches
between AG and AR based on which collective is optimal in the current settings,
and model the pareto-relationship between parallel and statistical efficiency
as a multi-objective optimization (MOO) problem to dynamically adjust CR and
accelerate training while still converging to high accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1&quot;&gt;Sahil Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1&quot;&gt;Martin Swany&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02494">
<title>ReconU-Net: a direct PET image reconstruction using U-Net architecture with back projection-induced skip connection. (arXiv:2312.02494v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2312.02494</link>
<description rdf:parseType="Literal">&lt;p&gt;[Objective] This study aims to introduce a novel back projection-induced
U-Net-shaped architecture, called ReconU-Net, for deep learning-based direct
positron emission tomography (PET) image reconstruction. Additionally, our
objective is to analyze the behavior of direct PET image reconstruction and
gain deeper insights by comparing the proposed ReconU-Net architecture with
other encoder-decoder architectures without skip connections. [Approach] The
proposed ReconU-Net architecture uniquely integrates the physical model of the
back projection operation into the skip connection. This distinctive feature
facilitates the effective transfer of intrinsic spatial information from the
input sinogram to the reconstructed image via an embedded physical model. The
proposed ReconU-Net was trained using Monte Carlo simulation data from the
Brainweb phantom and tested on both simulated and real Hoffman brain phantom
data. [Main results] The proposed ReconU-Net method generated a reconstructed
image with a more accurate structure compared to other deep learning-based
direct reconstruction methods. Further analysis showed that the proposed
ReconU-Net architecture has the ability to transfer features of multiple
resolutions, especially non-abstract high-resolution information, through skip
connections. Despite limited training on simulated data, the proposed
ReconU-Net successfully reconstructed the real Hoffman brain phantom, unlike
other deep learning-based direct reconstruction methods, which failed to
produce a reconstructed image. [Significance] The proposed ReconU-Net can
improve the fidelity of direct PET image reconstruction, even when dealing with
small training datasets, by leveraging the synergistic relationship between
data-driven modeling and the physics model of the imaging process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1&quot;&gt;Fumio Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1&quot;&gt;Kibo Ote&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02501">
<title>Inspecting Model Fairness in Ultrasound Segmentation Tasks. (arXiv:2312.02501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02501</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid expansion of machine learning and deep learning (DL),
researchers are increasingly employing learning-based algorithms to alleviate
diagnostic challenges across diverse medical tasks and applications. While
advancements in diagnostic precision are notable, some researchers have
identified a concerning trend: their models exhibit biased performance across
subgroups characterized by different sensitive attributes. This bias not only
infringes upon the rights of patients but also has the potential to lead to
life-altering consequences. In this paper, we inspect a series of DL
segmentation models using two ultrasound datasets, aiming to assess the
presence of model unfairness in these specific tasks. Our findings reveal that
even state-of-the-art DL algorithms demonstrate unfair behavior in ultrasound
segmentation tasks. These results serve as a crucial warning, underscoring the
necessity for careful model evaluation before their deployment in real-world
scenarios. Such assessments are imperative to ensure ethical considerations and
mitigate the risk of adverse impacts on patient outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zikang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fenghe Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1&quot;&gt;Quan Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jianrui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chunping Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02503">
<title>SAVE: Protagonist Diversification with Structure Agnostic Video Editing. (arXiv:2312.02503v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02503</link>
<description rdf:parseType="Literal">&lt;p&gt;Driven by the upsurge progress in text-to-image (T2I) generation models,
text-to-video (T2V) generation has experienced a significant advance as well.
Accordingly, tasks such as modifying the object or changing the style in a
video have been possible. However, previous works usually work well on trivial
and consistent shapes, and easily collapse on a difficult target that has a
largely different body shape from the original one. In this paper, we spot the
bias problem in the existing video editing method that restricts the range of
choices for the new protagonist and attempt to address this issue using the
conventional image-level personalization method. We adopt motion
personalization that isolates the motion from a single source video and then
modifies the protagonist accordingly. To deal with the natural discrepancy
between image and video, we propose a motion word with an inflated textual
embedding to properly represent the motion in a source video. We also regulate
the motion word to attend to proper motion-related areas by introducing a novel
pseudo optical flow, efficiently computed from the pre-calculated attention
maps. Finally, we decouple the motion from the appearance of the source video
with an additional pseudo word. Extensive experiments demonstrate the editing
capability of our method, taking a step toward more diverse and extensive video
editing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yeji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1&quot;&gt;Wonsik Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junsoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jeesoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1&quot;&gt;Nojun Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02512">
<title>AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation. (arXiv:2312.02512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02512</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. The demo page is available on
https://choijeongsoo.github.io/av2av.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02520">
<title>Towards More Unified In-context Visual Understanding. (arXiv:2312.02520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02520</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advancement of large language models (LLMs) has accelerated the
emergence of in-context learning (ICL) as a cutting-edge approach in the
natural language processing domain. Recently, ICL has been employed in visual
understanding tasks, such as semantic segmentation and image captioning,
yielding promising results. However, existing visual ICL framework can not
enable producing content across multiple modalities, which limits their
potential usage scenarios. To address this issue, we present a new ICL
framework for visual understanding with multi-modal output enabled. First, we
quantize and embed both text and visual prompt into a unified representational
space, structured as interleaved in-context sequences. Then a decoder-only
sparse transformer architecture is employed to perform generative modeling on
them, facilitating in-context learning. Thanks to this design, the model is
capable of handling in-context vision understanding tasks with multimodal
output in a unified pipeline. Experimental results demonstrate that our model
achieves competitive performance compared with specialized models and previous
ICL baselines. Overall, our research takes a further step toward unified
multimodal in-context learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_D/0/1/0/all/0/1&quot;&gt;Dianmo Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dongdong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhentao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiankun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1&quot;&gt;Qi Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Jianmin Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1&quot;&gt;Tao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shengwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Nenghai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02521">
<title>Retrieving Conditions from Reference Images for Diffusion Models. (arXiv:2312.02521v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02521</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent diffusion-based subject driven generative methods have enabled image
generations with good fidelity for specific objects or human portraits.
However, to achieve better versatility for applications, we argue that not only
improved datasets and evaluations are desired, but also more careful methods to
retrieve only relevant information from conditional images are anticipated. To
this end, we propose an anime figures dataset RetriBooru-V1, with enhanced
identity and clothing labels. We state new tasks enabled by this dataset, and
introduce a new diversity metric to measure success in completing these tasks,
quantifying the flexibility of image generations. We establish an RAG-inspired
baseline method, designed to retrieve precise conditional information from
reference images. Then, we compare with current methods on existing task to
demonstrate the capability of the proposed method. Finally, we provide baseline
experiment results on new tasks, and conduct ablation studies on the possible
structural choices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haoran Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jieren Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhihong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02528">
<title>Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline. (arXiv:2312.02528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02528</link>
<description rdf:parseType="Literal">&lt;p&gt;We conduct a comprehensive study on a new task named power battery detection
(PBD), which aims to localize the dense cathode and anode plates endpoints from
X-ray images to evaluate the quality of power batteries. Existing manufacturers
usually rely on human eye observation to complete PBD, which makes it difficult
to balance the accuracy and efficiency of detection. To address this issue and
drive more attention into this meaningful task, we first elaborately collect a
dataset, called X-ray PBD, which has $1,500$ diverse X-ray images selected from
thousands of power batteries of $5$ manufacturers, with $7$ different visual
interference. Then, we propose a novel segmentation-based solution for PBD,
termed multi-dimensional collaborative network (MDCNet). With the help of line
and counting predictors, the representation of the point segmentation branch
can be improved at both semantic and detail aspects. Besides, we design an
effective distance-adaptive mask generation strategy, which can alleviate the
visual challenge caused by the inconsistent distribution density of plates to
provide MDCNet with stable supervision. Without any bells and whistles, our
segmentation-based MDCNet consistently outperforms various other corner
detection, crowd counting and general/tiny object detection-based solutions,
making it a strong baseline that can help facilitate future research in PBD.
Finally, we share some potential difficulties and works for future researches.
The source code and datasets will be publicly available at
\href{&lt;a href=&quot;http://www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0&quot;&gt;this http URL&lt;/a&gt;}{X-ray
PBD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Youwei Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lihe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1&quot;&gt;Jiaming Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02535">
<title>Towards Open-set Gesture Recognition via Feature Activation Enhancement and Orthogonal Prototype Learning. (arXiv:2312.02535v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02535</link>
<description rdf:parseType="Literal">&lt;p&gt;Gesture recognition is a foundational task in human-machine interaction
(HMI). While there has been significant progress in gesture recognition based
on surface electromyography (sEMG), accurate recognition of predefined gestures
only within a closed set is still inadequate in practice. It is essential to
effectively discern and reject unknown gestures of disinterest in a robust
system. Numerous methods based on prototype learning (PL) have been proposed to
tackle this open set recognition (OSR) problem. However, they do not fully
explore the inherent distinctions between known and unknown classes. In this
paper, we propose a more effective PL method leveraging two novel and inherent
distinctions, feature activation level and projection inconsistency.
Specifically, the Feature Activation Enhancement Mechanism (FAEM) widens the
gap in feature activation values between known and unknown classes.
Furthermore, we introduce Orthogonal Prototype Learning (OPL) to construct
multiple perspectives. OPL acts to project a sample from orthogonal directions
to maximize the distinction between its two projections, where unknown samples
will be projected near the clusters of different known classes while known
samples still maintain intra-class similarity. Our proposed method
simultaneously achieves accurate closed-set classification for predefined
gestures and effective rejection for unknown gestures. Extensive experiments
demonstrate its efficacy and superiority in open-set gesture recognition based
on sEMG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Can Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengfeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Crystal Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Hualiang Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1&quot;&gt;Dahong Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02541">
<title>Explainable Severity ranking via pairwise n-hidden comparison: a case study of glaucoma. (arXiv:2312.02541v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.02541</link>
<description rdf:parseType="Literal">&lt;p&gt;Primary open-angle glaucoma (POAG) is a chronic and progressive optic nerve
condition that results in an acquired loss of optic nerve fibers and potential
blindness. The gradual onset of glaucoma results in patients progressively
losing their vision without being consciously aware of the changes. To diagnose
POAG and determine its severity, patients must undergo a comprehensive dilated
eye examination. In this work, we build a framework to rank, compare, and
interpret the severity of glaucoma using fundus images. We introduce a
siamese-based severity ranking using pairwise n-hidden comparisons. We
additionally have a novel approach to explaining why a specific image is deemed
more severe than others. Our findings indicate that the proposed severity
ranking model surpasses traditional ones in terms of diagnostic accuracy and
delivers improved saliency explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cuong V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1&quot;&gt;Shrikanth Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Benjamin Y. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pazzani_M/0/1/0/all/0/1&quot;&gt;Michael Pazzani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02545">
<title>Graph Information Bottleneck for Remote Sensing Segmentation. (arXiv:2312.02545v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02545</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing segmentation has a wide range of applications in environmental
protection, and urban change detection, etc. Despite the success of deep
learning-based remote sensing segmentation methods (e.g., CNN and Transformer),
they are not flexible enough to model irregular objects. In addition, existing
graph contrastive learning methods usually adopt the way of maximizing mutual
information to keep the node representations consistent between different graph
views, which may cause the model to learn task-independent redundant
information. To tackle the above problems, this paper treats images as graph
structures and introduces a simple contrastive vision GNN (SC-ViG) architecture
for remote sensing segmentation. Specifically, we construct a node-masked and
edge-masked graph view to obtain an optimal graph structure representation,
which can adaptively learn whether to mask nodes and edges. Furthermore, this
paper innovatively introduces information bottleneck theory into graph
contrastive learning to maximize task-related information while minimizing
task-independent redundant information. Finally, we replace the convolutional
module in UNet with the SC-ViG module to complete the segmentation and
classification tasks of remote sensing images. Extensive experiments on
publicly available real datasets demonstrate that our method outperforms
state-of-the-art remote sensing image segmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_Y/0/1/0/all/0/1&quot;&gt;Yuntao Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1&quot;&gt;Wei Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1&quot;&gt;Tao Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02546">
<title>Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning. (arXiv:2312.02546v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02546</link>
<description rdf:parseType="Literal">&lt;p&gt;Although vision models such as Contrastive Language-Image Pre-Training (CLIP)
show impressive generalization performance, their zero-shot robustness is still
limited under Out-of-Distribution (OOD) scenarios without fine-tuning. Instead
of undesirably providing human supervision as commonly done, it is possible to
take advantage of Multi-modal Large Language Models (MLLMs) that hold powerful
visual understanding abilities. However, MLLMs are shown to struggle with
vision problems due to the incompatibility of tasks, thus hindering their
utilization. In this paper, we propose to effectively leverage MLLMs to conduct
Machine Vision Therapy which aims to rectify the noisy predictions from vision
models. By fine-tuning with the denoised labels, the learning model performance
can be boosted in an unsupervised manner. To solve the incompatibility issue,
we propose a novel Denoising In-Context Learning (DICL) strategy to align
vision tasks with MLLMs. Concretely, by estimating a transition matrix that
captures the probability of one class being confused with another, an
instruction containing a correct exemplar and an erroneous one from the most
probable noisy class can be constructed. Such an instruction can help any MLLMs
with ICL ability to detect and rectify incorrect predictions of vision models.
Through extensive experiments on ImageNet, WILDS, DomainBed, and other OOD
datasets, we carefully validate the quantitative and qualitative effectiveness
of our method. Our code is available at
https://github.com/tmllab/Machine_Vision_Therapy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhuo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shibao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02548">
<title>GeNIe: Generative Hard Negative Images Through Diffusion. (arXiv:2312.02548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02548</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is crucial in training deep models, preventing them from
overfitting to limited data. Common data augmentation methods are effective,
but recent advancements in generative AI, such as diffusion models for image
generation, enable more sophisticated augmentation techniques that produce data
resembling natural images. We recognize that augmented samples closer to the
ideal decision boundary of a classifier are particularly effective and
efficient in guiding the learning process. We introduce GeNIe which leverages a
diffusion model conditioned on a text prompt to merge contrasting data points
(an image from the source category and a text prompt from the target category)
to generate challenging samples for the target category. Inspired by recent
image editing methods, we limit the number of diffusion iterations and the
amount of noise. This ensures that the generated image retains low-level and
contextual features from the source image, potentially conflicting with the
target category. Our extensive experiments, in few-shot and also long-tail
distribution settings, demonstrate the effectiveness of our novel augmentation
method, especially benefiting categories with a limited number of examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1&quot;&gt;Soroush Abbasi Koohpayegani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Anuj Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navaneet_K/0/1/0/all/0/1&quot;&gt;K L Navaneet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamali_Rad_H/0/1/0/all/0/1&quot;&gt;Hadi Jamali-Rad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1&quot;&gt;Hamed Pirsiavash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02549">
<title>DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding. (arXiv:2312.02549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02549</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Language Grounding seeks to localize video moments that semantically
correspond to a natural language query. Recent advances employ the attention
mechanism to learn the relations between video moments and the text query.
However, naive attention might not be able to appropriately capture such
relations, resulting in ineffective distributions where target video moments
are difficult to separate from the remaining ones. To resolve the issue, we
propose an energy-based model framework to explicitly learn moment-query
distributions. Moreover, we propose DemaFormer, a novel Transformer-based
architecture that utilizes exponential moving average with a learnable damping
factor to effectively encode moment-query inputs. Comprehensive experiments on
four public temporal language grounding datasets showcase the superiority of
our methods over the state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaobao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xinshuai Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cong-Duy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1&quot;&gt;See-Kiong Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02567">
<title>Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts. (arXiv:2312.02567v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02567</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning facilitates the collaborative learning of a global model
across multiple distributed medical institutions without centralizing data.
Nevertheless, the expensive cost of annotation on local clients remains an
obstacle to effectively utilizing local data. To mitigate this issue, federated
active learning methods suggest leveraging local and global model predictions
to select a relatively small amount of informative local data for annotation.
However, existing methods mainly focus on all local data sampled from the same
domain, making them unreliable in realistic medical scenarios with domain
shifts among different clients. In this paper, we make the first attempt to
assess the informativeness of local data derived from diverse domains and
propose a novel methodology termed Federated Evidential Active Learning (FEAL)
to calibrate the data evaluation under domain shift. Specifically, we introduce
a Dirichlet prior distribution in both local and global models to treat the
prediction as a distribution over the probability simplex and capture both
aleatoric and epistemic uncertainties by using the Dirichlet-based evidential
model. Then we employ the epistemic uncertainty to calibrate the aleatoric
uncertainty. Afterward, we design a diversity relaxation strategy to reduce
data redundancy and maintain data diversity. Extensive experiments and analyses
are conducted to show the superiority of FEAL over the state-of-the-art active
learning methods and the efficiency of FEAL under the federated active learning
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Benteng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Hengfei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kwang-Ting Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02568">
<title>Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent. (arXiv:2312.02568v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02568</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores promptable NeRF generation (e.g., text prompt or single
image prompt) for direct conditioning and fast generation of NeRF parameters
for the underlying 3D scenes, thus undoing complex intermediate steps while
providing full 3D generation with conditional control. Unlike previous
diffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,
Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a single
forward pass, leveraging a pre-trained implicit latent space of NeRF
parameters. Furthermore, in zero-shot tasks, our experiments demonstrate that
the NeRFs produced by our method serve as semantically informative
initializations, significantly accelerating the inference process of existing
prompt-to-NeRF methods. Specifically, we will show that our approach speeds up
the text-to-NeRF model DreamFusion and the 3D reconstruction speed of the
image-to-NeRF method Zero-1-to-3 by 3 to 5 times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianmeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02576">
<title>An Integrated System for Spatio-Temporal Summarization of 360-degrees Videos. (arXiv:2312.02576v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02576</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present an integrated system for spatiotemporal
summarization of 360-degrees videos. The video summary production mainly
involves the detection of salient events and their synopsis into a concise
summary. The analysis relies on state-of-the-art methods for saliency detection
in 360-degrees video (ATSal and SST-Sal) and video summarization (CA-SUM). It
also contains a mechanism that classifies a 360-degrees video based on the use
of static or moving camera during recording and decides which saliency
detection method will be used, as well as a 2D video production component that
is responsible to create a conventional 2D video containing the salient events
in the 360-degrees video. Quantitative evaluations using two datasets for
360-degrees video saliency detection (VR-EyeTracking, Sports-360) show the
accuracy and positive impact of the developed decision mechanism, and justify
our choice to use two different methods for detecting the salient events. A
qualitative analysis using content from these datasets, gives further insights
about the functionality of the decision mechanism, shows the pros and cons of
each used saliency detection method and demonstrates the advanced performance
of the trained summarization method against a more conventional approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontostathis_I/0/1/0/all/0/1&quot;&gt;Ioannis Kontostathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apostolidis_E/0/1/0/all/0/1&quot;&gt;Evlampios Apostolidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mezaris_V/0/1/0/all/0/1&quot;&gt;Vasileios Mezaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02605">
<title>Accelerating Learnt Video Codecs with Gradient Decay and Layer-wise Distillation. (arXiv:2312.02605v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.02605</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, end-to-end learnt video codecs have demonstrated their
potential to compete with conventional coding algorithms in term of compression
efficiency. However, most learning-based video compression models are
associated with high computational complexity and latency, in particular at the
decoder side, which limits their deployment in practical applications. In this
paper, we present a novel model-agnostic pruning scheme based on gradient decay
and adaptive layer-wise distillation. Gradient decay enhances parameter
exploration during sparsification whilst preventing runaway sparsity and is
superior to the standard Straight-Through Estimation. The adaptive layer-wise
distillation regulates the sparse training in various stages based on the
distortion of intermediate features. This stage-wise design efficiently updates
parameters with minimal computational overhead. The proposed approach has been
applied to three popular end-to-end learnt video codecs, FVC, DCVC, and
DCVC-HEM. Results confirm that our method yields up to 65% reduction in MACs
and 2x speed-up with less than 0.3dB drop in BD-PSNR. Supporting code and
supplementary material can be downloaded from:
https://jasminepp.github.io/lightweightdvc/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tianhao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Ge Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Heming Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02608">
<title>Panoptica -- instance-wise evaluation of 3D semantic and instance segmentation maps. (arXiv:2312.02608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02608</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces panoptica, a versatile and performance-optimized
package designed for computing instance-wise segmentation quality metrics from
2D and 3D segmentation maps. panoptica addresses the limitations of existing
metrics and provides a modular framework that complements the original
intersection over union-based panoptic quality with other metrics, such as the
distance metric Average Symmetric Surface Distance. The package is open-source,
implemented in Python, and accompanied by comprehensive documentation and
tutorials. panoptica employs a three-step metrics computation process to cover
diverse use cases. The efficacy of panoptica is demonstrated on various
real-world biomedical datasets, where an instance-wise evaluation is
instrumental for an accurate representation of the underlying clinical task.
Overall, we envision panoptica as a valuable tool facilitating in-depth
evaluation of segmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_H/0/1/0/all/0/1&quot;&gt;Hendrik M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchner_J/0/1/0/all/0/1&quot;&gt;Josef A. Buchner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1&quot;&gt;Ezequiel de la Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1&quot;&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosier_M/0/1/0/all/0/1&quot;&gt;Marcel Rosier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mekki_I/0/1/0/all/0/1&quot;&gt;Isra Mekki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1&quot;&gt;Suprosanna Shit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negwer_M/0/1/0/all/0/1&quot;&gt;Moritz Negwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Maskari_R/0/1/0/all/0/1&quot;&gt;Rami Al-Maskari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erturk_A/0/1/0/all/0/1&quot;&gt;Ali Ert&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinayahalingam_S/0/1/0/all/0/1&quot;&gt;Shankeeth Vinayahalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pati_S/0/1/0/all/0/1&quot;&gt;Sarthak Pati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan S. Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehrlich_S/0/1/0/all/0/1&quot;&gt;Stefan K. Ehrlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1&quot;&gt;Annika Reinke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Benedikt Wiestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piraud_M/0/1/0/all/0/1&quot;&gt;Marie Piraud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02613">
<title>A Unified Simulation Framework for Visual and Behavioral Fidelity in Crowd Analysis. (arXiv:2312.02613v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02613</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation is a powerful tool to easily generate annotated data, and a highly
desirable feature, especially in those domains where learning models need large
training datasets. Machine learning and deep learning solutions, have proven to
be extremely data-hungry and sometimes, the available real-world data are not
sufficient to effectively model the given task. Despite the initial skepticism
of a portion of the scientific community, the potential of simulation has been
largely confirmed in many application areas, and the recent developments in
terms of rendering and virtualization engines, have shown a good ability also
in representing complex scenes. This includes environmental factors, such as
weather conditions and surface reflectance, as well as human-related events,
like human actions and behaviors. We present a human crowd simulator, called
UniCrowd, and its associated validation pipeline. We show how the simulator can
generate annotated data, suitable for computer vision tasks, in particular for
detection and segmentation, as well as the related applications, as crowd
counting, human pose estimation, trajectory analysis and prediction, and
anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisagno_N/0/1/0/all/0/1&quot;&gt;Niccol&amp;#xf2; Bisagno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garau_N/0/1/0/all/0/1&quot;&gt;Nicola Garau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefani_A/0/1/0/all/0/1&quot;&gt;Antonio Luigi Stefani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conci_N/0/1/0/all/0/1&quot;&gt;Nicola Conci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02615">
<title>Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models. (arXiv:2312.02615v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.02615</link>
<description rdf:parseType="Literal">&lt;p&gt;Novelty detection is a fundamental task of machine learning which aims to
detect abnormal ($\textit{i.e.}$ out-of-distribution (OOD)) samples. Since
diffusion models have recently emerged as the de facto standard generative
framework with surprising generation results, novelty detection via diffusion
models has also gained much attention. Recent methods have mainly utilized the
reconstruction property of in-distribution samples. However, they often suffer
from detecting OOD samples that share similar background information to the
in-distribution data. Based on our observation that diffusion models can
\emph{project} any sample to an in-distribution sample with similar background
information, we propose \emph{Projection Regret (PR)}, an efficient novelty
detection method that mitigates the bias of non-semantic information. To be
specific, PR computes the perceptual distance between the test image and its
diffusion-based projection to detect abnormality. Since the perceptual distance
often fails to capture semantic changes when the background information is
dominant, we cancel out the background bias by comparing it against recursive
projections. Extensive experiments demonstrate that PR outperforms the prior
art of generative-model-based novelty detection methods by a significant
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sungik Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hankook Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Moontae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02616">
<title>Facilitating the Production of Well-tailored Video Summaries for Sharing on Social Media. (arXiv:2312.02616v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02616</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a web-based tool that facilitates the production of
tailored summaries for online sharing on social media. Through an interactive
user interface, it supports a ``one-click&apos;&apos; video summarization process. Based
on the integrated AI models for video summarization and aspect ratio
transformation, it facilitates the generation of multiple summaries of a
full-length video according to the needs of target platforms with regard to the
video&apos;s length and aspect ratio.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apostolidis_E/0/1/0/all/0/1&quot;&gt;Evlampios Apostolidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apostolidis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Apostolidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mezaris_V/0/1/0/all/0/1&quot;&gt;Vasileios Mezaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02617">
<title>DreaMo: Articulated 3D Reconstruction From A Single Casual Video. (arXiv:2312.02617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02617</link>
<description rdf:parseType="Literal">&lt;p&gt;Articulated 3D reconstruction has valuable applications in various domains,
yet it remains costly and demands intensive work from domain experts. Recent
advancements in template-free learning methods show promising results with
monocular videos. Nevertheless, these approaches necessitate a comprehensive
coverage of all viewpoints of the subject in the input video, thus limiting
their applicability to casually captured videos from online sources. In this
work, we study articulated 3D shape reconstruction from a single and casually
captured internet video, where the subject&apos;s view coverage is incomplete. We
propose DreaMo that jointly performs shape reconstruction while solving the
challenging low-coverage regions with view-conditioned diffusion prior and
several tailored regularizations. In addition, we introduce a skeleton
generation strategy to create human-interpretable skeletons from the learned
neural bones and skinning weights. We conduct our study on a self-collected
internet video collection characterized by incomplete view coverage. DreaMo
shows promising quality in novel-view rendering, detailed articulated shape
reconstruction, and skeleton generation. Extensive qualitative and quantitative
studies validate the efficacy of each proposed component, and show existing
methods are unable to solve correct geometry due to the incomplete view
coverage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming-Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chieh Hubert Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yen-Chi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Min Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02625">
<title>Diffusion Noise Feature: Accurate and Fast Generated Image Detection. (arXiv:2312.02625v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02625</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models have reached an advanced stage where they can produce
remarkably realistic images. However, this remarkable generative capability
also introduces the risk of disseminating false or misleading information.
Notably, existing image detectors for generated images encounter challenges
such as low accuracy and limited generalization. This paper seeks to address
this issue by seeking a representation with strong generalization capabilities
to enhance the detection of generated images. Our investigation has revealed
that real and generated images display distinct latent Gaussian representations
when subjected to an inverse diffusion process within a pre-trained diffusion
model. Exploiting this disparity, we can amplify subtle artifacts in generated
images. Building upon this insight, we introduce a novel image representation
known as Diffusion Noise Feature (DNF). DNF is an ensemble representation that
estimates the noise generated during the inverse diffusion process. A simple
classifier, e.g., ResNet, trained on DNF achieves high accuracy, robustness,
and generalization capabilities for detecting generated images, even from
previously unseen classes or models. We conducted experiments using a widely
recognized and standard dataset, achieving state-of-the-art effects of
Detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02638">
<title>Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs. (arXiv:2312.02638v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02638</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of transferring a temporal action segmentation system
initially designed for exocentric (fixed) cameras to an egocentric scenario,
where wearable cameras capture video data. The conventional supervised approach
requires the collection and labeling of a new set of egocentric videos to adapt
the model, which is costly and time-consuming. Instead, we propose a novel
methodology which performs the adaptation leveraging existing labeled
exocentric videos and a new set of unlabeled, synchronized
exocentric-egocentric video pairs, for which temporal action segmentation
annotations do not need to be collected. We implement the proposed methodology
with an approach based on knowledge distillation, which we investigate both at
the feature and model level. To evaluate our approach, we introduce a new
benchmark based on the Assembly101 dataset. Results demonstrate the feasibility
and effectiveness of the proposed method against classic unsupervised domain
adaptation and temporal sequence alignment approaches. Remarkably, without
bells and whistles, our best model performs on par with supervised approaches
trained on labeled egocentric data, without ever seeing a single egocentric
label, achieving a +15.99% (28.59% vs 12.60%) improvement in the edit score on
the Assembly101 dataset compared to a baseline model trained solely on
exocentric data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quattrocchi_C/0/1/0/all/0/1&quot;&gt;Camillo Quattrocchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1&quot;&gt;Antonino Furnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauro_D/0/1/0/all/0/1&quot;&gt;Daniele Di Mauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giuffrida_M/0/1/0/all/0/1&quot;&gt;Mario Valerio Giuffrida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1&quot;&gt;Giovanni Maria Farinella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02647">
<title>TPA3D: Triplane Attention for Fast Text-to-3D Generation. (arXiv:2312.02647v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02647</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the lack of large-scale text-3D correspondence data, recent text-to-3D
generation works mainly rely on utilizing 2D diffusion models for synthesizing
3D data. Since diffusion-based methods typically require significant
optimization time for both training and inference, the use of GAN-based models
would still be desirable for fast 3D generation. In this work, we propose
Triplane Attention for text-guided 3D generation (TPA3D), an end-to-end
trainable GAN-based deep learning model for fast text-to-3D generation. With
only 3D shape data and their rendered 2D images observed during training, our
TPA3D is designed to retrieve detailed visual descriptions for synthesizing the
corresponding 3D mesh data. This is achieved by the proposed attention
mechanisms on the extracted sentence and word-level text features. In our
experiments, we show that TPA3D generates high-quality 3D textured shapes
aligned with fine-grained descriptions, while impressive computation efficiency
can be observed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong-En Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bin-Shih Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sheng-Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02663">
<title>FaceStudio: Put Your Face Everywhere in Seconds. (arXiv:2312.02663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02663</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates identity-preserving image synthesis, an intriguing
task in image generation that seeks to maintain a subject&apos;s identity while
adding a personalized, stylistic touch. Traditional methods, such as Textual
Inversion and DreamBooth, have made strides in custom image creation, but they
come with significant drawbacks. These include the need for extensive resources
and time for fine-tuning, as well as the requirement for multiple reference
images. To overcome these challenges, our research introduces a novel approach
to identity-preserving synthesis, with a particular focus on human images. Our
model leverages a direct feed-forward mechanism, circumventing the need for
intensive fine-tuning, thereby facilitating quick and efficient image
generation. Central to our innovation is a hybrid guidance framework, which
combines stylized images, facial images, and textual prompts to guide the image
generation process. This unique combination enables our model to produce a
variety of applications, such as artistic portraits and identity-blended
images. Our experimental results, including both qualitative and quantitative
evaluations, demonstrate the superiority of our method over existing baseline
models and previous works, particularly in its remarkable efficiency and
ability to preserve the subject&apos;s identity with high fidelity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bin Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02672">
<title>Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark. (arXiv:2312.02672v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02672</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate the effectiveness of synthetic data in
enhancing hand-object interaction detection within the egocentric vision
domain. We introduce a simulator able to generate synthetic images of
hand-object interactions automatically labeled with hand-object contact states,
bounding boxes, and pixel-wise segmentation masks. Through comprehensive
experiments and comparative analyses on three egocentric datasets, VISOR,
EgoHOS, and ENIGMA-51, we demonstrate that the use of synthetic data and domain
adaptation techniques allows for comparable performance to conventional
supervised methods while requiring annotations on only a fraction of the real
data. When tested with in-domain synthetic data generated from 3D models of
real target environments and objects, our best models show consistent
performance improvements with respect to standard fully supervised approaches
based on labeled real data only. Our study also sets a new benchmark of domain
adaptation for egocentric hand-object interaction detection (HOI-Synth) and
provides baseline results to encourage the community to engage in this
challenging task. We release the generated data, code, and the simulator at the
following link: https://iplab.dmi.unict.it/HOI-Synth/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardi_R/0/1/0/all/0/1&quot;&gt;Rosario Leonardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1&quot;&gt;Antonino Furnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragusa_F/0/1/0/all/0/1&quot;&gt;Francesco Ragusa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1&quot;&gt;Giovanni Maria Farinella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02684">
<title>DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors. (arXiv:2312.02684v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02684</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds have shown significant potential in various domains, including
Simultaneous Localization and Mapping (SLAM). However, existing approaches
either rely on dense point clouds to achieve high localization accuracy or use
generalized descriptors to reduce map size. Unfortunately, these two aspects
seem to conflict with each other. To address this limitation, we propose a
unified architecture, DeepPointMap, achieving excellent preference on both
aspects. We utilize neural network to extract highly representative and sparse
neural descriptors from point clouds, enabling memory-efficient map
representation and accurate multi-scale localization tasks (e.g., odometry and
loop-closure). Moreover, we showcase the versatility of our framework by
extending it to more challenging multi-agent collaborative SLAM. The promising
results obtained in these scenarios further emphasize the effectiveness and
potential of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaze Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Ziheng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1&quot;&gt;Qi Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuejie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenchao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Rui Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02694">
<title>UPOCR: Towards Unified Pixel-Level OCR Interface. (arXiv:2312.02694v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02694</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the optical character recognition (OCR) field has been
proliferating with plentiful cutting-edge approaches for a wide spectrum of
tasks. However, these approaches are task-specifically designed with divergent
paradigms, architectures, and training strategies, which significantly
increases the complexity of research and maintenance and hinders the fast
deployment in applications. To this end, we propose UPOCR, a
simple-yet-effective generalist model for Unified Pixel-level OCR interface.
Specifically, the UPOCR unifies the paradigm of diverse OCR tasks as
image-to-image transformation and the architecture as a vision Transformer
(ViT)-based encoder-decoder. Learnable task prompts are introduced to push the
general feature representations extracted by the encoder toward task-specific
spaces, endowing the decoder with task awareness. Moreover, the model training
is uniformly aimed at minimizing the discrepancy between the generated and
ground-truth images regardless of the inhomogeneity among tasks. Experiments
are conducted on three pixel-level OCR tasks including text removal, text
segmentation, and tampered text detection. Without bells and whistles, the
experimental results showcase that the proposed method can simultaneously
achieve state-of-the-art performance on three tasks with a unified single
model, which provides valuable strategies and insights for future research on
generalist OCR models. Code will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chongyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yongxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1&quot;&gt;Kai Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fengjun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lianwen Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02696">
<title>Analyzing and Improving the Training Dynamics of Diffusion Models. (arXiv:2312.02696v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02696</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models currently dominate the field of data-driven image synthesis
with their unparalleled scaling to large datasets. In this paper, we identify
and rectify several causes for uneven and ineffective training in the popular
ADM diffusion model architecture, without altering its high-level structure.
Observing uncontrolled magnitude changes and imbalances in both the network
activations and weights over the course of training, we redesign the network
layers to preserve activation, weight, and update magnitudes on expectation. We
find that systematic application of this philosophy eliminates the observed
drifts and imbalances, resulting in considerably better networks at equal
computational complexity. Our modifications improve the previous record FID of
2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic
sampling.
&lt;/p&gt;
&lt;p&gt;As an independent contribution, we present a method for setting the
exponential moving average (EMA) parameters post-hoc, i.e., after completing
the training run. This allows precise tuning of EMA length without the cost of
performing several training runs, and reveals its surprising interactions with
network architecture, training time, and guidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1&quot;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1&quot;&gt;Miika Aittala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1&quot;&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1&quot;&gt;Janne Hellsten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1&quot;&gt;Timo Aila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1&quot;&gt;Samuli Laine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02699">
<title>Enhancing Vehicle Entrance and Parking Management: Deep Learning Solutions for Efficiency and Security. (arXiv:2312.02699v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02699</link>
<description rdf:parseType="Literal">&lt;p&gt;The auto-management of vehicle entrance and parking in any organization is a
complex challenge encompassing record-keeping, efficiency, and security
concerns. Manual methods for tracking vehicles and finding parking spaces are
slow and a waste of time. To solve the problem of auto management of vehicle
entrance and parking, we have utilized state-of-the-art deep learning models
and automated the process of vehicle entrance and parking into any
organization. To ensure security, our system integrated vehicle detection,
license number plate verification, and face detection and recognition models to
ensure that the person and vehicle are registered with the organization. We
have trained multiple deep-learning models for vehicle detection, license
number plate detection, face detection, and recognition, however, the YOLOv8n
model outperformed all the other models. Furthermore, License plate recognition
is facilitated by Google&apos;s Tesseract-OCR Engine. By integrating these
technologies, the system offers efficient vehicle detection, precise
identification, streamlined record keeping, and optimized parking slot
allocation in buildings, thereby enhancing convenience, accuracy, and security.
Future research opportunities lie in fine-tuning system performance for a wide
range of real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramzan_M/0/1/0/all/0/1&quot;&gt;Muhammad Umer Ramzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_U/0/1/0/all/0/1&quot;&gt;Usman Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naqvi_S/0/1/0/all/0/1&quot;&gt;Syed Haider Abbas Naqvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aslam_Z/0/1/0/all/0/1&quot;&gt;Zeeshan Aslam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tehseen/0/1/0/all/0/1&quot;&gt;Tehseen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1&quot;&gt;Husnain Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faheem_M/0/1/0/all/0/1&quot;&gt;Muhammad Faheem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02700">
<title>Revisit Human-Scene Interaction via Space Occupancy. (arXiv:2312.02700v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02700</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-scene Interaction (HSI) generation is a challenging task and crucial
for various downstream tasks. However, one of the major obstacles is the
limited data scale. High-quality data with simultaneously captured human and 3D
environments is rare, resulting in limited data diversity and complexity. In
this work, we argue that interaction with a scene is essentially interacting
with the space occupancy of the scene from an abstract physical perspective,
leading us to a unified novel view of Human-Occupancy Interaction. By treating
pure motion sequences as records of humans interacting with invisible scene
occupancy, we can aggregate motion-only data into a large-scale paired
human-occupancy interaction database: Motion Occupancy Base (MOB). Thus, the
need for costly paired motion-scene datasets with high-quality scene scans can
be substantially alleviated. With this new unified view of Human-Occupancy
interaction, a single motion controller is proposed to reach the target state
given the surrounding occupancy. Once trained on MOB with complex occupancy
layout, the controller could handle cramped scenes and generalize well to
general scenes with limited complexity. With no GT 3D scenes for training, our
method can generate realistic and stable HSI motions in diverse scenarios,
including both static and dynamic scenes. Our code and data would be made
publicly available at https://foruck.github.io/occu-page/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1&quot;&gt;Haowen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yanchao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02702">
<title>Neural Sign Actors: A diffusion model for 3D sign language production from text. (arXiv:2312.02702v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02702</link>
<description rdf:parseType="Literal">&lt;p&gt;Sign Languages (SL) serve as the predominant mode of communication for the
Deaf and Hard of Hearing communities. The advent of deep learning has aided
numerous methods in SL recognition and translation, achieving remarkable
results. However, Sign Language Production (SLP) poses a challenge for the
computer vision community as the motions generated must be realistic and have
precise semantic meanings. Most SLP methods rely on 2D data, thus impeding
their ability to attain a necessary level of realism. In this work, we propose
a diffusion-based SLP model trained on a curated large-scale dataset of 4D
signing avatars and their corresponding text transcripts. The proposed method
can generate dynamic sequences of 3D avatars from an unconstrained domain of
discourse using a diffusion process formed on a novel and anatomically informed
graph neural network defined on the SMPL-X body skeleton. Through a series of
quantitative and qualitative experiments, we show that the proposed method
considerably outperforms previous methods of SLP. We believe that this work
presents an important and necessary step towards realistic neural sign avatars,
bridging the communication gap between Deaf and hearing communities. The code,
method and generated data will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1&quot;&gt;Vasileios Baltatzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1&quot;&gt;Rolandos Alexandros Potamias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ververas_E/0/1/0/all/0/1&quot;&gt;Evangelos Ververas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guanxiong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02703">
<title>MyPortrait: Morphable Prior-Guided Personalized Portrait Generation. (arXiv:2312.02703v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02703</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating realistic talking faces is an interesting and long-standing topic
in the field of computer vision. Although significant progress has been made,
it is still challenging to generate high-quality dynamic faces with
personalized details. This is mainly due to the inability of the general model
to represent personalized details and the generalization problem to unseen
controllable parameters. In this work, we propose Myportrait, a simple,
general, and flexible framework for neural portrait generation. We incorporate
personalized prior in a monocular video and morphable prior in 3D face
morphable space for generating personalized details under novel controllable
parameters. Our proposed framework supports both video-driven and audio-driven
face animation given a monocular video of a single person. Distinguished by
whether the test data is sent to training or not, our method provides a
real-time online version and a high-quality offline version. Comprehensive
experiments in various metrics demonstrate the superior performance of our
method over the state-of-the-art methods. The code will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bo Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shihong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02705">
<title>Unified learning-based lossy and lossless JPEG recompression. (arXiv:2312.02705v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02705</link>
<description rdf:parseType="Literal">&lt;p&gt;JPEG is still the most widely used image compression algorithm. Most image
compression algorithms only consider uncompressed original image, while
ignoring a large number of already existing JPEG images. Recently, JPEG
recompression approaches have been proposed to further reduce the size of JPEG
files. However, those methods only consider JPEG lossless recompression, which
is just a special case of the rate-distortion theorem. In this paper, we
propose a unified lossly and lossless JPEG recompression framework, which
consists of learned quantization table and Markovian hierarchical variational
autoencoders. Experiments show that our method can achieve arbitrarily low
distortion when the bitrate is close to the upper bound, namely the bitrate of
the lossless compression model. To the best of our knowledge, this is the first
learned method that bridges the gap between lossy and lossless recompression of
JPEG images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Lina Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jixiang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tongda Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Hongwei Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02719">
<title>A Conditional Denoising Diffusion Probabilistic Model for Point Cloud Upsampling. (arXiv:2312.02719v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02719</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud upsampling (PCU) enriches the representation of raw point clouds,
significantly improving the performance in downstream tasks such as
classification and reconstruction. Most of the existing point cloud upsampling
methods focus on sparse point cloud feature extraction and upsampling module
design. In a different way, we dive deeper into directly modelling the gradient
of data distribution from dense point clouds. In this paper, we proposed a
conditional denoising diffusion probability model (DDPM) for point cloud
upsampling, called PUDM. Specifically, PUDM treats the sparse point cloud as a
condition, and iteratively learns the transformation relationship between the
dense point cloud and the noise. Simultaneously, PUDM aligns with a dual
mapping paradigm to further improve the discernment of point features. In this
context, PUDM enables learning complex geometry details in the ground truth
through the dominant features, while avoiding an additional upsampling module
design. Furthermore, to generate high-quality arbitrary-scale point clouds
during inference, PUDM exploits the prior knowledge of the scale between sparse
point clouds and dense point clouds during training by parameterizing a rate
factor. Moreover, PUDM exhibits strong noise robustness in experimental
results. In the quantitative and qualitative evaluations on PU1K and PUGAN,
PUDM significantly outperformed existing methods in terms of Chamfer Distance
(CD) and Hausdorff Distance (HD), achieving state of the art (SOTA)
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_W/0/1/0/all/0/1&quot;&gt;Wentao Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yuantian Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lingwu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoshui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Liang Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02725">
<title>R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction. (arXiv:2312.02725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02725</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, vision transformers have performed well in various computer vision
tasks, including voxel 3D reconstruction. However, the windows of the vision
transformer are not multi-scale, and there is no connection between the
windows, which limits the accuracy of voxel 3D reconstruction . Therefore, we
propose a shifted windows attention voxel 3D reconstruction network. To the
best of our knowledge, this is the first work to apply shifted window attention
to voxel 3D reconstruction. Experimental results on ShapeNet verify our method
achieves SOTA accuracy in single-view reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenhuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Meihua Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+li_z/0/1/0/all/0/1&quot;&gt;zehuan li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Mengxi Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02751">
<title>C-NERF: Representing Scene Changes as Directional Consistency Difference-based NeRF. (arXiv:2312.02751v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02751</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we aim to detect the changes caused by object variations in a
scene represented by the neural radiance fields (NeRFs). Given an arbitrary
view and two sets of scene images captured at different timestamps, we can
predict the scene changes in that view, which has significant potential
applications in scene monitoring and measuring. We conducted preliminary
studies and found that such an exciting task cannot be easily achieved by
utilizing existing NeRFs and 2D change detection methods with many false or
missing detections. The main reason is that the 2D change detection is based on
the pixel appearance difference between spatial-aligned image pairs and
neglects the stereo information in the NeRF. To address the limitations, we
propose the C-NERF to represent scene changes as directional consistency
difference-based NeRF, which mainly contains three modules. We first perform
the spatial alignment of two NeRFs captured before and after changes. Then, we
identify the change points based on the direction-consistent constraint; that
is, real change points have similar change representations across view
directions, but fake change points do not. Finally, we design the change map
rendering process based on the built NeRFs and can generate the change map of
an arbitrarily specified view direction. To validate the effectiveness, we
build a new dataset containing ten scenes covering diverse scenarios with
different changing objects. Our approach surpasses state-of-the-art 2D change
detection and NeRF-based methods by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Rui Huang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Binbin Jiang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingyi Zhao&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Wang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt; (3 and 4) ((1) College of Computer Science and Technology, Civil Aviation University of China, China, (2) University of South Carolina, The USA, (3) IHPC, Agency for Science, Technology and Research, Singapore, (4) CFAR, Agency for Science, Technology and Research, Singapore)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02753">
<title>C3: High-performance and low-complexity neural compression from a single image or video. (arXiv:2312.02753v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.02753</link>
<description rdf:parseType="Literal">&lt;p&gt;Most neural compression models are trained on large datasets of images or
videos in order to generalize to unseen data. Such generalization typically
requires large and expressive architectures with a high decoding complexity.
Here we introduce C3, a neural compression method with strong rate-distortion
(RD) performance that instead overfits a small model to each image or video
separately. The resulting decoding complexity of C3 can be an order of
magnitude lower than neural baselines with similar RD performance. C3 builds on
COOL-CHIC (Ladune et al.) and makes several simple and effective improvements
for images. We further develop new methodology to apply C3 to videos. On the
CLIC2020 image benchmark, we match the RD performance of VTM, the reference
implementation of the H.266 codec, with less than 3k MACs/pixel for decoding.
On the UVG video benchmark, we match the RD performance of the Video
Compression Transformer (Mentzer et al.), a well-established neural video
codec, with less than 5k MACs/pixel for decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunjik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bauer_M/0/1/0/all/0/1&quot;&gt;Matthias Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Theis_L/0/1/0/all/0/1&quot;&gt;Lucas Theis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schwarz_J/0/1/0/all/0/1&quot;&gt;Jonathan Richard Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dupont_E/0/1/0/all/0/1&quot;&gt;Emilien Dupont&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02762">
<title>Learning Cortical Anomaly through Masked Encoding for Unsupervised Heterogeneity Mapping. (arXiv:2312.02762v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.02762</link>
<description rdf:parseType="Literal">&lt;p&gt;The detection of heterogeneous mental disorders based on brain readouts
remains challenging due to the complexity of symptoms and the absence of
reliable biomarkers. This paper introduces CAM (Cortical Anomaly Detection
through Masked Image Modeling), a novel self-supervised framework designed for
the unsupervised detection of complex brain disorders using cortical surface
features. We employ this framework for the detection of individuals on the
psychotic spectrum and demonstrate its capabilities compared to state-ofthe-art
methods, achieving an AUC of 0.696 for Schizoaffective and 0.769 for
Schizophreniform, without the need for any labels. Furthermore, the analysis of
atypical cortical regions includes Pars Triangularis and several frontal areas,
often implicated in schizophrenia, provide further confidence in our approach.
Altogether, we demonstrate a scalable approach for anomaly detection of complex
brain disorders based on cortical abnormalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao-Chun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Andreassen_O/0/1/0/all/0/1&quot;&gt;Ole Andreassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Westlye_L/0/1/0/all/0/1&quot;&gt;Lars Tjelta Westlye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marquand_A/0/1/0/all/0/1&quot;&gt;Andre F. Marquand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beckmann_C/0/1/0/all/0/1&quot;&gt;Christian F. Beckmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wolfers_T/0/1/0/all/0/1&quot;&gt;Thomas Wolfers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02772">
<title>Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions. (arXiv:2312.02772v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02772</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, significant progress has been made in text-based motion generation,
enabling the generation of diverse and high-quality human motions that conform
to textual descriptions. However, it remains challenging to generate
fine-grained or stylized motions due to the lack of datasets annotated with
detailed textual descriptions. By adopting a divide-and-conquer strategy, we
propose a new framework named Fine-Grained Human Motion Diffusion Model
(FG-MDM) for human motion generation. Specifically, we first parse previous
vague textual annotation into fine-grained description of different body parts
by leveraging a large language model (GPT-3.5). We then use these fine-grained
descriptions to guide a transformer-based diffusion model. FG-MDM can generate
fine-grained and stylized motions even outside of the distribution of the
training data. Our experimental results demonstrate the superiority of FG-MDM
over previous methods, especially the strong generalization capability. We will
release our fine-grained textual annotations for HumanML3D and KIT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chuanchen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Junran Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yunlian Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02781">
<title>PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features. (arXiv:2312.02781v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02781</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-driven 3D facial animation has improved a lot recently while most
related works only utilize acoustic modality and neglect the influence of
visual and textual cues, leading to unsatisfactory results in terms of
precision and coherence. We argue that visual and textual cues are not trivial
information. Therefore, we present a novel framework, namely PMMTalk, using
complementary Pseudo Multi-Modal features for improving the accuracy of facial
animation. The framework entails three modules: PMMTalk encoder, cross-modal
alignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder
employs the off-the-shelf talking head generation architecture and speech
recognition technology to extract visual and textual information from speech,
respectively. Subsequently, the cross-modal alignment module aligns the
audio-image-text features at temporal and semantic levels. Then PMMTalk decoder
is employed to predict lip-syncing facial blendshape coefficients. Contrary to
prior methods, PMMTalk only requires an additional random reference face image
but yields more accurate results. Additionally, it is artist-friendly as it
seamlessly integrates into standard animation production workflows by
introducing facial blendshape coefficients. Finally, given the scarcity of 3D
talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual
Facial Animation (3D-CAVFA) dataset. Extensive experiments and user studies
show that our approach outperforms the state of the art. We recommend watching
the supplementary video.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tianshun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1&quot;&gt;Shengnan Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiqing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baihui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Benjia Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Ning Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Quan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_R/0/1/0/all/0/1&quot;&gt;Ruicong Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yanyan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Du Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Jun Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02813">
<title>BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models. (arXiv:2312.02813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02813</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have made tremendous progress in text-driven image and video
generation. Now text-to-image foundation models are widely applied to various
downstream image synthesis tasks, such as controllable image generation and
image editing, while downstream video synthesis tasks are less explored for
several reasons. First, it requires huge memory and compute overhead to train a
video generation foundation model. Even with video foundation models,
additional costly training is still required for downstream video synthesis
tasks. Second, although some works extend image diffusion models into videos in
a training-free manner, temporal consistency cannot be well kept. Finally,
these adaption methods are specifically designed for one task and fail to
generalize to different downstream video synthesis tasks. To mitigate these
issues, we propose a training-free general-purpose video synthesis framework,
coined as BIVDiff, via bridging specific image diffusion models and general
text-to-video foundation diffusion models. Specifically, we first use an image
diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video
generation, then perform Mixed Inversion on the generated video, and finally
input the inverted latents into the video diffusion model for temporal
smoothing. Decoupling image and video models enables flexible image model
selection for different purposes, which endows the framework with strong task
generalization and high efficiency. To validate the effectiveness and general
use of BIVDiff, we perform a wide range of video generation tasks, including
controllable video generation video editing, video inpainting and outpainting.
Our project page is available at https://bivdiff.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1&quot;&gt;Fengyuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaxi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02819">
<title>Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting. (arXiv:2312.02819v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02819</link>
<description rdf:parseType="Literal">&lt;p&gt;Weather forecasting requires not only accuracy but also the ability to
perform probabilistic prediction. However, deterministic weather forecasting
methods do not support probabilistic predictions, and conversely, probabilistic
models tend to be less accurate. To address these challenges, in this paper, we
introduce the \textbf{\textit{D}}eterministic \textbf{\textit{G}}uidance
\textbf{\textit{D}}iffusion \textbf{\textit{M}}odel (DGDM) for probabilistic
weather forecasting, integrating benefits of both deterministic and
probabilistic approaches. During the forward process, both the deterministic
and probabilistic models are trained end-to-end. In the reverse process,
weather forecasting leverages the predicted result from the deterministic
model, using as an intermediate starting point for the probabilistic model. By
fusing deterministic models with probabilistic models in this manner, DGDM is
capable of providing accurate forecasts while also offering probabilistic
predictions. To evaluate DGDM, we assess it on the global weather forecasting
dataset (WeatherBench) and the common video frame prediction benchmark (Moving
MNIST). We also introduce and evaluate the Pacific Northwest Windstorm
(PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM in
high-resolution regional forecasting. As a result of our experiments, DGDM
achieves state-of-the-art results not only in global forecasting but also in
regional forecasting. The code is available at:
\url{https://github.com/DongGeun-Yoon/DGDM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1&quot;&gt;Donggeun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minseok Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Doyi Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yeji Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1&quot;&gt;Donghyeon Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02821">
<title>RotaTR: Detection Transformer for Dense and Rotated Object. (arXiv:2312.02821v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02821</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting the objects in dense and rotated scenes is a challenging task.
Recent works on this topic are mostly based on Faster RCNN or Retinanet. As
they are highly dependent on the pre-set dense anchors and the NMS operation,
the approach is indirect and suboptimal.The end-to-end DETR-based detectors
have achieved great success in horizontal object detection and many other areas
like segmentation, tracking, action recognition and etc.However, the DETR-based
detectors perform poorly on dense rotated target tasks and perform worse than
most modern CNN-based detectors. In this paper, we find the most significant
reason for the poor performance is that the original attention can not
accurately focus on the oriented targets. Accordingly, we propose Rotated
object detection TRansformer (RotaTR) as an extension of DETR to oriented
detection. Specifically, we design Rotation Sensitive deformable (RSDeform)
attention to enhance the DETR&apos;s ability to detect oriented targets. It is used
to build the feature alignment module and rotation-sensitive decoder for our
model. We test RotaTR on four challenging-oriented benchmarks. It shows a great
advantage in detecting dense and oriented objects compared to the original
DETR. It also achieves competitive results when compared to the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuke_Z/0/1/0/all/0/1&quot;&gt;Zhu Yuke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yumeng_R/0/1/0/all/0/1&quot;&gt;Ruan Yumeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_G/0/1/0/all/0/1&quot;&gt;Guo Sheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02843">
<title>Are Vision Transformers More Data Hungry Than Newborn Visual Systems?. (arXiv:2312.02843v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02843</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) are top performing models on many computer vision
benchmarks and can accurately predict human behavior on object recognition
tasks. However, researchers question the value of using ViTs as models of
biological learning because ViTs are thought to be more data hungry than
brains, with ViTs requiring more training data to reach similar levels of
performance. To test this assumption, we directly compared the learning
abilities of ViTs and animals, by performing parallel controlled rearing
experiments on ViTs and newborn chicks. We first raised chicks in impoverished
visual environments containing a single object, then simulated the training
data available in those environments by building virtual animal chambers in a
video game engine. We recorded the first-person images acquired by agents
moving through the virtual chambers and used those images to train self
supervised ViTs that leverage time as a teaching signal, akin to biological
visual systems. When ViTs were trained through the eyes of newborn chicks, the
ViTs solved the same view invariant object recognition tasks as the chicks.
Thus, ViTs were not more data hungry than newborn visual systems: both learned
view invariant object representations in impoverished visual environments. The
flexible and generic attention based learning mechanism in ViTs combined with
the embodied data streams available to newborn animals appears sufficient to
drive the development of animal-like object recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_L/0/1/0/all/0/1&quot;&gt;Lalit Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_S/0/1/0/all/0/1&quot;&gt;Samantha M. W. Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_J/0/1/0/all/0/1&quot;&gt;Justin N. Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02877">
<title>A Dynamic Network for Efficient Point Cloud Registration. (arXiv:2312.02877v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02877</link>
<description rdf:parseType="Literal">&lt;p&gt;For the point cloud registration task, a significant challenge arises from
non-overlapping points that consume extensive computational resources while
negatively affecting registration accuracy. In this paper, we introduce a
dynamic approach, widely utilized to improve network efficiency in computer
vision tasks, to the point cloud registration task. We employ an iterative
registration process on point cloud data multiple times to identify regions
where matching points cluster, ultimately enabling us to remove noisy points.
Specifically, we begin with deep global sampling to perform coarse global
registration. Subsequently, we employ the proposed refined node proposal module
to further narrow down the registration region and perform local registration.
Furthermore, we utilize a spatial consistency-based classifier to evaluate the
results of each registration stage. The model terminates once it reaches
sufficient confidence, avoiding unnecessary computations. Extended experiments
demonstrate that our model significantly reduces time consumption compared to
other methods with similar results, achieving a speed improvement of over 41%
on indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) while
maintaining competitive registration recall requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_Y/0/1/0/all/0/1&quot;&gt;Yang Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02878">
<title>Towards More Practical Group Activity Detection: A New Benchmark and Model. (arXiv:2312.02878v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.02878</link>
<description rdf:parseType="Literal">&lt;p&gt;Group activity detection (GAD) is the task of identifying members of each
group and classifying the activity of the group at the same time in a video.
While GAD has been studied recently, there is still much room for improvement
in both dataset and methodology due to their limited capability to address
practical GAD scenarios. To resolve these issues, we first present a new
dataset, dubbed Caf\&apos;e. Unlike existing datasets, Caf\&apos;e is constructed
primarily for GAD and presents more practical evaluation scenarios and metrics,
as well as being large-scale and providing rich annotations. Along with the
dataset, we propose a new GAD model that deals with an unknown number of groups
and latent group members efficiently and effectively. We evaluated our model on
three datasets including Caf\&apos;e, where it outperformed previous work in terms
of both accuracy and inference speed. Both our dataset and code base will be
open to the public to promote future research on GAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongkeun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Youngkil Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsu Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Suha Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.06467">
<title>NeuroMixGDP: A Neural Collapse-Inspired Random Mixup for Private Data Release. (arXiv:2202.06467v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.06467</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy-preserving data release algorithms have gained increasing attention
for their ability to protect user privacy while enabling downstream machine
learning tasks. However, the utility of current popular algorithms is not
always satisfactory. Mixup of raw data provides a new way of data augmentation,
which can help improve utility. However, its performance drastically
deteriorates when differential privacy (DP) noise is added. To address this
issue, this paper draws inspiration from the recently observed Neural Collapse
(NC) phenomenon, which states that the last layer features of a neural network
concentrate on the vertices of a simplex as Equiangular Tight Frame (ETF). We
propose a scheme to mixup the Neural Collapse features to exploit the ETF
simplex structure and release noisy mixed features to enhance the utility of
the released data. By using Gaussian Differential Privacy (GDP), we obtain an
asymptotic rate for the optimal mixup degree. To further enhance the utility
and address the label collapse issue when the mixup degree is large, we propose
a Hierarchical sampling method to stratify the mixup samples on a small number
of classes. This method remarkably improves utility when the number of classes
is large. Extensive experiments demonstrate the effectiveness of our proposed
method in protecting against attacks and improving utility. In particular, our
approach shows significantly improved utility compared to directly training
classification networks with DPSGD on CIFAR100 and MiniImagenet datasets,
highlighting the benefits of using privacy-preserving data release. We release
reproducible code in https://github.com/Lidonghao1996/NeuroMixGDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Donghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03408">
<title>HRCTCov19 -- A High-Resolution Chest CT Scan Image Dataset for COVID-19 Diagnosis and Differentiation. (arXiv:2205.03408v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03408</link>
<description rdf:parseType="Literal">&lt;p&gt;Introduction: During the COVID-19 pandemic, computed tomography (CT) was a
popular method for diagnosing COVID-19 patients. HRCT (High-Resolution Computed
Tomography) is a form of computed tomography that uses advanced methods to
improve image resolution. Publicly accessible COVID-19 CT image datasets are
very difficult to come by due to privacy concerns, which impedes the study and
development of AI-powered COVID-19 diagnostic algorithms based on CT images.
Data description: To address this problem, we have introduced HRCTCov19, a new
COVID-19 high-resolution chest CT scan image dataset that includes not only
COVID-19 cases of Ground Glass Opacity (GGO), Crazy Paving, and Air Space
Consolidation but also CT images of cases with negative COVID-19. The HRCTCov19
dataset, which includes slice-level, and patient-level labels, has the
potential to aid COVID-19 research, especially for diagnosis and
differentiation using artificial intelligence algorithms, machine learning, and
deep learning methods. This dataset is accessible through the web at:
&lt;a href=&quot;http://databiox.com&quot;&gt;this http URL&lt;/a&gt; and includes 181,106 chest HRCT images from 395 patients
with four labels: GGO, Crazy Paving, Air Space Consolidation, and Negative.
Keywords: COVID-19, CT scan, Computed Tomography, Chest Image, Dataset, Medical
Imaging
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abedi_I/0/1/0/all/0/1&quot;&gt;Iraj Abedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vali_M/0/1/0/all/0/1&quot;&gt;Mahsa Vali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Otroshi_B/0/1/0/all/0/1&quot;&gt;Bentolhoda Otroshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zamanian_M/0/1/0/all/0/1&quot;&gt;Maryam Zamanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bolhasani_H/0/1/0/all/0/1&quot;&gt;Hamidreza Bolhasani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06561">
<title>Finding Point with Image: A Simple and Efficient Method for UAV Self-Localization. (arXiv:2208.06561v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06561</link>
<description rdf:parseType="Literal">&lt;p&gt;Image retrieval has emerged as a prominent solution for the self-localization
task of unmanned aerial vehicles (UAVs). However, this approach involves
complicated pre-processing and post-processing operations, placing significant
demands on both computational and storage resources. To mitigate this issue,
this paper presents an end-to-end positioning framework, namely Finding Point
with Image (FPI), which aims to directly identify the corresponding location of
a UAV in satellite-view images via a UAV-view image. To validate the
practicality of our framework, we construct a paired dataset, namely UL14, that
consists of UAV and satellite views. In addition, we establish two
transformer-based baseline models, Post Fusion and Mix Fusion, for end-to-end
training and inference. Through experiments, we can conclude that fusion in the
backbone network can achieve better performance than later fusion. Furthermore,
considering the singleness of paired images, Random Scale Crop (RSC) is
proposed to enrich the diversity of the paired data. Also, the ratio and weight
of positive and negative samples play a key role in model convergence.
Therefore, we conducted experimental verification and proposed a Weight Balance
Loss (WBL) to weigh the impact of positive and negative samples. Last, our
proposed baseline based on Mix Fusion structure exhibits superior performance
in time and storage efficiency, amounting to just 1/24 and 1/68, respectively,
while delivering comparable or even superior performance compared to the image
retrieval method. The dataset and code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1&quot;&gt;Ming Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_E/0/1/0/all/0/1&quot;&gt;Enhui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiahao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wankou Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00383">
<title>TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut. (arXiv:2209.00383v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00383</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we describe a graph-based algorithm that uses the features
obtained by a self-supervised transformer to detect and segment salient objects
in images and videos. With this approach, the image patches that compose an
image or video are organised into a fully connected graph, where the edge
between each pair of patches is labeled with a similarity score between patches
using features learned by the transformer. Detection and segmentation of
salient objects is then formulated as a graph-cut problem and solved using the
classical Normalized Cut algorithm. Despite the simplicity of this approach, it
achieves state-of-the-art results on several common image and video detection
and segmentation tasks. For unsupervised object discovery, this approach
outperforms the competing approaches by a margin of 6.1%, 5.7%, and 2.6%,
respectively, when tested with the VOC07, VOC12, and COCO20K datasets. For the
unsupervised saliency detection task in images, this method improves the score
for Intersection over Union (IoU) by 4.4%, 5.6% and 5.2%. When tested with the
ECSSD, DUTS, and DUT-OMRON datasets, respectively, compared to current
state-of-the-art techniques. This method also achieves competitive results for
unsupervised video object segmentation tasks with the DAVIS, SegTV2, and FBMS
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yangtao Wang&lt;/a&gt; (M-PSI), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt; (MIT CSAIL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuming Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Maomao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shell Xu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowley_J/0/1/0/all/0/1&quot;&gt;James L Crowley&lt;/a&gt; (M-PSI), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1&quot;&gt;Dominique Vaufreydaz&lt;/a&gt; (M-PSI)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13177">
<title>Fairness in Medical Image Analysis and Healthcare: A Literature Survey. (arXiv:2209.13177v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13177</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning-enabled medical imaging analysis has become a vital part of
the automatic diagnosis system. However, machine learning, especially deep
learning models have been shown to demonstrate a systematic bias towards
certain subgroups of people. For instance, they yield a preferential predictive
performance to males over females, which is unfair and potentially harmful
especially in healthcare scenarios. In this literature survey, we give a
comprehensive review of the current progress of fairness studies in medical
image analysis (MedIA) and healthcare. Specifically, we first discuss the
definitions of fairness, the source of unfairness and potential solutions.
Then, we discuss current research on fairness for MedIA categorized by fairness
evaluation and unfairness mitigation. Furthermore, we conduct extensive
experiments to evaluate the fairness of different medical imaging tasks.
Finally, we discuss the challenges and future directions in developing fair
MedIA and healthcare applications
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zikang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13838">
<title>Signed Binary Weight Networks. (arXiv:2211.13838v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13838</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient inference of Deep Neural Networks (DNNs) is essential to making AI
ubiquitous. Two important algorithmic techniques have shown promise for
enabling efficient inference - sparsity and binarization. These techniques
translate into weight sparsity and weight repetition at the hardware-software
level enabling the deployment of DNNs with critically low power and latency
requirements. We propose a new method called signed-binary networks to improve
efficiency further (by exploiting both weight sparsity and weight repetition
together) while maintaining similar accuracy. Our method achieves comparable
accuracy on ImageNet and CIFAR10 datasets with binary and can lead to 69%
sparsity. We observe real speedup when deploying these models on
general-purpose devices and show that this high percentage of unstructured
sparsity can lead to a further reduction in energy consumption on ASICs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhar_S/0/1/0/all/0/1&quot;&gt;Sachit Kuhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tumanov_A/0/1/0/all/0/1&quot;&gt;Alexey Tumanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03573">
<title>Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction. (arXiv:2301.03573v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03573</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite impressive performance, deep neural networks require significant
memory and computation costs, prohibiting their application in
resource-constrained scenarios. Sparse training is one of the most common
techniques to reduce these costs, however, the sparsity constraints add
difficulty to the optimization, resulting in an increase in training time and
instability. In this work, we aim to overcome this problem and achieve
space-time co-efficiency. To accelerate and stabilize the convergence of sparse
training, we analyze the gradient changes and develop an adaptive gradient
correction method. Specifically, we approximate the correlation between the
current and previous gradients, which is used to balance the two gradients to
obtain a corrected gradient. Our method can be used with the most popular
sparse training pipelines under both standard and adversarial setups.
Theoretically, we prove that our method can accelerate the convergence rate of
sparse training. Extensive experiments on multiple datasets, model
architectures, and sparsities demonstrate that our method outperforms leading
sparse training methods by up to \textbf{5.0\%} in accuracy given the same
number of training epochs, and reduces the number of training epochs by up to
\textbf{52.1\%} to achieve the same accuracy. Our code is available on:
\url{https://github.com/StevenBoys/AGENT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1&quot;&gt;Bowen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dongkuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shuren He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallick_B/0/1/0/all/0/1&quot;&gt;Bani K. Mallick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02367">
<title>FastPillars: A Deployment-friendly Pillar-based 3D Detector. (arXiv:2302.02367v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02367</link>
<description rdf:parseType="Literal">&lt;p&gt;The deployment of 3D detectors strikes one of the major challenges in
real-world self-driving scenarios. Existing BEV-based (i.e., Bird Eye View)
detectors favor sparse convolutions (known as SPConv) to speed up training and
inference, which puts a hard barrier for deployment, especially for on-device
applications. In this paper, to tackle the challenge of efficient 3D object
detection from an industry perspective, we devise a deployment-friendly
pillar-based 3D detector, termed FastPillars. First, we introduce a novel
lightweight Max-and-Attention Pillar Encoding (MAPE) module specially for
enhancing small 3D objects. Second, we propose a simple yet effective principle
for designing a backbone in pillar-based 3D detection. We construct FastPillars
based on these designs, achieving high performance and low latency without
SPConv. Extensive experiments on two large-scale datasets demonstrate the
effectiveness and efficiency of FastPillars for on-device 3D detection
regarding both performance and speed. Specifically, FastPillars delivers
state-of-the-art accuracy on Waymo Open Dataset with 1.8X speed up and 3.8
mAPH/L2 improvement over CenterPoint (SPConv-based). Our code is publicly
available at: https://github.com/StiphyJay/FastPillars.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sifan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chengjian Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1&quot;&gt;Zequn Jie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1&quot;&gt;Patrick Yin Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02858">
<title>TR3D: Towards Real-Time Indoor 3D Object Detection. (arXiv:2302.02858v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02858</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, sparse 3D convolutions have changed 3D object detection. Performing
on par with the voting-based approaches, 3D CNNs are memory-efficient and scale
to large scenes better. However, there is still room for improvement. With a
conscious, practice-oriented approach to problem-solving, we analyze the
performance of such methods and localize the weaknesses. Applying modifications
that resolve the found issues one by one, we end up with TR3D: a fast
fully-convolutional 3D object detection model trained end-to-end, that achieves
state-of-the-art results on the standard benchmarks, ScanNet v2, SUN RGB-D, and
S3DIS. Moreover, to take advantage of both point cloud and RGB inputs, we
introduce an early fusion of 2D and 3D features. We employ our fusion module to
make conventional 3D object detection methods multimodal and demonstrate an
impressive boost in performance. Our model with early feature fusion, which we
refer to as TR3D+FF, outperforms existing 3D object detection approaches on the
SUN RGB-D dataset. Overall, besides being accurate, both TR3D and TR3D+FF
models are lightweight, memory-efficient, and fast, thereby marking another
milestone on the way toward real-time 3D object detection. Code is available at
https://github.com/SamsungLabs/tr3d .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rukhovich_D/0/1/0/all/0/1&quot;&gt;Danila Rukhovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorontsova_A/0/1/0/all/0/1&quot;&gt;Anna Vorontsova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konushin_A/0/1/0/all/0/1&quot;&gt;Anton Konushin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06353">
<title>Contour-based Interactive Segmentation. (arXiv:2302.06353v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06353</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in interactive segmentation (IS) allow speeding up and
simplifying image editing and labeling greatly. The majority of modern IS
approaches accept user input in the form of clicks. However, using clicks may
require too many user interactions, especially when selecting small objects,
minor parts of an object, or a group of objects of the same type. In this
paper, we consider such a natural form of user interaction as a loose contour,
and introduce a contour-based IS method. We evaluate the proposed method on the
standard segmentation benchmarks, our novel UserContours dataset, and its
subset UserContours-G containing difficult segmentation cases. Through
experiments, we demonstrate that a single contour provides the same accuracy as
multiple clicks, thus reducing the required amount of user interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galeev_D/0/1/0/all/0/1&quot;&gt;Danil Galeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popenova_P/0/1/0/all/0/1&quot;&gt;Polina Popenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorontsova_A/0/1/0/all/0/1&quot;&gt;Anna Vorontsova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konushin_A/0/1/0/all/0/1&quot;&gt;Anton Konushin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08272">
<title>Revisiting Hidden Representations in Transfer Learning for Medical Imaging. (arXiv:2302.08272v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08272</link>
<description rdf:parseType="Literal">&lt;p&gt;While a key component to the success of deep learning is the availability of
massive amounts of training data, medical image datasets are often limited in
diversity and size. Transfer learning has the potential to bridge the gap
between related yet different domains. For medical applications, however, it
remains unclear whether it is more beneficial to pre-train on natural or
medical images. We aim to shed light on this problem by comparing
initialization on ImageNet and RadImageNet on seven medical classification
tasks. Our work includes a replication study, which yields results contrary to
previously published findings. In our experiments, ResNet50 models pre-trained
on ImageNet tend to outperform those trained on RadImageNet. To gain further
insights, we investigate the learned representations using Canonical
Correlation Analysis (CCA) and compare the predictions of the different models.
Our results indicate that, contrary to intuition, ImageNet and RadImageNet may
converge to distinct intermediate representations, which appear to diverge
further during fine-tuning. Despite these distinct representations, the
predictions of the models remain similar. Our findings show that the similarity
between networks before and after fine-tuning does not correlate with
performance gains, suggesting that the advantages of transfer learning might
not solely originate from the reuse of features in the early layers of a
convolutional neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juodelyte_D/0/1/0/all/0/1&quot;&gt;Dovile Juodelyte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Sanchez_A/0/1/0/all/0/1&quot;&gt;Amelia Jim&amp;#xe9;nez-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1&quot;&gt;Veronika Cheplygina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07527">
<title>Domain Generalization via Nuclear Norm Regularization. (arXiv:2303.07527v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07527</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to generalize to unseen domains is crucial for machine learning
systems deployed in the real world, especially when we only have data from
limited training domains. In this paper, we propose a simple and effective
regularization method based on the nuclear norm of the learned features for
domain generalization. Intuitively, the proposed regularizer mitigates the
impacts of environmental features and encourages learning domain-invariant
features. Theoretically, we provide insights into why nuclear norm
regularization is more effective compared to ERM and alternative regularization
methods. Empirically, we conduct extensive experiments on both synthetic and
real datasets. We show nuclear norm regularization achieves strong performance
compared to baselines in a wide range of domain generalization tasks. Moreover,
our regularizer is broadly applicable with various methods such as ERM and SWAD
with consistently improved performance, e.g., 1.7% and 0.9% test accuracy
improvements respectively on the DomainBed benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenmei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1&quot;&gt;Yifei Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Ying Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16894">
<title>ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16894</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding 3D scenes from multi-view inputs has been proven to alleviate
the view discrepancy issue in 3D visual grounding. However, existing methods
normally neglect the view cues embedded in the text modality and fail to weigh
the relative importance of different views. In this paper, we propose
ViewRefer, a multi-view framework for 3D visual grounding exploring how to
grasp the view knowledge from both text and 3D modalities. For the text branch,
ViewRefer leverages the diverse linguistic knowledge of large-scale language
models, e.g., GPT, to expand a single grounding text to multiple
geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer
fusion module with inter-view attention is introduced to boost the interaction
of objects across views. On top of that, we further present a set of learnable
multi-view prototypes, which memorize scene-agnostic knowledge for different
views, and enhance the framework from two perspectives: a view-guided attention
module for more robust text features, and a view-guided scoring strategy during
the final prediction. With our designed paradigm, ViewRefer achieves superior
performance on three benchmarks and surpasses the second-best by +2.8%, +1.5%,
and +1.35% on Sr3D, Nr3D, and ScanRefer. Code is released at
https://github.com/Ivan-Tang-3D/ViewRefer3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zoey Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yiwen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ray Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06059">
<title>Efficient Deep Learning Models for Privacy-preserving People Counting on Low-resolution Infrared Arrays. (arXiv:2304.06059v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06059</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultra-low-resolution Infrared (IR) array sensors offer a low-cost,
energy-efficient, and privacy-preserving solution for people counting, with
applications such as occupancy monitoring. Previous work has shown that Deep
Learning (DL) can yield superior performance on this task. However, the
literature was missing an extensive comparative analysis of various efficient
DL architectures for IR array-based people counting, that considers not only
their accuracy, but also the cost of deploying them on memory- and
energy-constrained Internet of Things (IoT) edge nodes. In this work, we
address this need by comparing 6 different DL architectures on a novel dataset
composed of IR images collected from a commercial 8x8 array, which we made
openly available. With a wide architectural exploration of each model type, we
obtain a rich set of Pareto-optimal solutions, spanning cross-validated
balanced accuracy scores in the 55.70-82.70% range. When deployed on a
commercial Microcontroller (MCU) by STMicroelectronics, the STM32L4A6ZG, these
models occupy 0.41-9.28kB of memory, and require 1.10-7.74ms per inference,
while consuming 17.18-120.43 $\mu$J of energy. Our models are significantly
more accurate than a previous deterministic method (up to +39.9%), while being
up to 3.53x faster and more energy efficient. Further, our models&apos; accuracy is
comparable to state-of-the-art DL solutions on similar resolution sensors,
despite a much lower complexity. All our models enable continuous, real-time
inference on a MCU-based IoT node, with years of autonomous operation without
battery recharging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daghero_F/0/1/0/all/0/1&quot;&gt;Francesco Daghero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castellano_M/0/1/0/all/0/1&quot;&gt;Marco Castellano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandolfi_L/0/1/0/all/0/1&quot;&gt;Luca Gandolfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calimera_A/0/1/0/all/0/1&quot;&gt;Andrea Calimera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1&quot;&gt;Enrico Macii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poncino_M/0/1/0/all/0/1&quot;&gt;Massimo Poncino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1&quot;&gt;Daniele Jahier Pagliari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06470">
<title>Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06470</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability of image and video generation models to create photorealistic
images has reached unprecedented heights, making it difficult to distinguish
between real and fake images in many cases. However, despite this progress, a
gap remains between the quality of generated images and those found in the real
world. To address this, we have reviewed a vast body of literature from both
academic publications and social media to identify qualitative shortcomings in
image generation models, which we have classified into five categories. By
understanding these failures, we can identify areas where these models need
improvement, as well as develop strategies for detecting deep fakes. The
prevalence of deep fakes in today&apos;s society is a serious concern, and our
findings can help mitigate their negative impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1&quot;&gt;Ali Borji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08956">
<title>PG-VTON: A Novel Image-Based Virtual Try-On Method via Progressive Inference Paradigm. (arXiv:2304.08956v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08956</link>
<description rdf:parseType="Literal">&lt;p&gt;Virtual try-on is a promising computer vision topic with a high commercial
value wherein a new garment is visually worn on a person with a photo-realistic
effect. Previous studies conduct their shape and content inference at one
stage, employing a single-scale warping mechanism and a relatively
unsophisticated content inference mechanism. These approaches have led to
suboptimal results in terms of garment warping and skin reservation under
challenging try-on scenarios. To address these limitations, we propose a novel
virtual try-on method via progressive inference paradigm (PGVTON) that
leverages a top-down inference pipeline and a general garment try-on strategy.
Specifically, we propose a robust try-on parsing inference method by
disentangling semantic categories and introducing consistency. Exploiting the
try-on parsing as the shape guidance, we implement the garment try-on via
warping-mapping-composition. To facilitate adaptation to a wide range of try-on
scenarios, we adopt a covering more and selecting one warping strategy and
explicitly distinguish tasks based on alignment. Additionally, we regulate
StyleGAN2 to implement re-naked skin inpainting, conditioned on the target skin
shape and spatial-agnostic skin features. Experiments demonstrate that our
method has state-of-the-art performance under two challenging scenarios. The
code will be available at https://github.com/NerdFNY/PGVTON.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_N/0/1/0/all/0/1&quot;&gt;Naiyu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lemiao Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuyou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zili Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kerui Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13277">
<title>U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical Satellite Time Series. (arXiv:2305.13277v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13277</link>
<description rdf:parseType="Literal">&lt;p&gt;Satellite image time series in the optical and infrared spectrum suffer from
frequent data gaps due to cloud cover, cloud shadows, and temporary sensor
outages. It has been a long-standing problem of remote sensing research how to
best reconstruct the missing pixel values and obtain complete, cloud-free image
sequences. We approach that problem from the perspective of representation
learning and develop U-TILISE, an efficient neural model that is able to
implicitly capture spatio-temporal patterns of the spectral intensities, and
that can therefore be trained to map a cloud-masked input sequence to a
cloud-free output sequence. The model consists of a convolutional spatial
encoder that maps each individual frame of the input sequence to a latent
encoding; an attention-based temporal encoder that captures dependencies
between those per-frame encodings and lets them exchange information along the
time dimension; and a convolutional spatial decoder that decodes the latent
embeddings back into multi-spectral images. We experimentally evaluate the
proposed model on EarthNet2021, a dataset of Sentinel-2 time series acquired
all over Europe, and demonstrate its superior ability to reconstruct the
missing pixels. Compared to a standard interpolation baseline, it increases the
PSNR by 1.8 dB at previously seen locations and by 1.3 dB at unseen locations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1&quot;&gt;Corinne Stucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1&quot;&gt;Vivien Sainte Fare Garnot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15583">
<title>Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps. (arXiv:2305.15583v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15583</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the
synthesis of high-quality images. However, their inference process
characteristically requires numerous, potentially hundreds, of iterative steps,
which could exaggerate the problem of exposure bias due to the training and
inference discrepancy. Previous work has attempted to mitigate this issue by
perturbing inputs during training, which consequently mandates the retraining
of the DPM. In this work, we conduct a systematic study of exposure bias in DPM
and, intriguingly, we find that the exposure bias could be alleviated with a
novel sampling method that we propose, without retraining the model. We
empirically and theoretically show that, during inference, for each backward
time step $t$ and corresponding state $\hat{x}_t$, there might exist another
time step $t_s$ which exhibits superior coupling with $\hat{x}_t$. Based on
this finding, we introduce a sampling method named Time-Shift Sampler. Our
framework can be seamlessly integrated to existing sampling algorithms, such as
DDPM, DDIM and other high-order solvers, inducing merely minimal additional
computations. Experimental results show our method brings significant and
consistent improvements in FID scores on different datasets and sampling
methods. For example, integrating Time-Shift Sampler to F-PNDM yields a
FID=3.88, achieving 44.49\% improvements as compared to F-PNDM, on CIFAR-10
with 10 sampling steps, which is more performant than the vanilla DDIM with 100
sampling steps. We will release the code upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_T/0/1/0/all/0/1&quot;&gt;Tingyu Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1&quot;&gt;Ruicong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1&quot;&gt;Marie-Francine Moens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15956">
<title>Anomaly Detection with Conditioned Denoising Diffusion Models. (arXiv:2305.15956v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15956</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional reconstruction-based methods have struggled to achieve
competitive performance in anomaly detection. In this paper, we introduce
Denoising Diffusion Anomaly Detection (DDAD), a novel denoising process for
image reconstruction conditioned on a target image. This ensures a coherent
restoration that closely resembles the target image. Our anomaly detection
framework employs the conditioning mechanism, where the target image is set as
the input image to guide the denoising process, leading to a defectless
reconstruction while maintaining nominal patterns. Anomalies are then localised
via a pixel-wise and feature-wise comparison of the input and reconstructed
image. Finally, to enhance the effectiveness of the feature-wise comparison, we
introduce a domain adaptation method that utilises nearly identical generated
examples from our conditioned denoising process to fine-tune the pretrained
feature extractor. The veracity of DDAD is demonstrated on various datasets
including MVTec and VisA benchmarks, achieving state-of-the-art results of
\(99.8 \%\) and \(98.9 \%\) image-level AUROC respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousakhan_A/0/1/0/all/0/1&quot;&gt;Arian Mousakhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayyub_J/0/1/0/all/0/1&quot;&gt;Jawad Tayyub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16450">
<title>Investigation of UAV Detection in Images with Complex Backgrounds and Rainy Artifacts. (arXiv:2305.16450v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16450</link>
<description rdf:parseType="Literal">&lt;p&gt;To detect unmanned aerial vehicles (UAVs) in real-time, computer vision and
deep learning approaches are evolving research areas. Interest in this problem
has grown due to concerns regarding the possible hazards and misuse of
employing UAVs in many applications. These include potential privacy
violations. To address the concerns, vision-based object detection methods have
been developed for UAV detection. However, UAV detection in images with complex
backgrounds and weather artifacts like rain has yet to be reasonably studied.
Hence, for this purpose, we prepared two training datasets. The first dataset
has the sky as its background and is called the Sky Background Dataset (SBD).
The second training dataset has more complex scenes (with diverse backgrounds)
and is named the Complex Background Dataset (CBD). Additionally, two test sets
were prepared: one containing clear images and the other with images with three
rain artifacts, named the Rainy Test Set (RTS). This work also focuses on
benchmarking state-of-the-art object detection models, and to the best of our
knowledge, it is the first to investigate the performance of recent and popular
vision-based object detection methods for UAV detection under challenging
conditions such as complex backgrounds, varying UAV sizes, and low-to-heavy
rainy conditions. The findings presented in the paper shall help provide
insights concerning the performance of the selected models for UAV detection
under challenging conditions and pave the way to develop more robust UAV
detection methods. The codes and datasets are available at:
https://github.com/AdnanMunir294/UAVD-CBRA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munir_A/0/1/0/all/0/1&quot;&gt;Adnan Munir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiqui_A/0/1/0/all/0/1&quot;&gt;Abdul Jabbar Siddiqui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1&quot;&gt;Saeed Anwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19798">
<title>Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19798</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a new line of works has emerged to understand and improve
self-attention in Transformers by treating it as a kernel machine. However,
existing works apply the methods for symmetric kernels to the asymmetric
self-attention, resulting in a nontrivial gap between the analytical
understanding and numerical implementation. In this paper, we provide a new
perspective to represent and optimize self-attention through asymmetric Kernel
Singular Value Decomposition (KSVD), which is also motivated by the low-rank
property of self-attention normally observed in deep layers. Through asymmetric
KSVD, $i$) a primal-dual representation of self-attention is formulated, where
the optimization objective is cast to maximize the projection variances in the
attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention,
is proposed via the primal representation of KSVD, avoiding explicit
computation of the kernel matrix in the dual; $iii$) with KKT conditions, we
prove that the stationary solution to the KSVD optimization in Primal-Attention
yields a zero-value objective. In this manner, KSVD optimization can be
implemented by simply minimizing a regularization loss, so that low-rank
property is promoted without extra decomposition. Numerical experiments show
state-of-the-art performance of our Primal-Attention with improved efficiency.
Moreover, we demonstrate that the deployed KSVD optimization regularizes
Primal-Attention with a sharper singular value decay than that of the canonical
self-attention, further verifying the great potential of our method. To the
best of our knowledge, this is the first work that provides a primal-dual
representation for the asymmetric kernel in self-attention and successfully
applies it to modeling and optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1&quot;&gt;Qinghua Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonin_F/0/1/0/all/0/1&quot;&gt;Francesco Tonin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1&quot;&gt;Johan A.K. Suykens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04037">
<title>Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04037</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a comprehensive analysis of quantitatively evaluating explainable
artificial intelligence (XAI) techniques for remote sensing image
classification. Our approach leverages state-of-the-art machine learning
approaches to perform remote sensing image classification across multiple
modalities. We investigate the results of the models qualitatively through XAI
methods. Additionally, we compare the XAI methods quantitatively through
various categories of desired properties. Through our analysis, we offer
insights and recommendations for selecting the most appropriate XAI method(s)
to gain a deeper understanding of the models&apos; decision-making processes. The
code for this work is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1&quot;&gt;Akshatha Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1&quot;&gt;Joshua Peeples&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05427">
<title>Grounded Text-to-Image Synthesis with Attention Refocusing. (arXiv:2306.05427v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05427</link>
<description rdf:parseType="Literal">&lt;p&gt;Driven by the scalable diffusion models trained on large-scale datasets,
text-to-image synthesis methods have shown compelling results. However, these
models still fail to precisely follow the text prompt involving multiple
objects, attributes, or spatial compositions. In this paper, we reveal the
potential causes in the diffusion model&apos;s cross-attention and self-attention
layers. We propose two novel losses to refocus attention maps according to a
given spatial layout during sampling. Creating the layouts manually requires
additional effort and can be tedious. Therefore, we explore using large
language models (LLM) to produce these layouts for our method. We conduct
extensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate
our proposed method. We show that our proposed attention refocusing effectively
improves the controllability of existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phung_Q/0/1/0/all/0/1&quot;&gt;Quynh Phung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Songwei Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia-Bin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10191">
<title>Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10191</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Neural Priming, a technique for adapting large pretrained models
to distribution shifts and downstream tasks given few or no labeled examples.
Presented with class names or unlabeled test samples, Neural Priming enables
the model to recall and conditions its parameters on relevant data seen
throughout pretraining, thereby priming it for the test distribution. Neural
Priming can be performed at test time, even for pretraining datasets as large
as LAION-2B. Performing lightweight updates on the recalled data significantly
improves accuracy across a variety of distribution shift and transfer learning
benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in
accuracy on ImageNet and 3.81% accuracy improvement on average across standard
transfer learning benchmarks. Further, using Neural Priming at inference to
adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2.
These results demonstrate the effectiveness of Neural Priming in addressing the
challenge of limited labeled data and changing distributions. Code is available
at github.com/RAIVNLab/neural-priming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1&quot;&gt;Matthew Wallingford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1&quot;&gt;Vivek Ramanujan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1&quot;&gt;Alex Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1&quot;&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1&quot;&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1&quot;&gt;Aniruddha Kembhavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11300">
<title>RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11300</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zilun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianwei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17046">
<title>Spiking Denoising Diffusion Probabilistic Models. (arXiv:2306.17046v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17046</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) have ultra-low energy consumption and high
biological plausibility due to their binary and bio-driven nature compared with
artificial neural networks (ANNs). While previous research has primarily
focused on enhancing the performance of SNNs in classification tasks, the
generative potential of SNNs remains relatively unexplored. In our paper, we
put forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new
class of SNN-based generative models that achieve high sample quality. To fully
exploit the energy efficiency of SNNs, we propose a purely Spiking U-Net
architecture, which achieves comparable performance to its ANN counterpart
using only 4 time steps, resulting in significantly reduced energy consumption.
Extensive experimental results reveal that our approach achieves
state-of-the-art on the generative tasks and substantially outperforms other
SNN-based generative models, achieving up to 12x and 6x improvement on the
CIFAR-10 and the CelebA datasets, respectively. Moreover, we propose a
threshold-guided strategy that can further improve the performances by 2.69% in
a training-free manner. The SDDPM symbolizes a significant advancement in the
field of SNN generation, injecting new perspectives and potential avenues of
exploration. Our code is available at https://github.com/AndyCao1125/SDDPM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiahang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hanzhong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00347">
<title>Spatial-Temporal Enhanced Transformer Towards Multi-Frame 3D Object Detection. (arXiv:2307.00347v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00347</link>
<description rdf:parseType="Literal">&lt;p&gt;The Detection Transformer (DETR) has revolutionized the design of CNN-based
object detection systems, showcasing impressive performance. However, its
potential in the domain of multi-frame 3D object detection remains largely
unexplored. In this paper, we present STEMD, a novel end-to-end framework for
multi-frame 3D object detection based on the DETR-like paradigm. STEMD treats
multi-frame 3D object detection as a sequence-to-sequence task and effectively
captures spatial-temporal dependencies at both the feature and query levels.
Specifically, to model the inter-object spatial interaction and complex
temporal dependencies, we introduce the spatial-temporal graph attention
network, which represents queries as nodes in a graph and enables effective
modeling of object interactions within a social context. To solve the problem
of missing hard cases in the proposed output of the encoder in the current
frame, we incorporate the output of the previous frame to initialize the query
input of the decoder. Moreover, to mitigate the issue of redundant detection
results, where the model generates numerous overlapping boxes from similar
queries, we consider an IoU regularization term in the loss function, which can
distinguish between queries matched with the ground-truth box and queries that
are similar but unmatched during the refinement process, leading to reduced
redundancy and more accurate detections. Through extensive experiments, we
demonstrate the effectiveness of our approach in handling challenging
scenarios, while incurring only a minor additional computational overhead. The
code is available at \url{https://github.com/Eaphan/STEMD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dapeng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00371">
<title>Learning Content-enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation. (arXiv:2307.00371v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00371</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain-generalized urban-scene semantic segmentation (USSS) aims to learn
generalized semantic predictions across diverse urban-scene styles. Unlike
domain gap challenges, USSS is unique in that the semantic categories are often
similar in different urban scenes, while the styles can vary significantly due
to changes in urban landscapes, weather conditions, lighting, and other
factors. Existing approaches typically rely on convolutional neural networks
(CNNs) to learn the content of urban scenes.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for
domain-generalized USSS. The main idea is to enhance the focus of the
fundamental component, the mask attention mechanism, in Transformer
segmentation models on content information. To achieve this, we introduce a
novel content-enhanced mask attention mechanism. It learns mask queries from
both the image feature and its down-sampled counterpart, as lower-resolution
image features usually contain more robust content information and are less
sensitive to style variations. These features are fused into a Transformer
decoder and integrated into a multi-resolution content-enhanced mask attention
learning scheme.
&lt;/p&gt;
&lt;p&gt;Extensive experiments conducted on various domain-generalized urban-scene
segmentation datasets demonstrate that the proposed CMFormer significantly
outperforms existing CNN-based methods for domain-generalized semantic
segmentation, achieving improvements of up to 14.00\% in terms of mIoU (mean
intersection over union). The source code is publicly available at
\url{https://github.com/BiQiWHU/CMFormer}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Q/0/1/0/all/0/1&quot;&gt;Qi Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shaodi You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1&quot;&gt;Theo Gevers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02848">
<title>Revisiting Computer-Aided Tuberculosis Diagnosis. (arXiv:2307.02848v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02848</link>
<description rdf:parseType="Literal">&lt;p&gt;Tuberculosis (TB) is a major global health threat, causing millions of deaths
annually. Although early diagnosis and treatment can greatly improve the
chances of survival, it remains a major challenge, especially in developing
countries. Recently, computer-aided tuberculosis diagnosis (CTD) using deep
learning has shown promise, but progress is hindered by limited training data.
To address this, we establish a large-scale dataset, namely the Tuberculosis
X-ray (TBX11K) dataset, which contains 11,200 chest X-ray (CXR) images with
corresponding bounding box annotations for TB areas. This dataset enables the
training of sophisticated detectors for high-quality CTD. Furthermore, we
propose a strong baseline, SymFormer, for simultaneous CXR image classification
and TB infection area detection. SymFormer incorporates Symmetric Search
Attention (SymAttention) to tackle the bilateral symmetry property of CXR
images for learning discriminative features. Since CXR images may not strictly
adhere to the bilateral symmetry property, we also propose Symmetric Positional
Encoding (SPE) to facilitate SymAttention through feature recalibration. To
promote future research on CTD, we build a benchmark by introducing evaluation
metrics, evaluating baseline models reformed from existing detectors, and
running an online challenge. Experiments show that SymFormer achieves
state-of-the-art performance on the TBX11K dataset. The data, code, and models
will be released at https://github.com/yun-liu/Tuberculosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shi-Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06614">
<title>Interpretable 2D Vision Models for 3D Medical Images. (arXiv:2307.06614v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06614</link>
<description rdf:parseType="Literal">&lt;p&gt;Training Artificial Intelligence (AI) models on 3D images presents unique
challenges compared to the 2D case: Firstly, the demand for computational
resources is significantly higher, and secondly, the availability of large
datasets for pre-training is often limited, impeding training success. This
study proposes a simple approach of adapting 2D networks with an intermediate
feature representation for processing 3D images. Our method employs attention
pooling to learn to assign each slice an importance weight and, by that, obtain
a weighted average of all 2D slices. These weights directly quantify the
contribution of each slice to the contribution and thus make the model
prediction inspectable. We show on all 3D MedMNIST datasets as benchmark and
two real-world datasets consisting of several hundred high-resolution CT or MRI
scans that our approach performs on par with existing methods. Furthermore, we
compare the in-built interpretability of our approach to HiResCam, a
state-of-the-art retrospective interpretability approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ziller_A/0/1/0/all/0/1&quot;&gt;Alexander Ziller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Erdur_A/0/1/0/all/0/1&quot;&gt;Ayhan Can Erdur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trigui_M/0/1/0/all/0/1&quot;&gt;Marwa Trigui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guvenir_A/0/1/0/all/0/1&quot;&gt;Alp G&amp;#xfc;venir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mueller_T/0/1/0/all/0/1&quot;&gt;Tamara T. Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_P/0/1/0/all/0/1&quot;&gt;Philip M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jungmann_F/0/1/0/all/0/1&quot;&gt;Friederike Jungmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brandt_J/0/1/0/all/0/1&quot;&gt;Johannes Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peeken_J/0/1/0/all/0/1&quot;&gt;Jan Peeken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1&quot;&gt;Georgios Kaissis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05810">
<title>Spintronics for image recognition: performance benchmarking via ultrafast data-driven simulations. (arXiv:2308.05810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05810</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a demonstration of image classification using an echo-state
network (ESN) relying on a single simulated spintronic nanostructure known as
the vortex-based spin-torque oscillator (STVO) delayed in time. We employ an
ultrafast data-driven simulation framework called the data-driven Thiele
equation approach (DD-TEA) to simulate the STVO dynamics. This allows us to
avoid the challenges associated with repeated experimental manipulation of such
a nanostructured system. We showcase the versatility of our solution by
successfully applying it to solve classification challenges with the MNIST,
EMNIST-letters and Fashion MNIST datasets. Through our simulations, we
determine that within a large ESN the results obtained using the STVO dynamics
as an activation function are comparable to the ones obtained with other
conventional nonlinear activation functions like the reLU and the sigmoid.
While achieving state-of-the-art accuracy levels on the MNIST dataset, our
model&apos;s performance on EMNIST-letters and Fashion MNIST is lower due to the
relative simplicity of the system architecture and the increased complexity of
the tasks. We expect that the DD-TEA framework will enable the exploration of
deeper architectures, ultimately leading to improved classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moureaux_A/0/1/0/all/0/1&quot;&gt;Anatole Moureaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chopin_C/0/1/0/all/0/1&quot;&gt;Chlo&amp;#xe9; Chopin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacques_L/0/1/0/all/0/1&quot;&gt;Laurent Jacques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_F/0/1/0/all/0/1&quot;&gt;Flavio Abreu Araujo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11487">
<title>Free Lunch for Gait Recognition: A Novel Relation Descriptor. (arXiv:2308.11487v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11487</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait recognition is to seek correct matches for query individuals by their
unique walking patterns. However, current methods focus solely on extracting
individual-specific features, overlooking ``interpersonal&quot; relationships. In
this paper, we propose a novel $\textbf{Relation Descriptor}$ that captures not
only individual features but also relations between test gaits and pre-selected
gait anchors. Specifically, we reinterpret classifier weights as gait anchors
and compute similarity scores between test features and these anchors, which
re-expresses individual gait features into a similarity relation distribution.
In essence, the relation descriptor offers a holistic perspective that
leverages the collective knowledge stored within the classifier&apos;s weights,
emphasizing meaningful patterns and enhancing robustness. Despite its
potential, relation descriptor poses dimensionality challenges since its
dimension depends on the training set&apos;s identity count. To address this, we
propose Farthest gait-Anchor Selection to identify the most discriminative gait
anchors and an Orthogonal Regularization Loss to increase diversity within gait
anchors. Compared to individual-specific features extracted from the backbone,
our relation descriptor can boost the performance nearly without any extra
costs. We evaluate the effectiveness of our method on the popular GREW, Gait3D,
OU-MVLP, CASIA-B, and CCPG, showing that our method consistently outperforms
the baselines and achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1&quot;&gt;Saihui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chunshui Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongzhen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianzhu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04331">
<title>Leveraging Model Fusion for Improved License Plate Recognition. (arXiv:2309.04331v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04331</link>
<description rdf:parseType="Literal">&lt;p&gt;License Plate Recognition (LPR) plays a critical role in various
applications, such as toll collection, parking management, and traffic law
enforcement. Although LPR has witnessed significant advancements through the
development of deep learning, there has been a noticeable lack of studies
exploring the potential improvements in results by fusing the outputs from
multiple recognition models. This research aims to fill this gap by
investigating the combination of up to 12 different models using
straightforward approaches, such as selecting the most confident prediction or
employing majority vote-based strategies. Our experiments encompass a wide
range of datasets, revealing substantial benefits of fusion approaches in both
intra- and cross-dataset setups. Essentially, fusing multiple models reduces
considerably the likelihood of obtaining subpar performance on a particular
dataset/scenario. We also found that combining models based on their speed is
an appealing approach. Specifically, for applications where the recognition
task can tolerate some additional time, though not excessively, an effective
strategy is to combine 4-6 models. These models may not be the most accurate
individually, but their fusion strikes an optimal balance between speed and
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1&quot;&gt;Rayson Laroca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanlorensi_L/0/1/0/all/0/1&quot;&gt;Luiz A. Zanlorensi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estevam_V/0/1/0/all/0/1&quot;&gt;Valter Estevam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1&quot;&gt;Rodrigo Minetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1&quot;&gt;David Menotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04437">
<title>Single View Refractive Index Tomography with Neural Fields. (arXiv:2309.04437v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04437</link>
<description rdf:parseType="Literal">&lt;p&gt;Refractive Index Tomography is the inverse problem of reconstructing the
continuously-varying 3D refractive index in a scene using 2D projected image
measurements. Although a purely refractive field is not directly visible, it
bends light rays as they travel through space, thus providing a signal for
reconstruction. The effects of such fields appear in many scientific computer
vision settings, ranging from refraction due to transparent cells in microscopy
to the lensing of distant galaxies caused by dark matter in astrophysics.
Reconstructing these fields is particularly difficult due to the complex
nonlinear effects of the refractive field on observed images. Furthermore,
while standard 3D reconstruction and tomography settings typically have access
to observations of the scene from many viewpoints, many refractive index
tomography problem settings only have access to images observed from a single
viewpoint. We introduce a method that leverages prior knowledge of light
sources scattered throughout the refractive medium to help disambiguate the
single-view refractive index tomography problem. We differentiably trace curved
rays through a neural field representation of the refractive field, and
optimize its parameters to best reproduce the observed image. We demonstrate
the efficacy of our approach by reconstructing simulated refractive fields,
analyze the effects of light source distribution on the recovered field, and
test our method on a simulated dark matter mapping problem where we
successfully recover the 3D refractive field caused by a realistic dark matter
distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Brandon Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levis_A/0/1/0/all/0/1&quot;&gt;Aviad Levis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Connor_L/0/1/0/all/0/1&quot;&gt;Liam Connor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1&quot;&gt;Pratul P. Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouman_K/0/1/0/all/0/1&quot;&gt;Katherine L. Bouman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11523">
<title>RMT: Retentive Networks Meet Vision Transformers. (arXiv:2309.11523v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11523</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has gained increasing attention in the computer
vision community in recent years. However, the core component of ViT,
Self-Attention, lacks explicit spatial priors and bears a quadratic
computational complexity, thereby constraining the applicability of ViT. To
alleviate these issues, we draw inspiration from the recent Retentive Network
(RetNet) in the field of NLP, and propose RMT, a strong vision backbone with
explicit spatial prior for general purposes. Specifically, we extend the
RetNet&apos;s temporal decay mechanism to the spatial domain, and propose a spatial
decay matrix based on the Manhattan distance to introduce the explicit spatial
prior to Self-Attention. Additionally, an attention decomposition form that
adeptly adapts to explicit spatial prior is proposed, aiming to reduce the
computational burden of modeling global information without disrupting the
spatial decay matrix. Based on the spatial decay matrix and the attention
decomposition form, we can flexibly integrate explicit spatial prior into the
vision backbone with linear complexity. Extensive experiments demonstrate that
RMT exhibits exceptional performance across various vision tasks. Specifically,
without extra training data, RMT achieves **84.8%** and **86.1%** top-1 acc on
ImageNet-1k with **27M/4.5GFLOPs** and **96M/18.2GFLOPs**. For downstream
tasks, RMT achieves **54.5** box AP and **47.2** mask AP on the COCO detection
task, and **52.8** mIoU on the ADE20K semantic segmentation task. Code is
available at https://github.com/qhfan/RMT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qihang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingrui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongmin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14162">
<title>Data Upcycling Knowledge Distillation for Image Super-Resolution. (arXiv:2309.14162v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14162</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation (KD) emerges as a promising yet challenging technique
for compressing deep neural networks, aiming to transfer extensive learning
representations from proficient and computationally intensive teacher models to
compact student models. However, current KD methods for super-resolution (SR)
models have limited performance and restricted applications, since the
characteristics of SR tasks are overlooked. In this paper, we put forth an
approach from the perspective of effective data utilization, namely, the Data
Upcycling Knowledge Distillation (DUKD), which facilitates the student model by
the prior knowledge the teacher provided through the upcycled in-domain data
derived from the input images. Besides, for the first time, we realize the
label consistency regularization in KD for SR models, which is implemented by
the paired invertible data augmentations. It constrains the training process of
KD and leads to better generalization capability of the student model. The
DUKD, due to its versatility, can be applied across a broad spectrum of
teacher-student architectures (e.g., CNN and Transformer models) and SR tasks,
such as single image SR, real-world SR, and SR quantization, and is in parallel
with other compression techniques. Comprehensive experiments on diverse
benchmarks demonstrate that the DUKD method significantly outperforms previous
art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Simiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hailing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhijun Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1&quot;&gt;Bingyi Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16948">
<title>Denoising Diffusion Bridge Models. (arXiv:2309.16948v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16948</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are powerful generative models that map noise to data using
stochastic processes. However, for many applications such as image editing, the
model input comes from a distribution that is not random noise. As such,
diffusion models must rely on cumbersome methods like guidance or projected
sampling to incorporate this information in the generative process. In our
work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural
alternative to this paradigm based on diffusion bridges, a family of processes
that interpolate between two paired distributions given as endpoints. Our
method learns the score of the diffusion bridge from data and maps from one
endpoint distribution to the other by solving a (stochastic) differential
equation based on the learned score. Our method naturally unifies several
classes of generative models, such as score-based diffusion models and
OT-Flow-Matching, allowing us to adapt existing design and architectural
choices to our more general problem. Empirically, we apply DDBMs to challenging
image datasets in both pixel and latent space. On standard image translation
problems, DDBMs achieve significant improvement over baseline methods, and,
when we reduce the problem to image generation by setting the source
distribution to random noise, DDBMs achieve comparable FID scores to
state-of-the-art methods despite being built for a more general task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Linqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1&quot;&gt;Aaron Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1&quot;&gt;Samar Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17059">
<title>GSDC Transformer: An Efficient and Effective Cue Fusion for Monocular Multi-Frame Depth Estimation. (arXiv:2309.17059v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17059</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth estimation provides an alternative approach for perceiving 3D
information in autonomous driving. Monocular depth estimation, whether with
single-frame or multi-frame inputs, has achieved significant success by
learning various types of cues and specializing in either static or dynamic
scenes. Recently, these cues fusion becomes an attractive topic, aiming to
enable the combined cues to perform well in both types of scenes. However,
adaptive cue fusion relies on attention mechanisms, where the quadratic
complexity limits the granularity of cue representation. Additionally, explicit
cue fusion depends on precise segmentation, which imposes a heavy burden on
mask prediction. To address these issues, we propose the GSDC Transformer, an
efficient and effective component for cue fusion in monocular multi-frame depth
estimation. We utilize deformable attention to learn cue relationships at a
fine scale, while sparse attention reduces computational requirements when
granularity increases. To compensate for the precision drop in dynamic scenes,
we represent scene attributes in the form of super tokens without relying on
precise shapes. Within each super token attributed to dynamic scenes, we gather
its relevant cues and learn local dense relationships to enhance cue fusion.
Our method achieves state-of-the-art performance on the KITTI dataset with
efficient fusion speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_N/0/1/0/all/0/1&quot;&gt;Naiyu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lemiao Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuyou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zili Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kerui Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01415">
<title>GPT-Driver: Learning to Drive with GPT. (arXiv:2310.01415v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01415</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple yet effective approach that can transform the OpenAI
GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion
planning is a core challenge in autonomous driving, aiming to plan a driving
trajectory that is safe and comfortable. Existing motion planners predominantly
leverage heuristic methods to forecast driving trajectories, yet these
approaches demonstrate insufficient generalization capabilities in the face of
novel and unseen driving scenarios. In this paper, we propose a novel approach
to motion planning that capitalizes on the strong reasoning capabilities and
generalization potential inherent to Large Language Models (LLMs). The
fundamental insight of our approach is the reformulation of motion planning as
a language modeling problem, a perspective not previously explored.
Specifically, we represent the planner inputs and outputs as language tokens,
and leverage the LLM to generate driving trajectories through a language
description of coordinate positions. Furthermore, we propose a novel
prompting-reasoning-finetuning strategy to stimulate the numerical reasoning
potential of the LLM. With this strategy, the LLM can describe highly precise
trajectory coordinates and also its internal decision-making process in natural
language. We evaluate our approach on the large-scale nuScenes dataset, and
extensive experiments substantiate the effectiveness, generalization ability,
and interpretability of our GPT-based motion planner. Code is now available at
https://github.com/PointsCoder/GPT-Driver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiageng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuxi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04406">
<title>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04406</link>
<description rdf:parseType="Literal">&lt;p&gt;While large language models (LLMs) have demonstrated impressive performance
on a range of decision-making tasks, they rely on simple acting processes and
fall short of broad deployment as autonomous agents. We introduce LATS
(Language Agent Tree Search), a general framework that synergizes the
capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration
from Monte Carlo tree search in model-based reinforcement learning, LATS
employs LLMs as agents, value functions, and optimizers, repurposing their
latent strengths for enhanced decision-making. What is crucial in this method
is the use of an environment for external feedback, which offers a more
deliberate and adaptive problem-solving mechanism that moves beyond the
limitations of existing techniques. Our experimental evaluation across diverse
domains, such as programming, HotPotQA, and WebShop, illustrates the
applicability of LATS for both reasoning and acting. In particular, LATS
achieves 94.4% for programming on HumanEval with GPT-4 and an average score of
75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness
and generality of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Andy Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kai Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlapentokh_Rothman_M/0/1/0/all/0/1&quot;&gt;Michal Shlapentokh-Rothman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05867">
<title>Domain-wise Invariant Learning for Panoptic Scene Graph Generation. (arXiv:2310.05867v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05867</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoptic Scene Graph Generation (PSG) involves the detection of objects and
the prediction of their corresponding relationships (predicates). However, the
presence of biased predicate annotations poses a significant challenge for PSG
models, as it hinders their ability to establish a clear decision boundary
among different predicates. This issue substantially impedes the practical
utility and real-world applicability of PSG models. To address the intrinsic
bias above, we propose a novel framework to infer potentially biased
annotations by measuring the predicate prediction risks within each
subject-object pair (domain), and adaptively transfer the biased annotations to
consistent ones by learning invariant predicate representation embeddings.
Experiments show that our method significantly improves the performance of
benchmark models, achieving a new state-of-the-art performance, and shows great
generalization and effectiveness on PSG dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;You Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07726">
<title>Warfare:Breaking the Watermark Protection of AI-Generated Content. (arXiv:2310.07726v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07726</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-Generated Content (AIGC) is gaining great popularity, with many emerging
commercial services and applications. These services leverage advanced
generative models, such as latent diffusion models and large language models,
to generate creative content (e.g., realistic images and fluent sentences) for
users. The usage of such generated content needs to be highly regulated, as the
service providers need to ensure the users do not violate the usage policies
(e.g., abuse for commercialization, generating and distributing unsafe
content). A promising solution to achieve this goal is watermarking, which adds
unique and imperceptible watermarks on the content for service verification and
attribution. Numerous watermarking approaches have been proposed recently.
However, in this paper, we show that an adversary can easily break these
watermarking mechanisms. Specifically, we consider two possible attacks. (1)
Watermark removal: the adversary can easily erase the embedded watermark from
the generated content and then use it freely bypassing the regulation of the
service provider. (2) Watermark forging: the adversary can create illegal
content with forged watermarks from another user, causing the service provider
to make wrong attributions. We propose Warfare, a unified methodology to
achieve both attacks in a holistic way. The key idea is to leverage a
pre-trained diffusion model for content processing and a generative adversarial
network for watermark removal or forging. We evaluate Warfare on different
datasets and embedding setups. The results prove that it can achieve high
success rates while maintaining the quality of the generated content. Compared
to existing diffusion model-based attacks, Warfare is 5,050~11,000x faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shangwei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08529">
<title>GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models. (arXiv:2310.08529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08529</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, the generation of 3D assets from text prompts has shown
impressive results. Both 2D and 3D diffusion models can help generate decent 3D
objects based on prompts. 3D diffusion models have good 3D consistency, but
their quality and generalization are limited as trainable 3D data is expensive
and hard to obtain. 2D diffusion models enjoy strong abilities of
generalization and fine generation, but 3D consistency is hard to guarantee.
This paper attempts to bridge the power from the two types of diffusion models
via the recent explicit and efficient 3D Gaussian splatting representation. A
fast 3D object generation framework, named as GaussianDreamer, is proposed,
where the 3D diffusion model provides priors for initialization and the 2D
diffusion model enriches the geometry and appearance. Operations of noisy point
growing and color perturbation are introduced to enhance the initialized
Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D
avatar within 15 minutes on one GPU, much faster than previous methods, while
the generated instances can be directly rendered in real time. Demos and code
are available at https://taoranyi.com/gaussiandreamer/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1&quot;&gt;Taoran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jiemin Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guanjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lingxi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10123">
<title>AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion. (arXiv:2310.10123v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we aim to solve complex real-world image restoration
situations, in which, one image may have a variety of unknown degradations. To
this end, we propose an all-in-one image restoration framework with latent
diffusion (AutoDIR), which can automatically detect and address multiple
unknown degradations. Our framework first utilizes a Blind Image Quality
Assessment Module (BIQA) to automatically detect and identify the unknown
dominant image degradation type of the image. Then, an All-in-One Image
Refinement (AIR) Module handles multiple kinds of degradation image restoration
with the guidance of BIQA. Finally, a Structure Correction Module (SCM) is
proposed to recover the image details distorted by AIR. Our comprehensive
evaluation demonstrates that AutoDIR outperforms state-of-the-art approaches by
achieving superior restoration results while supporting a wider range of tasks.
Notably, AutoDIR is also the first method to automatically handle real-scenario
images with multiple unknown degradations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yitong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tianfan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinwei Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17951">
<title>Understanding Parameter Saliency via Extreme Value Theory. (arXiv:2310.17951v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17951</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are being increasingly implemented throughout society in
recent years. It is useful to identify which parameters trigger
misclassification in diagnosing undesirable model behaviors. The concept of
parameter saliency is proposed and used to diagnose convolutional neural
networks (CNNs) by ranking convolution filters that may have caused
misclassification on the basis of parameter saliency. It is also shown that
fine-tuning the top ranking salient filters efficiently corrects
misidentification on ImageNet. However, there is still a knowledge gap in terms
of understanding why parameter saliency ranking can find the filters inducing
misidentification. In this work, we attempt to bridge the gap by analyzing
parameter saliency ranking from a statistical viewpoint, namely, extreme value
theory. We first show that the existing work implicitly assumes that the
gradient norm computed for each filter follows a normal distribution. Then, we
clarify the relationship between parameter saliency and the score based on the
peaks-over-threshold (POT) method, which is often used to model extreme values.
Finally, we reformulate parameter saliency in terms of the POT method, where
this reformulation is regarded as statistical anomaly detection and does not
require the implicit assumptions of the existing parameter-saliency
formulation. Our experimental results demonstrate that our reformulation can
detect malicious filters as well. Furthermore, we show that the existing
parameter saliency method exhibits a bias against the depth of layers in deep
neural networks. In particular, this bias has the potential to inhibit the
discovery of filters that cause misidentification in situations where domain
shift occurs. In contrast, parameter saliency based on POT shows less of this
bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19535">
<title>Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation. (arXiv:2310.19535v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19535</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to old CRT display technology and limited transmission bandwidth, early
film and TV broadcasts commonly used interlaced scanning. This meant each field
contained only half of the information. Since modern displays require full
frames, this has spurred research into deinterlacing, i.e. restoring the
missing information in legacy video content. In this paper, we present a
deep-learning-based method for deinterlacing animated and live-action content.
Our proposed method supports bidirectional spatio-temporal information
propagation across multiple scales to leverage information in both space and
time. More specifically, we design a Flow-guided Refinement Block (FRB) which
performs feature refinement including alignment, fusion, and rectification.
Additionally, our method can process multiple fields simultaneously, reducing
per-frame processing time, and potentially enabling real-time processing. Our
experimental results demonstrate that our proposed method achieves superior
performance compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroers_C/0/1/0/all/0/1&quot;&gt;Christopher Schroers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00230">
<title>DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing. (arXiv:2311.00230v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00230</link>
<description rdf:parseType="Literal">&lt;p&gt;Utilizing visual place recognition (VPR) technology to ascertain the
geographical location of publicly available images is a pressing issue for
real-world VPR applications. Although most current VPR methods achieve
favorable results under ideal conditions, their performance in complex
environments, characterized by lighting variations, seasonal changes, and
occlusions caused by moving objects, is generally unsatisfactory. In this
study, we utilize the DINOv2 model as the backbone network for trimming and
fine-tuning to extract robust image features. We propose a novel VPR
architecture called DINO-Mix, which combines a foundational vision model with
feature aggregation. This architecture relies on the powerful image feature
extraction capabilities of foundational vision models. We employ an
MLP-Mixer-based mix module to aggregate image features, resulting in globally
robust and generalizable descriptors that enable high-precision VPR. We
experimentally demonstrate that the proposed DINO-Mix architecture
significantly outperforms current state-of-the-art (SOTA) methods. In test sets
having lighting variations, seasonal changes, and occlusions (Tokyo24/7,
Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1
accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA
methods, our architecture exhibited an average accuracy improvement of 5.14%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gaoshuang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaofei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenglong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Luying Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Wenjian Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1&quot;&gt;Mingbo Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05197">
<title>Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection. (arXiv:2311.05197v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05197</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing reliance on Computed Tomography Pulmonary Angiography for
Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need for
improved diagnostic solutions. The primary objective of this study is to
leverage deep learning techniques to enhance the Computer Assisted Diagnosis of
PE. In this study, we propose a classifier-guided detection approach that
effectively leverages the classifier&apos;s probabilistic inference to direct the
detection predictions, marking a novel contribution in the domain of automated
PE diagnosis. Our end-to-end classification framework introduces an
Attention-Guided Convolutional Neural Network (AG-CNN) that leverages local
context by utilizing an attention mechanism. This approach emulates the
attention of a human expert by looking at both global appearances and local
lesion regions before forming a conclusive decision. The classifier achieves a
notable AUROC, sensitivity, specificity and F1-score of 0.927, 0.862, 0.879 and
0.805 respectively on the FUMPE dataset with Inception-v3 backbone
architecture. Moreover, AG-CNN outperforms the baseline DenseNet-121 model,
achieving an 8.1% AUROC gain. While prior studies have primarily focused on PE
detection in main arteries, our utilization of state-of-the-art object
detection models and ensembling techniques significantly enhances detection
accuracy for small embolisms in the peripheral arteries. Finally, our proposed
classifier-guided detection approach further refines the detection metrics
contributing new state-of-the-art to the community: mAP$_{50}$, sensitivity and
F1-score of 0.846, 0.901 and 0.779 respectively outperforming the former
benchmark with a significant 3.7% improvement in mAP$_{50}$. Our research aims
to elevate PE patient care by integrating AI solutions into clinical workflows,
highlighting the potential of human-AI collaboration in medical diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bushra_F/0/1/0/all/0/1&quot;&gt;Fabiha Bushra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarmun_R/0/1/0/all/0/1&quot;&gt;Rusab Sarmun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1&quot;&gt;Saidul Kabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_M/0/1/0/all/0/1&quot;&gt;Menatalla Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoghoul_S/0/1/0/all/0/1&quot;&gt;Sohaib Bassam Zoghoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mushtak_A/0/1/0/all/0/1&quot;&gt;Adam Mushtak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hashimi_I/0/1/0/all/0/1&quot;&gt;Israa Al-Hashimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Alqahtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08843">
<title>Personalized Video Relighting With an At-Home Light Stage. (arXiv:2311.08843v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08843</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a personalized video relighting algorithm that
produces high-quality and temporally consistent relit videos under any pose,
expression, and lighting condition in real-time. Existing relighting algorithms
typically rely either on publicly available synthetic data, which yields poor
relighting results, or instead on light stage data which is difficult to
obtain. We show that by just capturing video of a user watching YouTube videos
on a monitor we can train a personalized algorithm capable of performing
high-quality relighting under any condition. Our key contribution is a novel
neural relighting architecture that effectively separates the intrinsic
appearance features - the geometry and reflectance of the face - from the
source lighting and then combines them with the target lighting to generate a
relit image. This neural network architecture enables smoothing of intrinsic
appearance features leading to temporally stable video relighting. Both
qualitative and quantitative evaluations show that our architecture improves
portrait image relighting quality and temporal consistency over
state-of-the-art approaches on both casually captured `Light Stage at Your
Desk&apos; (LSYD) and light-stage-captured `One Light At a Time&apos; (OLAT) datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Myeong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christman_M/0/1/0/all/0/1&quot;&gt;Max Christman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_R/0/1/0/all/0/1&quot;&gt;Roni Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09655">
<title>Multi-View Spectrogram Transformer for Respiratory Sound Classification. (arXiv:2311.09655v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09655</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have been applied to audio spectrograms for respiratory
sound classification. Existing models often treat the spectrogram as a
synthetic image while overlooking its physical characteristics. In this paper,
a Multi-View Spectrogram Transformer (MVST) is proposed to embed different
views of time-frequency characteristics into the vision transformer.
Specifically, the proposed MVST splits the mel-spectrogram into different sized
patches, representing the multi-view acoustic elements of a respiratory sound.
These patches and positional embeddings are then fed into transformer encoders
to extract the attentional information among patches through a self-attention
mechanism. Finally, a gated fusion scheme is designed to automatically weigh
the multi-view features to highlight the best one in a specific scenario.
Experimental results on the ICBHI dataset demonstrate that the proposed MVST
significantly outperforms state-of-the-art methods for classifying respiratory
sounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wentao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jianfeng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1&quot;&gt;Ruibin Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xudong Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11567">
<title>InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models. (arXiv:2311.11567v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11567</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Models (MLLMs) are increasingly prominent in the
field of artificial intelligence. These models not only excel in traditional
vision-language tasks but also demonstrate impressive performance in
contemporary multi-modal benchmarks. Although many of these benchmarks attempt
to holistically evaluate MLLMs, they typically concentrate on basic reasoning
tasks, often yielding only simple yes/no or multi-choice responses. These
methods naturally lead to confusion and difficulties in conclusively
determining the reasoning capabilities of MLLMs. To mitigate this issue, we
manually curate a benchmark dataset specifically designed for MLLMs, with a
focus on complex reasoning tasks. Our benchmark comprises three key reasoning
categories: deductive, abductive, and analogical reasoning. The queries in our
dataset are intentionally constructed to engage the reasoning capabilities of
MLLMs in the process of generating answers. For a fair comparison across
various MLLMs, we incorporate intermediate reasoning steps into our evaluation
criteria. In instances where an MLLM is unable to produce a definitive answer,
its reasoning ability is evaluated by requesting intermediate reasoning steps.
If these steps align with our manual annotations, appropriate scores are
assigned. This evaluation scheme resembles methods commonly used in human
assessments, such as exams or assignments, and represents what we consider a
more effective assessment technique compared with existing benchmarks. We
evaluate a selection of representative MLLMs using this rigorously developed
open-ended multi-step elaborate reasoning benchmark, designed to challenge and
accurately measure their reasoning capabilities. The code and data will be
released at https://infimm.github.io/InfiMM-Eval/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaotian Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1&quot;&gt;Quanzeng You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wentao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mrini_K/0/1/0/all/0/1&quot;&gt;Khalil Mrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xudong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1&quot;&gt;Bohan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jianbo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11602">
<title>A Multi-In-Single-Out Network for Video Frame Interpolation without Optical Flow. (arXiv:2311.11602v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11602</link>
<description rdf:parseType="Literal">&lt;p&gt;In general, deep learning-based video frame interpolation (VFI) methods have
predominantly focused on estimating motion vectors between two input frames and
warping them to the target time. While this approach has shown impressive
performance for linear motion between two input frames, it exhibits limitations
when dealing with occlusions and nonlinear movements. Recently, generative
models have been applied to VFI to address these issues. However, as VFI is not
a task focused on generating plausible images, but rather on predicting
accurate intermediate frames between two given frames, performance limitations
still persist. In this paper, we propose a multi-in-single-out (MISO) based VFI
method that does not rely on motion vector estimation, allowing it to
effectively model occlusions and nonlinear motion. Additionally, we introduce a
novel motion perceptual loss that enables MISO-VFI to better capture the
spatio-temporal correlations within the video frames. Our MISO-VFI method
achieves state-of-the-art results on VFI benchmarks Vimeo90K, Middlebury, and
UCF101, with a significant performance gap compared to existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaemin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minseok Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyobin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dong-Geol Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12144">
<title>Applications of Large Scale Foundation Models for Autonomous Driving. (arXiv:2311.12144v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12144</link>
<description rdf:parseType="Literal">&lt;p&gt;Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12886">
<title>AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance. (arXiv:2311.12886v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12886</link>
<description rdf:parseType="Literal">&lt;p&gt;Image animation is a key task in computer vision which aims to generate
dynamic visual content from static image. Recent image animation methods employ
neural based rendering technique to generate realistic animations. Despite
these advancements, achieving fine-grained and controllable image animation
guided by text remains challenging, particularly for open-domain images
captured in diverse real environments. In this paper, we introduce an open
domain image animation method that leverages the motion prior of video
diffusion model. Our approach introduces targeted motion area guidance and
motion strength guidance, enabling precise control the movable area and its
motion speed. This results in enhanced alignment between the animated visual
elements and the prompting text, thereby facilitating a fine-grained and
interactive animation generation process for intricate motion sequences. We
validate the effectiveness of our method through rigorous experiments on an
open-domain dataset, with the results showcasing its superior performance.
Project page can be found at https://animationai.github.io/AnimateAnything.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zuozhuo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1&quot;&gt;Bingxue Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Siyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Long Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weizhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13307">
<title>Rethinking Radiology Report Generation via Causal Reasoning and Counterfactual Augmentation. (arXiv:2311.13307v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13307</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology Report Generation (RRG) draws attention as an interaction between
vision and language fields. Previous works inherited the ideology of
vision-to-language generation tasks,aiming to generate paragraphs with high
consistency as reports. However, one unique characteristic of RRG, the
independence between diseases, was neglected, leading to the injection of
disease co-occurrence as a confounder that effects the results through backdoor
path. Unfortunately, this confounder confuses the process of report generation
worse because of the biased RRG data distribution. In this paper, to rethink
this issue thoroughly, we reason about its causes and effects from a novel
perspective of statistics and causality, where the Joint Vision Coupling and
the Conditional Sentence Coherence Coupling are two aspects prone to implicitly
decrease the accuracy of reports. Then, a counterfactual augmentation strategy
that contains the Counterfactual Sample Synthesis and the Counterfactual Report
Reconstruction sub-methods is proposed to break these two aspects of spurious
effects. Experimental results and further analyses on two widely used datasets
justify our reasoning and proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiafan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1&quot;&gt;Wenbin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruxin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14062">
<title>Hardware Resilience Properties of Text-Guided Image Classifiers. (arXiv:2311.14062v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14062</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel method to enhance the reliability of image
classification models during deployment in the face of transient hardware
errors. By utilizing enriched text embeddings derived from GPT-3 with question
prompts per class and CLIP pretrained text encoder, we investigate their impact
as an initialization for the classification layer. Our approach achieves a
remarkable $5.5\times$ average increase in hardware reliability (and up to
$14\times$) across various architectures in the most critical layer, with
minimal accuracy drop ($0.3\%$ on average) compared to baseline PyTorch models.
Furthermore, our method seamlessly integrates with any image classification
backbone, showcases results across various network architectures, decreases
parameter and FLOPs overhead, and follows a consistent training recipe. This
research offers a practical and efficient solution to bolster the robustness of
image classification models against hardware failures, with potential
implications for future studies in this domain. Our code and models are
released at https://github.com/TalalWasim/TextGuidedResilience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1&quot;&gt;Syed Talal Wasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soboka_K/0/1/0/all/0/1&quot;&gt;Kabila Haile Soboka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoud_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Mahmoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1&quot;&gt;David Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1&quot;&gt;Gu-Yeon Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15138">
<title>Can SAM recognize crops? Quantifying the zero-shot performance of a semantic segmentation foundation model on generating crop-type maps using satellite imagery for precision agriculture. (arXiv:2311.15138v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15138</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change is increasingly disrupting worldwide agriculture, making
global food production less reliable. To tackle the growing challenges in
feeding the planet, cutting-edge management strategies, such as precision
agriculture, empower farmers and decision-makers with rich and actionable
information to increase the efficiency and sustainability of their farming
practices. Crop-type maps are key information for decision-support tools but
are challenging and costly to generate. We investigate the capabilities of Meta
AI&apos;s Segment Anything Model (SAM) for crop-map prediction task, acknowledging
its recent successes at zero-shot image segmentation. However, SAM being
limited to up-to 3 channel inputs and its zero-shot usage being class-agnostic
in nature pose unique challenges in using it directly for crop-type mapping. We
propose using clustering consensus metrics to assess SAM&apos;s zero-shot
performance in segmenting satellite imagery and producing crop-type maps.
Although direct crop-type mapping is challenging using SAM in zero-shot
setting, experiments reveal SAM&apos;s potential for swiftly and accurately
outlining fields in satellite images, serving as a foundation for subsequent
crop classification. This paper attempts to highlight a use-case of
state-of-the-art image segmentation models like SAM for crop-type mapping and
related specific needs of the agriculture industry, offering a potential avenue
for automatic, efficient, and cost-effective data products for precision
agriculture practices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurav_R/0/1/0/all/0/1&quot;&gt;Rutuja Gurav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1&quot;&gt;Het Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1&quot;&gt;Zhuocheng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldawy_A/0/1/0/all/0/1&quot;&gt;Ahmed Eldawy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scudiero_E/0/1/0/all/0/1&quot;&gt;Elia Scudiero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos Papalexakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15260">
<title>NeuRAD: Neural Rendering for Autonomous Driving. (arXiv:2311.15260v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15260</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRFs) have gained popularity in the autonomous
driving (AD) community. Recent methods show NeRFs&apos; potential for closed-loop
simulation, enabling testing of AD systems, and as an advanced training data
augmentation technique. However, existing methods often require long training
times, dense semantic supervision, or lack generalizability. This, in turn,
hinders the application of NeRFs for AD at scale. In this paper, we propose
NeuRAD, a robust novel view synthesis method tailored to dynamic AD data. Our
method features simple network design, extensive sensor modeling for both
camera and lidar -- including rolling shutter, beam divergence and ray dropping
-- and is applicable to multiple datasets out of the box. We verify its
performance on five popular AD datasets, achieving state-of-the-art performance
across the board. To encourage further development, we will openly release the
NeuRAD source code. See https://github.com/georghess/NeuRAD .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonderski_A/0/1/0/all/0/1&quot;&gt;Adam Tonderski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindstrom_C/0/1/0/all/0/1&quot;&gt;Carl Lindstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1&quot;&gt;Georg Hess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ljungbergh_W/0/1/0/all/0/1&quot;&gt;William Ljungbergh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1&quot;&gt;Lennart Svensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1&quot;&gt;Christoffer Petersson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15776">
<title>Stable Segment Anything Model. (arXiv:2311.15776v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15776</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) achieves remarkable promptable segmentation
given high-quality prompts which, however, often require good skills to
specify. To make SAM robust to casual prompts, this paper presents the first
comprehensive analysis on SAM&apos;s segmentation stability across a diverse
spectrum of prompt qualities, notably imprecise bounding boxes and insufficient
points. Our key finding reveals that given such low-quality prompts, SAM&apos;s mask
decoder tends to activate image features that are biased towards the background
or confined to specific object parts. To mitigate this issue, our key idea
consists of calibrating solely SAM&apos;s mask attention by adjusting the sampling
locations and amplitudes of image features, while the original SAM model
architecture and weights remain unchanged. Consequently, our deformable
sampling plugin (DSP) enables SAM to adaptively shift attention to the prompted
target regions in a data-driven manner, facilitated by our effective robust
training strategy (RTS). During inference, dynamic routing plugin (DRP) is
proposed that toggles SAM between the deformable and regular grid sampling
modes, conditioned on the input prompt quality. Thus, our solution, termed
Stable-SAM, offers several advantages: 1) improved SAM&apos;s segmentation stability
across a wide range of prompt qualities, while 2) retaining SAM&apos;s powerful
promptable segmentation efficiency and generality, with 3) minimal learnable
parameters (0.08 M) and fast adaptation (by 1 training epoch). Extensive
experiments across multiple datasets validate the effectiveness and advantages
of our approach, underscoring Stable-SAM as a more robust solution for
segmenting anything. Codes will be released upon acceptance.
https://github.com/fanq15/Stable-SAM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xin Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1&quot;&gt;Lei Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mingqiao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1&quot;&gt;Pengfei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16473">
<title>GS-IR: 3D Gaussian Splatting for Inverse Rendering. (arXiv:2311.16473v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16473</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian
Splatting (GS) that leverages forward mapping volume rendering to achieve
photorealistic novel view synthesis and relighting results. Unlike previous
works that use implicit neural representations and volume rendering (e.g.
NeRF), which suffer from low expressive power and high computational
complexity, we extend GS, a top-performance representation for novel view
synthesis, to estimate scene geometry, surface material, and environment
illumination from multi-view images captured under unknown lighting conditions.
There are two main problems when introducing GS to inverse rendering: 1) GS
does not support producing plausible normal natively; 2) forward mapping (e.g.
rasterization and splatting) cannot trace the occlusion like backward mapping
(e.g. ray tracing). To address these challenges, our GS-IR proposes an
efficient optimization scheme that incorporates a depth-derivation-based
regularization for normal estimation and a baking-based occlusion to model
indirect lighting. The flexible and expressive GS representation allows us to
achieve fast and compact geometry reconstruction, photorealistic novel view
synthesis, and effective physically-based rendering. We demonstrate the
superiority of our method over baseline methods through qualitative and
quantitative evaluations on various challenging scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhihao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Ying Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1&quot;&gt;Kui Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16495">
<title>Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement. (arXiv:2311.16495v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16495</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we explore egocentric whole-body motion capture using a single
fisheye camera, which simultaneously estimates human body and hand motion. This
task presents significant challenges due to three factors: the lack of
high-quality datasets, fisheye camera distortion, and human body
self-occlusion. To address these challenges, we propose a novel approach that
leverages FisheyeViT to extract fisheye image features, which are subsequently
converted into pixel-aligned 3D heatmap representations for 3D human body pose
prediction. For hand tracking, we incorporate dedicated hand detection and hand
pose estimation networks for regressing 3D hand poses. Finally, we develop a
diffusion-based whole-body motion prior model to refine the estimated
whole-body motion while accounting for joint uncertainties. To train these
networks, we collect a large synthetic dataset, EgoWholeBody, comprising
840,000 high-quality egocentric images captured across a diverse range of
whole-body motion sequences. Quantitative and qualitative evaluations
demonstrate the effectiveness of our method in producing high-quality
whole-body motion estimates from a single egocentric camera.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhe Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1&quot;&gt;Diogo Luvizon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1&quot;&gt;Kripasindhu Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Danhang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1&quot;&gt;Thabo Beeler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16512">
<title>CoSeR: Bridging Image and Language for Cognitive Super-Resolution. (arXiv:2311.16512v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16512</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing super-resolution (SR) models primarily focus on restoring local
texture details, often neglecting the global semantic information within the
scene. This oversight can lead to the omission of crucial semantic details or
the introduction of inaccurate textures during the recovery process. In our
work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering
SR models with the capacity to comprehend low-resolution images. We achieve
this by marrying image appearance and language understanding to generate a
cognitive embedding, which not only activates prior information from large
text-to-image diffusion models but also facilitates the generation of
high-quality reference images to optimize the SR process. To further improve
image fidelity, we propose a novel condition injection scheme called
&quot;All-in-Attention&quot;, consolidating all conditional information into a single
module. Consequently, our method successfully restores semantically correct and
photorealistic details, demonstrating state-of-the-art performance across
multiple benchmarks. Code: https://github.com/VINHYU/CoSeR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haoze Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_R/0/1/0/all/0/1&quot;&gt;Renjing Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyi Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16926">
<title>LLaFS: When Large-Language Models Meet Few-Shot Segmentation. (arXiv:2311.16926v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16926</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes LLaFS, the first attempt to leverage large language
models (LLMs) in few-shot segmentation. In contrast to the conventional
few-shot segmentation methods that only rely on the limited and biased
information from the annotated support images, LLaFS leverages the vast prior
knowledge gained by LLM as an effective supplement and directly uses the LLM to
segment images in a few-shot manner. To enable the text-based LLM to handle
image-related tasks, we carefully design an input instruction that allows the
LLM to produce segmentation results represented as polygons, and propose a
region-attribute table to simulate the human visual mechanism and provide
multi-modal guidance. We also synthesize pseudo samples and use curriculum
learning for pretraining to augment data and achieve better optimization. LLaFS
achieves state-of-the-art results on multiple datasets, showing the potential
of using LLMs for few-shot computer vision tasks. Code will be available at
https://github.com/lanyunzhu99/LLaFS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lanyun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianrun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1&quot;&gt;Deyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jieping Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17074">
<title>Self-Supervised Learning of Whole and Component-Based Semantic Representations for Person Re-Identification. (arXiv:2311.17074v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17074</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive Segmentation Models (ISMs) like the Segment Anything Model have
significantly improved various computer vision tasks, yet their application to
Person Re-identification (ReID) remains limited. On the other hand, existing
semantic pre-training models for ReID often have limitations like predefined
parsing ranges or coarse semantics. Additionally, ReID and Clothes-Changing
ReID (CC-ReID) are usually treated separately due to their different domains.
This paper investigates whether utilizing precise human-centric semantic
representation can boost the ReID performance and improve the generalization
among various ReID tasks. We propose SemReID, a self-supervised ReID model that
leverages ISMs for adaptive part-based semantic extraction, contributing to the
improvement of ReID performance. SemReID additionally refines its semantic
representation through techniques such as image masking and KoLeo
regularization. Evaluation across three types of ReID datasets -- standard
ReID, CC-ReID, and unconstrained ReID -- demonstrates superior performance
compared to state-of-the-art methods. In addition, recognizing the scarcity of
large person datasets with fine-grained semantics, we introduce the novel
LUPerson-Part dataset to assist ReID methods in acquiring the fine-grained part
semantics for robust performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kathirvel_R/0/1/0/all/0/1&quot;&gt;Ram Prabhakar Kathirvel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1&quot;&gt;Chun Pong Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18828">
<title>One-step Diffusion with Distribution Matching Distillation. (arXiv:2311.18828v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18828</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models generate high-quality images but require dozens of forward
passes. We introduce Distribution Matching Distillation (DMD), a procedure to
transform a diffusion model into a one-step image generator with minimal impact
on image quality. We enforce the one-step image generator match the diffusion
model at distribution level, by minimizing an approximate KL divergence whose
gradient can be expressed as the difference between 2 score functions, one of
the target distribution and the other of the synthetic distribution being
produced by our one-step generator. The score functions are parameterized as
two diffusion models trained separately on each distribution. Combined with a
simple regression loss matching the large-scale structure of the multi-step
diffusion outputs, our method outperforms all published few-step diffusion
approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot
COCO-30k, comparable to Stable Diffusion but orders of magnitude faster.
Utilizing FP16 inference, our model generates images at 20 FPS on modern
hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_T/0/1/0/all/0/1&quot;&gt;Tianwei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#xeb;l Gharbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1&quot;&gt;Fredo Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1&quot;&gt;William T. Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taesung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00072">
<title>CRAFT: Contextual Re-Activation of Filters for face recognition Training. (arXiv:2312.00072v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00072</link>
<description rdf:parseType="Literal">&lt;p&gt;The first layer of a deep CNN backbone applies filters to an image to extract
the basic features available to later layers. During training, some filters may
go inactive, mean ing all weights in the filter approach zero. An inactive fil
ter in the final model represents a missed opportunity to extract a useful
feature. This phenomenon is especially prevalent in specialized CNNs such as
for face recogni tion (as opposed to, e.g., ImageNet). For example, in one the
most widely face recognition model (ArcFace), about half of the convolution
filters in the first layer are inactive. We propose a novel approach designed
and tested specif ically for face recognition networks, known as &quot;CRAFT:
Contextual Re-Activation of Filters for Face Recognition Training&quot;. CRAFT
identifies inactive filters during training and reinitializes them based on the
context of strong filters at that stage in training. We show that CRAFT reduces
fraction of inactive filters from 44% to 32% on average and discovers filter
patterns not found by standard training. Compared to standard training without
reactivation, CRAFT demonstrates enhanced model accuracy on standard
face-recognition benchmark datasets including AgeDB-30, CPLFW, LFW, CALFW, and
CFP-FP, as well as on more challenging datasets like IJBB and IJBC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatta_A/0/1/0/all/0/1&quot;&gt;Aman Bhatta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mery_D/0/1/0/all/0/1&quot;&gt;Domingo Mery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haiyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1&quot;&gt;Kevin W. Bowyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00878">
<title>Grounding Everything: Emerging Localization Properties in Vision-Language Transformers. (arXiv:2312.00878v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00878</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language foundation models have shown remarkable performance in
various zero-shot settings such as image retrieval, classification, or
captioning. But so far, those models seem to fall behind when it comes to
zero-shot localization of referential expressions and objects in images. As a
result, they need to be fine-tuned for this task. In this paper, we show that
pretrained vision-language (VL) models allow for zero-shot open-vocabulary
object localization without any fine-tuning. To leverage those capabilities, we
propose a Grounding Everything Module (GEM) that generalizes the idea of
value-value attention introduced by CLIPSurgery to a self-self attention path.
We show that the concept of self-self attention corresponds to clustering, thus
enforcing groups of tokens arising from the same object to be similar while
preserving the alignment with the language space. To further guide the group
formation, we propose a set of regularizations that allows the model to finally
generalize across datasets and backbones. We evaluate the proposed GEM
framework on various benchmark tasks and datasets for semantic segmentation. It
shows that GEM not only outperforms other training-free open-vocabulary
localization methods, but also achieves state-of-the-art results on the
recently proposed OpenImagesV7 large-scale segmentation benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bousselham_W/0/1/0/all/0/1&quot;&gt;Walid Bousselham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1&quot;&gt;Felix Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1&quot;&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1&quot;&gt;Hilde Kuehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01003">
<title>Self-Evolving Neural Radiance Fields. (arXiv:2312.01003v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01003</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, neural radiance field (NeRF) has shown remarkable performance in
novel view synthesis and 3D reconstruction. However, it still requires abundant
high-quality images, limiting its applicability in real-world scenarios. To
overcome this limitation, recent works have focused on training NeRF only with
sparse viewpoints by giving additional regularizations, often called few-shot
NeRF. We observe that due to the under-constrained nature of the task, solely
using additional regularization is not enough to prevent the model from
overfitting to sparse viewpoints. In this paper, we propose a novel framework,
dubbed Self-Evolving Neural Radiance Fields (SE-NeRF), that applies a
self-training framework to NeRF to address these problems. We formulate
few-shot NeRF into a teacher-student framework to guide the network to learn a
more robust representation of the scene by training the student with additional
pseudo labels generated from the teacher. By distilling ray-level pseudo labels
using distinct distillation schemes for reliable and unreliable rays obtained
with our novel reliability estimation method, we enable NeRF to learn a more
accurate and robust geometry of the 3D scene. We show and evaluate that
applying our self-training framework to existing models improves the quality of
the rendered images and achieves state-of-the-art performance in multiple
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jaewoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jisang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jiwon Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seongchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_M/0/1/0/all/0/1&quot;&gt;Min-Seop Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01232">
<title>A Comprehensive Study of Vision Transformers in Image Classification Tasks. (arXiv:2312.01232v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01232</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Classification is a fundamental task in the field of computer vision
that frequently serves as a benchmark for gauging advancements in Computer
Vision. Over the past few years, significant progress has been made in image
classification due to the emergence of deep learning. However, challenges still
exist, such as modeling fine-grained visual information, high computation
costs, the parallelism of the model, and inconsistent evaluation protocols
across datasets. In this paper, we conduct a comprehensive survey of existing
papers on Vision Transformers for image classification. We first introduce the
popular image classification datasets that influenced the design of models.
Then, we present Vision Transformers models in chronological order, starting
with early attempts at adapting attention mechanism to vision tasks followed by
the adoption of vision transformers, as they have demonstrated success in
capturing intricate patterns and long-range dependencies within images.
Finally, we discuss open problems and shed light on opportunities for image
classification to facilitate new research ideas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalil_M/0/1/0/all/0/1&quot;&gt;Mahmoud Khalil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalil_A/0/1/0/all/0/1&quot;&gt;Ahmad Khalil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngom_A/0/1/0/all/0/1&quot;&gt;Alioune Ngom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01239">
<title>Motion Informed Needle Segmentation in Ultrasound Images. (arXiv:2312.01239v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01239</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmenting a moving needle in ultrasound images is challenging due to the
presence of artifacts, noise, and needle occlusion. This task becomes even more
demanding in scenarios where data availability is limited. Convolutional Neural
Networks (CNNs) have been successful in many computer vision applications, but
struggle to accurately segment needles without considering their motion. In
this paper, we present a novel approach for needle segmentation that combines
classical Kalman Filter (KF) techniques with data-driven learning,
incorporating both needle features and needle motion. Our method offers two key
contributions. First, we propose a compatible framework that seamlessly
integrates into commonly used encoder-decoder style architectures. Second, we
demonstrate superior performance compared to recent state-of-the-art needle
segmentation models using our novel convolutional neural network (CNN) based
KF-inspired block, achieving a 15\% reduction in pixel-wise needle tip error
and an 8\% reduction in length error. Third, to our knowledge we are the first
to implement a learnable filter to incorporate non-linear needle motion for
improving needle segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goel_R/0/1/0/all/0/1&quot;&gt;Raghavv Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morales_C/0/1/0/all/0/1&quot;&gt;Cecilia Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Manpreet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dubrawski_A/0/1/0/all/0/1&quot;&gt;Artur Dubrawski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1&quot;&gt;John Galeotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choset_H/0/1/0/all/0/1&quot;&gt;Howie Choset&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01529">
<title>T3D: Towards 3D Medical Image Understanding through Vision-Language Pre-training. (arXiv:2312.01529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01529</link>
<description rdf:parseType="Literal">&lt;p&gt;Expert annotation of 3D medical image for downstream analysis is
resource-intensive, posing challenges in clinical applications. Visual
self-supervised learning (vSSL), though effective for learning visual
invariance, neglects the incorporation of domain knowledge from medicine. To
incorporate medical knowledge into visual representation learning,
vision-language pre-training (VLP) has shown promising results in 2D image.
However, existing VLP approaches become generally impractical when applied to
high-resolution 3D medical images due to GPU hardware constraints and the
potential loss of critical details caused by downsampling, which is the
intuitive solution to hardware constraints. To address the above limitations,
we introduce T3D, the first VLP framework designed for high-resolution 3D
medical images. T3D incorporates two text-informed pretext tasks:
(\lowerromannumeral{1}) text-informed contrastive learning;
(\lowerromannumeral{2}) text-informed image restoration. These tasks focus on
learning 3D visual representations from high-resolution 3D medical images and
integrating clinical knowledge from radiology reports, without distorting
information through forced alignment of downsampled volumes with detailed
anatomical text. Trained on a newly curated large-scale dataset of 3D medical
images and radiology reports, T3D significantly outperforms current vSSL
methods in tasks like organ and tumor segmentation, as well as disease
classification. This underlines T3D&apos;s potential in representation learning for
3D medical image analysis. All data and code will be available upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1&quot;&gt;Cheng Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yinda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quilodran_Casas_C/0/1/0/all/0/1&quot;&gt;Cesar C&amp;#xe9;sar Quilodr&amp;#xe1;n-Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Anand Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Wenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1&quot;&gt;Rossella Arcucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01656">
<title>The Contemporary Art of Image Search: Iterative User Intent Expansion via Vision-Language Model. (arXiv:2312.01656v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01656</link>
<description rdf:parseType="Literal">&lt;p&gt;Image search is an essential and user-friendly method to explore vast
galleries of digital images. However, existing image search methods heavily
rely on proximity measurements like tag matching or image similarity, requiring
precise user inputs for satisfactory results. To meet the growing demand for a
contemporary image search engine that enables accurate comprehension of users&apos;
search intentions, we introduce an innovative user intent expansion framework.
Our framework leverages visual-language models to parse and compose multi-modal
user inputs to provide more accurate and satisfying results. It comprises
two-stage processes: 1) a parsing stage that incorporates a language parsing
module with large language models to enhance the comprehension of textual
inputs, along with a visual parsing module that integrates an interactive
segmentation module to swiftly identify detailed visual elements within images;
and 2) a logic composition stage that combines multiple user search intents
into a unified logic expression for more sophisticated operations in complex
searching scenarios. Moreover, the intent expansion framework enables users to
perform flexible contextualized interactions with the search results to further
specify or adjust their detailed search intents iteratively. We implemented the
framework into an image search system for NFT (non-fungible token) search and
conducted a user study to evaluate its usability and novel properties. The
results indicate that the proposed framework significantly improves users&apos;
image search experience. Particularly the parsing and contextualized
interactions prove useful in allowing users to express their search intents
more accurately and engage in a more enjoyable iterative search experience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yilin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shishi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wei Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01677">
<title>Multi-task Image Restoration Guided By Robust DINO Features. (arXiv:2312.01677v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01677</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task image restoration has gained significant interest due to its
inherent versatility and efficiency compared to its single-task counterpart.
Despite its potential, performance degradation is observed with an increase in
the number of tasks, primarily attributed to the distinct nature of each
restoration task. Addressing this challenge, we introduce
\mbox{\textbf{DINO-IR}}, a novel multi-task image restoration approach
leveraging robust features extracted from DINOv2. Our empirical analysis shows
that while shallow features of DINOv2 capture rich low-level image
characteristics, the deep features ensure a robust semantic representation
insensitive to degradations while preserving high-frequency contour details.
Building on these features, we devise specialized components, including
multi-layer semantic fusion module, DINO-Restore adaption and fusion module,
and DINO perception contrastive loss, to integrate DINOv2 features into the
restoration paradigm. Equipped with the aforementioned components, our DINO-IR
performs favorably against existing multi-task image restoration approaches in
various tasks by a large margin, indicating the superiority and necessity of
reinforcing the robust features for multi-task image restoration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1&quot;&gt;Chao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1&quot;&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jinshan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01697">
<title>Hulk: A Universal Knowledge Translator for Human-Centric Tasks. (arXiv:2312.01697v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01697</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centric perception tasks, e.g., human mesh recovery, pedestrian
detection, skeleton-based action recognition, and pose estimation, have wide
industrial applications, such as metaverse and sports analysis. There is a
recent surge to develop human-centric foundation models that can benefit a
broad range of human-centric perception tasks. While many human-centric
foundation models have achieved success, most of them only excel in 2D vision
tasks or require extensive fine-tuning for practical deployment in real-world
scenarios. These limitations severely restrict their usability across various
downstream tasks and situations. To tackle these problems, we present Hulk, the
first multimodal human-centric generalist model, capable of addressing most of
the mainstream tasks simultaneously without task-specific finetuning, covering
2D vision, 3D vision, skeleton-based, and vision-language tasks. The key to
achieving this is condensing various task-specific heads into two general
heads, one for discrete representations, e.g., languages, and the other for
continuous representations, e.g., location coordinates. The outputs of two
heads can be further stacked into four distinct input and output modalities.
This uniform representation enables Hulk to treat human-centric tasks as
modality translation, integrating knowledge across a wide range of tasks. To
validate the effectiveness of our proposed method, we conduct comprehensive
experiments on 11 benchmarks across 8 human-centric tasks. Experimental results
surpass previous methods substantially, demonstrating the superiority of our
proposed method. The code will be available on
https://github.com/OpenGVLab/HumanBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shixiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weizhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02087">
<title>VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence. (arXiv:2312.02087v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02087</link>
<description rdf:parseType="Literal">&lt;p&gt;Current diffusion-based video editing primarily focuses on
structure-preserved editing by utilizing various dense correspondences to
ensure temporal consistency and motion alignment. However, these approaches are
often ineffective when the target edit involves a shape change. To embark on
video editing with shape change, we explore customized video subject swapping
in this work, where we aim to replace the main subject in a source video with a
target subject having a distinct identity and potentially different shape. In
contrast to previous methods that rely on dense correspondences, we introduce
the VideoSwap framework that exploits semantic point correspondences, inspired
by our observation that only a small number of semantic points are necessary to
align the subject&apos;s motion trajectory and modify its shape. We also introduce
various user-point interactions (\eg, removing points and dragging points) to
address various semantic point correspondence. Extensive experiments
demonstrate state-of-the-art video subject swapping results across a variety of
real-world videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuchao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yipin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Licheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia-Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jay Zhangjie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David Junhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kevin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02111">
<title>TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology. (arXiv:2312.02111v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02111</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational pathology models rarely utilise data that will not be available
for inference. This means most models cannot learn from highly informative data
such as additional immunohistochemical (IHC) stains and spatial
transcriptomics. We present TriDeNT, a novel self-supervised method for
utilising privileged data that is not available during inference to improve
performance. We demonstrate the efficacy of this method for a range of
different paired data including immunohistochemistry, spatial transcriptomics
and expert nuclei annotations. In all settings, TriDeNT outperforms other
state-of-the-art methods in downstream tasks, with observed improvements of up
to 101%. Furthermore, we provide qualitative and quantitative measurements of
the features learned by these models and how they differ from baselines.
TriDeNT offers a novel method to distil knowledge from scarce or costly data
during training, to create significantly better models for routine inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farndale_L/0/1/0/all/0/1&quot;&gt;Lucas Farndale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Insall_R/0/1/0/all/0/1&quot;&gt;Robert Insall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Ke Yuan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>