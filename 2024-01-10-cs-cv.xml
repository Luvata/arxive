<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.05074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.05941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.13492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.15462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.05401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.08076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02916" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.02960">
<title>Forensic Video Analytic Software. (arXiv:2401.02960v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.02960</link>
<description rdf:parseType="Literal">&lt;p&gt;Law enforcement officials heavily depend on Forensic Video Analytic (FVA)
Software in their evidence extraction process. However present-day FVA software
are complex, time consuming, equipment dependent and expensive. Developing
countries struggle to gain access to this gateway to a secure haven. The term
forensic pertains the application of scientific methods to the investigation of
crime through post-processing, whereas surveillance is the close monitoring of
real-time feeds.
&lt;/p&gt;
&lt;p&gt;The principle objective of this Final Year Project was to develop an
efficient and effective FVA Software, addressing the shortcomings through a
stringent and systematic review of scholarly research papers, online databases
and legal documentation. The scope spans multiple object detection, multiple
object tracking, anomaly detection, activity recognition, tampering detection,
general and specific image enhancement and video synopsis.
&lt;/p&gt;
&lt;p&gt;Methods employed include many machine learning techniques, GPU acceleration
and efficient, integrated architecture development both for real-time and
postprocessing. For this CNN, GMM, multithreading and OpenCV C++ coding were
used. The implications of the proposed methodology would rapidly speed up the
FVA process especially through the novel video synopsis research arena. This
project has resulted in three research outcomes Moving Object Based Collision
Free Video Synopsis, Forensic and Surveillance Analytic Tool Architecture and
Tampering Detection Inter-Frame Forgery.
&lt;/p&gt;
&lt;p&gt;The results include forensic and surveillance panel outcomes with emphasis on
video synopsis and Sri Lankan context. Principal conclusions include the
optimization and efficient algorithm integration to overcome limitations in
processing power, memory and compromise between real-time performance and
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnarajah_A/0/1/0/all/0/1&quot;&gt;Anton Jeran Ratnarajah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goonetilleke_S/0/1/0/all/0/1&quot;&gt;Sahani Goonetilleke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tissera_D/0/1/0/all/0/1&quot;&gt;Dumindu Tissera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balagopalan_K/0/1/0/all/0/1&quot;&gt;Kapilan Balagopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1&quot;&gt;Ranga Rodrigo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02961">
<title>A Surrogate-Assisted Extended Generative Adversarial Network for Parameter Optimization in Free-Form Metasurface Design. (arXiv:2401.02961v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.02961</link>
<description rdf:parseType="Literal">&lt;p&gt;Metasurfaces have widespread applications in fifth-generation (5G) microwave
communication. Among the metasurface family, free-form metasurfaces excel in
achieving intricate spectral responses compared to regular-shape counterparts.
However, conventional numerical methods for free-form metasurfaces are
time-consuming and demand specialized expertise. Alternatively, recent studies
demonstrate that deep learning has great potential to accelerate and refine
metasurface designs. Here, we present XGAN, an extended generative adversarial
network (GAN) with a surrogate for high-quality free-form metasurface designs.
The proposed surrogate provides a physical constraint to XGAN so that XGAN can
accurately generate metasurfaces monolithically from input spectral responses.
In comparative experiments involving 20000 free-form metasurface designs, XGAN
achieves 0.9734 average accuracy and is 500 times faster than the conventional
methodology. This method facilitates the metasurface library building for
specific spectral responses and can be extended to various inverse design
problems, including optical metamaterials, nanophotonic devices, and drug
discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1&quot;&gt;Manna Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Feng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattoraj_J/0/1/0/all/0/1&quot;&gt;Joyjit Chattoraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yingzhi Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinxing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weijiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1&quot;&gt;My Ha Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02962">
<title>Automated Localization of Blood Vessels in Retinal Images. (arXiv:2401.02962v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.02962</link>
<description rdf:parseType="Literal">&lt;p&gt;Vessel structure is one of the most important parts of the retina which
physicians can detect many diseases by analysing its features. Localization of
blood vessels in retina images is an important process in medical image
analysis. This process is also more challenging with the presence of bright and
dark lesions. In this thesis, two automated vessel localization methods to
handle both healthy and unhealthy (pathological) retina images are analyzed.
Each method consists of two major steps and the second step is the same in the
two methods. In the first step, an algorithm is used to decrease the effect of
bright lesions. In Method 1, this algorithm is based on K- Means segmentation,
and in Method 2, it is based on a regularization procedure. In the second step
of both methods, a multi-scale line operator is used to localize the
line-shaped vascular structures and ignore the dark lesions which are generally
assumed to have irregular patterns. After the introduction of the methods, a
detailed quantitative and qualitative comparison of the methods with one
another as well as the state-of-the-art solutions in the literature based on
the segmentation results on the images of the two publicly available datasets,
DRIVE and STARE, is reported. The results demonstrate that the methods are
highly comparable with other solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Safarzadeh_V/0/1/0/all/0/1&quot;&gt;Vahid Mohammadi Safarzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02995">
<title>CANAMRF: An Attention-Based Model for Multimodal Depression Detection. (arXiv:2401.02995v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02995</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal depression detection is an important research topic that aims to
predict human mental states using multimodal data. Previous methods treat
different modalities equally and fuse each modality by na\&quot;ive mathematical
operations without measuring the relative importance between them, which cannot
obtain well-performed multimodal representations for downstream depression
tasks. In order to tackle the aforementioned concern, we present a Cross-modal
Attention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for
multimodal depression detection. CANAMRF is constructed by a multimodal feature
extractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid
Attention Module. Through experimentation on two benchmark datasets, CANAMRF
demonstrates state-of-the-art performance, underscoring the effectiveness of
our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03002">
<title>Prompt-driven Latent Domain Generalization for Medical Image Classification. (arXiv:2401.03002v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03002</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models for medical image analysis easily suffer from
distribution shifts caused by dataset artifacts bias, camera variations,
differences in the imaging station, etc., leading to unreliable diagnoses in
real-world clinical settings. Domain generalization (DG) methods, which aim to
train models on multiple domains to perform well on unseen domains, offer a
promising direction to solve the problem. However, existing DG methods assume
domain labels of each image are available and accurate, which is typically
feasible for only a limited number of medical datasets. To address these
challenges, we propose a novel DG framework for medical image classification
without relying on domain labels, called Prompt-driven Latent Domain
Generalization (PLDG). PLDG consists of unsupervised domain discovery and
prompt learning. This framework first discovers pseudo domain labels by
clustering the bias-associated style features, then leverages collaborative
domain prompts to guide a Vision Transformer to learn knowledge from discovered
diverse domains. To facilitate cross-domain knowledge learning between
different prompts, we introduce a domain prompt generator that enables
knowledge sharing between domain prompts and a shared prompt. A domain mixup
strategy is additionally employed for more flexible decision margins and
mitigates the risk of incorrect domain assignments. Extensive experiments on
three medical image classification tasks and one debiasing task demonstrate
that our method can achieve comparable or even superior performance than
conventional DG algorithms without relying on domain labels. Our code will be
publicly available upon the paper is accepted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Siyuan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_L/0/1/0/all/0/1&quot;&gt;Lie Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1&quot;&gt;Dwarikanath Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Betz_Stablein_B/0/1/0/all/0/1&quot;&gt;Brigid Betz-Stablein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mar_V/0/1/0/all/0/1&quot;&gt;Victoria Mar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janda_M/0/1/0/all/0/1&quot;&gt;Monika Janda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soyer_P/0/1/0/all/0/1&quot;&gt;Peter Soyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03005">
<title>Evolution of urban areas and land surface temperature. (arXiv:2401.03005v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/2401.03005</link>
<description rdf:parseType="Literal">&lt;p&gt;With the global population on the rise, our cities have been expanding to
accommodate the growing number of people. The expansion of cities generally
leads to the engulfment of peripheral areas. However, such expansion of urban
areas is likely to cause increment in areas with increased land surface
temperature (LST). By considering each summer as a data point, we form LST
multi-year time-series and cluster it to obtain spatio-temporal pattern. We
observe several interesting phenomena from these patterns, e.g., some clusters
show reasonable similarity to the built-up area, whereas the locations with
high temporal variation are seen more in the peripheral areas. Furthermore, the
LST center of mass shifts over the years for cities with development activities
tilted towards a direction. We conduct the above-mentioned studies for three
different cities in three different continents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Sudipan Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Verma_T/0/1/0/all/0/1&quot;&gt;Tushar Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Oliveira_D/0/1/0/all/0/1&quot;&gt;Dario Augusto Borges Oliveira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03037">
<title>CATFace: Cross-Attribute-Guided Transformer with Self-Attention Distillation for Low-Quality Face Recognition. (arXiv:2401.03037v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03037</link>
<description rdf:parseType="Literal">&lt;p&gt;Although face recognition (FR) has achieved great success in recent years, it
is still challenging to accurately recognize faces in low-quality images due to
the obscured facial details. Nevertheless, it is often feasible to make
predictions about specific soft biometric (SB) attributes, such as gender, and
baldness even in dealing with low-quality images. In this paper, we propose a
novel multi-branch neural network that leverages SB attribute information to
boost the performance of FR. To this end, we propose a cross-attribute-guided
transformer fusion (CATF) module that effectively captures the long-range
dependencies and relationships between FR and SB feature representations. The
synergy created by the reciprocal flow of information in the dual
cross-attention operations of the proposed CATF module enhances the performance
of FR. Furthermore, we introduce a novel self-attention distillation framework
that effectively highlights crucial facial regions, such as landmarks by
aligning low-quality images with those of their high-quality counterparts in
the feature space. The proposed self-attention distillation regularizes our
network to learn a unified quality-invariant feature representation in
unconstrained environments. We conduct extensive experiments on various FR
benchmarks varying in quality. Experimental results demonstrate the superiority
of our FR method compared to state-of-the-art FR studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talemi_N/0/1/0/all/0/1&quot;&gt;Niloufar Alipour Talemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashiani_H/0/1/0/all/0/1&quot;&gt;Hossein Kashiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1&quot;&gt;Nasser M. Nasrabadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03040">
<title>AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis. (arXiv:2401.03040v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03040</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic accident analysis is pivotal for enhancing public safety and
developing road regulations. Traditional approaches, although widely used, are
often constrained by manual analysis processes, subjective decisions, uni-modal
outputs, as well as privacy issues related to sensitive data. This paper
introduces the idea of AccidentGPT, a foundation model of traffic accident
analysis, which incorporates multi-modal input data to automatically
reconstruct the accident process video with dynamics details, and furthermore
provide multi-task analysis with multi-modal outputs. The design of the
AccidentGPT is empowered with a multi-modality prompt with feedback for
task-oriented adaptability, a hybrid training schema to leverage labelled and
unlabelled data, and a edge-cloud split configuration for data privacy. To
fully realize the functionalities of this model, we proposes several research
opportunities. This paper serves as the stepping stone to fill the gaps in
traditional approaches of traffic accident analysis and attract the research
community attention for automatic, objective, and privacy-preserving traffic
accident analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kebin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03043">
<title>Learning Multimodal Volumetric Features for Large-Scale Neuron Tracing. (arXiv:2401.03043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03043</link>
<description rdf:parseType="Literal">&lt;p&gt;The current neuron reconstruction pipeline for electron microscopy (EM) data
usually includes automatic image segmentation followed by extensive human
expert proofreading. In this work, we aim to reduce human workload by
predicting connectivity between over-segmented neuron pieces, taking both
microscopy image and 3D morphology features into account, similar to human
proofreading workflow. To this end, we first construct a dataset, named
FlyTracing, that contains millions of pairwise connections of segments
expanding the whole fly brain, which is three orders of magnitude larger than
existing datasets for neuron segment connection. To learn sophisticated
biological imaging features from the connectivity annotations, we propose a
novel connectivity-aware contrastive learning method to generate dense
volumetric EM image embedding. The learned embeddings can be easily
incorporated with any point or voxel-based morphological representations for
automatic neuron tracing. Extensive comparisons of different combination
schemes of image and morphological representation in identifying split errors
across the whole fly brain demonstrate the superiority of the proposed
approach, especially for the locations that contain severe imaging artifacts,
such as section missing and misalignment. The dataset and code are available at
https://github.com/Levishery/Flywire-Neuron-Tracing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qihua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuejin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03048">
<title>Latte: Latent Diffusion Transformer for Video Generation. (arXiv:2401.03048v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03048</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel Latent Diffusion Transformer, namely Latte, for video
generation. Latte first extracts spatio-temporal tokens from input videos and
then adopts a series of Transformer blocks to model video distribution in the
latent space. In order to model a substantial number of tokens extracted from
videos, four efficient variants are introduced from the perspective of
decomposing the spatial and temporal dimensions of input videos. To improve the
quality of generated videos, we determine the best practices of Latte through
rigorous experimental analysis, including video clip patch embedding, model
variants, timestep-class information injection, temporal positional embedding,
and learning strategies. Our comprehensive evaluation demonstrates that Latte
achieves state-of-the-art performance across four standard video generation
datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In
addition, we extend Latte to text-to-video generation (T2V) task, where Latte
achieves comparable results compared to recent T2V models. We strongly believe
that Latte provides valuable insights for future research on incorporating
Transformers into diffusion models for video generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1&quot;&gt;Gengyun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cunjian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03060">
<title>Super-Resolution Multi-Contrast Unbiased Eye Atlases With Deep Probabilistic Refinement. (arXiv:2401.03060v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03060</link>
<description rdf:parseType="Literal">&lt;p&gt;Eye morphology varies significantly across the population, especially for the
orbit and optic nerve. These variations limit the feasibility and robustness of
generalizing population-wise features of eye organs to an unbiased spatial
reference. To tackle these limitations, we propose a process for creating
high-resolution unbiased eye atlases. First, to restore spatial details from
scans with a low through-plane resolution compared to a high in-plane
resolution, we apply a deep learning-based super-resolution algorithm. Then, we
generate an initial unbiased reference with an iterative metric-based
registration using a small portion of subject scans. We register the remaining
scans to this template and refine the template using an unsupervised deep
probabilistic approach that generates a more expansive deformation field to
enhance the organ boundary alignment. We demonstrate this framework using
magnetic resonance images across four different MRI tissue contrasts,
generating four atlases in separate spatial alignments. For each tissue
contrast, we find a significant improvement in the average Dice score across
four labeled regions compared to a standard registration framework consisting
of rigid, affine, and deformable transformations. These results highlight the
effective alignment of eye organs and boundaries using our proposed process. By
combining super-resolution preprocessing and deep probabilistic models, we
address the challenge of generating an eye atlas to serve as a standardized
reference across a largely variable population.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saunders_A/0/1/0/all/0/1&quot;&gt;Adam M. Saunders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Michael E. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Remedios_S/0/1/0/all/0/1&quot;&gt;Samuel W. Remedios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yucheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bao_S/0/1/0/all/0/1&quot;&gt;Shunxing Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cho_C/0/1/0/all/0/1&quot;&gt;Chloe Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mawn_L/0/1/0/all/0/1&quot;&gt;Louise A. Mawn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rex_T/0/1/0/all/0/1&quot;&gt;Tonia S. Rex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schey_K/0/1/0/all/0/1&quot;&gt;Kevin L. Schey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dewey_B/0/1/0/all/0/1&quot;&gt;Blake E. Dewey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Spraggins_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Spraggins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prince_J/0/1/0/all/0/1&quot;&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1&quot;&gt;Yuankai Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1&quot;&gt;Bennett A. Landman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03070">
<title>Traffic Cameras to detect inland waterway barge traffic: An Application of machine learning. (arXiv:2401.03070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03070</link>
<description rdf:parseType="Literal">&lt;p&gt;Inland waterways are critical for freight movement, but limited means exist
for monitoring their performance and usage by freight-carrying vessels, e.g.,
barges. While methods to track vessels, e.g., tug and tow boats, are publicly
available through Automatic Identification Systems (AIS), ways to track freight
tonnages and commodity flows carried on barges along these critical marine
highways are non-existent, especially in real-time settings. This paper
develops a method to detect barge traffic on inland waterways using existing
traffic cameras with opportune viewing angles. Deep learning models,
specifically, You Only Look Once (YOLO), Single Shot MultiBox Detector (SSD),
and EfficientDet are employed. The model detects the presence of vessels and/or
barges from video and performs a classification (no vessel or barge, vessel
without barge, vessel with barge, and barge). A dataset of 331 annotated images
was collected from five existing traffic cameras along the Mississippi and Ohio
Rivers for model development. YOLOv8 achieves an F1-score of 96%, outperforming
YOLOv5, SSD, and EfficientDet models with 86%, 79%, and 77% respectively.
Sensitivity analysis was carried out regarding weather conditions (fog and
rain) and location (Mississippi and Ohio rivers). A background subtraction
technique was used to normalize video images across the various locations for
the location sensitivity analysis. This model can be used to detect the
presence of barges along river segments, which can be used for anonymous bulk
commodity tracking and monitoring. Such data is valuable for long-range
transportation planning efforts carried out by public transportation agencies,
in addition to operational and maintenance planning conducted by federal
agencies such as the US Army Corp of Engineers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agorku_G/0/1/0/all/0/1&quot;&gt;Geoffery Agorku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+PhD_S/0/1/0/all/0/1&quot;&gt;Sarah Hernandez PhD&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falquez_M/0/1/0/all/0/1&quot;&gt;Maria Falquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+PhD_S/0/1/0/all/0/1&quot;&gt;Subhadipto Poddar PhD&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amankwah_Nkyi_K/0/1/0/all/0/1&quot;&gt;Kwadwo Amankwah-Nkyi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03085">
<title>Consensus-Threshold Criterion for Offline Signature Verification using Convolutional Neural Network Learned Representations. (arXiv:2401.03085v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03085</link>
<description rdf:parseType="Literal">&lt;p&gt;A genuine signer&apos;s signature is naturally unstable even at short
time-intervals whereas, expert forgers always try to perfectly mimic a genuine
signer&apos;s signature. This presents a challenge which puts a genuine signer at
risk of being denied access, while a forge signer is granted access. The
implication is a high false acceptance rate (FAR) which is the percentage of
forge signature classified as belonging to a genuine class. Existing work have
only scratched the surface of signature verification because the
misclassification error remains high. In this paper, a consensus-threshold
distance-based classifier criterion is proposed for offline writer-dependent
signature verification. Using features extracted from SigNet and SigNet-F deep
convolutional neural network models, the proposed classifier minimizes FAR.
This is demonstrated via experiments on four datasets: GPDS-300, MCYT, CEDAR
and Brazilian PUC-PR datasets. On GPDS-300, the consensus threshold classifier
improves the state-of-the-art performance by achieving a 1.27% FAR compared to
8.73% and 17.31% recorded in literature. This performance is consistent across
other datasets and guarantees that the risk of imposters gaining access to
sensitive documents or transactions is minimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brimoh_P/0/1/0/all/0/1&quot;&gt;Paul Brimoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olisah_C/0/1/0/all/0/1&quot;&gt;Chollette C. Olisah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03105">
<title>Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models. (arXiv:2401.03105v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03105</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) are experiencing rapid growth,
yielding a plethora of noteworthy contributions in recent months. The
prevailing trend involves adopting data-driven methodologies, wherein diverse
instruction-following datasets are collected. However, a prevailing challenge
persists in these approaches, specifically in relation to the limited visual
perception ability, as CLIP-like encoders employed for extracting visual
information from inputs. Though these encoders are pre-trained on billions of
image-text pairs, they still grapple with the information loss dilemma, given
that textual captions only partially capture the contents depicted in images.
To address this limitation, this paper proposes to improve the visual
perception ability of MLLMs through a mixture-of-experts knowledge enhancement
mechanism. Specifically, we introduce a novel method that incorporates
multi-task encoders and visual tools into the existing MLLMs training and
inference pipeline, aiming to provide a more comprehensive and accurate
summarization of visual inputs. Extensive experiments have evaluated its
effectiveness of advancing MLLMs, showcasing improved visual perception
achieved through the integration of visual experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Longhui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lingxi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03108">
<title>Dress-Me-Up: A Dataset &amp; Method for Self-Supervised 3D Garment Retargeting. (arXiv:2401.03108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03108</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel self-supervised framework for retargeting
non-parameterized 3D garments onto 3D human avatars of arbitrary shapes and
poses, enabling 3D virtual try-on (VTON). Existing self-supervised 3D
retargeting methods only support parametric and canonical garments, which can
only be draped over parametric body, e.g. SMPL. To facilitate the
non-parametric garments and body, we propose a novel method that introduces
Isomap Embedding based correspondences matching between the garment and the
human body to get a coarse alignment between the two meshes. We perform neural
refinement of the coarse alignment in a self-supervised setting. Further, we
leverage a Laplacian detail integration method for preserving the inherent
details of the input garment. For evaluating our 3D non-parametric garment
retargeting framework, we propose a dataset of 255 real-world garments with
realistic noise and topological deformations. The dataset contains $44$ unique
garments worn by 15 different subjects in 5 distinctive poses, captured using a
multi-view RGBD capture setup. We show superior retargeting quality on
non-parametric garments and human avatars over existing state-of-the-art
methods, acting as the first-ever baseline on the proposed dataset for
non-parametric 3D garment retargeting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1&quot;&gt;Shanthika Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Kunwar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Astitva Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirikonda_D/0/1/0/all/0/1&quot;&gt;Dhawal Sirikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Amit Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Avinash Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03115">
<title>Transferable Learned Image Compression-Resistant Adversarial Perturbations. (arXiv:2401.03115v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03115</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks can readily disrupt the image classification system,
revealing the vulnerability of DNN-based recognition tasks. While existing
adversarial perturbations are primarily applied to uncompressed images or
compressed images by the traditional image compression method, i.e., JPEG,
limited studies have investigated the robustness of models for image
classification in the context of DNN-based image compression. With the rapid
evolution of advanced image compression, DNN-based learned image compression
has emerged as the promising approach for transmitting images in many
security-critical applications, such as cloud-based face recognition and
autonomous driving, due to its superior performance over traditional
compression. Therefore, there is a pressing need to fully investigate the
robustness of a classification system post-processed by learned image
compression. To bridge this research gap, we explore the adversarial attack on
a new pipeline that targets image classification models that utilize learned
image compressors as pre-processing modules. Furthermore, to enhance the
transferability of perturbations across various quality levels and
architectures of learned image compression models, we introduce a saliency
score-based sampling method to enable the fast generation of transferable
perturbation. Extensive experiments with popular attack methods demonstrate the
enhanced transferability of our proposed method when attacking images that have
been post-processed with different learned image compression models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yang Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuohang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1&quot;&gt;Ding Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03122">
<title>SAR Despeckling via Regional Denoising Diffusion Probabilistic Model. (arXiv:2401.03122v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03122</link>
<description rdf:parseType="Literal">&lt;p&gt;Speckle noise poses a significant challenge in maintaining the quality of
synthetic aperture radar (SAR) images, so SAR despeckling techniques have drawn
increasing attention. Despite the tremendous advancements of deep learning in
fixed-scale SAR image despeckling, these methods still struggle to deal with
large-scale SAR images. To address this problem, this paper introduces a novel
despeckling approach termed Region Denoising Diffusion Probabilistic Model
(R-DDPM) based on generative models. R-DDPM enables versatile despeckling of
SAR images across various scales, accomplished within a single training
session. Moreover, The artifacts in the fused SAR images can be avoided
effectively with the utilization of region-guided inverse sampling. Experiments
of our proposed R-DDPM on Sentinel-1 data demonstrates superior performance to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xuran Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhengpeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingzhe Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stankovic_L/0/1/0/all/0/1&quot;&gt;LJubisa Stankovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03131">
<title>A Physics-guided Generative AI Toolkit for Geophysical Monitoring. (arXiv:2401.03131v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03131</link>
<description rdf:parseType="Literal">&lt;p&gt;Full-waveform inversion (FWI) plays a vital role in geoscience to explore the
subsurface. It utilizes the seismic wave to image the subsurface velocity map.
As the machine learning (ML) technique evolves, the data-driven approaches
using ML for FWI tasks have emerged, offering enhanced accuracy and reduced
computational cost compared to traditional physics-based methods. However, a
common challenge in geoscience, the unprivileged data, severely limits ML
effectiveness. The issue becomes even worse during model pruning, a step
essential in geoscience due to environmental complexities. To tackle this, we
introduce the EdGeo toolkit, which employs a diffusion-based model guided by
physics principles to generate high-fidelity velocity maps. The toolkit uses
the acoustic wave equation to generate corresponding seismic waveform data,
facilitating the fine-tuning of pruned ML models. Our results demonstrate
significant improvements in SSIM scores and reduction in both MAE and MSE
across various pruning ratios. Notably, the ML model fine-tuned using data
generated by EdGeo yields superior quality of velocity maps, especially in
representing unprivileged features, outperforming other existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junhuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yi Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youzuo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03132">
<title>Vision Transformers and Bi-LSTM for Alzheimer&apos;s Disease Diagnosis from 3D MRI. (arXiv:2401.03132v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03132</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s is a brain disease that gets worse over time and affects memory,
thinking, and behavior. Alzheimer&apos;s disease (AD) can be treated and managed if
it is diagnosed early, which can slow the progression of symptoms and improve
quality of life. In this study, we suggested using the Visual Transformer (ViT)
and bi-LSTM to process MRI images for diagnosing Alzheimer&apos;s disease. We used
ViT to extract features from the MRI and then map them to a feature sequence.
Then, we used Bi-LSTM sequence modeling to keep the interdependencies between
related features. In addition, we evaluated the performance of the proposed
model for the binary classification of AD patients using data from the
Alzheimer&apos;s Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our
method against other deep learning models in the literature. The proposed
method performs well in terms of accuracy, precision, F-score, and recall for
the diagnosis of AD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akan_T/0/1/0/all/0/1&quot;&gt;Taymaz Akan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alp_S/0/1/0/all/0/1&quot;&gt;Sait Alp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhuiyanb_M/0/1/0/all/0/1&quot;&gt;Mohammad A. N Bhuiyanb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03142">
<title>Explicit Visual Prompts for Visual Object Tracking. (arXiv:2401.03142v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03142</link>
<description rdf:parseType="Literal">&lt;p&gt;How to effectively exploit spatio-temporal information is crucial to capture
target appearance changes in visual tracking. However, most deep learning-based
trackers mainly focus on designing a complicated appearance model or template
updating strategy, while lacking the exploitation of context between
consecutive frames and thus entailing the \textit{when-and-how-to-update}
dilemma. To address these issues, we propose a novel explicit visual prompts
framework for visual tracking, dubbed \textbf{EVPTrack}. Specifically, we
utilize spatio-temporal tokens to propagate information between consecutive
frames without focusing on updating templates. As a result, we cannot only
alleviate the challenge of \textit{when-to-update}, but also avoid the
hyper-parameters associated with updating strategies. Then, we utilize the
spatio-temporal tokens to generate explicit visual prompts that facilitate
inference in the current frame. The prompts are fed into a transformer encoder
together with the image tokens without additional processing. Consequently, the
efficiency of our model is improved by avoiding \textit{how-to-update}. In
addition, we consider multi-scale information as explicit visual prompts,
providing multiscale template features to enhance the EVPTrack&apos;s ability to
handle target scale changes. Extensive experimental results on six benchmarks
(i.e., LaSOT, LaSOT\rm $_{ext}$, GOT-10k, UAV123, TrackingNet, and TNL2K.)
validate that our EVPTrack can achieve competitive performance at a real-time
speed by effectively exploiting both spatio-temporal and multi-scale
information. Code and models are available at
https://github.com/GXNU-ZhongLab/EVPTrack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Liangtao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_B/0/1/0/all/0/1&quot;&gt;Bineng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1&quot;&gt;Qihua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Ning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianxian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03145">
<title>Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection. (arXiv:2401.03145v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03145</link>
<description rdf:parseType="Literal">&lt;p&gt;Industrial anomaly detection is generally addressed as an unsupervised task
that aims at locating defects with only normal training samples. Recently,
numerous 2D anomaly detection methods have been proposed and have achieved
promising results, however, using only the 2D RGB data as input is not
sufficient to identify imperceptible geometric surface anomalies. Hence, in
this work, we focus on multi-modal anomaly detection. Specifically, we
investigate early multi-modal approaches that attempted to utilize models
pre-trained on large-scale visual datasets, i.e., ImageNet, to construct
feature databases. And we empirically find that directly using these
pre-trained models is not optimal, it can either fail to detect subtle defects
or mistake abnormal features as normal ones. This may be attributed to the
domain gap between target industrial data and source data.Towards this problem,
we propose a Local-to-global Self-supervised Feature Adaptation (LSFA) method
to finetune the adaptors and learn task-oriented representation toward anomaly
detection.Both intra-modal adaptation and cross-modal alignment are optimized
from a local-to-global perspective in LSFA to ensure the representation quality
and consistency in the inference stage.Extensive experiments demonstrate that
our method not only brings a significant performance boost to feature embedding
based approaches, but also outperforms previous State-of-The-Art (SoTA) methods
prominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves
97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1&quot;&gt;Yuanpeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Boshen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenhai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Cai Rong Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03149">
<title>CaMML: Context-Aware Multimodal Learner for Large Models. (arXiv:2401.03149v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03149</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce Context-Aware MultiModal Learner (CaMML), for
tuning large multimodal models (LMMs). CaMML, a lightweight module, is crafted
to seamlessly integrate multimodal contextual samples into large models,
thereby empowering the model to derive knowledge from analogous,
domain-specific, up-to-date information and make grounded inferences.
Importantly, CaMML is highly scalable and can efficiently handle lengthy
multimodal context examples owing to its hierarchical design. Based on CaMML,
we have developed two multimodal models, CaMML-7B and CaMML-13B, that have
shown exceptional performance across an array of benchmark datasets for
multimodal tasks. Remarkably, CaMML-13B achieves the state-of-the-art
performance on over ten widely recognized multimodal benchmark datasets,
surpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any
external resources. Moreover, we have conducted extensive ablative studies to
inspect the inner workings of CaMML and performed qualitative analyses to
showcase its effectiveness in handling real-world challenging cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Boran Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03152">
<title>Controllable Image Synthesis of Industrial Data Using Stable Diffusion. (arXiv:2401.03152v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03152</link>
<description rdf:parseType="Literal">&lt;p&gt;Training supervised deep neural networks that perform defect detection and
segmentation requires large-scale fully-annotated datasets, which can be hard
or even impossible to obtain in industrial environments. Generative AI offers
opportunities to enlarge small industrial datasets artificially, thus enabling
the usage of state-of-the-art supervised approaches in the industry.
Unfortunately, also good generative models need a lot of data to train, while
industrial datasets are often tiny. Here, we propose a new approach for reusing
general-purpose pre-trained generative models on industrial data, ultimately
allowing the generation of self-labelled defective images. First, we let the
model learn the new concept, entailing the novel data distribution. Then, we
force it to learn to condition the generative process, producing industrial
images that satisfy well-defined topological characteristics and show defects
with a given geometry and location. To highlight the advantage of our approach,
we use the synthetic dataset to optimise a crack segmentor for a real
industrial use case. When the available data is small, we observe considerable
performance increase under several metrics, showing the method&apos;s potential in
production environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1&quot;&gt;Gabriele Valvano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agostino_A/0/1/0/all/0/1&quot;&gt;Antonino Agostino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magistris_G/0/1/0/all/0/1&quot;&gt;Giovanni De Magistris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziano_A/0/1/0/all/0/1&quot;&gt;Antonino Graziano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veneri_G/0/1/0/all/0/1&quot;&gt;Giacomo Veneri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03153">
<title>An Event-Oriented Diffusion-Refinement Method for Sparse Events Completion. (arXiv:2401.03153v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03153</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras or dynamic vision sensors (DVS) record asynchronous response to
brightness changes instead of conventional intensity frames, and feature
ultra-high sensitivity at low bandwidth. The new mechanism demonstrates great
advantages in challenging scenarios with fast motion and large dynamic range.
However, the recorded events might be highly sparse due to either limited
hardware bandwidth or extreme photon starvation in harsh environments. To
unlock the full potential of event cameras, we propose an inventive event
sequence completion approach conforming to the unique characteristics of event
data in both the processing stage and the output form. Specifically, we treat
event streams as 3D event clouds in the spatiotemporal domain, develop a
diffusion-based generative model to generate dense clouds in a coarse-to-fine
manner, and recover exact timestamps to maintain the temporal resolution of raw
data successfully. To validate the effectiveness of our method comprehensively,
we perform extensive experiments on three widely used public datasets with
different spatial resolutions, and additionally collect a novel event dataset
covering diverse scenarios with highly dynamic motions and under harsh
illumination. Besides generating high-quality dense events, our method can
benefit downstream applications such as object classification and intensity
frame reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuqi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1&quot;&gt;Jinli Suo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1&quot;&gt;Qionghai Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03157">
<title>ImageLab: Simplifying Image Processing Exploration for Novices and Experts Alike. (arXiv:2401.03157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03157</link>
<description rdf:parseType="Literal">&lt;p&gt;Image processing holds immense potential for societal benefit, yet its full
potential is often accessible only to tech-savvy experts. Bridging this
knowledge gap and providing accessible tools for users of all backgrounds
remains an unexplored frontier. This paper introduces &quot;ImageLab,&quot; a novel tool
designed to democratize image processing, catering to both novices and experts
by prioritizing interactive learning over theoretical complexity. ImageLab not
only serves as a valuable educational resource but also offers a practical
testing environment for seasoned practitioners. Through a comprehensive
evaluation of ImageLab&apos;s features, we demonstrate its effectiveness through a
user study done for a focused group of school children and university students
which enables us to get positive feedback on the tool. Our work represents a
significant stride toward enhancing image processing education and practice,
making it more inclusive and approachable for all.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dissanayaka_S/0/1/0/all/0/1&quot;&gt;Sahan Dissanayaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudanayaka_O/0/1/0/all/0/1&quot;&gt;Oshan Mudanayaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halloluwa_T/0/1/0/all/0/1&quot;&gt;Thilina Halloluwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1&quot;&gt;Chameera De Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03166">
<title>Short-Time Fourier Transform for deblurring Variational Autoencoders. (arXiv:2401.03166v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03166</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational Autoencoders (VAEs) are powerful generative models, however their
generated samples are known to suffer from a characteristic blurriness, as
compared to the outputs of alternative generating techniques. Extensive
research efforts have been made to tackle this problem, and several works have
focused on modifying the reconstruction term of the evidence lower bound
(ELBO). In particular, many have experimented with augmenting the
reconstruction loss with losses in the frequency domain. Such loss functions
usually employ the Fourier transform to explicitly penalise the lack of higher
frequency components in the generated samples, which are responsible for sharp
visual features. In this paper, we explore the aspects of previous such
approaches which aren&apos;t well understood, and we propose an augmentation to the
reconstruction term in response to them. Our reasoning leads us to use the
short-time Fourier transform and to emphasise on local phase coherence between
the input and output samples. We illustrate the potential of our proposed loss
on the MNIST dataset by providing both qualitative and quantitative results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dalal_V/0/1/0/all/0/1&quot;&gt;Vibhu Dalal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03167">
<title>PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations. (arXiv:2401.03167v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03167</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud registration is a crucial technique in 3D computer vision with a
wide range of applications. However, this task can be challenging, particularly
in large fields of view with dynamic objects, environmental noise, or other
perturbations. To address this challenge, we propose a model called PosDiffNet.
Our approach performs hierarchical registration based on window-level,
patch-level, and point-level correspondence. We leverage a graph neural partial
differential equation (PDE) based on Beltrami flow to obtain high-dimensional
features and position embeddings for point clouds. We incorporate position
embeddings into a Transformer module based on a neural ordinary differential
equation (ODE) to efficiently represent patches within points. We employ the
multi-level correspondence derived from the high feature similarity scores to
facilitate alignment between point clouds. Subsequently, we use registration
methods such as SVD-based algorithms to predict the transformation using
corresponding point pairs. We evaluate PosDiffNet on several 3D point cloud
datasets, verifying that it achieves state-of-the-art (SOTA) performance for
point cloud registration in large fields of view with perturbations. The
implementation code of experiments is available at
https://github.com/AI-IT-AVs/PosDiffNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_R/0/1/0/all/0/1&quot;&gt;Rui She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qiyu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1&quot;&gt;Tianyu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_X/0/1/0/all/0/1&quot;&gt;Xingchao Jian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03170">
<title>Preserving Silent Features for Domain Generalization. (arXiv:2401.03170v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03170</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) aims to improve the generalization ability of the
model trained on several known training domains over unseen test domains.
Previous work has shown that self-supervised contrastive pre-training improves
the robustness of the model on downstream tasks. However, in this paper, we
find that self-supervised models do not exhibit better generalization
performance than supervised models pre-trained on the same dataset in the DG
setting. We argue that this is owing to the fact that the richer intra-class
discriminative features extracted by self-supervised contrastive learning,
which we term silent features, are suppressed during supervised fine-tuning.
These silent features are likely to contain features that are more
generalizable on the test domain. In this work, we model and analyze this
feature suppression phenomenon and theoretically prove that preserving silent
features can achieve lower expected test domain risk under certain conditions.
In light of this, we propose a simple yet effective method termed STEP (Silent
Feature Preservation) to improve the generalization performance of the
self-supervised contrastive learning pre-trained model by alleviating the
suppression of silent features during the supervised fine-tuning process.
Experimental results show that STEP exhibits state-of-the-art performance on
standard DG benchmarks with significant distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chujie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianren Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03173">
<title>UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis. (arXiv:2401.03173v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03173</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, breast ultrasound has emerged as a crucial
diagnostic tool for early detection of breast cancer. However, the accuracy of
diagnosing the location of the affected area and the extent of the disease
depends on the experience of the physician. In this paper, we propose a novel
model called UGGNet, combining the power of the U-Net and VGG architectures to
enhance the performance of breast ultrasound image analysis. The U-Net
component of the model helps accurately segment the lesions, while the VGG
component utilizes deep convolutional layers to extract features. The fusion of
these two architectures in UGGNet aims to optimize both segmentation and
feature representation, providing a comprehensive solution for accurate
diagnosis in breast ultrasound images. Experimental results have demonstrated
that the UGGNet model achieves a notable accuracy of 78.2% on the &quot;Breast
Ultrasound Images Dataset.&quot;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Minh_T/0/1/0/all/0/1&quot;&gt;Tran Cao Minh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Quoc_N/0/1/0/all/0/1&quot;&gt;Nguyen Kim Quoc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vinh_P/0/1/0/all/0/1&quot;&gt;Phan Cong Vinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Phu_D/0/1/0/all/0/1&quot;&gt;Dang Nhu Phu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_V/0/1/0/all/0/1&quot;&gt;Vuong Xuan Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Ha Minh Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03177">
<title>Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks. (arXiv:2401.03177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03177</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-video retrieval is a challenging task that aims to identify relevant
videos given textual queries. Compared to conventional textual retrieval, the
main obstacle for text-video retrieval is the semantic gap between the textual
nature of queries and the visual richness of video content. Previous works
primarily focus on aligning the query and the video by finely aggregating
word-frame matching signals. Inspired by the human cognitive process of
modularly judging the relevance between text and video, the judgment needs
high-order matching signal due to the consecutive and complex nature of video
contents. In this paper, we propose chunk-level text-video matching, where the
query chunks are extracted to describe a specific retrieval unit, and the video
chunks are segmented into distinct clips from videos. We formulate the
chunk-level matching as n-ary correlations modeling between words of the query
and frames of the video and introduce a multi-modal hypergraph for n-ary
correlation modeling. By representing textual units and video frames as nodes
and using hyperedges to depict their relationships, a multi-modal hypergraph is
constructed. In this way, the query and the video can be aligned in a
high-order semantic space. In addition, to enhance the model&apos;s generalization
ability, the extracted features are fed into a variational inference component
for computation, obtaining the variational representation under the Gaussian
distribution. The incorporation of hypergraphs and variational inference allows
our model to capture complex, n-ary interactions among textual and visual
contents. Experimental results demonstrate that our proposed method achieves
state-of-the-art performance on the text-video retrieval task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Lixin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiashu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Long Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hengyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Suqi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hengzhu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03179">
<title>Multimodal Informative ViT: Information Aggregation and Distribution for Hyperspectral and LiDAR Classification. (arXiv:2401.03179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03179</link>
<description rdf:parseType="Literal">&lt;p&gt;In multimodal land cover classification (MLCC), a common challenge is the
redundancy in data distribution, where irrelevant information from multiple
modalities can hinder the effective integration of their unique features. To
tackle this, we introduce the Multimodal Informative Vit (MIVit), a system with
an innovative information aggregate-distributing mechanism. This approach
redefines redundancy levels and integrates performance-aware elements into the
fused representation, facilitating the learning of semantics in both forward
and backward directions. MIVit stands out by significantly reducing redundancy
in the empirical distribution of each modality&apos;s separate and fused features.
It employs oriented attention fusion (OAF) for extracting shallow local
features across modalities in horizontal and vertical dimensions, and a
Transformer feature extractor for extracting deep global features through
long-range attention. We also propose an information aggregation constraint
(IAC) based on mutual information, designed to remove redundant information and
preserve complementary information within embedded features. Additionally, the
information distribution flow (IDF) in MIVit enhances performance-awareness by
distributing global classification information across different modalities&apos;
feature maps. This architecture also addresses missing modality challenges with
lightweight independent modality classifiers, reducing the computational load
typically associated with Transformers. Our results show that MIVit&apos;s
bidirectional aggregate-distributing mechanism between modalities is highly
effective, achieving an average overall accuracy of 95.56% across three
multimodal datasets. This performance surpasses current state-of-the-art
methods in MLCC. The code for MIVit is accessible at
https://github.com/icey-zhang/MIViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jie Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Geng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daixun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seghouane_K/0/1/0/all/0/1&quot;&gt;Karim Seghouane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03182">
<title>Distribution-aware Interactive Attention Network and Large-scale Cloud Recognition Benchmark on FY-4A Satellite Image. (arXiv:2401.03182v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03182</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate cloud recognition and warning are crucial for various applications,
including in-flight support, weather forecasting, and climate research.
However, recent deep learning algorithms have predominantly focused on
detecting cloud regions in satellite imagery, with insufficient attention to
the specificity required for accurate cloud recognition. This limitation
inspired us to develop the novel FY-4A-Himawari-8 (FYH) dataset, which includes
nine distinct cloud categories and uses precise domain adaptation methods to
align 70,419 image-label pairs in terms of projection, temporal resolution, and
spatial resolution, thereby facilitating the training of supervised deep
learning networks. Given the complexity and diversity of cloud formations, we
have thoroughly analyzed the challenges inherent to cloud recognition tasks,
examining the intricate characteristics and distribution of the data. To
effectively address these challenges, we designed a Distribution-aware
Interactive-Attention Network (DIAnet), which preserves pixel-level details
through a high-resolution branch and a parallel multi-resolution cross-branch.
We also integrated a distribution-aware loss (DAL) to mitigate the imbalance
across cloud categories. An Interactive Attention Module (IAM) further enhances
the robustness of feature extraction combined with spatial and channel
information. Empirical evaluations on the FYH dataset demonstrate that our
method outperforms other cloud recognition networks, achieving superior
performance in terms of mean Intersection over Union (mIoU). The code for
implementing DIAnet is available at https://github.com/icey-zhang/DIAnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jie Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Mingxiang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03190">
<title>MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.03190</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are known for encoding a vast amount of factual
knowledge, but they often becomes outdated due to the ever-changing nature of
external information. A promising solution to this challenge is the utilization
of model editing methods to update the knowledge in an efficient manner.
However, the majority of existing model editing techniques are limited to
monolingual frameworks, thus failing to address the crucial issue of
cross-lingual knowledge synchronization for multilingual models. To tackle this
problem, we propose a simple yet effective method that trains multilingual
patch neuron to store cross-lingual knowledge. It can be easily adapted to
existing approaches to enhance their cross-lingual editing capabilities. To
evaluate our method, we conduct experiments using both the XNLI dataset and a
self-constructed XFEVER dataset. Experimental results demonstrate that our
proposed method achieves improved performance in cross-lingual editing tasks
without requiring excessive modifications to the original methodology, thereby
showcasing its user-friendly characteristics. Codes will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1&quot;&gt;Nianwen Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiqiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03191">
<title>DistFormer: Enhancing Local and Global Features for Monocular Per-Object Distance Estimation. (arXiv:2401.03191v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03191</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate per-object distance estimation is crucial in safety-critical
applications such as autonomous driving, surveillance, and robotics. Existing
approaches rely on two scales: local information (i.e., the bounding box
proportions) or global information, which encodes the semantics of the scene as
well as the spatial relations with neighboring objects. However, these
approaches may struggle with long-range objects and in the presence of strong
occlusions or unusual visual patterns. In this respect, our work aims to
strengthen both local and global cues. Our architecture -- named DistFormer --
builds upon three major components acting jointly: i) a robust context encoder
extracting fine-grained per-object representations; ii) a masked
encoder-decoder module exploiting self-supervision to promote the learning of
useful per-object features; iii) a global refinement module that aggregates
object representations and computes a joint, spatially-consistent estimation.
To evaluate the effectiveness of DistFormer, we conduct experiments on the
standard KITTI dataset and the large-scale NuScenes and MOTSynth datasets. Such
datasets cover various indoor/outdoor environments, changing weather
conditions, appearances, and camera viewpoints. Our comprehensive analysis
shows that DistFormer outperforms existing methods. Moreover, we further delve
into its generalization capabilities, showing its regularization benefits in
zero-shot synth-to-real transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panariello_A/0/1/0/all/0/1&quot;&gt;Aniello Panariello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mancusi_G/0/1/0/all/0/1&quot;&gt;Gianluca Mancusi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_F/0/1/0/all/0/1&quot;&gt;Fedy Haj Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porrello_A/0/1/0/all/0/1&quot;&gt;Angelo Porrello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1&quot;&gt;Simone Calderara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03195">
<title>Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features. (arXiv:2401.03195v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2401.03195</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing high-quality video with efficient bitrate is a main challenge in
video industry. The traditional one-size-fits-all scheme for bitrate ladders is
inefficient and reaching the best content-aware decision computationally
impractical due to extensive encodings required. To mitigate this, we propose a
bitrate and complexity efficient bitrate ladder prediction method using
transfer learning and spatio-temporal features. We propose: (1) using feature
maps from well-known pre-trained DNNs to predict rate-quality behavior with
limited training data; and (2) improving highest quality rung efficiency by
predicting minimum bitrate for top quality and using it for the top rung. The
method tested on 102 video scenes demonstrates 94.1% reduction in complexity
versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning
was thoroughly studied through four networks and ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falahati_A/0/1/0/all/0/1&quot;&gt;Ali Falahati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safavi_M/0/1/0/all/0/1&quot;&gt;Mohammad Karim Safavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_A/0/1/0/all/0/1&quot;&gt;Ardavan Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pakdaman_F/0/1/0/all/0/1&quot;&gt;Farhad Pakdaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03201">
<title>3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding. (arXiv:2401.03201v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03201</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable potential of multi-modal large language models (MLLMs) in
comprehending both vision and language information has been widely
acknowledged. However, the scarcity of 3D scenes-language pairs in comparison
to their 2D counterparts, coupled with the inadequacy of existing approaches in
understanding of 3D scenes by LLMs, poses a significant challenge. In response,
we collect and construct an extensive dataset comprising 75K
instruction-response pairs tailored for 3D scenes. This dataset addresses tasks
related to 3D VQA, 3D grounding, and 3D conversation. To further enhance the
integration of 3D spatial information into LLMs, we introduce a novel and
efficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment
stage between 3D scenes and language and extends the instruction prompt with
the 3D modality information including the entire scene and segmented objects.
We evaluate the effectiveness of our method across diverse tasks in the 3D
scene domain and find that our approach serves as a strategic means to enrich
LLMs&apos; comprehension of the 3D world. Our code is available at
https://github.com/staymylove/3DMIT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1&quot;&gt;Ruilong Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yifan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ruifei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangde Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03203">
<title>Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping. (arXiv:2401.03203v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03203</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Hi-Map, a novel monocular dense mapping approach
based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to
achieve efficient and high-fidelity mapping using only posed RGB inputs. Our
method eliminates the need for external depth priors derived from e.g., a depth
estimation model. Our key idea is to represent the scene as a hierarchical
feature grid that encodes the radiance and then factorizes it into feature
planes and vectors. As such, the scene representation becomes simpler and more
generalizable for fast and smooth convergence on new observations. This allows
for efficient computation while alleviating noise patterns by reducing the
complexity of the scene representation. Buttressed by the hierarchical
factorized representation, we leverage the Sign Distance Field (SDF) as a proxy
of rendering for inferring the volume density, demonstrating high mapping
fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen
the photometric cues and further boost the mapping quality, especially for the
distant and textureless regions. Extensive experiments demonstrate our method&apos;s
superiority in geometric and textural accuracy over the state-of-the-art
NeRF-based monocular mapping methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1&quot;&gt;Tongyan Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Haotian Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zidong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03220">
<title>MetaISP -- Exploiting Global Scene Structure for Accurate Multi-Device Color Rendition. (arXiv:2401.03220v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03220</link>
<description rdf:parseType="Literal">&lt;p&gt;Image signal processors (ISPs) are historically grown legacy software systems
for reconstructing color images from noisy raw sensor measurements. Each
smartphone manufacturer has developed its ISPs with its own characteristic
heuristics for improving the color rendition, for example, skin tones and other
visually essential colors. The recent interest in replacing the historically
grown ISP systems with deep-learned pipelines to match DSLR&apos;s image quality
improves structural features in the image. However, these works ignore the
superior color processing based on semantic scene analysis that distinguishes
mobile phone ISPs from DSLRs. Here, we present MetaISP, a single model designed
to learn how to translate between the color and local contrast characteristics
of different devices. MetaISP takes the RAW image from device A as input and
translates it to RGB images that inherit the appearance characteristics of
devices A, B, and C. We achieve this result by employing a lightweight deep
learning technique that conditions its output appearance based on the device of
interest. In this approach, we leverage novel attention mechanisms inspired by
cross-covariance to learn global scene semantics. Additionally, we use the
metadata that typically accompanies RAW images and estimate scene illuminants
when they are unavailable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1&quot;&gt;Matheus Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1&quot;&gt;Wolfgang Heidrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03221">
<title>MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image Translation by Prompts Redescription and Beyond. (arXiv:2401.03221v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03221</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, text-to-image diffusion models become a new paradigm in image
processing fields, including content generation, image restoration and
image-to-image translation. Given a target prompt, Denoising Diffusion
Probabilistic Models (DDPM) are able to generate realistic yet eligible images.
With this appealing property, the image translation task has the potential to
be free from target image samples for supervision. By using a target text
prompt for domain adaption, the diffusion model is able to implement zero-shot
image-to-image translation advantageously. However, the sampling and inversion
processes of DDPM are stochastic, and thus the inversion process often fail to
reconstruct the input content. Specifically, the displacement effect will
gradually accumulated during the diffusion and inversion processes, which led
to the reconstructed results deviating from the source domain. To make
reconstruction explicit, we propose a prompt redescription strategy to realize
a mirror effect between the source and reconstructed image in the diffusion
model (MirrorDiffusion). More specifically, a prompt redescription mechanism is
investigated to align the text prompts with latent code at each time step of
the Denoising Diffusion Implicit Models (DDIM) inversion to pursue a
structure-preserving reconstruction. With the revised DDIM inversion,
MirrorDiffusion is able to realize accurate zero-shot image translation by
editing optimized text prompts and latent code. Extensive experiments
demonstrate that MirrorDiffusion achieves superior performance over the
state-of-the-art methods on zero-shot image translation benchmarks by clear
margins and practical model stability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yupei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03250">
<title>Interpersonal Relationship Analysis with Dyadic EEG Signals via Learning Spatial-Temporal Patterns. (arXiv:2401.03250v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.03250</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpersonal relationship quality is pivotal in social and occupational
contexts. Existing analysis of interpersonal relationships mostly rely on
subjective self-reports, whereas objective quantification remains challenging.
In this paper, we propose a novel social relationship analysis framework using
spatio-temporal patterns derived from dyadic EEG signals, which can be applied
to quantitatively measure team cooperation in corporate team building, and
evaluate interpersonal dynamics between therapists and patients in psychiatric
therapy. First, we constructed a dyadic-EEG dataset from 72 pairs of
participants with two relationships (stranger or friend) when watching
emotional videos simultaneously. Then we proposed a deep neural network on
dyadic-subject EEG signals, in which we combine the dynamic graph convolutional
neural network for characterizing the interpersonal relationships among the EEG
channels and 1-dimension convolution for extracting the information from the
time sequence. To obtain the feature vectors from two EEG recordings that well
represent the relationship of two subjects, we integrate deep canonical
correlation analysis and triplet loss for training the network. Experimental
results show that the social relationship type (stranger or friend) between two
individuals can be effectively identified through their EEG data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wenqi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+liu_F/0/1/0/all/0/1&quot;&gt;Fang liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinxin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Niqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mingjin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guozhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03253">
<title>Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03253</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances achieved by deep learning models rely on the independent and
identically distributed assumption, hindering their applications in real-world
scenarios with domain shifts. To address the above issues, cross-domain
learning aims at extracting domain-invariant knowledge to reduce the domain
shift between training and testing data. However, in visual cross-domain
learning, traditional methods concentrate solely on the image modality,
neglecting the use of the text modality to alleviate the domain shift. In this
work, we propose Large Language models as Visual cross-dOmain learners (LLaVO).
LLaVO uses vision-language models to convert images into detailed textual
descriptions. A large language model is then finetuned on textual descriptions
of the source/target domain generated by a designed instruction template.
Extensive experimental results on various cross-domain tasks under the domain
generalization and unsupervised domain adaptation settings have demonstrated
the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Weisen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiangang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03257">
<title>RustNeRF: Robust Neural Radiance Field with Low-Quality Images. (arXiv:2401.03257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03257</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D
consistency, achieving impressive results in 3D scene modeling and
high-fidelity novel-view synthesis. However, there are limitations. First,
existing methods assume enough high-quality images are available for training
the NeRF model, ignoring real-world image degradation. Second, previous methods
struggle with ambiguity in the training set due to unmodeled inconsistencies
among different views. In this work, we present RustNeRF for real-world
high-quality NeRF. To improve NeRF&apos;s robustness under real-world inputs, we
train a 3D-aware preprocessing network that incorporates real-world degradation
modeling. We propose a novel implicit multi-view guidance to address
information loss during image degradation and restoration. Extensive
experiments demonstrate RustNeRF&apos;s advantages over existing approaches under
real-world degradation. The code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Ming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaofang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03262">
<title>Group Activity Recognition using Unreliable Tracked Pose. (arXiv:2401.03262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03262</link>
<description rdf:parseType="Literal">&lt;p&gt;Group activity recognition in video is a complex task due to the need for a
model to recognise the actions of all individuals in the video and their
complex interactions. Recent studies propose that optimal performance is
achieved by individually tracking each person and subsequently inputting the
sequence of poses or cropped images/optical flow into a model. This helps the
model to recognise what actions each person is performing before they are
merged to arrive at the group action class. However, all previous models are
highly reliant on high quality tracking and have only been evaluated using
ground truth tracking information. In practice it is almost impossible to
achieve highly reliable tracking information for all individuals in a group
activity video. We introduce an innovative deep learning-based group activity
recognition approach called Rendered Pose based Group Activity Recognition
System (RePGARS) which is designed to be tolerant of unreliable tracking and
pose information. Experimental results confirm that RePGARS outperforms all
existing group activity recognition algorithms tested which do not use ground
truth detection and tracking information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thilakarathne_H/0/1/0/all/0/1&quot;&gt;Haritha Thilakarathne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1&quot;&gt;Aiden Nibali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_S/0/1/0/all/0/1&quot;&gt;Stuart Morgan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03267">
<title>Autonomous Navigation in Complex Environments. (arXiv:2401.03267v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.03267</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the application of CNN-DNN network fusion to construct a
robot navigation controller within a simulated environment. The simulated
environment is constructed to model a subterranean rescue situation, such that
an autonomous agent is tasked with finding a goal within an unknown cavernous
system. Imitation learning is used to train the control algorithm to use LiDAR
and camera data to navigate the space and find the goal. The trained model is
then tested for robustness using Monte-Carlo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstenslager_A/0/1/0/all/0/1&quot;&gt;Andrew Gerstenslager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;Jomol Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKenna_L/0/1/0/all/0/1&quot;&gt;Liam McKenna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1&quot;&gt;Poorva Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03271">
<title>Analysis and Validation of Image Search Engines in Histopathology. (arXiv:2401.03271v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03271</link>
<description rdf:parseType="Literal">&lt;p&gt;Searching for similar images in archives of histology and histopathology
images is a crucial task that may aid in patient matching for various purposes,
ranging from triaging and diagnosis to prognosis and prediction. Whole slide
images (WSIs) are highly detailed digital representations of tissue specimens
mounted on glass slides. Matching WSI to WSI can serve as the critical method
for patient matching. In this paper, we report extensive analysis and
validation of four search methods bag of visual words (BoVW), Yottixel, SISH,
RetCCL, and some of their potential variants. We analyze their algorithms and
structures and assess their performance. For this evaluation, we utilized four
internal datasets ($1269$ patients) and three public datasets ($1207$
patients), totaling more than $200,000$ patches from $38$ different
classes/subtypes across five primary sites. Certain search engines, for
example, BoVW, exhibit notable efficiency and speed but suffer from low
accuracy. Conversely, search engines like Yottixel demonstrate efficiency and
speed, providing moderately accurate results. Recent proposals, including SISH,
display inefficiency and yield inconsistent outcomes, while alternatives like
RetCCL prove inadequate in both accuracy and efficiency. Further research is
imperative to address the dual aspects of accuracy and minimal storage
requirements in histopathological image search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lahr_I/0/1/0/all/0/1&quot;&gt;Isaiah Lahr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alfasly_S/0/1/0/all/0/1&quot;&gt;Saghir Alfasly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nejat_P/0/1/0/all/0/1&quot;&gt;Peyman Nejat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_J/0/1/0/all/0/1&quot;&gt;Jibran Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kottom_L/0/1/0/all/0/1&quot;&gt;Luke Kottom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumbhar_V/0/1/0/all/0/1&quot;&gt;Vaishnavi Kumbhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alsaafin_A/0/1/0/all/0/1&quot;&gt;Areej Alsaafin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shafique_A/0/1/0/all/0/1&quot;&gt;Abubakr Shafique&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hemati_S/0/1/0/all/0/1&quot;&gt;Sobhan Hemati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alabtah_G/0/1/0/all/0/1&quot;&gt;Ghazal Alabtah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Comfere_N/0/1/0/all/0/1&quot;&gt;Nneka Comfere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Murphee_D/0/1/0/all/0/1&quot;&gt;Dennis Murphee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mangold_A/0/1/0/all/0/1&quot;&gt;Aaron Mangold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yasir_S/0/1/0/all/0/1&quot;&gt;Saba Yasir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meroueh_C/0/1/0/all/0/1&quot;&gt;Chady Meroueh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Boardman_L/0/1/0/all/0/1&quot;&gt;Lisa Boardman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Vijay H. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garcia_J/0/1/0/all/0/1&quot;&gt;Joaquin J. Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1&quot;&gt;H.R. Tizhoosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03275">
<title>Real Time Human Detection by Unmanned Aerial Vehicles. (arXiv:2401.03275v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03275</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most important problems in computer vision and remote sensing is
object detection, which identifies particular categories of diverse things in
pictures. Two crucial data sources for public security are the thermal infrared
(TIR) remote sensing multi-scenario photos and videos produced by unmanned
aerial vehicles (UAVs). Due to the small scale of the target, complex scene
information, low resolution relative to the viewable videos, and dearth of
publicly available labeled datasets and training models, their object detection
procedure is still difficult. A UAV TIR object detection framework for pictures
and videos is suggested in this study. The Forward-looking Infrared (FLIR)
cameras used to gather ground-based TIR photos and videos are used to create
the ``You Only Look Once&apos;&apos; (YOLO) model, which is based on CNN architecture.
Results indicated that in the validating task, detecting human object had an
average precision at IOU (Intersection over Union) = 0.5, which was 72.5\%,
using YOLOv7 (YOLO version 7) state of the art model \cite{1}, while the
detection speed around 161 frames per second (FPS/second). The usefulness of
the YOLO architecture is demonstrated in the application, which evaluates the
cross-detection performance of people in UAV TIR videos under a YOLOv7 model in
terms of the various UAVs&apos; observation angles. The qualitative and quantitative
evaluation of object detection from TIR pictures and videos using deep-learning
models is supported favorably by this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guettala_W/0/1/0/all/0/1&quot;&gt;Walid Guettala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayah_A/0/1/0/all/0/1&quot;&gt;Ali Sayah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahloul_L/0/1/0/all/0/1&quot;&gt;Laid Kahloul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tibermacine_A/0/1/0/all/0/1&quot;&gt;Ahmed Tibermacine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03298">
<title>Multi-View 3D Instance Segmentation of Structural Anomalies for Enhanced Structural Inspection of Concrete Bridges. (arXiv:2401.03298v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03298</link>
<description rdf:parseType="Literal">&lt;p&gt;For effective structural damage assessment, the instances of damages need to
be localized in the world of a 3D model. Due to a lack of data, the detection
of structural anomalies can currently not be directly learned and performed in
3D space. In this work, a three-stage approach is presented, which uses the
good performance of detection models on image level to segment instances of
anomalies in the 3D space. In the detection stage, semantic segmentation
predictions are produced on image level. The mapping stage transfers the
image-level prediction onto the respective point cloud. In the extraction
stage, 3D anomaly instances are extracted from the segmented point cloud. Cloud
contraction is used to transform cracks into their medial axis representation.
For areal anomalies the bounding polygon is extracted by means of alpha shapes.
The approach covers the classes crack, spalling, and corrosion and the three
image-level segmentation models TopoCrack, nnU-Net, and DetectionHMA are
compared. Granted a localization tolerance of 4cm, IoUs of over 90% can be
achieved for crack and corrosion and 41% for spalling, which appears to be a
specifically challenging class. Detection on instance-level measured in AP is
about 45% for crack and spalling and 73% for corrosion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benz_C/0/1/0/all/0/1&quot;&gt;Christian Benz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodehorst_V/0/1/0/all/0/1&quot;&gt;Volker Rodehorst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03302">
<title>Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03302</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hashemi_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammad Hossein Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Safari_L/0/1/0/all/0/1&quot;&gt;Leila Safari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taromi_A/0/1/0/all/0/1&quot;&gt;Amirhossein Dadashzade Taromi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03312">
<title>Exploiting Data Hierarchy as a New Modality for Contrastive Learning. (arXiv:2401.03312v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03312</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates how hierarchically structured data can help neural
networks learn conceptual representations of cathedrals. The underlying
WikiScenes dataset provides a spatially organized hierarchical structure of
cathedral components. We propose a novel hierarchical contrastive training
approach that leverages a triplet margin loss to represent the data&apos;s spatial
hierarchy in the encoder&apos;s latent space. As such, the proposed approach
investigates if the dataset structure provides valuable information for
self-supervised learning. We apply t-SNE to visualize the resultant latent
space and evaluate the proposed approach by comparing it with other
dataset-specific contrastive learning methods using a common downstream
classification task. The proposed method outperforms the comparable
weakly-supervised and baseline methods. Our findings suggest that dataset
structure is a valuable modality for weakly-supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhalla_A/0/1/0/all/0/1&quot;&gt;Arjun Bhalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levenson_D/0/1/0/all/0/1&quot;&gt;Daniel Levenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernhard_J/0/1/0/all/0/1&quot;&gt;Jan Bernhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abilov_A/0/1/0/all/0/1&quot;&gt;Anton Abilov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03317">
<title>Spatiotemporally adaptive compression for scientific dataset with feature preservation -- a case study on simulation data with extreme climate events analysis. (arXiv:2401.03317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03317</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific discoveries are increasingly constrained by limited storage space
and I/O capacities. For time-series simulations and experiments, their data
often need to be decimated over timesteps to accommodate storage and I/O
limitations. In this paper, we propose a technique that addresses storage costs
while improving post-analysis accuracy through spatiotemporal adaptive,
error-controlled lossy compression. We investigate the trade-off between data
precision and temporal output rates, revealing that reducing data precision and
increasing timestep frequency lead to more accurate analysis outcomes.
Additionally, we integrate spatiotemporal feature detection with data
compression and demonstrate that performing adaptive error-bounded compression
in higher dimensional space enables greater compression ratios, leveraging the
error propagation theory of a transformation-based compressor.
&lt;/p&gt;
&lt;p&gt;To evaluate our approach, we conduct experiments using the well-known E3SM
climate simulation code and apply our method to compress variables used for
cyclone tracking. Our results show a significant reduction in storage size
while enhancing the quality of cyclone tracking analysis, both quantitatively
and qualitatively, in comparison to the prevalent timestep decimation approach.
Compared to three state-of-the-art lossy compressors lacking feature
preservation capabilities, our adaptive compression framework improves
perfectly matched cases in TC tracking by 26.4-51.3% at medium compression
ratios and by 77.3-571.1% at large compression ratios, with a merely 5-11%
computational overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1&quot;&gt;Qian Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengzhu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reshniak_V/0/1/0/all/0/1&quot;&gt;Viktor Reshniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jieyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1&quot;&gt;Anand Rangarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1&quot;&gt;Sanjay Ranka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_N/0/1/0/all/0/1&quot;&gt;Nicolas Vidal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Lipeng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullrich_P/0/1/0/all/0/1&quot;&gt;Paul Ullrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Podhorszki_N/0/1/0/all/0/1&quot;&gt;Norbert Podhorszki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_R/0/1/0/all/0/1&quot;&gt;Robert Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klasky_S/0/1/0/all/0/1&quot;&gt;Scott Klasky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03331">
<title>Walnut Detection Through Deep Learning Enhanced by Multispectral Synthetic Images. (arXiv:2401.03331v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03331</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate identification of walnuts within orchards brings forth a
plethora of advantages, profoundly amplifying the efficiency and productivity
of walnut orchard management. Nevertheless, the unique characteristics of
walnut trees, characterized by their closely resembling shapes, colors, and
textures between the walnuts and leaves, present a formidable challenge in
precisely distinguishing between them during the annotation process. In this
study, we present a novel approach to improve walnut detection efficiency,
utilizing YOLOv5 trained on an enriched image set that incorporates both real
and synthetic RGB and NIR images. Our analysis comparing results from our
original and augmented datasets shows clear improvements in detection when
using the synthetic images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kaiming Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1&quot;&gt;Tong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halubok_M/0/1/0/all/0/1&quot;&gt;Maryia Halubok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_B/0/1/0/all/0/1&quot;&gt;Brian N. Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03340">
<title>Classifying cow stall numbers using YOLO. (arXiv:2401.03340v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03340</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the CowStallNumbers dataset, a collection of images
extracted from videos focusing on cow teats, designed to advance the field of
cow stall number detection. The dataset comprises 1042 training images and 261
test images, featuring stall numbers ranging from 0 to 60. To enhance the
dataset, we performed fine-tuning on a YOLO model and applied data augmentation
techniques, including random crop, center crop, and random rotation. The
experimental outcomes demonstrate a notable 95.4\% accuracy in recognizing
stall numbers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajjarapu_D/0/1/0/all/0/1&quot;&gt;Dheeraj Vajjarapu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03349">
<title>Image Inpainting via Tractable Steering of Diffusion Models. (arXiv:2401.03349v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03349</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are the current state of the art for generating
photorealistic images. Controlling the sampling process for constrained image
generation tasks such as inpainting, however, remains challenging since exact
conditioning on such constraints is intractable. While existing methods use
various techniques to approximate the constrained posterior, this paper
proposes to exploit the ability of Tractable Probabilistic Models (TPMs) to
exactly and efficiently compute the constrained posterior, and to leverage this
signal to steer the denoising process of diffusion models. Specifically, this
paper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs).
Building upon prior advances, we further scale up PCs and make them capable of
guiding the image generation process of diffusion models. Empirical results
suggest that our approach can consistently improve the overall quality and
semantic coherence of inpainted images across three natural image datasets
(i.e., CelebA-HQ, ImageNet, and LSUN) with only ~10% additional computational
overhead brought by the TPM. Further, with the help of an image encoder and
decoder, our method can readily accept semantic constraints on specific regions
of the image, which opens up the potential for more controlled image generation
tasks. In addition to proposing a new framework for constrained image
generation, this paper highlights the benefit of more tractable models and
motivates the development of expressive TPMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1&quot;&gt;Guy Van den Broeck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.05074">
<title>Critical configurations for two projective views, a new approach. (arXiv:2112.05074v4 [math.AG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.05074</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of structure from motion is concerned with recovering
3-dimensional structure of an object from a set of 2-dimensional images.
Generally, all information can be uniquely recovered if enough images and image
points are provided, but there are certain cases where unique recovery is
impossible; these are called critical configurations. In this paper we use an
algebraic approach to study the critical configurations for two projective
cameras. We show that all critical configurations lie on quadric surfaces, and
classify exactly which quadrics constitute a critical configuration. The paper
also describes the relation between the different reconstructions when unique
reconstruction is impossible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Braatelund_M/0/1/0/all/0/1&quot;&gt;Martin Br&amp;#xe5;telund&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.05941">
<title>Domain-Invariant Proposals based on a Balanced Domain Classifier for Object Detection. (arXiv:2202.05941v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.05941</link>
<description rdf:parseType="Literal">&lt;p&gt;Object recognition from images means to automatically find object(s) of
interest and to return their category and location information. Benefiting from
research on deep learning, like convolutional neural networks~(CNNs) and
generative adversarial networks, the performance in this field has been
improved significantly, especially when training and test data are drawn from
similar distributions. However, mismatching distributions, i.e., domain shifts,
lead to a significant performance drop. In this paper, we build
domain-invariant detectors by learning domain classifiers via adversarial
training. Based on the previous works that align image and instance level
features, we mitigate the domain shift further by introducing a domain
adaptation component at the region level within Faster \mbox{R-CNN}. We embed a
domain classification network in the region proposal network~(RPN) using
adversarial learning. The RPN can now generate accurate region proposals in
different domains by effectively aligning the features between them. To
mitigate the unstable convergence during the adversarial learning, we introduce
a balanced domain classifier as well as a network learning rate adjustment
strategy. We conduct comprehensive experiments using four standard datasets.
The results demonstrate the effectiveness and robustness of our object
detection approach in domain shift scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhize Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuebin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1&quot;&gt;Le Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lixiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weise_T/0/1/0/all/0/1&quot;&gt;Thomas Weise&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.13492">
<title>Representation Recycling for Streaming Video Analysis. (arXiv:2204.13492v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.13492</link>
<description rdf:parseType="Literal">&lt;p&gt;We present StreamDEQ, a method that aims to infer frame-wise representations
on videos with minimal per-frame computation. Conventional deep networks do
feature extraction from scratch at each frame in the absence of ad-hoc
solutions. We instead aim to build streaming recognition models that can
natively exploit temporal smoothness between consecutive video frames. We
observe that the recently emerging implicit layer models provide a convenient
foundation to construct such models, as they define representations as the
fixed-points of shallow networks, which need to be estimated using iterative
methods. Our main insight is to distribute the inference iterations over the
temporal axis by using the most recent representation as a starting point at
each frame. This scheme effectively recycles the recent inference computations
and greatly reduces the needed processing time. Through extensive experimental
analysis, we show that StreamDEQ is able to recover near-optimal
representations in a few frames&apos; time and maintain an up-to-date representation
throughout the video duration. Our experiments on video semantic segmentation,
video object detection, and human pose estimation in videos show that StreamDEQ
achieves on-par accuracy with the baseline while being more than 2-4x faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ertenli_C/0/1/0/all/0/1&quot;&gt;Can Ufuk Ertenli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1&quot;&gt;Ramazan Gokberk Cinbis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1&quot;&gt;Emre Akbas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.15462">
<title>Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.15462</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a margin-based loss for tuning joint vision-language models so
that their gradient-based explanations are consistent with region-level
annotations provided by humans for relatively smaller grounding datasets. We
refer to this objective as Attention Mask Consistency (AMC) and demonstrate
that it produces superior visual grounding results than previous methods that
rely on using vision-language models to score the outputs of object detectors.
Particularly, a model trained with AMC on top of standard vision-language
modeling objectives obtains a state-of-the-art accuracy of 86.49% in the
Flickr30k visual grounding benchmark, an absolute improvement of 5.38% when
compared to the best previous model trained under the same level of
supervision. Our approach also performs exceedingly well on established
benchmarks for referring expression comprehension where it obtains 80.34%
accuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC
is effective, easy to implement, and is general as it can be adopted by any
vision-language model, and can use any type of region annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1&quot;&gt;Kushal Kafle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1&quot;&gt;Vicente Ordonez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.05401">
<title>Benchmarking Joint Face Spoofing and Forgery Detection with Visual and Physiological Cues. (arXiv:2208.05401v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.05401</link>
<description rdf:parseType="Literal">&lt;p&gt;Face anti-spoofing (FAS) and face forgery detection play vital roles in
securing face biometric systems from presentation attacks (PAs) and vicious
digital manipulation (e.g., deepfakes). Despite promising performance upon
large-scale data and powerful deep models, the generalization problem of
existing approaches is still an open issue. Most of recent approaches focus on
1) unimodal visual appearance or physiological (i.e., remote
photoplethysmography (rPPG)) cues; and 2) separated feature representation for
FAS or face forgery detection. On one side, unimodal appearance and rPPG
features are respectively vulnerable to high-fidelity face 3D mask and video
replay attacks, inspiring us to design reliable multi-modal fusion mechanisms
for generalized face attack detection. On the other side, there are rich common
features across FAS and face forgery detection tasks (e.g., periodic rPPG
rhythms and vanilla appearance for bonafides), providing solid evidence to
design a joint FAS and face forgery detection system in a multi-task learning
fashion. In this paper, we establish the first joint face spoofing and forgery
detection benchmark using both visual appearance and physiological rPPG cues.
To enhance the rPPG periodicity discrimination, we design a two-branch
physiological network using both facial spatio-temporal rPPG signal map and its
continuous wavelet transformed counterpart as inputs. To mitigate the modality
bias and improve the fusion efficacy, we conduct a weighted batch and layer
normalization for both appearance and rPPG features before multi-modal fusion.
We find that the generalization capacities of both unimodal (appearance or
rPPG) and multi-modal (appearance+rPPG) models can be obviously improved via
joint training on these two tasks. We hope this new benchmark will facilitate
the future research of both FAS and deepfake detection communities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zitong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Rizhao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jingang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1&quot;&gt;Alex C. Kot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.08076">
<title>Significance of Skeleton-based Features in Virtual Try-On. (arXiv:2208.08076v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.08076</link>
<description rdf:parseType="Literal">&lt;p&gt;The idea of \textit{Virtual Try-ON} (VTON) benefits e-retailing by giving an
user the convenience of trying a clothing at the comfort of their home. In
general, most of the existing VTON methods produce inconsistent results when a
person posing with his arms folded i.e., bent or crossed, wants to try an
outfit. The problem becomes severe in the case of long-sleeved outfits. As
then, for crossed arm postures, overlap among different clothing parts might
happen. The existing approaches, especially the warping-based methods employing
\textit{Thin Plate Spline (TPS)} transform can not tackle such cases. To this
end, we attempt a solution approach where the clothing from the source person
is segmented into semantically meaningful parts and each part is warped
independently to the shape of the person. To address the bending issue, we
employ hand-crafted geometric features consistent with human body geometry for
warping the source outfit. In addition, we propose two learning-based modules:
a synthesizer network and a mask prediction network. All these together attempt
to produce a photo-realistic, pose-robust VTON solution without requiring any
paired training data. Comparison with some of the benchmark methods clearly
establishes the effectiveness of the approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1&quot;&gt;Debapriya Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santra_S/0/1/0/all/0/1&quot;&gt;Sanchayan Santra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1&quot;&gt;Diganta Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanda_B/0/1/0/all/0/1&quot;&gt;Bhabatosh Chanda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14624">
<title>Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14624</link>
<description rdf:parseType="Literal">&lt;p&gt;Pruning neural networks has become popular in the last decade when it was
shown that a large number of weights can be safely removed from modern neural
networks without compromising accuracy. Numerous pruning methods have been
proposed since, each claiming to be better than prior art, however, at the cost
of increasingly complex pruning methodologies. These methodologies include
utilizing importance scores, getting feedback through back-propagation or
having heuristics-based pruning rules amongst others. In this work, we question
whether this pattern of introducing complexity is really necessary to achieve
better pruning results. We benchmark these SOTA techniques against a simple
pruning baseline, namely, Global Magnitude Pruning (Global MP), that ranks
weights in order of their magnitudes and prunes the smallest ones.
Surprisingly, we find that vanilla Global MP performs very well against the
SOTA techniques. When considering sparsity-accuracy trade-off, Global MP
performs better than all SOTA techniques at all sparsity ratios. When
considering FLOPs-accuracy trade-off, some SOTA techniques outperform Global MP
at lower sparsity ratios, however, Global MP starts performing well at high
sparsity ratios and performs very well at extremely high sparsity ratios.
Moreover, we find that a common issue that many pruning algorithms run into at
high sparsity rates, namely, layer-collapse, can be easily fixed in Global MP.
We explore why layer collapse occurs in networks and how it can be mitigated in
Global MP by utilizing a technique called Minimum Threshold. We showcase the
above findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1
and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is
available at https://github.com/manasgupta-1/GlobalMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Manas Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camci_E/0/1/0/all/0/1&quot;&gt;Efe Camci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keneta_V/0/1/0/all/0/1&quot;&gt;Vishandi Rudy Keneta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaidyanathan_A/0/1/0/all/0/1&quot;&gt;Abhishek Vaidyanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanodia_R/0/1/0/all/0/1&quot;&gt;Ritwik Kanodia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1&quot;&gt;Chuan-Sheng Foo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1&quot;&gt;Wu Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_L/0/1/0/all/0/1&quot;&gt;Lin Jie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01096">
<title>Recovering Sign Bits of DCT Coefficients in Digital Images as an Optimization Problem. (arXiv:2211.01096v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01096</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering unknown, missing, damaged, distorted, or lost information in DCT
coefficients is a common task in multiple applications of digital image
processing, including image compression, selective image encryption, and image
communication. This paper investigates the recovery of sign bits in DCT
coefficients of digital images, by proposing two different approximation
methods to solve a mixed integer linear programming (MILP) problem, which is
NP-hard in general. One method is a relaxation of the MILP problem to a linear
programming (LP) problem, and the other splits the original MILP problem into
some smaller MILP problems and an LP problem. We considered how the proposed
methods can be applied to JPEG-encoded images and conducted extensive
experiments to validate their performances. The experimental results showed
that the proposed methods outperformed other existing methods by a substantial
margin, both according to objective quality metrics and our subjective
evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1&quot;&gt;C.-C. Jay Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09782">
<title>Assessing Neural Network Robustness via Adversarial Pivotal Tuning. (arXiv:2211.09782v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09782</link>
<description rdf:parseType="Literal">&lt;p&gt;The robustness of image classifiers is essential to their deployment in the
real world. The ability to assess this resilience to manipulations or
deviations from the training data is thus crucial. These modifications have
traditionally consisted of minimal changes that still manage to fool
classifiers, and modern approaches are increasingly robust to them. Semantic
manipulations that modify elements of an image in meaningful ways have thus
gained traction for this purpose. However, they have primarily been limited to
style, color, or attribute changes. While expressive, these manipulations do
not make use of the full capabilities of a pretrained generative model. In this
work, we aim to bridge this gap. We show how a pretrained image generator can
be used to semantically manipulate images in a detailed, diverse, and
photorealistic way while still preserving the class of the original image.
Inspired by recent GAN-based image inversion methods, we propose a method
called Adversarial Pivotal Tuning (APT). Given an image, APT first finds a
pivot latent space input that reconstructs the image using a pretrained
generator. It then adjusts the generator&apos;s weights to create small yet semantic
manipulations in order to fool a pretrained classifier. APT preserves the full
expressive editing capabilities of the generative model. We demonstrate that
APT is capable of a wide range of class-preserving semantic image manipulations
that fool a variety of pretrained classifiers. Finally, we show that
classifiers that are robust to other benchmarks are not robust to APT
manipulations and suggest a method to improve them. Code available at:
https://captaine.github.io/apt/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christensen_P/0/1/0/all/0/1&quot;&gt;Peter Ebert Christensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe9;steinn Sn&amp;#xe6;bjarnarson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1&quot;&gt;Andrea Dittadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1&quot;&gt;Sagie Benaim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10867">
<title>Rethinking the Paradigm of Content Constraints in Unpaired Image-to-Image Translation. (arXiv:2211.10867v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10867</link>
<description rdf:parseType="Literal">&lt;p&gt;In an unpaired setting, lacking sufficient content constraints for
image-to-image translation (I2I) tasks, GAN-based approaches are usually prone
to model collapse. Current solutions can be divided into two categories,
reconstruction-based and Siamese network-based. The former requires that the
transformed or transforming image can be perfectly converted back to the
original image, which is sometimes too strict and limits the generative
performance. The latter involves feeding the original and generated images into
a feature extractor and then matching their outputs. This is not efficient
enough, and a universal feature extractor is not easily available. In this
paper, we propose EnCo, a simple but efficient way to maintain the content by
constraining the representational similarity in the latent space of patch-level
features from the same stage of the \textbf{En}coder and de\textbf{Co}der of
the generator. For the similarity function, we use a simple MSE loss instead of
contrastive loss, which is currently widely used in I2I tasks. Benefits from
the design, EnCo training is extremely efficient, while the features from the
encoder produce a more positive effect on the decoding, leading to more
satisfying generations. In addition, we rethink the role played by
discriminators in sampling patches and propose a discriminative
attention-guided (DAG) patch sampling strategy to replace random sampling. DAG
is parameter-free and only requires negligible computational overhead, while
significantly improving the performance of the model. Extensive experiments on
multiple datasets demonstrate the effectiveness and advantages of EnCo, and we
achieve multiple state-of-the-art compared to previous methods. Our code is
available at https://github.com/XiudingCai/EnCo-pytorch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xiuding Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1&quot;&gt;Dong Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Linjie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yu Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11010">
<title>Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric. (arXiv:2211.11010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11010</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining the Color and Event cameras (also called Dynamic Vision Sensors,
DVS) for robust object tracking is a newly emerging research topic in recent
years. Existing color-event tracking framework usually contains multiple
scattered modules which may lead to low efficiency and high computational
complexity, including feature extraction, fusion, matching, interactive
learning, etc. In this paper, we propose a single-stage backbone network for
Color-Event Unified Tracking (CEUTrack), which achieves the above functions
simultaneously. Given the event points and RGB frames, we first transform the
points into voxels and crop the template and search regions for both
modalities, respectively. Then, these regions are projected into tokens and
parallelly fed into the unified Transformer backbone network. The output
features will be fed into a tracking head for target object localization. Our
proposed CEUTrack is simple, effective, and efficient, which achieves over 75
FPS and new SOTA performance. To better validate the effectiveness of our model
and address the data deficiency of this task, we also propose a generic and
large-scale benchmark dataset for color-event tracking, termed COESOT, which
contains 90 categories and 1354 video sequences. Additionally, a new evaluation
metric named BOC is proposed in our evaluation toolkit to evaluate the
prominence with respect to the baseline methods. We hope the newly proposed
method, dataset, and evaluation metric provide a better platform for
color-event-based tracking. The dataset, toolkit, and source code will be
released on: \url{https://github.com/Event-AHU/COESOT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chuanming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Ju Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00970">
<title>Benchmarking the Robustness of LiDAR Semantic Segmentation Models. (arXiv:2301.00970v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00970</link>
<description rdf:parseType="Literal">&lt;p&gt;When using LiDAR semantic segmentation models for safety-critical
applications such as autonomous driving, it is essential to understand and
improve their robustness with respect to a large range of LiDAR corruptions. In
this paper, we aim to comprehensively analyze the robustness of LiDAR semantic
segmentation models under various corruptions. To rigorously evaluate the
robustness and generalizability of current approaches, we propose a new
benchmark called SemanticKITTI-C, which features 16 out-of-domain LiDAR
corruptions in three groups, namely adverse weather, measurement noise and
cross-device discrepancy. Then, we systematically investigate 11 LiDAR semantic
segmentation models, especially spanning different input representations (e.g.,
point clouds, voxels, projected images, and etc.), network architectures and
training schemes. Through this study, we obtain two insights: 1) We find out
that the input representation plays a crucial role in robustness. Specifically,
under specific corruptions, different representations perform variously. 2)
Although state-of-the-art methods on LiDAR semantic segmentation achieve
promising results on clean data, they are less robust when dealing with noisy
data. Finally, based on the above observations, we design a robust LiDAR
segmentation model (RLSeg) which greatly boosts the robustness with simple but
effective modifications. It is promising that our benchmark, comprehensive
analysis, and observations can boost future research in robust LiDAR semantic
segmentation for safety-critical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chaoda Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Ying Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dengxin Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13128">
<title>Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology. (arXiv:2301.13128v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13128</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization is one of the main challenges of computational pathology.
Slide preparation heterogeneity and the diversity of scanners lead to poor
model performance when used on data from medical centers not seen during
training. In order to achieve stain invariance in breast invasive carcinoma
patch classification, we implement a stain translation strategy using cycleGANs
for unsupervised image-to-image translation. We compare three cycleGAN-based
approaches to a baseline classification model obtained without any stain
invariance strategy. Two of the proposed approaches use cycleGAN&apos;s translations
at inference or training in order to build stain-specific classification
models. The last method uses them for stain data augmentation during training.
This constrains the classification model to learn stain-invariant features.
Baseline metrics are set by training and testing the baseline classification
model on a reference stain. We assessed performances using three medical
centers with H&amp;amp;E and H&amp;amp;E&amp;amp;S staining. Every approach tested in this study
improves baseline metrics without needing labels on target stains. The stain
augmentation-based approach produced the best results on every stain. Each
method&apos;s pros and cons are studied and discussed in this paper. However,
training highly performing cycleGANs models in itself represents a challenge.
In this work, we introduce a systematical method for optimizing cycleGAN
training by setting a novel stopping criterion. This method has the benefit of
not requiring any visual inspection of cycleGAN results and proves superiority
to methods using a predefined number of training epochs. In addition, we also
study the minimal amount of data required for cycleGAN training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nerrienet_N/0/1/0/all/0/1&quot;&gt;Nicolas Nerrienet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peyret_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Peyret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sockeel_M/0/1/0/all/0/1&quot;&gt;Marie Sockeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sockeel_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Sockeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06815">
<title>Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes. (arXiv:2302.06815v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06815</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust autonomous driving requires agents to accurately identify unexpected
areas (anomalies) in urban scenes. To this end, some critical issues remain
open: how to design advisable metric to measure anomalies, and how to properly
generate training samples of anomaly data? Classical effort in anomaly
detection usually resorts to pixel-wise uncertainty or sample synthesis, which
ignores the contextual information and sometimes requires auxiliary data with
fine-grained annotations. On the contrary, in this paper, we exploit the strong
context-dependent nature of the segmentation task and design an energy-guided
self-supervised framework for anomaly segmentation, which optimizes an anomaly
head by maximizing the likelihood of self-generated anomaly pixels. For this
purpose, we design two estimators to model anomaly likelihood, one is a
task-agnostic binary estimator and the other depicts the likelihood as residual
of task-oriented joint energy. Based on the proposed estimators, we devise an
adaptive self-supervised training framework, which exploits the contextual
reliance and estimated likelihood to refine mask annotations in anomaly areas.
We conduct extensive experiments on challenging Fishyscapes and Road Anomaly
benchmarks, demonstrating that without any auxiliary data or synthetic models,
our method can still achieve comparable performance to supervised competitors.
Code is available at https://github.com/yuanpengtu/SLEEG..
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1&quot;&gt;Yuanpeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Boshen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Cai Rong Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10630">
<title>LIT-Former: Linking In-plane and Through-plane Transformers for Simultaneous CT Image Denoising and Deblurring. (arXiv:2302.10630v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10630</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies 3D low-dose computed tomography (CT) imaging. Although
various deep learning methods were developed in this context, typically they
focus on 2D images and perform denoising due to low-dose and deblurring for
super-resolution separately. Up to date, little work was done for simultaneous
in-plane denoising and through-plane deblurring, which is important to obtain
high-quality 3D CT images with lower radiation and faster imaging speed. For
this task, a straightforward method is to directly train an end-to-end 3D
network. However, it demands much more training data and expensive
computational costs. Here, we propose to link in-plane and through-plane
transformers for simultaneous in-plane denoising and through-plane deblurring,
termed as LIT-Former, which can efficiently synergize in-plane and
through-plane sub-tasks for 3D CT imaging and enjoy the advantages of both
convolution and transformer networks. LIT-Former has two novel designs:
efficient multi-head self-attention modules (eMSM) and efficient convolutional
feedforward networks (eCFN). First, eMSM integrates in-plane 2D self-attention
and through-plane 1D self-attention to efficiently capture global interactions
of 3D self-attention, the core unit of transformer networks. Second, eCFN
integrates 2D convolution and 1D convolution to extract local information of 3D
convolution in the same fashion. As a result, the proposed LIT-Former synergize
these two subtasks, significantly reducing the computational complexity as
compared to 3D counterparts and enabling rapid convergence. Extensive
experimental results on simulated and clinical datasets demonstrate superior
performance over state-of-the-art models. The source code is made available at
https://github.com/hao1635/LIT-Former.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1&quot;&gt;Chuang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Qi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04751">
<title>Multimodal Parameter-Efficient Few-Shot Class Incremental Learning. (arXiv:2303.04751v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04751</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-Shot Class Incremental Learning (FSCIL) is a challenging continual
learning task, where limited training examples are available during several
learning sessions. To succeed in this task, it is necessary to avoid
over-fitting new classes caused by biased distributions in the few-shot
training sets. The general approach to address this issue involves enhancing
the representational capability of a pre-defined backbone architecture by
adding special modules for backward compatibility with older classes. However,
this approach has not yet solved the dilemma of ensuring high classification
accuracy over time while reducing the gap between the performance obtained on
larger training sets and the smaller ones. In this work, we propose an
alternative approach called Continual Parameter-Efficient CLIP (CPE-CLIP) to
reduce the loss of information between different learning sessions. Instead of
adapting additional modules to address information loss, we leverage the vast
knowledge acquired by CLIP in large-scale pre-training and its effectiveness in
generalizing to new concepts. Our approach is multimodal and
parameter-efficient, relying on learnable prompts for both the language and
vision encoders to enable transfer learning across sessions. We also introduce
prompt regularization to improve performance and prevent forgetting. Our
experimental results demonstrate that CPE-CLIP significantly improves FSCIL
performance compared to state-of-the-art proposals while also drastically
reducing the number of learnable parameters and training costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAlessandro_M/0/1/0/all/0/1&quot;&gt;Marco D&amp;#x27;Alessandro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_A/0/1/0/all/0/1&quot;&gt;Alberto Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calabres_E/0/1/0/all/0/1&quot;&gt;Enrique Calabr&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galar_M/0/1/0/all/0/1&quot;&gt;Mikel Galar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06583">
<title>Improving Masked Autoencoders by Learning Where to Mask. (arXiv:2303.06583v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06583</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked image modeling is a promising self-supervised learning method for
visual data. It is typically built upon image patches with random masks, which
largely ignores the variation of information density between them. The question
is: Is there a better masking strategy than random sampling and how can we
learn it? We empirically study this problem and initially find that introducing
object-centric priors in mask sampling can significantly improve the learned
representations. Inspired by this observation, we present AutoMAE, a fully
differentiable framework that uses Gumbel-Softmax to interlink an
adversarially-trained mask generator and a mask-guided image modeling process.
In this way, our approach can adaptively find patches with higher information
density for different images, and further strike a balance between the
information gain obtained from image reconstruction and its practical training
difficulty. In our experiments, AutoMAE is shown to provide effective
pretraining models on standard self-supervised benchmarks and downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haijian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wendong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10428">
<title>A Region-Prompted Adapter Tuning for Visual Abductive Reasoning. (arXiv:2303.10428v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10428</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Abductive Reasoning is an emerging vision-language (VL) topic where
the model needs to retrieve/generate a likely textual hypothesis from a visual
input (image or its part) using backward reasoning based on commonsense. Unlike
in conventional VL retrieval or captioning tasks, where entities of texts
appear in the image, in abductive inferences, the relevant facts about
inferences are not readily apparent in the input images. Besides, these
inferences are causally linked to specific regional visual cues and would
change as cues change. Existing works highlight cues utilizing a specific
prompt (e.g., colorful prompt). Then, a full fine-tuning of a VL foundation
model is launched to tweak its function from perception to deduction. However,
the colorful prompt uniformly patchify ``regional hints&apos;&apos; and ``global
context&apos;&apos; at the same granularity level and may lose fine-grained visual
details crucial for VAR. Meanwhile, full fine-tuning of VLF on limited data
would easily be overfitted.
&lt;/p&gt;
&lt;p&gt;To tackle this, we propose a simple yet effective Region-Prompted Adapter
(RPA), a hybrid parameter-efficient fine-tuning method that leverages the
strengths of detailed cues and efficient training for the VAR task.
RPA~consists of two novel modules: Regional Prompt Generator (RPG) and
Adapter$^\textbf{+}$. The prior encodes ``regional visual hints&apos;&apos; and ``global
contexts&apos;&apos; into visual prompts separately at fine and coarse-grained levels.
The latter extends the vanilla adapters with a new Map Adapter, which modifies
the attention map using a trainable low-dim query/key projection. Additionally,
we propose a new Dual-Contrastive Loss to regress the visual feature toward
features of factual description and plausible hypothesis. Experiments on the
Sherlock demonstrate that RPA outperforms previous SOTAs, achieving the 1st
rank on leaderboards (Comparison to Human Accuracy: RPA~31.74 vs CPT-CLIP
29.58).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ee_Y/0/1/0/all/0/1&quot;&gt;Yeo Keat Ee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1&quot;&gt;Basura Fernando&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14806">
<title>A Contrastive Learning Scheme with Transformer Innate Patches. (arXiv:2303.14806v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14806</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Contrastive Transformer, a contrastive learning scheme
using the Transformer innate patches. Contrastive Transformer enables existing
contrastive learning techniques, often used for image classification, to
benefit dense downstream prediction tasks such as semantic segmentation. The
scheme performs supervised patch-level contrastive learning, selecting the
patches based on the ground truth mask, subsequently used for hard-negative and
hard-positive sampling. The scheme applies to all vision-transformer
architectures, is easy to implement, and introduces minimal additional memory
footprint. Additionally, the scheme removes the need for huge batch sizes, as
each patch is treated as an image.
&lt;/p&gt;
&lt;p&gt;We apply and test Contrastive Transformer for the case of aerial image
segmentation, known for low-resolution data, large class imbalance, and similar
semantic classes. We perform extensive experiments to show the efficacy of the
Contrastive Transformer scheme on the ISPRS Potsdam aerial image segmentation
dataset. Additionally, we show the generalizability of our scheme by applying
it to multiple inherently different Transformer architectures. Ultimately, the
results show a consistent increase in mean IoU across all classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jyhne_S/0/1/0/all/0/1&quot;&gt;Sander Riis&amp;#xf8;en Jyhne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1&quot;&gt;Per-Arne Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16818">
<title>SimDistill: Simulated Multi-modal Distillation for BEV 3D Object Detection. (arXiv:2303.16818v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16818</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view camera-based 3D object detection has become popular due to its low
cost, but accurately inferring 3D geometry solely from camera data remains
challenging and may lead to inferior performance. Although distilling precise
3D geometry knowledge from LiDAR data could help tackle this challenge, the
benefits of LiDAR information could be greatly hindered by the significant
modality gap between different sensory modalities. To address this issue, we
propose a Simulated multi-modal Distillation (SimDistill) method by carefully
crafting the model architecture and distillation strategy. Specifically, we
devise multi-modal architectures for both teacher and student models, including
a LiDAR-camera fusion-based teacher and a simulated fusion-based student. Owing
to the ``identical&apos;&apos; architecture design, the student can mimic the teacher to
generate multi-modal features with merely multi-view images as input, where a
geometry compensation module is introduced to bridge the modality gap.
Furthermore, we propose a comprehensive multi-modal distillation scheme that
supports intra-modal, cross-modal, and multi-modal fusion distillation
simultaneously in the Bird&apos;s-eye-view space. Incorporating them together, our
SimDistill can learn better feature representations for 3D object detection
while maintaining a cost-effective camera-only deployment. Extensive
experiments validate the effectiveness and superiority of SimDistill over
state-of-the-art methods, achieving an improvement of 4.8\% mAP and 4.1\% NDS
over the baseline detector. The source code will be released at
https://github.com/ViTAE-Transformer/SimDistill.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haimei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00501">
<title>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS. (arXiv:2304.00501v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00501</link>
<description rdf:parseType="Literal">&lt;p&gt;YOLO has become a central real-time object detection system for robotics,
driverless cars, and video monitoring applications. We present a comprehensive
analysis of YOLO&apos;s evolution, examining the innovations and contributions in
each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with
Transformers. We start by describing the standard metrics and postprocessing;
then, we discuss the major changes in network architecture and training tricks
for each model. Finally, we summarize the essential lessons from YOLO&apos;s
development and provide a perspective on its future, highlighting potential
research directions to enhance real-time object detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terven_J/0/1/0/all/0/1&quot;&gt;Juan Terven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordova_Esparza_D/0/1/0/all/0/1&quot;&gt;Diana Cordova-Esparza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09691">
<title>DarSwin: Distortion Aware Radial Swin Transformer. (arXiv:2304.09691v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09691</link>
<description rdf:parseType="Literal">&lt;p&gt;Wide-angle lenses are commonly used in perception tasks requiring a large
field of view. Unfortunately, these lenses produce significant distortions,
making conventional models that ignore the distortion effects unable to adapt
to wide-angle images. In this paper, we present a novel transformer-based model
that automatically adapts to the distortion produced by wide-angle lenses. Our
proposed image encoder architecture, dubbed DarSwin, leverages the physical
characteristics of such lenses analytically defined by the radial distortion
profile. In contrast to conventional transformer-based architectures, DarSwin
comprises a radial patch partitioning, a distortion-based sampling technique
for creating token embeddings, and an angular position encoding for radial
patch merging. Compared to other baselines, DarSwin achieves the best results
on different datasets with significant gains when trained on bounded levels of
distortions (very low, low, medium, and high) and tested on all, including
out-of-distribution distortions. While the base DarSwin architecture requires
knowledge of the radial distortion profile, we show it can be combined with a
self-calibration network that estimates such a profile from the input image
itself, resulting in a completely uncalibrated pipeline. Finally, we also
present DarSwin-Unet, which extends DarSwin, to an encoder-decoder architecture
suitable for pixel-level tasks. We demonstrate its performance on depth
estimation and show through extensive experiments that DarSwin-Unet can perform
zero-shot adaptation to unseen distortions of different wide-angle lenses. The
code and models are publicly available at https://lvsn.github.io/darswin/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athwale_A/0/1/0/all/0/1&quot;&gt;Akshaya Athwale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shili_I/0/1/0/all/0/1&quot;&gt;Ichrak Shili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergeron_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;mile Bergeron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afrasiyabi_A/0/1/0/all/0/1&quot;&gt;Arman Afrasiyabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lague_J/0/1/0/all/0/1&quot;&gt;Justin Lag&amp;#xfc;e&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_O/0/1/0/all/0/1&quot;&gt;Ola Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Lalonde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10771">
<title>A Revisit of the Normalized Eight-Point Algorithm and A Self-Supervised Deep Solution. (arXiv:2304.10771v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10771</link>
<description rdf:parseType="Literal">&lt;p&gt;The Normalized Eight-Point algorithm has been widely viewed as the
cornerstone in two-view geometry computation, where the seminal Hartley&apos;s
normalization greatly improves the performance of the direct linear
transformation (DLT) algorithm. A natural question is, whether there exists and
how to find other normalization methods that may further improve the
performance as per each input sample. In this paper, we provide a novel
perspective and make two contributions towards this fundamental problem: 1) We
revisit the normalized eight-point algorithm and make a theoretical
contribution by showing the existence of different and better normalization
algorithms; 2) We present a deep convolutional neural network with a
self-supervised learning strategy to the normalization. Given eight pairs of
correspondences, our network directly predicts the normalization matrices, thus
learning to normalize each input sample. Our learning-based normalization
module could be integrated with both traditional (e.g., RANSAC) and deep
learning framework (affording good interpretability) with minimal efforts.
Extensive experiments on both synthetic and real images show the effectiveness
of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1&quot;&gt;Bin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuchao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1&quot;&gt;Yongduek Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1&quot;&gt;Mingyi He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11906">
<title>Transformer-based stereo-aware 3D object detection from binocular images. (arXiv:2304.11906v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11906</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have shown promising progress in various visual object detection
tasks, including monocular 2D/3D detection and surround-view 3D detection. More
importantly, the attention mechanism in the Transformer model and the image
correspondence in binocular stereo are both similarity-based. However, directly
applying existing Transformer-based detectors to binocular stereo 3D object
detection leads to slow convergence and significant precision drops. We argue
that a key cause of this defect is that existing Transformers ignore the
stereo-specific image correspondence information. In this paper, we explore the
model design of Transformers in binocular 3D object detection, focusing
particularly on extracting and encoding the task-specific image correspondence
information. To achieve this goal, we present TS3D, a Transformer-based
Stereo-aware 3D object detector. In the TS3D, a Disparity-Aware Positional
Encoding (DAPE) module is proposed to embed the image correspondence
information into stereo features. The correspondence is encoded as normalized
sub-pixel-level disparity and is used in conjunction with sinusoidal 2D
positional encoding to provide the 3D location information of the scene. To
extract enriched multi-scale stereo features, we propose a Stereo Preserving
Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the
correspondence information while fusing intra-scale and aggregating cross-scale
stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection
average precision on the KITTI test set and takes 88 ms to detect objects from
each binocular image pair. It is competitive with advanced counterparts in
terms of both precision and inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hanqing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yanwei Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiale Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16727">
<title>A Novel real-time arrhythmia detection model using YOLOv8. (arXiv:2305.16727v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16727</link>
<description rdf:parseType="Literal">&lt;p&gt;In a landscape characterized by heightened connectivity and mobility, coupled
with a surge in cardiovascular ailments, the imperative to curtail healthcare
expenses through remote monitoring of cardiovascular health has become more
pronounced. The accurate detection and classification of cardiac arrhythmias
are pivotal for diagnosing individuals with heart irregularities. This study
underscores the feasibility of employing electrocardiograms (ECG) measurements
in the home environment for real-time arrhythmia detection. Presenting a fresh
application for arrhythmia detection, this paper leverages the cutting-edge
You-Only-Look-Once (YOLO)v8 algorithm to categorize single-lead ECG signals. We
introduce a novel loss-modified YOLOv8 model, fine-tuned on the MIT-BIH
arrhythmia dataset, enabling real-time continuous monitoring. The obtained
results substantiate the efficacy of our approach, with the model attaining an
average accuracy of 99.5% and 0.992 mAP@50, and a rapid detection time of 0.002
seconds on an NVIDIA Tesla V100. Our investigation exemplifies the potential of
real-time arrhythmia detection, enabling users to visually interpret the model
output within the comfort of their homes. Furthermore, this study lays the
groundwork for an extension into a real-time explainable AI (XAI) model capable
of deployment in the healthcare sector, thereby significantly advancing the
realm of healthcare solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ang_G/0/1/0/all/0/1&quot;&gt;Guang Jun Nicholas Ang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goil_A/0/1/0/all/0/1&quot;&gt;Aritejh Kr Goil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Henryk Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lew_J/0/1/0/all/0/1&quot;&gt;Jieyi Jeric Lew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_X/0/1/0/all/0/1&quot;&gt;Xin Chun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustaffa_R/0/1/0/all/0/1&quot;&gt;Raihan Bin Ahmad Mustaffa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jason_T/0/1/0/all/0/1&quot;&gt;Timotius Jason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woon_Z/0/1/0/all/0/1&quot;&gt;Ze Ting Woon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bingquan Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03374">
<title>PGformer: Proxy-Bridged Game Transformer for Multi-Person Highly Interactive Extreme Motion Prediction. (arXiv:2306.03374v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03374</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-person motion prediction is a challenging task, especially for
real-world scenarios of highly interacted persons. Most previous works have
been devoted to studying the case of weak interactions (e.g., walking
together), in which typically forecasting each human pose in isolation can
still achieve good performances. This paper focuses on collaborative motion
prediction for multiple persons with extreme motions and attempts to explore
the relationships between the highly interactive persons&apos; pose trajectories.
Specifically, a novel cross-query attention (XQA) module is proposed to
bilaterally learn the cross-dependencies between the two pose sequences
tailored for this situation. A proxy unit is additionally introduced to bridge
the involved persons, which cooperates with our proposed XQA module and subtly
controls the bidirectional spatial information flows. These designs are then
integrated into a Transformer-based architecture and the resulting model is
called Proxy-bridged Game Transformer (PGformer) for multi-person interactive
motion prediction. Its effectiveness has been evaluated on the challenging ExPI
dataset, which involves highly interactive actions. Our PGformer consistently
outperforms the state-of-the-art methods in both short- and long-term
predictions by a large margin. Besides, our approach can also be compatible
with the weakly interacted CMU-Mocap and MuPoTS-3D datasets and extended to the
case of more than 2 individuals with encouraging results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yanwen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jintai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Peng-Tao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1&quot;&gt;Eddy K. F. Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guodong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11335">
<title>DamWorld: Progressive Reasoning with World Models for Robotic Manipulation. (arXiv:2306.11335v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11335</link>
<description rdf:parseType="Literal">&lt;p&gt;The research on embodied AI has greatly promoted the development of robot
manipulation. However, it still faces significant challenges in various aspects
such as benchmark construction, multi-modal perception and decision-making, and
physical execution. Previous robot manipulation simulators were primarily
designed to enrich manipulation types and types of objects while neglecting the
balance between physical manipulation and language instruction complexity in
multi-modal environments. This paper proposes a new robot manipulation
simulator and builds a comprehensive and systematic robot manipulation
benchmark with progressive reasoning tasks called SeaWave (i.e., a progressive
reasoning benchmark). It provides a standard test platform for embedded AI
agents in a multi-modal environment, which can evaluate and execute four levels
of human natural language instructions at the same time.
&lt;/p&gt;
&lt;p&gt;Previous world model-based robot manipulation work lacked research on the
perception and decision-making of complex instructions in multi-modal
environments. To this end, we propose a new world model tailored for
cross-modal robot manipulation called DamWorld. Specifically, DamWorld takes
the current visual scene and predicted execution actions based on natural
language instructions as input, and uses the next action frame to supervise the
output of the world model to force the model to learn robot manipulation
consistent with world knowledge. Compared with the renowned baselines (e.g.,
RT-1), our DamWorld improves the manipulation success rate by 5.6% on average
on four levels of progressive reasoning tasks. It is worth noting that on the
most challenging level 4 manipulation task, DamWorld still improved by 9.0%
compared to prior works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Pengzhen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaidong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hetao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengda Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mas Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03538">
<title>Language-free Compositional Action Generation via Decoupling Refinement. (arXiv:2307.03538v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03538</link>
<description rdf:parseType="Literal">&lt;p&gt;Composing simple elements into complex concepts is crucial yet challenging,
especially for 3D action generation. Existing methods largely rely on extensive
neural language annotations to discern composable latent semantics, a process
that is often costly and labor-intensive. In this study, we introduce a novel
framework to generate compositional actions without reliance on language
auxiliaries. Our approach consists of three main components: Action Coupling,
Conditional Action Generation, and Decoupling Refinement. Action Coupling
utilizes an energy model to extract the attention masks of each sub-action,
subsequently integrating two actions using these attentions to generate
pseudo-training examples. Then, we employ a conditional generative model, CVAE,
to learn a latent space, facilitating the diverse generation. Finally, we
propose Decoupling Refinement, which leverages a self-supervised pre-trained
model MAE to ensure semantic consistency between the sub-actions and
compositional actions. This refinement process involves rendering generated 3D
actions into 2D space, decoupling these images into two sub-segments, using the
MAE model to restore the complete image from sub-segments, and constraining the
recovered images to match images rendered from raw sub-actions. Due to the lack
of existing datasets containing both sub-actions and compositional actions, we
created two new datasets, named HumanAct-C and UESTC-C, and present a
corresponding evaluation metric. Both qualitative and quantitative assessments
are conducted to show our efficacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao-Ping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08919">
<title>Systematic comparison of semi-supervised and self-supervised learning for medical image classification. (arXiv:2307.08919v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08919</link>
<description rdf:parseType="Literal">&lt;p&gt;In many medical image classification problems, labeled data is scarce while
unlabeled data is more available. Semi-supervised learning and self-supervised
learning are two different research directions that can improve accuracy by
learning from extra unlabeled data. Recent methods from both directions have
reported significant gains on traditional benchmarks. Yet past benchmarks do
not focus on medical tasks and rarely compare self- and semi- methods together
on equal footing. Furthermore, past benchmarks often handle hyperparameter
tuning suboptimally. First, they may not tune hyperparameters at all, leading
to underfitting. Second, when tuning does occur, it often unrealistically uses
a labeled validation set much larger than the train set. Both cases make
previously published rankings of methods difficult to translate to practical
settings. This study contributes a systematic evaluation of self- and semi-
methods with a unified experimental protocol intended to guide a practitioner
with scarce overall labeled data and a limited compute budget. We answer two
key questions: Can hyperparameter tuning be effective with realistic-sized
validation sets? If so, when all methods are tuned well, which self- or
semi-supervised methods reach the best accuracy? Our study compares 13
representative semi- and self-supervised methods to strong labeled-set-only
baselines on 4 medical datasets. From 20000+ total GPU hours of computation, we
provide valuable best practices to resource-constrained, results-focused
practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ruijie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1&quot;&gt;Shuchin Aeron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Michael C. Hughes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08994">
<title>Human Action Recognition in Still Images Using ConViT. (arXiv:2307.08994v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08994</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the relationship between different parts of an image is crucial
in a variety of applications, including object recognition, scene
understanding, and image classification. Despite the fact that Convolutional
Neural Networks (CNNs) have demonstrated impressive results in classifying and
detecting objects, they lack the capability to extract the relationship between
different parts of an image, which is a crucial factor in Human Action
Recognition (HAR). To address this problem, this paper proposes a new module
that functions like a convolutional layer that uses Vision Transformer (ViT).
In the proposed model, the Vision Transformer can complement a convolutional
neural network in a variety of tasks by helping it to effectively extract the
relationship among various parts of an image. It is shown that the proposed
model, compared to a simple CNN, can extract meaningful parts of an image and
suppress the misleading parts. The proposed model has been evaluated on the
Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5% mean
Average Precision (mAP) and 91.5% mAP results, respectively, which are
promising compared to other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseyni_S/0/1/0/all/0/1&quot;&gt;Seyed Rohollah Hosseyni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seyedin_S/0/1/0/all/0/1&quot;&gt;Sanaz Seyedin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taheri_H/0/1/0/all/0/1&quot;&gt;Hasan Taheri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15421">
<title>MLIC++: Linear Complexity Attention-based Multi-Reference Entropy Modeling for Learned Image Compression. (arXiv:2307.15421v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15421</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learned image compression has achieved impressive performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in enhancing rate-distortion performance. However,
existing global context modules rely on computationally intensive quadratic
complexity computations to capture global correlations. This quadratic
complexity imposes limitations on the potential of high-resolution image
coding. Moreover, effectively capturing local, global, and channel-wise
contexts with acceptable even linear complexity within a single entropy model
remains a challenge. To address these limitations, we propose the Linear
Complexity Attention-based Multi-Reference Entropy Model (MEM++). MEM++
effectively captures the diverse range of correlations inherent in the latent
representation. Specifically, the latent representation is first divided into
multiple slices. When compressing a particular slice, the previously compressed
slices serve as its channel-wise contexts. To capture local contexts without
sacrificing performance, we introduce a novel checkerboard attention module.
Additionally, to capture global contexts, we propose the linear complexity
attention-based global correlations capturing by leveraging the decomposition
of the softmax operation. The attention map of the previously decoded slice is
implicitly computed and employed to predict global correlations in the current
slice. Based on MEM++, we propose image compression model MLIC++. Extensive
experimental evaluations demonstrate that our MLIC++ achieves state-of-the-art
performance, reducing BD-rate by 13.39% on the Kodak dataset compared to
VTM-17.0 in PSNR. Furthermore, MLIC++ exhibits linear GPU memory consumption
with resolution, making it highly suitable for high-resolution image coding.
Code and pre-trained models are available at
https://github.com/JiangWeibeta/MLIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ronggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04397">
<title>LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake Extraction from Remote Sensing Imagery. (arXiv:2308.04397v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04397</link>
<description rdf:parseType="Literal">&lt;p&gt;Lake extraction from remote sensing images is challenging due to the complex
lake shapes and inherent data noises. Existing methods suffer from blurred
segmentation boundaries and poor foreground modeling. This paper proposes a
hybrid CNN-Transformer architecture, called LEFormer, for accurate lake
extraction. LEFormer contains three main modules: CNN encoder, Transformer
encoder, and cross-encoder fusion. The CNN encoder effectively recovers local
spatial information and improves fine-scale details. Simultaneously, the
Transformer encoder captures long-range dependencies between sequences of any
length, allowing them to obtain global features and context information. The
cross-encoder fusion module integrates the local and global features to improve
mask prediction. Experimental results show that LEFormer consistently achieves
state-of-the-art performance and efficiency on the Surface Water and the
Qinghai-Tibet Plateau Lake datasets. Specifically, LEFormer achieves 90.86% and
97.42% mIoU on two datasets with a parameter count of 3.61M, respectively,
while being 20 minor than the previous best lake extraction method. The source
code is available at https://github.com/BastianChen/LEFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_P/0/1/0/all/0/1&quot;&gt;Pin Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13794">
<title>SOGDet: Semantic-Occupancy Guided Multi-view 3D Object Detection. (arXiv:2308.13794v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13794</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of autonomous driving, accurate and comprehensive perception of
the 3D environment is crucial. Bird&apos;s Eye View (BEV) based methods have emerged
as a promising solution for 3D object detection using multi-view images as
input. However, existing 3D object detection methods often ignore the physical
context in the environment, such as sidewalk and vegetation, resulting in
sub-optimal performance. In this paper, we propose a novel approach called
SOGDet (Semantic-Occupancy Guided Multi-view 3D Object Detection), that
leverages a 3D semantic-occupancy branch to improve the accuracy of 3D object
detection. In particular, the physical context modeled by semantic occupancy
helps the detector to perceive the scenes in a more holistic view. Our SOGDet
is flexible to use and can be seamlessly integrated with most existing
BEV-based methods. To evaluate its effectiveness, we apply this approach to
several state-of-the-art baselines and conduct extensive experiments on the
exclusive nuScenes dataset. Our results show that SOGDet consistently enhance
the performance of three baseline methods in terms of nuScenes Detection Score
(NDS) and mean Average Precision (mAP). This indicates that the combination of
3D object detection and 3D semantic occupancy leads to a more comprehensive
perception of the 3D environment, thereby aiding build more robust autonomous
driving systems. The codes are available at: https://github.com/zhouqiu/SOGDet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qiu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jinming Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_H/0/1/0/all/0/1&quot;&gt;Hanchao Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yifang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kun_Y/0/1/0/all/0/1&quot;&gt;Yu Kun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00096">
<title>AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation. (arXiv:2309.00096v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00096</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary semantic segmentation is a challenging task that requires
segmenting novel object categories at inference time. Recent studies have
explored vision-language pre-training to handle this task, but suffer from
unrealistic assumptions in practical scenarios, i.e., low-quality textual
category names. For example, this paradigm assumes that new textual categories
will be accurately and completely provided, and exist in lexicons during
pre-training. However, exceptions often happen when encountering ambiguity for
brief or incomplete names, new words that are not present in the pre-trained
lexicons, and difficult-to-describe categories for users. To address these
issues, this work proposes a novel attribute decomposition-aggregation
framework, AttrSeg, inspired by human cognition in understanding new concepts.
Specifically, in the decomposition stage, we decouple class names into diverse
attribute descriptions to complement semantic contexts from multiple
perspectives. Two attribute construction strategies are designed: using large
language models for common categories, and involving manually labeling for
human-invented categories. In the aggregation stage, we group diverse
attributes into an integrated global description, to form a discriminative
classifier that distinguishes the target object from others. One hierarchical
aggregation architecture is further proposed to achieve multi-level
aggregations, leveraging the meticulously designed clustering module. The final
results are obtained by computing the similarity between aggregated attributes
and images embeddings. To evaluate the effectiveness, we annotate three types
of datasets with attribute descriptions, and conduct extensive experiments and
ablation studies. The results show the superior performance of attribute
decomposition-aggregation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chaofan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Chen Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00314">
<title>ARFA: An Asymmetric Receptive Field Autoencoder Model for Spatiotemporal Prediction. (arXiv:2309.00314v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00314</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatiotemporal prediction aims to generate future sequences by paradigms
learned from historical contexts. It is essential in numerous domains, such as
traffic flow prediction and weather forecasting. Recently, research in this
field has been predominantly driven by deep neural networks based on
autoencoder architectures. However, existing methods commonly adopt autoencoder
architectures with identical receptive field sizes. To address this issue, we
propose an Asymmetric Receptive Field Autoencoder (ARFA) model, which
introduces corresponding sizes of receptive field modules tailored to the
distinct functionalities of the encoder and decoder. In the encoder, we present
a large kernel module for global spatiotemporal feature extraction. In the
decoder, we develop a small kernel module for local spatiotemporal information
reconstruction. Experimental results demonstrate that ARFA consistently
achieves state-of-the-art performance on popular datasets. Additionally, we
construct the RainBench, a large-scale radar echo dataset for precipitation
prediction, to address the scarcity of meteorological data in the domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Li Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00378">
<title>Long-Term Ad Memorability: Understanding and Generating Memorable Ads. (arXiv:2309.00378v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00378</link>
<description rdf:parseType="Literal">&lt;p&gt;Marketers spend billions of dollars on advertisements but to what end? At the
time of purchase, if customers cannot recognize the brand for which they saw an
ad, the money spent on the ad is essentially wasted. Despite its importance in
marketing, until now, there has been no study on the memorability of ads in the
ML literature. Most studies have been conducted on short-term recall (&amp;lt;5 mins)
on specific content types like object and action videos. On the other hand, the
advertising industry only cares about long-term memorability, and ads are
almost always highly multimodal, depicting a story through its different
modalities. With this motivation, we release the first large-scale memorability
dataset, LAMDBA, consisting of 1749 participants and 2205 ads covering 276
brands. Running statistical tests over different participant subpopulations and
ad types, we find many interesting insights into what makes an ad memorable.
For e.g., we find that brands that use commercials with fast-moving scenes are
more memorable than those with slower scenes (p=8e-10) and that people who use
ad-blockers remember fewer ads than those who don&apos;t (p=5e-3). Next, to simulate
the memorability of marketing materials for a particular audience, we present a
novel model, Henry, trained to leverage real-world knowledge of LLMs and visual
knowledge to predict the memorability. We test Henry on all the prominent
memorability datasets in literature (both images and videos) and achieve
state-of-the-art performance across all of them. Henry shows strong
generalization showing better results in 0-shot on unseen datasets. Next, we
propose the task of memorable ad generation and release a large-scale ad
dataset, UltraLAMBDA, consisting of 4 million ads with their Henry-assigned
memorability scores. We show that aligning Henry to generate memorable content
improves memorability scores by more than 25%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+I_H/0/1/0/all/0/1&quot;&gt;Harini S I&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Somesh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1&quot;&gt;Yaman K Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1&quot;&gt;Aanisha Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1&quot;&gt;Veeky Baths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajiv Ratn Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09196">
<title>Efficient Pyramid Channel Attention Network for Pathological Myopia Recognition. (arXiv:2309.09196v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09196</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathological myopia (PM) is the leading ocular disease for impaired vision
worldwide. Clinically, the characteristic of pathology distribution in PM is
global-local on the fundus image, which plays a significant role in assisting
clinicians in diagnosing PM. However, most existing deep neural networks
focused on designing complex architectures but rarely explored the pathology
distribution prior of PM. To tackle this issue, we propose an efficient pyramid
channel attention (EPCA) module, which fully leverages the potential of the
clinical pathology prior of PM with pyramid pooling and multi-scale context
fusion. Then, we construct EPCA-Net for automatic PM recognition based on
fundus images by stacking a sequence of EPCA modules. Moreover, motivated by
the recent pretraining-and-finetuning paradigm, we attempt to adapt pre-trained
natural image models for PM recognition by freezing them and treating the EPCA
and other attention modules as adapters. In addition, we construct a PM
recognition benchmark termed PM-fundus by collecting fundus images of PM from
publicly available datasets. The comprehensive experiments demonstrate the
superiority of our EPCA-Net over state-of-the-art methods in the PM recognition
task. The results also show that our method based on the
pretraining-and-finetuning paradigm achieves competitive performance through
comparisons to part of previous methods based on traditional fine-tuning
paradigm with fewer tunable parameters, which has the potential to leverage
more natural image foundation models to address the PM recognition task in
limited medical data regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jilu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiangtian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09272">
<title>Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation. (arXiv:2309.09272v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09272</link>
<description rdf:parseType="Literal">&lt;p&gt;With the frequent use of self-supervised monocular depth estimation in
robotics and autonomous driving, the model&apos;s efficiency is becoming
increasingly important. Most current approaches apply much larger and more
complex networks to improve the precision of depth estimation. Some researchers
incorporated Transformer into self-supervised monocular depth estimation to
achieve better performance. However, this method leads to high parameters and
high computation. We present a fully convolutional depth estimation network
using contextual feature fusion. Compared to UNet++ and HRNet, we use
high-resolution and low-resolution features to reserve information on small
targets and fast-moving objects instead of long-range fusion. We further
promote depth estimation results employing lightweight channel attention based
on convolution in the decoder stage. Our method reduces the parameters without
sacrificing accuracy. Experiments on the KITTI benchmark show that our method
can get better results than many large models, such as Monodepth2, with only 30
parameters. The source code is available at
https://github.com/boyagesmile/DNA-Depth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boya_W/0/1/0/all/0/1&quot;&gt;Wang Boya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuo_W/0/1/0/all/0/1&quot;&gt;Wang Shuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Ye Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziwen_D/0/1/0/all/0/1&quot;&gt;Dou Ziwen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07236">
<title>AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive Speech-Driven 3D Facial Animation. (arXiv:2310.07236v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07236</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-driven 3D facial animation aims at generating facial movements that
are synchronized with the driving speech, which has been widely explored
recently. Existing works mostly neglect the person-specific talking style in
generation, including facial expression and head pose styles. Several works
intend to capture the personalities by fine-tuning modules. However, limited
training data leads to the lack of vividness. In this work, we propose AdaMesh,
a novel adaptive speech-driven facial animation approach, which learns the
personalized talking style from a reference video of about 10 seconds and
generates vivid facial expressions and head poses. Specifically, we propose
mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,
which efficiently captures the facial expression style. For the personalized
pose style, we propose a pose adapter by building a discrete pose prior and
retrieving the appropriate style embedding with a semantic-aware pose style
matrix without fine-tuning. Extensive experimental results show that our
approach outperforms state-of-the-art methods, preserves the talking style in
the reference video, and generates vivid facial animation. The supplementary
video and code will be available at https://adamesh.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1&quot;&gt;Weihong Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shun Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Boshi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Shiyin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haozhi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11598">
<title>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors. (arXiv:2310.11598v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11598</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning neural implicit representations has achieved remarkable performance
in 3D reconstruction from multi-view images. Current methods use volume
rendering to render implicit representations into either RGB or depth images
that are supervised by multi-view ground truth. However, rendering a view each
time suffers from incomplete depth at holes and unawareness of occluded
structures from the depth supervision, which severely affects the accuracy of
geometry inference via volume rendering. To resolve this issue, we propose to
learn neural implicit representations from multi-view RGBD images through
volume rendering with an attentive depth fusion prior. Our prior allows neural
networks to perceive coarse 3D structures from the Truncated Signed Distance
Function (TSDF) fused from all depth images available for rendering. The TSDF
enables accessing the missing depth at holes on one depth image and the
occluded parts that are invisible from the current view. By introducing a novel
attention mechanism, we allow neural networks to directly use the depth fusion
prior with the inferred occupancy as the learned implicit function. Our
attention mechanism works with either a one-time fused TSDF that represents a
whole scene or an incrementally fused TSDF that represents a partial scene in
the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on
widely used benchmarks including synthetic and real-world scans show our
superiority over the latest neural implicit methods. Project page:
https://machineperceptionlab.github.io/Attentive_DF_Prior/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Pengchong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13378">
<title>ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD Map Construction. (arXiv:2310.13378v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13378</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel end-to-end pipeline for online long-range vectorized
high-definition (HD) map construction using on-board camera sensors. The
vectorized representation of HD maps, employing polylines and polygons to
represent map elements, is widely used by downstream tasks. However, previous
schemes designed with reference to dynamic object detection overlook the
structural constraints within linear map elements, resulting in performance
degradation in long-range scenarios. In this paper, we exploit the properties
of map elements to improve the performance of map construction. We extract more
accurate bird&apos;s eye view (BEV) features guided by their linear structure, and
then propose a hierarchical sparse map representation to further leverage the
scalability of vectorized map elements and design a progressive decoding
mechanism and a supervision strategy based on this representation. Our
approach, ScalableMap, demonstrates superior performance on the nuScenes
dataset, especially in long-range scenarios, surpassing previous
state-of-the-art model by 6.5 mAP while achieving 18.3 FPS. Code is available
at https://github.com/jingy1yu/ScalableMap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zizhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shengfu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1&quot;&gt;Jizhang Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15725">
<title>Ranking-based Adaptive Query Generation for DETRs in Crowded Pedestrian Detection. (arXiv:2310.15725v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15725</link>
<description rdf:parseType="Literal">&lt;p&gt;DEtection TRansformer (DETR) and its variants (DETRs) have been successfully
applied to crowded pedestrian detection, which achieved promising performance.
However, we find that, in different degrees of crowded scenes, the number of
DETRs&apos; queries must be adjusted manually, otherwise, the performance would
degrade to varying degrees. In this paper, we first analyze the two current
query generation methods and summarize four guidelines for designing the
adaptive query generation method. Then, we propose Rank-based Adaptive Query
Generation (RAQG) to alleviate the problem. Specifically, we design a rank
prediction head that can predict the rank of the lowest confidence positive
training sample produced by the encoder. Based on the predicted rank, we design
an adaptive selection method that can adaptively select coarse detection
results produced by the encoder to generate queries. Moreover, to train the
rank prediction head better, we propose Soft Gradient L1 Loss. The gradient of
Soft Gradient L1 Loss is continuous, which can describe the relationship
between the loss value and the updated value of model parameters granularly.
Our method is simple and effective, which can be plugged into any DETRs to make
it query-adaptive in theory. The experimental results on Crowdhuman dataset and
Citypersons dataset show that our method can adaptively generate queries for
DETRs and achieve competitive results. Especially, our method achieves
state-of-the-art 39.4% MR on Crowdhuman dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1&quot;&gt;Jiaxu Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1&quot;&gt;Ji Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16781">
<title>Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16781</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the mapping between sound and meaning in human language is assumed
to be largely arbitrary, research in cognitive science has shown that there are
non-trivial correlations between particular sounds and meanings across
languages and demographic groups, a phenomenon known as sound symbolism. Among
the many dimensions of meaning, sound symbolism is particularly salient and
well-demonstrated with regards to cross-modal associations between language and
the visual domain. In this work, we address the question of whether sound
symbolism is reflected in vision-and-language models such as CLIP and Stable
Diffusion. Using zero-shot knowledge probing to investigate the inherent
knowledge of these models, we find strong evidence that they do show this
pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our
work provides a novel method for demonstrating sound symbolism and
understanding its nature using computational tools. Our code will be made
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1&quot;&gt;Morris Alper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1&quot;&gt;Hadar Averbuch-Elor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16783">
<title>S$^3$-TTA: Scale-Style Selection for Test-Time Augmentation in Biomedical Image Segmentation. (arXiv:2310.16783v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16783</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep-learning models have been successful in biomedical image segmentation.
To generalize for real-world deployment, test-time augmentation (TTA) methods
are often used to transform the test image into different versions that are
hopefully closer to the training domain. Unfortunately, due to the vast
diversity of instance scale and image styles, many augmented test images
produce undesirable results, thus lowering the overall performance. This work
proposes a new TTA framework, S$^3$-TTA, which selects the suitable image scale
and style for each test image based on a transformation consistency metric. In
addition, S$^3$-TTA constructs an end-to-end augmentation-segmentation
joint-training pipeline to ensure a task-oriented augmentation. On public
benchmarks for cell and lung segmentation, S$^3$-TTA demonstrates improvements
over the prior art by 3.4% and 1.3%, respectively, by simply augmenting the
input data in testing phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1&quot;&gt;Kangxian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordone_S/0/1/0/all/0/1&quot;&gt;Sebastian Cajas Ordone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Donglai Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17468">
<title>Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17468</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, image-text matching has attracted more and more attention from
academia and industry, which is fundamental to understanding the latent
correspondence across visual and textual modalities. However, most existing
methods implicitly assume the training pairs are well-aligned while ignoring
the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby
inevitably leading to a performance drop. Although some methods attempt to
address such noise, they still face two challenging problems: excessive
memorizing/overfitting and unreliable correction for NC, especially under high
noise. To address the two problems, we propose a generalized Cross-modal Robust
Complementary Learning framework (CRCL), which benefits from a novel Active
Complementary Loss (ACL) and an efficient Self-refining Correspondence
Correction (SCC) to improve the robustness of existing methods. Specifically,
ACL exploits active and complementary learning losses to reduce the risk of
providing erroneous supervision, leading to theoretically and experimentally
demonstrated robustness against NC. SCC utilizes multiple self-refining
processes with momentum correction to enlarge the receptive field for
correcting correspondences, thereby alleviating error accumulation and
achieving accurate and stable corrections. We carry out extensive experiments
on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify
the superior robustness of our CRCL against synthetic and real-world noisy
correspondences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Peng Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03035">
<title>GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation. (arXiv:2311.03035v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03035</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have revolutionized the field of computer vision,
yet their deployments on resource-constrained devices remain challenging due to
high computational demands. To expedite pre-trained ViTs, token pruning and
token merging approaches have been developed, which aim at reducing the number
of tokens involved in the computation. However, these methods still have some
limitations, such as image information loss from pruned tokens and inefficiency
in the token-matching process. In this paper, we introduce a novel Graph-based
Token Propagation (GTP) method to resolve the challenge of balancing model
efficiency and information preservation for efficient ViTs. Inspired by graph
summarization algorithms, GTP meticulously propagates less significant tokens&apos;
information to spatially and semantically connected tokens that are of greater
importance. Consequently, the remaining few tokens serve as a summarization of
the entire token graph, allowing the method to reduce computational complexity
while preserving essential information of eliminated tokens. Combined with an
innovative token selection strategy, GTP can efficiently identify image tokens
to be propagated. Extensive experiments have validated GTP&apos;s effectiveness,
demonstrating both efficiency and performance improvements. Specifically, GTP
decreases the computational complexity of both DeiT-S and DeiT-B by up to 26%
with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and
remarkably surpasses the state-of-the-art token merging method on various
backbones at an even faster inference speed. The source code is available at
https://github.com/Ackesnal/GTP-ViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yanping Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12075">
<title>BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning. (arXiv:2311.12075v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12075</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying backdoor attacks is valuable for model copyright protection and
enhancing defenses. While existing backdoor attacks have successfully infected
multimodal contrastive learning models such as CLIP, they can be easily
countered by specialized backdoor defenses for MCL models. This paper reveals
the threats in this practical scenario that backdoor attacks can remain
effective even after defenses and introduces the \emph{\toolns} attack, which
is resistant to backdoor detection and model fine-tuning defenses. To achieve
this, we draw motivations from the perspective of the Bayesian rule and propose
a dual-embedding guided framework for backdoor attacks. Specifically, we ensure
that visual trigger patterns approximate the textual target semantics in the
embedding space, making it challenging to detect the subtle parameter
variations induced by backdoor learning on such natural trigger patterns.
Additionally, we optimize the visual trigger patterns to align the poisoned
samples with target vision features in order to hinder the backdoor unlearning
through clean fine-tuning. Extensive experiments demonstrate that our attack
significantly outperforms state-of-the-art baselines (+45.3% ASR) in the
presence of SoTA backdoor defenses, rendering these mitigation and detection
strategies virtually ineffective. Furthermore, our approach effectively attacks
some more rigorous scenarios like downstream tasks. We believe that this paper
raises awareness regarding the potential threats associated with the practical
application of multimodal contrastive learning and encourages the development
of more robust defense mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingli Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Ee-Chien Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14925">
<title>Coordinate-based Neural Network for Fourier Phase Retrieval. (arXiv:2311.14925v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14925</link>
<description rdf:parseType="Literal">&lt;p&gt;Fourier phase retrieval is essential for high-definition imaging of nanoscale
structures across diverse fields, notably coherent diffraction imaging. This
study presents the Single impliCit neurAl Network (SCAN), a tool built upon
coordinate neural networks meticulously designed for enhanced phase retrieval
performance. Remedying the drawbacks of conventional iterative methods which
are easiliy trapped into local minimum solutions and sensitive to noise, SCAN
adeptly connects object coordinates to their amplitude and phase within a
unified network in an unsupervised manner. While many existing methods
primarily use Fourier magnitude in their loss function, our approach
incorporates both the predicted magnitude and phase, enhancing retrieval
accuracy. Comprehensive tests validate SCAN&apos;s superiority over traditional and
other deep learning models regarding accuracy and noise robustness. We also
demonstrate that SCAN excels in the ptychography setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tingyou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zixin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1&quot;&gt;Yong S. Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaojing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jizhou Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17552">
<title>An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance. (arXiv:2311.17552v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17552</link>
<description rdf:parseType="Literal">&lt;p&gt;Tiger conservation necessitates the strategic deployment of multifaceted
initiatives encompassing the preservation of ecological habitats, anti-poaching
measures, and community involvement for sustainable growth in the tiger
population. With the advent of artificial intelligence, tiger surveillance can
be automated using object detection. In this paper, an accurate illumination
invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger
detection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without
illumination enhancement. The illumination enhancement improves the mAP by
0.7%. The approaches elevate the state-of-the-art performance on the ATRW
dataset by approximately 6% to 7%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pendharkar_G/0/1/0/all/0/1&quot;&gt;Gaurav Pendharkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheal_A/0/1/0/all/0/1&quot;&gt;A.Ancy Micheal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misquitta_J/0/1/0/all/0/1&quot;&gt;Jason Misquitta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaippada_R/0/1/0/all/0/1&quot;&gt;Ranjeesh Kaippada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03020">
<title>Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment. (arXiv:2312.03020v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03020</link>
<description rdf:parseType="Literal">&lt;p&gt;This research introduces a sophisticated transfer learning model based on
Google&apos;s MobileNetV2 for breast cancer tumor classification into normal,
benign, and malignant categories, utilizing a dataset of 1576 ultrasound images
(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of
0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and
MCC of 0.74. It examines image intensity distributions and misclassification
errors, offering improvements for future applications. Addressing dataset
imbalances, the study ensures a generalizable model. This work, using a dataset
from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,
emphasizes MobileNetV2&apos;s potential in medical imaging, aiming to improve
diagnostic precision in oncology. Additionally, the paper explores
Streamlit-based deployment for real-time tumor classification, demonstrating
MobileNetV2&apos;s applicability in medical imaging and setting a benchmark for
future research in oncology diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Surya_A/0/1/0/all/0/1&quot;&gt;Aaditya Surya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Aditya Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kabore_J/0/1/0/all/0/1&quot;&gt;Jarnell Kabore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sasikumar_S/0/1/0/all/0/1&quot;&gt;Subash Sasikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06164">
<title>Implicit Shape Modeling for Anatomical Structure Refinement of Volumetric Medical Images. (arXiv:2312.06164v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06164</link>
<description rdf:parseType="Literal">&lt;p&gt;Shape modeling of volumetric data is essential for medical image analysis and
computer-aided intervention. In practice, automated shape reconstruction cannot
always achieve satisfactory results due to limited image resolution and a lack
of sufficiently detailed shape priors used as constraints. In this paper, a
unified framework is proposed for 3D shape modelling and segmentation
refinement based on implicit neural networks. To learn a sharable shape prior
from different instances within the same category during training, physical
details of volumetric data are firstly used to construct Physical-Informed
Continuous Coordinate Transform (PICCT) for implicit shape modeling. For
improved shape representation, implicit shape constraints based on Signed
Distance Function (SDF) are used for both instances and latent templates. For
inference, a Template Interaction Module (TIM) is proposed to refine 3D shapes
produced by Convolutional Neural Networks (CNNs) via deforming deep implicit
templates with latent codes. Experimental results on validation datasets
involving liver, pancreas and lung segmentation demonstrate the superiority of
our approach in shape refinement and reconstruction. The Chamfer Distance/Earth
Mover&apos;s Distance achieved by the proposed method are 0.232/0.087 for the Liver
dataset, 0.128/0.069 for the Pancreas dataset, and 0.417/0.100 for the Lung
Lobe dataset, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1&quot;&gt;Xin You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang-Zhong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yun Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06454">
<title>Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images. (arXiv:2312.06454v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06454</link>
<description rdf:parseType="Literal">&lt;p&gt;Directly predicting human epidermal growth factor receptor 2 (HER2) status
from widely available hematoxylin and eosin (HE)-stained whole slide images
(WSIs) can reduce technical costs and expedite treatment selection. Accurately
predicting HER2 requires large collections of multi-site WSIs. Federated
learning enables collaborative training of these WSIs without gigabyte-size
WSIs transportation and data privacy concerns. However, federated learning
encounters challenges in addressing label imbalance in multi-site WSIs from the
real world. Moreover, existing WSI classification methods cannot simultaneously
exploit local context information and long-range dependencies in the site-end
feature representation of federated learning. To address these issues, we
present a point transformer with federated learning for multi-site HER2 status
prediction from HE-stained WSIs. Our approach incorporates two novel designs.
We propose a dynamic label distribution strategy and an auxiliary classifier,
which helps to establish a well-initialized model and mitigate label
distribution variations across sites. Additionally, we propose a farthest
cosine sampling based on cosine distance. It can sample the most distinctive
features and capture the long-range dependencies. Extensive experiments and
analysis show that our method achieves state-of-the-art performance at four
sites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can
generalize to two unseen sites with 229 WSIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Lizhi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_B/0/1/0/all/0/1&quot;&gt;Bensheng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bu_H/0/1/0/all/0/1&quot;&gt;Hong Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jie Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07507">
<title>NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention for Emotion Understanding. (arXiv:2312.07507v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07507</link>
<description rdf:parseType="Literal">&lt;p&gt;In the task of emotion recognition from videos, a key improvement has been to
focus on emotions over time rather than a single frame. There are many
architectures to address this task such as GRUs, LSTMs, Self-Attention,
Transformers, and Temporal Convolutional Networks (TCNs). However, these
methods suffer from high memory usage, large amounts of operations, or poor
gradients. We propose a method known as Neighborhood Attention with
Convolutions TCN (NAC-TCN) which incorporates the benefits of attention and
Temporal Convolutional Networks while ensuring that causal relationships are
understood which results in a reduction in computation and memory cost. We
accomplish this by introducing a causal version of Dilated Neighborhood
Attention while incorporating it with convolutions. Our model achieves
comparable, better, or state-of-the-art performance over TCNs, TCAN, LSTMs, and
GRUs while requiring fewer parameters on standard emotion recognition datasets.
We publish our code online for easy reproducibility and use in other projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Alexander Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;William Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10578">
<title>SAME: Sample Reconstruction against Model Extraction Attacks. (arXiv:2312.10578v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10578</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning models have shown significant performance across various
domains, their deployment needs extensive resources and advanced computing
infrastructure. As a solution, Machine Learning as a Service (MLaaS) has
emerged, lowering the barriers for users to release or productize their deep
learning models. However, previous studies have highlighted potential privacy
and security concerns associated with MLaaS, and one primary threat is model
extraction attacks. To address this, there are many defense solutions but they
suffer from unrealistic assumptions and generalization issues, making them less
practical for reliable protection. Driven by these limitations, we introduce a
novel defense mechanism, SAME, based on the concept of sample reconstruction.
This strategy imposes minimal prerequisites on the defender&apos;s capabilities,
eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user
query history, white-box model access, and additional intervention during model
training. It is compatible with existing active defense methods. Our extensive
experiments corroborate the superior efficacy of SAME over state-of-the-art
solutions. Our code is available at https://github.com/xythink/SAME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiqian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11973">
<title>Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11973</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO&apos;s effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Haeyong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12478">
<title>ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval. (arXiv:2312.12478v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12478</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robust
performance in generalized test scenarios, wherein data may belong to strictly
unknown domains and categories during training. Recently, pre-trained models
with prompt tuning have shown strong generalization capabilities and attained
noteworthy achievements in various downstream tasks, such as few-shot learning
and video-text retrieval. However, applying them directly to UCDR may not
sufficiently to handle both domain shift (i.e., adapting to unfamiliar domains)
and semantic shift (i.e., transferring to unknown categories). To this end, we
propose Prompting-to-Simulate (ProS), the first method to apply prompt tuning
for UCDR. ProS employs a two-step process to simulate Content-aware Dynamic
Prompts (CaDP) which can impact models to produce generalized features for
UCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Units
to individually capture domain and semantic knowledge in a mask-and-align way.
Then, in Context-aware Simulator Learning stage, we train a Content-aware
Prompt Simulator under a simulated test scenarios to produce the corresponding
CaDP. Extensive experiments conducted on three benchmark datasets show that our
method achieves new state-of-the-art performance without bringing excessive
parameters. Our method is publicly available at
https://anonymous.4open.science/r/ProS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1&quot;&gt;Pengpeng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13314">
<title>Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13314</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image synthesis, i.e., generating images from user-provided semantic
label maps, is an important conditional image generation task as it allows to
control both the content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art in generative image
modeling, the iterative nature of their inference process makes them
computationally demanding. Other approaches such as GANs are more efficient as
they only need a single feed-forward pass for generation, but the image quality
tends to suffer on large and diverse datasets. In this work, we propose a new
class of GAN discriminators for semantic image synthesis that generates highly
realistic images by exploiting feature backbone networks pre-trained for tasks
such as image classification. We also introduce a new generator architecture
with better context modeling and using cross-attention to inject noise into
latent variables, leading to more diverse generated images. Our model, which we
dub DP-SIMS, achieves state-of-the-art results in terms of image quality and
consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two orders of magnitude less
compute for inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrada_T/0/1/0/all/0/1&quot;&gt;Tariq Berrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1&quot;&gt;Karteek Alahari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14924">
<title>Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14924</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent successes in analyzing images with deep neural networks are almost
exclusively achieved with Convolutional Neural Networks (CNNs). The training of
these CNNs, and in fact of all deep neural network architectures, uses the
backpropagation algorithm where the output of the network is compared with the
desired result and the difference is then used to tune the weights of the
network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton
suggested an alternative way of training which passes the desired results
together with the images at the input of the network. This so called Forward
Forward (FF) algorithm has up to now only been used in fully connected
networks. In this paper, we show how the FF paradigm can be extended to CNNs.
Our FF-trained CNN, featuring a novel spatially-extended labeling technique,
achieves a classification accuracy of 99.16% on the MNIST hand-written digits
dataset. We show how different hyperparameters affect the performance of the
proposed algorithm and compare the results with CNN trained with the standard
backpropagation approach. Furthermore, we use Class Activation Maps to
investigate which type of features are learnt by the FF algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scodellaro_R/0/1/0/all/0/1&quot;&gt;Riccardo Scodellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Ajinkya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_F/0/1/0/all/0/1&quot;&gt;Frauke Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroter_M/0/1/0/all/0/1&quot;&gt;Matthias Schr&amp;#xf6;ter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15731">
<title>Adaptive FSS: A Novel Few-Shot Segmentation Framework via Prototype Enhancement. (arXiv:2312.15731v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15731</link>
<description rdf:parseType="Literal">&lt;p&gt;The Few-Shot Segmentation (FSS) aims to accomplish the novel class
segmentation task with a few annotated images. Current FSS research based on
meta-learning focus on designing a complex interaction mechanism between the
query and support feature. However, unlike humans who can rapidly learn new
things from limited samples, the existing approach relies solely on fixed
feature matching to tackle new tasks, lacking adaptability. In this paper, we
propose a novel framework based on the adapter mechanism, namely Adaptive FSS,
which can efficiently adapt the existing FSS model to the novel classes. In
detail, we design the Prototype Adaptive Module (PAM), which utilizes accurate
category information provided by the support set to derive class prototypes,
enhancing class-specific information in the multi-stage representation. In
addition, our approach is compatible with in diverse FSS methods with different
backbones by simply inserting PAM between the layers of the encoder.
Experiments demonstrate that our method effectively improves the performance of
the FSS models (e.g., MSANet, HDMNet, FPTrans, and DCAMA) and achieve new
state-of-the-art (SOTA) results (i.e., 72.4\% and 79.1\% mIoU on PASCAL-5$^i$
1-shot and 5-shot settings, 52.7\% and 60.0\% mIoU on COCO-20$^i$ 1-shot and
5-shot settings). Our code can be available at
https://github.com/jingw193/AdaptiveFSS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinagyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yisi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haoran Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianxiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16218">
<title>Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks. (arXiv:2312.16218v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16218</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving image-to-3D from a single view is an ill-posed problem, and current
neural reconstruction methods addressing it through diffusion models still rely
on scene-specific optimization, constraining their generalization capability.
To overcome the limitations of existing approaches regarding generalization and
consistency, we introduce a novel neural rendering technique. Our approach
employs the signed distance function as the surface representation and
incorporates generalizable priors through geometry-encoding volumes and
HyperNetworks. Specifically, our method builds neural encoding volumes from
generated multi-view inputs. We adjust the weights of the SDF network
conditioned on an input image at test-time to allow model adaptation to novel
scenes in a feed-forward manner via HyperNetworks. To mitigate artifacts
derived from the synthesized views, we propose the use of a volume transformer
module to improve the aggregation of image features instead of processing each
viewpoint separately. Through our proposed method, dubbed as Hyper-VolTran, we
avoid the bottleneck of scene-specific optimization and maintain consistency
across the images generated from multiple viewpoints. Our experiments show the
advantages of our proposed approach with consistent results and rapid
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_C/0/1/0/all/0/1&quot;&gt;Christian Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Sen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1&quot;&gt;Juan-Manuel Perez-Rua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengmeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benhalloum_A/0/1/0/all/0/1&quot;&gt;Amine Benhalloum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16451">
<title>Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16451</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have shown remarkable performance in image
classification. However, their performance significantly deteriorates with
corrupted input data. Domain generalization methods have been proposed to train
robust models against out-of-distribution data. Data augmentation in the
frequency domain is one of such approaches that enable a model to learn phase
features to establish domain-invariant representations. This approach changes
the amplitudes of the input data while preserving the phases. However, using
fixed phases leads to susceptibility to phase fluctuations because amplitudes
and phase fluctuations commonly occur in out-of-distribution. In this study, to
address this problem, we introduce an approach using finite variation of the
phases of input data rather than maintaining fixed phases. Based on the
assumption that the degree of domain-invariant features varies for each phase,
we propose a method to distinguish phases based on this degree. In addition, we
propose a method called vital phase augmentation (VIPAug) that applies the
variation to the phases differently according to the degree of domain-invariant
features of given phases. The model depends more on the vital phases that
contain more domain-invariant features for attaining robustness to amplitude
and phase fluctuations. We present experimental evaluations of our proposed
approach, which exhibited improved performance for both clean and corrupted
data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100
datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet
datasets. Our code is available at https://github.com/excitedkid/vipaug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ingyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wooju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1&quot;&gt;Hyun Myung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17205">
<title>EFHQ: Multi-purpose ExtremePose-Face-HQ dataset. (arXiv:2312.17205v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17205</link>
<description rdf:parseType="Literal">&lt;p&gt;The existing facial datasets, while having plentiful images at near frontal
views, lack images with extreme head poses, leading to the downgraded
performance of deep learning models when dealing with profile or pitched faces.
This work aims to address this gap by introducing a novel dataset named Extreme
Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k
high-quality images of faces at extreme poses. To produce such a massive
dataset, we utilize a novel and meticulous dataset processing pipeline to
curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many
high-resolution face videos captured in various settings. Our dataset can
complement existing datasets on various facial-related tasks, such as facial
synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation,
and face reenactment. Specifically, training with EFHQ helps models generalize
well across diverse poses, significantly improving performance in scenarios
involving extreme views, confirmed by extensive experiments. Additionally, we
utilize EFHQ to define a challenging cross-view face verification benchmark, in
which the performance of SOTA face recognition models drops 5-37% compared to
frontal-to-frontal scenarios, aiming to stimulate studies on face recognition
under severe pose conditions in the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1&quot;&gt;Trung Tuan Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1&quot;&gt;Duc Hong Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1&quot;&gt;Cuong Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1&quot;&gt;Anh Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00260">
<title>GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance. (arXiv:2401.00260v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00260</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decade, visual gaze estimation has garnered growing attention
within the research community, thanks to its wide-ranging application
scenarios. While existing estimation approaches have achieved remarkable
success in enhancing prediction accuracy, they primarily infer gaze directions
from single-image signals and discard the huge potentials of the currently
dominant text guidance. Notably, visual-language collaboration has been
extensively explored across a range of visual tasks, such as image synthesis
and manipulation, leveraging the remarkable transferability of large-scale
Contrastive Language-Image Pre-training (CLIP) model. Nevertheless, existing
gaze estimation approaches ignore the rich semantic cues conveyed by linguistic
signals and priors in CLIP feature space, thereby yielding performance
setbacks. In pursuit of making up this gap, we delve deeply into the text-eye
collaboration protocol and introduce a novel gaze estimation framework in this
paper, referred to as GazeCLIP. Specifically, we intricately design a
linguistic description generator to produce text signals with coarse
directional cues. Additionally, a CLIP-based backbone that excels in
characterizing text-eye pairs for gaze estimation is presented. This is
followed by the implementation of a fine-grained multi-modal fusion module
aimed at modeling the interrelationships between heterogeneous inputs.
Extensive experiments on three challenging datasets demonstrate the superiority
of the proposed GazeCLIP which surpasses the previous approaches and achieves
the state-of-the-art estimation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1&quot;&gt;Hao Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuanghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huachun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00608">
<title>Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00608</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera traps are valuable tools in animal ecology for biodiversity monitoring
and conservation. However, challenges like poor generalization to deployment at
new unseen locations limit their practical application. Images are naturally
associated with heterogeneous forms of context possibly in different
modalities. In this work, we leverage the structured context associated with
the camera trap images to improve out-of-distribution generalization for the
task of species identification in camera traps. For example, a photo of a wild
animal may be associated with information about where and when it was taken, as
well as structured biology knowledge about the animal species. While typically
overlooked by existing work, bringing back such context offers several
potential benefits for better image understanding, such as addressing data
scarcity and enhancing generalization. However, effectively integrating such
heterogeneous context into the visual domain is a challenging problem. To
address this, we propose a novel framework that reformulates species
classification as link prediction in a multimodal knowledge graph (KG). This
framework seamlessly integrates various forms of multimodal context for visual
recognition. We apply this framework for out-of-distribution species
classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets
and achieve competitive performance with state-of-the-art approaches.
Furthermore, our framework successfully incorporates biological taxonomy for
improved generalization and enhances sample efficiency for recognizing
under-represented species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1&quot;&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weidi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Cheng-Hao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong-You Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1&quot;&gt;Tanya Berger-Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1&quot;&gt;Charles Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00926">
<title>Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00926</link>
<description rdf:parseType="Literal">&lt;p&gt;In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients&apos; blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model&apos;s feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianjun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuxing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1&quot;&gt;Feiwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yu Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01074">
<title>AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided Diagnosis. (arXiv:2401.01074v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01074</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical data collected for making a diagnostic decision are typically
multi-modal and provide complementary perspectives of a subject. A
computer-aided diagnosis system welcomes multi-modal inputs; however, how to
effectively fuse such multi-modal data is a challenging task and attracts a lot
of attention in the medical research field. In this paper, we propose a
transformer-based framework, called Alifuse, for aligning and fusing
multi-modal medical data. Specifically, we convert images and unstructured and
structured texts into vision and language tokens, and use intramodal and
intermodal attention mechanisms to learn holistic representations of all
imaging and non-imaging data for classification. We apply Alifuse to classify
Alzheimer&apos;s disease and obtain state-of-the-art performance on five public
datasets, by outperforming eight baselines. The source code will be available
online later.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiuhui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yi Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01286">
<title>A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01286</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs&apos; behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zekun Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shengyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jintian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01383">
<title>Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01383</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital&apos;s local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pistos_M/0/1/0/all/0/1&quot;&gt;Michalis Pistos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weili Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1&quot;&gt;Islem Rekik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01470">
<title>TPC-ViT: Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01470</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01505">
<title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports. (arXiv:2401.01505v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01505</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning over sports videos for question answering is an important task with
numerous applications, such as player training and information retrieval.
However, this task has not been explored due to the lack of relevant datasets
and the challenging nature it presents. Most datasets for video question
answering (VideoQA) focus mainly on general and coarse-grained understanding of
daily-life videos, which is not applicable to sports scenarios requiring
professional action understanding and fine-grained motion analysis. In this
paper, we introduce the first dataset, named Sports-QA, specifically designed
for the sports VideoQA task. The Sports-QA dataset includes various types of
questions, such as descriptions, chronologies, causalities, and counterfactual
conditions, covering multiple sports. Furthermore, to address the
characteristics of the sports VideoQA task, we propose a new Auto-Focus
Transformer (AFT) capable of automatically focusing on particular scales of
temporal information for question answering. We conduct extensive experiments
on Sports-QA, including baseline studies and the evaluation of different
methods. The results demonstrate that our AFT achieves state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haopeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1&quot;&gt;Andong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1&quot;&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01651">
<title>AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01651</link>
<description rdf:parseType="Literal">&lt;p&gt;The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks. We have open-sourced the
dataset and evaluation code on the project website:
https://www.benchcouncil.org/AIGCBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fanda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wanling Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01918">
<title>Distilling Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection. (arXiv:2401.01918v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01918</link>
<description rdf:parseType="Literal">&lt;p&gt;Striking a balance between precision and efficiency presents a prominent
challenge in the bird&apos;s-eye-view (BEV) 3D object detection. Although previous
camera-based BEV methods achieved remarkable performance by incorporating
long-term temporal information, most of them still face the problem of low
efficiency. One potential solution is knowledge distillation. Existing
distillation methods only focus on reconstructing spatial features, while
overlooking temporal knowledge. To this end, we propose TempDistiller, a
Temporal knowledge Distiller, to acquire long-term memory from a teacher
detector when provided with a limited number of frames. Specifically, a
reconstruction target is formulated by integrating long-term temporal knowledge
through self-attention operation applied to feature teachers. Subsequently,
novel features are generated for masked student features via a generator.
Ultimately, we utilize this reconstruction target to reconstruct the student
features. In addition, we also explore temporal relational knowledge when
inputting full frames for the student model. We verify the effectiveness of the
proposed method on the nuScenes benchmark. The experimental results show our
method obtain an enhancement of +1.6 mAP and +1.1 NDS compared to the baseline,
a speed improvement of approximately 6 FPS after compressing temporal
knowledge, and the most accurate velocity estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Haowen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1&quot;&gt;Dong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jintao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_R/0/1/0/all/0/1&quot;&gt;Rui Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Weihao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yanyan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02142">
<title>GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation. (arXiv:2401.02142v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02142</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel cascaded diffusion-based generative
framework for text-driven human motion synthesis, which exploits a strategy
named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy
sets up generation objectives by grouping body joints of detailed skeletons in
close semantic proximity together and then replacing each of such joint group
with a single body-part node. Such an operation recursively abstracts a human
pose to coarser and coarser skeletons at multiple granularity levels. With
gradually increasing the abstraction level, human motion becomes more and more
concise and stable, significantly benefiting the cross-modal motion synthesis
task. The whole text-driven human motion synthesis problem is then divided into
multiple abstraction levels and solved with a multi-stage generation framework
with a cascaded latent diffusion model: an initial generator first generates
the coarsest human motion guess from a given text description; then, a series
of successive generators gradually enrich the motion details based on the
textual description and the previous synthesized results. Notably, we further
integrate GUESS with the proposed dynamic multi-condition fusion mechanism to
dynamically balance the cooperative effects of the given textual condition and
synthesized coarse motion prompt in different generation stages. Extensive
experiments on large-scale datasets verify that GUESS outperforms existing
state-of-the-art methods by large margins in terms of accuracy, realisticness,
and diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xuehao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Shaoyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhongqian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02317">
<title>BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model. (arXiv:2401.02317v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02317</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the challenge of image resolution variation for the
Segment Anything Model (SAM). SAM, known for its zero-shot generalizability,
exhibits a performance degradation when faced with datasets with varying image
sizes. Previous approaches tend to resize the image to a fixed size or adopt
structure modifications, hindering the preservation of SAM&apos;s rich prior
knowledge. Besides, such task-specific tuning necessitates a complete
retraining of the model, which is cost-expensive and unacceptable for
deployment in the downstream tasks. In this paper, we reformulate this issue as
a length extrapolation problem, where token sequence length varies while
maintaining a consistent patch size for images of different sizes. To this end,
we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM&apos;s
adaptability to varying image resolutions while eliminating the need for
structure modifications. Firstly, we introduce a new scaling factor to ensure
consistent magnitude in the attention layer&apos;s dot product values when the token
sequence length changes. Secondly, we present a bias-mode attention mask that
allows each token to prioritize neighboring information, mitigating the impact
of untrained distant information. Our BA-SAM demonstrates efficacy in two
scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,
including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to
significantly mitigate performance degradation in the zero-shot setting and
achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we
propose a generalized model and benchmark, showcasing BA-SAM&apos;s generalizability
across all four datasets simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yiran Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xuequan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02565">
<title>Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging. (arXiv:2401.02565v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02565</link>
<description rdf:parseType="Literal">&lt;p&gt;In the dynamic landscape of medical artificial intelligence, this study
explores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)
model, a Vision Language Foundation model, under targeted adversarial
conditions. Leveraging the Kather Colon dataset with 7,180 H&amp;amp;E images across
nine tissue types, our investigation employs Projected Gradient Descent (PGD)
adversarial attacks to intentionally induce misclassifications. The outcomes
reveal a 100% success rate in manipulating PLIP&apos;s predictions, underscoring its
susceptibility to adversarial perturbations. The qualitative analysis of
adversarial examples delves into the interpretability challenges, shedding
light on nuanced changes in predictions induced by adversarial manipulations.
These findings contribute crucial insights into the interpretability, domain
adaptation, and trustworthiness of Vision Language Models in medical imaging.
The study emphasizes the pressing need for robust defenses to ensure the
reliability of AI models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Veerla_J/0/1/0/all/0/1&quot;&gt;Jai Prakash Veerla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thota_P/0/1/0/all/0/1&quot;&gt;Poojitha Thota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guttikonda_P/0/1/0/all/0/1&quot;&gt;Partha Sai Guttikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nilizadeh_S/0/1/0/all/0/1&quot;&gt;Shirin Nilizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luber_J/0/1/0/all/0/1&quot;&gt;Jacob M. Luber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02804">
<title>DiffBody: Diffusion-based Pose and Shape Editing of Human Images. (arXiv:2401.02804v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02804</link>
<description rdf:parseType="Literal">&lt;p&gt;Pose and body shape editing in a human image has received increasing
attention. However, current methods often struggle with dataset biases and
deteriorate realism and the person&apos;s identity when users make large edits. We
propose a one-shot approach that enables large edits with identity
preservation. To enable large edits, we fit a 3D body model, project the input
image onto the 3D model, and change the body&apos;s pose and shape. Because this
initial textured body model has artifacts due to occlusion and the inaccurate
body shape, the rendered image undergoes a diffusion-based refinement, in which
strong noise destroys body structure and identity whereas insufficient noise
does not help. We thus propose an iterative refinement with weak noise, applied
first for the whole body and then for the face. We further enhance the realism
by fine-tuning text embeddings via self-supervised learning. Our quantitative
and qualitative evaluations demonstrate that our method outperforms other
existing methods across various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okuyama_Y/0/1/0/all/0/1&quot;&gt;Yuta Okuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_Y/0/1/0/all/0/1&quot;&gt;Yuki Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanamori_Y/0/1/0/all/0/1&quot;&gt;Yoshihiro Kanamori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02916">
<title>Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction. (arXiv:2401.02916v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02916</link>
<description rdf:parseType="Literal">&lt;p&gt;Human trajectory forecasting is a critical challenge in fields such as
robotics and autonomous driving. Due to the inherent uncertainty of human
actions and intentions in real-world scenarios, various unexpected occurrences
may arise. To uncover latent motion patterns in human behavior, we introduce a
novel memory-based method, named Motion Pattern Priors Memory Network. Our
method involves constructing a memory bank derived from clustered prior
knowledge of motion patterns observed in the training set trajectories. We
introduce an addressing mechanism to retrieve the matched pattern and the
potential target distributions for each prediction from the memory bank, which
enables the identification and retrieval of natural motion patterns exhibited
by agents, subsequently using the target priors memory token to guide the
diffusion model to generate predictions. Extensive experiments validate the
effectiveness of our approach, achieving state-of-the-art trajectory prediction
accuracy. The code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>