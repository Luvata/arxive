<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.02226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.05788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10926" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.11100">
<title>CSSL-RHA: Contrastive Self-Supervised Learning for Robust Handwriting Authentication. (arXiv:2307.11100v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11100</link>
<description rdf:parseType="Literal">&lt;p&gt;Handwriting authentication is a valuable tool used in various fields, such as
fraud prevention and cultural heritage protection. However, it remains a
challenging task due to the complex features, severe damage, and lack of
supervision. In this paper, we propose a novel Contrastive Self-Supervised
Learning framework for Robust Handwriting Authentication (CSSL-RHA) to address
these issues. It can dynamically learn complex yet important features and
accurately predict writer identities. Specifically, to remove the negative
effects of imperfections and redundancy, we design an information-theoretic
filter for pre-processing and propose a novel adaptive matching scheme to
represent images as patches of local regions dominated by more important
features. Through online optimization at inference time, the most informative
patch embeddings are identified as the &quot;most important&quot; elements. Furthermore,
we employ contrastive self-supervised training with a momentum-based paradigm
to learn more general statistical structures of handwritten data without
supervision. We conduct extensive experiments on five benchmark datasets and
our manually annotated dataset EN-HA, which demonstrate the superiority of our
CSSL-RHA compared to baselines. Additionally, we show that our proposed model
can still effectively achieve authentication even under abnormal circumstances,
such as data falsification and corruption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1&quot;&gt;Luntian Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wen Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11108">
<title>Flatness-Aware Minimization for Domain Generalization. (arXiv:2307.11108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11108</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) seeks to learn robust models that generalize well
under unknown distribution shifts. As a critical aspect of DG, optimizer
selection has not been explored in depth. Currently, most DG methods follow the
widely used benchmark, DomainBed, and utilize Adam as the default optimizer for
all datasets. However, we reveal that Adam is not necessarily the optimal
choice for the majority of current DG methods and datasets. Based on the
perspective of loss landscape flatness, we propose a novel approach,
Flatness-Aware Minimization for Domain Generalization (FAD), which can
efficiently optimize both zeroth-order and first-order flatness simultaneously
for DG. We provide theoretical analyses of the FAD&apos;s out-of-distribution (OOD)
generalization error and convergence. Our experimental results demonstrate the
superiority of FAD on various DG datasets. Additionally, we confirm that FAD is
capable of discovering flatter optima in comparison to other zeroth-order and
first-order flatness-aware optimization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renzhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yancheng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Pengfei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cu_P/0/1/0/all/0/1&quot;&gt;Peng Cu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11112">
<title>Comparison between transformers and convolutional models for fine-grained classification of insects. (arXiv:2307.11112v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11112</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained classification is challenging due to the difficulty of finding
discriminatory features. This problem is exacerbated when applied to
identifying species within the same taxonomical class. This is because species
are often sharing morphological characteristics that make them difficult to
differentiate. We consider the taxonomical class of Insecta. The identification
of insects is essential in biodiversity monitoring as they are one of the
inhabitants at the base of many ecosystems. Citizen science is doing brilliant
work of collecting images of insects in the wild giving the possibility to
experts to create improved distribution maps in all countries. We have billions
of images that need to be automatically classified and deep neural network
algorithms are one of the main techniques explored for fine-grained tasks. At
the SOTA, the field of deep learning algorithms is extremely fruitful, so how
to identify the algorithm to use? We focus on Odonata and Coleoptera orders,
and we propose an initial comparative study to analyse the two best-known layer
structures for computer vision: transformer and convolutional layers. We
compare the performance of T2TViT, a fully transformer-base, EfficientNet, a
fully convolutional-base, and ViTAE, a hybrid. We analyse the performance of
the three models in identical conditions evaluating the performance per
species, per morph together with sex, the inference time, and the overall
performance with unbalanced datasets of images from smartphones. Although we
observe high performances with all three families of models, our analysis shows
that the hybrid model outperforms the fully convolutional-base and fully
transformer-base models on accuracy performance and the fully transformer-base
model outperforms the others on inference speed and, these prove the
transformer to be robust to the shortage of samples and to be faster at
inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1&quot;&gt;Rita Pucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalkman_V/0/1/0/all/0/1&quot;&gt;Vincent J. Kalkman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11118">
<title>Diffusion Sampling with Momentum for Mitigating Divergence Artifacts. (arXiv:2307.11118v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11118</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable success of diffusion models in image generation, slow
sampling remains a persistent issue. To accelerate the sampling process, prior
studies have reformulated diffusion sampling as an ODE/SDE and introduced
higher-order numerical methods. However, these methods often produce divergence
artifacts, especially with a low number of sampling steps, which limits the
achievable acceleration. In this paper, we investigate the potential causes of
these artifacts and suggest that the small stability regions of these methods
could be the principal cause. To address this issue, we propose two novel
techniques. The first technique involves the incorporation of Heavy Ball (HB)
momentum, a well-known technique for improving optimization, into existing
diffusion numerical methods to expand their stability regions. We also prove
that the resulting methods have first-order convergence. The second technique,
called Generalized Heavy Ball (GHVB), constructs a new high-order method that
offers a variable trade-off between accuracy and artifact suppression.
Experimental results show that our techniques are highly effective in reducing
artifacts and improving image quality, surpassing state-of-the-art diffusion
solvers on both pixel-based and latent-based diffusion models for low-step
sampling. Our research provides novel insights into the design of numerical
methods for future diffusion work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wizadwongsa_S/0/1/0/all/0/1&quot;&gt;Suttisak Wizadwongsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinchuthakun_W/0/1/0/all/0/1&quot;&gt;Worameth Chinchuthakun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khungurn_P/0/1/0/all/0/1&quot;&gt;Pramook Khungurn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Amit Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1&quot;&gt;Supasorn Suwajanakorn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11130">
<title>Frequency-aware optical coherence tomography image super-resolution via conditional generative adversarial neural network. (arXiv:2307.11130v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2307.11130</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical coherence tomography (OCT) has stimulated a wide range of medical
image-based diagnosis and treatment in fields such as cardiology and
ophthalmology. Such applications can be further facilitated by deep
learning-based super-resolution technology, which improves the capability of
resolving morphological structures. However, existing deep learning-based
method only focuses on spatial distribution and disregard frequency fidelity in
image reconstruction, leading to a frequency bias. To overcome this limitation,
we propose a frequency-aware super-resolution framework that integrates three
critical frequency-based modules (i.e., frequency transformation, frequency
skip connection, and frequency alignment) and frequency-based loss function
into a conditional generative adversarial network (cGAN). We conducted a
large-scale quantitative study from an existing coronary OCT dataset to
demonstrate the superiority of our proposed framework over existing deep
learning frameworks. In addition, we confirmed the generalizability of our
framework by applying it to fish corneal images and rat retinal images,
demonstrating its capability to super-resolve morphological details in eye
imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueshen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongshan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kang_Mieler_J/0/1/0/all/0/1&quot;&gt;Jennifer J. Kang-Mieler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ling_Y/0/1/0/all/0/1&quot;&gt;Yuye Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yu Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11141">
<title>Towards General Game Representations: Decomposing Games Pixels into Content and Style. (arXiv:2307.11141v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11141</link>
<description rdf:parseType="Literal">&lt;p&gt;On-screen game footage contains rich contextual information that players
process when playing and experiencing a game. Learning pixel representations of
games can benefit artificial intelligence across several downstream tasks
including game-playing agents, procedural content generation, and player
modelling. The generalizability of these methods, however, remains a challenge,
as learned representations should ideally be shared across games with similar
game mechanics. This could allow, for instance, game-playing agents trained on
one game to perform well in similar games with no re-training. This paper
explores how generalizable pre-trained computer vision encoders can be for such
tasks, by decomposing the latent space into content embeddings and style
embeddings. The goal is to minimize the domain gap between games of the same
genre when it comes to game content critical for downstream tasks, and ignore
differences in graphical style. We employ a pre-trained Vision Transformer
encoder and a decomposition technique based on game genres to obtain separate
content and style embeddings. Our findings show that the decomposed embeddings
achieve style invariance across multiple games while still maintaining strong
content extraction capabilities. We argue that the proposed decomposition of
content and style offers better generalization capacities across game
environments independently of the downstream task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_C/0/1/0/all/0/1&quot;&gt;Chintan Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makantasis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Makantasis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1&quot;&gt;Antonios Liapis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1&quot;&gt;Georgios N. Yannakakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11197">
<title>Heuristic Hyperparameter Choice for Image Anomaly Detection. (arXiv:2307.11197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11197</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) in images is a fundamental computer vision problem by
deep learning neural network to identify images deviating significantly from
normality. The deep features extracted from pretrained models have been proved
to be essential for AD based on multivariate Gaussian distribution analysis.
However, since models are usually pretrained on a large dataset for
classification tasks such as ImageNet, they might produce lots of redundant
features for AD, which increases computational cost and degrades the
performance. We aim to do the dimension reduction of Negated Principal
Component Analysis (NPCA) for these features. So we proposed some heuristic to
choose hyperparameter of NPCA algorithm for getting as fewer components of
features as possible while ensuring a good performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o P. C. Bertoldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decenciere_E/0/1/0/all/0/1&quot;&gt;Etienne Decenci&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11227">
<title>UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models. (arXiv:2307.11227v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11227</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate the task of data pre-selection, which aims to
select instances for labeling from an unlabeled dataset through a single pass,
thereby optimizing performance for undefined downstream tasks with a limited
annotation budget. Previous approaches to data pre-selection relied solely on
visual features extracted from foundation models, such as CLIP and BLIP-2, but
largely ignored the powerfulness of text features. In this work, we argue that,
with proper design, the joint feature space of both vision and text can yield a
better representation for data pre-selection. To this end, we introduce UP-DP,
a simple yet effective unsupervised prompt learning approach that adapts
vision-language models, like BLIP-2, for data pre-selection. Specifically, with
the BLIP-2 parameters frozen, we train text prompts to extract the joint
features with improved representation, ensuring a diverse cluster structure
that covers the entire dataset. We extensively compare our method with the
state-of-the-art using seven benchmark datasets in different settings,
achieving up to a performance gain of 20%. Interestingly, the prompts learned
from one dataset demonstrate significant generalizability and can be applied
directly to enhance the feature extraction of BLIP-2 from other datasets. To
the best of our knowledge, UP-DP is the first work to incorporate unsupervised
prompt learning in a vision-language model for data pre-selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behpour_S/0/1/0/all/0/1&quot;&gt;Sima Behpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thang Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wenbin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1&quot;&gt;Liang Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liu Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11253">
<title>Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention. (arXiv:2307.11253v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11253</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has shown excellent performance in analysing medical images.
However, datasets are difficult to obtain due privacy issues, standardization
problems, and lack of annotations. We address these problems by producing
realistic synthetic images using a combination of 3D technologies and
generative adversarial networks. We propose CUT-seg, a joint training where a
segmentation model and a generative model are jointly trained to produce
realistic images while learning to segment polyps. We take advantage of recent
one-sided translation models because they use significantly less memory,
allowing us to add a segmentation model in the training loop. CUT-seg performs
better, is computationally less expensive, and requires less real images than
other memory-intensive image translation approaches that require two stage
training. Promising results are achieved on five real polyp segmentation
datasets using only one real image and zero real annotations. As a part of this
study we release Synth-Colon, an entirely synthetic dataset that includes 20000
realistic colon images and additional details about depth and 3D geometry:
https://enric1994.github.io/synth-colon
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreu_E/0/1/0/all/0/1&quot;&gt;Enric Moreu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arazo_E/0/1/0/all/0/1&quot;&gt;Eric Arazo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11259">
<title>Towards Non-Parametric Models for Confidence Aware Image Prediction from Low Data using Gaussian Processes. (arXiv:2307.11259v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11259</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to envision future states is crucial to informed decision making
while interacting with dynamic environments. With cameras providing a prevalent
and information rich sensing modality, the problem of predicting future states
from image sequences has garnered a lot of attention. Current state of the art
methods typically train large parametric models for their predictions. Though
often able to predict with accuracy, these models rely on the availability of
large training datasets to converge to useful solutions. In this paper we focus
on the problem of predicting future images of an image sequence from very
little training data. To approach this problem, we use non-parametric models to
take a probabilistic approach to image prediction. We generate probability
distributions over sequentially predicted images and propagate uncertainty
through time to generate a confidence metric for our predictions. Gaussian
Processes are used for their data efficiency and ability to readily incorporate
new training data online. We showcase our method by successfully predicting
future frames of a smooth fluid simulation environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinde_N/0/1/0/all/0/1&quot;&gt;Nikhil U. Shinde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_F/0/1/0/all/0/1&quot;&gt;Florian Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1&quot;&gt;Michael C. Yip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11261">
<title>SimCol3D -- 3D Reconstruction during Colonoscopy Challenge. (arXiv:2307.11261v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11261</link>
<description rdf:parseType="Literal">&lt;p&gt;Colorectal cancer is one of the most common cancers in the world. While
colonoscopy is an effective screening technique, navigating an endoscope
through the colon to detect polyps is challenging. A 3D map of the observed
surfaces could enhance the identification of unscreened colon tissue and serve
as a training platform. However, reconstructing the colon from video footage
remains unsolved due to numerous factors such as self-occlusion, reflective
surfaces, lack of texture, and tissue deformation that limit feature-based
methods. Learning-based approaches hold promise as robust alternatives, but
necessitate extensive datasets. By establishing a benchmark, the 2022 EndoVis
sub-challenge SimCol3D aimed to facilitate data-driven depth and pose
prediction during colonoscopy. The challenge was hosted as part of MICCAI 2022
in Singapore. Six teams from around the world and representatives from academia
and industry participated in the three sub-challenges: synthetic depth
prediction, synthetic pose prediction, and real pose prediction. This paper
describes the challenge, the submitted methods, and their results. We show that
depth prediction in virtual colonoscopy is robustly solvable, while pose
estimation remains an open research question.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rau_A/0/1/0/all/0/1&quot;&gt;Anita Rau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1&quot;&gt;Sophia Bano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yueming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azagra_P/0/1/0/all/0/1&quot;&gt;Pablo Azagra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morlana_J/0/1/0/all/0/1&quot;&gt;Javier Morlana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanderson_E/0/1/0/all/0/1&quot;&gt;Edward Sanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matuszewski_B/0/1/0/all/0/1&quot;&gt;Bogdan J. Matuszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Young Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dong-Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_E/0/1/0/all/0/1&quot;&gt;Erez Posner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frank_N/0/1/0/all/0/1&quot;&gt;Netanel Frank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elangovan_V/0/1/0/all/0/1&quot;&gt;Varshini Elangovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raviteja_S/0/1/0/all/0/1&quot;&gt;Sista Raviteja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiquan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalithkumar_S/0/1/0/all/0/1&quot;&gt;Seenivasan Lalithkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Mobarakol Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; M.M. Montiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11274">
<title>Screening Mammography Breast Cancer Detection. (arXiv:2307.11274v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11274</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast cancer is a leading cause of cancer-related deaths, but current
programs are expensive and prone to false positives, leading to unnecessary
follow-up and patient anxiety. This paper proposes a solution to automated
breast cancer detection, to improve the efficiency and accuracy of screening
programs. Different methodologies were tested against the RSNA dataset of
radiographic breast images of roughly 20,000 female patients and yielded an
average validation case pF1 score of 0.56 across methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chakraborty_D/0/1/0/all/0/1&quot;&gt;Debajyoti Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11285">
<title>MAS: Towards Resource-Efficient Federated Multiple-Task Learning. (arXiv:2307.11285v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11285</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous FL tasks could overload resource-constrained devices. In
this work, we propose the first FL system to effectively coordinate and train
multiple simultaneous FL tasks. We first formalize the problem of training
simultaneous FL tasks. Then, we present our new approach, MAS (Merge and
Split), to optimize the performance of training multiple simultaneous FL tasks.
MAS starts by merging FL tasks into an all-in-one FL task with a multi-task
architecture. After training for a few rounds, MAS splits the all-in-one FL
task into two or more FL tasks by using the affinities among tasks measured
during the all-in-one training. It then continues training each split of FL
tasks based on model parameters from the all-in-one training. Extensive
experiments demonstrate that MAS outperforms other methods while reducing
training time by 2x and reducing energy consumption by 40%. We hope this work
will inspire the community to further study and optimize training simultaneous
FL tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yonggang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11307">
<title>EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos. (arXiv:2307.11307v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11307</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing soft tissues from stereo endoscope videos is an essential
prerequisite for many medical applications. Previous methods struggle to
produce high-quality geometry and appearance due to their inadequate
representations of 3D scenes. To address this issue, we propose a novel
neural-field-based method, called EndoSurf, which effectively learns to
represent a deforming surface from an RGBD sequence. In EndoSurf, we model
surface dynamics, shape, and texture with three neural fields. First, 3D points
are transformed from the observed space to the canonical space using the
deformation field. The signed distance function (SDF) field and radiance field
then predict their SDFs and colors, respectively, with which RGBD images can be
synthesized via differentiable volume rendering. We constrain the learned shape
by tailoring multiple regularization strategies and disentangling geometry and
appearance. Experiments on public endoscope datasets demonstrate that EndoSurf
significantly outperforms existing solutions, particularly in reconstructing
high-fidelity shapes. Code is available at
https://github.com/Ruyi-Zha/endosurf.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_R/0/1/0/all/0/1&quot;&gt;Ruyi Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xuelian Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11308">
<title>DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport. (arXiv:2307.11308v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11308</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling from diffusion probabilistic models (DPMs) can be viewed as a
piecewise distribution transformation, which generally requires hundreds or
thousands of steps of the inverse diffusion trajectory to get a high-quality
image. Recent progress in designing fast samplers for DPMs achieves a trade-off
between sampling speed and sample quality by knowledge distillation or
adjusting the variance schedule or the denoising equation. However, it can&apos;t be
optimal in both aspects and often suffer from mode mixture in short steps. To
tackle this problem, we innovatively regard inverse diffusion as an optimal
transport (OT) problem between latents at different stages and propose the
DPM-OT, a unified learning framework for fast DPMs with a direct expressway
represented by OT map, which can generate high-quality samples within around 10
function evaluations. By calculating the semi-discrete optimal transport map
between the data latents and the white noise, we obtain an expressway from the
prior distribution to the data distribution, while significantly alleviating
the problem of mode mixture. In addition, we give the error bound of the
proposed method, which theoretically guarantees the stability of the algorithm.
Extensive experiments validate the effectiveness and advantages of DPM-OT in
terms of speed and quality (FID and mode mixture), thus representing an
efficient solution for generative modeling. Source codes are available at
https://github.com/cognaclee/DPM-OT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zezeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;ShengHao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1&quot;&gt;Na Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhongxuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xianfeng Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11315">
<title>Generating Image-Specific Text Improves Fine-grained Image Classification. (arXiv:2307.11315v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11315</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent vision-language models outperform vision-only models on many image
classification tasks. However, because of the absence of paired text/image
descriptions, it remains difficult to fine-tune these models for fine-grained
image classification. In this work, we propose a method, GIST, for generating
image-specific fine-grained text descriptions from image-only datasets, and
show that these text descriptions can be used to improve classification. Key
parts of our method include 1. prompting a pretrained large language model with
domain-specific prompts to generate diverse fine-grained text descriptions for
each class and 2. using a pretrained vision-language model to match each image
to label-preserving text descriptions that capture relevant visual features in
the image. We demonstrate the utility of GIST by fine-tuning vision-language
models on the image-and-generated-text pairs to learn an aligned
vision-language representation space for improved classification. We evaluate
our learned representation space in full-shot and few-shot scenarios across
four diverse fine-grained classification datasets, each from a different
domain. Our method achieves an average improvement of $4.1\%$ in accuracy over
CLIP linear probes and an average of $1.1\%$ improvement in accuracy over the
previous state-of-the-art image-text classification method on the full-shot
datasets. Our method achieves similar improvements across few-shot regimes.
Code is available at https://github.com/emu1729/GIST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_E/0/1/0/all/0/1&quot;&gt;Emily Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1&quot;&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1&quot;&gt;John Guttag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11317">
<title>XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge. (arXiv:2307.11317v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11317</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming Linear Discriminant Analysis (LDA) while proven in
Class-incremental Learning deployments at the edge with limited classes (upto
1000), has not been proven for deployment in extreme classification scenarios.
In this paper, we present: (a) XLDA, a framework for Class-IL in edge
deployment where LDA classifier is proven to be equivalent to FC layer
including in extreme classification scenarios, and (b) optimizations to enable
XLDA-based training and inference for edge deployment where there is a
constraint on available compute resources. We show up to 42x speed up using a
batched training approach and up to 5x inference speedup with nearest neighbor
search on extreme datasets like AliProducts (50k classes) and Google Landmarks
V2 (81k classes)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1&quot;&gt;Karan Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veerendranath_V/0/1/0/all/0/1&quot;&gt;Vishruth Veerendranath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebbar_A/0/1/0/all/0/1&quot;&gt;Anushka Hebbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1&quot;&gt;Raghavendra Bhat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11323">
<title>HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework. (arXiv:2307.11323v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11323</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of autonomous driving, 3D object detection is a very important
perception module. Although the current SOTA algorithm combines Camera and
Lidar sensors, limited by the high price of Lidar, the current mainstream
landing schemes are pure Camera sensors or Camera+Radar sensors. In this study,
we propose a new detection algorithm called HVDetFusion, which is a multi-modal
detection algorithm that not only supports pure camera data as input for
detection, but also can perform fusion input of radar data and camera data. The
camera stream does not depend on the input of Radar data, thus addressing the
downside of previous methods. In the pure camera stream, we modify the
framework of Bevdet4D for better perception and more efficient inference, and
this stream has the whole 3D detection output. Further, to incorporate the
benefits of Radar signals, we use the prior information of different object
positions to filter the false positive information of the original radar data,
according to the positioning information and radial velocity information
recorded by the radar sensors to supplement and fuse the BEV features generated
by the original camera data, and the effect is further improved in the process
of fusion training. Finally, HVDetFusion achieves the new state-of-the-art
67.4\% NDS on the challenging nuScenes test set among all camera-radar 3D
object detectors. The code is available at
https://github.com/HVXLab/HVDetFusion
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_K/0/1/0/all/0/1&quot;&gt;Kai Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1&quot;&gt;Shuman Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoteng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11334">
<title>Improving Transferability of Adversarial Examples via Bayesian Attacks. (arXiv:2307.11334v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11334</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a substantial extension of our work published at ICLR.
Our ICLR work advocated for enhancing transferability in adversarial examples
by incorporating a Bayesian formulation into model parameters, which
effectively emulates the ensemble of infinitely many deep neural networks,
while, in this paper, we introduce a novel extension by incorporating the
Bayesian formulation into the model input as well, enabling the joint
diversification of both the model input and model parameters. Our empirical
findings demonstrate that: 1) the combination of Bayesian formulations for both
the model input and model parameters yields significant improvements in
transferability; 2) by introducing advanced approximations of the posterior
distribution over the model input, adversarial transferability achieves further
enhancement, surpassing all state-of-the-arts when attacking without model
fine-tuning. Moreover, we propose a principled approach to fine-tune model
parameters in such an extended Bayesian formulation. The derived optimization
objective inherently encourages flat minima in the parameter space and input
space. Extensive experiments demonstrate that our method achieves a new
state-of-the-art on transfer-based attacks, improving the average success rate
on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with
our ICLR basic Bayesian method. We will make our code publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qizhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaochen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11335">
<title>Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields. (arXiv:2307.11335v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11335</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the tremendous progress in neural radiance fields (NeRF), we still
face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF
presents fine-detailed and anti-aliased renderings but takes days for training,
while Instant-ngp can accomplish the reconstruction in a few minutes but
suffers from blurring or aliasing when rendering at various distances or
resolutions due to ignoring the sampling area. To this end, we propose a novel
Tri-Mip encoding that enables both instant reconstruction and anti-aliased
high-fidelity rendering for neural radiance fields. The key is to factorize the
pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can
efficiently perform 3D area sampling by taking advantage of 2D pre-filtered
feature maps, which significantly elevates the rendering quality without
sacrificing efficiency. To cope with the novel Tri-Mip representation, we
propose a cone-casting rendering technique to efficiently sample anti-aliased
3D features with the Tri-Mip encoding considering both pixel imaging and
observing distance. Extensive experiments on both synthetic and real-world
datasets demonstrate our method achieves state-of-the-art rendering quality and
reconstruction speed while maintaining a compact representation that reduces
25% model size compared against Instant-ngp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bangbang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuewen Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11336">
<title>Character Time-series Matching For Robust License Plate Recognition. (arXiv:2307.11336v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11336</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic License Plate Recognition (ALPR) is becoming a popular study area
and is applied in many fields such as transportation or smart city. However,
there are still several limitations when applying many current methods to
practical problems due to the variation in real-world situations such as light
changes, unclear License Plate (LP) characters, and image quality. Almost
recent ALPR algorithms process on a single frame, which reduces accuracy in
case of worse image quality. This paper presents methods to improve license
plate recognition accuracy by tracking the license plate in multiple frames.
First, the Adaptive License Plate Rotation algorithm is applied to correctly
align the detected license plate. Second, we propose a method called Character
Time-series Matching to recognize license plate characters from many
consequence frames. The proposed method archives high performance in the
UFPR-ALPR dataset which is \boldmath$96.7\%$ accuracy in real-time on RTX A5000
GPU card. We also deploy the algorithm for the Vietnamese ALPR system. The
accuracy for license plate detection and character recognition are 0.881 and
0.979 $mAP^{test}$@.5 respectively. The source code is available at
https://github.com/chequanghuy/Character-Time-series-Matching.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Q/0/1/0/all/0/1&quot;&gt;Quang Huy Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thanh_T/0/1/0/all/0/1&quot;&gt;Tung Do Thanh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Van_C/0/1/0/all/0/1&quot;&gt;Cuong Truong Van&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11342">
<title>Tuning Pre-trained Model via Moment Probing. (arXiv:2307.11342v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11342</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, efficient fine-tuning of large-scale pre-trained models has
attracted increasing research interests, where linear probing (LP) as a
fundamental module is involved in exploiting the final representations for
task-dependent classification. However, most of the existing methods focus on
how to effectively introduce a few of learnable parameters, and little work
pays attention to the commonly used LP module. In this paper, we propose a
novel Moment Probing (MP) method to further explore the potential of LP.
Distinguished from LP which builds a linear classification head based on the
mean of final features (e.g., word tokens for ViT) or classification tokens,
our MP performs a linear classifier on feature distribution, which provides the
stronger representation ability by exploiting richer statistical information
inherent in features. Specifically, we represent feature distribution by its
characteristic function, which is efficiently approximated by using first- and
second-order moments of features. Furthermore, we propose a multi-head
convolutional cross-covariance (MHC$^3$) to compute second-order moments in an
efficient and effective manner. By considering that MP could affect feature
learning, we introduce a partially shared module to learn two recalibrating
parameters (PSRP) for backbones based on MP, namely MP$_{+}$. Extensive
experiments on ten benchmarks using various models show that our MP
significantly outperforms LP and is competitive with counterparts at less
training cost, while our MP$_{+}$ achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Mingze Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11360">
<title>ParGANDA: Making Synthetic Pedestrians A Reality For Object Detection. (arXiv:2307.11360v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11360</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection is the key technique to a number of Computer Vision
applications, but it often requires large amounts of annotated data to achieve
decent results. Moreover, for pedestrian detection specifically, the collected
data might contain some personally identifiable information (PII), which is
highly restricted in many countries. This label intensive and privacy
concerning task has recently led to an increasing interest in training the
detection models using synthetically generated pedestrian datasets collected
with a photo-realistic video game engine. The engine is able to generate
unlimited amounts of data with precise and consistent annotations, which gives
potential for significant gains in the real-world applications. However, the
use of synthetic data for training introduces a synthetic-to-real domain shift
aggravating the final performance. To close the gap between the real and
synthetic data, we propose to use a Generative Adversarial Network (GAN), which
performsparameterized unpaired image-to-image translation to generate more
realistic images. The key benefit of using the GAN is its intrinsic preference
of low-level changes to geometric ones, which means annotations of a given
synthetic image remain accurate even after domain translation is performed thus
eliminating the need for labeling real data. We extensively experimented with
the proposed method using MOTSynth dataset to train and MOT17 and MOT20
detection datasets to test, with experimental results demonstrating the
effectiveness of this method. Our approach not only produces visually plausible
samples but also does not require any labels of the real domain thus making it
applicable to the variety of downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reshetova_D/0/1/0/all/0/1&quot;&gt;Daria Reshetova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guanhang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puyat_M/0/1/0/all/0/1&quot;&gt;Marcel Puyat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chunhui Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huizhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11364">
<title>Photo2Relief: Let Human in the Photograph Stand Out. (arXiv:2307.11364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11364</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a technique for making humans in photographs
protrude like reliefs. Unlike previous methods which mostly focus on the face
and head, our method aims to generate art works that describe the whole body
activity of the character. One challenge is that there is no ground-truth for
supervised deep learning. We introduce a sigmoid variant function to manipulate
gradients tactfully and train our neural networks by equipping with a loss
function defined in gradient domain. The second challenge is that actual
photographs often across different light conditions. We used image-based
rendering technique to address this challenge and acquire rendering images and
depth data under different lighting conditions. To make a clear division of
labor in network modules, a two-scale architecture is proposed to create
high-quality relief from a single photograph. Extensive experimental results on
a variety of scenes show that our method is a highly effective solution for
generating digital 2.5D artwork from photographs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1&quot;&gt;Zhongping Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_F/0/1/0/all/0/1&quot;&gt;Feifei Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanshuo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu-Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11375">
<title>LatentAugment: Data Augmentation via Guided Manipulation of GAN&apos;s Latent Space. (arXiv:2307.11375v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11375</link>
<description rdf:parseType="Literal">&lt;p&gt;Data Augmentation (DA) is a technique to increase the quantity and diversity
of the training data, and by that alleviate overfitting and improve
generalisation. However, standard DA produces synthetic data for augmentation
with limited diversity. Generative Adversarial Networks (GANs) may unlock
additional information in a dataset by generating synthetic samples having the
appearance of real images. However, these models struggle to simultaneously
address three key requirements: fidelity and high-quality samples; diversity
and mode coverage; and fast sampling. Indeed, GANs generate high-quality
samples rapidly, but have poor mode coverage, limiting their adoption in DA
applications. We propose LatentAugment, a DA strategy that overcomes the low
diversity of GANs, opening up for use in DA applications. Without external
supervision, LatentAugment modifies latent vectors and moves them into latent
space regions to maximise the synthetic images&apos; diversity and fidelity. It is
also agnostic to the dataset and the downstream task. A wide set of experiments
shows that LatentAugment improves the generalisation of a deep model
translating from MRI-to-CT beating both standard DA as well GAN-based sampling.
Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic
samples show superior mode coverage and diversity. Code is available at:
https://github.com/ltronchin/LatentAugment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tronchin_L/0/1/0/all/0/1&quot;&gt;Lorenzo Tronchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1&quot;&gt;Minh H. Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1&quot;&gt;Paolo Soda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lofstedt_T/0/1/0/all/0/1&quot;&gt;Tommy L&amp;#xf6;fstedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11386">
<title>CLR: Channel-wise Lightweight Reprogramming for Continual Learning. (arXiv:2307.11386v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11386</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning aims to emulate the human ability to continually
accumulate knowledge over sequential tasks. The main challenge is to maintain
performance on previously learned tasks after learning new tasks, i.e., to
avoid catastrophic forgetting. We propose a Channel-wise Lightweight
Reprogramming (CLR) approach that helps convolutional neural networks (CNNs)
overcome catastrophic forgetting during continual learning. We show that a CNN
model trained on an old task (or self-supervised proxy task) could be
``reprogrammed&quot; to solve a new task by using our proposed lightweight (very
cheap) reprogramming parameter. With the help of CLR, we have a better
stability-plasticity trade-off to solve continual learning problems: To
maintain stability and retain previous task ability, we use a common
task-agnostic immutable part as the shared ``anchor&quot; parameter set. We then add
task-specific lightweight reprogramming parameters to reinterpret the outputs
of the immutable parts, to enable plasticity and integrate new knowledge. To
learn sequential tasks, we only train the lightweight reprogramming parameters
to learn each new task. Reprogramming parameters are task-specific and
exclusive to each task, which makes our method immune to catastrophic
forgetting. To minimize the parameter requirement of reprogramming to learn new
tasks, we make reprogramming lightweight by only adjusting essential kernels
and learning channel-wise linear mappings from anchor parameters to
task-specific domain knowledge. We show that, for general CNNs, the CLR
parameter increase is less than 0.6\% for any new task. Our method outperforms
13 state-of-the-art continual learning baselines on a new challenging sequence
of 53 image classification datasets. Code and data are available at
https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yunhao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuecheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1&quot;&gt;Shuo Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiaping Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1&quot;&gt;Laurent Itti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11397">
<title>Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation. (arXiv:2307.11397v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11397</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation is a challenging task, particularly due to inter-
and intra-observer variability, even between medical experts. In this paper, we
propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer
variation NetwOrk (Pionono). It captures the labeling behavior of each rater
with a multidimensional probability distribution and integrates this
information with the feature maps of the image to produce probabilistic
segmentation predictions. The model is optimized by variational inference and
can be trained end-to-end. It outperforms state-of-the-art models such as
STAPLE, Probabilistic U-Net, and models based on confusion matrices.
Additionally, Pionono predicts multiple coherent segmentation maps that mimic
the rater&apos;s expert opinion, which provides additional valuable information for
the diagnostic process. Experiments on real-world cancer segmentation datasets
demonstrate the high accuracy and efficiency of Pionono, making it a powerful
tool for medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schmidt_A/0/1/0/all/0/1&quot;&gt;Arne Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morales_Alvarez_P/0/1/0/all/0/1&quot;&gt;Pablo Morales-&amp;#xc1;lvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1&quot;&gt;Rafael Molina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11404">
<title>Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition. (arXiv:2307.11404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11404</link>
<description rdf:parseType="Literal">&lt;p&gt;Most research on facial expression recognition (FER) is conducted in highly
controlled environments, but its performance is often unacceptable when applied
to real-world situations. This is because when unexpected objects occlude the
face, the FER network faces difficulties extracting facial features and
accurately predicting facial expressions. Therefore, occluded FER (OFER) is a
challenging problem. Previous studies on occlusion-aware FER have typically
required fully annotated facial images for training. However, collecting facial
images with various occlusions and expression annotations is time-consuming and
expensive. Latent-OFER, the proposed method, can detect occlusions, restore
occluded parts of the face as if they were unoccluded, and recognize them,
improving FER accuracy. This approach involves three steps: First, the vision
transformer (ViT)-based occlusion patch detector masks the occluded position by
training only latent vectors from the unoccluded patches using the support
vector data description algorithm. Second, the hybrid reconstruction network
generates the masking position as a complete image using the ViT and
convolutional neural network (CNN). Last, the expression-relevant latent vector
extractor retrieves and uses expression-related information from all latent
vectors by applying a CNN-based class activation map. This mechanism has a
significant advantage in preventing performance degradation from occlusion by
unseen objects. The experimental results on several databases demonstrate the
superiority of the proposed method over state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Isack Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1&quot;&gt;Eungi Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Seok Bong Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11410">
<title>Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning. (arXiv:2307.11410v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11410</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in personalized image generation using diffusion models has
been significant. However, development in the area of open-domain and
non-fine-tuning personalized image generation is proceeding rather slowly. In
this paper, we propose Subject-Diffusion, a novel open-domain personalized
image generation model that, in addition to not requiring test-time
fine-tuning, also only requires a single reference image to support
personalized generation of single- or multi-subject in any domain. Firstly, we
construct an automatic data labeling tool and use the LAION-Aesthetics dataset
to construct a large-scale dataset consisting of 76M images and their
corresponding subject detection bounding boxes, segmentation masks and text
descriptions. Secondly, we design a new unified framework that combines text
and image semantics by incorporating coarse location and fine-grained reference
image control to maximize subject fidelity and generalization. Furthermore, we
also adopt an attention control mechanism to support multi-subject generation.
Extensive qualitative and quantitative results demonstrate that our method
outperforms other SOTA frameworks in single, multiple, and human customized
image generation. Please refer to our
\href{https://oppo-mente-lab.github.io/subject_diffusion/}{project page}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Junhao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haonan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11411">
<title>Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11411</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) are brain-inspired energy-efficient models
that encode information in spatiotemporal dynamics. Recently, deep SNNs trained
directly have shown great success in achieving high performance on
classification tasks with very few time steps. However, how to design a
directly-trained SNN for the regression task of object detection still remains
a challenging problem. To address this problem, we propose EMS-YOLO, a novel
directly-trained SNN framework for object detection, which is the first trial
to train a deep SNN with surrogate gradients for object detection rather than
ANN-SNN conversion strategies. Specifically, we design a full-spike residual
block, EMS-ResNet, which can effectively extend the depth of the
directly-trained SNN with low power consumption. Furthermore, we theoretically
analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding.
The results demonstrate that our approach outperforms the state-of-the-art
ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time
steps (only 4 time steps). It is shown that our model could achieve comparable
performance to the ANN with the same architecture while consuming 5.83 times
less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qiaoyi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1&quot;&gt;Yuhong Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yifan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Shijie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11413">
<title>A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11413</link>
<description rdf:parseType="Literal">&lt;p&gt;Examinations are a crucial part of the learning process, and academic
institutions invest significant resources into maintaining their integrity by
preventing cheating from students or facilitators. However, cheating has become
rampant in examination setups, compromising their integrity. The traditional
method of relying on invigilators to monitor every student is impractical and
ineffective. To address this issue, there is a need to continuously record exam
sessions to monitor students for suspicious activities. However, these
recordings are often too lengthy for invigilators to analyze effectively, and
fatigue may cause them to miss significant details. To widen the coverage,
invigilators could use fixed overhead or wearable cameras. This paper
introduces a framework that uses automation to analyze videos and detect
suspicious activities during examinations efficiently and effectively. We
utilized the OpenPose framework and Convolutional Neural Network (CNN) to
identify students exchanging objects during exams. This detection system is
vital in preventing cheating and promoting academic integrity, fairness, and
quality education for institutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moyo_R/0/1/0/all/0/1&quot;&gt;Reuben Moyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ndebvu_S/0/1/0/all/0/1&quot;&gt;Stanley Ndebvu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimba_M/0/1/0/all/0/1&quot;&gt;Michael Zimba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mbelwa_J/0/1/0/all/0/1&quot;&gt;Jimmy Mbelwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11418">
<title>FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields. (arXiv:2307.11418v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11418</link>
<description rdf:parseType="Literal">&lt;p&gt;As recent advances in Neural Radiance Fields (NeRF) have enabled
high-fidelity 3D face reconstruction and novel view synthesis, its manipulation
also became an essential task in 3D vision. However, existing manipulation
methods require extensive human labor, such as a user-provided semantic mask
and manual attribute search unsuitable for non-expert users. Instead, our
approach is designed to require a single text to manipulate a face
reconstructed with NeRF. To do so, we first train a scene manipulator, a latent
code-conditional deformable NeRF, over a dynamic scene to control a face
deformation using the latent code. However, representing a scene deformation
with a single latent code is unfavorable for compositing local deformations
observed in different instances. As so, our proposed Position-conditional
Anchor Compositor (PAC) learns to represent a manipulated scene with spatially
varying latent codes. Their renderings with the scene manipulator are then
optimized to yield high cosine similarity to a target text in CLIP embedding
space for text-driven manipulation. To the best of our knowledge, our approach
is the first to address the text-driven manipulation of a face reconstructed
with NeRF. Extensive results, comparisons, and ablation studies demonstrate the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sungwon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyung_J/0/1/0/all/0/1&quot;&gt;Junha Hyung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daejin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Min-Jung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11434">
<title>Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11434</link>
<description rdf:parseType="Literal">&lt;p&gt;The batch size is an essential parameter to tune during the development of
new neural networks. Amongst other quality indicators, it has a large degree of
influence on the model&apos;s accuracy, generalisability, training times and
parallelisability. This fact is generally known and commonly studied. However,
during the application phase of a deep learning model, when the model is
utilised by an end-user for inference, we find that there is a disregard for
the potential benefits of introducing a batch size. In this study, we examine
the effect of input batching on the energy consumption and response times of
five fully-trained neural networks for computer vision that were considered
state-of-the-art at the time of their publication. The results suggest that
batching has a significant effect on both of these metrics. Furthermore, we
present a timeline of the energy efficiency and accuracy of neural networks
over the past decade. We find that in general, energy consumption rises at a
much steeper pace than accuracy and question the necessity of this evolution.
Additionally, we highlight one particular network, ShuffleNetV2(2018), that
achieved a competitive performance for its time while maintaining a much lower
energy consumption. Nevertheless, we highlight that the results are model
dependent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarally_T/0/1/0/all/0/1&quot;&gt;Tim Yarally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feitosa_D/0/1/0/all/0/1&quot;&gt;Daniel Feitosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sallou_J/0/1/0/all/0/1&quot;&gt;June Sallou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1&quot;&gt;Arie van Deursen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11438">
<title>Attention Consistency Refined Masked Frequency Forgery Representation for Generalizing Face Forgery Detection. (arXiv:2307.11438v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11438</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the successful development of deep image generation technology, visual
data forgery detection would play a more important role in social and economic
security. Existing forgery detection methods suffer from unsatisfactory
generalization ability to determine the authenticity in the unseen domain. In
this paper, we propose a novel Attention Consistency Refined masked frequency
forgery representation model toward generalizing face forgery detection
algorithm (ACMF). Most forgery technologies always bring in high-frequency
aware cues, which make it easy to distinguish source authenticity but difficult
to generalize to unseen artifact types. The masked frequency forgery
representation module is designed to explore robust forgery cues by randomly
discarding high-frequency information. In addition, we find that the forgery
attention map inconsistency through the detection network could affect the
generalizability. Thus, the forgery attention consistency is introduced to
force detectors to focus on similar attention regions for better generalization
ability. Experiment results on several public face forgery datasets
(FaceForensic++, DFD, Celeb-DF, and WDF datasets) demonstrate the superior
performance of the proposed method compared with the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chunlei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Ruimin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11458">
<title>Strip-MLP: Efficient Token Interaction for Vision MLP. (arXiv:2307.11458v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11458</link>
<description rdf:parseType="Literal">&lt;p&gt;Token interaction operation is one of the core modules in MLP-based models to
exchange and aggregate information between different spatial locations.
However, the power of token interaction on the spatial dimension is highly
dependent on the spatial resolution of the feature maps, which limits the
model&apos;s expressive ability, especially in deep layers where the feature are
down-sampled to a small spatial size. To address this issue, we present a novel
method called \textbf{Strip-MLP} to enrich the token interaction power in three
ways. Firstly, we introduce a new MLP paradigm called Strip MLP layer that
allows the token to interact with other tokens in a cross-strip manner,
enabling the tokens in a row (or column) to contribute to the information
aggregations in adjacent but different strips of rows (or columns). Secondly, a
\textbf{C}ascade \textbf{G}roup \textbf{S}trip \textbf{M}ixing \textbf{M}odule
(CGSMM) is proposed to overcome the performance degradation caused by small
spatial feature size. The module allows tokens to interact more effectively in
the manners of within-patch and cross-patch, which is independent to the
feature spatial size. Finally, based on the Strip MLP layer, we propose a novel
\textbf{L}ocal \textbf{S}trip \textbf{M}ixing \textbf{M}odule (LSMM) to boost
the token interaction power in the local region. Extensive experiments
demonstrate that Strip-MLP significantly improves the performance of MLP-based
models on small datasets and obtains comparable or even better results on
ImageNet. In particular, Strip-MLP models achieve higher average Top-1 accuracy
than existing MLP-based models by +2.44\% on Caltech-101 and +2.16\% on
CIFAR-100. The source codes will be available
at~\href{https://github.com/Med-Process/Strip_MLP{https://github.com/Med-Process/Strip\_MLP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1&quot;&gt;Guiping Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shengda Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenjian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1&quot;&gt;Xiangyuan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongmei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11466">
<title>MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction. (arXiv:2307.11466v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11466</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving accurate material segmentation for 3-channel RGB images is
challenging due to the considerable variation in a material&apos;s appearance.
Hyperspectral images, which are sets of spectral measurements sampled at
multiple wavelengths, theoretically offer distinct information for material
identification, as variations in intensity of electromagnetic radiation
reflected by a surface depend on the material composition of a scene. However,
existing hyperspectral datasets are impoverished regarding the number of images
and material categories for the dense material segmentation task, and
collecting and annotating hyperspectral images with a spectral camera is
prohibitively expensive. To address this, we propose a new model, the
MatSpectNet to segment materials with recovered hyperspectral images from RGB
images. The network leverages the principles of colour perception in modern
cameras to constrain the reconstructed hyperspectral images and employs the
domain adaptation method to generalise the hyperspectral reconstruction
capability from a spectral recovery dataset to material segmentation datasets.
The reconstructed hyperspectral images are further filtered using learned
response curves and enhanced with human perception. The performance of
MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces
dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase
in average pixel accuracy and a 3.42% improvement in mean class accuracy
compared with the most recent publication. The project code is attached to the
supplementary material and will be published on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_Y/0/1/0/all/0/1&quot;&gt;Yuwen Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yihong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasmahapatra_S/0/1/0/all/0/1&quot;&gt;Srinandan Dasmahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hansung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11469">
<title>Distribution Shift Matters for Knowledge Distillation with Webly Collected Images. (arXiv:2307.11469v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11469</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation aims to learn a lightweight student network from a
pre-trained teacher network. In practice, existing knowledge distillation
methods are usually infeasible when the original training data is unavailable
due to some privacy issues and data management considerations. Therefore,
data-free knowledge distillation approaches proposed to collect training
instances from the Internet. However, most of them have ignored the common
distribution shift between the instances from original training data and webly
collected data, affecting the reliability of the trained student network. To
solve this problem, we propose a novel method dubbed ``Knowledge Distillation
between Different Distributions&quot; (KD$^{3}$), which consists of three
components. Specifically, we first dynamically select useful training instances
from the webly collected data according to the combined predictions of teacher
network and student network. Subsequently, we align both the weighted features
and classifier parameters of the two networks for knowledge memorization.
Meanwhile, we also build a new contrastive learning block called
MixDistribution to generate perturbed data with a new distribution for instance
alignment, so that the student network can further learn a
distribution-invariant representation. Intensive experiments on various
benchmark datasets demonstrate that our proposed KD$^{3}$ can outperform the
state-of-the-art data-free knowledge distillation approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jialiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11470">
<title>Physics-Aware Semi-Supervised Underwater Image Enhancement. (arXiv:2307.11470v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11470</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater images normally suffer from degradation due to the transmission
medium of water bodies. Both traditional prior-based approaches and deep
learning-based methods have been used to address this problem. However, the
inflexible assumption of the former often impairs their effectiveness in
handling diverse underwater scenes, while the generalization of the latter to
unseen images is usually weakened by insufficient data. In this study, we
leverage both the physics-based underwater Image Formation Model (IFM) and deep
learning techniques for Underwater Image Enhancement (UIE). To this end, we
propose a novel Physics-Aware Dual-Stream Underwater Image Enhancement Network,
i.e., PA-UIENet, which comprises a Transmission Estimation Steam (T-Stream) and
an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE
task by explicitly estimating the degradation parameters of the IFM. We also
adopt an IFM-inspired semi-supervised learning framework, which exploits both
the labeled and unlabeled images, to address the issue of insufficient data.
Our method performs better than, or at least comparably to, eight baselines
across five testing sets in the degradation estimation and UIE tasks. This
should be due to the fact that it not only can model the degradation but also
can learn the characteristics of diverse underwater scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Hao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xinghui Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11471">
<title>Robust Visual Question Answering: Datasets, Methods, and Future Challenges. (arXiv:2307.11471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11471</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering requires a system to provide an accurate natural
language answer given an image and a natural language question. However, it is
widely recognized that previous generic VQA methods often exhibit a tendency to
memorize biases present in the training data rather than learning proper
behaviors, such as grounding images before predicting answers. Therefore, these
methods usually achieve high in-distribution but poor out-of-distribution
performance. In recent years, various datasets and debiasing methods have been
proposed to evaluate and enhance the VQA robustness, respectively. This paper
provides the first comprehensive survey focused on this emerging fashion.
Specifically, we first provide an overview of the development process of
datasets from in-distribution and out-of-distribution perspectives. Then, we
examine the evaluation metrics employed by these datasets. Thirdly, we propose
a typology that presents the development process, similarities and differences,
robustness comparison, and technical features of existing debiasing methods.
Furthermore, we analyze and discuss the robustness of representative
vision-and-language pre-training models on VQA. Finally, through a thorough
review of the available literature and experimental analysis, we discuss the
key areas for future research from various viewpoints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jie Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pinghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1&quot;&gt;Dechen Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1&quot;&gt;Hongbin Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junzhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11477">
<title>SA-BEV: Generating Semantic-Aware Bird&apos;s-Eye-View Feature for Multi-view 3D Object Detection. (arXiv:2307.11477v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11477</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the pure camera-based Bird&apos;s-Eye-View (BEV) perception provides a
feasible solution for economical autonomous driving. However, the existing
BEV-based multi-view 3D detectors generally transform all image features into
BEV features, without considering the problem that the large proportion of
background information may submerge the object information. In this paper, we
propose Semantic-Aware BEV Pooling (SA-BEVPool), which can filter out
background information according to the semantic segmentation of image features
and transform image features into semantic-aware BEV features. Accordingly, we
propose BEV-Paste, an effective data augmentation strategy that closely matches
with semantic-aware BEV feature. In addition, we design a Multi-Scale
Cross-Task (MSCT) head, which combines task-specific and cross-task information
to predict depth distribution and semantic segmentation more accurately,
further improving the quality of semantic-aware BEV feature. Finally, we
integrate the above modules into a novel multi-view 3D object detection
framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves
state-of-the-art performance. Code has been available at
https://github.com/mengtan00/SA-BEV.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11482">
<title>Redemption from Range-view for Accurate 3D Object Detection. (arXiv:2307.11482v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11482</link>
<description rdf:parseType="Literal">&lt;p&gt;Most recent approaches for 3D object detection predominantly rely on
point-view or bird&apos;s-eye view representations, with limited exploration of
range-view-based methods. The range-view representation suffers from scale
variation and surface texture deficiency, both of which pose significant
limitations for developing corresponding methods. Notably, the surface texture
loss problem has been largely ignored by all existing methods, despite its
significant impact on the accuracy of range-view-based 3D object detection. In
this study, we propose Redemption from Range-view R-CNN (R2 R-CNN), a novel and
accurate approach that comprehensively explores the range-view representation.
Our proposed method addresses scale variation through the HD Meta Kernel, which
captures range-view geometry information in multiple scales. Additionally, we
introduce Feature Points Redemption (FPR) to recover the lost 3D surface
texture information from the range view, and Synchronous-Grid RoI Pooling
(S-Grid RoI Pooling), a multi-scaled approach with multiple receptive fields
for accurate box refinement. Our R2 R-CNN outperforms existing range-view-based
methods, achieving state-of-the-art performance on both the KITTI benchmark and
the Waymo Open Dataset. Our study highlights the critical importance of
addressing the surface texture loss problem for accurate 3D object detection in
range-view-based methods. Codes will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qiao Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11513">
<title>Bone mineral density estimation from a plain X-ray image by learning decomposition into projections of bone-segmented computed tomography. (arXiv:2307.11513v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11513</link>
<description rdf:parseType="Literal">&lt;p&gt;Osteoporosis is a prevalent bone disease that causes fractures in fragile
bones, leading to a decline in daily living activities. Dual-energy X-ray
absorptiometry (DXA) and quantitative computed tomography (QCT) are highly
accurate for diagnosing osteoporosis; however, these modalities require special
equipment and scan protocols. To frequently monitor bone health, low-cost,
low-dose, and ubiquitously available diagnostic methods are highly anticipated.
In this study, we aim to perform bone mineral density (BMD) estimation from a
plain X-ray image for opportunistic screening, which is potentially useful for
early diagnosis. Existing methods have used multi-stage approaches consisting
of extraction of the region of interest and simple regression to estimate BMD,
which require a large amount of training data. Therefore, we propose an
efficient method that learns decomposition into projections of bone-segmented
QCT for BMD estimation under limited datasets. The proposed method achieved
high accuracy in BMD estimation, where Pearson correlation coefficients of
0.880 and 0.920 were observed for DXA-measured BMD and QCT-measured BMD
estimation tasks, respectively, and the root mean square of the coefficient of
variation values were 3.27 to 3.79% for four measurements with different poses.
Furthermore, we conducted extensive validation experiments, including
multi-pose, uncalibrated-CT, and compression experiments toward actual
application in routine clinical practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1&quot;&gt;Yoshito Otake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uemura_K/0/1/0/all/0/1&quot;&gt;Keisuke Uemura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soufi_M/0/1/0/all/0/1&quot;&gt;Mazen Soufi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takao_M/0/1/0/all/0/1&quot;&gt;Masaki Takao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Talbot_H/0/1/0/all/0/1&quot;&gt;Hugues Talbot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Okada_S/0/1/0/all/0/1&quot;&gt;Seiji Okada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sugano_N/0/1/0/all/0/1&quot;&gt;Nobuhiko Sugano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11514">
<title>CORE: Cooperative Reconstruction for Multi-Agent Perception. (arXiv:2307.11514v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11514</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents CORE, a conceptually simple, effective and
communication-efficient model for multi-agent cooperative perception. It
addresses the task from a novel perspective of cooperative reconstruction,
based on two key insights: 1) cooperating agents together provide a more
holistic observation of the environment, and 2) the holistic observation can
serve as valuable supervision to explicitly guide the model learning how to
reconstruct the ideal observation based on collaboration. CORE instantiates the
idea with three major components: a compressor for each agent to create more
compact feature representation for efficient broadcasting, a lightweight
attentive collaboration component for cross-agent message aggregation, and a
reconstruction module to reconstruct the observation based on aggregated
feature representations. This learning-to-reconstruct idea is task-agnostic,
and offers clear and reasonable supervision to inspire more effective
collaboration, eventually promoting perception tasks. We validate CORE on
OPV2V, a large-scale multi-agent percetion dataset, in two tasks, i.e., 3D
object detection and semantic segmentation. Results demonstrate that the model
achieves state-of-the-art performance on both tasks, and is more
communication-efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binglu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaozhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianfei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11518">
<title>BatMobility: Towards Flying Without Seeing for Autonomous Drones. (arXiv:2307.11518v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.11518</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned aerial vehicles (UAVs) rely on optical sensors such as cameras and
lidar for autonomous operation. However, such optical sensors are error-prone
in bad lighting, inclement weather conditions including fog and smoke, and
around textureless or transparent surfaces. In this paper, we ask: is it
possible to fly UAVs without relying on optical sensors, i.e., can UAVs fly
without seeing? We present BatMobility, a lightweight mmWave radar-only
perception system for UAVs that eliminates the need for optical sensors.
BatMobility enables two core functionalities for UAVs -- radio flow estimation
(a novel FMCW radar-based alternative for optical flow based on
surface-parallel doppler shift) and radar-based collision avoidance. We build
BatMobility using commodity sensors and deploy it as a real-time system on a
small off-the-shelf quadcopter running an unmodified flight controller. Our
evaluation shows that BatMobility achieves comparable or better performance
than commercial-grade optical sensors across a wide range of scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sie_E/0/1/0/all/0/1&quot;&gt;Emerson Sie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zikun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasisht_D/0/1/0/all/0/1&quot;&gt;Deepak Vasisht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11519">
<title>Multi-modal Hate Speech Detection using Machine Learning. (arXiv:2307.11519v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11519</link>
<description rdf:parseType="Literal">&lt;p&gt;With the continuous growth of internet users and media content, it is very
hard to track down hateful speech in audio and video. Converting video or audio
into text does not detect hate speech accurately as human sometimes uses
hateful words as humorous or pleasant in sense and also uses different voice
tones or show different action in the video. The state-ofthe-art hate speech
detection models were mostly developed on a single modality. In this research,
a combined approach of multimodal system has been proposed to detect hate
speech from video contents by extracting feature images, feature values
extracted from the audio, text and used machine learning and Natural language
processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boishakhi_F/0/1/0/all/0/1&quot;&gt;Fariha Tahosin Boishakhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shill_P/0/1/0/all/0/1&quot;&gt;Ponkoj Chandra Shill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Golam Rabiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11526">
<title>CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields. (arXiv:2307.11526v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11526</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) have the potential to be a major representation
of media. Since training a NeRF has never been an easy task, the protection of
its model copyright should be a priority. In this paper, by analyzing the pros
and cons of possible copyright protection solutions, we propose to protect the
copyright of NeRF models by replacing the original color representation in NeRF
with a watermarked color representation. Then, a distortion-resistant rendering
scheme is designed to guarantee robust message extraction in 2D renderings of
NeRF. Our proposed method can directly protect the copyright of NeRF models
while maintaining high rendering quality and bit accuracy when compared among
optional solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1&quot;&gt;Ka Chun Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1&quot;&gt;Simon See&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1&quot;&gt;Renjie Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11528">
<title>Improving Viewpoint Robustness for Visual Recognition via Adversarial Training. (arXiv:2307.11528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11528</link>
<description rdf:parseType="Literal">&lt;p&gt;Viewpoint invariance remains challenging for visual recognition in the 3D
world, as altering the viewing directions can significantly impact predictions
for the same object. While substantial efforts have been dedicated to making
neural networks invariant to 2D image translations and rotations, viewpoint
invariance is rarely investigated. Motivated by the success of adversarial
training in enhancing model robustness, we propose Viewpoint-Invariant
Adversarial Training (VIAT) to improve the viewpoint robustness of image
classifiers. Regarding viewpoint transformation as an attack, we formulate VIAT
as a minimax optimization problem, where the inner maximization characterizes
diverse adversarial viewpoints by learning a Gaussian mixture distribution
based on the proposed attack method GMVFool. The outer minimization obtains a
viewpoint-invariant classifier by minimizing the expected loss over the
worst-case viewpoint distributions that can share the same one for different
objects within the same category. Based on GMVFool, we contribute a large-scale
dataset called ImageNet-V+ to benchmark viewpoint robustness. Experimental
results show that VIAT significantly improves the viewpoint robustness of
various image classifiers based on the diversity of adversarial viewpoints
generated by GMVFool. Furthermore, we propose ViewRS, a certified viewpoint
robustness method that provides a certified radius and accuracy to demonstrate
the effectiveness of VIAT from the theoretical perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Shouwei Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jianteng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11530">
<title>UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN. (arXiv:2307.11530v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11530</link>
<description rdf:parseType="Literal">&lt;p&gt;Fundus photography is an essential examination for clinical and differential
diagnosis of fundus diseases. Recently, Ultra-Wide-angle Fundus (UWF)
techniques, UWF Fluorescein Angiography (UWF-FA) and UWF Scanning Laser
Ophthalmoscopy (UWF-SLO) have been gradually put into use. However, Fluorescein
Angiography (FA) and UWF-FA require injecting sodium fluorescein which may have
detrimental influences. To avoid negative impacts, cross-modality medical image
generation algorithms have been proposed. Nevertheless, current methods in
fundus imaging could not produce high-resolution images and are unable to
capture tiny vascular lesion areas. This paper proposes a novel conditional
generative adversarial network (UWAT-GAN) to synthesize UWF-FA from UWF-SLO.
Using multi-scale generators and a fusion module patch to better extract global
and local information, our model can generate high-resolution images. Moreover,
an attention transmit module is proposed to help the decoder learn effectively.
Besides, a supervised approach is used to train the network using multiple new
weighted losses on different scales of data. Experiments on an in-house UWF
image dataset demonstrate the superiority of the UWAT-GAN over the
state-of-the-art methods. The source code is available at:
https://github.com/Tinysqua/UWAT-GAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhaojie Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhanghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Pengxue Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wangting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaochong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elazab_A/0/1/0/all/0/1&quot;&gt;Ahmed Elazab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_G/0/1/0/all/0/1&quot;&gt;Gangyong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Ruiquan Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11543">
<title>KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation. (arXiv:2307.11543v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11543</link>
<description rdf:parseType="Literal">&lt;p&gt;Object pose estimation is a fundamental computer vision task exploited in
several robotics and augmented reality applications. Many established
approaches rely on predicting 2D-3D keypoint correspondences using RANSAC
(Random sample consensus) and estimating the object pose using the PnP
(Perspective-n-Point) algorithm. Being RANSAC non-differentiable,
correspondences cannot be directly learned in an end-to-end fashion. In this
paper, we address the stereo image-based object pose estimation problem by (i)
introducing a differentiable RANSAC layer into a well-known monocular pose
estimation network; (ii) exploiting an uncertainty-driven multi-view PnP solver
which can fuse information from multiple views. We evaluate our approach on a
challenging public stereo object pose estimation dataset, yielding
state-of-the-art results against other recent approaches. Furthermore, in our
ablation study, we show that the differentiable RANSAC layer plays a
significant role in the accuracy of the proposed method. We release with this
paper the open-source implementation of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donadi_I/0/1/0/all/0/1&quot;&gt;Ivano Donadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretto_A/0/1/0/all/0/1&quot;&gt;Alberto Pretto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11545">
<title>Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation. (arXiv:2307.11545v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11545</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter Efficient Tuning (PET) has gained attention for reducing the number
of parameters while maintaining performance and providing better hardware
resource savings, but few studies investigate dense prediction tasks and
interaction between modalities. In this paper, we do an investigation of
efficient tuning problems on referring image segmentation. We propose a novel
adapter called Bridger to facilitate cross-modal information exchange and
inject task-specific information into the pre-trained model. We also design a
lightweight decoder for image segmentation. Our approach achieves comparable or
superior performance with only 1.61\% to 3.38\% backbone parameter updates,
evaluated on challenging benchmarks. The code is available at
\url{https://github.com/kkakkkka/ETRIS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zunnan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yibing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11550">
<title>YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation. (arXiv:2307.11550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11550</link>
<description rdf:parseType="Literal">&lt;p&gt;6D object pose estimation is a crucial prerequisite for autonomous robot
manipulation applications. The state-of-the-art models for pose estimation are
convolutional neural network (CNN)-based. Lately, Transformers, an architecture
originally proposed for natural language processing, is achieving
state-of-the-art results in many computer vision tasks as well. Equipped with
the multi-head self-attention mechanism, Transformers enable simple
single-stage end-to-end architectures for learning object detection and 6D
object pose estimation jointly. In this work, we propose YOLOPose (short form
for You Only Look Once Pose estimation), a Transformer-based multi-object 6D
pose estimation method based on keypoint regression and an improved variant of
the YOLOPose model. In contrast to the standard heatmaps for predicting
keypoints in an image, we directly regress the keypoints. Additionally, we
employ a learnable orientation estimation module to predict the orientation
from the keypoints. Along with a separate translation estimation module, our
model is end-to-end differentiable. Our method is suitable for real-time
applications and achieves results comparable to state-of-the-art methods. We
analyze the role of object queries in our architecture and reveal that the
object queries specialize in detecting objects in specific image regions.
Furthermore, we quantify the accuracy trade-off of using datasets of smaller
sizes to train our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1&quot;&gt;Arul Selvam Periyasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1&quot;&gt;Arash Amini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsaturyan_V/0/1/0/all/0/1&quot;&gt;Vladimir Tsaturyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1&quot;&gt;Sven Behnke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11558">
<title>Advancing Visual Grounding with Scene Knowledge: Benchmark and Method. (arXiv:2307.11558v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11558</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual grounding (VG) aims to establish fine-grained alignment between vision
and language. Ideally, it can be a testbed for vision-and-language models to
evaluate their understanding of the images and texts and their reasoning
abilities over their joint space. However, most existing VG datasets are
constructed using simple description texts, which do not require sufficient
reasoning over the images and texts. This has been demonstrated in a recent
study~\cite{luo2022goes}, where a simple LSTM-based text encoder without
pretraining can achieve state-of-the-art performance on mainstream VG datasets.
Therefore, in this paper, we propose a novel benchmark of \underline{S}cene
\underline{K}nowledge-guided \underline{V}isual \underline{G}rounding (SK-VG),
where the image content and referring expressions are not sufficient to ground
the target objects, forcing the models to have a reasoning ability on the
long-form scene knowledge. To perform this task, we propose two approaches to
accept the triple-type input, where the former embeds knowledge into the image
features before the image-query interaction; the latter leverages linguistic
structure to assist in computing the image-text matching. We conduct extensive
experiments to analyze the above methods and show that the proposed approaches
achieve promising results but still leave room for improvement, including
performance and interpretability. The dataset and code are available at
\url{https://github.com/zhjohnchan/SK-VG}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruifei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yibing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11567">
<title>CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph. (arXiv:2307.11567v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11567</link>
<description rdf:parseType="Literal">&lt;p&gt;The thickness of the cortical band is linked to various neurological and
psychiatric conditions, and is often estimated through surface-based methods
such as Freesurfer in MRI studies. The DiReCT method, which calculates cortical
thickness using a diffeomorphic deformation of the gray-white matter interface
towards the pial surface, offers an alternative to surface-based methods.
Recent studies using a synthetic cortical thickness phantom have demonstrated
that the combination of DiReCT and deep-learning-based segmentation is more
sensitive to subvoxel cortical thinning than Freesurfer.
&lt;/p&gt;
&lt;p&gt;While anatomical segmentation of a T1-weighted image now takes seconds,
existing implementations of DiReCT rely on iterative image registration methods
which can take up to an hour per volume. On the other hand, learning-based
deformable image registration methods like VoxelMorph have been shown to be
faster than classical methods while improving registration accuracy. This paper
proposes CortexMorph, a new method that employs unsupervised deep learning to
directly regress the deformation field needed for DiReCT. By combining
CortexMorph with a deep-learning-based segmentation model, it is possible to
estimate region-wise thickness in seconds from a T1-weighted image, while
maintaining the ability to detect cortical atrophy. We validate this claim on
the OASIS-3 dataset and the synthetic cortical thickness phantom of Rusak et
al.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McKinley_R/0/1/0/all/0/1&quot;&gt;Richard McKinley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rummel_C/0/1/0/all/0/1&quot;&gt;Christian Rummel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11603">
<title>Cascaded multitask U-Net using topological loss for vessel segmentation and centerline extraction. (arXiv:2307.11603v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11603</link>
<description rdf:parseType="Literal">&lt;p&gt;Vessel segmentation and centerline extraction are two crucial preliminary
tasks for many computer-aided diagnosis tools dealing with vascular diseases.
Recently, deep-learning based methods have been widely applied to these tasks.
However, classic deep-learning approaches struggle to capture the complex
geometry and specific topology of vascular networks, which is of the utmost
importance in most applications. To overcome these limitations, the clDice
loss, a topological loss that focuses on the vessel centerlines, has been
recently proposed. This loss requires computing, with a proposed soft-skeleton
algorithm, the skeletons of both the ground truth and the predicted
segmentation. However, the soft-skeleton algorithm provides suboptimal results
on 3D images, which makes the clDice hardly suitable on 3D images. In this
paper, we propose to replace the soft-skeleton algorithm by a U-Net which
computes the vascular skeleton directly from the segmentation. We show that our
method provides more accurate skeletons than the soft-skeleton algorithm. We
then build upon this network a cascaded U-Net trained with the clDice loss to
embed topological constraints during the segmentation. The resulting model is
able to predict both the vessel segmentation and centerlines with a more
accurate topology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rouge_P/0/1/0/all/0/1&quot;&gt;Pierre Roug&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Passat_N/0/1/0/all/0/1&quot;&gt;Nicolas Passat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Merveille_O/0/1/0/all/0/1&quot;&gt;Odyss&amp;#xe9;e Merveille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11604">
<title>Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation. (arXiv:2307.11604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11604</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical imaging has witnessed remarkable progress but usually requires a
large amount of high-quality annotated data which is time-consuming and costly
to obtain. To alleviate this burden, semi-supervised learning has garnered
attention as a potential solution. In this paper, we present Meta-Learning for
Bootstrapping Medical Image Segmentation (MLB-Seg), a novel method for tackling
the challenge of semi-supervised medical image segmentation. Specifically, our
approach first involves training a segmentation model on a small set of clean
labeled images to generate initial labels for unlabeled data. To further
optimize this bootstrapping process, we introduce a per-pixel weight mapping
system that dynamically assigns weights to both the initialized labels and the
model&apos;s own predictions. These weights are determined using a meta-process that
prioritizes pixels with loss gradient directions closer to those of clean data,
which is based on a small set of precisely annotated images. To facilitate the
meta-learning process, we additionally introduce a consistency-based Pseudo
Label Enhancement (PLE) scheme that improves the quality of the model&apos;s own
predictions by ensembling predictions from various augmented versions of the
same input. In order to improve the quality of the weight maps obtained through
multiple augmentations of a single input, we introduce a mean teacher into the
PLE scheme. This method helps to reduce noise in the weight maps and stabilize
its generation process. Our extensive experimental results on public atrial and
prostate segmentation datasets demonstrate that our proposed method achieves
state-of-the-art results under semi-supervision. Our code is available at
https://github.com/aijinrjinr/MLB-Seg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1&quot;&gt;Qingyue Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lequan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Lei Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11618">
<title>Divide and Adapt: Active Domain Adaptation via Customized Learning. (arXiv:2307.11618v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11618</link>
<description rdf:parseType="Literal">&lt;p&gt;Active domain adaptation (ADA) aims to improve the model adaptation
performance by incorporating active learning (AL) techniques to label a
maximally-informative subset of target samples. Conventional AL methods do not
consider the existence of domain shift, and hence, fail to identify the truly
valuable samples in the context of domain adaptation. To accommodate active
learning and domain adaption, the two naturally different tasks, in a
collaborative framework, we advocate that a customized learning strategy for
the target data is the key to the success of ADA solutions. We present
Divide-and-Adapt (DiaNA), a new ADA framework that partitions the target
instances into four categories with stratified transferable properties. With a
novel data subdivision protocol based on uncertainty and domainness, DiaNA can
accurately recognize the most gainful samples. While sending the informative
instances for annotation, DiaNA employs tailored learning strategies for the
remaining categories. Furthermore, we propose an informativeness score that
unifies the data partitioning criteria. This enables the use of a Gaussian
mixture model (GMM) to automatically sample unlabeled data into the proposed
four categories. Thanks to the &quot;divideand-adapt&quot; spirit, DiaNA can handle data
with large variations of domain gap. In addition, we show that DiaNA can
generalize to different domain adaptation settings, such as unsupervised domain
adaptation (UDA), semi-supervised domain adaptation (SSDA), source-free domain
adaptation (SFDA), etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Duojun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jichang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weikai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junshi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11636">
<title>OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?. (arXiv:2307.11636v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11636</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Runjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11638">
<title>Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing. (arXiv:2307.11638v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11638</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral imaging (HSI) captures a greater level of spectral detail than
traditional optical imaging, making it a potentially valuable intraoperative
tool when precise tissue differentiation is essential. Hardware limitations of
current optical systems used for handheld real-time video HSI result in a
limited focal depth, thereby posing usability issues for integration of the
technology into the operating room. This work integrates a focus-tunable liquid
lens into a video HSI exoscope, and proposes novel video autofocusing methods
based on deep reinforcement learning. A first-of-its-kind robotic focal-time
scan was performed to create a realistic and reproducible testing dataset. We
benchmarked our proposed autofocus algorithm against traditional policies, and
found our novel approach to perform significantly ($p&amp;lt;0.05$) better than
traditional techniques ($0.070\pm.098$ mean absolute focal error compared to
$0.146\pm.148$). In addition, we performed a blinded usability trial by having
two neurosurgeons compare the system with different autofocus policies, and
found our novel approach to be the most favourable, making our system a
desirable addition for intraoperative HSI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budd_C/0/1/0/all/0/1&quot;&gt;Charlie Budd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianrong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacCormac_O/0/1/0/all/0/1&quot;&gt;Oscar MacCormac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Martin Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mower_C/0/1/0/all/0/1&quot;&gt;Christopher Mower&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janatka_M/0/1/0/all/0/1&quot;&gt;Mirek Janatka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trotouin_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;o Trotouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1&quot;&gt;Jonathan Shapey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergholt_M/0/1/0/all/0/1&quot;&gt;Mads S. Bergholt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11643">
<title>Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11643</link>
<description rdf:parseType="Literal">&lt;p&gt;As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model&apos;s
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosma_G/0/1/0/all/0/1&quot;&gt;Georgina Cosma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bugby_S/0/1/0/all/0/1&quot;&gt;Sarah Bugby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finke_A/0/1/0/all/0/1&quot;&gt;Axel Finke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_J/0/1/0/all/0/1&quot;&gt;Jason Watkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11654">
<title>FEDD -- Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification. (arXiv:2307.11654v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11654</link>
<description rdf:parseType="Literal">&lt;p&gt;Skin diseases affect millions of people worldwide, across all ethnicities.
Increasing diagnosis accessibility requires fair and accurate segmentation and
classification of dermatology images. However, the scarcity of annotated
medical images, especially for rare diseases and underrepresented skin tones,
poses a challenge to the development of fair and accurate models. In this
study, we introduce a Fair, Efficient, and Diverse Diffusion-based framework
for skin lesion segmentation and malignancy classification. FEDD leverages
semantically meaningful feature embeddings learned through a denoising
diffusion probabilistic backbone and processes them via linear probes to
achieve state-of-the-art performance on Diverse Dermatology Images (DDI). We
achieve an improvement in intersection over union of 0.18, 0.13, 0.06, and 0.07
while using only 5%, 10%, 15%, and 20% labeled samples, respectively.
Additionally, FEDD trained on 10% of DDI demonstrates malignancy classification
accuracy of 81%, 14% higher compared to the state-of-the-art. We showcase high
efficiency in data-constrained scenarios while providing fair performance for
diverse skin tones and rare malignancy conditions. Our newly annotated DDI
segmentation masks and training code can be found on
https://github.com/hectorcarrion/fedd.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrion_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;ctor Carri&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_N/0/1/0/all/0/1&quot;&gt;Narges Norouzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11661">
<title>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11661</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP&apos;s default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1&quot;&gt;Mayug Maniparambil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorster_C/0/1/0/all/0/1&quot;&gt;Chris Vorster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_D/0/1/0/all/0/1&quot;&gt;Derek Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_N/0/1/0/all/0/1&quot;&gt;Noel Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11702">
<title>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization. (arXiv:2307.11702v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11702</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every
pixel of a given image, has recently shown promising potential. However,
existing methods remain mostly scene-specific or limited to small scenes and
thus hardly scale to realistic datasets. In this paper, we propose a new
paradigm where a single generic SCR model is trained once to be then deployed
to new test scenes, regardless of their scale and without further finetuning.
For a given query image, it collects inputs from off-the-shelf image retrieval
techniques and Structure-from-Motion databases: a list of relevant database
images with sparse pointwise 2D-3D annotations. The model is based on the
transformer architecture and can take a variable number of images and sparse
2D-3D annotations as input. It is trained on a few diverse datasets and
significantly outperforms other scene regression approaches on several
benchmarks, including scene-specific models, for visual localization. In
particular, we set a new state of the art on the Cambridge localization
benchmark, even outperforming feature-matching-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1&quot;&gt;Jerome Revaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabon_Y/0/1/0/all/0/1&quot;&gt;Yohann Cabon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bregier_R/0/1/0/all/0/1&quot;&gt;Romain Br&amp;#xe9;gier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;JongMin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1&quot;&gt;Philippe Weinzaepfel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11706">
<title>3D Skeletonization of Complex Grapevines for Robotic Pruning. (arXiv:2307.11706v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.11706</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic pruning of dormant grapevines is an area of active research in order
to promote vine balance and grape quality, but so far robotic efforts have
largely focused on planar, simplified vines not representative of commercial
vineyards. This paper aims to advance the robotic perception capabilities
necessary for pruning in denser and more complex vine structures by extending
plant skeletonization techniques. The proposed pipeline generates skeletal
grapevine models that have lower reprojection error and higher connectivity
than baseline algorithms. We also show how 3D and skeletal information enables
prediction accuracy of pruning weight for dense vines surpassing prior work,
where pruning weight is an important vine metric influencing pruning site
selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_E/0/1/0/all/0/1&quot;&gt;Eric Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayanth_S/0/1/0/all/0/1&quot;&gt;Sushanth Jayanth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silwal_A/0/1/0/all/0/1&quot;&gt;Abhisesh Silwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantor_G/0/1/0/all/0/1&quot;&gt;George Kantor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11748">
<title>BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection Evaluation. (arXiv:2307.11748v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11748</link>
<description rdf:parseType="Literal">&lt;p&gt;Scale-wise evaluation of object detectors is important for real-world
applications. However, existing metrics are either coarse or not sufficiently
reliable. In this paper, we propose novel scale-wise metrics that strike a
balance between fineness and reliability, using a filter bank consisting of
triangular and trapezoidal band-pass filters. We conduct experiments with two
methods on two datasets and show that the proposed metrics can highlight the
differences between the methods and between the datasets. Code is available at
https://github.com/shinya7y/UniverseNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinya_Y/0/1/0/all/0/1&quot;&gt;Yosuke Shinya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.02226">
<title>Terabyte-scale supervised 3D training and benchmarking dataset of the mouse kidney. (arXiv:2108.02226v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2108.02226</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of machine learning algorithms, when used for segmenting 3D
biomedical images, does not reach the level expected based on results achieved
with 2D photos. This may be explained by the comparative lack of high-volume,
high-quality training datasets, which require state-of-the-art imaging
facilities, domain experts for annotation and large computational and personal
resources. The HR-Kidney dataset presented in this work bridges this gap by
providing 1.7 TB of artefact-corrected synchrotron radiation-based X-ray
phase-contrast microtomography images of whole mouse kidneys and validated
segmentations of 33 729 glomeruli, which corresponds to a one to two orders of
magnitude increase over currently available biomedical datasets. The image sets
also contain the underlying raw data, threshold- and morphology-based
semi-automatic segmentations of renal vasculature and uriniferous tubules, as
well as true 3D manual annotations. We therewith provide a broad basis for the
scientific community to build upon and expand in the fields of image
processing, data augmentation and machine learning, in particular unsupervised
and semi-supervised learning investigations, as well as transfer learning and
generative adversarial networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1&quot;&gt;Willy Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossinelli_D/0/1/0/all/0/1&quot;&gt;Diego Rossinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_G/0/1/0/all/0/1&quot;&gt;Georg Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenger_R/0/1/0/all/0/1&quot;&gt;Roland H. Wenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hieber_S/0/1/0/all/0/1&quot;&gt;Simone Hieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1&quot;&gt;Bert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurtcuoglu_V/0/1/0/all/0/1&quot;&gt;Vartan Kurtcuoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00657">
<title>SiamixFormer: a fully-transformer Siamese network with temporal Fusion for accurate building detection and change detection in bi-temporal remote sensing images. (arXiv:2208.00657v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00657</link>
<description rdf:parseType="Literal">&lt;p&gt;Building detection and change detection using remote sensing images can help
urban and rescue planning. Moreover, they can be used for building damage
assessment after natural disasters. Currently, most of the existing models for
building detection use only one image (pre-disaster image) to detect buildings.
This is based on the idea that post-disaster images reduce the model&apos;s
performance because of presence of destroyed buildings. In this paper, we
propose a siamese model, called SiamixFormer, which uses pre- and post-disaster
images as input. Our model has two encoders and has a hierarchical transformer
architecture. The output of each stage in both encoders is given to a temporal
transformer for feature fusion in a way that query is generated from
pre-disaster images and (key, value) is generated from post-disaster images. To
this end, temporal features are also considered in feature fusion. Another
advantage of using temporal transformers in feature fusion is that they can
better maintain large receptive fields generated by transformer encoders
compared with CNNs. Finally, the output of the temporal transformer is given to
a simple MLP decoder at each stage. The SiamixFormer model is evaluated on xBD,
and WHU datasets, for building detection and on LEVIR-CD and CDD datasets for
change detection and could outperform the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadian_A/0/1/0/all/0/1&quot;&gt;Amir Mohammadian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaderi_F/0/1/0/all/0/1&quot;&gt;Foad Ghaderi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.05788">
<title>Semantic Self-adaptation: Enhancing Generalization with a Single Sample. (arXiv:2208.05788v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.05788</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of out-of-domain generalization is a critical weakness of deep
networks for semantic segmentation. Previous studies relied on the assumption
of a static model, i. e., once the training process is complete, model
parameters remain fixed at test time. In this work, we challenge this premise
with a self-adaptive approach for semantic segmentation that adjusts the
inference process to each input sample. Self-adaptation operates on two levels.
First, it fine-tunes the parameters of convolutional layers to the input image
using consistency regularization. Second, in Batch Normalization layers,
self-adaptation interpolates between the training and the reference
distribution derived from a single test sample. Despite both techniques being
well known in the literature, their combination sets new state-of-the-art
accuracy on synthetic-to-real generalization benchmarks. Our empirical study
suggests that self-adaptation may complement the established practice of model
regularization at training time for improving deep network generalization to
out-of-domain data. Our code and pre-trained models are available at
https://github.com/visinf/self-adaptive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1&quot;&gt;Sherwin Bahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_O/0/1/0/all/0/1&quot;&gt;Oliver Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamfir_E/0/1/0/all/0/1&quot;&gt;Eduard Zamfir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araslanov_N/0/1/0/all/0/1&quot;&gt;Nikita Araslanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1&quot;&gt;Stefan Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03297">
<title>Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems. (arXiv:2210.03297v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03297</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-based attacks construct adversarial examples against a machine
learning (ML) model by making only hard-label queries. These attacks have
mainly been applied directly to standalone neural networks. However, in
practice, ML models are just one component of a larger learning system. We find
that by adding a single preprocessor in front of a classifier, state-of-the-art
query-based attacks are up to 7$\times$ less effective at attacking a
prediction pipeline than at attacking the model alone. We explain this
discrepancy by the fact that most preprocessors introduce some notion of
invariance to the input space. Hence, attacks that are unaware of this
invariance inevitably waste a large number of queries to re-discover or
overcome it. We, therefore, develop techniques to (i) reverse-engineer the
preprocessor and then (ii) use this extracted information to attack the
end-to-end system. Our preprocessors extraction method requires only a few
hundred queries, and our preprocessor-aware attacks recover the same efficacy
as when attacking the model alone. The code can be found at
https://github.com/google-research/preprocessor-aware-black-box-attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1&quot;&gt;Chawin Sitawarin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1&quot;&gt;Florian Tram&amp;#xe8;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09563">
<title>FedForgery: Generalized Face Forgery Detection with Residual Federated Learning. (arXiv:2210.09563v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09563</link>
<description rdf:parseType="Literal">&lt;p&gt;With the continuous development of deep learning in the field of image
generation models, a large number of vivid forged faces have been generated and
spread on the Internet. These high-authenticity artifacts could grow into a
threat to society security. Existing face forgery detection methods directly
utilize the obtained public shared or centralized data for training but ignore
the personal privacy and security issues when personal data couldn&apos;t be
centralizedly shared in real-world scenarios. Additionally, different
distributions caused by diverse artifact types would further bring adverse
influences on the forgery detection task. To solve the mentioned problems, the
paper proposes a novel generalized residual Federated learning for face Forgery
detection (FedForgery). The designed variational autoencoder aims to learn
robust discriminative residual feature maps to detect forgery faces (with
diverse or even unknown artifact types). Furthermore, the general federated
learning strategy is introduced to construct distributed detection model
trained collaboratively with multiple local decentralized devices, which could
further boost the representation generalization. Experiments conducted on
publicly available face forgery detection datasets prove the superior
performance of the proposed FedForgery. The designed novel generalized face
forgery detection protocols and source code would be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Z/0/1/0/all/0/1&quot;&gt;Zhan Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chunlei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10495">
<title>ADPS: Asymmetric Distillation Post-Segmentation Method for Image Anomaly Detection. (arXiv:2210.10495v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10495</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Distillation-based Anomaly Detection (KDAD) methods rely on the
teacher-student paradigm to detect and segment anomalous regions by contrasting
the unique features extracted by both networks. However, existing KDAD methods
suffer from two main limitations: 1) the student network can effortlessly
replicate the teacher network&apos;s representations, and 2) the features of the
teacher network serve solely as a ``reference standard&quot; and are not fully
leveraged. Toward this end, we depart from the established paradigm and instead
propose an innovative approach called Asymmetric Distillation Post-Segmentation
(ADPS). Our ADPS employs an asymmetric distillation paradigm that takes
distinct forms of the same image as the input of the teacher-student networks,
driving the student network to learn discriminating representations for
anomalous regions.
&lt;/p&gt;
&lt;p&gt;Meanwhile, a customized Weight Mask Block (WMB) is proposed to generate a
coarse anomaly localization mask that transfers the distilled knowledge
acquired from the asymmetric paradigm to the teacher network. Equipped with
WMB, the proposed Post-Segmentation Module (PSM) is able to effectively detect
and segment abnormal regions with fine structures and clear boundaries.
Experimental results demonstrate that the proposed ADPS outperforms the
state-of-the-art methods in detecting and segmenting anomalies. Surprisingly,
ADPS significantly improves Average Precision (AP) metric by 9% and 20% on the
MVTec AD and KolektorSDD2 datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_P/0/1/0/all/0/1&quot;&gt;Peng Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zechao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06828">
<title>Enhancing Few-shot Image Classification with Cosine Transformer. (arXiv:2211.06828v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06828</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the few-shot image classification problem, where the
classification task is performed on unlabeled query samples given a small
amount of labeled support samples only. One major challenge of the few-shot
learning problem is the large variety of object visual appearances that
prevents the support samples to represent that object comprehensively. This
might result in a significant difference between support and query samples,
therefore undermining the performance of few-shot algorithms. In this paper, we
tackle the problem by proposing Few-shot Cosine Transformer (FS-CT), where the
relational map between supports and queries is effectively obtained for the
few-shot tasks. The FS-CT consists of two parts, a learnable prototypical
embedding network to obtain categorical representations from support samples
with hard cases, and a transformer encoder to effectively achieve the
relational map from two different support and query samples. We introduce
Cosine Attention, a more robust and stable attention module that enhances the
transformer module significantly and therefore improves FS-CT performance from
5% to over 20% in accuracy compared to the default scaled dot-product
mechanism. Our method performs competitive results in mini-ImageNet, CUB-200,
and CIFAR-FS on 1-shot learning and 5-shot learning tasks across backbones and
few-shot configurations. We also developed a custom few-shot dataset for Yoga
pose recognition to demonstrate the potential of our algorithm for practical
application. Our FS-CT with cosine attention is a lightweight, simple few-shot
algorithm that can be applied for a wide range of applications, such as
healthcare, medical, and security surveillance. The official implementation
code of our Few-shot Cosine Transformer is available at
https://github.com/vinuni-vishc/Few-Shot-Cosine-Transformer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quang-Huy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cuong Q. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1&quot;&gt;Dung D. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hieu H. Pham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03434">
<title>Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer. (arXiv:2212.03434v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03434</link>
<description rdf:parseType="Literal">&lt;p&gt;The long-standing theory that a colour-naming system evolves under dual
pressure of efficient communication and perceptual mechanism is supported by
more and more linguistic studies, including analysing four decades of
diachronic data from the Nafaanra language. This inspires us to explore whether
machine learning could evolve and discover a similar colour-naming system via
optimising the communication efficiency represented by high-level recognition
performance. Here, we propose a novel colour quantisation transformer,
CQFormer, that quantises colour space while maintaining the accuracy of machine
recognition on the quantised images. Given an RGB image, Annotation Branch maps
it into an index map before generating the quantised image with a colour
palette; meanwhile the Palette Branch utilises a key-point detection way to
find proper colours in the palette among the whole colour space. By interacting
with colour annotation, CQFormer is able to balance both the machine vision
accuracy and colour perceptual structure such as distinct and stable colour
distribution for discovered colour system. Very interestingly, we even observe
the consistent evolution pattern between our artificial colour system and basic
colour terms across human languages. Besides, our colour quantisation method
also offers an efficient quantisation method that effectively compresses the
image storage while maintaining high performance in high-level recognition
tasks such as classification and detection. Extensive experiments demonstrate
the superior performance of our method with extremely low bit-rate colours,
showing potential to integrate into quantisation network to quantities from
image to network activation. The source code is available at
https://github.com/ryeocthiv/CQFormer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shenghan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zenghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06262">
<title>Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges. (arXiv:2301.06262v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06262</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative perception is essential to address occlusion and sensor failure
issues in autonomous driving. In recent years, theoretical and experimental
investigations of novel works for collaborative perception have increased
tremendously. So far, however, few reviews have focused on systematical
collaboration modules and large-scale collaborative perception datasets. This
work reviews recent achievements in this field to bridge this gap and motivate
future research. We start with a brief overview of collaboration schemes. After
that, we systematically summarize the collaborative perception methods for
ideal scenarios and real-world issues. The former focuses on collaboration
modules and efficiency, and the latter is devoted to addressing the problems in
actual application. Furthermore, we present large-scale public datasets and
summarize quantitative results on these benchmarks. Finally, we highlight gaps
and overlook challenges between current academic research and real-world
applications. The project page is
https://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yushan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huifang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1&quot;&gt;Congyan Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yidong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04246">
<title>Shortcut Detection with Variational Autoencoders. (arXiv:2302.04246v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;For real-world applications of machine learning (ML), it is essential that
models make predictions based on well-generalizing features rather than
spurious correlations in the data. The identification of such spurious
correlations, also known as shortcuts, is a challenging problem and has so far
been scarcely addressed. In this work, we present a novel approach to detect
shortcuts in image and audio datasets by leveraging variational autoencoders
(VAEs). The disentanglement of features in the latent space of VAEs allows us
to discover feature-target correlations in datasets and semi-automatically
evaluate them for ML shortcuts. We demonstrate the applicability of our method
on several real-world datasets and identify shortcuts that have not been
discovered before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1&quot;&gt;Nicolas M. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roschmann_S/0/1/0/all/0/1&quot;&gt;Simon Roschmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperl_P/0/1/0/all/0/1&quot;&gt;Philip Sperl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1&quot;&gt;Konstantin B&amp;#xf6;ttinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04973">
<title>Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames. (arXiv:2302.04973v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04973</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically discovering composable abstractions from raw perceptual data is
a long-standing challenge in machine learning. Recent slot-based neural
networks that learn about objects in a self-supervised manner have made
exciting progress in this direction. However, they typically fall short at
adequately capturing spatial symmetries present in the visual world, which
leads to sample inefficiency, such as when entangling object appearance and
pose. In this paper, we present a simple yet highly effective method for
incorporating spatial symmetries via slot-centric reference frames. We
incorporate equivariance to per-object pose transformations into the attention
and generation mechanism of Slot Attention by translating, scaling, and
rotating position encodings. These changes result in little computational
overhead, are easy to implement, and can result in large gains in terms of data
efficiency and overall improvements to object discovery. We evaluate our method
on a wide range of synthetic object discovery benchmarks namely CLEVR,
Tetrominoes, CLEVRTex, Objects Room and MultiShapeNet, and show promising
improvements on the challenging real-world Waymo Open dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1&quot;&gt;Ondrej Biza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1&quot;&gt;Sjoerd van Steenkiste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1&quot;&gt;Gamaleldin F. Elsayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1&quot;&gt;Aravindh Mahendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1&quot;&gt;Thomas Kipf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11827">
<title>Open Challenges for Monocular Single-shot 6D Object Pose Estimation. (arXiv:2302.11827v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11827</link>
<description rdf:parseType="Literal">&lt;p&gt;Object pose estimation is a non-trivial task that enables robotic
manipulation, bin picking, augmented reality, and scene understanding, to name
a few use cases. Monocular object pose estimation gained considerable momentum
with the rise of high-performing deep learning-based solutions and is
particularly interesting for the community since sensors are inexpensive and
inference is fast. Prior works establish the comprehensive state of the art for
diverse pose estimation problems. Their broad scopes make it difficult to
identify promising future directions. We narrow down the scope to the problem
of single-shot monocular 6D object pose estimation, which is commonly used in
robotics, and thus are able to identify such trends. By reviewing recent
publications in robotics and computer vision, the state of the art is
established at the union of both fields. Following that, we identify promising
research directions in order to help researchers to formulate relevant research
ideas and effectively advance the state of the art. Findings include that
methods are sophisticated enough to overcome the domain shift and that
occlusion handling is a fundamental challenge. We also highlight problems such
as novel object pose estimation and challenging materials handling as central
challenges to advance robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thalhammer_S/0/1/0/all/0/1&quot;&gt;Stefan Thalhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honig_P/0/1/0/all/0/1&quot;&gt;Peter H&amp;#xf6;nig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weibel_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Weibel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincze_M/0/1/0/all/0/1&quot;&gt;Markus Vincze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03056">
<title>MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal calibration. (arXiv:2303.03056v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03056</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent advances in autonomous driving and the decreasing cost of
LiDARs, the use of multimodal sensor systems is on the rise. However, in order
to make use of the information provided by a variety of complimentary sensors,
it is necessary to accurately calibrate them. We take advantage of recent
advances in computer graphics and implicit volumetric scene representation to
tackle the problem of multi-sensor spatial and temporal calibration. Thanks to
a new formulation of the Neural Radiance Field (NeRF) optimization, we are able
to jointly optimize calibration parameters along with scene representation
based on radiometric and geometric measurements. Our method enables accurate
and robust calibration from data captured in uncontrolled and unstructured
urban environments, making our solution more scalable than existing calibration
solutions. We demonstrate the accuracy and robustness of our method in urban
scenes typically encountered in autonomous driving scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herau_Q/0/1/0/all/0/1&quot;&gt;Quentin Herau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piasco_N/0/1/0/all/0/1&quot;&gt;Nathan Piasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennehar_M/0/1/0/all/0/1&quot;&gt;Moussab Bennehar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1&quot;&gt;Luis Rold&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1&quot;&gt;Dzmitry Tsishkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Migniot_C/0/1/0/all/0/1&quot;&gt;Cyrille Migniot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1&quot;&gt;Pascal Vasseur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demonceaux_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Demonceaux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05966">
<title>Score-Based Generative Models for Medical Image Segmentation using Signed Distance Functions. (arXiv:2303.05966v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05966</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation is a crucial task that relies on the ability to
accurately identify and isolate regions of interest in medical images. Thereby,
generative approaches allow to capture the statistical properties of
segmentation masks that are dependent on the respective structures. In this
work we propose a conditional score-based generative modeling framework to
represent the signed distance function (SDF) leading to an implicit
distribution of segmentation masks. The advantage of leveraging the SDF is a
more natural distortion when compared to that of binary masks. By learning the
score function of the conditional distribution of SDFs we can accurately sample
from the distribution of segmentation masks, allowing for the evaluation of
statistical quantities. Thus, this probabilistic representation allows for the
generation of uncertainty maps represented by the variance, which can aid in
further analysis and enhance the predictive robustness. We qualitatively and
quantitatively illustrate competitive performance of the proposed method on a
public nuclei and gland segmentation data set, highlighting its potential
utility in medical image segmentation applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogensperger_L/0/1/0/all/0/1&quot;&gt;Lea Bogensperger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narnhofer_D/0/1/0/all/0/1&quot;&gt;Dominik Narnhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_F/0/1/0/all/0/1&quot;&gt;Filip Ilic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pock_T/0/1/0/all/0/1&quot;&gt;Thomas Pock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06146">
<title>StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces. (arXiv:2303.06146v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06146</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in face manipulation using StyleGAN have produced impressive
results. However, StyleGAN is inherently limited to cropped aligned faces at a
fixed image resolution it is pre-trained on. In this paper, we propose a simple
and effective solution to this limitation by using dilated convolutions to
rescale the receptive fields of shallow layers in StyleGAN, without altering
any model parameters. This allows fixed-size small features at shallow layers
to be extended into larger ones that can accommodate variable resolutions,
making them more robust in characterizing unaligned faces. To enable real face
inversion and manipulation, we introduce a corresponding encoder that provides
the first-layer feature of the extended StyleGAN in addition to the latent
style code. We validate the effectiveness of our method using unaligned face
inputs of various resolutions in a diverse set of face manipulation tasks,
including facial attribute editing, super-resolution, sketch/mask-to-face
translation, and face toonification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Liming Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09975">
<title>MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09975</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been exploding interest in embracing Transformer-based
architectures for medical image segmentation. However, the lack of large-scale
annotated medical datasets make achieving performances equivalent to those in
natural images challenging. Convolutional networks, in contrast, have higher
inductive biases and consequently, are easily trainable to high performance.
Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet
by mirroring Transformer blocks. In this work, we improve upon this to design a
modernized and scalable convolutional architecture customized to challenges of
data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired
large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D
Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up
and downsampling blocks to preserve semantic richness across scales, 3) A novel
technique to iteratively increase kernel sizes by upsampling small kernel
networks, to prevent performance saturation on limited medical data, 4)
Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt.
This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities
and varying dataset sizes, representing a modernized deep architecture for
medical image segmentation. Our code is made publicly available at:
https://github.com/MIC-DKFZ/MedNeXt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Saikat Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Koehler_G/0/1/0/all/0/1&quot;&gt;Gregor Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ulrich_C/0/1/0/all/0/1&quot;&gt;Constantin Ulrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1&quot;&gt;Michael Baumgartner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1&quot;&gt;Jens Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1&quot;&gt;Paul F. Jaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10656">
<title>More From Less: Self-Supervised Knowledge Distillation for Routine Histopathology Data. (arXiv:2303.10656v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10656</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical imaging technologies are generating increasingly large amounts of
high-quality, information-dense data. Despite the progress, practical use of
advanced imaging technologies for research and diagnosis remains limited by
cost and availability, so information-sparse data such as H&amp;amp;E stains are relied
on in practice. The study of diseased tissue requires methods which can
leverage these information-dense data to extract more value from routine,
information-sparse data. Using self-supervised deep learning, we demonstrate
that it is possible to distil knowledge during training from information-dense
data into models which only require information-sparse data for inference. This
improves downstream classification accuracy on information-sparse data, making
it comparable with the fully-supervised baseline. We find substantial effects
on the learned representations, and this training process identifies subtle
features which otherwise go undetected. This approach enables the design of
models which require only routine images, but contain insights from
state-of-the-art data, allowing better use of the available resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farndale_L/0/1/0/all/0/1&quot;&gt;Lucas Farndale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Insall_R/0/1/0/all/0/1&quot;&gt;Robert Insall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Ke Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11057">
<title>Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation. (arXiv:2303.11057v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11057</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding and manipulating deformable objects (e.g., ropes and fabrics)
is an essential yet challenging task with broad applications. Difficulties come
from complex states and dynamics, diverse configurations and high-dimensional
action space of deformable objects. Besides, the manipulation tasks usually
require multiple steps to accomplish, and greedy policies may easily lead to
local optimal states. Existing studies usually tackle this problem using
reinforcement learning or imitating expert demonstrations, with limitations in
modeling complex states or requiring hand-crafted expert policies. In this
paper, we study deformable object manipulation using dense visual affordance,
with generalization towards diverse states, and propose a novel kind of
foresightful dense affordance, which avoids local optima by estimating states&apos;
values for long-term manipulation. We propose a framework for learning this
representation, with novel designs such as multi-stage stable learning and
efficient self-supervised data collection without experts. Experiments
demonstrate the superiority of our proposed foresightful dense affordance.
Project page: https://hyperplane-lab.github.io/DeformableAffordance
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11630">
<title>BoxSnake: Polygonal Instance Segmentation with Box Supervision. (arXiv:2303.11630v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11630</link>
<description rdf:parseType="Literal">&lt;p&gt;Box-supervised instance segmentation has gained much attention as it requires
only simple box annotations instead of costly mask or polygon annotations.
However, existing box-supervised instance segmentation models mainly focus on
mask-based frameworks. We propose a new end-to-end training technique, termed
BoxSnake, to achieve effective polygonal instance segmentation using only box
annotations for the first time. Our method consists of two loss functions: (1)
a point-based unary loss that constrains the bounding box of predicted polygons
to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss
that encourages the predicted polygons to fit the object boundaries. Compared
with the mask-based weakly-supervised methods, BoxSnake further reduces the
performance gap between the predicted segmentation and the bounding box, and
shows significant superiority on the Cityscapes dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Lin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15823">
<title>Automated wildlife image classification: An active learning tool for ecological applications. (arXiv:2303.15823v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15823</link>
<description rdf:parseType="Literal">&lt;p&gt;Wildlife camera trap images are being used extensively to investigate animal
abundance, habitat associations, and behavior, which is complicated by the fact
that experts must first classify the images manually. Artificial intelligence
systems can take over this task but usually need a large number of
already-labeled training images to achieve sufficient performance. This
requirement necessitates human expert labor and poses a particular challenge
for projects with few cameras or short durations. We propose a label-efficient
learning strategy that enables researchers with small or medium-sized image
databases to leverage the potential of modern machine learning, thus freeing
crucial resources for subsequent analyses.
&lt;/p&gt;
&lt;p&gt;Our methodological proposal is two-fold: (1) We improve current strategies of
combining object detection and image classification by tuning the
hyperparameters of both models. (2) We provide an active learning (AL) system
that allows training deep learning models very efficiently in terms of required
human-labeled training images. We supply a software package that enables
researchers to use these methods directly and thereby ensure the broad
applicability of the proposed framework in ecological practice.
&lt;/p&gt;
&lt;p&gt;We show that our tuning strategy improves predictive performance. We
demonstrate how the AL pipeline reduces the amount of pre-labeled data needed
to achieve a specific predictive performance and that it is especially valuable
for improving out-of-sample predictive performance.
&lt;/p&gt;
&lt;p&gt;We conclude that the combination of tuning and AL increases predictive
performance substantially. Furthermore, we argue that our work can broadly
impact the community through the ready-to-use software package provided.
Finally, the publication of our models tailored to European wildlife data
enriches existing model bases mostly trained on data from Africa and North
America.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1&quot;&gt;Ludwig Bothmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wimmer_L/0/1/0/all/0/1&quot;&gt;Lisa Wimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charrakh_O/0/1/0/all/0/1&quot;&gt;Omid Charrakh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_T/0/1/0/all/0/1&quot;&gt;Tobias Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edelhoff_H/0/1/0/all/0/1&quot;&gt;Hendrik Edelhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_W/0/1/0/all/0/1&quot;&gt;Wibke Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benjamin_C/0/1/0/all/0/1&quot;&gt;Caryl Benjamin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzel_A/0/1/0/all/0/1&quot;&gt;Annette Menzel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10769">
<title>Deep Multiview Clustering by Contrasting Cluster Assignments. (arXiv:2304.10769v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10769</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiview clustering (MVC) aims to reveal the underlying structure of
multiview data by categorizing data samples into clusters. Deep learning-based
methods exhibit strong feature learning capabilities on large-scale datasets.
For most existing deep MVC methods, exploring the invariant representations of
multiple views is still an intractable problem. In this paper, we propose a
cross-view contrastive learning (CVCL) method that learns view-invariant
representations and produces clustering results by contrasting the cluster
assignments among multiple views. Specifically, we first employ deep
autoencoders to extract view-dependent features in the pretraining stage. Then,
a cluster-level CVCL strategy is presented to explore consistent semantic label
information among the multiple views in the fine-tuning stage. Thus, the
proposed CVCL method is able to produce more discriminative cluster assignments
by virtue of this learning strategy. Moreover, we provide a theoretical
analysis of soft cluster assignment alignment. Extensive experimental results
obtained on several datasets demonstrate that the proposed CVCL method
outperforms several state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Hua Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1&quot;&gt;Wai Lok Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14133">
<title>VERITE: A Robust Benchmark for Multimodal Misinformation Detection Accounting for Unimodal Bias. (arXiv:2304.14133v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14133</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimedia content has become ubiquitous on social media platforms, leading
to the rise of multimodal misinformation (MM) and the urgent need for effective
strategies to detect and prevent its spread. In recent years, the challenge of
multimodal misinformation detection (MMD) has garnered significant attention by
researchers and has mainly involved the creation of annotated, weakly
annotated, or synthetically generated training datasets, along with the
development of various deep learning MMD models. However, the problem of
unimodal bias in MMD benchmarks -- where biased or unimodal methods outperform
their multimodal counterparts on an inherently multimodal task -- has been
overlooked. In this study, we systematically investigate and identify the
presence of unimodal bias in widely-used MMD benchmarks (VMU-Twitter, COSMOS),
raising concerns about their suitability for reliable evaluation. To address
this issue, we introduce the &quot;VERification of Image-TExtpairs&quot; (VERITE)
benchmark for MMD which incorporates real-world data, excludes &quot;asymmetric
multimodal misinformation&quot; and utilizes &quot;modality balancing&quot;. We conduct an
extensive comparative study with a Transformer-based architecture that shows
the ability of VERITE to effectively address unimodal bias, rendering it a
robust evaluation framework for MMD. Furthermore, we introduce a new method --
termed Crossmodal HArd Synthetic MisAlignment (CHASMA) -- for generating
realistic synthetic training data that preserve crossmodal relations between
legitimate images and false human-written captions. By leveraging CHASMA in the
training process, we observe consistent and notable improvements in predictive
performance on VERITE; with a 9.2% increase in accuracy. We release our code
at: https://github.com/stevejpapad/image-text-verification
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1&quot;&gt;Stefanos-Iordanis Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1&quot;&gt;Christos Koutlis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1&quot;&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrantonakis_P/0/1/0/all/0/1&quot;&gt;Panagiotis C. Petrantonakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16649">
<title>FSD: Fully-Specialized Detector via Neural Architecture Search. (arXiv:2305.16649v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16649</link>
<description rdf:parseType="Literal">&lt;p&gt;Most generic object detectors are mainly built for standard object detection
tasks such as COCO and PASCAL VOC. They might not work well and/or efficiently
on tasks of other domains consisting of images that are visually different from
standard datasets. To this end, many advances have been focused on adapting a
general-purposed object detector with limited domain-specific designs. However,
designing a successful task-specific detector requires extraneous manual
experiments and parameter tuning through trial and error. In this paper, we
first propose and examine a fully-automatic pipeline to design a
fully-specialized detector (FSD) which mainly incorporates a
neural-architectural-searched model by exploring ideal network structures over
the backbone and task-specific head. On the DeepLesion dataset, extensive
results show that FSD can achieve 3.1 mAP gain while using approximately 40%
fewer parameters on binary lesion detection task and improved the mAP by around
10% on multi-type lesion detection task via our region-aware graph modeling
compared with existing general-purposed medical lesion detection networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yudian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18310">
<title>Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark. (arXiv:2305.18310v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18310</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently significant progress has been made in human action recognition and
behavior prediction using deep learning techniques, leading to improved
vision-based semantic understanding. However, there is still a lack of
high-quality motion datasets for small bio-robotics, which presents more
challenging scenarios for long-term movement prediction and behavior control
based on third-person observation. In this study, we introduce RatPose, a
bio-robot motion prediction dataset constructed by considering the influence
factors of individuals and environments based on predefined annotation rules.
To enhance the robustness of motion prediction against these factors, we
propose a Dual-stream Motion-Scenario Decoupling (\textit{DMSD}) framework that
effectively separates scenario-oriented and motion-oriented features and
designs a scenario contrast loss and motion clustering loss for overall
training. With such distinctive architecture, the dual-branch feature flow
information is interacted and compensated in a decomposition-then-fusion
manner. Moreover, we demonstrate significant performance improvements of the
proposed \textit{DMSD} framework on different difficulty-level tasks. We also
implement long-term discretized trajectory prediction tasks to verify the
generalization ability of the proposed dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiaxin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaohua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nenggan Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18453">
<title>Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18453</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand for artificial intelligence (AI) in healthcare is rapidly
increasing. However, significant challenges arise from data scarcity and
privacy concerns, particularly in medical imaging. While existing generative
models have achieved success in image synthesis and image-to-image translation
tasks, there remains a gap in the generation of 3D semantic medical images. To
address this gap, we introduce Med-DDPM, a diffusion model specifically
designed for semantic 3D medical image synthesis, effectively tackling data
scarcity and privacy issues. The novelty of Med-DDPM lies in its incorporation
of semantic conditioning, enabling precise control during the image generation
process. Our model outperforms Generative Adversarial Networks (GANs) in terms
of stability and performance, generating diverse and anatomically coherent
images with high visual fidelity. Comparative analysis against state-of-the-art
augmentation techniques demonstrates that Med-DDPM produces comparable results,
highlighting its potential as a data augmentation tool for enhancing model
accuracy. In conclusion, Med-DDPM pioneers 3D semantic medical image synthesis
by delivering high-quality and anatomically coherent images. Furthermore, the
integration of semantic conditioning with Med-DDPM holds promise for image
anonymization in the field of biomedical imaging, showcasing the capabilities
of the model in addressing challenges related to data scarcity and privacy
concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dorjsembe_Z/0/1/0/all/0/1&quot;&gt;Zolnamar Dorjsembe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pao_H/0/1/0/all/0/1&quot;&gt;Hsing-Kuo Pao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Odonchimed_S/0/1/0/all/0/1&quot;&gt;Sodtavilan Odonchimed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Furen Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19920">
<title>MSKdeX: Musculoskeletal (MSK) decomposition from an X-ray image for fine-grained estimation of lean muscle mass and muscle volume. (arXiv:2305.19920v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19920</link>
<description rdf:parseType="Literal">&lt;p&gt;Musculoskeletal diseases such as sarcopenia and osteoporosis are major
obstacles to health during aging. Although dual-energy X-ray absorptiometry
(DXA) and computed tomography (CT) can be used to evaluate musculoskeletal
conditions, frequent monitoring is difficult due to the cost and accessibility
(as well as high radiation exposure in the case of CT). We propose a method
(named MSKdeX) to estimate fine-grained muscle properties from a plain X-ray
image, a low-cost, low-radiation, and highly accessible imaging modality,
through musculoskeletal decomposition leveraging fine-grained segmentation in
CT. We train a multi-channel quantitative image translation model to decompose
an X-ray image into projections of CT of individual muscles to infer the lean
muscle mass and muscle volume. We propose the object-wise intensity-sum loss, a
simple yet surprisingly effective metric invariant to muscle deformation and
projection direction, utilizing information in CT and X-ray images collected
from the same patient. While our method is basically an unpaired image-to-image
translation, we also exploit the nature of the bone&apos;s rigidity, which provides
the paired data through 2D-3D rigid registration, adding strong pixel-wise
supervision in unpaired training. Through the evaluation using a 539-patient
dataset, we showed that the proposed method significantly outperformed
conventional methods. The average Pearson correlation coefficient between the
predicted and CT-derived ground truth metrics was increased from 0.460 to
0.863. We believe our method opened up a new musculoskeletal diagnosis method
and has the potential to be extended to broader applications in multi-channel
quantitative image translation tasks. Our source code will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otake_Y/0/1/0/all/0/1&quot;&gt;Yoshito Otake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uemura_K/0/1/0/all/0/1&quot;&gt;Keisuke Uemura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takao_M/0/1/0/all/0/1&quot;&gt;Masaki Takao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soufi_M/0/1/0/all/0/1&quot;&gt;Mazen Soufi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hiasa_Y/0/1/0/all/0/1&quot;&gt;Yuta Hiasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talbot_H/0/1/0/all/0/1&quot;&gt;Hugues Talbot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okata_S/0/1/0/all/0/1&quot;&gt;Seiji Okata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugano_N/0/1/0/all/0/1&quot;&gt;Nobuhiko Sugano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00988">
<title>Continual Learning for Abdominal Multi-Organ and Tumor Segmentation. (arXiv:2306.00988v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00988</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to dynamically extend a model to new data and classes is critical
for multiple organ and tumor segmentation. However, due to privacy regulations,
accessing previous data and annotations can be problematic in the medical
domain. This poses a significant barrier to preserving the high segmentation
accuracy of the old classes when learning from new classes because of the
catastrophic forgetting problem. In this paper, we first empirically
demonstrate that simply using high-quality pseudo labels can fairly mitigate
this problem in the setting of organ segmentation. Furthermore, we put forward
an innovative architecture designed specifically for continuous organ and tumor
segmentation, which incurs minimal computational overhead. Our proposed design
involves replacing the conventional output layer with a suite of lightweight,
class-specific heads, thereby offering the flexibility to accommodate newly
emerging classes. These heads enable independent predictions for newly
introduced and previously learned classes, effectively minimizing the impact of
new classes on old ones during the course of continual learning. We further
propose incorporating Contrastive Language-Image Pretraining (CLIP) embeddings
into the organ-specific heads. These embeddings encapsulate the semantic
information of each class, informed by extensive image-text co-training. The
proposed method is evaluated on both in-house and public abdominal CT datasets
under organ and tumor segmentation tasks. Empirical results suggest that the
proposed design improves the segmentation performance of a baseline neural
network on newly-introduced and previously-learned classes along the learning
trajectory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huimiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zongwei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07308">
<title>Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior. (arXiv:2306.07308v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07308</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral Image (HSI)s cover hundreds or thousands of narrow spectral
bands, conveying a wealth of spatial and spectral information. However, due to
the instrumental errors and the atmospheric changes, the HSI obtained in
practice are often contaminated by noise and dead pixels(lines), resulting in
missing information that may severely compromise the subsequent applications.
We introduce here a novel HSI missing pixel prediction algorithm, called Low
Rank and Sparsity Constraint Plug-and-Play (LRS-PnP). It is shown that LRS-PnP
is able to predict missing pixels and bands even when all spectral bands of the
image are missing. The proposed LRS-PnP algorithm is further extended to a
self-supervised model by combining the LRS-PnP with the Deep Image Prior (DIP),
called LRS-PnP-DIP. In a series of experiments with real data, It is shown that
the LRS-PnP-DIP either achieves state-of-the-art inpainting performance
compared to other learning-based methods, or outperforms them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yaghoobi_M/0/1/0/all/0/1&quot;&gt;Mehrdad Yaghoobi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02953">
<title>SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks. (arXiv:2307.02953v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02953</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, U-shaped networks have dominated the field of medical image
segmentation due to their simple and easily tuned structure. However, existing
U-shaped segmentation networks: 1) mostly focus on designing complex
self-attention modules to compensate for the lack of long-term dependence based
on convolution operation, which increases the overall number of parameters and
computational complexity of the network; 2) simply fuse the features of encoder
and decoder, ignoring the connection between their spatial locations. In this
paper, we rethink the above problem and build a lightweight medical image
segmentation network, called SegNetr. Specifically, we introduce a novel
SegNetr block that can perform local-global interactions dynamically at any
stage and with only linear complexity. At the same time, we design a general
information retention skip connection (IRSC) to preserve the spatial location
information of encoder features and achieve accurate fusion with the decoder
features. We validate the effectiveness of SegNetr on four mainstream medical
image segmentation datasets, with 59\% and 76\% fewer parameters and GFLOPs
than vanilla U-Net, while achieving segmentation performance comparable to
state-of-the-art methods. Notably, the components proposed in this paper can be
applied to other U-shaped networks to improve their segmentation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junlong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chengrui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Min Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03512">
<title>Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03512</link>
<description rdf:parseType="Literal">&lt;p&gt;When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleni_P/0/1/0/all/0/1&quot;&gt;Paolo Soleni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaart_W/0/1/0/all/0/1&quot;&gt;Wouter B. Verschoof-van der Vaart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokalj_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;iga Kokalj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traviglia_A/0/1/0/all/0/1&quot;&gt;Arianna Traviglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04378">
<title>Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains. (arXiv:2307.04378v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04378</link>
<description rdf:parseType="Literal">&lt;p&gt;Diabetic Retinopathy (DR) is a common complication of diabetes and a leading
cause of blindness worldwide. Early and accurate grading of its severity is
crucial for disease management. Although deep learning has shown great
potential for automated DR grading, its real-world deployment is still
challenging due to distribution shifts among source and target domains, known
as the domain generalization problem. Existing works have mainly attributed the
performance degradation to limited domain shifts caused by simple visual
discrepancies, which cannot handle complex real-world scenarios. Instead, we
present preliminary evidence suggesting the existence of three-fold
generalization issues: visual and degradation style shifts, diagnostic pattern
diversity, and data imbalance. To tackle these issues, we propose a novel
unified framework named Generalizable Diabetic Retinopathy Grading Network
(GDRNet). GDRNet consists of three vital components: fundus visual-artifact
augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and
domain-class-aware re-balancing (DCR). FundusAug generates realistic augmented
images via visual transformation and image degradation, while DahLoss jointly
leverages pixel-level consistency and image-level semantics to capture the
diverse diagnostic patterns and build generalizable feature representations.
Moreover, DCR mitigates the data imbalance from a domain-class view and avoids
undesired over-emphasis on rare domain-class pairs. Finally, we design a
publicly available benchmark for fair evaluations. Extensive comparison
experiments against advanced methods and exhaustive ablation studies
demonstrate the effectiveness and generalization ability of GDRNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1&quot;&gt;Haoxuan Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuhan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Haibo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04541">
<title>Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis. (arXiv:2307.04541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04541</link>
<description rdf:parseType="Literal">&lt;p&gt;Fueled by deep learning, computer-aided diagnosis achieves huge advances.
However, out of controlled lab environments, algorithms could face multiple
challenges. Open set recognition (OSR), as an important one, states that
categories unseen in training could appear in testing. In medical fields, it
could derive from incompletely collected training datasets and the constantly
emerging new or rare diseases. OSR requires an algorithm to not only correctly
classify known classes, but also recognize unknown classes and forward them to
experts for further diagnosis. To tackle OSR, we assume that known classes
could densely occupy small parts of the embedding space and the remaining
sparse regions could be recognized as unknowns. Following it, we propose Open
Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin
Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing
intra-class compactness and inter-class separability, together with an adaptive
scaling factor to strengthen the generalization capacity. The latter, called
Open-Space Suppression (OSS), opens the classifier by recognizing sparse
embedding space as unknowns using proposed feature space descriptors. Besides,
since medical OSR is still a nascent field, two publicly available benchmark
datasets are proposed for comparison. Extensive ablation studies and feature
visualization demonstrate the effectiveness of each design. Compared with
state-of-the-art methods, MLAS achieves superior performances, measured by ACC,
AUROC, and OSCR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jicong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06666">
<title>Transformer-based end-to-end classification of variable-length volumetric data. (arXiv:2307.06666v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06666</link>
<description rdf:parseType="Literal">&lt;p&gt;The automatic classification of 3D medical data is memory-intensive. Also,
variations in the number of slices between samples is common. Na\&quot;ive solutions
such as subsampling can solve these problems, but at the cost of potentially
eliminating relevant diagnosis information. Transformers have shown promising
performance for sequential data analysis. However, their application for long
sequences is data, computationally, and memory demanding. In this paper, we
propose an end-to-end Transformer-based framework that allows to classify
volumetric data of variable length in an efficient fashion. Particularly, by
randomizing the input volume-wise resolution(#slices) during training, we
enhance the capacity of the learnable positional embedding assigned to each
volume slice. Consequently, the accumulated positional information in each
positional embedding can be generalized to the neighbouring slices, even for
high-resolution volumes at the test time. By doing so, the model will be more
robust to variable volume length and amenable to different computational
budgets. We evaluated the proposed approach in retinal OCT volume
classification and achieved 21.96% average improvement in balanced accuracy on
a 9-class diagnostic task, compared to state-of-the-art video transformers. Our
findings show that varying the volume-wise resolution of the input during
training results in more informative volume representation as compared to
training with fixed number of slices per volume.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oghbaie_M/0/1/0/all/0/1&quot;&gt;Marzieh Oghbaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_T/0/1/0/all/0/1&quot;&gt;Teresa Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emre_T/0/1/0/all/0/1&quot;&gt;Taha Emre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1&quot;&gt;Ursula Schmidt-Erfurth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1&quot;&gt;Hrvoje Bogunovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08092">
<title>Gait Data Augmentation using Physics-Based Biomechanical Simulation. (arXiv:2307.08092v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08092</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on addressing the problem of data scarcity for gait
analysis. Standard augmentation methods may produce gait sequences that are not
consistent with the biomechanical constraints of human walking. To address this
issue, we propose a novel framework for gait data augmentation by using
OpenSIM, a physics-based simulator, to synthesize biomechanically plausible
walking sequences. The proposed approach is validated by augmenting the WBDS
and CASIA-B datasets and then training gait-based classifiers for 3D gender
gait classification and 2D gait person identification respectively.
Experimental results indicate that our augmentation approach can improve the
performance of model-based gait classifiers and deliver state-of-the-art
results for gait-based person identification with an accuracy of up to 96.11%
on the CASIA-B dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekaran_M/0/1/0/all/0/1&quot;&gt;Mritula Chandrasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francik_J/0/1/0/all/0/1&quot;&gt;Jarek Francik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makris_D/0/1/0/all/0/1&quot;&gt;Dimitrios Makris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09004">
<title>Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09004</link>
<description rdf:parseType="Literal">&lt;p&gt;Ordinal regression refers to classifying object instances into ordinal
categories. It has been widely studied in many scenarios, such as medical
disease grading, movie rating, etc. Known methods focused only on learning
inter-class ordinal relationships, but still incur limitations in
distinguishing adjacent categories thus far. In this paper, we propose a simple
sequence prediction framework for ordinal regression called Ord2Seq, which, for
the first time, transforms each ordinal category label into a special label
sequence and thus regards an ordinal regression task as a sequence prediction
process. In this way, we decompose an ordinal regression task into a series of
recursive binary classification steps, so as to subtly distinguish adjacent
categories. Comprehensive experiments show the effectiveness of distinguishing
adjacent categories for performance improvement and our new approach exceeds
state-of-the-art performances in four different scenarios. Codes are available
at https://github.com/wjh892521292/Ord2Seq.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jintai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tingting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Danny Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09815">
<title>LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network. (arXiv:2307.09815v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09815</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent
blur is a challenging task.~Existing blur map-based deblurring methods have
demonstrated promising results. In this paper, we propose, to the best of our
knowledge, the first framework to introduce the contrastive language-image
pre-training framework (CLIP) to achieve accurate blur map estimation from DP
pairs unsupervisedly. To this end, we first carefully design text prompts to
enable CLIP to understand blur-related geometric prior knowledge from the DP
pair. Then, we propose a format to input stereo DP pair to the CLIP without any
fine-tuning, where the CLIP is pre-trained on monocular images. Given the
estimated blur map, we introduce a blur-prior attention block, a blur-weighting
loss and a blur-aware loss to recover the all-in-focus image. Our method
achieves state-of-the-art performance in extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miaomiao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10577">
<title>Ethosight: A Reasoning-Guided Iterative Learning System for Nuanced Perception based on Joint-Embedding &amp; Contextual Label Affinity. (arXiv:2307.10577v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10577</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional computer vision models often require extensive manual effort for
data acquisition, annotation and validation, particularly when detecting subtle
behavioral nuances or events. The difficulty in distinguishing routine
behaviors from potential risks in real-world applications, such as
differentiating routine shopping from potential shoplifting, further
complicates the process. Moreover, these models may demonstrate high false
positive rates and imprecise event detection when exposed to real-world
scenarios that differ significantly from the conditions of the training data.
&lt;/p&gt;
&lt;p&gt;To overcome these hurdles, we present Ethosight, a novel zero-shot computer
vision system. Ethosight initiates with a clean slate based on user
requirements and semantic knowledge of interest. Using localized label affinity
calculations and a reasoning-guided iterative learning loop, Ethosight infers
scene details and iteratively refines the label set. Reasoning mechanisms can
be derived from large language models like GPT4, symbolic reasoners like
OpenNARS\cite{wang2013}\cite{wang2006}, or hybrid systems.
&lt;/p&gt;
&lt;p&gt;Our evaluations demonstrate Ethosight&apos;s efficacy across 40 complex use cases,
spanning domains such as health, safety, and security. Detailed results and
case studies within the main body of this paper and an appendix underscore a
promising trajectory towards enhancing the adaptability and resilience of
computer vision models in detecting and extracting subtle and nuanced
behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorisson_K/0/1/0/all/0/1&quot;&gt;Kristinn R. Thorisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrosyan_V/0/1/0/all/0/1&quot;&gt;Vahagn Petrosyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_P/0/1/0/all/0/1&quot;&gt;Patrick Hammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kynoch_B/0/1/0/all/0/1&quot;&gt;Brandon Kynoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tangrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10698">
<title>Reverse Knowledge Distillation: Training a Large Model using a Small One for Retinal Image Matching on Limited Data. (arXiv:2307.10698v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10698</link>
<description rdf:parseType="Literal">&lt;p&gt;Retinal image matching plays a crucial role in monitoring disease progression
and treatment response. However, datasets with matched keypoints between
temporally separated pairs of images are not available in abundance to train
transformer-based model. We propose a novel approach based on reverse knowledge
distillation to train large models with limited data while preventing
overfitting. Firstly, we propose architectural modifications to a CNN-based
semi-supervised method called SuperRetina that help us improve its results on a
publicly available dataset. Then, we train a computationally heavier model
based on a vision transformer encoder using the lighter CNN-based model, which
is counter-intuitive in the field knowledge-distillation research where
training lighter models based on heavier ones is the norm. Surprisingly, such
reverse knowledge distillation improves generalization even further. Our
experiments suggest that high-dimensional fitting in representation space may
prevent overfitting unlike training directly to match the final output. We also
provide a public dataset with annotations for retinal image keypoint detection
and matching to help the research community develop algorithms for retinal
image applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasser_S/0/1/0/all/0/1&quot;&gt;Sahar Almahfouz Nasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupte_N/0/1/0/all/0/1&quot;&gt;Nihar Gupte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1&quot;&gt;Amit Sethi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10711">
<title>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing customization methods require access to multiple reference examples
to align pre-trained diffusion probabilistic models (DPMs) with user-provided
concepts. This paper aims to address the challenge of DPM customization when
the only available supervision is a differentiable metric defined on the
generated contents. Since the sampling procedure of DPMs involves recursive
calls to the denoising UNet, na\&quot;ive gradient backpropagation requires storing
the intermediate states of all iterations, resulting in extremely high memory
consumption. To overcome this issue, we propose a novel method AdjointDPM,
which first generates new samples from diffusion models by solving the
corresponding probability-flow ODEs. It then uses the adjoint sensitivity
method to backpropagate the gradients of the loss to the models&apos; parameters
(including conditioning signals, network weights, and initial noises) by
solving another augmented ODE. To reduce numerical errors in both the forward
generation and gradient backpropagation processes, we further reparameterize
the probability-flow ODE and augmented ODE as simple non-stiff ODEs using
exponential integration. Finally, we demonstrate the effectiveness of
AdjointDPM on three interesting tasks: converting visual effects into
identification text embeddings, finetuning DPMs for specific types of
stylization, and optimizing initial noise to generate adversarial samples for
security auditing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10829">
<title>Exact Diffusion Inversion via Bi-directional Integration Approximation. (arXiv:2307.10829v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, different methods have been proposed to address the inconsistency
issue of DDIM inversion to enable image editing, such as EDICT
\cite{Wallace23EDICT} and Null-text inversion \cite{Mokady23NullTestInv}.
However, the above methods introduce considerable computational overhead. In
this paper, we propose a new technique, named \emph{bi-directional integration
approximation} (BDIA), to perform exact diffusion inversion with neglible
computational overhead. Suppose we would like to estimate the next diffusion
state $\boldsymbol{z}_{i-1}$ at timestep $t_i$ with the historical information
$(i,\boldsymbol{z}_i)$ and $(i+1,\boldsymbol{z}_{i+1})$. We first obtain the
estimated Gaussian noise $\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i)$, and
then apply the DDIM update procedure twice for approximating the ODE
integration over the next time-slot $[t_i, t_{i-1}]$ in the forward manner and
the previous time-slot $[t_i, t_{t+1}]$ in the backward manner. The DDIM step
for the previous time-slot is used to refine the integration approximation made
earlier when computing $\boldsymbol{z}_i$. One nice property with BDIA-DDIM is
that the update expression for $\boldsymbol{z}_{i-1}$ is a linear combination
of $(\boldsymbol{z}_{i+1}, \boldsymbol{z}_i,
\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i))$. This allows for exact
backward computation of $\boldsymbol{z}_{i+1}$ given $(\boldsymbol{z}_i,
\boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. Experiments
on both image reconstruction and image editing were conducted, confirming our
statement. BDIA can also be applied to improve the performance of other ODE
solvers in addition to DDIM. In our work, it is found that applying BDIA to the
EDM sampling procedure produces slightly better FID score over CIFAR10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;J. P. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. Bastiaan Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10926">
<title>Confidence intervals for performance estimates in 3D medical image segmentation. (arXiv:2307.10926v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10926</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical segmentation models are evaluated empirically. As such an evaluation
is based on a limited set of example images, it is unavoidably noisy. Beyond a
mean performance measure, reporting confidence intervals is thus crucial.
However, this is rarely done in medical image segmentation. The width of the
confidence interval depends on the test set size and on the spread of the
performance measure (its standard-deviation across of the test set). For
classification, many test images are needed to avoid wide confidence intervals.
Segmentation, however, has not been studied, and it differs by the amount of
information brought by a given test image. In this paper, we study the typical
confidence intervals in medical image segmentation. We carry experiments on 3D
image segmentation using the standard nnU-net framework, two datasets from the
Medical Decathlon challenge and two performance measures: the Dice accuracy and
the Hausdorff distance. We show that the parametric confidence intervals are
reasonable approximations of the bootstrap estimates for varying test set sizes
and spread of the performance metric. Importantly, we show that the test size
needed to achieve a given precision is often much lower than for classification
tasks. Typically, a 1% wide confidence interval requires about 100-200 test
samples when the spread is low (standard-deviation around 3%). More difficult
segmentation tasks may lead to higher spreads and require over 1000 samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jurdi_R/0/1/0/all/0/1&quot;&gt;R. El Jurdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Varoquaux_G/0/1/0/all/0/1&quot;&gt;G. Varoquaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Colliot_O/0/1/0/all/0/1&quot;&gt;O. Colliot&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>