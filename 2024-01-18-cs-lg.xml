<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/1809.04564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1902.01453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.12410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.05825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.04614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.12190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2101.00009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2101.04645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.01223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.01861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.00032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.00116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.12056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.09824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.10399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.13004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.10083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.11802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.13322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.05104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.00932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.12787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.00285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.09507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.10199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.14314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.01215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.14566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.20004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14129" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/1809.04564">
<title>On the Generalization of Stochastic Gradient Descent with Momentum. (arXiv:1809.04564v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.04564</link>
<description rdf:parseType="Literal">&lt;p&gt;While momentum-based accelerated variants of stochastic gradient descent
(SGD) are widely used when training machine learning models, there is little
theoretical understanding on the generalization error of such methods. In this
work, we first show that there exists a convex loss function for which the
stability gap for multiple epochs of SGD with standard heavy-ball momentum
(SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze
a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM)
under a broad range of step-sizes, and show that it can train machine learning
models for multiple epochs with a guarantee for generalization. Finally, for
the special case of strongly convex loss functions, we find a range of momentum
such that multiple epochs of standard SGDM, as a special form of SGDEM, also
generalizes. Extending our results on generalization, we also develop an upper
bound on the expected true risk, in terms of the number of training steps,
sample size, and momentum. Our experimental evaluations verify the consistency
between the numerical results and our theoretical bounds. SGDEM improves the
generalization error of SGDM when training ResNet-18 on ImageNet in practical
distributed settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramezani_Kebrya_A/0/1/0/all/0/1&quot;&gt;Ali Ramezani-Kebrya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonakopoulos_K/0/1/0/all/0/1&quot;&gt;Kimon Antonakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1&quot;&gt;Ashish Khisti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1&quot;&gt;Ben Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1902.01453">
<title>PVNet: A LRCN Architecture for Spatio-Temporal Photovoltaic PowerForecasting from Numerical Weather Prediction. (arXiv:1902.01453v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1902.01453</link>
<description rdf:parseType="Literal">&lt;p&gt;Photovoltaic (PV) power generation has emerged as one of the lead renewable
energy sources. Yet, its production is characterized by high uncertainty, being
dependent on weather conditions like solar irradiance and temperature.
Predicting PV production, even in the 24-hour forecast, remains a challenge and
leads energy providers to left idling - often carbon emitting - plants. In this
paper, we introduce a Long-Term Recurrent Convolutional Network using Numerical
Weather Predictions (NWP) to predict, in turn, PV production in the 24-hour and
48-hour forecast horizons. This network architecture fully leverages both
temporal and spatial weather data, sampled over the whole geographical area of
interest. We train our model on an NWP dataset from the National Oceanic and
Atmospheric Administration (NOAA) to predict spatially aggregated PV production
in Germany. We compare its performance to the persistence model and
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathe_J/0/1/0/all/0/1&quot;&gt;Johan Mathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1&quot;&gt;Nina Miolane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebastien_N/0/1/0/all/0/1&quot;&gt;Nicolas Sebastien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lequeux_J/0/1/0/all/0/1&quot;&gt;Jeremie Lequeux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.12410">
<title>On Biased Compression for Distributed Learning. (arXiv:2002.12410v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2002.12410</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last few years, various communication compression techniques have
emerged as an indispensable tool helping to alleviate the communication
bottleneck in distributed learning. However, despite the fact biased
compressors often show superior performance in practice when compared to the
much more studied and understood unbiased compressors, very little is known
about them. In this work we study three classes of biased compression
operators, two of which are new, and their performance when applied to
(stochastic) gradient descent and distributed (stochastic) gradient descent. We
show for the first time that biased compressors can lead to linear convergence
rates both in the single node and distributed settings. We prove that
distributed compressed SGD method, employed with error feedback mechanism,
enjoys the ergodic rate $O\left( \delta L \exp \left[-\frac{\mu K}{\delta
L}\right] + \frac{(C + \delta D)}{K\mu}\right)$, where $\delta\ge 1$ is a
compression parameter which grows when more compression is applied, $L$ and
$\mu$ are the smoothness and strong convexity constants, $C$ captures
stochastic gradient noise ($C=0$ if full gradients are computed on each node)
and $D$ captures the variance of the gradients at the optimum ($D=0$ for
over-parameterized models). Further, via a theoretical study of several
synthetic and empirical distributions of communicated gradients, we shed light
on why and by how much biased compressors outperform their unbiased variants.
Finally, we propose several new biased compressors with promising theoretical
guarantees and practical performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beznosikov_A/0/1/0/all/0/1&quot;&gt;Aleksandr Beznosikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1&quot;&gt;Samuel Horv&amp;#xe1;th&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safaryan_M/0/1/0/all/0/1&quot;&gt;Mher Safaryan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.05825">
<title>Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions. (arXiv:2008.05825v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2008.05825</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural-network based predictions of event properties in astro-particle
physics are getting more and more common. However, in many cases the result is
just utilized as a point prediction. Statistical uncertainties, coverage,
systematic uncertainties or a goodness-of-fit measure are often not calculated.
Here we describe a certain choice of training and network architecture that
allows to incorporate all these properties into a single network model. We show
that a KL-divergence objective of the joint distribution of data and labels
allows to unify supervised learning and variational autoencoders (VAEs) under
one umbrella of stochastic variational inference. The unification motivates an
extended supervised learning scheme which allows to calculate a goodness-of-fit
p-value for the neural network model. Conditional normalizing flows amortized
with a neural network are crucial in this construction. We discuss how to
calculate coverage probabilities without numerical integration for specific
&quot;base-ordered&quot; contours that are unique to normalizing flows. Furthermore we
show how systematic uncertainties can be included via effective marginalization
during training. The proposed extended supervised training incorporates (1)
coverage calculation, (2) systematics and (3) a goodness-of-fit measure in a
single machine-learning model. There are in principle no constraints on the
shape of the involved distributions, in fact the machinery works with complex
multi-modal distributions defined on product spaces like $\mathbb{R}^n \times
\mathbb{S}^m$. The coverage calculation, however, requires care in its
interpretation when the distributions are too degenerate. We see great
potential for exploiting this per-event information in event selections or for
fast astronomical alerts which require uncertainty guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glusenkamp_T/0/1/0/all/0/1&quot;&gt;Thorsten Gl&amp;#xfc;senkamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.04614">
<title>End-to-end Kernel Learning via Generative Random Fourier Features. (arXiv:2009.04614v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2009.04614</link>
<description rdf:parseType="Literal">&lt;p&gt;Random Fourier features (RFFs) provide a promising way for kernel learning in
a spectral case. Current RFFs-based kernel learning methods usually work in a
two-stage way. In the first-stage process, learning the optimal feature map is
often formulated as a target alignment problem, which aims to align the learned
kernel with the pre-defined target kernel (usually the ideal kernel). In the
second-stage process, a linear learner is conducted with respect to the mapped
random features. Nevertheless, the pre-defined kernel in target alignment is
not necessarily optimal for the generalization of the linear learner. Instead,
in this paper, we consider a one-stage process that incorporates the kernel
learning and linear learner into a unifying framework. To be specific, a
generative network via RFFs is devised to implicitly learn the kernel, followed
by a linear classifier parameterized as a full-connected layer. Then the
generative network and the classifier are jointly trained by solving the
empirical risk minimization (ERM) problem to reach a one-stage solution. This
end-to-end scheme naturally allows deeper features, in correspondence to a
multi-layer structure, and shows superior generalization performance over the
classical two-stage, RFFs-based methods in real-world classification tasks.
Moreover, inspired by the randomized resampling mechanism of the proposed
method, its enhanced adversarial robustness is investigated and experimentally
verified.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fanghui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.12190">
<title>Towards Robust Neural Networks via Orthogonal Diversity. (arXiv:2010.12190v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2010.12190</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the
images generated by adversarial attacks, which raises researches on the
adversarial robustness of DNNs. A series of methods represented by the
adversarial training and its variants have proven as one of the most effective
techniques in enhancing the DNN robustness. Generally, adversarial training
focuses on enriching the training data by involving perturbed data. Such data
augmentation effect of the involved perturbed data in adversarial training does
not contribute to the robustness of DNN itself and usually suffers from clean
accuracy drop. Towards the robustness of DNN itself, we in this paper propose a
novel defense that aims at augmenting the model in order to learn features that
are adaptive to diverse inputs, including adversarial examples. More
specifically, to augment the model, multiple paths are embedded into the
network, and an orthogonality constraint is imposed on these paths to guarantee
the diversity among them. A margin-maximization loss is then designed to
further boost such DIversity via Orthogonality (DIO). In this way, the proposed
DIO augments the model and enhances the robustness of DNN itself as the learned
features can be corrected by these mutually-orthogonal paths. Extensive
empirical results on various data sets, structures and attacks verify the
stronger adversarial robustness of the proposed DIO utilizing model
augmentation. Besides, DIO can also be flexibly combined with different data
augmentation techniques (e.g., TRADES and DDPM), further promoting robustness
gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1&quot;&gt;Qinghua Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jia Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1&quot;&gt;Feipeng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2101.00009">
<title>Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)</title>
<link>http://arxiv.org/abs/2101.00009</link>
<description rdf:parseType="Literal">&lt;p&gt;Many causal and structural parameters are linear functionals of an underlying
regression. The Riesz representer is a key component in the asymptotic variance
of a semiparametrically estimated linear functional. We propose an adversarial
framework to estimate the Riesz representer using general function spaces. We
prove a nonasymptotic mean square rate in terms of an abstract quantity called
the critical radius, then specialize it for neural networks, random forests,
and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use
critical radius theory -- in place of Donsker theory -- to prove asymptotic
normality without sample splitting, uncovering a ``complexity-rate robustness&apos;&apos;
condition. This condition has practical consequences: inference without sample
splitting is possible in several machine learning settings, which may improve
finite sample performance compared to sample splitting. Our estimators achieve
nominal coverage in highly nonlinear simulations where previous methods break
down. They shed new light on the heterogeneous effects of matching grants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Chernozhukov_V/0/1/0/all/0/1&quot;&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Newey_W/0/1/0/all/0/1&quot;&gt;Whitney Newey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rahul Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2101.04645">
<title>Double-Adversarial Activation Anomaly Detection: Adversarial Autoencoders are Anomaly Generators. (arXiv:2101.04645v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2101.04645</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_J/0/1/0/all/0/1&quot;&gt;J.-P. Schulze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperl_P/0/1/0/all/0/1&quot;&gt;P. Sperl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1&quot;&gt;K. B&amp;#xf6;ttinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.01223">
<title>Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention. (arXiv:2102.01223v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2102.01223</link>
<description rdf:parseType="Literal">&lt;p&gt;Characters do not convey meaning, but sequences of characters do. We propose
an unsupervised distributional method to learn the abstract meaningful units in
a sequence of characters. Rather than segmenting the sequence, our Dynamic
Capacity Slot Attention model discovers continuous representations of the
objects in the sequence, extending an architecture for object discovery in
images. We train our model on different languages and evaluate the quality of
the obtained representations with forward and reverse probing classifiers.
These experiments show that our model succeeds in discovering units which are
similar to those proposed previously in form, content and level of abstraction,
and which show promise for capturing meaningful information at a higher level
of abstraction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behjati_M/0/1/0/all/0/1&quot;&gt;Melika Behjati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1&quot;&gt;James Henderson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.01861">
<title>Follow Your Nose -- Which Code Smells are Worth Chasing?. (arXiv:2103.01861v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2103.01861</link>
<description rdf:parseType="Literal">&lt;p&gt;The common use case of code smells assumes causality: Identify a smell,
remove it, and by doing so improve the code. We empirically investigate their
fitness to this use. We present a list of properties that code smells should
have if they indeed cause lower quality. We evaluated the smells in 31,687 Java
files from 677 GitHub repositories, all the repositories with 200+ commits in
2019. We measured the influence of smells on four metrics for quality,
productivity, and bug detection efficiency. Out of 151 code smells computed by
the CheckStyle smell detector, less than 20% were found to be potentially
causal, and only a handful are rather robust. The strongest smells deal with
simplicity, defensive programming, and abstraction. Files without the
potentially causal smells are 50% more likely to be of high quality.
Unfortunately, most smells are not removed, and developers tend to remove the
easy ones and not the effective ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amit_I/0/1/0/all/0/1&quot;&gt;Idan Amit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezra_N/0/1/0/all/0/1&quot;&gt;Nili Ben Ezra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feitelson_D/0/1/0/all/0/1&quot;&gt;Dror G. Feitelson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.00032">
<title>Convolutional Dynamic Alignment Networks for Interpretable Classifications. (arXiv:2104.00032v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2104.00032</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which linearly transform their input with
weight vectors that dynamically align with task-relevant patterns. As a result,
CoDA-Nets model the classification prediction through a series of
input-dependent linear transformations, allowing for linear decomposition of
the output into individual input contributions. Given the alignment of the
DAUs, the resulting contribution maps align with discriminative input patterns.
These model-inherent decompositions are of high visual quality and outperform
existing attribution methods under quantitative metrics. Further, CoDA-Nets
constitute performant classifiers, achieving on par results to ResNet and VGG
models on e.g. CIFAR-10 and TinyImagenet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohle_M/0/1/0/all/0/1&quot;&gt;Moritz B&amp;#xf6;hle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.00116">
<title>On the Benefits of Inducing Local Lipschitzness for Robust Generative Adversarial Imitation Learning. (arXiv:2107.00116v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2107.00116</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore methodologies to improve the robustness of generative adversarial
imitation learning (GAIL) algorithms to observation noise. Towards this
objective, we study the effect of local Lipschitzness of the discriminator and
the generator on the robustness of policies learned by GAIL. In many robotics
applications, the learned policies by GAIL typically suffer from a degraded
performance at test time since the observations from the environment might be
corrupted by noise. Hence, robustifying the learned policies against the
observation noise is of critical importance. To this end, we propose a
regularization method to induce local Lipschitzness in the generator and the
discriminator of adversarial imitation learning methods. We show that the
modified objective leads to learning significantly more robust policies.
Moreover, we demonstrate -- both theoretically and experimentally -- that
training a locally Lipschitz discriminator leads to a locally Lipschitz
generator, thereby improving the robustness of the resultant policy. We perform
extensive experiments on simulated robot locomotion environments from the
MuJoCo suite that demonstrate the proposed method learns policies that
significantly outperform the state-of-the-art generative adversarial imitation
learning algorithm when applied to test scenarios with noise-corrupted
observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1&quot;&gt;Farzan Memarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1&quot;&gt;Abolfazl Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.12056">
<title>Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2108.12056</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow&apos;s machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of learning and retaining new information without being exposed to it
arbitrarily often. Past efforts to engineer such systems have sought to build
or regulate artificial neural networks using disjoint sets of weights that are
uniquely sensitive to specific tasks or inputs. This has not yet enabled
continual learning over long sequences of previously unseen data without
corrupting existing knowledge: a problem known as catastrophic forgetting. In
this paper, we introduce a system that can learn sequentially over previously
unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is
done by controlling the activity of weights in a convolutional neural network
on the basis of inputs using top-down regulation generated by a second
feed-forward neural network. We find that our method learns continually under
domain transfer with sparse bursts of activity in weights that are recycled
across tasks, rather than by maintaining task-specific modules. Sparse synaptic
bursting is found to balance activity and suppression such that new functions
can be learned without corrupting extant knowledge, thus mirroring the balance
of order and disorder in systems at the edge of chaos. This behavior emerges
during a prior pre-training (or &apos;meta-learning&apos;) phase in which regulated
synapses are selectively disinhibited, or grown, from an initial state of
uniform suppression through prediction error minimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1&quot;&gt;Shawn L. Beaulieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.09824">
<title>Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.09824</link>
<description rdf:parseType="Literal">&lt;p&gt;New fashion product sales forecasting is a challenging problem that involves
many business dynamics and cannot be solved by classical forecasting
approaches. In this paper, we investigate the effectiveness of systematically
probing exogenous knowledge in the form of Google Trends time series and
combining it with multi-modal information related to a brand-new fashion item,
in order to effectively forecast its sales despite the lack of past data. In
particular, we propose a neural network-based approach, where an encoder learns
a representation of the exogenous time series, while the decoder forecasts the
sales based on the Google Trends encoding and the available visual and metadata
information. Our model works in a non-autoregressive manner, avoiding the
compounding effect of large first-step errors. As a second contribution, we
present VISUELLE, a publicly available dataset for the task of new fashion
product sales forecasting, containing multimodal information for 5577 real, new
products sold between 2016-2019 from Nunalie, an Italian fast-fashion company.
The dataset is equipped with images of products, metadata, related sales, and
associated Google Trends. We use VISUELLE to compare our approach against
state-of-the-art alternatives and several baselines, showing that our neural
network-based approach is the most accurate in terms of both percentage and
absolute error. It is worth noting that the addition of exogenous knowledge
boosts the forecasting accuracy by 1.5% in terms of Weighted Absolute
Percentage Error (WAPE), revealing the importance of exploiting informative
external information. The code and dataset are both available at
https://github.com/HumaticsLAB/GTM-Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1&quot;&gt;Geri Skenderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1&quot;&gt;Christian Joppi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denitto_M/0/1/0/all/0/1&quot;&gt;Matteo Denitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1&quot;&gt;Marco Cristani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.10399">
<title>SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking. (arXiv:2109.10399v4 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2109.10399</link>
<description rdf:parseType="Literal">&lt;p&gt;Subseasonal forecasting of the weather two to six weeks in advance is
critical for resource allocation and advance disaster notice but poses many
challenges for the forecasting community. At this forecast horizon,
physics-based dynamical models have limited skill, and the targets for
prediction depend in a complex manner on both local weather variables and
global climate variables. Recently, machine learning methods have shown promise
in advancing the state of the art but only at the cost of complex data
curation, integrating expert knowledge with aggregation across multiple
relevant data sources, file formats, and temporal and spatial resolutions. To
streamline this process and accelerate future development, we introduce
SubseasonalClimateUSA, a curated dataset for training and benchmarking
subseasonal forecasting models in the United States. We use this dataset to
benchmark a diverse suite of models, including operational dynamical models,
classical meteorological baselines, and ten state-of-the-art machine learning
and deep learning-based methods from the literature. Overall, our benchmarks
suggest simple and effective ways to extend the accuracy of current operational
models. SubseasonalClimateUSA is regularly updated and accessible via the
https://github.com/microsoft/subseasonal_data/ Python package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mouatadid_S/0/1/0/all/0/1&quot;&gt;Soukayna Mouatadid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Orenstein_P/0/1/0/all/0/1&quot;&gt;Paulo Orenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Flaspohler_G/0/1/0/all/0/1&quot;&gt;Genevieve Flaspohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Oprescu_M/0/1/0/all/0/1&quot;&gt;Miruna Oprescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Judah Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Franklyn Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Knight_S/0/1/0/all/0/1&quot;&gt;Sean Knight&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Geogdzhayeva_M/0/1/0/all/0/1&quot;&gt;Maria Geogdzhayeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Levang_S/0/1/0/all/0/1&quot;&gt;Sam Levang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fraenkel_E/0/1/0/all/0/1&quot;&gt;Ernest Fraenkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.13004">
<title>Optimising for Interpretability: Convolutional Dynamic Alignment Networks. (arXiv:2109.13004v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2109.13004</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which are optimised to transform their inputs
with dynamically computed weight vectors that align with task-relevant
patterns. As a result, CoDA Nets model the classification prediction through a
series of input-dependent linear transformations, allowing for linear
decomposition of the output into individual input contributions. Given the
alignment of the DAUs, the resulting contribution maps align with
discriminative input patterns. These model-inherent decompositions are of high
visual quality and outperform existing attribution methods under quantitative
metrics. Further, CoDA Nets constitute performant classifiers, achieving on par
results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,
CoDA Nets can be combined with conventional neural network models to yield
powerful classifiers that more easily scale to complex datasets such as
Imagenet whilst exhibiting an increased interpretable depth, i.e., the output
can be explained well in terms of contributions from intermediate layers within
the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bohle_M/0/1/0/all/0/1&quot;&gt;Moritz B&amp;#xf6;hle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.10083">
<title>Contrastive Active Inference. (arXiv:2110.10083v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.10083</link>
<description rdf:parseType="Literal">&lt;p&gt;Active inference is a unifying theory for perception and action resting upon
the idea that the brain maintains an internal model of the world by minimizing
free energy. From a behavioral perspective, active inference agents can be seen
as self-evidencing beings that act to fulfill their optimistic predictions,
namely preferred outcomes or goals. In contrast, reinforcement learning
requires human-designed rewards to accomplish any desired outcome. Although
active inference could provide a more natural self-supervised objective for
control, its applicability has been limited because of the shortcomings in
scaling the approach to complex environments. In this work, we propose a
contrastive objective for active inference that strongly reduces the
computational burden in learning the agent&apos;s generative model and planning
future actions. Our method performs notably better than likelihood-based active
inference in image-based tasks, while also being computationally cheaper and
easier to train. We compare to reinforcement learning agents that have access
to human-designed reward functions, showing that our approach closely matches
their performance. Finally, we also show that contrastive methods perform
significantly better in the case of distractors in the environment and that our
method is able to generalize goals to variations in the background. Website and
code: https://contrastive-aif.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzaglia_P/0/1/0/all/0/1&quot;&gt;Pietro Mazzaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1&quot;&gt;Tim Verbelen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhoedt_B/0/1/0/all/0/1&quot;&gt;Bart Dhoedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.11802">
<title>Pruning Self-attentions into Convolutional Layers in Single Path. (arXiv:2111.11802v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.11802</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have achieved impressive performance over various
computer vision tasks. However, modeling global correlations with multi-head
self-attention (MSA) layers leads to two widely recognized issues: the massive
computational resource consumption and the lack of intrinsic inductive bias for
modeling local visual patterns. To solve both issues, we devise a simple yet
effective method named Single-Path Vision Transformer pruning (SPViT), to
efficiently and automatically compress the pre-trained ViTs into compact models
with proper locality added. Specifically, we first propose a novel
weight-sharing scheme between MSA and convolutional operations, delivering a
single-path space to encode all candidate operations. In this way, we cast the
operation search problem as finding which subset of parameters to use in each
MSA layer, which significantly reduces the computational cost and optimization
difficulty, and the convolution kernels can be well initialized using
pre-trained MSA parameters. Relying on the single-path space, we introduce
learnable binary gates to encode the operation choices in MSA layers.
Similarly, we further employ learnable gates to encode the fine-grained MLP
expansion ratios of FFN layers. In this way, our SPViT optimizes the learnable
gates to automatically explore from a vast and unified search space and
flexibly adjust the MSA-FFN pruning proportions for each individual dense
model. We conduct extensive experiments on two representative ViTs showing that
our SPViT achieves a new SOTA for pruning on ImageNet-1k. For example, our
SPViT can trim 52.0% FLOPs for DeiT-B and get an impressive 0.6% top-1 accuracy
gain simultaneously. The source code is available at
https://github.com/ziplab/SPViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haoyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zizheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.13322">
<title>Random-reshuffled SARAH does not need a full gradient computations. (arXiv:2111.13322v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.13322</link>
<description rdf:parseType="Literal">&lt;p&gt;The StochAstic Recursive grAdient algoritHm (SARAH) algorithm is a variance
reduced variant of the Stochastic Gradient Descent (SGD) algorithm that needs a
gradient of the objective function from time to time. In this paper, we remove
the necessity of a full gradient computation. This is achieved by using a
randomized reshuffling strategy and aggregating stochastic gradients obtained
in each epoch. The aggregated stochastic gradients serve as an estimate of a
full gradient in the SARAH algorithm. We provide a theoretical analysis of the
proposed approach and conclude the paper with numerical experiments that
demonstrate the efficiency of this approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beznosikov_A/0/1/0/all/0/1&quot;&gt;Aleksandr Beznosikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1&quot;&gt;Martin Tak&amp;#xe1;&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02423">
<title>Improved Information Theoretic Generalization Bounds for Distributed and Federated Learning. (arXiv:2202.02423v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02423</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider information-theoretic bounds on expected generalization error for
statistical learning problems in a networked setting. In this setting, there
are $K$ nodes, each with its own independent dataset, and the models from each
node have to be aggregated into a final centralized model. We consider both
simple averaging of the models as well as more complicated multi-round
algorithms. We give upper bounds on the expected generalization error for a
variety of problems, such as those with Bregman divergence or Lipschitz
continuous losses, that demonstrate an improved dependence of $1/K$ on the
number of nodes. These &quot;per node&quot; bounds are in terms of the mutual information
between the training dataset and the trained weights at each node, and are
therefore useful in describing the generalization properties inherent to having
communication or privacy constraints at each node.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1&quot;&gt;L. P. Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dytso_A/0/1/0/all/0/1&quot;&gt;Alex Dytso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1&quot;&gt;H. V. Poor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.05104">
<title>Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation. (arXiv:2204.05104v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.05104</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation (DA) tries to tackle the scenarios when the test data does
not fully follow the same distribution of the training data, and multi-source
domain adaptation (MSDA) is very attractive for real world applications. By
learning from large-scale unlabeled samples, self-supervised learning has now
become a new trend in deep learning. It is worth noting that both
self-supervised learning and multi-source domain adaptation share a similar
goal: they both aim to leverage unlabeled data to learn more expressive
representations. Unfortunately, traditional multi-task self-supervised learning
faces two challenges: (1) the pretext task may not strongly relate to the
downstream task, thus it could be difficult to learn useful knowledge being
shared from the pretext task to the target task; (2) when the same feature
extractor is shared between the pretext task and the downstream one and only
different prediction heads are used, it is ineffective to enable inter-task
information exchange and knowledge sharing. To address these issues, we propose
a novel \textbf{S}elf-\textbf{S}upervised \textbf{G}raph Neural Network (SSG),
where a graph neural network is used as the bridge to enable more effective
inter-task information exchange and knowledge sharing. More expressive
representation is learned by adopting a mask token strategy to mask some domain
information. Our extensive experiments have demonstrated that our proposed SSG
method has achieved state-of-the-art results over four multi-source domain
adaptation datasets, which have shown the effectiveness of our proposed SSG
method from different aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1&quot;&gt;Feng Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yangzhou Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhongchao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rui_Y/0/1/0/all/0/1&quot;&gt;Yong Rui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.00932">
<title>Understanding CNNs from excitations. (arXiv:2205.00932v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.00932</link>
<description rdf:parseType="Literal">&lt;p&gt;Saliency maps have proven to be a highly efficacious approach for explicating
the decisions of Convolutional Neural Networks. However, extant methodologies
predominantly rely on gradients, which constrain their ability to explicate
complex models. Furthermore, such approaches are not fully adept at leveraging
negative gradient information to improve interpretive veracity. In this study,
we present a novel concept, termed positive and negative excitation, which
enables the direct extraction of positive and negative excitation for each
layer, thus enabling complete layer-by-layer information utilization sans
gradients. To organize these excitations into final saliency maps, we introduce
a double-chain backpropagation procedure. A comprehensive experimental
evaluation, encompassing both binary classification and multi-classification
tasks, was conducted to gauge the effectiveness of our proposed method.
Encouragingly, the results evince that our approach offers a significant
improvement over the state-of-the-art methods in terms of salient pixel
removal, minor pixel removal, and inconspicuous adversarial perturbation
generation guidance. Additionally, we verify the correlation between positive
and negative excitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1&quot;&gt;Zijian Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qianmu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1&quot;&gt;Zhichao Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03990">
<title>Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics. (arXiv:2205.03990v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03990</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional data-driven deep learning models often struggle with high
training costs, error accumulation, and poor generalizability in complex
physical processes. Physics-informed deep learning (PiDL) addresses these
challenges by incorporating physical principles into the model. Most PiDL
approaches regularize training by embedding governing equations into the loss
function, yet this depends heavily on extensive hyperparameter tuning to weigh
each loss term. To this end, we propose to leverage physics prior knowledge by
``baking&apos;&apos; the discretized governing equations into the neural network
architecture via the connection between the partial differential equations
(PDE) operators and network structures, resulting in a PDE-preserved neural
network (PPNN). This method, embedding discretized PDEs through convolutional
residual networks in a multi-resolution setting, largely improves the
generalizability and long-term prediction accuracy, outperforming conventional
black-box models. The effectiveness and merit of the proposed methods have been
demonstrated across various spatiotemporal dynamical systems governed by
spatiotemporal PDEs, including reaction-diffusion, Burgers&apos;, and Navier-Stokes
equations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin-Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Min Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian-Xun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.12787">
<title>Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.12787</link>
<description rdf:parseType="Literal">&lt;p&gt;While AlphaZero-style reinforcement learning (RL) algorithms excel in various
board games, in this paper we show that they face challenges on impartial games
where players share pieces. We present a concrete example of a game - namely
the children&apos;s game of Nim - and other impartial games that seem to be a
stumbling block for AlphaZero-style and similar self-play reinforcement
learning algorithms.
&lt;/p&gt;
&lt;p&gt;Our work is built on the challenges posed by the intricacies of data
distribution on the ability of neural networks to learn parity functions,
exacerbated by the noisy labels issue. Our findings are consistent with recent
studies showing that AlphaZero-style algorithms are vulnerable to adversarial
attacks and adversarial perturbations, showing the difficulty of learning to
master the games in all legal states.
&lt;/p&gt;
&lt;p&gt;We show that Nim can be learned on small boards, but the learning progress of
AlphaZero-style algorithms dramatically slows down when the board size
increases. Intuitively, the difference between impartial games like Nim and
partisan games like Chess and Go can be explained by the fact that if a small
part of the board is covered for impartial games it is typically not possible
to predict whether the position is won or lost as there is often zero
correlation between the visible part of a partly blanked-out position and its
correct evaluation. This situation starkly contrasts partisan games where a
partly blanked-out board position typically provides abundant or at least
non-trifle information about the value of the fully uncovered position.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riis_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Riis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13114">
<title>Contextual Pandora&apos;s Box. (arXiv:2205.13114v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13114</link>
<description rdf:parseType="Literal">&lt;p&gt;Pandora&apos;s Box is a fundamental stochastic optimization problem, where the
decision-maker must find a good alternative while minimizing the search cost of
exploring the value of each alternative. In the original formulation, it is
assumed that accurate distributions are given for the values of all the
alternatives, while recent work studies the online variant of Pandora&apos;s Box
where the distributions are originally unknown. In this work, we study
Pandora&apos;s Box in the online setting, while incorporating context. At every
round, we are presented with a number of alternatives each having a context, an
exploration cost and an unknown value drawn from an unknown distribution that
may change at every round. Our main result is a no-regret algorithm that
performs comparably well to the optimal algorithm which knows all prior
distributions exactly. Our algorithm works even in the bandit setting where the
algorithm never learns the values of the alternatives that were not explored.
The key technique that enables our result is a novel modification of the
realizability condition in contextual bandits that connects a context to a
sufficient statistic of each alternative&apos;s distribution (its &quot;reservation
value&quot;) rather than its mean.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atsidakou_A/0/1/0/all/0/1&quot;&gt;Alexia Atsidakou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1&quot;&gt;Constantine Caramanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gergatsouli_E/0/1/0/all/0/1&quot;&gt;Evangelia Gergatsouli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadigenopoulos_O/0/1/0/all/0/1&quot;&gt;Orestis Papadigenopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1&quot;&gt;Christos Tzamos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14855">
<title>Leave-one-out Singular Subspace Perturbation Analysis for Spectral Clustering. (arXiv:2205.14855v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14855</link>
<description rdf:parseType="Literal">&lt;p&gt;The singular subspaces perturbation theory is of fundamental importance in
probability and statistics. It has various applications across different
fields. We consider two arbitrary matrices where one is a leave-one-column-out
submatrix of the other one and establish a novel perturbation upper bound for
the distance between the two corresponding singular subspaces. It is
well-suited for mixture models and results in a sharper and finer statistical
analysis than classical perturbation bounds such as Wedin&apos;s Theorem. Empowered
by this leave-one-out perturbation theory, we provide a deterministic entrywise
analysis for the performance of spectral clustering under mixture models. Our
analysis leads to an explicit exponential error rate for spectral clustering of
sub-Gaussian mixture models. For the mixture of isotropic Gaussians, the rate
is optimal under a weaker signal-to-noise condition than that of L{\&quot;o}ffler et
al. (2021).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anderson Y. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Harrison H. Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.00285">
<title>Stochastic Gradient Methods with Preconditioned Updates. (arXiv:2206.00285v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2206.00285</link>
<description rdf:parseType="Literal">&lt;p&gt;This work considers the non-convex finite sum minimization problem. There are
several algorithms for such problems, but existing methods often work poorly
when the problem is badly scaled and/or ill-conditioned, and a primary goal of
this work is to introduce methods that alleviate this issue. Thus, here we
include a preconditioner based on Hutchinson&apos;s approach to approximating the
diagonal of the Hessian, and couple it with several gradient-based methods to
give new scaled algorithms: Scaled SARAH and Scaled L-SVRG. Theoretical
complexity guarantees under smoothness assumptions are presented. We prove
linear convergence when both smoothness and the PL condition are assumed. Our
adaptively scaled methods use approximate partial second-order curvature
information and, therefore, can better mitigate the impact of badly scaled
problems. This improved practical performance is demonstrated in the numerical
experiments also presented in this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sadiev_A/0/1/0/all/0/1&quot;&gt;Abdurakhmon Sadiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Beznosikov_A/0/1/0/all/0/1&quot;&gt;Aleksandr Beznosikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Almansoori_A/0/1/0/all/0/1&quot;&gt;Abdulla Jasem Almansoori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kamzolov_D/0/1/0/all/0/1&quot;&gt;Dmitry Kamzolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tappenden_R/0/1/0/all/0/1&quot;&gt;Rachael Tappenden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1&quot;&gt;Martin Tak&amp;#xe1;&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08756">
<title>Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay. (arXiv:2206.08756v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08756</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the tensor-on-tensor regression, where the goal is to connect tensor
responses to tensor covariates with a low Tucker rank parameter tensor/matrix
without the prior knowledge of its intrinsic rank. We propose the Riemannian
gradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with
the challenge of unknown rank by studying the effect of rank
over-parameterization. We provide the first convergence guarantee for the
general tensor-on-tensor regression by showing that RGD and RGN respectively
converge linearly and quadratically to a statistically optimal estimate in both
rank correctly-parameterized and over-parameterized settings. Our theory
reveals an intriguing phenomenon: Riemannian optimization methods naturally
adapt to over-parameterization without modifications to their implementation.
We also prove the statistical-computational gap in scalar-on-tensor regression
by a direct low-degree polynomial argument. Our theory demonstrates a &quot;blessing
of statistical-computational gap&quot; phenomenon: in a wide range of scenarios in
tensor-on-tensor regression for tensors of order three or higher, the
computationally required sample size matches what is needed by moderate rank
over-parameterization when considering computationally feasible estimators,
while there are no such benefits in the matrix settings. This shows moderate
rank over-parameterization is essentially &quot;cost-free&quot; in terms of sample size
in tensor-on-tensor regression of order three or higher. Finally, we conduct
simulation studies to show the advantages of our proposed methods and to
corroborate our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yuetian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru R. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.09507">
<title>Resource-Efficient Separation Transformer. (arXiv:2206.09507v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2206.09507</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have recently achieved state-of-the-art performance in speech
separation. These models, however, are computationally demanding and require a
lot of learnable parameters. This paper explores Transformer-based speech
separation with a reduced computational cost. Our main contribution is the
development of the Resource-Efficient Separation Transformer (RE-SepFormer), a
self-attention-based architecture that reduces the computational burden in two
ways. First, it uses non-overlapping blocks in the latent space. Second, it
operates on compact latent summaries calculated from each chunk. The
RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and
WHAM! datasets in both causal and non-causal settings. Remarkably, it scales
significantly better than the previous Transformer-based architectures in terms
of memory and inference time, making it more suitable for processing long
mixtures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Libera_L/0/1/0/all/0/1&quot;&gt;Luca Della Libera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Subakan_C/0/1/0/all/0/1&quot;&gt;Cem Subakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1&quot;&gt;Mirco Ravanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cornell_S/0/1/0/all/0/1&quot;&gt;Samuele Cornell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lepoutre_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Lepoutre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grondin_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Grondin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11030">
<title>KeyCLD: Learning Constrained Lagrangian Dynamics in Keypoint Coordinates from Images. (arXiv:2206.11030v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11030</link>
<description rdf:parseType="Literal">&lt;p&gt;We present KeyCLD, a framework to learn Lagrangian dynamics from images.
Learned keypoints represent semantic landmarks in images and can directly
represent state dynamics. We show that interpreting this state as Cartesian
coordinates, coupled with explicit holonomic constraints, allows expressing the
dynamics with a constrained Lagrangian. KeyCLD is trained unsupervised
end-to-end on sequences of images. Our method explicitly models the mass
matrix, potential energy and the input matrix, thus allowing energy based
control. We demonstrate learning of Lagrangian dynamics from images on the
dm_control pendulum, cartpole and acrobot environments. KeyCLD can be learned
on these systems, whether they are unactuated, underactuated or fully actuated.
Trained models are able to produce long-term video predictions, showing that
the dynamics are accurately learned. We compare with Lag-VAE, Lag-caVAE and
HGN, and investigate the benefit of the Lagrangian prior and the constraint
function. KeyCLD achieves the highest valid prediction time on all benchmarks.
Additionally, a very straightforward energy shaping controller is successfully
applied on the fully actuated systems. Please refer to our project page for
code and additional results: https://rdaems.github.io/keycld/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daems_R/0/1/0/all/0/1&quot;&gt;Rembert Daems&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taets_J/0/1/0/all/0/1&quot;&gt;Jeroen Taets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+wyffels_F/0/1/0/all/0/1&quot;&gt;Francis wyffels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crevecoeur_G/0/1/0/all/0/1&quot;&gt;Guillaume Crevecoeur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08195">
<title>SPIRAL: A superlinearly convergent incremental proximal algorithm for nonconvex finite sum minimization. (arXiv:2207.08195v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08195</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SPIRAL, a SuPerlinearly convergent Incremental pRoximal
ALgorithm, for solving nonconvex regularized finite sum problems under a
relative smoothness assumption. Each iteration of SPIRAL consists of an inner
and an outer loop. It combines incremental gradient updates with a linesearch
that has the remarkable property of never being triggered asymptotically,
leading to superlinear convergence under mild assumptions at the limit point.
Simulation results with L-BFGS directions on different convex, nonconvex, and
non-Lipschitz differentiable problems show that our algorithm, as well as its
adaptive variant, are competitive to the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Behmandpoor_P/0/1/0/all/0/1&quot;&gt;Pourya Behmandpoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Latafat_P/0/1/0/all/0/1&quot;&gt;Puya Latafat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Themelis_A/0/1/0/all/0/1&quot;&gt;Andreas Themelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moonen_M/0/1/0/all/0/1&quot;&gt;Marc Moonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1&quot;&gt;Panagiotis Patrinos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.10199">
<title>Provably tuning the ElasticNet across instances. (arXiv:2207.10199v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.10199</link>
<description rdf:parseType="Literal">&lt;p&gt;An important unresolved challenge in the theory of regularization is to set
the regularization coefficients of popular techniques like the ElasticNet with
general provable guarantees. We consider the problem of tuning the
regularization parameters of Ridge regression, LASSO, and the ElasticNet across
multiple problem instances, a setting that encompasses both cross-validation
and multi-task hyperparameter optimization. We obtain a novel structural result
for the ElasticNet which characterizes the loss as a function of the tuning
parameters as a piecewise-rational function with algebraic boundaries. We use
this to bound the structural complexity of the regularized loss functions and
show generalization guarantees for tuning the ElasticNet regression
coefficients in the statistical setting. We also consider the more challenging
online learning setting, where we show vanishing average expected regret
relative to the optimal parameter pair. We further extend our results to tuning
classification algorithms obtained by thresholding regression fits regularized
by Ridge, LASSO, or ElasticNet. Our results are the first general
learning-theoretic guarantees for this important class of problems that avoid
strong assumptions on the data distribution. Furthermore, our guarantees hold
for both validation and popular information criterion objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1&quot;&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1&quot;&gt;Mikhail Khodak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1&quot;&gt;Dravyansh Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1&quot;&gt;Ameet Talwalkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.14314">
<title>Supplementing Recurrent Neural Network Wave Functions with Symmetry and Annealing to Improve Accuracy. (arXiv:2207.14314v2 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2207.14314</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are a class of neural networks that have
emerged from the paradigm of artificial intelligence and has enabled lots of
interesting advances in the field of natural language processing.
Interestingly, these architectures were shown to be powerful ansatze to
approximate the ground state of quantum systems. Here, we build over the
results of [Phys. Rev. Research 2, 023358 (2020)] and construct a more powerful
RNN wave function ansatz in two dimensions. We use symmetry and annealing to
obtain accurate estimates of ground state energies of the two-dimensional (2D)
Heisenberg model, on the square lattice and on the triangular lattice. We show
that our method is superior to Density Matrix Renormalisation Group (DMRG) for
system sizes larger than or equal to $14 \times 14$ on the triangular lattice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hibat_Allah_M/0/1/0/all/0/1&quot;&gt;Mohamed Hibat-Allah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Melko_R/0/1/0/all/0/1&quot;&gt;Roger G. Melko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Carrasquilla_J/0/1/0/all/0/1&quot;&gt;Juan Carrasquilla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.01215">
<title>NAPA: Intermediate-level Variational Native-pulse Ansatz for Variational Quantum Algorithms. (arXiv:2208.01215v5 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2208.01215</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational quantum algorithms (VQAs) have demonstrated great potentials in
the Noisy Intermediate Scale Quantum (NISQ) era. In the workflow of VQA, the
parameters of ansatz are iteratively updated to approximate the desired quantum
states. We have seen various efforts to draft better ansatz with less gates.
Some works consider the physical meaning of the underlying circuits, while
others adopt the ideas of neural architecture search (NAS) for ansatz
generator. However, these designs do not exploit the full advantages of VQAs.
Because most techniques target gate ansatz, and the parameters are usually
rotation angles of the gates. In quantum computers, the gate ansatz will
eventually be transformed into control signals such as microwave pulses on
superconducting qubits. These control pulses need elaborate calibrations to
minimize the errors such as over-rotation and under-rotation. In the case of
VQAs, this procedure will introduce redundancy, but the variational properties
of VQAs can naturally handle problems of over-rotation and under-rotation by
updating the amplitude and frequency parameters. Therefore, we propose NAPA, a
native-pulse ansatz generator framework for VQAs. We generate native-pulse
ansatz with trainable parameters for amplitudes and frequencies. In our
proposed NAPA, we are tuning parametric pulses, which are natively supported on
NISQ computers. Given the limited availability of gradient-based optimizers for
pulse-level quantum programs, we choose to deploy non-gradient optimizers in
our framework. To constrain the number of parameters sent to the optimizer, we
adopt a progressive way to generate our native-pulse ansatz. Experiments are
conducted on both simulators and quantum devices for Variational Quantum
Eigensolver (VQE) tasks to evaluate our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhiding Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jinglei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanrui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hua_F/0/1/0/all/0/1&quot;&gt;Fei Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhixin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yongshan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1&quot;&gt;Fred Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xuehai Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yiyu Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10962">
<title>Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach. (arXiv:2208.10962v3 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10962</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying reaction coordinates(RCs) is an active area of research, given
the crucial role RCs play in determining the progress of a chemical reaction.
The choice of the reaction coordinate is often based on heuristic knowledge.
However, an essential criterion for the choice is that the coordinate should
capture both the reactant and product states unequivocally. Also, the
coordinate should be the slowest one so that all the other degrees of freedom
can easily equilibrate along the reaction coordinate. Also, the coordinate
should be the slowest one so that all the other degrees of freedom can easily
equilibrate along the reaction coordinate. We used a regularised sparse
autoencoder, an energy-based model, to discover a crucial set of reaction
coordinates. Along with discovering reaction coordinates, our model also
predicts the evolution of a molecular dynamics(MD) trajectory. We showcased
that including sparsity enforcing regularisation helps in choosing a small but
important set of reaction coordinates. We used two model systems to demonstrate
our approach: alanine dipeptide system and proflavine and DNA system, which
exhibited intercalation of proflavine into DNA minor groove in an aqueous
environment. We model MD trajectory as a multivariate time series, and our
latent variable model performs the task of multi-step time series prediction.
This idea is inspired by the popular sparse coding approach - to represent each
input sample as a linear combination of few elements taken from a set of
representative patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhijit Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02935">
<title>Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02935</link>
<description rdf:parseType="Literal">&lt;p&gt;There is no, nor will there ever be, single best clustering algorithm, but we
would still like to be able to distinguish between methods which work well on
certain task types and those that systematically underperform. Clustering
algorithms are traditionally evaluated using either internal or external
validity measures. Internal measures quantify different aspects of the obtained
partitions, e.g., the average degree of cluster compactness or point
separability. Yet, their validity is questionable, because the clusterings they
promote can sometimes be meaningless. External measures, on the other hand,
compare the algorithms&apos; outputs to the fixed ground truth groupings that are
provided by experts. In this paper, we argue that the commonly-used classical
partition similarity scores, such as the normalised mutual information,
Fowlkes--Mallows, or adjusted Rand index, miss some desirable properties. In
particular, they do not identify worst-case scenarios correctly nor are they
easily interpretable. As a consequence, it can be difficult to evaluate
clustering algorithms on diverse benchmark datasets. To remedy these issues, we
propose and analyse a new measure: a version of the optimal set-matching
accuracy, which is normalised, monotonic with respect to some similarity
relation, scale invariant, and corrected for the imbalancedness of cluster
sizes (but neither symmetric nor adjusted for chance).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagolewski_M/0/1/0/all/0/1&quot;&gt;Marek Gagolewski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07225">
<title>MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via Mixing Recurrent Soft Decision Trees. (arXiv:2209.07225v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07225</link>
<description rdf:parseType="Literal">&lt;p&gt;While achieving tremendous success in various fields, existing multi-agent
reinforcement learning (MARL) with a black-box neural network architecture
makes decisions in an opaque manner that hinders humans from understanding the
learned knowledge and how input observations influence decisions. Instead,
existing interpretable approaches, such as traditional linear models and
decision trees, usually suffer from weak expressivity and low accuracy. To
address this apparent dichotomy between performance and interpretability, our
solution, MIXing Recurrent soft decision Trees (MIXRTs), is a novel
interpretable architecture that can represent explicit decision processes via
the root-to-leaf path and reflect each agent&apos;s contribution to the team.
Specifically, we construct a novel soft decision tree to address partial
observability by leveraging the advances in recurrent neural networks, and
demonstrate which features influence the decision-making process through the
tree-based model. Then, based on the value decomposition framework, we linearly
assign credit to each agent by explicitly mixing individual action values to
estimate the joint action value using only local observations, providing new
insights into how agents cooperate to accomplish the task. Theoretical analysis
shows that MIXRTs guarantees the structural constraint on additivity and
monotonicity in the factorization of joint action values. Evaluations on the
challenging Spread and StarCraft II tasks show that MIXRTs achieves competitive
performance compared to widely investigated methods and delivers more
straightforward explanations of the decision processes. We explore a promising
path toward developing learning algorithms with both high performance and
interpretability, potentially shedding light on new interpretable paradigms for
MARL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zichuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuanyang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunlin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14408">
<title>RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow. (arXiv:2209.14408v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14408</link>
<description rdf:parseType="Literal">&lt;p&gt;When applied to autonomous vehicle (AV) settings, action recognition can
enhance an environment model&apos;s situational awareness. This is especially
prevalent in scenarios where traditional geometric descriptions and heuristics
in AVs are insufficient. However, action recognition has traditionally been
studied for humans, and its limited adaptability to noisy, un-clipped,
un-pampered, raw RGB data has limited its application in other fields. To push
for the advancement and adoption of action recognition into AVs, this work
proposes a novel two-stage action recognition system, termed RALACs. RALACs
formulates the problem of action recognition for road scenes, and bridges the
gap between it and the established field of human action recognition. This work
shows how attention layers can be useful for encoding the relations across
agents, and stresses how such a scheme can be class-agnostic. Furthermore, to
address the dynamic nature of agents on the road, RALACs constructs a novel
approach to adapting Region of Interest (ROI) Alignment to agent tracks for
downstream action classification. Finally, our scheme also considers the
problem of active agent detection, and utilizes a novel application of fusing
optical flow maps to discern relevant agents in a road scene. We show that our
proposed scheme can outperform the baseline on the ICCV2021 Road Challenge
dataset and by deploying it on a real vehicle platform, we provide preliminary
insight to the usefulness of action recognition in decision making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Eddy Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_A/0/1/0/all/0/1&quot;&gt;Alex Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budhwani_A/0/1/0/all/0/1&quot;&gt;Alikasim Budhwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leather_O/0/1/0/all/0/1&quot;&gt;Owen Leather&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dempster_R/0/1/0/all/0/1&quot;&gt;Rowan Dempster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Sharman_M/0/1/0/all/0/1&quot;&gt;Mohammad Al-Sharman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayside_D/0/1/0/all/0/1&quot;&gt;Derek Rayside&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melek_W/0/1/0/all/0/1&quot;&gt;William Melek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07893">
<title>Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07893</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian processes are frequently deployed as part of larger machine learning
and decision-making systems, for instance in geospatial modeling, Bayesian
optimization, or in latent Gaussian models. Within a system, the Gaussian
process model needs to perform in a stable and reliable manner to ensure it
interacts correctly with other parts of the system. In this work, we study the
numerical stability of scalable sparse approximations based on inducing points.
To do so, we first review numerical stability, and illustrate typical
situations in which Gaussian process models can be unstable. Building on
stability theory originally developed in the interpolation literature, we
derive sufficient and in certain cases necessary conditions on the inducing
points for the computations performed to be numerically stable. For
low-dimensional tasks such as geospatial modeling, we propose an automated
method for computing inducing points satisfying these conditions. This is done
via a modification of the cover tree data structure, which is of independent
interest. We additionally propose an alternative sparse approximation for
regression with a Gaussian likelihood which trades off a small amount of
performance to further improve stability. We provide illustrative examples
showing the relationship between stability of calculations and predictive
performance of inducing point methods on spatial tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1&quot;&gt;Alexander Terenin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burt_D/0/1/0/all/0/1&quot;&gt;David R. Burt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Artemev_A/0/1/0/all/0/1&quot;&gt;Artem Artemev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flaxman_S/0/1/0/all/0/1&quot;&gt;Seth Flaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1&quot;&gt;Mark van der Wilk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rasmussen_C/0/1/0/all/0/1&quot;&gt;Carl Edward Rasmussen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ge_H/0/1/0/all/0/1&quot;&gt;Hong Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16584">
<title>Interpretable CNN-Multilevel Attention Transformer for Rapid Recognition of Pneumonia from Chest X-Ray Images. (arXiv:2210.16584v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16584</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest imaging plays an essential role in diagnosing and predicting patients
with COVID-19 with evidence of worsening respiratory status. Many deep
learning-based approaches for pneumonia recognition have been developed to
enable computer-aided diagnosis. However, the long training and inference time
makes them inflexible, and the lack of interpretability reduces their
credibility in clinical medical practice. This paper aims to develop a
pneumonia recognition framework with interpretability, which can understand the
complex relationship between lung features and related diseases in chest X-ray
(CXR) images to provide high-speed analytics support for medical practice. To
reduce the computational complexity to accelerate the recognition process, a
novel multi-level self-attention mechanism within Transformer has been proposed
to accelerate convergence and emphasize the task-related feature regions.
Moreover, a practical CXR image data augmentation has been adopted to address
the scarcity of medical image data problems to boost the model&apos;s performance.
The effectiveness of the proposed method has been demonstrated on the classic
COVID-19 recognition task using the widespread pneumonia CXR image dataset. In
addition, abundant ablation experiments validate the effectiveness and
necessity of all of the components of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shengchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Sufen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Mengxing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chenyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02658">
<title>Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02658</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, machine learning (ML) has become a popular approach to support
self-adaptation. ML has been used to deal with several problems in
self-adaptation, such as maintaining an up-to-date runtime model under
uncertainty and scalable decision-making. Yet, exploiting ML comes with
inherent challenges. In this paper, we focus on a particularly important
challenge for learning-based self-adaptive systems: drift in adaptation spaces.
With adaptation space we refer to the set of adaptation options a self-adaptive
system can select from at a given time to adapt based on the estimated quality
properties of the adaptation options. Drift of adaptation spaces originates
from uncertainties, affecting the quality properties of the adaptation options.
Such drift may imply that eventually no adaptation option can satisfy the
initial set of the adaptation goals, deteriorating the quality of the system,
or adaptation options may emerge that allow enhancing the adaptation goals. In
ML, such shift corresponds to novel class appearance, a type of concept drift
in target data that common ML techniques have problems dealing with. To tackle
this problem, we present a novel approach to self-adaptation that enhances
learning-based self-adaptive systems with a lifelong ML layer. We refer to this
approach as lifelong self-adaptation. The lifelong ML layer tracks the system
and its environment, associates this knowledge with the current tasks,
identifies new tasks based on differences, and updates the learning models of
the self-adaptive system accordingly. A human stakeholder may be involved to
support the learning process and adjust the learning and goal models. We
present a general architecture for lifelong self-adaptation and apply it to the
case of drift of adaptation spaces that affects the decision-making in
self-adaptation. We validate the approach for a series of scenarios using the
DeltaIoT exemplar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gheibi_O/0/1/0/all/0/1&quot;&gt;Omid Gheibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1&quot;&gt;Danny Weyns&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10747">
<title>Exploring validation metrics for offline model-based optimisation with diffusion models. (arXiv:2211.10747v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10747</link>
<description rdf:parseType="Literal">&lt;p&gt;In model-based optimisation (MBO) we are interested in using machine learning
to design candidates that maximise some measure of reward with respect to a
black box function called the (ground truth) oracle, which is expensive to
compute since it involves executing a real world process. In offline MBO we
wish to do so without assuming access to such an oracle during training or
validation, with makes evaluation non-straightforward. While an approximation
to the ground oracle can be trained and used in place of it during model
validation to measure the mean reward over generated candidates, the evaluation
is approximate and vulnerable to adversarial examples. Measuring the mean
reward of generated candidates over this approximation is one such `validation
metric&apos;, whereas we are interested in a more fundamental question which is
finding which validation metrics correlate the most with the ground truth. This
involves proposing validation metrics and quantifying them over many datasets
for which the ground truth is known, for instance simulated environments. This
is encapsulated under our proposed evaluation framework which is also designed
to measure extrapolation, which is the ultimate goal behind leveraging
generative models for MBO. While our evaluation framework is model agnostic we
specifically evaluate denoising diffusion models due to their state-of-the-art
performance, as well as derive interesting insights such as ranking the most
effective validation metrics as well as discussing important hyperparameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beckham_C/0/1/0/all/0/1&quot;&gt;Christopher Beckham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Piche_A/0/1/0/all/0/1&quot;&gt;Alexandre Piche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vazquez_D/0/1/0/all/0/1&quot;&gt;David Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Christopher Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11691">
<title>Deep Signature Algorithm for Multi-dimensional Path-Dependent Options. (arXiv:2211.11691v3 [q-fin.CP] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11691</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the deep signature algorithms for path-dependent
options. We extend the backward scheme in [Hur\&apos;e-Pham-Warin. Mathematics of
Computation 89, no. 324 (2020)] for state-dependent FBSDEs with reflections to
path-dependent FBSDEs with reflections, by adding the signature layer to the
backward scheme. Our algorithm applies to both European and American type
option pricing problems while the payoff function depends on the whole paths of
the underlying forward stock process. We prove the convergence analysis of our
numerical algorithm with explicit dependence on the truncation order of the
signature and the neural network approximation errors. Numerical examples for
the algorithm are provided including: Amerasian option under the Black-Scholes
model, American option with a path-dependent geometric mean payoff function,
and the Shiryaev&apos;s optimal stopping problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Bayraktar_E/0/1/0/all/0/1&quot;&gt;Erhan Bayraktar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03597">
<title>DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. (arXiv:2212.03597v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03597</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances on deep learning models come at the price of formidable
training cost. The increasing model size is one of the root causes, but another
less-emphasized fact is that data scale is actually increasing at a similar
speed as model scale, and the training cost is proportional to both of them.
Compared to the rapidly evolving model architecture, how to efficiently use the
training data (especially for the expensive foundation model pretraining) is
both less explored and difficult to realize due to the lack of a convenient
framework that focuses on data efficiency capabilities. To this end, we present
DeepSpeed Data Efficiency, a framework that makes better use of data, increases
training efficiency, and improves model quality. Specifically, we propose and
combine two data efficiency techniques: efficient data sampling via a general
curriculum learning library, and efficient data routing via a novel random
layerwise token dropping technique. For GPT-3 1.3B language model pretraining,
our work achieves 12.5x less data/time/cost (\$3.7K if rent on Azure), while
still maintaining 95% of model quality compared to baseline with full data and
cost (\$46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also
achieve the same model quality with up to 2x less data/time/cost, or achieve
better model quality under same data/time/cost. DeepSpeed Data Efficiency is
easy to use and tune, enabling us to easily apply it and verify its benefit on
additional tasks including GPT-3 MoE model pretraining and small-scale
GPT-2/ViT finetuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Conglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Connor Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08949">
<title>Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off. (arXiv:2212.08949v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08949</link>
<description rdf:parseType="Literal">&lt;p&gt;A default assumption in reinforcement learning (RL) and optimal control is
that observations arrive at discrete time points on a fixed clock cycle. Yet,
many applications involve continuous-time systems where the time
discretization, in principle, can be managed. The impact of time discretization
on RL methods has not been fully characterized in existing theory, but a more
detailed analysis of its effect could reveal opportunities for improving
data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation
for LQR systems and uncover a fundamental trade-off between approximation and
statistical error in value estimation. Importantly, these two errors behave
differently to time discretization, leading to an optimal choice of temporal
resolution for a given data budget. These findings show that managing the
temporal resolution can provably improve policy evaluation efficiency in LQR
systems with finite data. Empirically, we demonstrate the trade-off in
numerical simulations of LQR instances and standard RL benchmarks for
non-linear continuous control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zichen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschner_J/0/1/0/all/0/1&quot;&gt;Johannes Kirschner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junxi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanini_F/0/1/0/all/0/1&quot;&gt;Francesco Zanini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1&quot;&gt;Alex Ayoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghan_M/0/1/0/all/0/1&quot;&gt;Masood Dehghan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.14566">
<title>Pontryagin Optimal Control via Neural Networks. (arXiv:2212.14566v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.14566</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving real-world optimal control problems are challenging tasks, as the
complex, high-dimensional system dynamics are usually unrevealed to the
decision maker. It is thus hard to find the optimal control actions
numerically. To deal with such modeling and computation challenges, in this
paper, we integrate Neural Networks with the Pontryagin&apos;s Maximum Principle
(PMP), and propose a sample efficient framework NN-PMP-Gradient. The resulting
controller can be implemented for systems with unknown and complex dynamics. By
taking an iterative approach, the proposed framework not only utilizes the
accurate surrogate models parameterized by neural networks, it also efficiently
recovers the optimality conditions along with the optimal action sequences via
PMP conditions. Numerical simulations on Linear Quadratic Regulator, energy
arbitrage of grid-connected lossy battery, control of single pendulum, and two
MuJoCo locomotion tasks demonstrate our proposed NN-PMP-Gradient is a general
and versatile computation tool for finding optimal solutions. And compared with
the widely applied model-free and model-based reinforcement learning (RL)
algorithms, our NN-PMP-Gradient achieves higher sample-efficiency and
performance in terms of control objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chengyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yize Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09734">
<title>Topological Learning in Multi-Class Data Sets. (arXiv:2301.09734v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09734</link>
<description rdf:parseType="Literal">&lt;p&gt;We specialize techniques from topological data analysis to the problem of
characterizing the topological complexity (as defined in the body of the paper)
of a multi-class data set. As a by-product, a topological classifier is defined
that uses an open sub-covering of the data set. This sub-covering can be used
to construct a simplicial complex whose topological features (e.g., Betti
numbers) provide information about the classification problem. We use these
topological constructs to study the impact of topological complexity on
learning in feedforward deep neural networks (DNNs). We hypothesize that
topological complexity is negatively correlated with the ability of a fully
connected feedforward deep neural network to learn to classify data correctly.
We evaluate our topological classification algorithm on multiple constructed
and open source data sets. We also validate our hypothesis regarding the
relationship between topological complexity and learning in DNN&apos;s on multiple
data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffin_C/0/1/0/all/0/1&quot;&gt;Christopher Griffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karn_T/0/1/0/all/0/1&quot;&gt;Trevor Karn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apple_B/0/1/0/all/0/1&quot;&gt;Benjamin Apple&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10260">
<title>Learned Interferometric Imaging for the SPIDER Instrument. (arXiv:2301.10260v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10260</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segmented Planar Imaging Detector for Electro-Optical Reconnaissance
(SPIDER) is an optical interferometric imaging device that aims to offer an
alternative to the large space telescope designs of today with reduced size,
weight and power consumption. This is achieved through interferometric imaging.
State-of-the-art methods for reconstructing images from interferometric
measurements adopt proximal optimization techniques, which are computationally
expensive and require handcrafted priors. In this work we present two
data-driven approaches for reconstructing images from measurements made by the
SPIDER instrument. These approaches use deep learning to learn prior
information from training data, increasing the reconstruction quality, and
significantly reducing the computation time required to recover images by
orders of magnitude. Reconstruction time is reduced to ${\sim} 10$
milliseconds, opening up the possibility of real-time imaging with SPIDER for
the first time. Furthermore, we show that these methods can also be applied in
domains where training data is scarce, such as astronomical imaging, by
leveraging transfer learning from domains where plenty of training data are
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Mars_M/0/1/0/all/0/1&quot;&gt;Matthijs Mars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Betcke_M/0/1/0/all/0/1&quot;&gt;Marta M. Betcke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+McEwen_J/0/1/0/all/0/1&quot;&gt;Jason D. McEwen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11578">
<title>Learning to Unlearn: Instance-wise Unlearning for Pre-trained Classifiers. (arXiv:2301.11578v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11578</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the recent advent of regulations for data protection (e.g., the General
Data Protection Regulation), there has been increasing demand in deleting
information learned from sensitive data in pre-trained models without
retraining from scratch. The inherent vulnerability of neural networks towards
adversarial attacks and unfairness also calls for a robust method to remove or
correct information in an instance-wise fashion, while retaining the predictive
performance across remaining data. To this end, we consider instance-wise
unlearning, of which the goal is to delete information on a set of instances
from a pre-trained model, by either misclassifying each instance away from its
original prediction or relabeling the instance to a different label. We also
propose two methods that reduce forgetting on the remaining data: 1) utilizing
adversarial examples to overcome forgetting at the representation-level and 2)
leveraging weight importance metrics to pinpoint network parameters guilty of
propagating unwanted information. Both methods only require the pre-trained
model and data instances to forget, allowing painless application to real-life
settings where the entire training set is unavailable. Through extensive
experimentation on various image classification benchmarks, we show that our
approach effectively preserves knowledge of remaining data while unlearning
given instances in both single-task and continual unlearning scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1&quot;&gt;Sungmin Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sungjun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1&quot;&gt;Dasol Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1&quot;&gt;Taesup Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Moontae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12755">
<title>Efficient Node Selection in Private Personalized Decentralized Learning. (arXiv:2301.12755v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12755</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized decentralized learning is a promising paradigm for distributed
learning, enabling each node to train a local model on its own data and
collaborate with other nodes to improve without sharing any data. However, this
approach poses significant privacy risks, as nodes may inadvertently disclose
sensitive information about their data or preferences through their
collaboration choices. In this paper, we propose Private Personalized
Decentralized Learning (PPDL), a novel approach that combines secure
aggregation and correlated adversarial multi-armed bandit optimization to
protect node privacy while facilitating efficient node selection. By leveraging
dependencies between different arms, represented by potential collaborators, we
demonstrate that PPDL can effectively identify suitable collaborators solely
based on aggregated models. Additionally, we show that PPDL surpasses previous
non-private methods in model performance on standard benchmarks under label and
covariate shift scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zec_E/0/1/0/all/0/1&quot;&gt;Edvin Listo Zec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostman_J/0/1/0/all/0/1&quot;&gt;Johan &amp;#xd6;stman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mogren_O/0/1/0/all/0/1&quot;&gt;Olof Mogren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillblad_D/0/1/0/all/0/1&quot;&gt;Daniel Gillblad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01463">
<title>Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy. (arXiv:2302.01463v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01463</link>
<description rdf:parseType="Literal">&lt;p&gt;We study gradient descent under linearly correlated noise. Our work is
motivated by recent practical methods for optimization with differential
privacy (DP), such as DP-FTRL, which achieve strong performance in settings
where privacy amplification techniques are infeasible (such as in federated
learning). These methods inject privacy noise through a matrix factorization
mechanism, making the noise linearly correlated over iterations. We propose a
simplified setting that distills key facets of these methods and isolates the
impact of linearly correlated noise. We analyze the behavior of gradient
descent in this setting, for both convex and non-convex functions. Our analysis
is demonstrably tighter than prior work and recovers multiple important special
cases exactly (including anticorrelated perturbed gradient descent). We use our
results to develop new, effective matrix factorizations for differentially
private optimization, and highlight the benefits of these factorizations
theoretically and empirically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koloskova_A/0/1/0/all/0/1&quot;&gt;Anastasia Koloskova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKenna_R/0/1/0/all/0/1&quot;&gt;Ryan McKenna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rush_K/0/1/0/all/0/1&quot;&gt;Keith Rush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMahan_B/0/1/0/all/0/1&quot;&gt;Brendan McMahan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04391">
<title>The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04391</link>
<description rdf:parseType="Literal">&lt;p&gt;In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The dev dataset evaluation
results and human evaluation results verify our idea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tong Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04925">
<title>Information Theoretic Lower Bounds for Information Theoretic Upper Bounds. (arXiv:2302.04925v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04925</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the relationship between the mutual information between the output
model and the empirical sample and the generalization of the algorithm in the
context of stochastic convex optimization. Despite increasing interest in
information-theoretic generalization bounds, it is uncertain if these bounds
can provide insight into the exceptional performance of various learning
algorithms. Our study of stochastic convex optimization reveals that, for true
risk minimization, dimension-dependent mutual information is necessary. This
indicates that existing information-theoretic generalization bounds fall short
in capturing the generalization capabilities of algorithms like SGD and
regularized ERM, which have dimension-independent sample complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1&quot;&gt;Roi Livni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12024">
<title>Comparative Study of Coupling and Autoregressive Flows through Robust Statistical Tests. (arXiv:2302.12024v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12024</link>
<description rdf:parseType="Literal">&lt;p&gt;Normalizing Flows have emerged as a powerful brand of generative models, as
they not only allow for efficient sampling of complicated target distributions,
but also deliver density estimation by construction. We propose here an
in-depth comparison of coupling and autoregressive flows, both of the affine
and rational quadratic spline type, considering four different architectures:
Real-valued Non-Volume Preserving (RealNVP), Masked Autoregressive Flow (MAF),
Coupling Rational Quadratic Spline (C-RQS), and Autoregressive Rational
Quadratic Spline (A-RQS). We focus on a set of multimodal target distributions
of increasing dimensionality ranging from 4 to 400. The performances are
compared by means of different test-statistics for two-sample tests, built from
known distance measures: the sliced Wasserstein distance, the
dimension-averaged one-dimensional Kolmogorov-Smirnov test, and the Frobenius
norm of the difference between correlation matrices. Furthermore, we include
estimations of the variance of both the metrics and the trained models. Our
results indicate that the A-RQS algorithm stands out both in terms of accuracy
and training speed. Nonetheless, all the algorithms are generally able, without
too much fine-tuning, to learn complicated distributions with limited training
data and in a reasonable time, of the order of hours on a Tesla A40 GPU. The
only exception is the C-RQS, which takes significantly longer to train, does
not always provide good accuracy, and becomes unstable for large
dimensionalities. All algorithms have been implemented using TensorFlow2 and
TensorFlow Probability and made available on
\href{https://github.com/NF4HEP/NormalizingFlowsHD}{GitHub}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coccaro_A/0/1/0/all/0/1&quot;&gt;Andrea Coccaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Letizia_M/0/1/0/all/0/1&quot;&gt;Marco Letizia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Reyes_Gonzalez_H/0/1/0/all/0/1&quot;&gt;Humberto Reyes-Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Torre_R/0/1/0/all/0/1&quot;&gt;Riccardo Torre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02561">
<title>CAMEL: Curvature-Augmented Manifold Embedding and Learning. (arXiv:2303.02561v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02561</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel method, named Curvature-Augmented Manifold Embedding and Learning
(CAMEL), is proposed for high dimensional data classification, dimension
reduction, and visualization. CAMEL utilizes a topology metric defined on the
Riemannian manifold, and a unique Riemannian metric for both distance and
curvature to enhance its expressibility. The method also employs a smooth
partition of unity operator on the Riemannian manifold to convert localized
orthogonal projection to global embedding, which captures both the overall
topological structure and local similarity simultaneously. The local orthogonal
vectors provide a physical interpretation of the significant characteristics of
clusters. Therefore, CAMEL not only provides a low-dimensional embedding but
also interprets the physics behind this embedding. CAMEL has been evaluated on
various benchmark datasets and has shown to outperform state-of-the-art
methods, especially for high-dimensional datasets. The method&apos;s distinct
benefits are its high expressibility, interpretability, and scalability. The
paper provides a detailed discussion on Riemannian distance and curvature
metrics, physical interpretability, hyperparameter effect, manifold stability,
and computational efficiency for a holistic understanding of CAMEL. Finally,
the paper presents the limitations and future work of CAMEL along with key
conclusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03374">
<title>To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning. (arXiv:2303.03374v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03374</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning and ensembling are two popular techniques for improving the
performance and robustness of neural networks. Due to the high cost of
pre-training, ensembles of models fine-tuned from a single pre-trained
checkpoint are often used in practice. Such models end up in the same basin of
the loss landscape, which we call the pre-train basin, and thus have limited
diversity. In this work, we show that ensembles trained from a single
pre-trained checkpoint may be improved by better exploring the pre-train basin,
however, leaving the basin results in losing the benefits of transfer learning
and in degradation of the ensemble quality. Based on the analysis of existing
exploration methods, we propose a more effective modification of the Snapshot
Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger
ensembles and uniform model soups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadrtdinov_I/0/1/0/all/0/1&quot;&gt;Ildus Sadrtdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pozdeev_D/0/1/0/all/0/1&quot;&gt;Dmitrii Pozdeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1&quot;&gt;Ekaterina Lobacheva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11593">
<title>Difficulty in chirality recognition for Transformer architectures learning chemical structures from string. (arXiv:2303.11593v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11593</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen rapid development of descriptor generation based on
representation learning of extremely diverse molecules, especially those that
apply natural language processing (NLP) models to SMILES, a literal
representation of molecular structure. However, little research has been done
on how these models understand chemical structure. To address this black box,
we investigated the relationship between the learning progress of SMILES and
chemical structure using a representative NLP model, the Transformer. We show
that while the Transformer learns partial structures of molecules quickly, it
requires extended training to understand overall structures. Consistently, the
accuracy of molecular property predictions using descriptors generated from
models at different learning steps was similar from the beginning to the end of
training. Furthermore, we found that the Transformer requires particularly long
training to learn chirality and sometimes stagnates with low performance due to
misunderstanding of enantiomers. These findings are expected to deepen the
understanding of NLP models in chemistry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikai_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Yoshikai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mizuno_T/0/1/0/all/0/1&quot;&gt;Tadahaya Mizuno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nemoto_S/0/1/0/all/0/1&quot;&gt;Shumpei Nemoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusuhara_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Kusuhara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13506">
<title>The Quantization Model of Neural Scaling. (arXiv:2303.13506v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13506</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Quantization Model of neural scaling laws, explaining both the
observed power law dropoff of loss with model and data size, and also the
sudden emergence of new capabilities with scale. We derive this model from what
we call the Quantization Hypothesis, where network knowledge and skills are
&quot;quantized&quot; into discrete chunks ($\textbf{quanta}$). We show that when quanta
are learned in order of decreasing use frequency, then a power law in use
frequencies explains observed power law scaling of loss. We validate this
prediction on toy datasets, then study how scaling curves decompose for large
language models. Using language model gradients, we automatically decompose
model behavior into a diverse set of skills (quanta). We tentatively find that
the frequency at which these quanta are used in the training distribution
roughly follows a power law corresponding with the empirical scaling exponent
for language models, a prediction of our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaud_E/0/1/0/all/0/1&quot;&gt;Eric J. Michaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girit_U/0/1/0/all/0/1&quot;&gt;Uzay Girit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1&quot;&gt;Max Tegmark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14226">
<title>Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions. (arXiv:2303.14226v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14226</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider a setting where there are $N$ heterogeneous units and $p$
interventions. Our goal is to learn unit-specific potential outcomes for any
combination of these $p$ interventions, i.e., $N \times 2^p$ causal parameters.
Choosing a combination of interventions is a problem that naturally arises in a
variety of applications such as factorial design experiments, recommendation
engines, combination therapies in medicine, conjoint analysis, etc. Running $N
\times 2^p$ experiments to estimate the various parameters is likely expensive
and/or infeasible as $N$ and $p$ grow. Further, with observational data there
is likely confounding, i.e., whether or not a unit is seen under a combination
is correlated with its potential outcome under that combination. To address
these challenges, we propose a novel latent factor model that imposes structure
across units (i.e., the matrix of potential outcomes is approximately rank
$r$), and combinations of interventions (i.e., the coefficients in the Fourier
expansion of the potential outcomes is approximately $s$ sparse). We establish
identification for all $N \times 2^p$ parameters despite unobserved
confounding. We propose an estimation procedure, Synthetic Combinations, and
establish it is finite-sample consistent and asymptotically normal under
precise conditions on the observation pattern. Our results imply consistent
estimation given $\text{poly}(r) \times \left( N + s^2p\right)$ observations,
while previous methods have sample complexity scaling as $\min(N \times s^2p, \
\ \text{poly(r)} \times (N + 2^p))$. We use Synthetic Combinations to propose a
data-efficient experimental design. Empirically, Synthetic Combinations
outperforms competing approaches on a real-world dataset on movie
recommendations. Lastly, we extend our analysis to do causal inference where
the intervention is a permutation over $p$ items (e.g., rankings).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Abhineet Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Anish Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vijaykumar_S/0/1/0/all/0/1&quot;&gt;Suhas Vijaykumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14822">
<title>MGTBench: Benchmarking Machine-Generated Text Detection. (arXiv:2303.14822v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14822</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, powerful large language models (LLMs) such as ChatGPT have
demonstrated revolutionary power in a variety of tasks. Consequently, the
detection of machine-generated texts (MGTs) is becoming increasingly crucial as
LLMs become more advanced and prevalent. These models have the ability to
generate human-like language, making it challenging to discern whether a text
is authored by a human or a machine. This raises concerns regarding
authenticity, accountability, and potential bias. However, existing methods for
detecting MGTs are evaluated using different model architectures, datasets, and
experimental settings, resulting in a lack of a comprehensive evaluation
framework that encompasses various methodologies. Furthermore, it remains
unclear how existing detection methods would perform against powerful LLMs. In
this paper, we fill this gap by proposing the first benchmark framework for MGT
detection against powerful LLMs, named MGTBench. Extensive evaluations on
public datasets with curated texts generated by various powerful LLMs such as
ChatGPT-turbo and Claude demonstrate the effectiveness of different detection
methods. Our ablation study shows that a larger number of words in general
leads to better performance and most detection methods can achieve similar
performance with much fewer training samples. Moreover, we delve into a more
challenging task: text attribution. Our findings indicate that the model-based
detection methods still perform well in the text attribution task. To
investigate the robustness of different detection methods, we consider three
adversarial attacks, namely paraphrasing, random spacing, and adversarial
perturbations. We discover that these attacks can significantly diminish
detection effectiveness, underscoring the critical need for the development of
more robust detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xinlei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xinyue Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1&quot;&gt;Michael Backes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17503">
<title>Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning. (arXiv:2303.17503v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17503</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Pgx, a suite of board game reinforcement learning (RL)
environments written in JAX and optimized for GPU/TPU accelerators. By
leveraging JAX&apos;s auto-vectorization and parallelization over accelerators, Pgx
can efficiently scale to thousands of simultaneous simulations over
accelerators. In our experiments on a DGX-A100 workstation, we discovered that
Pgx can simulate RL environments 10-100x faster than existing implementations
available in Python. Pgx includes RL environments commonly used as benchmarks
in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx
offers miniature game sets and baseline models to facilitate rapid research
cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm
with Pgx environments. Overall, Pgx provides high-performance environment
simulators for researchers to accelerate their RL experiments. Pgx is available
at &lt;a href=&quot;http://github.com/sotetsuk/pgx.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyamada_S/0/1/0/all/0/1&quot;&gt;Sotetsu Koyamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okano_S/0/1/0/all/0/1&quot;&gt;Shinri Okano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimori_S/0/1/0/all/0/1&quot;&gt;Soichiro Nishimori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murata_Y/0/1/0/all/0/1&quot;&gt;Yu Murata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habara_K/0/1/0/all/0/1&quot;&gt;Keigo Habara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kita_H/0/1/0/all/0/1&quot;&gt;Haruka Kita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishii_S/0/1/0/all/0/1&quot;&gt;Shin Ishii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04162">
<title>Design of Two-Level Incentive Mechanisms for Hierarchical Federated Learning. (arXiv:2304.04162v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04162</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical Federated Learning (HFL) is a distributed machine learning
paradigm tailored for multi-tiered computation architectures, which supports
massive access of devices&apos; models simultaneously. To enable efficient HFL, it
is crucial to design suitable incentive mechanisms to ensure that devices
actively participate in local training. However, there are few studies on
incentive mechanism design for HFL. In this paper, we design two-level
incentive mechanisms for the HFL with a two-tiered computing structure to
encourage the participation of entities in each tier in the HFL training. In
the lower-level game, we propose a coalition formation game to joint optimize
the edge association and bandwidth allocation problem, and obtain efficient
coalition partitions by the proposed preference rule, which can be proven to be
stable by exact potential game. In the upper-level game, we design the
Stackelberg game algorithm, which not only determines the optimal number of
edge aggregations for edge servers to maximize their utility, but also optimize
the unit reward provided for the edge aggregation performance to ensure the
interests of cloud servers. Furthermore, numerical results indicate that the
proposed algorithms can achieve better performance than the benchmark schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_S/0/1/0/all/0/1&quot;&gt;Shunfeng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuwen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kunlun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1&quot;&gt;Feng Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05492">
<title>Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (arXiv:2304.05492v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05492</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential recommendation models, models that learn from chronological
user-item interactions, outperform traditional recommendation models in many
settings. Despite the success of sequential recommendation models, their
robustness has recently come into question. Two properties unique to the nature
of sequential recommendation models may impair their robustness - the cascade
effects induced during training and the model&apos;s tendency to rely too heavily on
temporal information. To address these vulnerabilities, we propose
Cascade-guided Adversarial training, a new adversarial training procedure that
is specifically designed for sequential recommendation models. Our approach
harnesses the intrinsic cascade effects present in sequential modeling to
produce strategic adversarial perturbations to item embeddings during training.
Experiments on training state-of-the-art sequential models on four public
datasets from different domains show that our training approach produces
superior model ranking accuracy and superior model robustness to real item
replacement perturbations when compared to both standard model training and
generic adversarial training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1&quot;&gt;Shelby Heinecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09221">
<title>Convergence of stochastic gradient descent under a local Lojasiewicz condition for deep neural networks. (arXiv:2304.09221v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09221</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the convergence of stochastic gradient descent (SGD) for non-convex
objective functions. We establish the local convergence with positive
probability under the local \L{}ojasiewicz condition introduced by Chatterjee
in \cite{chatterjee2022convergence} and an additional local structural
assumption of the loss function landscape. A key component of our proof is to
ensure that the whole trajectories of SGD stay inside the local region with a
positive probability. We also provide examples of neural networks with finite
widths such that our assumptions hold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1&quot;&gt;Jing An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jianfeng Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06435">
<title>Phase transitions in the mini-batch size for sparse and dense two-layer neural networks. (arXiv:2305.06435v3 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06435</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of mini-batches of data in training artificial neural networks is
nowadays very common. Despite its broad usage, theories explaining
quantitatively how large or small the optimal mini-batch size should be are
missing. This work presents a systematic attempt at understanding the role of
the mini-batch size in training two-layer neural networks. Working in the
teacher-student scenario, with a sparse teacher, and focusing on tasks of
different complexity, we quantify the effects of changing the mini-batch size
$m$. We find that often the generalization performances of the student strongly
depend on $m$ and may undergo sharp phase transitions at a critical value
$m_c$, such that for $m&amp;lt;m_c$ the training process fails, while for $m&amp;gt;m_c$ the
student learns perfectly or generalizes very well the teacher. Phase
transitions are induced by collective phenomena firstly discovered in
statistical mechanics and later observed in many fields of science. Observing a
phase transition by varying the mini-batch size across different architectures
raises several questions about the role of this hyperparameter in the neural
network learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Marino_R/0/1/0/all/0/1&quot;&gt;Raffaele Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ricci_Tersenghi_F/0/1/0/all/0/1&quot;&gt;Federico Ricci-Tersenghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09373">
<title>Multi-task convolutional neural network for image aesthetic assessment. (arXiv:2305.09373v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09373</link>
<description rdf:parseType="Literal">&lt;p&gt;As people&apos;s aesthetic preferences for images are far from understood, image
aesthetic assessment is a challenging artificial intelligence task. The range
of factors underlying this task is almost unlimited, but we know that some
aesthetic attributes affect those preferences. In this study, we present a
multi-task convolutional neural network that takes into account these
attributes. The proposed neural network jointly learns the attributes along
with the overall aesthetic scores of images. This multi-task learning framework
allows for effective generalization through the utilization of shared
representations. Our experiments demonstrate that the proposed method
outperforms the state-of-the-art approaches in predicting overall aesthetic
scores for images in one benchmark of image aesthetics. We achieve near-human
performance in terms of overall aesthetic scores when considering the
Spearman&apos;s rank correlations. Moreover, our model pioneers the application of
multi-tasking in another benchmark, serving as a new baseline for future
research. Notably, our approach achieves this performance while using fewer
parameters compared to existing multi-task neural networks in the literature,
and consequently makes our method more efficient in terms of computational
complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soydaner_D/0/1/0/all/0/1&quot;&gt;Derya Soydaner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagemans_J/0/1/0/all/0/1&quot;&gt;Johan Wagemans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10818">
<title>Diffusion Language Models Generation Can Be Halted Early. (arXiv:2305.10818v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10818</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Language models (DLMs) are a promising avenue for text generation
due to their practical properties on tractable controllable generation. They
also have the advantage of not having to predict text autoregressively.
However, despite these notable features, DLMs have not yet reached the
performance levels of their Autoregressive counterparts. One of the ways to
reduce the performance gap between these two types of language models is to
speed up the generation of DLMs. Therefore, we propose a pioneering methodology
to address this issue in this work. It enables the execution of more generation
steps within a given time frame, potentially leading to higher-quality outputs.
Specifically, our methods estimate DLMs completeness of text generation and
allow adaptive halting of the generation process. We test and refine our
methods on Plaid, SSD, and CDCD DLMs and create a cohesive perspective on their
generation workflows. Finally, we confirm that our methods allow halting Plaid,
SSD, and CDCD models and decrease the generation time by $10$-$40$% without a
drop in the quality of model samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaina_S/0/1/0/all/0/1&quot;&gt;Sofia Maria Lo Cicero Vaina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1&quot;&gt;Nikita Balagansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1&quot;&gt;Daniil Gavrilov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11417">
<title>Complexity of Deep Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11417</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the complexity of feed-forward neural networks
by examining the concept of functional equivalence, which suggests that
different network parameterizations can lead to the same function. We utilize
the permutation invariance property to derive a novel covering number bound for
the class of feedforward neural networks, which reveals that the complexity of
a neural network can be reduced by exploiting this property. We discuss the
extensions to convolutional neural networks, residual networks, and
attention-based models. We demonstrate that functional equivalence benefits
optimization, as overparameterized networks tend to be easier to train since
increasing network width leads to a diminishing volume of the effective
parameter space. Our findings offer new insights into overparameterization and
have significant implications for understanding generalization and optimization
in deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guohao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11554">
<title>ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11554</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1&quot;&gt;Shibo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14062">
<title>Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v4 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14062</link>
<description rdf:parseType="Literal">&lt;p&gt;Photoplethysmography (PPG) refers to the measurement of variations in blood
volume using light and is a feature of most wearable devices. The PPG signals
provide insight into the body&apos;s circulatory system and can be employed to
extract various bio-features, such as heart rate and vascular ageing. Although
several algorithms have been proposed for this purpose, many exhibit
limitations, including heavy reliance on human calibration, high signal quality
requirements, and a lack of generalisation. In this paper, we introduce a PPG
signal processing framework that integrates graph theory and computer vision
algorithms, to provide an analysis framework which is amplitude-independent and
invariant to affine transformations. It also requires minimal preprocessing,
fuses information through RGB channels and exhibits robust generalisation
across tasks and datasets. The proposed VGTL-net achieves state-of-the-art
performance in the prediction of vascular ageing and demonstrates robust
estimation of continuous blood pressure waveforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miao_Y/0/1/0/all/0/1&quot;&gt;Yuyang Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Davies_H/0/1/0/all/0/1&quot;&gt;Harry J. Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandic_D/0/1/0/all/0/1&quot;&gt;Danilo P. Mandic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15944">
<title>How to Turn Your Knowledge Graph Embeddings into Generative Models. (arXiv:2305.15944v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15944</link>
<description rdf:parseType="Literal">&lt;p&gt;Some of the most successful knowledge graph embedding (KGE) models for link
prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based
models. Under this perspective they are not amenable for exact
maximum-likelihood estimation (MLE), sampling and struggle to integrate logical
constraints. This work re-interprets the score functions of these KGEs as
circuits -- constrained computational graphs allowing efficient
marginalisation. Then, we design two recipes to obtain efficient generative
circuit models by either restricting their activations to be non-negative or
squaring their outputs. Our interpretation comes with little or no loss of
performance for link prediction, while the circuits framework unlocks exact
learning by MLE, efficient sampling of new triples, and guarantee that logical
constraints are satisfied by design. Furthermore, our models scale more
gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loconte_L/0/1/0/all/0/1&quot;&gt;Lorenzo Loconte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauro_N/0/1/0/all/0/1&quot;&gt;Nicola Di Mauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peharz_R/0/1/0/all/0/1&quot;&gt;Robert Peharz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1&quot;&gt;Antonio Vergari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16215">
<title>Koopman Kernel Regression. (arXiv:2305.16215v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16215</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning approaches for decision making, such as reinforcement
learning, rely on simulators or predictive models to forecast the
time-evolution of quantities of interest, e.g., the state of an agent or the
reward of a policy. Forecasts of such complex phenomena are commonly described
by highly nonlinear dynamical systems, making their use in optimization-based
decision-making challenging. Koopman operator theory offers a beneficial
paradigm for addressing this problem by characterizing forecasts via linear
time-invariant (LTI) ODEs, turning multi-step forecasts into sparse matrix
multiplication. Though there exists a variety of learning approaches, they
usually lack crucial learning-theoretic guarantees, making the behavior of the
obtained models with increasing data and dimensionality unclear. We address the
aforementioned by deriving a universal Koopman-invariant reproducing kernel
Hilbert space (RKHS) that solely spans transformations into LTI dynamical
systems. The resulting Koopman Kernel Regression (KKR) framework enables the
use of statistical learning tools from function approximation for novel
convergence results and generalization error bounds under weaker assumptions
than existing work. Our experiments demonstrate superior forecasting
performance compared to Koopman operator and sequential data predictors in
RKHS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bevanda_P/0/1/0/all/0/1&quot;&gt;Petar Bevanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beier_M/0/1/0/all/0/1&quot;&gt;Max Beier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederer_A/0/1/0/all/0/1&quot;&gt;Armin Lederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sosnowski_S/0/1/0/all/0/1&quot;&gt;Stefan Sosnowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1&quot;&gt;Sandra Hirche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16284">
<title>DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16284</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new easy-to-implement parameter-free gradient-based
optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is
efficient -- matching the convergence rate of optimally tuned gradient descent
in convex optimization up to a logarithmic factor without tuning any
parameters, and universal -- automatically adapting to both smooth and
nonsmooth problems. While popular algorithms following the AdaGrad framework
compute a running average of the squared gradients to use for normalization,
DoWG maintains a new distance-based weighted version of the running average,
which is crucial to achieve the desired properties. To complement our theory,
we also show empirically that DoWG trains at the edge of stability, and
validate its effectiveness on practical machine learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaled_A/0/1/0/all/0/1&quot;&gt;Ahmed Khaled&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1&quot;&gt;Konstantin Mishchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16501">
<title>Strategic Classification under Unknown Personalized Manipulation. (arXiv:2305.16501v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16501</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the fundamental mistake bound and sample complexity in the strategic
classification, where agents can strategically manipulate their feature vector
up to an extent in order to be predicted as positive. For example, given a
classifier determining college admission, student candidates may try to take
easier classes to improve their GPA, retake SAT and change schools in an effort
to fool the classifier. Ball manipulations are a widely studied class of
manipulations in the literature, where agents can modify their feature vector
within a bounded radius ball. Unlike most prior work, our work considers
manipulations to be personalized, meaning that agents can have different levels
of manipulation abilities (e.g., varying radii for ball manipulations), and
unknown to the learner.
&lt;/p&gt;
&lt;p&gt;We formalize the learning problem in an interaction model where the learner
first deploys a classifier and the agent manipulates the feature vector within
their manipulation set to game the deployed classifier. We investigate various
scenarios in terms of the information available to the learner during the
interaction, such as observing the original feature vector before or after
deployment, observing the manipulated feature vector, or not seeing either the
original or the manipulated feature vector. We begin by providing online
mistake bounds and PAC sample complexity in these scenarios for ball
manipulations. We also explore non-ball manipulations and show that, even in
the simplest scenario where both the original and the manipulated feature
vectors are revealed, the mistake bounds and sample complexity are lower
bounded by $\Omega(|H|)$ when the target function belongs to a known class $H$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Han Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1&quot;&gt;Avrim Blum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montasser_O/0/1/0/all/0/1&quot;&gt;Omar Montasser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17402">
<title>Learning and Collusion in Multi-unit Auctions. (arXiv:2305.17402v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17402</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider repeated multi-unit auctions with uniform pricing, which are
widely used in practice for allocating goods such as carbon licenses. In each
round, $K$ identical units of a good are sold to a group of buyers that have
valuations with diminishing marginal returns. The buyers submit bids for the
units, and then a price $p$ is set per unit so that all the units are sold. We
consider two variants of the auction, where the price is set to the $K$-th
highest bid and $(K+1)$-st highest bid, respectively.
&lt;/p&gt;
&lt;p&gt;We analyze the properties of this auction in both the offline and online
settings. In the offline setting, we consider the problem that one player $i$
is facing: given access to a data set that contains the bids submitted by
competitors in past auctions, find a bid vector that maximizes player $i$&apos;s
cumulative utility on the data set. We design a polynomial time algorithm for
this problem, by showing it is equivalent to finding a maximum-weight path on a
carefully constructed directed acyclic graph.
&lt;/p&gt;
&lt;p&gt;In the online setting, the players run learning algorithms to update their
bids as they participate in the auction over time. Based on our offline
algorithm, we design efficient online learning algorithms for bidding. The
algorithms have sublinear regret, under both full information and bandit
feedback structures. We complement our online learning algorithms with regret
lower bounds.
&lt;/p&gt;
&lt;p&gt;Finally, we analyze the quality of the equilibria in the worst case through
the lens of the core solution concept in the game among the bidders. We show
that the $(K+1)$-st price format is susceptible to collusion among the bidders;
meanwhile, the $K$-th price format does not have this issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branzei_S/0/1/0/all/0/1&quot;&gt;Simina Br&amp;#xe2;nzei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derakhshan_M/0/1/0/all/0/1&quot;&gt;Mahsa Derakhshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yanjun Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17547">
<title>Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17547</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Translatotron 3, a novel approach to unsupervised direct
speech-to-speech translation from monolingual speech-text datasets by combining
masked autoencoder, unsupervised embedding mapping, and back-translation.
Experimental results in speech-to-speech translation tasks between Spanish and
English show that Translatotron 3 outperforms a baseline cascade system,
reporting $18.14$ BLEU points improvement on the synthesized
Unpaired-Conversational dataset. In contrast to supervised approaches that
necessitate real paired data, or specialized modeling to replicate
para-/non-linguistic information such as pauses, speaking rates, and speaker
identity, Translatotron 3 showcases its capability to retain it. Audio samples
can be found at &lt;a href=&quot;http://google-research.github.io/lingvo-lab/translatotron3&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1&quot;&gt;Eliya Nachmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levkovitch_A/0/1/0/all/0/1&quot;&gt;Alon Levkovitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yifan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asawaroengchai_C/0/1/0/all/0/1&quot;&gt;Chulayuth Asawaroengchai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1&quot;&gt;Heiga Zen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1&quot;&gt;Michelle Tadmor Ramanovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18342">
<title>Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18342</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative neural models hold great promise in enhancing programming
education by synthesizing new content. We seek to design neural models that can
automatically generate programming tasks for a given specification in the
context of visual programming domains. Despite the recent successes of large
generative models like GPT-4, our initial results show that these models are
ineffective in synthesizing visual programming tasks and struggle with logical
and spatial reasoning. We propose a novel neuro-symbolic technique,
NeurTaskSyn, that can synthesize programming tasks for a specification given in
the form of desired programming concepts exercised by its solution code and
constraints on the visual task. NeurTaskSyn has two components: the first
component is trained via imitation learning procedure to generate possible
solution codes, and the second component is trained via reinforcement learning
procedure to guide an underlying symbolic execution engine that generates
visual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn
through an extensive empirical evaluation and a qualitative study on reference
tasks taken from the Hour of Code: Classic Maze challenge by Code-dot-org and
the Intro to Programming with Karel course by CodeHS-dot-com.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1&quot;&gt;Victor-Alexandru P&amp;#x103;durean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzannetos_G/0/1/0/all/0/1&quot;&gt;Georgios Tzannetos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1&quot;&gt;Adish Singla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18455">
<title>Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18455</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the ease of training, ability to scale, and high sample quality,
diffusion models (DMs) have become the preferred option for generative
modeling, with numerous pre-trained models available for a wide variety of
datasets. Containing intricate information about data distributions,
pre-trained DMs are valuable assets for downstream applications. In this work,
we consider learning from pre-trained DMs and transferring their knowledge to
other generative models in a data-free fashion. Specifically, we propose a
general framework called Diff-Instruct to instruct the training of arbitrary
generative models as long as the generated samples are differentiable with
respect to the model parameters. Our proposed Diff-Instruct is built on a
rigorous mathematical foundation where the instruction process directly
corresponds to minimizing a novel divergence we call Integral Kullback-Leibler
(IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL
divergence along a diffusion process, which we show to be more robust in
comparing distributions with misaligned supports. We also reveal non-trivial
connections of our method to existing works such as DreamFusion, and generative
adversarial training. To demonstrate the effectiveness and universality of
Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion
models and refining existing GAN models. The experiments on distilling
pre-trained diffusion models show that Diff-Instruct results in
state-of-the-art single-step diffusion-based models. The experiments on
refining GAN models show that the Diff-Instruct can consistently improve the
pre-trained generators of GAN models across various settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weijian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tianyang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shifeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19706">
<title>Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming. (arXiv:2305.19706v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19706</link>
<description rdf:parseType="Literal">&lt;p&gt;Global optimization of decision trees has shown to be promising in terms of
accuracy, size, and consequently human comprehensibility. However, many of the
methods used rely on general-purpose solvers for which scalability remains an
issue. Dynamic programming methods have been shown to scale much better because
they exploit the tree structure by solving subtrees as independent subproblems.
However, this only works when an objective can be optimized separately for
subtrees. We explore this relationship in detail and show the necessary and
sufficient conditions for such separability and generalize previous dynamic
programming approaches into a framework that can optimize any combination of
separable objectives and constraints. Experiments on five application domains
show the general applicability of this framework, while outperforming the
scalability of general-purpose solvers by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linden_J/0/1/0/all/0/1&quot;&gt;Jacobus G. M. van der Linden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weerdt_M/0/1/0/all/0/1&quot;&gt;Mathijs M. de Weerdt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirovic_E/0/1/0/all/0/1&quot;&gt;Emir Demirovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.20004">
<title>Learning to solve Bayesian inverse problems: An amortized variational inference approach using Gaussian and Flow guides. (arXiv:2305.20004v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.20004</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems, i.e., estimating parameters of physical models from
experimental data, are ubiquitous in science and engineering. The Bayesian
formulation is the gold standard because it alleviates ill-posedness issues and
quantifies epistemic uncertainty. Since analytical posteriors are not typically
available, one resorts to Markov chain Monte Carlo sampling or approximate
variational inference. However, inference needs to be rerun from scratch for
each new set of data. This drawback limits the applicability of the Bayesian
formulation to real-time settings, e.g., health monitoring of engineered
systems, and medical diagnosis. The objective of this paper is to develop a
methodology that enables real-time inference by learning the Bayesian inverse
map, i.e., the map from data to posteriors. Our approach is as follows. We
parameterize the posterior distribution as a function of data. This work
outlines two distinct approaches to do this. The first method involves
parameterizing the posterior using an amortized full-rank Gaussian guide,
implemented through neural networks. The second method utilizes a Conditional
Normalizing Flow guide, employing conditional invertible neural networks for
cases where the target posterior is arbitrarily complex. In both approaches, we
learn the network parameters by amortized variational inference which involves
maximizing the expectation of evidence lower bound over all possible datasets
compatible with the model. We demonstrate our approach by solving a set of
benchmark problems from science and engineering. Our results show that the
posterior estimates of our approach are in agreement with the corresponding
ground truth obtained by Markov chain Monte Carlo. Once trained, our approach
provides the posterior distribution for a given observation just at the cost of
a forward pass of the neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karumuri_S/0/1/0/all/0/1&quot;&gt;Sharmila Karumuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bilionis_I/0/1/0/all/0/1&quot;&gt;Ilias Bilionis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00196">
<title>Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00196</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the infinite-horizon restless bandit problem with the average reward
criterion, in both discrete-time and continuous-time settings. A fundamental
goal is to efficiently compute policies that achieve a diminishing optimality
gap as the number of arms, $N$, grows large. Existing results on asymptotic
optimality all rely on the uniform global attractor property (UGAP), a complex
and challenging-to-verify assumption. In this paper, we propose a general,
simulation-based framework, Follow-the-Virtual-Advice, that converts any
single-armed policy into a policy for the original $N$-armed problem. This is
done by simulating the single-armed policy on each arm and carefully steering
the real state towards the simulated state. Our framework can be instantiated
to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the
discrete-time setting, our result holds under a simpler synchronization
assumption, which covers some problem instances that violate UGAP. More
notably, in the continuous-time setting, we do not require \emph{any}
additional assumptions beyond the standard unichain condition. In both
settings, our work is the first asymptotic optimality result that does not
require UGAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yige Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qiaomin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weina Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01222">
<title>Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01222</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose UnMixMatch, a semi-supervised learning framework which can learn
effective representations from unconstrained unlabelled data in order to scale
up performance. Most existing semi-supervised methods rely on the assumption
that labelled and unlabelled samples are drawn from the same distribution,
which limits the potential for improvement through the use of free-living
unlabeled data. Consequently, the generalizability and scalability of
semi-supervised learning are often hindered by this assumption. Our method aims
to overcome these constraints and effectively utilize unconstrained unlabelled
data in semi-supervised learning. UnMixMatch consists of three main components:
a supervised learner with hard augmentations that provides strong
regularization, a contrastive consistency regularizer to learn underlying
representations from the unlabelled data, and a self-supervised loss to enhance
the representations that are learnt from the unlabelled data. We perform
extensive experiments on 4 commonly used datasets and demonstrate superior
performance over existing semi-supervised methods with a performance boost of
4.79%. Extensive ablation and sensitivity studies show the effectiveness and
impact of each of the proposed components of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Shuvendu Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1&quot;&gt;Ali Etemad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03072">
<title>Explore to Generalize in Zero-Shot RL. (arXiv:2306.03072v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03072</link>
<description rdf:parseType="Literal">&lt;p&gt;We study zero-shot generalization in reinforcement learning-optimizing a
policy on a set of training tasks to perform well on a similar but unseen test
task. To mitigate overfitting, previous work explored different notions of
invariance to the task. However, on problems such as the ProcGen Maze, an
adequate solution that is invariant to the task visualization does not exist,
and therefore invariance-based approaches fail. Our insight is that learning a
policy that effectively $\textit{explores}$ the domain is harder to memorize
than a policy that maximizes reward for a specific task, and therefore we
expect such learned behavior to generalize well; we indeed demonstrate this
empirically on several domains that are difficult for invariance-based
approaches. Our $\textit{Explore to Generalize}$ algorithm (ExpGen) builds on
this insight: we train an additional ensemble of agents that optimize reward.
At test time, either the ensemble agrees on an action, and we generalize well,
or we take exploratory actions, which generalize well and drive us to a novel
part of the state space, where the ensemble may potentially agree again. We
show that our approach is the state-of-the-art on tasks of the ProcGen
challenge that have thus far eluded effective generalization, yielding a
success rate of $83\%$ on the Maze task and $74\%$ on Heist with $200$ training
levels. ExpGen can also be combined with an invariance based approach to gain
the best of both worlds, setting new state-of-the-art results on ProcGen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisselman_E/0/1/0/all/0/1&quot;&gt;Ev Zisselman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavie_I/0/1/0/all/0/1&quot;&gt;Itai Lavie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1&quot;&gt;Aviv Tamar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03266">
<title>Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03266</link>
<description rdf:parseType="Literal">&lt;p&gt;Message passing neural networks (MPNNs) have emerged as the most popular
framework of graph neural networks (GNNs) in recent years. However, their
expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test.
Some works are inspired by $k$-WL/FWL (Folklore WL) and design the
corresponding neural versions. Despite the high expressive power, there are
serious limitations in this line of research. In particular, (1) $k$-WL/FWL
requires at least $O(n^k)$ space complexity, which is impractical for large
graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the
only adjustable hyper-parameter being $k$. To tackle the first limitation, we
propose an extension, $(k,t)$-FWL. We theoretically prove that even if we fix
the space complexity to $O(n^k)$ (for any $k\geq 2$) in $(k,t)$-FWL, we can
construct an expressiveness hierarchy up to solving the graph isomorphism
problem. To tackle the second problem, we propose $k$-FWL+, which considers any
equivariant set as neighbors instead of all nodes, thereby greatly expanding
the design space of $k$-FWL. Combining these two modifications results in a
flexible and powerful framework $(k,t)$-FWL+. We demonstrate $(k,t)$-FWL+ can
implement most existing models with matching expressiveness. We then introduce
an instance of $(k,t)$-FWL+ called Neighborhood$^2$-FWL (N$^2$-FWL), which is
practically and theoretically sound. We prove that N$^2$-FWL is no less
powerful than 3-WL, and can encode many substructures while only requiring
$O(n^2)$ space. Finally, we design its neural version named N$^2$-GNN and
evaluate its performance on various tasks. N$^2$-GNN achieves record-breaking
results on ZINC-Subset (0.059), outperforming previous SOTA results by 10.6%.
Moreover, N$^2$-GNN achieves new SOTA results on the BREC dataset (71.8%) among
all existing high-expressive GNN methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiarui Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lecheng Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fuhai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04746">
<title>Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models. (arXiv:2306.04746v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04746</link>
<description rdf:parseType="Literal">&lt;p&gt;In computational social science (CSS), researchers analyze documents to
explain social and political phenomena. In most scenarios, CSS researchers
first obtain labels for documents and then explain labels using interpretable
regression analyses in the second step. One increasingly common way to annotate
documents cheaply at scale is through large language models (LLMs). However,
like other scalable ways of producing annotations, such surrogate labels are
often imperfect and biased. We present a new algorithm for using imperfect
annotation surrogates for downstream statistical analyses while guaranteeing
statistical properties -- like asymptotic unbiasedness and proper uncertainty
quantification -- which are fundamental to CSS research. We show that direct
use of surrogate labels in downstream statistical analyses leads to substantial
bias and invalid confidence intervals, even with high surrogate accuracy of
80-90%. To address this, we build on debiased machine learning to propose the
design-based supervised learning (DSL) estimator. DSL employs a doubly-robust
procedure to combine surrogate labels with a smaller number of high-quality,
gold-standard labels. Our approach guarantees valid inference for downstream
statistical analyses, even when surrogates are arbitrarily biased and without
requiring stringent assumptions, by controlling the probability of sampling
documents for gold-standard labeling. Both our theoretical analysis and
experimental results show that DSL provides valid statistical inference while
achieving root mean squared errors comparable to existing alternatives that
focus only on prediction without inferential guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Egami_N/0/1/0/all/0/1&quot;&gt;Naoki Egami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hinck_M/0/1/0/all/0/1&quot;&gt;Musashi Hinck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stewart_B/0/1/0/all/0/1&quot;&gt;Brandon M. Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hanying Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05323">
<title>Advancing Italian Biomedical Information Extraction with Transformers-based Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05323</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of computerized medical records in hospitals has reduced
burdensome activities like manual writing and information fetching. However,
the data contained in medical records are still far underutilized, primarily
because extracting data from unstructured textual medical records takes time
and effort. Information Extraction, a subfield of Natural Language Processing,
can help clinical practitioners overcome this limitation by using automated
text-mining pipelines. In this work, we created the first Italian
neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to
develop a Transformers-based model. Moreover, we collected and leveraged three
external independent datasets to implement an effective multicenter model, with
overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned
are: (i) the crucial role of a consistent annotation process and (ii) a
fine-tuning strategy that combines classical methods with a &quot;low-resource&quot;
approach. This allowed us to establish methodological guidelines that pave the
way for Natural Language Processing studies in less-resourced languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crema_C/0/1/0/all/0/1&quot;&gt;Claudio Crema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buonocore_T/0/1/0/all/0/1&quot;&gt;Tommaso Mario Buonocore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fostinelli_S/0/1/0/all/0/1&quot;&gt;Silvia Fostinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parimbelli_E/0/1/0/all/0/1&quot;&gt;Enea Parimbelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verde_F/0/1/0/all/0/1&quot;&gt;Federico Verde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fundaro_C/0/1/0/all/0/1&quot;&gt;Cira Fundar&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manera_M/0/1/0/all/0/1&quot;&gt;Marina Manera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramusino_M/0/1/0/all/0/1&quot;&gt;Matteo Cotta Ramusino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capelli_M/0/1/0/all/0/1&quot;&gt;Marco Capelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1&quot;&gt;Alfredo Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binetti_G/0/1/0/all/0/1&quot;&gt;Giuliano Binetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellazzi_R/0/1/0/all/0/1&quot;&gt;Riccardo Bellazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redolfi_A/0/1/0/all/0/1&quot;&gt;Alberto Redolfi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06446">
<title>ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06446</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have shown impressive performance and have become
a unified backbone for multiple vision tasks. However, both the attention
mechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently
efficient due to dense multiplications, leading to costly training and
inference. To this end, we propose to reparameterize pre-trained ViTs with a
mixture of multiplication primitives, e.g., bitwise shifts and additions,
towards a new type of multiplication-reduced model, dubbed
$\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on
GPUs without requiring training from scratch. Specifically, all
$\texttt{MatMuls}$ among queries, keys, and values are reparameterized using
additive kernels, after mapping queries and keys to binary codes in Hamming
space. The remaining MLPs or linear layers are then reparameterized with shift
kernels. We utilize TVM to implement and optimize those customized kernels for
practical hardware deployment on GPUs. We find that such a reparameterization
on attention maintains model accuracy, while inevitably leading to accuracy
drops when being applied to MLPs. To marry the best of both worlds, we further
propose a new mixture of experts (MoE) framework to reparameterize MLPs by
taking multiplication or its primitives as experts, e.g., multiplication and
shift, and designing a new latency-aware load-balancing loss. Such a loss helps
to train a generic router for assigning a dynamic amount of input tokens to
different experts according to their latency. Extensive experiments on various
2D/3D Transformer-based vision tasks consistently validate the effectiveness of
our proposed ShiftAddViT, achieving up to $\textbf{5.18$\times$}$ latency
reductions on GPUs and $\textbf{42.9}$% energy savings, while maintaining a
comparable accuracy as original or efficient ViTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1&quot;&gt;Haoran You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Huihong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yipin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yingyan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09467">
<title>AQuA: A Benchmarking Tool for Label Quality Assessment. (arXiv:2306.09467v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09467</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models are only as good as the data they are trained
on. But recent studies have found datasets widely used to train and evaluate ML
models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on
the train set hurt ML models&apos; ability to generalize, and they impact evaluation
and model selection using the test set. Consequently, learning in the presence
of labeling errors is an active area of research, yet this field lacks a
comprehensive benchmark to evaluate these methods. Most of these methods are
evaluated on a few computer vision datasets with significant variance in the
experimental protocols. With such a large pool of methods and inconsistent
evaluation, it is also unclear how ML practitioners can choose the right models
to assess label quality in their data. To this end, we propose a benchmarking
environment AQuA to rigorously evaluate methods that enable machine learning in
the presence of label noise. We also introduce a design space to delineate
concrete design choices of label error detection models. We hope that our
proposed design space and benchmark enable practitioners to choose the right
tools to improve their label quality and that our benchmark enables objective
and rigorous evaluation of machine learning tools facing mislabeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1&quot;&gt;Mononito Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanil_V/0/1/0/all/0/1&quot;&gt;Vedant Sanil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhry_A/0/1/0/all/0/1&quot;&gt;Arjun Choudhry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1&quot;&gt;Arvind Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udompanyawit_C/0/1/0/all/0/1&quot;&gt;Chalisa Udompanyawit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1&quot;&gt;Artur Dubrawski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09526">
<title>Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09526</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation Learning (IL) is a widely used framework for learning imitative
behavior from demonstrations. It is especially appealing for solving complex
real-world tasks where handcrafting reward function is difficult, or when the
goal is to mimic human expert behavior. However, the learned imitative policy
can only follow the behavior in the demonstration. When applying the imitative
policy, we may need to customize the policy behavior to meet different
requirements coming from diverse downstream tasks. Meanwhile, we still want the
customized policy to maintain its imitative nature. To this end, we formulate a
new problem setting called policy customization. It defines the learning task
as training a policy that inherits the characteristics of the prior policy
while satisfying some additional requirements imposed by a target downstream
task. We propose a novel and principled approach to interpret and determine the
trade-off between the two task objectives. Specifically, we formulate the
customization problem as a Markov Decision Process (MDP) with a reward function
that combines 1) the inherent reward of the demonstration; and 2) the add-on
reward specified by the downstream task. We propose a novel framework, Residual
Q-learning, which can solve the formulated MDP by leveraging the prior policy
without knowing the inherent reward or value function of the prior policy. We
derive a family of residual Q-learning algorithms that can realize offline and
online policy customization, and show that the proposed algorithms can
effectively accomplish policy customization tasks in various environments. Demo
videos and code are available on our website:
https://sites.google.com/view/residualq-learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_H/0/1/0/all/0/1&quot;&gt;Haruki Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercat_J/0/1/0/all/0/1&quot;&gt;Jean Mercat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09666">
<title>A Smooth Binary Mechanism for Efficient Private Continual Observation. (arXiv:2306.09666v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09666</link>
<description rdf:parseType="Literal">&lt;p&gt;In privacy under continual observation we study how to release differentially
private estimates based on a dataset that evolves over time. The problem of
releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the
value of each $x_i$ is to be private) is particularly well-studied, and a
generalized form is used in state-of-the-art methods for private stochastic
gradient descent (SGD). The seminal binary mechanism privately releases the
first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently,
Henzinger et al. and Denisov et al. showed that it is possible to improve on
the binary mechanism in two ways: The variance of the noise can be reduced by a
(large) constant factor, and also made more even across time steps. However,
their algorithms for generating the noise distribution are not as efficient as
one would like in terms of computation time and (in particular) space. We
address the efficiency problem by presenting a simple alternative to the binary
mechanism in which 1) generating the noise takes constant average time per
value, 2) the variance is reduced by a factor about 4 compared to the binary
mechanism, and 3) the noise distribution at each step is identical.
Empirically, a simple Python implementation of our approach outperforms the
running time of the approach of Henzinger et al., as well as an attempt to
improve their algorithm using high-performance algorithms for multiplication
with Toeplitz matrices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersson_J/0/1/0/all/0/1&quot;&gt;Joel Daniel Andersson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1&quot;&gt;Rasmus Pagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10506">
<title>Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions. (arXiv:2306.10506v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10506</link>
<description rdf:parseType="Literal">&lt;p&gt;MCMC algorithms offer empirically efficient tools for sampling from a target
distribution $\pi(x) \propto \exp(-V(x))$. However, on the theory side, MCMC
algorithms suffer from slow mixing rate when $\pi(x)$ is non-log-concave. Our
work examines this gap and shows that when Poincar\&apos;e-style inequality holds on
a subset $\mathcal{X}$ of the state space, the conditional distribution of MCMC
iterates over $\mathcal{X}$ mixes fast to the true conditional distribution.
This fast mixing guarantee can hold in cases when global mixing is provably
slow. We formalize the statement and quantify the conditional mixing rate. We
further show that conditional mixing can have interesting implications for
sampling from mixtures of Gaussians, parameter estimation for Gaussian mixture
models and Gibbs-sampling with well-connected local minima.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yusong Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11589">
<title>Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent. (arXiv:2306.11589v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11589</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian processes are a powerful framework for quantifying uncertainty and
for sequential decision-making but are limited by the requirement of solving
linear systems. In general, this has a cubic cost in dataset size and is
sensitive to conditioning. We explore stochastic gradient algorithms as a
computationally efficient method of approximately solving these linear systems:
we develop low-variance optimization objectives for sampling from the posterior
and extend these to inducing points. Counterintuitively, stochastic gradient
descent often produces accurate predictions, even in cases where it does not
converge quickly to the optimum. We explain this through a spectral
characterization of the implicit bias from non-convergence. We show that
stochastic gradient descent produces predictive distributions close to the true
posterior both in regions with sufficient data coverage, and in regions
sufficiently far away from the data. Experimentally, stochastic gradient
descent achieves state-of-the-art performance on sufficiently large-scale or
ill-conditioned regression tasks. Its uncertainty estimates match the
performance of significantly more expensive baselines on a large-scale Bayesian
optimization task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jihao Andreas Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antoran_J/0/1/0/all/0/1&quot;&gt;Javier Antor&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padhy_S/0/1/0/all/0/1&quot;&gt;Shreyas Padhy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janz_D/0/1/0/all/0/1&quot;&gt;David Janz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terenin_A/0/1/0/all/0/1&quot;&gt;Alexander Terenin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12974">
<title>Adaptive Bernstein Change Detector for High-Dimensional Data Streams. (arXiv:2306.12974v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12974</link>
<description rdf:parseType="Literal">&lt;p&gt;Change detection is of fundamental importance when analyzing data streams.
Detecting changes both quickly and accurately enables monitoring and prediction
systems to react, e.g., by issuing an alarm or by updating a learning
algorithm. However, detecting changes is challenging when observations are
high-dimensional. In high-dimensional data, change detectors should not only be
able to identify when changes happen, but also in which subspace they occur.
Ideally, one should also quantify how severe they are. Our approach, ABCD, has
these properties. ABCD learns an encoder-decoder model and monitors its
accuracy over a window of adaptive size. ABCD derives a change score based on
Bernstein&apos;s inequality to detect deviations in terms of accuracy, which
indicate changes. Our experiments demonstrate that ABCD outperforms its best
competitor by up to 20% in F1-score on average. It can also accurately estimate
changes&apos; subspace, together with a severity measure that correlates with the
ground truth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyden_M/0/1/0/all/0/1&quot;&gt;Marco Heyden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fouche_E/0/1/0/all/0/1&quot;&gt;Edouard Fouch&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arzamasov_V/0/1/0/all/0/1&quot;&gt;Vadim Arzamasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenn_T/0/1/0/all/0/1&quot;&gt;Tanja Fenn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalinke_F/0/1/0/all/0/1&quot;&gt;Florian Kalinke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohm_K/0/1/0/all/0/1&quot;&gt;Klemens B&amp;#xf6;hm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17361">
<title>iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17361</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural causal models (SCMs) are widely used in various disciplines to
represent causal relationships among variables in complex systems.
Unfortunately, the underlying causal structure is often unknown, and estimating
it from data remains a challenging task. In many situations, however, the end
goal is to localize the changes (shifts) in the causal mechanisms between
related datasets instead of learning the full causal structure of the
individual datasets. Some applications include root cause analysis, analyzing
gene regulatory network structure changes between healthy and cancerous
individuals, or explaining distribution shifts. This paper focuses on
identifying the causal mechanism shifts in two or more related datasets over
the same set of variables -- without estimating the entire DAG structure of
each SCM. Prior work under this setting assumed linear models with Gaussian
noises; instead, in this work we assume that each SCM belongs to the more
general class of nonlinear additive noise models (ANMs). A key technical
contribution of this work is to show that the Jacobian of the score function
for the mixture distribution allows for the identification of shifts under
general non-parametric functional mechanisms. Once the shifted variables are
identified, we leverage recent work to estimate the structural differences, if
any, for the shifted variables. Experiments on synthetic and real-world data
are provided to showcase the applicability of this approach. Code implementing
the proposed method is open-source and publicly available at
https://github.com/kevinsbello/iSCAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_K/0/1/0/all/0/1&quot;&gt;Kevin Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1&quot;&gt;Bryon Aragam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00673">
<title>ENN: A Neural Network with DCT Adaptive Activation Functions. (arXiv:2307.00673v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00673</link>
<description rdf:parseType="Literal">&lt;p&gt;The expressiveness of neural networks highly depends on the nature of the
activation function, although these are usually assumed predefined and fixed
during the training stage. Under a signal processing perspective, in this paper
we present Expressive Neural Network (ENN), a novel model in which the
non-linear activation functions are modeled using the Discrete Cosine Transform
(DCT) and adapted using backpropagation during training. This parametrization
keeps the number of trainable parameters low, is appropriate for gradient-based
schemes, and adapts to different learning tasks. This is the first non-linear
model for activation functions that relies on a signal processing perspective,
providing high flexibility and expressiveness to the network. We contribute
with insights in the explainability of the network at convergence by recovering
the concept of bump, this is, the response of each activation function in the
output space. Finally, through exhaustive experiments we show that the model
can adapt to classification and regression tasks. The performance of ENN
outperforms state of the art benchmarks, providing above a 40% gap in accuracy
in some scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Martinez_Gost_M/0/1/0/all/0/1&quot;&gt;Marc Martinez-Gost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Perez_Neira_A/0/1/0/all/0/1&quot;&gt;Ana P&amp;#xe9;rez-Neira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lagunas_M/0/1/0/all/0/1&quot;&gt;Miguel &amp;#xc1;ngel Lagunas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00859">
<title>CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00859</link>
<description rdf:parseType="Literal">&lt;p&gt;In the expansive realm of drug discovery, with approximately 15,000 known
drugs and only around 4,200 approved, the combinatorial nature of the chemical
space presents a formidable challenge. While Artificial Intelligence (AI) has
emerged as a powerful ally, traditional AI frameworks face significant hurdles.
This manuscript introduces CardiGraphormer, a groundbreaking approach that
synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and
Cardinality Preserving Attention to revolutionize drug discovery.
CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving
Attention, leverages SSL to learn potent molecular representations and employs
GNNs to extract molecular fingerprints, enhancing predictive performance and
interpretability while reducing computation time. It excels in handling complex
data like molecular structures and performs tasks associated with nodes, pairs
of nodes, subgraphs, or entire graph structures. CardiGraphormer&apos;s potential
applications in drug discovery and drug interactions are vast, from identifying
new drug targets to predicting drug-to-drug interactions and enabling novel
drug discovery. This innovative approach provides an AI-enhanced methodology in
drug development, utilizing SSL combined with GNNs to overcome existing
limitations and pave the way for a richer exploration of the vast combinatorial
chemical space in drug discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhijit Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02251">
<title>RanPAC: Random Projections and Pre-trained Models for Continual Learning. (arXiv:2307.02251v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02251</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) aims to incrementally learn different tasks (such as
classification) in a non-stationary data stream without forgetting old ones.
Most CL works focus on tackling catastrophic forgetting under a
learning-from-scratch paradigm. However, with the increasing prominence of
foundation models, pre-trained models equipped with informative representations
have become available for various downstream requirements. Several CL methods
based on pre-trained models have been explored, either utilizing pre-extracted
features directly (which makes bridging distribution gaps challenging) or
incorporating adaptors (which may be subject to forgetting). In this paper, we
propose a concise and effective approach for CL with pre-trained models. Given
that forgetting occurs during parameter updating, we contemplate an alternative
approach that exploits training-free random projectors and class-prototype
accumulation, which thus bypasses the issue. Specifically, we inject a frozen
Random Projection layer with nonlinear activation between the pre-trained
model&apos;s feature representations and output head, which captures interactions
between features with expanded dimensionality, providing enhanced linear
separability for class-prototype-based CL. We also demonstrate the importance
of decorrelating the class-prototypes to reduce the distribution disparity when
using pre-trained representations. These techniques prove to be effective and
circumvent the problem of forgetting for both class- and domain-incremental
continual learning. Compared to previous methods applied to pre-trained
ViT-B/16 models, we reduce final error rates by between 20% and 62% on seven
class-incremental benchmarks, despite not using any rehearsal memory. We
conclude that the full potential of pre-trained models for simple, effective,
and fast CL has not hitherto been fully tapped. Code is at
github.com/RanPAC/RanPAC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonnell_M/0/1/0/all/0/1&quot;&gt;Mark D. McDonnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1&quot;&gt;Dong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parveneh_A/0/1/0/all/0/1&quot;&gt;Amin Parveneh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1&quot;&gt;Ehsan Abbasnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1&quot;&gt;Anton van den Hengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03393">
<title>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes&apos; text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs. Our codes and
datasets are available at https://github.com/CurryTang/Graph-LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhikai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaochi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03590">
<title>Accelerated Optimization Landscape of Linear-Quadratic Regulator. (arXiv:2307.03590v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03590</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear-quadratic regulator (LQR) is a landmark problem in the field of
optimal control, which is the concern of this paper. Generally, LQR is
classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based
on whether the full state is obtained. It has been suggested in existing
literature that both SLQR and OLQR could be viewed as \textit{constrained
nonconvex matrix optimization} problems in which the only variable to be
optimized is the feedback gain matrix. In this paper, we introduce a
first-order accelerated optimization framework of handling the LQR problem, and
give its convergence analysis for the cases of SLQR and OLQR, respectively.
&lt;/p&gt;
&lt;p&gt;Specifically, a Lipschiz Hessian property of LQR performance criterion is
presented, which turns out to be a crucial property for the application of
modern optimization techniques. For the SLQR problem, a continuous-time hybrid
dynamic system is introduced, whose solution trajectory is shown to converge
exponentially to the optimal feedback gain with Nesterov-optimal order
$1-\frac{1}{\sqrt{\kappa}}$ ($\kappa$ the condition number). Then, the
symplectic Euler scheme is utilized to discretize the hybrid dynamic system,
and a Nesterov-type method with a restarting rule is proposed that preserves
the continuous-time convergence rate, i.e., the discretized algorithm admits
the Nesterov-optimal convergence order. For the OLQR problem, a Hessian-free
accelerated framework is proposed, which is a two-procedure method consisting
of semiconvex function optimization and negative curvature exploitation. In a
time $\mathcal{O}(\epsilon^{-7/4}\log(1/\epsilon))$, the method can find an
$\epsilon$-stationary point of the performance criterion; this entails that the
method improves upon the $\mathcal{O}(\epsilon^{-2})$ complexity of vanilla
gradient descent. Moreover, our method provides the second-order guarantee of
stationary point.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lechen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuan-Hua Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10443">
<title>Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10443</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the significant progress made by transformer models in machine
reading comprehension tasks, they still fall short in handling complex
reasoning tasks due to the absence of explicit knowledge in the input sequence.
To address this limitation, many recent works have proposed injecting external
knowledge into the model. However, selecting relevant external knowledge,
ensuring its availability, and requiring additional processing steps remain
challenging. In this paper, we introduce a novel attention pattern that
integrates reasoning knowledge derived from a heterogeneous graph into the
transformer architecture without relying on external knowledge. The proposed
attention pattern comprises three key elements: global-local attention for word
tokens, graph attention for entity tokens that exhibit strong attention towards
tokens connected in the graph as opposed to those unconnected, and the
consideration of the type of relationship between each entity token and word
token. This results in optimized attention between the two if a relationship
exists. The pattern is coupled with special relative position labels, allowing
it to integrate with LUKE&apos;s entity-aware self-attention mechanism. The
experimental findings corroborate that our model outperforms both the
cutting-edge LUKE-Graph and the baseline LUKE model across two distinct
datasets: ReCoRD, emphasizing commonsense reasoning, and WikiHop, focusing on
multi-hop reasoning challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foolad_S/0/1/0/all/0/1&quot;&gt;Shima Foolad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1&quot;&gt;Kourosh Kiani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13304">
<title>QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13304</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies post-training parameter quantization in large language
models (LLMs). We introduce quantization with incoherence processing (QuIP), a
new method based on the insight that quantization benefits from
$\textit{incoherent}$ weight and Hessian matrices, i.e., from the weights being
even in magnitude and the directions in which it is important to round them
accurately being unaligned with the coordinate axes. QuIP consists of two
steps: (1) an adaptive rounding procedure minimizing a quadratic proxy
objective; (2) efficient pre- and post-processing that ensures weight and
Hessian incoherence via multiplication by random orthogonal matrices. We
complement QuIP with the first theoretical analysis for an LLM-scale
quantization algorithm, and show that our theory also applies to an existing
method, OPTQ. Empirically, we find that our incoherence preprocessing improves
several existing quantization algorithms and yields the first LLM quantization
methods that produce viable results using only two bits per weight. Our code
can be found at https://github.com/Cornell-RelaxML/QuIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1&quot;&gt;Jerry Chee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yaohui Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuleshov_V/0/1/0/all/0/1&quot;&gt;Volodymyr Kuleshov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13716">
<title>FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13716</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional federated learning uses the number of samples to calculate the
weights of each client model and uses this fixed weight value to fusion the
global model. However, in practical scenarios, each client&apos;s device and data
heterogeneity leads to differences in the quality of each client&apos;s model. Thus
the contribution to the global model is not wholly determined by the sample
size. In addition, if clients intentionally upload low-quality or malicious
models, using these models for aggregation will lead to a severe decrease in
global model accuracy. Traditional federated learning algorithms do not address
these issues. To solve this probelm, we propose FedDRL, a model fusion approach
using reinforcement learning based on a two staged approach. In the first
stage, Our method could filter out malicious models and selects trusted client
models to participate in the model fusion. In the second stage, the FedDRL
algorithm adaptively adjusts the weights of the trusted client models and
aggregates the optimal global model. We also define five model fusion scenarios
and compare our method with two baseline algorithms in those scenarios. The
experimental results show that our algorithm has higher reliability than other
algorithms while maintaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Leiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Cihao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Sibo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziling Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yuming Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheewei Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16149">
<title>A Novel DDPM-based Ensemble Approach for Energy Theft Detection in Smart Grids. (arXiv:2307.16149v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16149</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy theft, characterized by manipulating energy consumption readings to
reduce payments, poses a dual threat-causing financial losses for grid
operators and undermining the performance of smart grids. Effective Energy
Theft Detection (ETD) methods become crucial in mitigating these risks by
identifying such fraudulent activities in their early stages. However, the
majority of current ETD methods rely on supervised learning, which is hindered
by the difficulty of labelling data and the risk of overfitting known attacks.
To address these challenges, several unsupervised ETD methods have been
proposed, focusing on learning the normal patterns from honest users,
specifically the reconstruction of input. However, our investigation reveals a
limitation in current unsupervised ETD methods, as they can only detect
anomalous behaviours in users exhibiting regular patterns. Users with
high-variance behaviours pose a challenge to these methods. In response, this
paper introduces a Denoising Diffusion Probabilistic Model (DDPM)-based ETD
approach. This innovative approach demonstrates impressive ETD performance on
high-variance smart grid data by incorporating additional attributes correlated
with energy consumption. The proposed methods improve the average ETD
performance on high-variance smart grid data from below 0.5 to over 0.9 w.r.t.
AUC. On the other hand, our experimental findings indicate that while the
state-of-the-art ETD methods based on reconstruction error can identify ETD
attacks for the majority of users, they prove ineffective in detecting attacks
for certain users. To address this, we propose a novel ensemble approach that
considers both reconstruction error and forecasting error, enhancing the
robustness of the ETD methodology. The proposed ensemble method improves the
average ETD performance on the stealthiest attacks from nearly 0 to 0.5 w.r.t.
5%-TPR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1&quot;&gt;Asif Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gope_P/0/1/0/all/0/1&quot;&gt;Prosanta Gope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikdar_B/0/1/0/all/0/1&quot;&gt;Biplab Sikdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04748">
<title>Fuzz4All: Universal Fuzzing with Large Language Models. (arXiv:2308.04748v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04748</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzing has achieved tremendous success in discovering bugs and
vulnerabilities in various software systems. Systems under test (SUTs) that
take in programming or formal language as inputs, e.g., compilers, runtime
engines, constraint solvers, and software libraries with accessible APIs, are
especially important as they are fundamental building blocks of software
development. However, existing fuzzers for such systems often target a specific
language, and thus cannot be easily applied to other languages or even other
versions of the same language. Moreover, the inputs generated by existing
fuzzers are often limited to specific features of the input language, and thus
can hardly reveal bugs related to other or new features. This paper presents
Fuzz4All, the first fuzzer that is universal in the sense that it can target
many different input languages and many different features of these languages.
The key idea behind Fuzz4All is to leverage large language models (LLMs) as an
input generation and mutation engine, which enables the approach to produce
diverse and realistic inputs for any practically relevant language. To realize
this potential, we present a novel autoprompting technique, which creates LLM
prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop,
which iteratively updates the prompt to create new fuzzing inputs. We evaluate
Fuzz4All on nine systems under test that take in six different languages (C,
C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six
languages, that universal fuzzing achieves higher coverage than existing,
language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in
widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit
quantum computing platform, with 64 bugs already confirmed by developers as
previously unknown.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chunqiu Steven Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paltenghi_M/0/1/0/all/0/1&quot;&gt;Matteo Paltenghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jia Le Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pradel_M/0/1/0/all/0/1&quot;&gt;Michael Pradel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lingming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05141">
<title>Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators. (arXiv:2308.05141v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05141</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the challenge of sound propagation simulations in 3D virtual rooms
with moving sources, which have applications in virtual/augmented reality, game
audio, and spatial computing. Solutions to the wave equation can describe wave
phenomena such as diffraction and interference. However, simulating them using
conventional numerical discretization methods with hundreds of source and
receiver positions is intractable, making stimulating a sound field with moving
sources impractical. To overcome this limitation, we propose using deep
operator networks to approximate linear wave-equation operators. This enables
the rapid prediction of sound propagation in realistic 3D acoustic scenes with
moving sources, achieving millisecond-scale computations. By learning a compact
surrogate model, we avoid the offline calculation and storage of impulse
responses for all relevant source/listener pairs. Our experiments, including
various complex scene geometries, show good agreement with reference solutions,
with root mean squared errors ranging from 0.02 Pa to 0.10 Pa. Notably, our
method signifies a paradigm shift as no prior machine learning approach has
achieved precise predictions of complete wave fields within realistic domains.
We anticipate that our findings will drive further exploration of deep neural
operator methods, advancing research in immersive user experiences within
virtual environments.$
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borrel_Jensen_N/0/1/0/all/0/1&quot;&gt;Nikolas Borrel-Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1&quot;&gt;Somdatta Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engsig_Karup_A/0/1/0/all/0/1&quot;&gt;Allan P. Engsig-Karup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Cheol-Ho Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06399">
<title>Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06399</link>
<description rdf:parseType="Literal">&lt;p&gt;Maize, a crucial crop globally cultivated across vast regions, especially in
sub-Saharan Africa, Asia, and Latin America, occupies 197 million hectares as
of 2021. Various statistical and machine learning models, including
mixed-effect models, random coefficients models, random forests, and deep
learning architectures, have been devised to predict maize yield. These models
consider factors such as genotype, environment, genotype-environment
interaction, and field management. However, the existing models often fall
short of fully exploiting the complex network of causal relationships among
these factors and the hierarchical structure inherent in agronomic data. This
study introduces an innovative approach integrating random effects into
Bayesian networks (BNs), leveraging their capacity to model causal and
probabilistic relationships through directed acyclic graphs. Rooted in the
linear mixed-effects models framework and tailored for hierarchical data, this
novel approach demonstrates enhanced BN learning. Application to a real-world
agronomic trial produces a model with improved interpretability, unveiling new
causal connections. Notably, the proposed method significantly reduces the
error rate in maize yield prediction from 28% to 17%. These results advocate
for the preference of BNs in constructing practical decision support tools for
hierarchical agronomic data, facilitating causal inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valleggi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Valleggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stefanini_F/0/1/0/all/0/1&quot;&gt;Federico Mattia Stefanini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07272">
<title>Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning. (arXiv:2308.07272v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07272</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based pre-trained language models (PLMs) paradigm have succeeded
substantially in few-shot natural language processing (NLP) tasks. However,
prior discrete prompt optimization methods require expert knowledge to design
the base prompt set and identify high-quality prompts, which is costly,
inefficient, and subjective. Meanwhile, existing continuous prompt optimization
methods improve the performance by learning the ideal prompts through the
gradient information of PLMs, whose high computational cost, and low
readability and generalizability are often concerning. To address the research
gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt
Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment
strategy for readability prompt set generation based on GPT-4. Furthermore, we
propose an efficient prompt screening metric to identify high-quality prompts
with linear complexity. Finally, we construct a reinforcement learning (RL)
framework based on policy gradients to match the prompts to inputs optimally.
By training a policy network with only 0.67% of the PLM parameter size on the
tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA)
method by 1.52% in accuracy on average on four open-source datasets. Moreover,
subsequent experiments also demonstrate that $DP_2O$ has good universality,
robustness, and generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengzhengxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Duyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yu Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11272">
<title>FoX: Formation-aware exploration in multi-agent reinforcement learning. (arXiv:2308.11272v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11272</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep multi-agent reinforcement learning (MARL) has gained
significant popularity due to its success in various cooperative multi-agent
tasks. However, exploration still remains a challenging problem in MARL due to
the partial observability of the agents and the exploration space that can grow
exponentially as the number of agents increases. Firstly, in order to address
the scalability issue of the exploration space, we define a formation-based
equivalence relation on the exploration space and aim to reduce the search
space by exploring only meaningful states in different formations. Then, we
propose a novel formation-aware exploration (FoX) framework that encourages
partially observable agents to visit the states in diverse formations by
guiding them to be well aware of their current formation solely based on their
own observations. Numerical results show that the proposed FoX framework
significantly outperforms the state-of-the-art MARL algorithms on Google
Research Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC)
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yonghyeon Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sunwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1&quot;&gt;Junghyuk Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Seungyul Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13983">
<title>Interpolation of mountain weather forecasts by machine learning. (arXiv:2308.13983v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13983</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in numerical simulation methods based on physical models and
their combination with machine learning have improved the accuracy of weather
forecasts. However, the accuracy decreases in complex terrains such as
mountainous regions because these methods usually use grids of several
kilometers square and simple machine learning models. While deep learning has
also made significant progress in recent years, its direct application is
difficult to utilize the physical knowledge used in the simulation. This paper
proposes a method that uses machine learning to interpolate future weather in
mountainous regions using forecast data from surrounding plains and past
observed data to improve weather forecasts in mountainous regions. We focus on
mountainous regions in Japan and predict temperature and precipitation mainly
using LightGBM as a machine learning model. Despite the use of a small dataset,
through feature engineering and model tuning, our method partially achieves
improvements in the RMSE with significantly less training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Iwase_K/0/1/0/all/0/1&quot;&gt;Kazuma Iwase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Takenawa_T/0/1/0/all/0/1&quot;&gt;Tomoyuki Takenawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16900">
<title>Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16900</link>
<description rdf:parseType="Literal">&lt;p&gt;We present WineSensed, a large multimodal wine dataset for studying the
relations between visual perception, language, and flavor. The dataset
encompasses 897k images of wine labels and 824k reviews of wines curated from
the Vivino platform. It has over 350k unique bottlings, annotated with year,
region, rating, alcohol percentage, price, and grape composition. We obtained
fine-grained flavor annotations on a subset by conducting a wine-tasting
experiment with 256 participants who were asked to rank wines based on their
similarity in flavor, resulting in more than 5k pairwise flavor distances. We
propose a low-dimensional concept embedding algorithm that combines human
experience with automatic machine similarity kernels. We demonstrate that this
shared concept embedding space improves upon separate embedding spaces for
coarse flavor classification (alcohol percentage, country, grape, price,
rating) and aligns with the intricate human perception of flavor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bender_T/0/1/0/all/0/1&quot;&gt;Thoranna Bender&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorensen_S/0/1/0/all/0/1&quot;&gt;Simon Moe S&amp;#xf8;rensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashani_A/0/1/0/all/0/1&quot;&gt;Alireza Kashani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hjorleifsson_K/0/1/0/all/0/1&quot;&gt;K. Eldjarn Hjorleifsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyldig_G/0/1/0/all/0/1&quot;&gt;Grethe Hyldig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1&quot;&gt;Frederik Warburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01816">
<title>Adaptive Model Pruning and Personalization for Federated Learning over Wireless Networks. (arXiv:2309.01816v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01816</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables distributed learning across edge devices
while protecting data privacy. However, the learning accuracy decreases due to
the heterogeneity of devices&apos; data, and the computation and communication
latency increase when updating large-scale learning models on devices with
limited computational capability and wireless resources. We consider a FL
framework with partial model pruning and personalization to overcome these
challenges. This framework splits the learning model into a global part with
model pruning shared with all devices to learn data representations and a
personalized part to be fine-tuned for a specific device, which adapts the
model size during FL to reduce both computation and communication latency and
increases the learning accuracy for devices with non-independent and
identically distributed data. The computation and communication latency and
convergence of the proposed FL framework are mathematically analyzed. To
maximize the convergence rate and guarantee learning accuracy, Karush Kuhn
Tucker (KKT) conditions are deployed to jointly optimize the pruning ratio and
bandwidth allocation. Finally, experimental results demonstrate that the
proposed FL framework achieves a remarkable reduction of approximately 50
percent computation and communication latency compared with FL with partial
model personalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaonan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnarajah_T/0/1/0/all/0/1&quot;&gt;Tharmalingam Ratnarajah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellathurai_M/0/1/0/all/0/1&quot;&gt;Mathini Sellathurai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06627">
<title>A Sequentially Fair Mechanism for Multiple Sensitive Attributes. (arXiv:2309.06627v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06627</link>
<description rdf:parseType="Literal">&lt;p&gt;In the standard use case of Algorithmic Fairness, the goal is to eliminate
the relationship between a sensitive variable and a corresponding score.
Throughout recent years, the scientific community has developed a host of
definitions and tools to solve this task, which work well in many practical
applications. However, the applicability and effectivity of these tools and
definitions becomes less straightfoward in the case of multiple sensitive
attributes. To tackle this issue, we propose a sequential framework, which
allows to progressively achieve fairness across a set of sensitive features. We
accomplish this by leveraging multi-marginal Wasserstein barycenters, which
extends the standard notion of Strong Demographic Parity to the case with
multiple sensitive characteristics. This method also provides a closed-form
solution for the optimal, sequentially fair predictor, permitting a clear
interpretation of inter-sensitive feature correlations. Our approach seamlessly
extends to approximate fairness, enveloping a framework accommodating the
trade-off between risk and unfairness. This extension permits a targeted
prioritization of fairness improvements for a specific attribute within a set
of sensitive attributes, allowing for a case specific adaptation. A data-driven
estimation procedure for the derived solution is developed, and comprehensive
numerical experiments are conducted on both synthetic and real datasets. Our
empirical findings decisively underscore the practical efficacy of our
post-processing approach in fostering fair decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratz_P/0/1/0/all/0/1&quot;&gt;Philipp Ratz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charpentier_A/0/1/0/all/0/1&quot;&gt;Arthur Charpentier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07156">
<title>Transparency in Sleep Staging: Deep Learning Method for EEG Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v4 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07156</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated Sleep stage classification using raw single channel EEG is a
critical tool for sleep quality assessment and disorder diagnosis. However,
modelling the complexity and variability inherent in this signal is a
challenging task, limiting their practicality and effectiveness in clinical
settings. To mitigate these challenges, this study presents an end-to-end deep
learning (DL) model which integrates squeeze and excitation blocks within the
residual network to extract features and stacked Bi-LSTM to understand complex
temporal dependencies. A distinctive aspect of this study is the adaptation of
GradCam for sleep staging, marking the first instance of an explainable DL
model in this domain with alignment of its decision-making with sleep expert&apos;s
insights. We evaluated our model on the publically available datasets
(SleepEDF-20, SleepEDF-78, and SHHS), achieving Macro-F1 scores of 82.5, 78.9,
and 81.9, respectively. Additionally, a novel training efficiency enhancement
strategy was implemented by increasing stride size, leading to 8x faster
training times with minimal impact on performance. Comparative analyses
underscore our model outperforms all existing baselines, indicating its
potential for clinical usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Shivam Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maiti_S/0/1/0/all/0/1&quot;&gt;Suvadeep Maiti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mythirayee_S/0/1/0/all/0/1&quot;&gt;S. Mythirayee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rajendran_S/0/1/0/all/0/1&quot;&gt;Srijithesh Rajendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bapi_R/0/1/0/all/0/1&quot;&gt;Raju Surampudi Bapi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09677">
<title>Single and Few-step Diffusion for Generative Speech Enhancement. (arXiv:2309.09677v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09677</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have shown promising results in speech enhancement, using a
task-adapted diffusion process for the conditional generation of clean speech
given a noisy mixture. However, at test time, the neural network used for score
estimation is called multiple times to solve the iterative reverse process.
This results in a slow inference process and causes discretization errors that
accumulate over the sampling trajectory. In this paper, we address these
limitations through a two-stage training approach. In the first stage, we train
the diffusion model the usual way using the generative denoising score matching
loss. In the second stage, we compute the enhanced signal by solving the
reverse process and compare the resulting estimate to the clean speech target
using a predictive loss. We show that using this second training stage enables
achieving the same performance as the baseline model using only 5 function
evaluations instead of 60 function evaluations. While the performance of usual
generative diffusion algorithms drops dramatically when lowering the number of
function evaluations (NFEs) to obtain single-step diffusion, we show that our
proposed method keeps a steady performance and therefore largely outperforms
the diffusion baseline in this setting and also generalizes better than its
predictive counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lay_B/0/1/0/all/0/1&quot;&gt;Bunlong Lay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lemercier_J/0/1/0/all/0/1&quot;&gt;Jean-Marie Lemercier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Richter_J/0/1/0/all/0/1&quot;&gt;Julius Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1&quot;&gt;Timo Gerkmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11856">
<title>Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization. (arXiv:2309.11856v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11856</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient training of large-scale graph neural networks (GNNs) has been
studied with a specific focus on reducing their memory consumption. Work by Liu
et al. (2022) proposed extreme activation compression (EXACT) which
demonstrated drastic reduction in memory consumption by performing quantization
of the intermediate activation maps down to using INT2 precision. They showed
little to no reduction in performance while achieving large reductions in GPU
memory consumption. In this work, we present an improvement to the EXACT
strategy by using block-wise quantization of the intermediate activation maps.
We experimentally analyze different block sizes and show further reduction in
memory consumption (&amp;gt;15%), and runtime speedup per epoch (about 5%) even when
performing extreme extents of quantization with similar performance trade-offs
as with the original EXACT. Further, we present a correction to the assumptions
on the distribution of intermediate activation maps in EXACT (assumed to be
uniform) and show improved variance estimations of the quantization and
dequantization steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eliassen_S/0/1/0/all/0/1&quot;&gt;Sebastian Eliassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Selvan_R/0/1/0/all/0/1&quot;&gt;Raghavendra Selvan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12252">
<title>Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12252</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential models, such as Recurrent Neural Networks and Neural Ordinary
Differential Equations, have long suffered from slow training due to their
inherent sequential nature. For many years this bottleneck has persisted, as
many thought sequential models could not be parallelized. We challenge this
long-held belief with our parallel algorithm that accelerates GPU evaluation of
sequential models by up to 3 orders of magnitude faster without compromising
output accuracy. The algorithm does not need any special structure in the
sequential models&apos; architecture, making it applicable to a wide range of
architectures. Using our method, training sequential models can be more than 10
times faster than the common sequential method without any meaningful
difference in the training results. Leveraging this accelerated training, we
discovered the efficacy of the Gated Recurrent Unit in a long time series
classification problem with 17k time samples. By overcoming the training
bottleneck, our work serves as the first step to unlock the potential of
non-linear sequential models for long sequence problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1&quot;&gt;Yi Heng Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selfridge_J/0/1/0/all/0/1&quot;&gt;Joshua Selfridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasim_M/0/1/0/all/0/1&quot;&gt;Muhammad Firmansyah Kasim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12689">
<title>AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12689</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixup is an effective data augmentation method that generates new augmented
samples by aggregating linear combinations of different original samples.
However, if there are noises or aberrant features in the original samples,
Mixup may propagate them to the augmented samples, leading to over-sensitivity
of the model to these outliers . To solve this problem, this paper proposes a
new Mixup method called AMPLIFY. This method uses the Attention mechanism of
Transformer itself to reduce the influence of noises and aberrant values in the
original samples on the prediction results, without increasing additional
trainable parameters, and the computational cost is very low, thereby avoiding
the problem of high resource consumption in common Mixup methods such as
Sentence Mixup . The experimental results show that, under a smaller
computational resource cost, AMPLIFY outperforms other Mixup methods in text
classification tasks on 7 benchmark datasets, providing new ideas and new ways
to further improve the performance of pre-trained models based on the Attention
mechanism, such as BERT, ALBERT, RoBERTa, and GPT. Our code can be obtained at
https://github.com/kiwi-lilo/AMPLIFY.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Leixin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13915">
<title>Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds. (arXiv:2309.13915v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13915</link>
<description rdf:parseType="Literal">&lt;p&gt;Policy gradient methods equipped with deep neural networks have achieved
great success in solving high-dimensional reinforcement learning (RL) problems.
However, current analyses cannot explain why they are resistant to the curse of
dimensionality. In this work, we study the sample complexity of the neural
policy mirror descent (NPMD) algorithm with deep convolutional neural networks
(CNN). Motivated by the empirical observation that many high-dimensional
environments have state spaces possessing low-dimensional structures, such as
those taking images as states, we consider the state space to be a
$d$-dimensional manifold embedded in the $D$-dimensional Euclidean space with
intrinsic dimension $d\ll D$. We show that in each iteration of NPMD, both the
value function and the policy can be well approximated by CNNs. The
approximation errors are controlled by the size of the networks, and the
smoothness of the previous networks can be inherited. As a result, by properly
choosing the network size and hyperparameters, NPMD can find an
$\epsilon$-optimal policy with $\widetilde{O}(\epsilon^{-\frac{d}{\alpha}-2})$
samples in expectation, where $\alpha\in(0,1]$ indicates the smoothness of
environment. Compared to previous work, our result exhibits that NPMD can
leverage the low-dimensional structure of state space to escape from the curse
of dimensionality, explaining the efficacy of deep policy gradient algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minshuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14324">
<title>Towards General-Purpose Text-Instruction-Guided Voice Conversion. (arXiv:2309.14324v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14324</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel voice conversion (VC) model, guided by text
instructions such as &quot;articulate slowly with a deep tone&quot; or &quot;speak in a
cheerful boyish voice&quot;. Unlike traditional methods that rely on reference
utterances to determine the attributes of the converted speech, our model adds
versatility and specificity to voice conversion. The proposed VC model is a
neural codec language model which processes a sequence of discrete codes,
resulting in the code sequence of converted speech. It utilizes text
instructions as style prompts to modify the prosody and emotional information
of the given speech. In contrast to previous approaches, which often rely on
employing separate encoders like prosody and content encoders to handle
different aspects of the source speech, our model handles various information
of speech in an end-to-end manner. Experiments have demonstrated the impressive
capabilities of our model in comprehending instructions and delivering
reasonable results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuan_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Kuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen An Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_T/0/1/0/all/0/1&quot;&gt;Tsu-Yuan Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tse-Yang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Ho-Lam Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shuo-yiin Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16369">
<title>Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification. (arXiv:2309.16369v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16369</link>
<description rdf:parseType="Literal">&lt;p&gt;The correlation between the sharpness of loss minima and generalisation in
the context of deep neural networks has been subject to discussion for a long
time. Whilst mostly investigated in the context of selected benchmark data sets
in the area of computer vision, we explore this aspect for the acoustic scene
classification task of the DCASE2020 challenge data. Our analysis is based on
two-dimensional filter-normalised visualisations and a derived sharpness
measure. Our exploratory analysis shows that sharper minima tend to show better
generalisation than flat minima -even more so for out-of-domain data, recorded
from previously unseen devices-, thus adding to the dispute about better
generalisation capabilities of flat minima. We further find that, in
particular, the choice of optimisers is a main driver of the sharpness of
minima and we discuss resulting limitations with respect to comparability. Our
code, trained model states and loss landscape visualisations are publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milling_M/0/1/0/all/0/1&quot;&gt;Manuel Milling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triantafyllopoulos_A/0/1/0/all/0/1&quot;&gt;Andreas Triantafyllopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsangko_I/0/1/0/all/0/1&quot;&gt;Iosif Tsangko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampp_S/0/1/0/all/0/1&quot;&gt;Simon David Noel Rampp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Wolfgang Schuller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16540">
<title>Unsupervised Pretraining for Fact Verification by Language Model Distillation. (arXiv:2309.16540v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16540</link>
<description rdf:parseType="Literal">&lt;p&gt;Fact verification aims to verify a claim using evidence from a trustworthy
knowledge base. To address this challenge, algorithms must produce features for
every claim that are both semantically meaningful, and compact enough to find a
semantic alignment with the source information. In contrast to previous work,
which tackled the alignment problem by learning over annotated corpora of
claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact
Verification via Language Model Distillation), a novel unsupervised pretraining
framework that leverages pre-trained language models to distil self-supervised
features into high-quality claim-fact alignments without the need for
annotations. This is enabled by a novel contrastive loss function that
encourages features to attain high-quality claim and evidence alignments whilst
preserving the semantic relationships across the corpora. Notably, we present
results that achieve a new state-of-the-art on FB15k-237 (+5.3% Hits@1) and
FEVER (+8% accuracy) with linear evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazaga_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe1;n Bazaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micklem_G/0/1/0/all/0/1&quot;&gt;Gos Micklem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16672">
<title>Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16672</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision research has long aimed to build systems that are robust to
spatial transformations found in natural data. Traditionally, this is done
using data augmentation or hard-coding invariances into the architecture.
However, too much or too little invariance can hurt, and the correct amount is
unknown a priori and dependent on the instance. Ideally, the appropriate
invariance would be learned from data and inferred at test-time.
&lt;/p&gt;
&lt;p&gt;We treat invariance as a prediction problem. Given any image, we use a
normalizing flow to predict a distribution over transformations and average the
predictions over them. Since this distribution only depends on the instance, we
can align instances before classifying them and generalize invariance across
classes. The same distribution can also be used to adapt to out-of-distribution
poses. This normalizing flow is trained end-to-end and can learn a much larger
range of transformations than Augerino and InstaAug. When used as data
augmentation, our method shows accuracy and robustness gains on CIFAR 10,
CIFAR10-LT, and TinyImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_U/0/1/0/all/0/1&quot;&gt;Utkarsh Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1&quot;&gt;Carlos Esteves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1&quot;&gt;Ameesh Makadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Stella X. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17011">
<title>Feature Interaction Aware Automated Data Representation Transformation. (arXiv:2309.17011v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17011</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating an effective representation space is crucial for mitigating the
curse of dimensionality, enhancing model generalization, addressing data
sparsity, and leveraging classical models more effectively. Recent advancements
in automated feature engineering (AutoFE) have made significant progress in
addressing various challenges associated with representation learning, issues
such as heavy reliance on intensive labor and empirical experiences, lack of
explainable explicitness, and inflexible feature space reconstruction embedded
into downstream tasks. However, these approaches are constrained by: 1)
generation of potentially unintelligible and illogical reconstructed feature
spaces, stemming from the neglect of expert-level cognitive processes; 2) lack
of systematic exploration, which subsequently results in slower model
convergence for identification of optimal feature space. To address these, we
introduce an interaction-aware reinforced generation perspective. We redefine
feature space reconstruction as a nested process of creating meaningful
features and controlling feature set size through selection. We develop a
hierarchical reinforcement learning structure with cascading Markov Decision
Processes to automate feature and operation selection, as well as feature
crossing. By incorporating statistical measures, we reward agents based on the
interaction strength between selected features, resulting in intelligent and
efficient exploration of the feature space that emulates human decision-making.
Extensive experiments are conducted to validate our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azim_E/0/1/0/all/0/1&quot;&gt;Ehtesamul Azim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dongjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kunpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanjie Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04585">
<title>Interventions Against Machine-Assisted Statistical Discrimination. (arXiv:2310.04585v2 [econ.TH] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04585</link>
<description rdf:parseType="Literal">&lt;p&gt;This article studies how to intervene against statistical discrimination,
when it is based on beliefs generated by machine learning, rather than by
humans. Unlike beliefs formed by a human mind, machine learning-generated
beliefs are verifiable. This allows interventions to move beyond simple,
belief-free designs like affirmative action, to more sophisticated ones, that
constrain decision makers in ways that depend on what they are thinking. Such
mind reading interventions can perform well where affirmative action does not,
even when the beliefs being conditioned on are possibly incorrect and biased.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;John Y. Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06872">
<title>On sparse regression, Lp-regularization, and automated model discovery. (arXiv:2310.06872v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06872</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse regression and feature extraction are the cornerstones of knowledge
discovery from massive data. Their goal is to discover interpretable and
predictive models that provide simple relationships among scientific variables.
While the statistical tools for model discovery are well established in the
context of linear regression, their generalization to nonlinear regression in
material modeling is highly problem-specific and insufficiently understood.
Here we explore the potential of neural networks for automatic model discovery
and induce sparsity by a hybrid approach that combines two strategies:
regularization and physical constraints. We integrate the concept of Lp
regularization for subset selection with constitutive neural networks that
leverage our domain knowledge in kinematics and thermodynamics. We train our
networks with both, synthetic and real data, and perform several thousand
discovery runs to infer common guidelines and trends: L2 regularization or
ridge regression is unsuitable for model discovery; L1 regularization or lasso
promotes sparsity, but induces strong bias; only L0 regularization allows us to
transparently fine-tune the trade-off between interpretability and
predictability, simplicity and accuracy, and bias and variance. With these
insights, we demonstrate that Lp regularized constitutive neural networks can
simultaneously discover both, interpretable models and physically meaningful
parameters. We anticipate that our findings will generalize to alternative
discovery techniques such as sparse and symbolic regression, and to other
domains such as biology, chemistry, or medicine. Our ability to automatically
discover material models from data could have tremendous applications in
generative material design and open new opportunities to manipulate matter,
alter properties of existing materials, and discover new materials with
user-defined properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCulloch_J/0/1/0/all/0/1&quot;&gt;Jeremy A. McCulloch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierre_S/0/1/0/all/0/1&quot;&gt;Skyler R. St. Pierre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linka_K/0/1/0/all/0/1&quot;&gt;Kevin Linka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhl_E/0/1/0/all/0/1&quot;&gt;Ellen Kuhl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07958">
<title>Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v5 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07958</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning vulnerability detection has shown promising results in recent
years. However, an important challenge that still blocks it from being very
useful in practice is that the model is not robust under perturbation and it
cannot generalize well over the out-of-distribution (OOD) data, e.g., applying
a trained model to unseen projects in real world. We hypothesize that this is
because the model learned non-robust features, e.g., variable names, that have
spurious correlations with labels. When the perturbed and OOD datasets no
longer have the same spurious features, the model prediction fails. To address
the challenge, in this paper, we introduced causality into deep learning
vulnerability detection. Our approach CausalVul consists of two phases. First,
we designed novel perturbations to discover spurious features that the model
may use to make predictions. Second, we applied the causal learning algorithms,
specifically, do-calculus, on top of existing deep learning models to
systematically remove the use of spurious features and thus promote causal
based prediction. Our results show that CausalVul consistently improved the
model accuracy, robustness and OOD performance for all the state-of-the-art
models and datasets we experimented. To the best of our knowledge, this is the
first work that introduces do calculus based causal learning to software
engineering models and shows it&apos;s indeed useful for improving the model
accuracy, robustness and generalization. Our replication package is located at
https://figshare.com/s/0ffda320dcb96c249ef2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Mahbubur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceka_I/0/1/0/all/0/1&quot;&gt;Ira Ceka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Chengzhi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Saikat Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1&quot;&gt;Baishakhi Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1&quot;&gt;Wei Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07985">
<title>Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization. (arXiv:2310.07985v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07985</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural combinatorial optimization (NCO) is a promising learning-based
approach for solving challenging combinatorial optimization problems without
specialized algorithm design by experts. However, most constructive NCO methods
cannot solve problems with large-scale instance sizes, which significantly
diminishes their usefulness for real-world applications. In this work, we
propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong
generalization ability to address this critical issue. The LEHD model can learn
to dynamically capture the relationships between all available nodes of varying
sizes, which is beneficial for model generalization to problems of various
scales. Moreover, we develop a data-efficient training scheme and a flexible
solution construction mechanism for the proposed LEHD model. By training on
small-scale problem instances, the LEHD model can generate nearly optimal
solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle
Routing Problem (CVRP) with up to 1000 nodes, and also generalizes well to
solve real-world TSPLib and CVRPLib problems. These results confirm our
proposed LEHD model can significantly improve the state-of-the-art performance
for constructive NCO. The code is available at
https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingfu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenkun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08661">
<title>Counting and Algorithmic Generalization with Transformers. (arXiv:2310.08661v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08661</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithmic generalization in machine learning refers to the ability to learn
the underlying algorithm that generates data in a way that generalizes
out-of-distribution. This is generally considered a difficult task for most
machine learning algorithms. Here, we analyze algorithmic generalization when
counting is required, either implicitly or explicitly. We show that standard
Transformers are based on architectural decisions that hinder
out-of-distribution performance for such tasks. In particular, we discuss the
consequences of using layer normalization and of normalizing the attention
weights via softmax. With ablation of the problematic operations, we
demonstrate that a modified transformer can exhibit a good algorithmic
generalization performance on counting while using a very lightweight
architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellette_S/0/1/0/all/0/1&quot;&gt;Simon Ouellette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_R/0/1/0/all/0/1&quot;&gt;Rolf Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jud_H/0/1/0/all/0/1&quot;&gt;Hansueli Jud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09605">
<title>Penetrative AI: Making LLMs Comprehend the Physical World. (arXiv:2310.09605v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09605</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in Large Language Models (LLMs) have demonstrated their
remarkable capabilities across a range of tasks. Questions, however, persist
about the nature of LLMs and their potential to integrate common-sense human
knowledge when performing tasks involving information about the real physical
world. This paper delves into these questions by exploring how LLMs can be
extended to interact with and reason about the physical world through IoT
sensors and actuators, a concept that we term &quot;Penetrative AI&quot;. The paper
explores such an extension at two levels of LLMs&apos; ability to penetrate into the
physical world via the processing of sensory signals. Our preliminary findings
indicate that LLMs, with ChatGPT being the representative example in our
exploration, have considerable and unique proficiency in employing the embedded
world knowledge for interpreting IoT sensor data and reasoning over them about
tasks in the physical realm. Not only this opens up new applications for LLMs
beyond traditional text-based tasks, but also enables new ways of incorporating
human knowledge in cyber-physical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huatao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Liying Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1&quot;&gt;Mani Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11952">
<title>Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11952</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we aim to establish a strong connection between two significant
bodies of machine learning research: continual learning and sequence modeling.
That is, we propose to formulate continual learning as a sequence modeling
problem, allowing advanced sequence models to be utilized for continual
learning. Under this formulation, the continual learning process becomes the
forward pass of a sequence model. By adopting the meta-continual learning (MCL)
framework, we can train the sequence model at the meta-level, on multiple
continual learning episodes. As a specific example of our new formulation, we
demonstrate the application of Transformers and their efficient variants as MCL
methods. Our experiments on seven benchmarks, covering both classification and
regression, show that sequence models can be an attractive solution for general
MCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soochan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1&quot;&gt;Jaehyeon Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gunhee Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12248">
<title>A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12248</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL
-- have seen recent use as a way to express non-Markovian objectives in
reinforcement learning. We introduce a model-based probably approximately
correct (PAC) learning algorithm for omega-regular objectives in Markov
decision processes (MDPs). As part of the development of our algorithm, we
introduce the epsilon-recurrence time: a measure of the speed at which a policy
converges to the satisfaction of the omega-regular objective in the limit. We
prove that our algorithm only requires a polynomial number of samples in the
relevant parameters, and perform experiments which confirm our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1&quot;&gt;Mateo Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somenzi_F/0/1/0/all/0/1&quot;&gt;Fabio Somenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1&quot;&gt;Ashutosh Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12447">
<title>Constrained Reweighting of Distributions: an Optimal Transport Approach. (arXiv:2310.12447v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12447</link>
<description rdf:parseType="Literal">&lt;p&gt;We commonly encounter the problem of identifying an optimally weight adjusted
version of the empirical distribution of observed data, adhering to predefined
constraints on the weights. Such constraints often manifest as restrictions on
the moments, tail behaviour, shapes, number of modes, etc., of the resulting
weight adjusted empirical distribution. In this article, we substantially
enhance the flexibility of such methodology by introducing a nonparametrically
imbued distributional constraints on the weights, and developing a general
framework leveraging the maximum entropy principle and tools from optimal
transport. The key idea is to ensure that the maximum entropy weight adjusted
empirical distribution of the observed data is close to a pre-specified
probability distribution in terms of the optimal transport metric while
allowing for subtle departures. The versatility of the framework is
demonstrated in the context of three disparate applications where data
re-weighting is warranted to satisfy side constraints on the optimization
problem at the heart of the statistical task: namely, portfolio allocation,
semi-parametric inference for complex surveys, and ensuring algorithmic
fairness in machine learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Abhisek Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhattacharya_A/0/1/0/all/0/1&quot;&gt;Anirban Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pati_D/0/1/0/all/0/1&quot;&gt;Debdeep Pati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13349">
<title>DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13349</link>
<description rdf:parseType="Literal">&lt;p&gt;Voxel-based multiple testing is widely used in neuroimaging data analysis.
Traditional false discovery rate (FDR) control methods often ignore the spatial
dependence among the voxel-based tests and thus suffer from substantial loss of
testing power. While recent spatial FDR control methods have emerged, their
validity and optimality remain questionable when handling the complex spatial
dependencies of the brain. Concurrently, deep learning methods have
revolutionized image segmentation, a task closely related to voxel-based
multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR
control method that leverages unsupervised deep learning-based image
segmentation to address the voxel-based multiple testing problem. Numerical
studies, including comprehensive simulations and Alzheimer&apos;s disease FDG-PET
image analysis, demonstrate DeepFDR&apos;s superiority over existing methods.
DeepFDR not only excels in FDR control and effectively diminishes the false
nondiscovery rate, but also boasts exceptional computational efficiency highly
suited for tackling large-scale neuroimaging data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehyo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shu_H/0/1/0/all/0/1&quot;&gt;Hai Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jia_Q/0/1/0/all/0/1&quot;&gt;Qiran Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leon_M/0/1/0/all/0/1&quot;&gt;Mony de Leon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14053">
<title>Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14053</link>
<description rdf:parseType="Literal">&lt;p&gt;Code Large Language Models (Code LLMs) are being increasingly employed in
real-life applications, so evaluating them is critical. While the conventional
accuracy evaluates the performance of Code LLMs on a set of individual tasks,
their self-consistency across different tasks is overlooked. Intuitively, a
trustworthy model should be self-consistent when generating natural language
specifications for its own code and generating code for its own specifications.
Failure to preserve self-consistency reveals a lack of understanding of the
shared semantics underlying natural language and programming language, and
therefore undermines the trustworthiness of a model. In this paper, we first
formally define the self-consistency of Code LLMs and then design a framework,
IdentityChain, which effectively and efficiently evaluates the self-consistency
and conventional accuracy of a model at the same time. We study eleven Code
LLMs and show that they fail to preserve self-consistency, which is indeed a
distinct aspect from conventional accuracy. Furthermore, we show that
IdentityChain can be used as a model debugging tool to expose weaknesses of
Code LLMs by demonstrating three major weaknesses that we identify in current
models using IdentityChain. Our code is available at
https://github.com/marcusm117/IdentityChain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1&quot;&gt;Marcus J. Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yangruibo Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buratti_L/0/1/0/all/0/1&quot;&gt;Luca Buratti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujar_S/0/1/0/all/0/1&quot;&gt;Saurabh Pujar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_G/0/1/0/all/0/1&quot;&gt;Gail Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1&quot;&gt;Suman Jana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1&quot;&gt;Baishakhi Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14753">
<title>Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules. (arXiv:2310.14753v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14753</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked graph modeling excels in the self-supervised representation learning
of molecular graphs. Scrutinizing previous studies, we can reveal a common
scheme consisting of three key components: (1) graph tokenizer, which breaks a
molecular graph into smaller fragments (i.e., subgraphs) and converts them into
tokens; (2) graph masking, which corrupts the graph with masks; (3) graph
autoencoder, which first applies an encoder on the masked graph to generate the
representations, and then employs a decoder on the representations to recover
the tokens of the original graph. However, the previous MGM studies focus
extensively on graph masking and encoder, while there is limited understanding
of tokenizer and decoder. To bridge the gap, we first summarize popular
molecule tokenizers at the granularity of node, edge, motif, and Graph Neural
Networks (GNNs), and then examine their roles as the MGM&apos;s reconstruction
targets. Further, we explore the potential of adopting an expressive decoder in
MGM. Our results show that a subgraph-level tokenizer and a sufficiently
expressive decoder with remask decoding have a large impact on the encoder&apos;s
representation learning. Finally, we propose a novel MGM method SimSGT,
featuring a Simple GNN-based Tokenizer (SGT) and an effective decoding
strategy. We empirically validate that our method outperforms the existing
molecule self-supervised learning methods. Our codes and checkpoints are
available at https://github.com/syr-cn/SimSGT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yaorui Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;An Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16221">
<title>Hierarchical Randomized Smoothing. (arXiv:2310.16221v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16221</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1&quot;&gt;Yan Scholten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1&quot;&gt;Jan Schuchardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Bojchevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16652">
<title>How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels. (arXiv:2310.16652v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16652</link>
<description rdf:parseType="Literal">&lt;p&gt;Because of its privacy-preserving capability, federated learning (FL) has
attracted significant attention from both academia and industry. However, when
being implemented over wireless networks, it is not clear how much
communication error can be tolerated by FL. This paper investigates the
robustness of FL to the uplink and downlink communication error. Our
theoretical analysis reveals that the robustness depends on two critical
parameters, namely the number of clients and the numerical range of model
parameters. It is also shown that the uplink communication in FL can tolerate a
higher bit error rate (BER) than downlink communication, and this difference is
quantified by a proposed formula. The findings and theoretical analyses are
further validated by extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1&quot;&gt;Linping Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shenghui Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsui_C/0/1/0/all/0/1&quot;&gt;Chi-Ying Tsui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuyi Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17072">
<title>IMMP++: Isometric Motion Manifold Primitives with Parametric Curve Models. (arXiv:2310.17072v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17072</link>
<description rdf:parseType="Literal">&lt;p&gt;The Motion Manifold Primitive (MMP) produces, for a given task, a continuous
manifold of trajectories, each of which can successfully complete the task,
addressing the challenge of high dimensionality in trajectory data. However,
the discrete-time trajectory representations used in existing MMP methods lack
important functionalities of movement primitives (e.g., temporal modulation,
via-points modulation, etc.) found in other conventional methods that employ
parametric curve representations. To address these limitations, we introduce
Motion Manifold Primitives++ (MMP++), which combines the advantages of the MMP
and conventional methods by applying the MMP framework to the parametric curve
representations. However, we observe that the performance of MMP++ can
sometimes degrade significantly due to geometric distortion in the latent space
-- by distortion, we mean that similar motions are not located nearby in the
latent space. To mitigate this issue, we propose Isometric Motion Manifold
Primitives++ (IMMP++), where the latent coordinate space preserves the geometry
of the manifold. Experimental results with 2-DoF planar motions and 7-DoF robot
arm tasks demonstrate that MMP++ and IMMP++ outperform existing methods, in
some cases by a significant margin, while maintaining the advantages of
parametric curve representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yonghyeon Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19273">
<title>The Memory Perturbation Equation: Understanding Model&apos;s Sensitivity to Data. (arXiv:2310.19273v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19273</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding model&apos;s sensitivity to its training data is crucial but can
also be challenging and costly, especially during training. To simplify such
issues, we present the Memory-Perturbation Equation (MPE) which relates model&apos;s
sensitivity to perturbation in its training data. Derived using Bayesian
principles, the MPE unifies existing sensitivity measures, generalizes them to
a wide-variety of models and algorithms, and unravels useful properties
regarding sensitivities. Our empirical results show that sensitivity estimates
obtained during training can be used to faithfully predict generalization on
unseen test data. The proposed equation is expected to be useful for future
research on robust and adaptive learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nickl_P/0/1/0/all/0/1&quot;&gt;Peter Nickl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tailor_D/0/1/0/all/0/1&quot;&gt;Dharmesh Tailor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollenhoff_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19680">
<title>Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19680</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies have been
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes PLM-integrated NMT
(PiNMT) model to overcome the identified problems. PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieve
state-of-the-art performance on the IWSLT&apos;14 En$\leftrightarrow$De dataset.
This study&apos;s outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Soon-Jae Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Chang-Sung Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20598">
<title>Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms. (arXiv:2310.20598v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20598</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce and study online conversion with switching costs, a family of
online problems that capture emerging problems at the intersection of energy
and sustainability. In this problem, an online player attempts to purchase
(alternatively, sell) fractional shares of an asset during a fixed time horizon
with length $T$. At each time step, a cost function (alternatively, price
function) is revealed, and the player must irrevocably decide an amount of
asset to convert. The player also incurs a switching cost whenever their
decision changes in consecutive time steps, i.e., when they increase or
decrease their purchasing amount. We introduce competitive (robust)
threshold-based algorithms for both the minimization and maximization variants
of this problem, and show they are optimal among deterministic online
algorithms. We then propose learning-augmented algorithms that take advantage
of untrusted black-box advice (such as predictions from a machine learning
model) to achieve significantly better average-case performance without
sacrificing worst-case competitive guarantees. Finally, we empirically evaluate
our proposed algorithms using a carbon-aware EV charging case study, showing
that our algorithms substantially improve on baseline methods for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechowicz_A/0/1/0/all/0/1&quot;&gt;Adam Lechowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christianson_N/0/1/0/all/0/1&quot;&gt;Nicolas Christianson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bashir_N/0/1/0/all/0/1&quot;&gt;Noman Bashir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajiesmaili_M/0/1/0/all/0/1&quot;&gt;Mohammad Hajiesmaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1&quot;&gt;Adam Wierman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1&quot;&gt;Prashant Shenoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01017">
<title>Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01017</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lunjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01352">
<title>Deep learning based Image Compression for Microscopy Images: An Empirical Study. (arXiv:2311.01352v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01352</link>
<description rdf:parseType="Literal">&lt;p&gt;With the fast development of modern microscopes and bioimaging techniques, an
unprecedentedly large amount of imaging data are being generated, stored,
analyzed, and even shared through networks. The size of the data poses great
challenges for current data infrastructure. One common way to reduce the data
size is by image compression. This present study analyzes classic and deep
learning based image compression methods, and their impact on deep learning
based image processing models. Deep learning based label-free prediction models
(i.e., predicting fluorescent images from bright field images) are used as an
example application for comparison and analysis. Effective image compression
methods could help reduce the data size significantly without losing necessary
information, and therefore reduce the burden on data management infrastructure
and permit fast transmission through the network for data sharing or cloud
computing. To compress images in such a wanted way, multiple classical lossy
image compression techniques are compared to several AI-based compression
models provided by and trained with the CompressAI toolbox using python. These
different compression techniques are compared in compression ratio, multiple
image similarity measures and, most importantly, the prediction accuracy from
label-free models on compressed images. We found that AI-based compression
techniques largely outperform the classic ones and will minimally affect the
downstream label-free task in 2D cases. In the end, we hope the present study
could shed light on the potential of deep learning based image compression and
the impact of image compression on downstream deep learning based image
analysis models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sollmann_J/0/1/0/all/0/1&quot;&gt;Jan Sollmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianxu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01644">
<title>Should Under-parameterized Student Networks Copy or Average Teacher Weights?. (arXiv:2311.01644v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01644</link>
<description rdf:parseType="Literal">&lt;p&gt;Any continuous function $f^*$ can be approximated arbitrarily well by a
neural network with sufficiently many neurons $k$. We consider the case when
$f^*$ itself is a neural network with one hidden layer and $k$ neurons.
Approximating $f^*$ with a neural network with $n&amp;lt; k$ neurons can thus be seen
as fitting an under-parameterized &quot;student&quot; network with $n$ neurons to a
&quot;teacher&quot; network with $k$ neurons. As the student has fewer neurons than the
teacher, it is unclear, whether each of the $n$ student neurons should copy one
of the teacher neurons or rather average a group of teacher neurons. For
shallow neural networks with erf activation function and for the standard
Gaussian input distribution, we prove that &quot;copy-average&quot; configurations are
critical points if the teacher&apos;s incoming vectors are orthonormal and its
outgoing weights are unitary. Moreover, the optimum among such configurations
is reached when $n-1$ student neurons each copy one teacher neuron and the
$n$-th student neuron averages the remaining $k-n+1$ teacher neurons. For the
student network with $n=1$ neuron, we provide additionally a closed-form
solution of the non-trivial critical point(s) for commonly used activation
functions through solving an equivalent constrained optimization problem.
Empirically, we find for the erf activation function that gradient flow
converges either to the optimal copy-average critical point or to another point
where each student neuron approximately copies a different teacher neuron.
Finally, we find similar results for the ReLU activation function, suggesting
that the optimal solution of underparameterized networks has a universal
structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simsek_B/0/1/0/all/0/1&quot;&gt;Berfin &amp;#x15e;im&amp;#x15f;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bendjeddou_A/0/1/0/all/0/1&quot;&gt;Amire Bendjeddou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1&quot;&gt;Wulfram Gerstner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brea_J/0/1/0/all/0/1&quot;&gt;Johanni Brea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02332">
<title>Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02332</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also questions practical
implementation of such models, bringing attention to the dynamics between
decision support systems and healthcare providers. Despite advancements,
challenges such as data biases and the scarcity of &quot;big data&quot; in many
biomedical domains persist. We conclude with a discussion on effective
innovation and collaborative efforts to further the miss
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1&quot;&gt;Elisa Warner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joonsang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;William Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1&quot;&gt;Tanveer Syeda-Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1&quot;&gt;Charles Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1&quot;&gt;Olivier Gevaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Arvind Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02794">
<title>Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder. (arXiv:2311.02794v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02794</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models of observations under interventions have been a vibrant
topic of interest across machine learning and the sciences in recent years. For
example, in drug discovery, there is a need to model the effects of diverse
interventions on cells in order to characterize unknown biological mechanisms
of action. We propose the Sparse Additive Mechanism Shift Variational
Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and
interpretability for perturbation models. SAMS-VAE models the latent state of a
perturbed sample as the sum of a local latent variable capturing
sample-specific variation and sparse global variables of latent intervention
effects. Crucially, SAMS-VAE sparsifies these global latent variables for
individual perturbations to identify disentangled, perturbation-specific latent
subspaces that are flexibly composable. We evaluate SAMS-VAE both
quantitatively and qualitatively on a range of tasks using two popular single
cell sequencing datasets. In order to measure perturbation-specific
model-properties, we also introduce a framework for evaluation of perturbation
models based on average treatment effects with links to posterior predictive
checks. SAMS-VAE outperforms comparable models in terms of generalization
across in-distribution and out-of-distribution tasks, including a combinatorial
reasoning task under resource paucity, and yields interpretable latent
structures which correlate strongly to known biological mechanisms. Our results
suggest SAMS-VAE is an interesting addition to the modeling toolkit for machine
learning-driven scientific discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bereket_M/0/1/0/all/0/1&quot;&gt;Michael Bereket&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1&quot;&gt;Theofanis Karaletsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03351">
<title>Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization. (arXiv:2311.03351v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03351</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining offline and online reinforcement learning (RL) is crucial for
efficient and safe learning. However, previous approaches treat offline and
online learning as separate procedures, resulting in redundant designs and
limited performance. We ask: Can we achieve straightforward yet effective
offline and online learning without introducing extra conservatism or
regularization? In this study, we propose Uni-o4, which utilizes an on-policy
objective for both offline and online learning. Owning to the alignment of
objectives in two phases, the RL agent can transfer between offline and online
learning seamlessly. This property enhances the flexibility of the learning
paradigm, allowing for arbitrary combinations of pretraining, fine-tuning,
offline, and online learning. In the offline phase, specifically, Uni-o4
leverages diverse ensemble policies to address the mismatch issues between the
estimated behavior policy and the offline dataset. Through a simple offline
policy evaluation (OPE) approach, Uni-o4 can achieve multi-step policy
improvement safely. We demonstrate that by employing the method above, the
fusion of these two paradigms can yield superior offline initialization as well
as stable and rapid online fine-tuning capabilities. Through real-world robot
tasks, we highlight the benefits of this paradigm for rapid deployment in
challenging, previously unseen real-world environments. Additionally, through
comprehensive evaluations using numerous simulated benchmarks, we substantiate
that our method achieves state-of-the-art performance in both offline and
offline-to-online fine-tuning learning. Our website:
https://lei-kun.github.io/uni-o4/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_K/0/1/0/all/0/1&quot;&gt;Kun Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhengmao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chenhao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kaizhe Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huazhe Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04131">
<title>Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04131</link>
<description rdf:parseType="Literal">&lt;p&gt;While transformer models exhibit strong capabilities on linguistic tasks,
their complex architectures make them difficult to interpret. Recent work has
aimed to reverse engineer transformer models into human-readable
representations called circuits that implement algorithmic functions. We extend
this research by analyzing and comparing circuits for similar sequence
continuation tasks, which include increasing sequences of digits, number words,
and months. Through the application of circuit analysis techniques, we identify
key sub-circuits responsible for detecting sequence members and for predicting
the next member in a sequence. Our analysis reveals that semantically related
sequences rely on shared circuit subgraphs with analogous roles. Overall,
documenting shared computational structures enables better prediction of model
behaviors, identification of errors, and safer editing procedures. This
mechanistic understanding of transformers is a critical step towards building
more robust, aligned, and interpretable language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1&quot;&gt;Michael Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05559">
<title>Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures. (arXiv:2311.05559v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05559</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum computing offers the potential for superior computational
capabilities, particularly for data-intensive tasks. However, the current state
of quantum hardware puts heavy restrictions on input size. To address this,
hybrid transfer learning solutions have been developed, merging pre-trained
classical models, capable of handling extensive inputs, with variational
quantum circuits. Yet, it remains unclear how much each component -- classical
and quantum -- contributes to the model&apos;s results. We propose a novel hybrid
architecture: instead of utilizing a pre-trained network for compression, we
employ an autoencoder to derive a compressed version of the input data. This
compressed data is then channeled through the encoder part of the autoencoder
to the quantum component. We assess our model&apos;s classification capabilities
against two state-of-the-art hybrid transfer learning architectures, two purely
classical architectures and one quantum architecture. Their accuracy is
compared across four datasets: Banknote Authentication, Breast Cancer
Wisconsin, MNIST digits, and AudioMNIST. Our research suggests that classical
components significantly influence classification in hybrid transfer learning,
a contribution often mistakenly ascribed to the quantum element. The
performance of our model aligns with that of a variational quantum circuit
using amplitude embedding, positioning it as a feasible alternative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kolle_M/0/1/0/all/0/1&quot;&gt;Michael K&amp;#xf6;lle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Maurer_J/0/1/0/all/0/1&quot;&gt;Jonas Maurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Altmann_P/0/1/0/all/0/1&quot;&gt;Philipp Altmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sunkel_L/0/1/0/all/0/1&quot;&gt;Leo S&amp;#xfc;nkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Stein_J/0/1/0/all/0/1&quot;&gt;Jonas Stein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1&quot;&gt;Claudia Linnhoff-Popien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05739">
<title>Deep Learning Architecture for Network-Efficiency at the Edge. (arXiv:2311.05739v3 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05739</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing number of AI-driven applications in the mobile devices has led to
solutions that integrate deep learning models with the available edge-cloud
resources; due to multiple benefits such as reduction in on-device energy
consumption, improved latency, improved network usage, and certain privacy
improvements, split learning, where deep learning models are split away from
the mobile device and computed in a distributed manner, has become an
extensively explored topic. Combined with compression-aware methods where
learning adapts to compression of communicated data, the benefits of this
approach have further improved and could serve as an alternative to established
approaches like federated learning methods. In this work, we develop an
adaptive compression-aware split learning method (&apos;deprune&apos;) to improve and
train deep learning models so that they are much more network-efficient (use
less network resources and are faster), which would make them ideal to deploy
in weaker devices with the help of edge-cloud resources. This method is also
extended (&apos;prune&apos;) to very quickly train deep learning models, through a
transfer learning approach, that trades off little accuracy for much more
network-efficient inference abilities. We show that the &apos;deprune&apos; method can
reduce network usage by 4x when compared with a split-learning approach (that
does not use our method) without loss of accuracy, while also improving
accuracy over compression-aware split-learning by 4 percent. Lastly, we show
that the &apos;prune&apos; method can reduce the training time for certain models by up
to 6x without affecting the accuracy when compared against a compression-aware
split-learning approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudvari_A/0/1/0/all/0/1&quot;&gt;Akrit Mudvari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vainio_A/0/1/0/all/0/1&quot;&gt;Antero Vainio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ofeidis_I/0/1/0/all/0/1&quot;&gt;Iason Ofeidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarkoma_S/0/1/0/all/0/1&quot;&gt;Sasu Tarkoma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1&quot;&gt;Leandros Tassiulas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06748">
<title>How do Minimum-Norm Shallow Denoisers Look in Function Space?. (arXiv:2311.06748v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06748</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network (NN) denoisers are an essential building block in many common
tasks, ranging from image reconstruction to image generation. However, the
success of these models is not well understood from a theoretical perspective.
In this paper, we aim to characterize the functions realized by shallow ReLU NN
denoisers -- in the common theoretical setting of interpolation (i.e., zero
training loss) with a minimal representation cost (i.e., minimal $\ell^2$ norm
weights). First, for univariate data, we derive a closed form for the NN
denoiser function, find it is contractive toward the clean data points, and
prove it generalizes better than the empirical MMSE estimator at a low noise
level. Next, for multivariate data, we find the NN denoiser functions in a
closed form under various geometric assumptions on the training data: data
contained in a low-dimensional subspace, data contained in a union of one-sided
rays, or several types of simplexes. These functions decompose into a sum of
simple rank-one piecewise linear interpolations aligned with edges and/or faces
connecting training samples. We empirically verify this alignment phenomenon on
synthetic data and real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeno_C/0/1/0/all/0/1&quot;&gt;Chen Zeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ongie_G/0/1/0/all/0/1&quot;&gt;Greg Ongie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blumenfeld_Y/0/1/0/all/0/1&quot;&gt;Yaniv Blumenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weinberger_N/0/1/0/all/0/1&quot;&gt;Nir Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08724">
<title>Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08724</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a method for knowledge graph construction in power
distribution networks. This method leverages entity features, which involve
their semantic, phonetic, and syntactic characteristics, in both the knowledge
graph of distribution network and the dispatching texts. An enhanced model
based on Convolutional Neural Network, is utilized for effectively matching
dispatch text entities with those in the knowledge graph. The effectiveness of
this model is evaluated through experiments in real-world power distribution
dispatch scenarios. The results indicate that, compared with the baselines, the
proposed model excels in linking a variety of entity types, demonstrating high
overall accuracy in power distribution knowledge graph construction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Che Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09620">
<title>GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection. (arXiv:2311.09620v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09620</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting out-of-distribution (OOD) examples is crucial to guarantee the
reliability and safety of deep neural networks in real-world settings. In this
paper, we offer an innovative perspective on quantifying the disparities
between in-distribution (ID) and OOD data -- analyzing the uncertainty that
arises when models attempt to explain their predictive decisions. This
perspective is motivated by our observation that gradient-based attribution
methods encounter challenges in assigning feature importance to OOD data,
thereby yielding divergent explanation patterns. Consequently, we investigate
how attribution gradients lead to uncertain explanation outcomes and introduce
two forms of abnormalities for OOD detection: the zero-deflation abnormality
and the channel-wise average abnormality. We then propose GAIA, a simple and
effective approach that incorporates Gradient Abnormality Inspection and
Aggregation. The effectiveness of GAIA is validated on both commonly utilized
(CIFAR) and large-scale (ImageNet-1k) benchmarks. Specifically, GAIA reduces
the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to
advanced post-hoc methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinggang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianzong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Jiguang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jing Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11798">
<title>Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods. (arXiv:2311.11798v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11798</link>
<description rdf:parseType="Literal">&lt;p&gt;Partial differential equations are often used in the spatial-temporal
modeling of complex dynamical systems in many engineering applications. In this
work, we build on the recent progress of operator learning and present a
data-driven modeling framework that is continuous in both space and time. A key
feature of the proposed model is the resolution-invariance with respect to both
spatial and temporal discretizations, without demanding abundant training data
in different temporal resolutions. To improve the long-term performance of the
calibrated model, we further propose a hybrid optimization scheme that
leverages both gradient-based and derivative-free optimization methods and
efficiently trains on both short-term time series and long-term statistics. We
investigate the performance of the spatial-temporal continuous learning
framework with three numerical examples, including the viscous Burgers&apos;
equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation.
The results confirm the resolution-invariance of the proposed modeling
framework and also demonstrate stable long-term simulations with only
short-term time series data. In addition, we show that the proposed model can
better predict long-term statistics via the hybrid optimization scheme with a
combined use of short-term and long-term data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chuanqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jin-Long Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12082">
<title>Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight Beamformer for Ultrasound Single-Angle Plane Wave Imaging. (arXiv:2311.12082v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12082</link>
<description rdf:parseType="Literal">&lt;p&gt;Accelerating compute intensive non-real-time beam-forming algorithms in
ultrasound imaging using deep learning architectures has been gaining momentum
in the recent past. Nonetheless, the complexity of the state-of-the-art deep
learning techniques poses challenges for deployment on resource-constrained
edge devices. In this work, we propose a novel vision transformer based tiny
beamformer (Tiny-VBF), which works on the raw radio-frequency channel data
acquired through single-angle plane wave insonification. The output of our
Tiny-VBF provides fast envelope detection requiring very low frame rate, i.e.
0.34 GOPs/Frame for a frame size of 368 x 128 in comparison to the
state-of-the-art deep learning models. It also exhibited an 8% increase in
contrast and gains of 5% and 33% in axial and lateral resolution respectively
when compared to Tiny-CNN on in-vitro dataset. Additionally, our model showed a
4.2% increase in contrast and gains of 4% and 20% in axial and lateral
resolution respectively when compared against conventional Delay-and-Sum (DAS)
beamformer. We further propose an accelerator architecture and implement our
Tiny-VBF model on a Zynq UltraScale+ MPSoC ZCU104 FPGA using a hybrid
quantization scheme with 50% less resource consumption compared to the
floating-point implementation, while preserving the image quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahoof_A/0/1/0/all/0/1&quot;&gt;Abdul Rahoof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chaturvedi_V/0/1/0/all/0/1&quot;&gt;Vivek Chaturvedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1&quot;&gt;Mahesh Raveendranatha Panicker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13326">
<title>Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13326</link>
<description rdf:parseType="Literal">&lt;p&gt;Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1&quot;&gt;Woosung Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1&quot;&gt;Insu Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yuntae Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Gimin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Chang Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15565">
<title>Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15565</link>
<description rdf:parseType="Literal">&lt;p&gt;My research investigates the use of cutting-edge hybrid deep learning models
to accurately differentiate between AI-generated text and human writing. I
applied a robust methodology, utilising a carefully selected dataset comprising
AI and human texts from various sources, each tagged with instructions.
Advanced natural language processing techniques facilitated the analysis of
textual features. Combining sophisticated neural networks, the custom model
enabled it to detect nuanced differences between AI and human content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1&quot;&gt;Abiodun Finbarrs Oketunji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16167">
<title>Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16167</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an end-to-end adaptive sampling neural network
(MMPDE-Net) based on the moving mesh method, which can adaptively generate new
sampling points by solving the moving mesh PDE. This model focuses on improving
the quality of sampling points generation. Moreover, we develop an iterative
algorithm based on MMPDE-Net, which makes the sampling points more precise and
controllable. Since MMPDE-Net is a framework independent of the deep learning
solver, we combine it with physics-informed neural networks (PINN) to propose
moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error
analysis under some assumptions. Finally, we demonstrate the performance
improvement of MS-PINN compared to PINN through numerical experiments of four
typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yangtao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiaolin He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16522">
<title>Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16522</link>
<description rdf:parseType="Literal">&lt;p&gt;To enhance the intelligence degree in operation and maintenance, a novel
method for fault detection in power grids is proposed. The proposed GNN-based
approach first identifies fault nodes through a specialized feature extraction
method coupled with a knowledge graph. By incorporating temporal data, the
method leverages the status of nodes from preceding and subsequent time periods
to help current fault detection. To validate the effectiveness of the node
features, a correlation analysis of the output features from each node was
conducted. The results from experiments show that this method can accurately
locate fault nodes in simulation scenarios with a remarkable accuracy.
Additionally, the graph neural network based feature modeling allows for a
qualitative examination of how faults spread across nodes, which provides
valuable insights for analyzing fault nodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1&quot;&gt;Hao Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Si Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chuanfu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Che Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18744">
<title>$\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks: Benchmarking against Classical Neural Networks. (arXiv:2311.18744v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18744</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive comparative analysis of the performance
of Equivariant Quantum Neural Networks (EQNN) and Quantum Neural Networks
(QNN), juxtaposed against their classical counterparts: Equivariant Neural
Networks (ENN) and Deep Neural Networks (DNN). We evaluate the performance of
each network with two toy examples for a binary classification task, focusing
on model complexity (measured by the number of parameters) and the size of the
training data set. Our results show that the $\mathbb{Z}_2\times \mathbb{Z}_2$
EQNN and the QNN provide superior performance for smaller parameter sets and
modest training data samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhongtian Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cara_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xe7;al Comajoan Cara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dahale_G/0/1/0/all/0/1&quot;&gt;Gopal Ramesh Dahale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Forestano_R/0/1/0/all/0/1&quot;&gt;Roy T. Forestano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gleyzer_S/0/1/0/all/0/1&quot;&gt;Sergei Gleyzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Justice_D/0/1/0/all/0/1&quot;&gt;Daniel Justice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kong_K/0/1/0/all/0/1&quot;&gt;Kyoungchul Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Magorsch_T/0/1/0/all/0/1&quot;&gt;Tom Magorsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Matchev_K/0/1/0/all/0/1&quot;&gt;Konstantin T. Matchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Matcheva_K/0/1/0/all/0/1&quot;&gt;Katia Matcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Unlu_E/0/1/0/all/0/1&quot;&gt;Eyup B. Unlu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01520">
<title>Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian networks (BNs) are a foundational model in machine learning and
causal inference. Their graphical structure can handle high-dimensional
problems, divide them into a sparse collection of smaller ones, underlies Judea
Pearl&apos;s causality, and determines their explainability and interpretability.
Despite their popularity, there are almost no resources in the literature on
how to compute Shannon&apos;s entropy and the Kullback-Leibler (KL) divergence for
BNs under their most common distributional assumptions. In this paper, we
provide computationally efficient algorithms for both by leveraging BNs&apos;
graphical structure, and we illustrate them with a complete set of numerical
examples. In the process, we show it is possible to reduce the computational
complexity of KL from cubic to quadratic for Gaussian BNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02102">
<title>Mitigating Data Injection Attacks on Federated Learning. (arXiv:2312.02102v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02102</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a technique that allows multiple entities to
collaboratively train models using their data without compromising data
privacy. However, despite its advantages, federated learning can be susceptible
to false data injection attacks. In these scenarios, a malicious entity with
control over specific agents in the network can manipulate the learning
process, leading to a suboptimal model. Consequently, addressing these data
injection attacks presents a significant research challenge in federated
learning systems. In this paper, we propose a novel technique to detect and
mitigate data injection attacks on federated learning systems. Our mitigation
method is a local scheme, performed during a single instance of training by the
coordinating node, allowing the mitigation during the convergence of the
algorithm. Whenever an agent is suspected to be an attacker, its data will be
ignored for a certain period, this decision will often be re-evaluated. We
prove that with probability 1, after a finite time, all attackers will be
ignored while the probability of ignoring a trustful agent becomes 0, provided
that there is a majority of truthful agents. Simulations show that when the
coordinating node detects and isolates all the attackers, the model recovers
and converges to the truthful model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalom_O/0/1/0/all/0/1&quot;&gt;Or Shalom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leshem_A/0/1/0/all/0/1&quot;&gt;Amir Leshem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajwa_W/0/1/0/all/0/1&quot;&gt;Waheed U. Bajwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02683">
<title>Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler. (arXiv:2312.02683v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02683</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are a new class of generative models that have recently been
applied to speech enhancement successfully. Previous works have demonstrated
their superior performance in mismatched conditions compared to state-of-the
art discriminative models. However, this was investigated with a single
database for training and another one for testing, which makes the results
highly dependent on the particular databases. Moreover, recent developments
from the image generation literature remain largely unexplored for speech
enhancement. These include several design aspects of diffusion models, such as
the noise schedule or the reverse sampler. In this work, we systematically
assess the generalization performance of a diffusion-based speech enhancement
model by using multiple speech, noise and binaural room impulse response (BRIR)
databases to simulate mismatched acoustic conditions. We also experiment with a
noise schedule and a sampler that have not been applied to speech enhancement
before. We show that the proposed system substantially benefits from using
multiple databases for training, and achieves superior performance compared to
state-of-the-art discriminative models in both matched and mismatched
conditions. We also show that a Heun-based sampler achieves superior
performance at a smaller computational cost compared to a sampler commonly used
for speech enhancement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gonzalez_P/0/1/0/all/0/1&quot;&gt;Philippe Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zheng-Hua Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ostergaard_J/0/1/0/all/0/1&quot;&gt;Jan &amp;#xd8;stergaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jensen_J/0/1/0/all/0/1&quot;&gt;Jesper Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alstrom_T/0/1/0/all/0/1&quot;&gt;Tommy Sonne Alstr&amp;#xf8;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+May_T/0/1/0/all/0/1&quot;&gt;Tobias May&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02708">
<title>Provable Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More. (arXiv:2312.02708v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02708</link>
<description rdf:parseType="Literal">&lt;p&gt;A machine learning model is traditionally considered robust if its prediction
remains (almost) constant under input perturbations with small norm. However,
real-world tasks like molecular property prediction or point cloud segmentation
have inherent equivariances, such as rotation or permutation equivariance. In
such tasks, even perturbations with large norm do not necessarily change an
input&apos;s semantic content. Furthermore, there are perturbations for which a
model&apos;s prediction explicitly needs to change. For the first time, we propose a
sound notion of adversarial robustness that accounts for task equivariance. We
then demonstrate that provable robustness can be achieved by (1) choosing a
model that matches the task&apos;s equivariances (2) certifying traditional
adversarial robustness. Certification methods are, however, unavailable for
many models, such as those with continuous equivariances. We close this gap by
developing the framework of equivariance-preserving randomized smoothing, which
enables architecture-agnostic certification. We additionally derive the first
architecture-specific graph edit distance certificates, i.e. sound robustness
guarantees for isomorphism equivariant tasks like node classification. Overall,
a sound notion of robustness is an important prerequisite for future work at
the intersection of robust and geometric machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1&quot;&gt;Jan Schuchardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1&quot;&gt;Yan Scholten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03006">
<title>Multi-Weight Ranking for Multi-Criteria Decision Making. (arXiv:2312.03006v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03006</link>
<description rdf:parseType="Literal">&lt;p&gt;Cone distribution functions from statistics are turned into Multi-Criteria
Decision Making tools. It is demonstrated that this procedure can be considered
as an upgrade of the weighted sum scalarization insofar as it absorbs a whole
collection of weighted sum scalarizations at once instead of fixing a
particular one in advance. As examples show, this type of scalarization--in
contrast to a pure weighted sum scalarization-is also able to detect
``non-convex&quot; parts of the Pareto frontier. Situations are characterized in
which different types of rank reversal occur, and it is explained why this
might even be useful for analyzing the ranking procedure. The ranking functions
are then extended to sets providing unary indicators for set preferences which
establishes, for the first time, the link between set optimization methods and
set-based multi-objective optimization. A potential application in machine
learning is outlined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamel_A/0/1/0/all/0/1&quot;&gt;Andreas H Hamel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostner_D/0/1/0/all/0/1&quot;&gt;Daniel Kostner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04234">
<title>Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04234</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers, renowned for their self-attention mechanism, have achieved
state-of-the-art performance across various tasks in natural language
processing, computer vision, time-series modeling, etc. However, one of the
challenges with deep Transformer models is the oversmoothing problem, where
representations across layers converge to indistinguishable values, leading to
significant performance degradation. We interpret the original self-attention
as a simple graph filter and redesign it from a graph signal processing (GSP)
perspective. We propose graph-filter-based self-attention (GFSA) to learn a
general yet effective one, whose complexity, however, is slightly larger than
that of the original self-attention mechanism. We demonstrate that GFSA
improves the performance of Transformers in various fields, including computer
vision, natural language processing, graph pattern classification, speech
recognition, and code classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongwhan Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wi_H/0/1/0/all/0/1&quot;&gt;Hyowon Wi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jayoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yehjin Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kookjin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trask_N/0/1/0/all/0/1&quot;&gt;Nathaniel Trask&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Noseong Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04350">
<title>CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04350</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to perform causal reasoning is widely considered a core feature
of intelligence. In this work, we investigate whether large language models
(LLMs) can coherently reason about causality. Much of the existing work in
natural language processing (NLP) focuses on evaluating commonsense causal
reasoning in LLMs, thus failing to assess whether a model can perform causal
inference in accordance with a set of well-defined formal rules. To address
this, we propose a new NLP task, causal inference in natural language, inspired
by the &quot;causal inference engine&quot; postulated by Judea Pearl et al. We compose a
large dataset, CLadder, with 10K samples: based on a collection of causal
graphs and queries (associational, interventional, and counterfactual), we
obtain symbolic questions and ground-truth answers, through an oracle causal
inference engine. These are then translated into natural language. We evaluate
multiple LLMs on our dataset, and we introduce and evaluate a bespoke
chain-of-thought prompting strategy, CausalCoT. We show that our task is highly
challenging for LLMs, and we conduct an in-depth analysis to gain deeper
insights into the causal reasoning abilities of LLMs. Our data is open-sourced
at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found
at https://github.com/causalNLP/cladder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhijing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1&quot;&gt;Felix Leeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gresele_L/0/1/0/all/0/1&quot;&gt;Luigi Gresele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1&quot;&gt;Ojasv Kamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1&quot;&gt;Kevin Blin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adauto_F/0/1/0/all/0/1&quot;&gt;Fernando Gonzalez Adauto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1&quot;&gt;Max Kleiman-Weiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04432">
<title>FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning. (arXiv:2312.04432v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04432</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a collaborative learning paradigm allowing
multiple clients to jointly train a model without sharing their training data.
However, FL is susceptible to poisoning attacks, in which the adversary injects
manipulated model updates into the federated model aggregation process to
corrupt or destroy predictions (untargeted poisoning) or implant hidden
functionalities (targeted poisoning or backdoors). Existing defenses against
poisoning attacks in FL have several limitations, such as relying on specific
assumptions about attack types and strategies or data distributions or not
sufficiently robust against advanced injection techniques and strategies and
simultaneously maintaining the utility of the aggregated model. To address the
deficiencies of existing defenses, we take a generic and completely different
approach to detect poisoning (targeted and untargeted) attacks. We present
FreqFed, a novel aggregation mechanism that transforms the model updates (i.e.,
weights) into the frequency domain, where we can identify the core frequency
components that inherit sufficient information about weights. This allows us to
effectively filter out malicious updates during local training on the clients,
regardless of attack types, strategies, and clients&apos; data distributions. We
extensively evaluate the efficiency and effectiveness of FreqFed in different
application domains, including image classification, word prediction, IoT
intrusion detection, and speech recognition. We demonstrate that FreqFed can
mitigate poisoning attacks effectively with a negligible impact on the utility
of the aggregated model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fereidooni_H/0/1/0/all/0/1&quot;&gt;Hossein Fereidooni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pegoraro_A/0/1/0/all/0/1&quot;&gt;Alessandro Pegoraro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rieger_P/0/1/0/all/0/1&quot;&gt;Phillip Rieger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dmitrienko_A/0/1/0/all/0/1&quot;&gt;Alexandra Dmitrienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_A/0/1/0/all/0/1&quot;&gt;Ahmad-Reza Sadeghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05910">
<title>Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05910</link>
<description rdf:parseType="Literal">&lt;p&gt;The Gaussian process state-space models (GPSSMs) represent a versatile class
of data-driven nonlinear dynamical system models. However, the presence of
numerous latent variables in GPSSM incurs unresolved issues for existing
variational inference approaches, particularly under the more realistic
non-mean-field (NMF) assumption, including extensive training effort,
compromised inference accuracy, and infeasibility for online applications,
among others. In this paper, we tackle these challenges by incorporating the
ensemble Kalman filter (EnKF), a well-established model-based filtering
technique, into the NMF variational inference framework to approximate the
posterior distribution of the latent states. This novel marriage between EnKF
and GPSSM not only eliminates the need for extensive parameterization in
learning variational distributions, but also enables an interpretable,
closed-form approximation of the evidence lower bound (ELBO). Moreover, owing
to the streamlined parameterization via the EnKF, the new GPSSM model can be
easily accommodated in online learning applications. We demonstrate that the
resulting EnKF-aided online algorithm embodies a principled objective function
by ensuring data-fitting accuracy while incorporating model regularizations to
mitigate overfitting. We also provide detailed analysis and fresh insights for
the proposed algorithms. Comprehensive evaluation across diverse real and
synthetic datasets corroborates the superior learning and inference performance
of our EnKF-aided variational inference algorithms compared to existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhidi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiyong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Feng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiery_A/0/1/0/all/0/1&quot;&gt;Alexandre Hoang Thi&amp;#xe9;ry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07586">
<title>Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07586</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a sampling method that provides first-principle
non-linear correction for classifier-free guided DDPMs. Such correction forces
the guided DDPMs to respect the Fokker-Planck equation of their underlying
diffusion process, in a way that is training-free, derivative-free, and
compatible with existing sampling methods. Experiments show that characteristic
guidance enhances control and reduces color and exposure issues in image
generation, proving effective in diverse applications ranging from latent space
sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Candi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yuan Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07899">
<title>Morphological Profiling for Drug Discovery in the Era of Deep Learning. (arXiv:2312.07899v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07899</link>
<description rdf:parseType="Literal">&lt;p&gt;Morphological profiling is a valuable tool in phenotypic drug discovery. The
advent of high-throughput automated imaging has enabled the capturing of a wide
range of morphological features of cells or organisms in response to
perturbations at the single-cell resolution. Concurrently, significant advances
in machine learning and deep learning, especially in computer vision, have led
to substantial improvements in analyzing large-scale high-content images at
high-throughput. These efforts have facilitated understanding of compound
mechanism-of-action (MOA), drug repurposing, characterization of cell
morphodynamics under perturbation, and ultimately contributing to the
development of novel therapeutics. In this review, we provide a comprehensive
overview of the recent advances in the field of morphological profiling. We
summarize the image profiling analysis workflow, survey a broad spectrum of
analysis strategies encompassing feature engineering- and deep learning-based
approaches, and introduce publicly available benchmark datasets. We place a
particular emphasis on the application of deep learning in this pipeline,
covering cell segmentation, image representation learning, and multimodal
learning. Additionally, we illuminate the application of morphological
profiling in phenotypic drug discovery and highlight potential challenges and
opportunities in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qiaosi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ratnayake_R/0/1/0/all/0/1&quot;&gt;Ranjala Ratnayake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Seabra_G/0/1/0/all/0/1&quot;&gt;Gustavo Seabra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhe Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fang_R/0/1/0/all/0/1&quot;&gt;Ruogu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Lina Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yousong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kahveci_T/0/1/0/all/0/1&quot;&gt;Tamer Kahveci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luesch_H/0/1/0/all/0/1&quot;&gt;Hendrik Luesch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanjun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08723">
<title>StemGen: A music generation model that listens. (arXiv:2312.08723v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08723</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end generation of musical audio using deep learning techniques has
seen an explosion of activity recently. However, most models concentrate on
generating fully mixed music in response to abstract conditioning information.
In this work, we present an alternative paradigm for producing music generation
models that can listen and respond to musical context. We describe how such a
model can be constructed using a non-autoregressive, transformer-based model
architecture and present a number of novel architectural and sampling
improvements. We train the described architecture on both an open-source and a
proprietary dataset. We evaluate the produced models using standard quality
metrics and a new approach based on music information retrieval descriptors.
The resulting model reaches the audio quality of state-of-the-art
text-conditioned models, as well as exhibiting strong musical coherence with
its context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_J/0/1/0/all/0/1&quot;&gt;Julian D. Parker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spijkervet_J/0/1/0/all/0/1&quot;&gt;Janne Spijkervet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosta_K/0/1/0/all/0/1&quot;&gt;Katerina Kosta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yesiler_F/0/1/0/all/0/1&quot;&gt;Furkan Yesiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_B/0/1/0/all/0/1&quot;&gt;Boris Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Ju-Chiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avent_M/0/1/0/all/0/1&quot;&gt;Matt Avent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jitong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1&quot;&gt;Duc Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14507">
<title>Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and Spectral Optimal Transport. (arXiv:2312.14507v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14507</link>
<description rdf:parseType="Literal">&lt;p&gt;In neural audio signal processing, pitch conditioning has been used to
enhance the performance of synthesizers. However, jointly training pitch
estimators and synthesizers is a challenge when using standard audio-to-audio
reconstruction loss, leading to reliance on external pitch trackers. To address
this issue, we propose using a spectral loss function inspired by optimal
transportation theory that minimizes the displacement of spectral energy. We
validate this approach through an unsupervised autoencoding task that fits a
harmonic template to harmonic signals. We jointly estimate the fundamental
frequency and amplitudes of harmonics using a lightweight encoder and
reconstruct the signals using a differentiable harmonic synthesizer. The
proposed approach offers a promising direction for improving unsupervised
parameter estimation in neural audio applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torres_B/0/1/0/all/0/1&quot;&gt;Bernardo Torres&lt;/a&gt; (S2A, IDS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peeters_G/0/1/0/all/0/1&quot;&gt;Geoffroy Peeters&lt;/a&gt; (S2A, IDS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;l Richard&lt;/a&gt; (S2A, IDS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14698">
<title>Time-changed normalizing flows for accurate SDE modeling. (arXiv:2312.14698v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14698</link>
<description rdf:parseType="Literal">&lt;p&gt;The generative paradigm has become increasingly important in machine learning
and deep learning models. Among popular generative models are normalizing
flows, which enable exact likelihood estimation by transforming a base
distribution through diffeomorphic transformations. Extending the normalizing
flow framework to handle time-indexed flows gave dynamic normalizing flows, a
powerful tool to model time series, stochastic processes, and neural stochastic
differential equations (SDEs). In this work, we propose a novel variant of
dynamic normalizing flows, a Time Changed Normalizing Flow (TCNF), based on
time deformation of a Brownian motion which constitutes a versatile and
extensive family of Gaussian processes. This approach enables us to effectively
model some SDEs, that cannot be modeled otherwise, including standard ones such
as the well-known Ornstein-Uhlenbeck process, and generalizes prior
methodologies, leading to improved results and better inference and prediction
capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekri_N/0/1/0/all/0/1&quot;&gt;Naoufal El Bekri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drumetz_L/0/1/0/all/0/1&quot;&gt;Lucas Drumetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vermet_F/0/1/0/all/0/1&quot;&gt;Franck Vermet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14972">
<title>A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production. (arXiv:2312.14972v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14972</link>
<description rdf:parseType="Literal">&lt;p&gt;Many companies rely on APIs of managed AI models such as OpenAI&apos;s GPT-4 to
create AI-enabled experiences in their products. Along with the benefits of
ease of use and shortened time to production, this reliance on proprietary APIs
has downsides in terms of model control, performance reliability, up-time
predictability, and cost. At the same time, there has been a flurry of open
source small language models (SLMs) that have been made available for
commercial use. However, their readiness to replace existing capabilities
remains unclear, and a systematic approach to test these models is not readily
available. In this paper, we present a systematic evaluation methodology for,
and characterization of, modern open source SLMs and their trade-offs when
replacing a proprietary LLM APIs for a real-world product feature. We have
designed SLaM, an automated analysis tool that enables the quantitative and
qualitative testing of product features utilizing arbitrary SLMs. Using SLaM,
we examine both the quality and the performance characteristics of modern SLMs
relative to an existing customer-facing OpenAI-based implementation. We find
that across 9 SLMs and 29 variants, we observe competitive quality-of-results
for our use case, significant performance consistency improvement, and a cost
reduction of 5x-29x when compared to OpenAI GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irugalbandara_C/0/1/0/all/0/1&quot;&gt;Chandra Irugalbandara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendra_A/0/1/0/all/0/1&quot;&gt;Ashish Mahendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daynauth_R/0/1/0/all/0/1&quot;&gt;Roland Daynauth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arachchige_T/0/1/0/all/0/1&quot;&gt;Tharuka Kasthuri Arachchige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flautner_K/0/1/0/all/0/1&quot;&gt;Krisztian Flautner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Lingjia Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yiping Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1&quot;&gt;Jason Mars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15551">
<title>Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15551</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the recent empirical success of incorporating public data into
differentially private learning, we theoretically investigate how a shared
representation learned from public data can improve private learning. We
explore two common scenarios of transfer learning for linear regression, both
of which assume the public and private tasks (regression vectors) share a
low-rank subspace in a high-dimensional space. In the first single-task
transfer scenario, the goal is to learn a single model shared across all users,
each corresponding to a row in a dataset. We provide matching upper and lower
bounds showing that our algorithm achieves the optimal excess risk within a
natural class of algorithms that search for the linear model within the given
subspace estimate. In the second scenario of multitask model personalization,
we show that with sufficient public data, users can avoid private coordination,
as purely local learning within the given subspace achieves the same utility.
Taken together, our results help to characterize the benefits of public data
across common regimes of private transfer learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thaker_P/0/1/0/all/0/1&quot;&gt;Pratiksha Thaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1&quot;&gt;Amrith Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1&quot;&gt;Virginia Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15965">
<title>Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15965</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network(DNN) generalization is limited by the over-reliance of
current offline reinforcement learning techniques on conservative processing of
existing datasets. This method frequently results in algorithms that settle for
suboptimal solutions that only adjust to a certain dataset. Similarly, in
online reinforcement learning, the previously imposed punitive pessimism also
deprives the model of its exploratory potential. Our research proposes a novel
framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL).
OPARL employs a unique dual-actor approach: an optimistic actor dedicated to
exploration and a pessimistic actor focused on utilization, thereby effectively
differentiating between exploration and utilization strategies. This unique
combination in reinforcement learning methods fosters a more balanced and
efficient approach. It enables the optimization of policies that focus on
actions yielding high rewards through pessimistic utilization strategies, while
also ensuring extensive state coverage via optimistic exploration. Experiments
and theoretical study demonstrates OPARL improves agents&apos; capacities for
application and exploration. In the most tasks of DMControl benchmark and
Mujoco environment, OPARL performed better than state-of-the-art methods. Our
code has released on https://github.com/yydsok/OPARL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingpu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qirui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Helin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zirui Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Miao Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16895">
<title>RLPlanner: Reinforcement Learning based Floorplanning for Chiplets with Fast Thermal Analysis. (arXiv:2312.16895v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16895</link>
<description rdf:parseType="Literal">&lt;p&gt;Chiplet-based systems have gained significant attention in recent years due
to their low cost and competitive performance. As the complexity and
compactness of a chiplet-based system increase, careful consideration must be
given to microbump assignments, interconnect delays, and thermal limitations
during the floorplanning stage. This paper introduces RLPlanner, an efficient
early-stage floorplanning tool for chiplet-based systems with a novel fast
thermal evaluation method. RLPlanner employs advanced reinforcement learning to
jointly minimize total wirelength and temperature. To alleviate the
time-consuming thermal calculations, RLPlanner incorporates the developed fast
thermal evaluation method to expedite the iterations and optimizations.
Comprehensive experiments demonstrate that our proposed fast thermal evaluation
method achieves a mean absolute error (MAE) of 0.25 K and delivers over 120x
speed-up compared to the open-source thermal solver HotSpot. When integrated
with our fast thermal evaluation method, RLPlanner achieves an average
improvement of 20.28\% in minimizing the target objective (a combination of
wirelength and temperature), within a similar running time, compared to the
classic simulated annealing method with HotSpot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xingchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiping Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hanming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Leilai Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaolei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17482">
<title>MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. (arXiv:2312.17482v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17482</link>
<description rdf:parseType="Literal">&lt;p&gt;Although BERT-style encoder models are heavily used in NLP research, many
researchers do not pretrain their own BERTs from scratch due to the high cost
of training. In the past half-decade since BERT first rose to prominence, many
advances have been made with other transformer architectures and training
configurations that have yet to be systematically incorporated into BERT. Here,
we introduce MosaicBERT, a BERT-style encoder architecture and training recipe
that is empirically optimized for fast pretraining. This efficient architecture
incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear
Units (GLU), a module to dynamically remove padded tokens, and low precision
LayerNorm into the classic transformer encoder block. The training recipe
includes a 30% masking ratio for the Masked Language Modeling (MLM) objective,
bfloat16 precision, and vocabulary size optimized for GPU throughput, in
addition to best-practices from RoBERTa and other encoder models. When
pretrained from scratch on the C4 dataset, this base model achieves a
downstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs
at a cost of roughly $20. We plot extensive accuracy vs. pretraining speed
Pareto curves and show that MosaicBERT base and large are consistently Pareto
optimal when compared to a competitive BERT base and large. This empirical
speed up in pretraining enables researchers and engineers to pretrain custom
BERT-style models at low cost instead of finetune on existing generic models.
We open source our model weights and code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portes_J/0/1/0/all/0/1&quot;&gt;Jacob Portes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1&quot;&gt;Alex Trott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havens_S/0/1/0/all/0/1&quot;&gt;Sam Havens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_D/0/1/0/all/0/1&quot;&gt;Daniel King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venigalla_A/0/1/0/all/0/1&quot;&gt;Abhinav Venigalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadeem_M/0/1/0/all/0/1&quot;&gt;Moin Nadeem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sardana_N/0/1/0/all/0/1&quot;&gt;Nikhil Sardana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khudia_D/0/1/0/all/0/1&quot;&gt;Daya Khudia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1&quot;&gt;Jonathan Frankle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00744">
<title>Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v4 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00744</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning for Hamiltonian regression of quantum systems in material
research necessitates satisfying the covariance laws, among which achieving
SO(3)-equivariance without sacrificing the expressiveness capability of
networks remains an elusive challenge due to the restriction to non-linear
mappings on guaranteeing theoretical equivariance. To alleviate the
covariance-expressiveness dilemma, we propose a hybrid framework with two
cascaded regression stages. The first stage, i.e., a theoretically-guaranteed
covariant neural network modeling symmetry properties of 3D atom systems,
predicts baseline Hamiltonians with theoretically covariant features extracted,
assisting the second stage in learning covariance. Meanwhile, the second stage,
powered by a non-linear 3D graph Transformer network we propose for structural
modeling of atomic systems, refines the first stage&apos;s output as a fine-grained
prediction of Hamiltonians with better expressiveness capability. The
combination of a theoretically covariant yet inevitably less expressive model
with a highly expressive non-linear network enables precise, generalizable
predictions while maintaining robust covariance under coordinate
transformations. Our method achieves state-of-the-art performance in
Hamiltonian prediction for electronic structure calculations, confirmed through
experiments on six crystalline material databases. The codes and configuration
scripts are available in the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yin_S/0/1/0/all/0/1&quot;&gt;Shi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xinyang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xudong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tianyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haochong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lixin He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00773">
<title>Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00773</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic mixture models are acknowledged as a valuable tool for
unsupervised outlier detection owing to their interpretability and intuitive
grounding in statistical principles. Within this framework, Dirichlet process
mixture models emerge as a compelling alternative to conventional finite
mixture models for both clustering and outlier detection tasks. However,
despite their evident advantages, the widespread adoption of Dirichlet process
mixture models in unsupervised outlier detection has been hampered by
challenges related to computational inefficiency and sensitivity to outliers
during the construction of detectors. To tackle these challenges, we propose a
novel outlier detection method based on ensembles of Dirichlet process Gaussian
mixtures. The proposed method is a fully unsupervised algorithm that
capitalizes on random subspace and subsampling ensembles, not only ensuring
efficient computation but also enhancing the robustness of the resulting
outlier detector. Moreover, the proposed method leverages variational inference
for Dirichlet process mixtures to ensure efficient and fast computation.
Empirical studies with benchmark datasets demonstrate that our method
outperforms existing approaches for unsupervised outlier detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongwook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Juyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hee Cheol Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Seonghyun Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00828">
<title>Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00828</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of sampling discrete field configurations $\phi$ from
the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the
lattice-discretization of the continuous Euclidean action $\mathcal S$ of some
quantum field theory. Since such densities arise as the approximation of the
underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal
S[\phi(x)]}$, we frame the task as an instance of operator learning. In
particular, we propose to approximate a time-dependent operator $\mathcal V_t$
whose time integral provides a mapping between the functional distributions of
the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal
S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal
Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the
operator $\mathcal V_t$ can be discretized to a finite dimensional,
time-dependent vector field $V_t$ which in turn induces a continuous
normalizing flow between finite dimensional distributions over the chosen
lattice. This flow can then be trained to be a diffeormorphism between the
discretized free and target theories $[d\phi] Z_0^{-1} e^{-S_{0}[\phi]}$,
$[d\phi] Z^{-1}e^{-S[\phi]}$. We run experiments on the $\phi^4$-theory to
explore to what extent such operator-based flow architectures generalize to
lattice sizes they were not trained on and show that pretraining on smaller
lattices can lead to speedup over training only a target lattice size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mate_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint M&amp;#xe1;t&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Fleuret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00910">
<title>WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00910</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion segmentation is a complex yet indispensable task in autonomous
driving. The challenges introduced by the ego-motion of the cameras, radial
distortion in fisheye lenses, and the need for temporal consistency make the
task more complicated, rendering traditional and standard Convolutional Neural
Network (CNN) approaches less effective. The consequent laborious data
labeling, representation of diverse and uncommon scenarios, and extensive data
capture requirements underscore the imperative of synthetic data for improving
machine learning model performance. To this end, we employ the PD-WoodScape
synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye
dataset. Thus, we present the WoodScape fisheye motion segmentation challenge
for autonomous driving, held as part of the CVPR 2023 Workshop on
Omnidirectional Computer Vision (OmniCV). As one of the first competitions
focused on fisheye motion segmentation, we aim to explore and evaluate the
potential and impact of utilizing synthetic data in this domain. In this paper,
we provide a detailed analysis on the competition which attracted the
participation of 112 global teams and a total of 234 submissions. This study
delineates the complexities inherent in the task of motion segmentation,
emphasizes the significance of fisheye datasets, articulate the necessity for
synthetic datasets and the resultant domain gap they engender, outlining the
foundational blueprint for devising successful solutions. Subsequently, we
delve into the details of the baseline experiments and winning methods
evaluating their qualitative and quantitative results, providing with useful
insights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1&quot;&gt;Saravanabalagi Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cibik_N/0/1/0/all/0/1&quot;&gt;Nathaniel Cibik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1&quot;&gt;John McDonald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01326">
<title>An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01326</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1&quot;&gt;Urchade Zaratiana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1&quot;&gt;Nadi Tomeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1&quot;&gt;Pierre Holat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1&quot;&gt;Thierry Charnois&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01841">
<title>Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01841</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental (and largely open) challenge in sequential decision-making is
dealing with non-stationary environments, where exogenous environmental
conditions change over time. Such problems are traditionally modeled as
non-stationary Markov decision processes (NSMDP). However, existing approaches
for decision-making in NSMDPs have two major shortcomings: first, they assume
that the updated environmental dynamics at the current time are known (although
future dynamics can change); and second, planning is largely pessimistic, i.e.,
the agent acts ``safely&apos;&apos; to account for the non-stationary evolution of the
environment. We argue that both these assumptions are invalid in practice --
updated environmental conditions are rarely known, and as the agent interacts
with the environment, it can learn about the updated dynamics and avoid being
pessimistic, at least in states whose dynamics it is confident about. We
present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree
Search (ADA-MCTS)} that addresses these challenges. We show that the agent can
learn the updated dynamics of the environment over time and then act as it
learns, i.e., if the agent is in a region of the state space about which it has
updated knowledge, it can avoid being pessimistic. To quantify ``updated
knowledge,&apos;&apos; we disintegrate the aleatoric and epistemic uncertainty in the
agent&apos;s updated belief and show how the agent can use these estimates for
decision-making. We compare the proposed approach with the multiple
state-of-the-art approaches in decision-making across multiple well-established
open-source problems and empirically show that our approach is faster and
highly adaptive without sacrificing safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Baiting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02333">
<title>Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02333</link>
<description rdf:parseType="Literal">&lt;p&gt;The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allu_U/0/1/0/all/0/1&quot;&gt;Uday Allu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_B/0/1/0/all/0/1&quot;&gt;Biddwan Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripathi_V/0/1/0/all/0/1&quot;&gt;Vishesh Tripathi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02810">
<title>Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02810</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural network (PINN) is a data-driven solver for partial
and ordinary differential equations(ODEs/PDEs). It provides a unified framework
to address both forward and inverse problems. However, the complexity of the
objective function often leads to training failures. This issue is particularly
prominent when solving high-frequency and multi-scale problems. We proposed
using transfer learning to boost the robustness and convergence of training
PINN, starting training from low-frequency problems and gradually approaching
high-frequency problems. Through two case studies, we discovered that transfer
learning can effectively train PINN to approximate solutions from low-frequency
problems to high-frequency problems without increasing network parameters.
Furthermore, it requires fewer data points and less training time. We
elaborately described our training strategy, including optimizer selection, and
suggested guidelines for using transfer learning to train neural networks for
solving more complex problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustajab_A/0/1/0/all/0/1&quot;&gt;Abdul Hannan Mustajab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Hao Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizvi_Z/0/1/0/all/0/1&quot;&gt;Zarghaam Rizvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wuttke_F/0/1/0/all/0/1&quot;&gt;Frank Wuttke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04374">
<title>Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04374</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the complexity and lack of transparency in deep neural networks (DNNs),
extensive efforts have been made to make these systems more interpretable or
explain their behaviors in accessible terms. Unlike most reviews, which focus
on algorithmic and model-centric perspectives, this work takes a &quot;data-centric&quot;
view, examining how data collection, processing, and analysis contribute to
explainable AI (XAI). We categorize existing work into three categories subject
to their purposes: interpretations of deep models, referring to feature
attributions and reasoning processes that correlate data points with model
outputs; influences of training data, examining the impact of training data
nuances, such as data valuation and sample anomalies, on decision-making
processes; and insights of domain knowledge, discovering latent patterns and
fostering new knowledge from data and models to advance social values and
scientific discovery. Specifically, we distill XAI methodologies into data
mining operations on training and testing data across modalities, such as
images, text, and tabular data, as well as on training logs, checkpoints,
models and other DNN behavior descriptors. In this way, our study offers a
comprehensive, data-centric examination of XAI from a lens of data mining
methods and applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiamin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinhao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zeyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04464">
<title>PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04464</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive amounts of unlabelled data are captured by Earth Observation (EO)
satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.
This makes Remote Sensing a data-rich domain well suited to Machine Learning
(ML) solutions. However, a bottleneck in applying ML models to EO is the lack
of annotated data as annotation is a labour-intensive and costly process. As a
result, research in this domain has focused on Self-Supervised Learning and
Foundation Model approaches. This paper addresses the need to evaluate
different Foundation Models on a fair and uniform benchmark by introducing the
PhilEO Bench, a novel evaluation framework for EO Foundation Models. The
framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset
containing labels for three downstream tasks, building density estimation, road
segmentation, and land cover classification. We present experiments using our
framework evaluating different Foundation Models, including Prithvi and SatMAE,
at multiple n-shots and convergence rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fibaek_C/0/1/0/all/0/1&quot;&gt;Casper Fibaek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilleri_L/0/1/0/all/0/1&quot;&gt;Luke Camilleri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luyts_A/0/1/0/all/0/1&quot;&gt;Andreas Luyts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Dionelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1&quot;&gt;Bertrand Le Saux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05394">
<title>Iterative Regularization with k-support Norm: An Important Complement to Sparse Recovery. (arXiv:2401.05394v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05394</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse recovery is ubiquitous in machine learning and signal processing. Due
to the NP-hard nature of sparse recovery, existing methods are known to suffer
either from restrictive (or even unknown) applicability conditions, or high
computational cost. Recently, iterative regularization methods have emerged as
a promising fast approach because they can achieve sparse recovery in one pass
through early stopping, rather than the tedious grid-search used in the
traditional methods. However, most of those iterative methods are based on the
$\ell_1$ norm which requires restrictive applicability conditions and could
fail in many cases. Therefore, achieving sparse recovery with iterative
regularization methods under a wider range of conditions has yet to be further
explored. To address this issue, we propose a novel iterative regularization
algorithm, IRKSN, based on the $k$-support norm regularizer rather than the
$\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and
compare them with traditional conditions for recovery with $\ell_1$ norm
regularizers. Additionally, we give an early stopping bound on the model error
of IRKSN with explicit constants, achieving the standard linear rate for sparse
recovery. Finally, we illustrate the applicability of our algorithm on several
experiments, including a support recovery experiment with a correlated design
matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vazelhes_W/0/1/0/all/0/1&quot;&gt;William de Vazelhes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mukhoty_B/0/1/0/all/0/1&quot;&gt;Bhaskar Mukhoty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiao-Tong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bin Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17816">
<title>Fixed point actions from convolutional neural networks. (arXiv:2311.17816v1 [hep-lat] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.17816</link>
<description rdf:parseType="Literal">&lt;p&gt;Lattice gauge-equivariant convolutional neural networks (L-CNNs) can be used
to form arbitrarily shaped Wilson loops and can approximate any gauge-covariant
or gauge-invariant function on the lattice. Here we use L-CNNs to describe
fixed point (FP) actions which are based on renormalization group
transformations. FP actions are classically perfect, i.e., they have no lattice
artifacts on classical gauge-field configurations satisfying the equations of
motion, and therefore possess scale invariant instanton solutions. FP actions
are tree-level Symanzik-improved to all orders in the lattice spacing and can
produce physical predictions with very small lattice artifacts even on coarse
lattices. We find that L-CNNs are much more accurate at parametrizing the FP
action compared to older approaches. They may therefore provide a way to
circumvent critical slowing down and topological freezing towards the continuum
limit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-lat/1/au:+Holland_K/0/1/0/all/0/1&quot;&gt;Kieran Holland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-lat/1/au:+Ipp_A/0/1/0/all/0/1&quot;&gt;Andreas Ipp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-lat/1/au:+Muller_D/0/1/0/all/0/1&quot;&gt;David I. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-lat/1/au:+Wenger_U/0/1/0/all/0/1&quot;&gt;Urs Wenger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14129">
<title>WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.14129</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly evolving healthcare industry, platforms now have access to not
only traditional medical records, but also diverse data sets encompassing
various patient interactions, such as those from healthcare web portals. To
address this rich diversity of data, we introduce WellFactor: a method that
derives patient profiles by integrating information from these sources. Central
to our approach is the utilization of constrained low-rank approximation.
WellFactor is optimized to handle the sparsity that is often inherent in
healthcare data. Moreover, by incorporating task-specific label information,
our method refines the embedding results, offering a more informed perspective
on patients. One important feature of WellFactor is its ability to compute
embeddings for new, previously unobserved patient data instantaneously,
eliminating the need to revisit the entire data set or recomputing the
embedding. Comprehensive evaluations on real-world healthcare data demonstrate
WellFactor&apos;s effectiveness. It produces better results compared to other
existing methods in classification performance, yields meaningful clustering of
patients, and delivers consistent results in patient similarity searches and
predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dongjin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1&quot;&gt;Andy Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozturk_O/0/1/0/all/0/1&quot;&gt;Ozgur Ozturk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_D/0/1/0/all/0/1&quot;&gt;Deep Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drake_B/0/1/0/all/0/1&quot;&gt;Barry Drake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haidarian_H/0/1/0/all/0/1&quot;&gt;Hamid Haidarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_F/0/1/0/all/0/1&quot;&gt;Faizan Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Haesun Park&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>