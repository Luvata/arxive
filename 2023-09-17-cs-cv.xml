<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-09-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.08790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.08797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.01210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06987" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2309.07173">
<title>Using Unsupervised and Supervised Learning and Digital Twin for Deep Convective Ice Storm Classification. (arXiv:2309.07173v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07173</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart Ice Cloud Sensing (SMICES) is a small-sat concept in which a primary
radar intelligently targets ice storms based on information collected by a
lookahead radiometer. Critical to the intelligent targeting is accurate
identification of storm/cloud types from eight bands of radiance collected by
the radiometer. The cloud types of interest are: clear sky, thin cirrus,
cirrus, rainy anvil, and convection core.
&lt;/p&gt;
&lt;p&gt;We describe multi-step use of Machine Learning and Digital Twin of the
Earth&apos;s atmosphere to derive such a classifier. First, a digital twin of
Earth&apos;s atmosphere called a Weather Research Forecast (WRF) is used generate
simulated lookahead radiometer data as well as deeper &quot;science&quot; hidden
variables. The datasets simulate a tropical region over the Caribbean and a
non-tropical region over the Atlantic coast of the United States. A K-means
clustering over the scientific hidden variables was utilized by human experts
to generate an automatic labelling of the data - mapping each physical data
point to cloud types by scientists informed by mean/centroids of hidden
variables of the clusters. Next, classifiers were trained with the inputs of
the simulated radiometer data and its corresponding label. The classifiers of a
random decision forest (RDF), support vector machine (SVM), Gaussian na\&quot;ive
bayes, feed forward artificial neural network (ANN), and a convolutional neural
network (CNN) were trained. Over the tropical dataset, the best performing
classifier was able to identify non-storm and storm clouds with over 80%
accuracy in each class for a held-out test set. Over the non-tropical dataset,
the best performing classifier was able to classify non-storm clouds with over
90% accuracy and storm clouds with over 40% accuracy. Additionally both sets of
classifiers were shown to be resilient to instrument noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swope_J/0/1/0/all/0/1&quot;&gt;Jason Swope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1&quot;&gt;Steve Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunkel_E/0/1/0/all/0/1&quot;&gt;Emily Dunkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosch_Lluis_X/0/1/0/all/0/1&quot;&gt;Xavier Bosch-Lluis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Q/0/1/0/all/0/1&quot;&gt;Qing Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deal_W/0/1/0/all/0/1&quot;&gt;William Deal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07186">
<title>LCReg: Long-Tailed Image Classification with Latent Categories based Recognition. (arXiv:2309.07186v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07186</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we tackle the challenging problem of long-tailed image
recognition. Previous long-tailed recognition approaches mainly focus on data
augmentation or re-balancing strategies for the tail classes to give them more
attention during model training. However, these methods are limited by the
small number of training images for the tail classes, which results in poor
feature representations. To address this issue, we propose the Latent
Categories based long-tail Recognition (LCReg) method. Our hypothesis is that
common latent features shared by head and tail classes can be used to improve
feature representation. Specifically, we learn a set of class-agnostic latent
features shared by both head and tail classes, and then use semantic data
augmentation on the latent features to implicitly increase the diversity of the
training sample. We conduct extensive experiments on five long-tailed image
recognition datasets, and the results show that our proposed method
significantly improves the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weide Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhonghua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fayao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07243">
<title>LInKs &quot;Lifting Independent Keypoints&quot; -- Partial Pose Lifting for Occlusion Handling with Improved Accuracy in 2D-3D Human Pose Estimation. (arXiv:2309.07243v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07243</link>
<description rdf:parseType="Literal">&lt;p&gt;We present LInKs, a novel unsupervised learning method to recover 3D human
poses from 2D kinematic skeletons obtained from a single image, even when
occlusions are present. Our approach follows a unique two-step process, which
involves first lifting the occluded 2D pose to the 3D domain, followed by
filling in the occluded parts using the partially reconstructed 3D coordinates.
This lift-then-fill approach leads to significantly more accurate results
compared to models that complete the pose in 2D space alone. Additionally, we
improve the stability and likelihood estimation of normalising flows through a
custom sampling function replacing PCA dimensionality reduction previously used
in prior work. Furthermore, we are the first to investigate if different parts
of the 2D kinematic skeleton can be lifted independently which we find by
itself reduces the error of current lifting approaches. We attribute this to
the reduction of long-range keypoint correlations. In our detailed evaluation,
we quantify the error under various realistic occlusion scenarios, showcasing
the versatility and applicability of our model. Our results consistently
demonstrate the superiority of handling all types of occlusions in 3D space
when compared to others that complete the pose in 2D space. Our approach also
exhibits consistent accuracy in scenarios without occlusion, as evidenced by a
7.9% reduction in reconstruction error compared to prior works on the Human3.6M
dataset. Furthermore, our method excels in accurately retrieving complete 3D
poses even in the presence of occlusions, making it highly applicable in
situations where complete 2D pose information is unavailable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardy_P/0/1/0/all/0/1&quot;&gt;Peter Hardy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hansung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07254">
<title>Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement. (arXiv:2309.07254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07254</link>
<description rdf:parseType="Literal">&lt;p&gt;While diffusion models demonstrate a remarkable capability for generating
high-quality images, their tendency to `replicate&apos; training data raises privacy
concerns. Although recent research suggests that this replication may stem from
the insufficient generalization of training data captions and duplication of
training images, effective mitigation strategies remain elusive. To address
this gap, our paper first introduces a generality score that measures the
caption generality and employ large language model (LLM) to generalize training
captions. Subsequently, we leverage generalized captions and propose a novel
dual fusion enhancement approach to mitigate the replication of diffusion
models. Our empirical results demonstrate that our proposed methods can
significantly reduce replication by 43.5% compared to the original diffusion
model while maintaining the diversity and quality of generations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dake Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1&quot;&gt;Peter A. Beerel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07255">
<title>Automated segmentation of rheumatoid arthritis immunohistochemistry stained synovial tissue. (arXiv:2309.07255v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.07255</link>
<description rdf:parseType="Literal">&lt;p&gt;Rheumatoid Arthritis (RA) is a chronic, autoimmune disease which primarily
affects the joint&apos;s synovial tissue. It is a highly heterogeneous disease, with
wide cellular and molecular variability observed in synovial tissues. Over the
last two decades, the methods available for their study have advanced
considerably. In particular, Immunohistochemistry stains are well suited to
highlighting the functional organisation of samples. Yet, analysis of
IHC-stained synovial tissue samples is still overwhelmingly done manually and
semi-quantitatively by expert pathologists. This is because in addition to the
fragmented nature of IHC stained synovial tissue, there exist wide variations
in intensity and colour, strong clinical centre batch effect, as well as the
presence of many undesirable artefacts present in gigapixel Whole Slide Images
(WSIs), such as water droplets, pen annotation, folded tissue, blurriness, etc.
There is therefore a strong need for a robust, repeatable automated tissue
segmentation algorithm which can cope with this variability and provide support
to imaging pipelines. We train a UNET on a hand-curated, heterogeneous
real-world multi-centre clinical dataset R4RA, which contains multiple types of
IHC staining. The model obtains a DICE score of 0.865 and successfully segments
different types of IHC staining, as well as dealing with variance in colours,
intensity and common WSIs artefacts from the different clinical centres. It can
be used as the first step in an automated image analysis pipeline for synovial
tissue samples stained with IHC, increasing speed, reproducibility and
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gallagher_Syed_A/0/1/0/all/0/1&quot;&gt;Amaya Gallagher-Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Abbas Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rivellese_F/0/1/0/all/0/1&quot;&gt;Felice Rivellese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pitzalis_C/0/1/0/all/0/1&quot;&gt;Costantino Pitzalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Myles J. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Slabaugh_G/0/1/0/all/0/1&quot;&gt;Gregory Slabaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barnes_M/0/1/0/all/0/1&quot;&gt;Michael R. Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07268">
<title>So you think you can track?. (arXiv:2309.07268v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07268</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces a multi-camera tracking dataset consisting of 234 hours
of video data recorded concurrently from 234 overlapping HD cameras covering a
4.2 mile stretch of 8-10 lane interstate highway near Nashville, TN. The video
is recorded during a period of high traffic density with 500+ objects typically
visible within the scene and typical object longevities of 3-15 minutes. GPS
trajectories from 270 vehicle passes through the scene are manually corrected
in the video data to provide a set of ground-truth trajectories for
recall-oriented tracking metrics, and object detections are provided for each
camera in the scene (159 million total before cross-camera fusion). Initial
benchmarking of tracking-by-detection algorithms is performed against the GPS
trajectories, and a best HOTA of only 9.5% is obtained (best recall 75.9% at
IOU 0.1, 47.9 average IDs per ground truth object), indicating the benchmarked
trackers do not perform sufficiently well at the long temporal and spatial
durations required for traffic scene understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloudemans_D/0/1/0/all/0/1&quot;&gt;Derek Gloudemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zachar_G/0/1/0/all/0/1&quot;&gt;Gergely Zach&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanbing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Junyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nice_M/0/1/0/all/0/1&quot;&gt;Matt Nice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunting_M/0/1/0/all/0/1&quot;&gt;Matt Bunting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbour_W/0/1/0/all/0/1&quot;&gt;William Barbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprinkle_J/0/1/0/all/0/1&quot;&gt;Jonathan Sprinkle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piccoli_B/0/1/0/all/0/1&quot;&gt;Benedetto Piccoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monache_M/0/1/0/all/0/1&quot;&gt;Maria Laura Delle Monache&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayen_A/0/1/0/all/0/1&quot;&gt;Alexandre Bayen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seibold_B/0/1/0/all/0/1&quot;&gt;Benjamin Seibold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Work_D/0/1/0/all/0/1&quot;&gt;Daniel B. Work&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07277">
<title>Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07277</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have achieved widespread popularity due to
their unprecedented image generation capability. In particular, their ability
to synthesize and modify human faces has spurred research into using generated
face images in both training data augmentation and model performance
assessments. In this paper, we study the efficacy and shortcomings of
generative models in the context of face generation. Utilizing a combination of
qualitative and quantitative measures, including embedding-based metrics and
user studies, we present a framework to audit the characteristics of generated
faces conditioned on a set of social attributes. We applied our framework on
faces generated through state-of-the-art text-to-image diffusion models. We
identify several limitations of face image generation that include faithfulness
to the text prompt, demographic disparities, and distributional shifts.
Furthermore, we present an analytical model that provides insights into how
training data selection contributes to the performance of generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1&quot;&gt;Harrison Rosenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Shimaa Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1&quot;&gt;Guruprasad V Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinayak_R/0/1/0/all/0/1&quot;&gt;Ramya Korlakai Vinayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1&quot;&gt;Kassem Fawaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07293">
<title>GAN-based Algorithm for Efficient Image Inpainting. (arXiv:2309.07293v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07293</link>
<description rdf:parseType="Literal">&lt;p&gt;Global pandemic due to the spread of COVID-19 has post challenges in a new
dimension on facial recognition, where people start to wear masks. Under such
condition, the authors consider utilizing machine learning in image inpainting
to tackle the problem, by complete the possible face that is originally covered
in mask. In particular, autoencoder has great potential on retaining important,
general features of the image as well as the generative power of the generative
adversarial network (GAN). The authors implement a combination of the two
models, context encoders and explain how it combines the power of the two
models and train the model with 50,000 images of influencers faces and yields a
solid result that still contains space for improvements. Furthermore, the
authors discuss some shortcomings with the model, their possible improvements,
as well as some area of study for future investigation for applicative
perspective, as well as directions to further enhance and refine the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zehao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1&quot;&gt;Yuan Ju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07297">
<title>Multi-Modal Hybrid Learning and Sequential Training for RGB-T Saliency Detection. (arXiv:2309.07297v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07297</link>
<description rdf:parseType="Literal">&lt;p&gt;RGB-T saliency detection has emerged as an important computer vision task,
identifying conspicuous objects in challenging scenes such as dark
environments. However, existing methods neglect the characteristics of
cross-modal features and rely solely on network structures to fuse RGB and
thermal features. To address this, we first propose a Multi-Modal Hybrid loss
(MMHL) that comprises supervised and self-supervised loss functions. The
supervised loss component of MMHL distinctly utilizes semantic features from
different modalities, while the self-supervised loss component reduces the
distance between RGB and thermal features. We further consider both spatial and
channel information during feature fusion and propose the Hybrid Fusion Module
to effectively fuse RGB and thermal features. Lastly, instead of jointly
training the network with cross-modal features, we implement a sequential
training strategy which performs training only on RGB images in the first stage
and then learns cross-modal features in the second stage. This training
strategy improves saliency detection performance without computational
overhead. Results from performance evaluation and ablation studies demonstrate
the superior performance achieved by the proposed method compared with the
existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1&quot;&gt;Guangyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1&quot;&gt;Jitesh Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Youngjun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07322">
<title>$\texttt{NePhi}$: Neural Deformation Fields for Approximately Diffeomorphic Medical Image Registration. (arXiv:2309.07322v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07322</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes $\texttt{NePhi}$, a neural deformation model which results
in approximately diffeomorphic transformations. In contrast to the predominant
voxel-based approaches, $\texttt{NePhi}$ represents deformations functionally
which allows for memory-efficient training and inference. This is of particular
importance for large volumetric registrations. Further, while medical image
registration approaches representing transformation maps via multi-layer
perceptrons have been proposed, $\texttt{NePhi}$ facilitates both pairwise
optimization-based registration $\textit{as well as}$ learning-based
registration via predicted or optimized global and local latent codes. Lastly,
as deformation regularity is a highly desirable property for most medical image
registration tasks, $\texttt{NePhi}$ makes use of gradient inverse consistency
regularization which empirically results in approximately diffeomorphic
transformations. We show the performance of $\texttt{NePhi}$ on two 2D
synthetic datasets as well as on real 3D lung registration. Our results show
that $\texttt{NePhi}$ can achieve similar accuracies as voxel-based
representations in a single-resolution registration setting while using less
memory and allowing for faster instance-optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Soumyadip Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_H/0/1/0/all/0/1&quot;&gt;Hastings Greer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estepar_R/0/1/0/all/0/1&quot;&gt;Ra&amp;#xfa;l San Jos&amp;#xe9; Est&amp;#xe9;par&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1&quot;&gt;Marc Niethammer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07330">
<title>Automated Assessment of Critical View of Safety in Laparoscopic Cholecystectomy. (arXiv:2309.07330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07330</link>
<description rdf:parseType="Literal">&lt;p&gt;Cholecystectomy (gallbladder removal) is one of the most common procedures in
the US, with more than 1.2M procedures annually. Compared with classical open
cholecystectomy, laparoscopic cholecystectomy (LC) is associated with
significantly shorter recovery period, and hence is the preferred method.
However, LC is also associated with an increase in bile duct injuries (BDIs),
resulting in significant morbidity and mortality. The primary cause of BDIs
from LCs is misidentification of the cystic duct with the bile duct. Critical
view of safety (CVS) is the most effective of safety protocols, which is said
to be achieved during the surgery if certain criteria are met. However, due to
suboptimal understanding and implementation of CVS, the BDI rates have remained
stable over the last three decades. In this paper, we develop deep-learning
techniques to automate the assessment of CVS in LCs. An innovative aspect of
our research is on developing specialized learning techniques by incorporating
domain knowledge to compensate for the limited training data available in
practice. In particular, our CVS assessment process involves a fusion of two
segmentation maps followed by an estimation of a certain region of interest
based on anatomical structures close to the gallbladder, and then finally
determination of each of the three CVS criteria via rule-based assessment of
structural information. We achieved a gain of over 11.8% in mIoU on relevant
classes with our two-stream semantic segmentation approach when compared to a
single-model baseline, and 1.84% in mIoU with our proposed Sobel loss function
when compared to a Transformer-based baseline model. For CVS criteria, we
achieved up to 16% improvement and, for the overall CVS assessment, we achieved
5% improvement in balanced accuracy compared to DeepCVS under the same
experiment settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunfan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1&quot;&gt;Himanshu Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Haibin Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_I/0/1/0/all/0/1&quot;&gt;IV Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1&quot;&gt;Prateek Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1&quot;&gt;Georgios Georgakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasson_A/0/1/0/all/0/1&quot;&gt;Aaron Sasson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07332">
<title>Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07332</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately labeling biomedical data presents a challenge. Traditional
semi-supervised learning methods often under-utilize available unlabeled data.
To address this, we propose a novel reliability-based training data cleaning
method employing inductive conformal prediction (ICP). This method capitalizes
on a small set of accurately labeled training data and leverages ICP-calculated
reliability metrics to rectify mislabeled data and outliers within vast
quantities of noisy training data. The efficacy of the method is validated
across three classification tasks within distinct modalities: filtering
drug-induced-liver-injury (DILI) literature with title and abstract, predicting
ICU admission of COVID-19 patients through CT radiomics and electronic health
records, and subtyping breast cancer using RNA-sequencing data. Varying levels
of noise to the training labels were introduced through label permutation.
Results show significant enhancements in classification performance: accuracy
enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC
enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and
accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing
experiments (up to 74.6% and 89.0%). Our method offers the potential to
substantially boost classification performance in multi-modal biomedical
machine learning tasks. Importantly, it accomplishes this without necessitating
an excessive volume of meticulously curated training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xianghao Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qinmei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yuanning Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1&quot;&gt;Olivier Gevaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07361">
<title>Judging a video by its bitstream cover. (arXiv:2309.07361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07361</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifying videos into distinct categories, such as Sport and Music Video,
is crucial for multimedia understanding and retrieval, especially in an age
where an immense volume of video content is constantly being generated.
Traditional methods require video decompression to extract pixel-level features
like color, texture, and motion, thereby increasing computational and storage
demands. Moreover, these methods often suffer from performance degradation in
low-quality videos. We present a novel approach that examines only the
post-compression bitstream of a video to perform classification, eliminating
the need for bitstream. We validate our approach using a custom-built data set
comprising over 29,000 YouTube video clips, totaling 6,000 hours and spanning
11 distinct categories. Our preliminary evaluations indicate precision,
accuracy, and recall rates well over 80%. The algorithm operates approximately
15,000 times faster than real-time for 30fps videos, outperforming traditional
Dynamic Time Warping (DTW) algorithm by six orders of magnitude.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuxing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yunan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jiangtao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chen Ye Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07387">
<title>VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue. (arXiv:2309.07387v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07387</link>
<description rdf:parseType="Literal">&lt;p&gt;Visually-grounded dialog systems, which integrate multiple modes of
communication such as text and visual inputs, have become an increasingly
popular area of investigation. However, the absence of a standardized
evaluation framework poses a challenge in assessing the development of this
field. To this end, we propose \textbf{VDialogUE}, a \textbf{V}isually-grounded
\textbf{Dialog}ue benchmark for \textbf{U}nified \textbf{E}valuation. It
defines five core multi-modal dialogue tasks and covers six datasets.
Furthermore, in order to provide a comprehensive assessment of the model&apos;s
performance across all tasks, we developed a novel evaluation metric called
VDscore, which is based on the Analytic Hierarchy Process~(AHP) method.
Additionally, we present a straightforward yet efficient baseline model, named
\textbf{VISIT}~(\textbf{VIS}ually-grounded d\textbf{I}alog
\textbf{T}ransformer), to promote the advancement of general multi-modal
dialogue systems. It progressively builds its multi-modal foundation and
dialogue capability via a two-stage pre-training strategy.
&lt;/p&gt;
&lt;p&gt;We believe that the VDialogUE benchmark, along with the evaluation scripts
and our baseline models, will accelerate the development of visually-grounded
dialog systems and lead to the development of more sophisticated and effective
pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunshui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1&quot;&gt;Binyuan Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaochao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wanwei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Run Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1&quot;&gt;Yuxing Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07390">
<title>Unleashing the Power of Depth and Pose Estimation Neural Networks by Designing Compatible Endoscopic Images. (arXiv:2309.07390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07390</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have witnessed depth and pose estimation framework on
unannotated datasets as a effective pathway to succeed in endoscopic
navigation. Most current techniques are dedicated to developing more advanced
neural networks to improve the accuracy. However, existing methods ignore the
special properties of endoscopic images, resulting in an inability to fully
unleash the power of neural networks. In this study, we conduct a detail
analysis of the properties of endoscopic images and improve the compatibility
of images and neural networks, to unleash the power of current neural networks.
First, we introcude the Mask Image Modelling (MIM) module, which inputs partial
image information instead of complete image information, allowing the network
to recover global information from partial pixel information. This enhances the
network&apos; s ability to perceive global information and alleviates the phenomenon
of local overfitting in convolutional neural networks due to local artifacts.
Second, we propose a lightweight neural network to enhance the endoscopic
images, to explicitly improve the compatibility between images and neural
networks. Extensive experiments are conducted on the three public datasets and
one inhouse dataset, and the proposed modules improve baselines by a large
margin. Furthermore, the enhanced images we proposed, which have higher network
compatibility, can serve as an effective data augmentation method and they are
able to extract more stable feature points in traditional feature point
matching tasks and achieve outstanding performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yun Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07394">
<title>Nucleus-aware Self-supervised Pretraining Using Unpaired Image-to-image Translation for Histopathology Images. (arXiv:2309.07394v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07394</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised pretraining attempts to enhance model performance by
obtaining effective features from unlabeled data, and has demonstrated its
effectiveness in the field of histopathology images. Despite its success, few
works concentrate on the extraction of nucleus-level information, which is
essential for pathologic analysis. In this work, we propose a novel
nucleus-aware self-supervised pretraining framework for histopathology images.
The framework aims to capture the nuclear morphology and distribution
information through unpaired image-to-image translation between histopathology
images and pseudo mask images. The generation process is modulated by both
conditional and stochastic style representations, ensuring the reality and
diversity of the generated histopathology images for pretraining. Further, an
instance segmentation guided strategy is employed to capture instance-level
information. The experiments on 7 datasets show that the proposed pretraining
method outperforms supervised ones on Kather classification, multiple instance
learning, and 5 dense-prediction tasks with the transfer learning protocol, and
yields superior results than other self-supervised approaches on 8
semi-supervised tasks. Our project is publicly available at
https://github.com/zhiyuns/UNITPathSSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhiyun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1&quot;&gt;Penghui Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junpeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kailu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_J/0/1/0/all/0/1&quot;&gt;Jianzhong Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_M/0/1/0/all/0/1&quot;&gt;Maode Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yubo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07398">
<title>Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07398</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional adversarial attacks concentrate on manipulating clean examples in
the pixel space by adding adversarial perturbations. By contrast, semantic
adversarial attacks focus on changing semantic attributes of clean examples,
such as color, context, and features, which are more feasible in the real
world. In this paper, we propose a framework to quickly generate a semantic
adversarial attack by leveraging recent diffusion models since semantic
information is included in the latent space of well-trained diffusion models.
Then there are two variants of this framework: 1) the Semantic Transformation
(ST) approach fine-tunes the latent space of the generated image and/or the
diffusion model itself; 2) the Latent Masking (LM) approach masks the latent
space with another target image and local backpropagation-based interpretation
methods. Additionally, the ST approach can be applied in either white-box or
black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ
datasets, and our framework demonstrates great fidelity, generalizability, and
transferability compared to other baselines. Our approaches achieve
approximately 100% attack success rate in multiple settings with the best FID
as 36.61. Code is available at
https://github.com/steven202/semantic_adv_via_dm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Edward Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamm_M/0/1/0/all/0/1&quot;&gt;Matthew Stamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07400">
<title>HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis. (arXiv:2309.07400v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07400</link>
<description rdf:parseType="Literal">&lt;p&gt;In computation pathology, the pyramid structure of gigapixel Whole Slide
Images (WSIs) has recently been studied for capturing various information from
individual cell interactions to tissue microenvironments. This hierarchical
structure is believed to be beneficial for cancer diagnosis and prognosis
tasks. However, most previous hierarchical WSI analysis works (1) only
characterize local or global correlations within the WSI pyramids and (2) use
only unidirectional interaction between different resolutions, leading to an
incomplete picture of WSI pyramids. To this end, this paper presents a novel
Hierarchical Interaction Graph-Transformer (i.e., HIGT) for WSI analysis. With
Graph Neural Network and Transformer as the building commons, HIGT can learn
both short-range local information and long-range global representation of the
WSI pyramids. Considering that the information from different resolutions is
complementary and can benefit each other during the learning process, we
further design a novel Bidirectional Interaction block to establish
communication between different levels within the WSI pyramids. Finally, we
aggregate both coarse-grained and fine-grained features learned from different
levels together for slide-level prediction. We evaluate our methods on two
public WSI datasets from TCGA projects, i.e., kidney carcinoma (KICA) and
esophageal carcinoma (ESCA). Experimental results show that our HIGT
outperforms both hierarchical and non-hierarchical state-of-the-art methods on
both tumor subtyping and staging tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiqin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shujun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lequan Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07403">
<title>Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance. (arXiv:2309.07403v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07403</link>
<description rdf:parseType="Literal">&lt;p&gt;In real-world scenarios, typical visual recognition systems could fail under
two major causes, i.e., the misclassification between known classes and the
excusable misbehavior on unknown-class images. To tackle these deficiencies,
flexible visual recognition should dynamically predict multiple classes when
they are unconfident between choices and reject making predictions when the
input is entirely out of the training distribution. Two challenges emerge along
with this novel task. First, prediction uncertainty should be separately
quantified as confusion depicting inter-class uncertainties and ignorance
identifying out-of-distribution samples. Second, both confusion and ignorance
should be comparable between samples to enable effective decision-making. In
this paper, we propose to model these two sources of uncertainty explicitly
with the theory of Subjective Logic. Regarding recognition as an
evidence-collecting process, confusion is then defined as conflicting evidence,
while ignorance is the absence of evidence. By predicting Dirichlet
concentration parameters for singletons, comprehensive subjective opinions,
including confusion and ignorance, could be achieved via further evidence
combinations. Through a series of experiments on synthetic data analysis,
visual recognition, and open-set detection, we demonstrate the effectiveness of
our methods in quantifying two sources of uncertainties and dealing with
flexible recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1&quot;&gt;Gang Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07409">
<title>Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos. (arXiv:2309.07409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07409</link>
<description rdf:parseType="Literal">&lt;p&gt;A key challenge with procedure planning in instructional videos lies in how
to handle a large decision space consisting of a multitude of action types that
belong to various tasks. To understand real-world video content, an AI agent
must proficiently discern these action types (e.g., pour milk, pour water, open
lid, close lid, etc.) based on brief visual observation. Moreover, it must
adeptly capture the intricate semantic relation of the action types and task
goals, along with the variable action sequences. Recently, notable progress has
been made via the integration of diffusion models and visual representation
learning to address the challenge. However, existing models employ rudimentary
mechanisms to utilize task information to manage the decision space. To
overcome this limitation, we introduce a simple yet effective enhancement - a
masked diffusion model. The introduced mask acts akin to a task-oriented
attention filter, enabling the diffusion/denoising process to concentrate on a
subset of action types. Furthermore, to bolster the accuracy of task
classification, we harness more potent visual representation learning
techniques. In particular, we learn a joint visual-text embedding, where a text
embedding is generated by prompting a pre-trained vision-language model to
focus on human actions. We evaluate the method on three public datasets and
achieve state-of-the-art performance on multiple metrics. Code is available at
https://github.com/ffzzy840304/Masked-PDPP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1&quot;&gt;Ali Koksal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qianli Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Joo-Hwee Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07425">
<title>JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale. (arXiv:2309.07425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07425</link>
<description rdf:parseType="Literal">&lt;p&gt;The semantic understanding of indoor 3D point cloud data is crucial for a
range of subsequent applications, including indoor service robots, navigation
systems, and digital twin engineering. Global features are crucial for
achieving high-quality semantic and instance segmentation of indoor point
clouds, as they provide essential long-range context information. To this end,
we propose JSMNet, which combines a multi-layer network with a global feature
self-attention module to jointly segment three-dimensional point cloud
semantics and instances. To better express the characteristics of indoor
targets, we have designed a multi-resolution feature adaptive fusion module
that takes into account the differences in point cloud density caused by
varying scanner distances from the target. Additionally, we propose a framework
for joint semantic and instance segmentation by integrating semantic and
instance features to achieve superior results. We conduct experiments on S3DIS,
which is a large three-dimensional indoor point cloud dataset. Our proposed
method is compared against other methods, and the results show that it
outperforms existing methods in semantic and instance segmentation and provides
better results in target local area segmentation. Specifically, our proposed
method outperforms PointNet (Qi et al., 2017a) by 16.0% and 26.3% in terms of
semantic segmentation mIoU in S3DIS (Area 5) and instance segmentation mPre,
respectively. Additionally, it surpasses ASIS (Wang et al., 2019) by 6.0% and
4.6%, respectively, as well as JSPNet (Chen et al., 2022) by a margin of 3.3%
for semantic segmentation mIoU and a slight improvement of 0.3% for instance
segmentation mPre.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuochen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenxin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07428">
<title>Physical Invisible Backdoor Based on Camera Imaging. (arXiv:2309.07428v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07428</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor attack aims to compromise a model, which returns an adversary-wanted
output when a specific trigger pattern appears yet behaves normally for clean
inputs. Current backdoor attacks require changing pixels of clean images, which
results in poor stealthiness of attacks and increases the difficulty of the
physical implementation. This paper proposes a novel physical invisible
backdoor based on camera imaging without changing nature image pixels.
Specifically, a compromised model returns a target label for images taken by a
particular camera, while it returns correct results for other images. To
implement and evaluate the proposed backdoor, we take shots of different
objects from multi-angles using multiple smartphones to build a new dataset of
21,500 images. Conventional backdoor attacks work ineffectively with some
classical models, such as ResNet18, over the above-mentioned dataset.
Therefore, we propose a three-step training strategy to mount the backdoor
attack. First, we design and train a camera identification model with the phone
IDs to extract the camera fingerprint feature. Subsequently, we elaborate a
special network architecture, which is easily compromised by our backdoor
attack, by leveraging the attributes of the CFA interpolation algorithm and
combining it with the feature extraction block in the camera identification
model. Finally, we transfer the backdoor from the elaborated special network
architecture to the classical architecture model via teacher-student
distillation learning. Since the trigger of our method is related to the
specific phone, our attack works effectively in the physical world. Experiment
results demonstrate the feasibility of our proposed approach and robustness
against various backdoor defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yusheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_N/0/1/0/all/0/1&quot;&gt;Nan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07439">
<title>DePT: Decoupled Prompt Tuning. (arXiv:2309.07439v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07439</link>
<description rdf:parseType="Literal">&lt;p&gt;This work breaks through the Base-New Tradeoff (BNT)dilemma in prompt tuning,
i.e., the better the tuned model generalizes to the base (or target) task, the
worse it generalizes to new tasks, and vice versa. Specifically, through an
in-depth analysis of the learned features of the base and new tasks, we observe
that the BNT stems from a channel bias issue, i.e., the vast majority of
feature channels are occupied by base-specific knowledge, resulting in the
collapse of taskshared knowledge important to new tasks. To address this, we
propose the Decoupled Prompt Tuning (DePT) framework, which decouples
base-specific knowledge from feature channels into an isolated feature space
during prompt tuning, so as to maximally preserve task-shared knowledge in the
original feature space for achieving better zero-shot generalization on new
tasks. Importantly, our DePT is orthogonal to existing prompt tuning methods,
hence it can improve all of them. Extensive experiments on 11 datasets show the
strong flexibility and effectiveness of DePT. Our code and pretrained models
are available at https://github.com/Koorye/DePT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shihan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hengtao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07444">
<title>Research on self-cross transformer model of point cloud change detecter. (arXiv:2309.07444v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07444</link>
<description rdf:parseType="Literal">&lt;p&gt;With the vigorous development of the urban construction industry, engineering
deformation or changes often occur during the construction process. To combat
this phenomenon, it is necessary to detect changes in order to detect
construction loopholes in time, ensure the integrity of the project and reduce
labor costs. Or the inconvenience and injuriousness of the road. In the study
of change detection in 3D point clouds, researchers have published various
research methods on 3D point clouds. Directly based on but mostly based
ontraditional threshold distance methods (C2C, M3C2, M3C2-EP), and some are to
convert 3D point clouds into DSM, which loses a lot of original information.
Although deep learning is used in remote sensing methods, in terms of change
detection of 3D point clouds, it is more converted into two-dimensional
patches, and neural networks are rarely applied directly. We prefer that the
network is given at the level of pixels or points. Variety. Therefore, in this
article, our network builds a network for 3D point cloud change detection, and
proposes a new module Cross transformer suitable for change detection.
Simultaneously simulate tunneling data for change detection, and do test
experiments with our network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaoxu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haili Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenxin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07461">
<title>Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2309.07461</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread integration of Internet of Things (IoT) devices across all
facets of life has ushered in an era of interconnectedness, creating new
avenues for cybersecurity challenges and underscoring the need for robust
intrusion detection systems. However, traditional security systems are designed
with a closed-world perspective and often face challenges in dealing with the
ever-evolving threat landscape, where new and unfamiliar attacks are constantly
emerging. In this paper, we introduce a framework aimed at mitigating the open
set recognition (OSR) problem in the realm of Network Intrusion Detection
Systems (NIDS) tailored for IoT environments. Our framework capitalizes on
image-based representations of packet-level data, extracting spatial and
temporal patterns from network traffic. Additionally, we integrate stacking and
sub-clustering techniques, enabling the identification of unknown attacks by
effectively modeling the complex and diverse nature of benign behavior. The
empirical results prominently underscore the framework&apos;s efficacy, boasting an
impressive 88\% detection rate for previously unseen attacks when compared
against existing approaches and recent advancements. Future work will perform
extensive experimentation across various openness levels and attack scenarios,
further strengthening the adaptability and performance of our proposed solution
in safeguarding IoT environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrukh_Y/0/1/0/all/0/1&quot;&gt;Yasir Ali Farrukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1&quot;&gt;Syed Wali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1&quot;&gt;Irfan Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1&quot;&gt;Nathaniel D. Bastian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07471">
<title>EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization. (arXiv:2309.07471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07471</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual localization is the task of estimating a 6-DoF camera pose of a query
image within a provided 3D reference map. Thanks to recent advances in various
3D sensors, 3D point clouds are becoming a more accurate and affordable option
for building the reference map, but research to match the points of 3D point
clouds with pixels in 2D images for visual localization remains challenging.
Existing approaches that jointly learn 2D-3D feature matching suffer from low
inliers due to representational differences between the two modalities, and the
methods that bypass this problem into classification have an issue of poor
refinement. In this work, we propose EP2P-Loc, a novel large-scale visual
localization method that mitigates such appearance discrepancy and enables
end-to-end training for pose estimation. To increase the number of inliers, we
propose a simple algorithm to remove invisible 3D points in the image, and find
all 2D-3D correspondences without keypoint detection. To reduce memory usage
and search complexity, we take a coarse-to-fine approach where we extract
patch-level features from 2D images, then perform 2D patch classification on
each 3D point, and obtain the exact corresponding 2D pixel coordinates through
positional encoding. Finally, for the first time in this task, we employ a
differentiable PnP for end-to-end training. In the experiments on newly curated
large-scale indoor and outdoor benchmarks based on 2D-3D-S and KITTI, we show
that our method achieves the state-of-the-art performance compared to existing
visual localization and image-to-point cloud registration methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minjung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1&quot;&gt;Junseo Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gunhee Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07495">
<title>HDTR-Net: A Real-Time High-Definition Teeth Restoration Network for Arbitrary Talking Face Generation Methods. (arXiv:2309.07495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07495</link>
<description rdf:parseType="Literal">&lt;p&gt;Talking Face Generation (TFG) aims to reconstruct facial movements to achieve
high natural lip movements from audio and facial features that are under
potential connections. Existing TFG methods have made significant advancements
to produce natural and realistic images. However, most work rarely takes visual
quality into consideration. It is challenging to ensure lip synchronization
while avoiding visual quality degradation in cross-modal generation methods. To
address this issue, we propose a universal High-Definition Teeth Restoration
Network, dubbed HDTR-Net, for arbitrary TFG methods. HDTR-Net can enhance teeth
regions at an extremely fast speed while maintaining synchronization, and
temporal consistency. In particular, we propose a Fine-Grained Feature Fusion
(FGFF) module to effectively capture fine texture feature information around
teeth and surrounding regions, and use these features to fine-grain the feature
map to enhance the clarity of teeth. Extensive experiments show that our method
can be adapted to arbitrary TFG methods without suffering from lip
synchronization and frame coherence. Another advantage of HDTR-Net is its
real-time generation ability. Also under the condition of high-definition
restoration of talking face video synthesis, its inference speed is $300\%$
faster than the current state-of-the-art face restoration based on
super-resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Mingqiang Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07499">
<title>Efficiently Robustify Pre-trained Models. (arXiv:2309.07499v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07499</link>
<description rdf:parseType="Literal">&lt;p&gt;A recent trend in deep learning algorithms has been towards training large
scale models, having high parameter count and trained on big dataset. However,
robustness of such large scale models towards real-world settings is still a
less-explored topic. In this work, we first benchmark the performance of these
models under different perturbations and datasets thereby representing
real-world shifts, and highlight their degrading performance under these
shifts. We then discuss on how complete model fine-tuning based existing
robustification schemes might not be a scalable option given very large scale
networks and can also lead them to forget some of the desired characterstics.
Finally, we propose a simple and cost-effective method to solve this problem,
inspired by knowledge transfer literature. It involves robustifying smaller
models, at a lower computation cost, and then use them as teachers to tune a
fraction of these large scale networks, reducing the overall computational
overhead. We evaluate our proposed method under various vision perturbations
including ImageNet-C,R,S,A datasets and also for transfer learning, zero-shot
evaluation setups on different datasets. Benchmark results show that our method
is able to induce robustness to these large scale models efficiently, requiring
significantly lower time and also preserves the transfer learning, zero-shot
properties of the original model which none of the existing methods are able to
achieve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1&quot;&gt;Nishant Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behl_H/0/1/0/all/0/1&quot;&gt;Harkirat Behl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1&quot;&gt;Yogesh Singh Rawat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1&quot;&gt;Vibhav Vineet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07509">
<title>DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks. (arXiv:2309.07509v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07509</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating realistic talking faces is a complex and widely discussed task
with numerous applications. In this paper, we present DiffTalker, a novel model
designed to generate lifelike talking faces through audio and landmark
co-driving. DiffTalker addresses the challenges associated with directly
applying diffusion models to audio control, which are traditionally trained on
text-image pairs. DiffTalker consists of two agent networks: a
transformer-based landmarks completion network for geometric accuracy and a
diffusion-based face generation network for texture details. Landmarks play a
pivotal role in establishing a seamless connection between the audio and image
domains, facilitating the incorporation of knowledge from pre-trained diffusion
models. This innovative approach efficiently produces articulate-speaking
faces. Experimental results showcase DiffTalker&apos;s superior performance in
producing clear and geometrically accurate talking faces, all without the need
for additional alignment between audio and image features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zipeng Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xulong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Ning Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jing Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianzong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07510">
<title>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceiving and manipulating 3D articulated objects in diverse environments is
essential for home-assistant robots. Recent studies have shown that point-level
affordance provides actionable priors for downstream manipulation tasks.
However, existing works primarily focus on single-object scenarios with
homogeneous agents, overlooking the realistic constraints imposed by the
environment and the agent&apos;s morphology, e.g., occlusions and physical
limitations. In this paper, we propose an environment-aware affordance
framework that incorporates both object-level actionable priors and environment
constraints. Unlike object-centric affordance approaches, learning
environment-aware affordance faces the challenge of combinatorial explosion due
to the complexity of various occlusions, characterized by their quantities,
geometries, positions and poses. To address this and enhance data efficiency,
we introduce a novel contrastive affordance learning framework capable of
training on scenes containing a single occluder and generalizing to scenes with
complex occluder combinations. Experiments demonstrate the effectiveness of our
proposed approach in learning affordance considering environment constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_G/0/1/0/all/0/1&quot;&gt;Guanqi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07513">
<title>RecycleNet: Latent Feature Recycling Leads to Iterative Decision Refinement. (arXiv:2309.07513v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07513</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable success of deep learning systems over the last decade,
a key difference still remains between neural network and human
decision-making: As humans, we cannot only form a decision on the spot, but
also ponder, revisiting an initial guess from different angles, distilling
relevant information, arriving at a better decision. Here, we propose
RecycleNet, a latent feature recycling method, instilling the pondering
capability for neural networks to refine initial decisions over a number of
recycling steps, where outputs are fed back into earlier network layers in an
iterative fashion. This approach makes minimal assumptions about the neural
network architecture and thus can be implemented in a wide variety of contexts.
Using medical image segmentation as the evaluation environment, we show that
latent feature recycling enables the network to iteratively refine initial
predictions even beyond the iterations seen during training, converging towards
an improved decision. We evaluate this across a variety of segmentation
benchmarks and show consistent improvements even compared with top-performing
segmentation methods. This allows trading increased computation time for
improved performance, which can be beneficial, especially for safety-critical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_G/0/1/0/all/0/1&quot;&gt;Gregor Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_T/0/1/0/all/0/1&quot;&gt;Tassilo Wald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulrich_C/0/1/0/all/0/1&quot;&gt;Constantin Ulrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmerer_D/0/1/0/all/0/1&quot;&gt;David Zimmerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_P/0/1/0/all/0/1&quot;&gt;Paul F. Jaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franke_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg K.H. Franke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohl_S/0/1/0/all/0/1&quot;&gt;Simon Kohl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H. Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07515">
<title>Dhan-Shomadhan: A Dataset of Rice Leaf Disease Classification for Bangladeshi Local Rice. (arXiv:2309.07515v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07515</link>
<description rdf:parseType="Literal">&lt;p&gt;This dataset represents almost all the harmful diseases for rice in
Bangladesh. This dataset consists of 1106 image of five harmful diseases called
Brown Spot, Leaf Scaled, Rice Blast, Rice Turngo, Steath Blight in two
different background variation named field background picture and white
background picture. Two different background variation helps the dataset to
perform more accurately so that the user can use this data for field use as
well as white background for decision making. The data is collected from rice
field of Dhaka Division. This dataset can use for rice leaf diseases
classification, diseases detection using Computer Vision and Pattern
Recognition for different rice leaf disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;Md. Fahad Hossain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07524">
<title>A Multi-scale Generalized Shrinkage Threshold Network for Image Blind Deblurring in Remote Sensing. (arXiv:2309.07524v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07524</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing images are essential for many earth science applications, but
their quality can be degraded due to limitations in sensor technology and
complex imaging environments. To address this, various remote sensing image
deblurring methods have been developed to restore sharp, high-quality images
from degraded observational data. However, most traditional model-based
deblurring methods usually require predefined hand-craft prior assumptions,
which are difficult to handle in complex applications, and most deep
learning-based deblurring methods are designed as a black box, lacking
transparency and interpretability. In this work, we propose a novel blind
deblurring learning framework based on alternating iterations of shrinkage
thresholds, alternately updating blurring kernels and images, with the
theoretical foundation of network design. Additionally, we propose a learnable
blur kernel proximal mapping module to improve the blur kernel evaluation in
the kernel domain. Then, we proposed a deep proximal mapping module in the
image domain, which combines a generalized shrinkage threshold operator and a
multi-scale prior feature extraction block. This module also introduces an
attention mechanism to adaptively adjust the prior importance, thus avoiding
the drawbacks of hand-crafted image prior terms. Thus, a novel multi-scale
generalized shrinkage threshold network (MGSTNet) is designed to specifically
focus on learning deep geometric prior features to enhance image restoration.
Experiments demonstrate the superiority of our MGSTNet framework on remote
sensing image datasets compared to existing deblurring methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yujie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaohong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07537">
<title>Universality of underlying mechanism for successful deep learning. (arXiv:2309.07537v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07537</link>
<description rdf:parseType="Literal">&lt;p&gt;An underlying mechanism for successful deep learning (DL) with a limited deep
architecture and dataset, namely VGG-16 on CIFAR-10, was recently presented
based on a quantitative method to measure the quality of a single filter in
each layer. In this method, each filter identifies small clusters of possible
output labels, with additional noise selected as labels out of the clusters.
This feature is progressively sharpened with the layers, resulting in an
enhanced signal-to-noise ratio (SNR) and higher accuracy. In this study, the
suggested universal mechanism is verified for VGG-16 and EfficientNet-B0
trained on the CIFAR-100 and ImageNet datasets with the following main results.
First, the accuracy progressively increases with the layers, whereas the noise
per filter typically progressively decreases. Second, for a given deep
architecture, the maximal error rate increases approximately linearly with the
number of output labels. Third, the average filter cluster size and the number
of clusters per filter at the last convolutional layer adjacent to the output
layer are almost independent of the number of dataset labels in the range [3,
1,000], while a high SNR is preserved. The presented DL mechanism suggests
several techniques, such as applying filter&apos;s cluster connections (AFCC), to
improve the computational complexity and accuracy of deep architectures and
furthermore pinpoints the simplification of pre-existing structures while
maintaining their accuracies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meir_Y/0/1/0/all/0/1&quot;&gt;Yuval Meir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzach_Y/0/1/0/all/0/1&quot;&gt;Yarden Tzach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodassman_S/0/1/0/all/0/1&quot;&gt;Shiri Hodassman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tevet_O/0/1/0/all/0/1&quot;&gt;Ofek Tevet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanter_I/0/1/0/all/0/1&quot;&gt;Ido Kanter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07609">
<title>Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation. (arXiv:2309.07609v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07609</link>
<description rdf:parseType="Literal">&lt;p&gt;The robotic manipulation of Deformable Linear Objects (DLOs) is a vital and
challenging task that is important in many practical applications. Classical
model-based approaches to this problem require an accurate model to capture how
robot motions affect the deformation of the DLO. Nowadays, data-driven models
offer the best tradeoff between quality and computation time. This paper
analyzes several learning-based 3D models of the DLO and proposes a new one
based on the Transformer architecture that achieves superior accuracy, even on
the DLOs of different lengths, thanks to the proposed scaling method. Moreover,
we introduce a data augmentation technique, which improves the prediction
performance of almost all considered DLO data-driven models. Thanks to this
technique, even a simple Multilayer Perceptron (MLP) achieves close to
state-of-the-art performance while being significantly faster to evaluate. In
the experiments, we compare the performance of the learning-based 3D models of
the DLO on several challenging datasets quantitatively and demonstrate their
applicability in the task of shaping a DLO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kicki_P/0/1/0/all/0/1&quot;&gt;Piotr Kicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bidzinski_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Bidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walas_K/0/1/0/all/0/1&quot;&gt;Krzysztof Walas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07616">
<title>Road Disease Detection based on Latent Domain Background Feature Separation and Suppression. (arXiv:2309.07616v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07616</link>
<description rdf:parseType="Literal">&lt;p&gt;Road disease detection is challenging due to the the small proportion of road
damage in target region and the diverse background,which introduce lots of
domain information.Besides, disease categories have high similarity,makes the
detection more difficult. In this paper, we propose a new LDBFSS(Latent Domain
Background Feature Separation and Suppression) network which could perform
background information separation and suppression without domain supervision
and contrastive enhancement of object features.We combine our LDBFSS network
with YOLOv5 model to enhance disease features for better road disease
detection. As the components of LDBFSS network, we first design a latent domain
discovery module and a domain adversarial learning module to obtain pseudo
domain labels through unsupervised method, guiding domain discriminator and
model to train adversarially to suppress background information. In addition,
we introduce a contrastive learning module and design k-instance contrastive
loss, optimize the disease feature representation by increasing the inter-class
distance and reducing the intra-class distance for object features. We
conducted experiments on two road disease detection datasets, GRDDC and CNRDD,
and compared with other models,which show an increase of nearly 4% on GRDDC
dataset compared with optimal model, and an increase of 4.6% on CNRDD dataset.
Experimental results prove the effectiveness and superiority of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Juwu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jiangtao Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07623">
<title>SwitchGPT: Adapting Large Language Models for Non-Text Outputs. (arXiv:2309.07623v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07623</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), primarily trained on text-based datasets,
exhibit exceptional proficiencies in understanding and executing complex
linguistic instructions via text outputs. However, they falter when requests to
generate non-text ones. Concurrently, modality conversion models, such as
text-to-image, despite generating high-quality images, suffer from a lack of
extensive textual pretraining. As a result, these models are only capable of
accommodating specific image descriptions rather than comprehending more
complex instructions. To bridge this gap, we propose a novel approach,
\methodname, from a modality conversion perspective that evolves a text-based
LLM into a multi-modal one. We specifically employ a minimal dataset to
instruct LLMs to recognize the intended output modality as directed by the
instructions. Consequently, the adapted LLM can effectively summon various
off-the-shelf modality conversion models from the model zoos to generate
non-text responses. This circumvents the necessity for complicated pretraining
that typically requires immense quantities of paired multi-modal data, while
simultaneously inheriting the extensive knowledge of LLMs and the ability of
high-quality generative models. To evaluate and compare the adapted multi-modal
LLM with its traditional counterparts, we have constructed a multi-modal
instruction benchmark that solicits diverse modality outputs. The experiment
results reveal that, with minimal training, LLMs can be conveniently adapted to
comprehend requests for non-text responses, thus achieving higher flexibility
in multi-modal scenarios. Code and data will be made available at
https://github.com/xinke-wang/SwitchGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07640">
<title>Indoor Scene Reconstruction with Fine-Grained Details Using Hybrid Representation and Normal Prior Enhancement. (arXiv:2309.07640v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07640</link>
<description rdf:parseType="Literal">&lt;p&gt;The reconstruction of indoor scenes from multi-view RGB images is challenging
due to the coexistence of flat and texture-less regions alongside delicate and
fine-grained regions. Recent methods leverage neural radiance fields aided by
predicted surface normal priors to recover the scene geometry. These methods
excel in producing complete and smooth results for floor and wall areas.
However, they struggle to capture complex surfaces with high-frequency
structures due to the inadequate neural representation and the inaccurately
predicted normal priors. To improve the capacity of the implicit
representation, we propose a hybrid architecture to represent low-frequency and
high-frequency regions separately. To enhance the normal priors, we introduce a
simple yet effective image sharpening and denoising technique, coupled with a
network that estimates the pixel-wise uncertainty of the predicted surface
normal vectors. Identifying such uncertainty can prevent our model from being
misled by unreliable surface normal supervisions that hinder the accurate
reconstruction of intricate geometries. Experiments on the benchmark datasets
show that our method significantly outperforms existing methods in terms of
reconstruction quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Sheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yubin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Matthieu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yu-Hui Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07654">
<title>Towards Robust and Unconstrained Full Range of Rotation Head Pose Estimation. (arXiv:2309.07654v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07654</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the head pose of a person is a crucial problem for numerous
applications that is yet mainly addressed as a subtask of frontal pose
prediction. We present a novel method for unconstrained end-to-end head pose
estimation to tackle the challenging task of full range of orientation head
pose prediction. We address the issue of ambiguous rotation labels by
introducing the rotation matrix formalism for our ground truth data and propose
a continuous 6D rotation matrix representation for efficient and robust direct
regression. This allows to efficiently learn full rotation appearance and to
overcome the limitations of the current state-of-the-art. Together with new
accumulated training data that provides full head pose rotation data and a
geodesic loss approach for stable learning, we design an advanced model that is
able to predict an extended range of head orientations. An extensive evaluation
on public datasets demonstrates that our method significantly outperforms other
state-of-the-art methods in an efficient and robust manner, while its advanced
prediction range allows the expansion of the application area. We open-source
our training and testing code along with our trained models:
https://github.com/thohemp/6DRepNet360.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hempel_T/0/1/0/all/0/1&quot;&gt;Thorsten Hempel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelrahman_A/0/1/0/all/0/1&quot;&gt;Ahmed A. Abdelrahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hamadi_A/0/1/0/all/0/1&quot;&gt;Ayoub Al-Hamadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07668">
<title>CoRF : Colorizing Radiance Fields using Knowledge Distillation. (arXiv:2309.07668v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07668</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance field (NeRF) based methods enable high-quality novel-view
synthesis for multi-view images. This work presents a method for synthesizing
colorized novel views from input grey-scale multi-view images. When we apply
image or video-based colorization methods on the generated grey-scale novel
views, we observe artifacts due to inconsistency across views. Training a
radiance field network on the colorized grey-scale image sequence also does not
solve the 3D consistency issue. We propose a distillation based method to
transfer color knowledge from the colorization networks trained on natural
images to the radiance field network. Specifically, our method uses the
radiance field network as a 3D representation and transfers knowledge from
existing 2D colorization methods. The experimental results demonstrate that the
proposed method produces superior colorized novel views for indoor and outdoor
scenes while maintaining cross-view consistency than baselines. Further, we
show the efficacy of our method on applications like colorization of radiance
field network trained from 1.) Infra-Red (IR) multi-view images and 2.) Old
grey-scale multi-view image sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhiman_A/0/1/0/all/0/1&quot;&gt;Ankit Dhiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1&quot;&gt;R Srinath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Srinjay Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boregowda_L/0/1/0/all/0/1&quot;&gt;Lokesh R Boregowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1&quot;&gt;R Venkatesh Babu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07698">
<title>Dataset Condensation via Generative Model. (arXiv:2309.07698v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07698</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset condensation aims to condense a large dataset with a lot of training
samples into a small set. Previous methods usually condense the dataset into
the pixels format. However, it suffers from slow optimization speed and large
number of parameters to be optimized. When increasing image resolutions and
classes, the number of learnable parameters grows accordingly, prohibiting
condensation methods from scaling up to large datasets with diverse classes.
Moreover, the relations among condensed samples have been neglected and hence
the feature distribution of condensed samples is often not diverse. To solve
these problems, we propose to condense the dataset into another format, a
generative model. Such a novel format allows for the condensation of large
datasets because the size of the generative model remains relatively stable as
the number of classes or image resolution increases. Furthermore, an
intra-class and an inter-class loss are proposed to model the relation of
condensed samples. Intra-class loss aims to create more diverse samples for
each class by pushing each sample away from the others of the same class.
Meanwhile, inter-class loss increases the discriminability of samples by
widening the gap between the centers of different classes. Extensive
comparisons with state-of-the-art methods and our ablation studies confirm the
effectiveness of our method and its individual component. To our best
knowledge, we are the first to successfully conduct condensation on
ImageNet-1k.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David Junhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chuhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Song Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07704">
<title>NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. (arXiv:2309.07704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07704</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate dietary intake estimation is critical for informing policies and
programs to support healthy eating, as malnutrition has been directly linked to
decreased quality of life. However self-reporting methods such as food diaries
suffer from substantial bias. Other conventional dietary assessment techniques
and emerging alternative approaches such as mobile applications incur high time
costs and may necessitate trained personnel. Recent work has focused on using
computer vision and machine learning to automatically estimate dietary intake
from food images, but the lack of comprehensive datasets with diverse
viewpoints, modalities and food annotations hinders the accuracy and realism of
such methods. To address this limitation, we introduce NutritionVerse-Synth,
the first large-scale dataset of 84,984 photorealistic synthetic 2D food images
with associated dietary information and multimodal annotations (including depth
images, instance masks, and semantic masks). Additionally, we collect a real
image dataset, NutritionVerse-Real, containing 889 images of 251 dishes to
evaluate realism. Leveraging these novel datasets, we develop and benchmark
NutritionVerse, an empirical study of various dietary intake estimation
approaches, including indirect segmentation-based and direct prediction
networks. We further fine-tune models pretrained on synthetic data with real
images to provide insights into the fusion of synthetic and real data. Finally,
we release both datasets (NutritionVerse-Synth, NutritionVerse-Real) on
https://www.kaggle.com/nutritionverse/datasets as part of an open initiative to
accelerate machine learning for dietary sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1&quot;&gt;Chi-en Amy Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1&quot;&gt;Matthew Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Saeejith Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_O/0/1/0/all/0/1&quot;&gt;Olivia Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_K/0/1/0/all/0/1&quot;&gt;Krish Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_P/0/1/0/all/0/1&quot;&gt;Pengcheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_H/0/1/0/all/0/1&quot;&gt;Heather Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirkpatrick_S/0/1/0/all/0/1&quot;&gt;Sharon Kirkpatrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07749">
<title>OmnimatteRF: Robust Omnimatte with 3D Background Modeling. (arXiv:2309.07749v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07749</link>
<description rdf:parseType="Literal">&lt;p&gt;Video matting has broad applications, from adding interesting effects to
casually captured movies to assisting video production professionals. Matting
with associated effects such as shadows and reflections has also attracted
increasing research activity, and methods like Omnimatte have been proposed to
separate dynamic foreground objects of interest into their own layers. However,
prior works represent video backgrounds as 2D image layers, limiting their
capacity to express more complicated scenes, thus hindering application to
real-world videos. In this paper, we propose a novel video matting method,
OmnimatteRF, that combines dynamic 2D foreground layers and a 3D background
model. The 2D layers preserve the details of the subjects, while the 3D
background robustly reconstructs scenes in real-world videos. Extensive
experiments demonstrate that our method reconstructs scenes with better quality
on various videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Geng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia-Bin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Changil Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yipeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1&quot;&gt;Matthias Zwicker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saraf_A/0/1/0/all/0/1&quot;&gt;Ayush Saraf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07752">
<title>DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis. (arXiv:2309.07752v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07752</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present the decomposed triplane-hash neural radiance fields
(DT-NeRF), a framework that significantly improves the photorealistic rendering
of talking faces and achieves state-of-the-art results on key evaluation
datasets. Our architecture decomposes the facial region into two specialized
triplanes: one specialized for representing the mouth, and the other for the
broader facial features. We introduce audio features as residual terms and
integrate them as query vectors into our model through an audio-mouth-face
transformer. Additionally, our method leverages the capabilities of Neural
Radiance Fields (NeRF) to enrich the volumetric representation of the entire
face through additive volumetric rendering techniques. Comprehensive
experimental evaluations corroborate the effectiveness and superiority of our
proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoqian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07753">
<title>Co-Salient Object Detection with Semantic-Level Consensus Extraction and Dispersion. (arXiv:2309.07753v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07753</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a group of images, co-salient object detection (CoSOD) aims to
highlight the common salient object in each image. There are two factors
closely related to the success of this task, namely consensus extraction, and
the dispersion of consensus to each image. Most previous works represent the
group consensus using local features, while we instead utilize a hierarchical
Transformer module for extracting semantic-level consensus. Therefore, it can
obtain a more comprehensive representation of the common object category, and
exclude interference from other objects that share local similarities with the
target object. In addition, we propose a Transformer-based dispersion module
that takes into account the variation of the co-salient object in different
scenes. It distributes the consensus to the image feature maps in an
image-specific way while making full use of interactions within the group.
These two modules are integrated with a ViT encoder and an FPN-like decoder to
form an end-to-end trainable network, without additional branch and auxiliary
loss. The proposed method is evaluated on three commonly used CoSOD datasets
and achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peiran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yadong Mu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07760">
<title>PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07760</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained vision-language models such as CLIP have demonstrated great
potential in zero-shot transferability to downstream tasks. However, to attain
optimal performance, the manual selection of prompts is necessary to improve
alignment between the downstream image distribution and the textual class
descriptions. This manual prompt engineering is the major challenge for
deploying such models in practice since it requires domain expertise and is
extremely time-consuming. To avoid non-trivial prompt engineering, recent work
Context Optimization (CoOp) introduced the concept of prompt learning to the
vision domain using learnable textual tokens. While CoOp can achieve
substantial improvements over manual prompts, its learned context is worse
generalizable to wider unseen classes within the same dataset. In this work, we
present Prompt Learning with Reparameterization Encoder (PRE) - a simple and
efficient method that enhances the generalization ability of the learnable
prompt to unseen classes while maintaining the capacity to learn Base classes.
Instead of directly optimizing the prompts, PRE employs a prompt encoder to
reparameterize the input prompt embeddings, enhancing the exploration of
task-specific knowledge from few-shot samples. Experiments and extensive
ablation studies on 8 benchmarks demonstrate that our approach is an efficient
method for prompt learning. Specifically, PRE achieves a notable enhancement of
5.60% in average accuracy on New classes and 3% in Harmonic mean compared to
CoOp in the 16-shot setting, all achieved within a good training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minh_A/0/1/0/all/0/1&quot;&gt;Anh Pham Thi Minh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07778">
<title>Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.07778</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational pathology uses artificial intelligence to enable precision
medicine and decision support systems through the analysis of whole slide
images. It has the potential to revolutionize the diagnosis and treatment of
cancer. However, a major challenge to this objective is that for many specific
computational pathology tasks the amount of data is inadequate for development.
To address this challenge, we created Virchow, a 632 million parameter deep
neural network foundation model for computational pathology. Using
self-supervised learning, Virchow is trained on 1.5 million hematoxylin and
eosin stained whole slide images from diverse tissue groups, which is orders of
magnitude more data than previous works. When evaluated on downstream tasks
including tile-level pan-cancer detection and subtyping and slide-level
biomarker prediction, Virchow outperforms state-of-the-art systems both on
internal datasets drawn from the same population as the pretraining data as
well as external public datasets. Virchow achieves 93% balanced accuracy for
pancancer tile classification, and AUCs of 0.983 for colon microsatellite
instability status prediction and 0.967 for breast CDH1 status prediction. The
gains in performance highlight the importance of pretraining on massive
pathology image datasets, suggesting pretraining on even larger datasets could
continue improving performance for many high-impact applications where limited
amounts of training data are available, such as drug outcome prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vorontsov_E/0/1/0/all/0/1&quot;&gt;Eugene Vorontsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bozkurt_A/0/1/0/all/0/1&quot;&gt;Alican Bozkurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Casson_A/0/1/0/all/0/1&quot;&gt;Adam Casson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shaikovski_G/0/1/0/all/0/1&quot;&gt;George Shaikovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zelechowski_M/0/1/0/all/0/1&quot;&gt;Michal Zelechowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mathieu_P/0/1/0/all/0/1&quot;&gt;Philippe Mathieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eck_A/0/1/0/all/0/1&quot;&gt;Alexander van Eck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Viret_J/0/1/0/all/0/1&quot;&gt;Julian Viret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Robert_E/0/1/0/all/0/1&quot;&gt;Eric Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Kan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kun_J/0/1/0/all/0/1&quot;&gt;Jeremy D. Kun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Matthew C. H. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bernhard_J/0/1/0/all/0/1&quot;&gt;Jan Bernhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Godrich_R/0/1/0/all/0/1&quot;&gt;Ran A. Godrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oakley_G/0/1/0/all/0/1&quot;&gt;Gerard Oakley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Millar_E/0/1/0/all/0/1&quot;&gt;Ewan Millar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hanna_M/0/1/0/all/0/1&quot;&gt;Matthew Hanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Retamero_J/0/1/0/all/0/1&quot;&gt;Juan Retamero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moye_W/0/1/0/all/0/1&quot;&gt;William A. Moye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yousfi_R/0/1/0/all/0/1&quot;&gt;Razik Yousfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klimstra_D/0/1/0/all/0/1&quot;&gt;David Klimstra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rothrock_B/0/1/0/all/0/1&quot;&gt;Brandon Rothrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fuchs_T/0/1/0/all/0/1&quot;&gt;Thomas J. Fuchs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07796">
<title>For A More Comprehensive Evaluation of 6DoF Object Pose Tracking. (arXiv:2309.07796v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07796</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous evaluations on 6DoF object pose tracking have presented obvious
limitations along with the development of this area. In particular, the
evaluation protocols are not unified for different methods, the widely-used
YCBV dataset contains significant annotation error, and the error metrics also
may be biased. As a result, it is hard to fairly compare the methods, which has
became a big obstacle for developing new algorithms. In this paper we
contribute a unified benchmark to address the above problems. For more accurate
annotation of YCBV, we propose a multi-view multi-object global pose refinement
method, which can jointly refine the poses of all objects and view cameras,
resulting in sub-pixel sub-millimeter alignment errors. The limitations of
previous scoring methods and error metrics are analyzed, based on which we
introduce our improved evaluation methods. The unified benchmark takes both
YCBV and BCOT as base datasets, which are shown to be complementary in scene
categories. In experiments, we validate the precision and reliability of the
proposed global pose refinement method with a realistic semi-synthesized
dataset particularly for YCBV, and then present the benchmark results unifying
learning&amp;amp;non-learning and RGB&amp;amp;RGBD methods, with some finds not discovered in
previous studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1&quot;&gt;Fan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuangbing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xueying Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Changhe Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07808">
<title>What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07808</link>
<description rdf:parseType="Literal">&lt;p&gt;More research attention has recently been given to end-to-end autonomous
driving technologies where the entire driving pipeline is replaced with a
single neural network because of its simpler structure and faster inference
time. Despite this appealing approach largely reducing the components in
driving pipeline, its simplicity also leads to interpretability problems and
safety issues &lt;a href=&quot;/abs/2003.06404&quot;&gt;arXiv:2003.06404&lt;/a&gt;. The trained policy is not always compliant with
the traffic rules and it is also hard to discover the reason for the
misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are
also critical to autonomous driving&apos;s security and feasibility to perceive the
surrounding environment under complex driving scenarios. In this paper, we
proposed P-CSG, a novel penalty-based imitation learning approach with cross
semantics generation sensor fusion technologies to increase the overall
performance of End-to-End Autonomous Driving. We conducted an assessment of our
model&apos;s performance using the Town 05 Long benchmark, achieving an impressive
driving score improvement of over 15%. Furthermore, we conducted robustness
evaluations against adversarial attacks like FGSM and Dot attacks, revealing a
substantial increase in robustness compared to baseline models.More detailed
information, such as code-based resources, ablation studies and videos can be
found at https://hk-zh.github.io/p-csg-plus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongkuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_A/0/1/0/all/0/1&quot;&gt;Aifen Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Letian Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07819">
<title>Decomposition of linear tensor transformations. (arXiv:2309.07819v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07819</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main issues in computing a tensor decomposition is how to choose
the number of rank-one components, since there is no finite algorithms for
determining the rank of a tensor. A commonly used approach for this purpose is
to find a low-dimensional subspace by solving an optimization problem and
assuming the number of components is fixed. However, even though this algorithm
is efficient and easy to implement, it often converges to poor local minima and
suffers from outliers and noise. The aim of this paper is to develop a
mathematical framework for exact tensor decomposition that is able to represent
a tensor as the sum of a finite number of low-rank tensors. In the paper three
different problems will be carried out to derive: i) the decomposition of a
non-negative self-adjoint tensor operator; ii) the decomposition of a linear
tensor transformation; iii) the decomposition of a generic tensor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turchetti_C/0/1/0/all/0/1&quot;&gt;Claudio Turchetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07823">
<title>Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery. (arXiv:2309.07823v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07823</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic road extraction from satellite imagery using deep learning is a
viable alternative to traditional manual mapping. Therefore it has received
considerable attention recently. However, most of the existing methods are
supervised and require pixel-level labeling, which is tedious and error-prone.
To make matters worse, the earth has a diverse range of terrain, vegetation,
and man-made objects. It is well known that models trained in one area
generalize poorly to other areas. Various shooting conditions such as light and
angel, as well as different image processing techniques further complicate the
issue. It is impractical to develop training data to cover all image styles.
This paper proposes to leverage OpenStreetMap road data as weak labels and
large scale satellite imagery to pre-train semantic segmentation models. Our
extensive experimental results show that the prediction accuracy increases with
the amount of the weakly labeled data, as well as the road density in the areas
chosen for training. Using as much as 100 times more data than the widely used
DeepGlobe road dataset, our model with the D-LinkNet architecture and the
ResNet-50 backbone exceeds the top performer of the current DeepGlobe
leaderboard. Furthermore, due to large-scale pre-training, our model
generalizes much better than those trained with only the curated datasets,
implying great application potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1&quot;&gt;Shiqiao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1&quot;&gt;Zonglin Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Siwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07846">
<title>MC-NeRF: Muti-Camera Neural Radiance Fields for Muti-Camera Image Acquisition Systems. (arXiv:2309.07846v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07846</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) employ multi-view images for 3D scene
representation and have shown remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
NeRF-based methods often assume a global unique camera and seldom consider
scenarios with multiple cameras. Besides, some pose-robust methods still remain
susceptible to suboptimal solutions when poses are poor initialized. In this
paper, we propose MC-NeRF, a method can jointly optimize both intrinsic and
extrinsic parameters for bundle-adjusting Neural Radiance Fields. Firstly, we
conduct a theoretical analysis to tackle the degenerate case and coupling issue
that arise from the joint optimization between intrinsic and extrinsic
parameters. Secondly, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Lastly, we present a global
end-to-end network with training sequence that enables the regression of
intrinsic and extrinsic parameters, along with the rendering network. Moreover,
most existing datasets are designed for unique camera, we create a new dataset
that includes four different styles of multi-camera acquisition systems,
allowing readers to generate custom datasets. Experiments confirm the
effectiveness of our method when each image corresponds to different camera
parameters. Specifically, we adopt up to 110 images with 110 different
intrinsic and extrinsic parameters, to achieve 3D scene representation without
providing initial poses. The Code and supplementary materials are available at
https://in2-viaun.github.io/MC-NeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Lutong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yufeng Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1&quot;&gt;Mengyin Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07849">
<title>TFNet: Exploiting Temporal Cues for Fast and Accurate LiDAR Semantic Segmentation. (arXiv:2309.07849v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07849</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR semantic segmentation plays a crucial role in enabling autonomous
driving and robots to understand their surroundings accurately and robustly.
There are different types of methods, such as point-based, range image-based,
and polar-based. Among these, range image-based methods are widely used due to
their balance between accuracy and speed. However, they face a significant
challenge known as the ``many-to-one&apos;&apos; problem caused by the range image&apos;s
limited horizontal and vertical angular resolution, where around 20% of the 3D
points are occluded during model inference based on our observation. In this
paper, we present TFNet, a range image-based LiDAR semantic segmentation method
that utilizes temporal information to address this issue. Specifically, we
incorporate a temporal fusion layer to extract useful information from previous
scans and integrate it with the current scan. We then design a max-voting-based
post-processing technique to correct false predictions, particularly those
caused by the ``many-to-one&apos;&apos; issue. Experiments on two benchmarks and seven
backbones of three modalities demonstrate the effectiveness and scalability of
our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;ShiJie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xieyuanli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Teli Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1&quot;&gt;Wang Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1&quot;&gt;Juergen Gall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Junwei Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07866">
<title>Gradient constrained sharpness-aware prompt learning for vision-language models. (arXiv:2309.07866v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07866</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper targets a novel trade-off problem in generalizable prompt learning
for vision-language models (VLM), i.e., improving the performance on unseen
classes while maintaining the performance on seen classes. Comparing with
existing generalizable methods that neglect the seen classes degradation, the
setting of this problem is more strict and fits more closely with practical
applications. To solve this problem, we start from the optimization
perspective, and leverage the relationship between loss landscape geometry and
model generalization ability. By analyzing the loss landscape of the
state-of-the-art method and the widely-used Sharpness-aware Minimization (SAM),
we conclude that the trade-off performance correlates to both loss value and
loss sharpness, while each of them are indispensable. However, we find the
optimizing gradient of existing methods cannot always maintain high consistency
with both loss value and loss sharpness during the whole optimization
procedure. To this end, we propose an novel SAM-based method for prompt
learning, denoted as Gradient Constrained Sharpness-aware Context Optimization
(GCSCoOp), to dynamically constrains the optimizing gradient, thus achieving
above two-fold optimization objective simultaneously. Extensive experiments
verify the effectiveness of GCSCoOp in the trade-off problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liangchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07878">
<title>Using network metrics to explore the community structure that underlies movement patterns. (arXiv:2309.07878v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2309.07878</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims to explore the community structure of Santiago de Chile by
analyzing the movement patterns of its residents. We use a dataset containing
the approximate locations of home and work places for a subset of anonymized
residents to construct a network that represents the movement patterns within
the city. Through the analysis of this network, we aim to identify the
communities or sub-cities that exist within Santiago de Chile and gain insights
into the factors that drive the spatial organization of the city. We employ
modularity optimization algorithms and clustering techniques to identify the
communities within the network. Our results present that the novelty of
combining community detection algorithms with segregation tools provides new
insights to further the understanding of the complex geography of segregation
during working hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minh_A/0/1/0/all/0/1&quot;&gt;Anh Pham Thi Minh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Abhishek Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1&quot;&gt;Soumya Snigdha Kundu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07880">
<title>mEBAL2 Database and Benchmark: Image-based Multispectral Eyeblink Detection. (arXiv:2309.07880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07880</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces a new multispectral database and novel approaches for
eyeblink detection in RGB and Near-Infrared (NIR) individual images. Our
contributed dataset (mEBAL2, multimodal Eye Blink and Attention Level
estimation, Version 2) is the largest existing eyeblink database, representing
a great opportunity to improve data-driven multispectral approaches for blink
detection and related applications (e.g., attention level estimation and
presentation attack detection in face biometrics). mEBAL2 includes 21,100 image
sequences from 180 different students (more than 2 million labeled images in
total) while conducting a number of e-learning tasks of varying difficulty or
taking a real course on HTML initiation through the edX MOOC platform. mEBAL2
uses multiple sensors, including two Near-Infrared (NIR) and one RGB camera to
capture facial gestures during the execution of the tasks, as well as an
Electroencephalogram (EEG) band to get the cognitive activity of the user and
blinking events. Furthermore, this work proposes a Convolutional Neural Network
architecture as benchmark for blink detection on mEBAL2 with performances up to
97%. Different training methodologies are implemented using the RGB spectrum,
NIR spectrum, and the combination of both to enhance the performance on
existing eyeblink detectors. We demonstrate that combining NIR and RGB images
during training improves the performance of RGB eyeblink detectors (i.e.,
detection based only on a RGB image). Finally, the generalization capacity of
the proposed eyeblink detectors is validated in wilder and more challenging
environments like the HUST-LEBW dataset to show the usefulness of mEBAL2 to
train a new generation of data-driven approaches for eyeblink detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daza_R/0/1/0/all/0/1&quot;&gt;Roberto Daza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07888">
<title>A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors. (arXiv:2309.07888v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07888</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel local-global feature fusion framework for body-weight
exercise recognition with floor-based dynamic pressure maps. One step further
from the existing studies using deep neural networks mainly focusing on global
feature extraction, the proposed framework aims to combine local and global
features using image processing techniques and the YOLO object detection to
localize pressure profiles from different body parts and consider physical
constraints. The proposed local feature extraction method generates two sets of
high-level local features consisting of cropped pressure mapping and numerical
features such as angular orientation, location on the mat, and pressure area.
In addition, we adopt a knowledge distillation for regularization to preserve
the knowledge of the global feature extraction and improve the performance of
the exercise recognition. Our experimental results demonstrate a notable 11
percent improvement in F1 score for exercise recognition while preserving
label-specific features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1&quot;&gt;Davinder Pal Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_L/0/1/0/all/0/1&quot;&gt;Lala Shakti Swarup Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1&quot;&gt;Sungho Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1&quot;&gt;Paul Lukowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07891">
<title>HandNeRF: Learning to Reconstruct Hand-Object Interaction Scene from a Single RGB Image. (arXiv:2309.07891v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07891</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a method to learn hand-object interaction prior for
reconstructing a 3D hand-object scene from a single RGB image. The inference as
well as training-data generation for 3D hand-object scene reconstruction is
challenging due to the depth ambiguity of a single image and occlusions by the
hand and object. We turn this challenge into an opportunity by utilizing the
hand shape to constrain the possible relative configuration of the hand and
object geometry. We design a generalizable implicit function, HandNeRF, that
explicitly encodes the correlation of the 3D hand shape features and 2D object
features to predict the hand and object scene geometry. With experiments on
real-world datasets, we show that HandNeRF is able to reconstruct hand-object
scenes of novel grasp configurations more accurately than comparable methods.
Moreover, we demonstrate that object reconstruction from HandNeRF ensures more
accurate execution of a downstream task, such as grasping for robotic
hand-over.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hongsuk Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavan_Dafle_N/0/1/0/all/0/1&quot;&gt;Nikhil Chavan-Dafle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiacheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isler_V/0/1/0/all/0/1&quot;&gt;Volkan Isler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyunsoo Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07906">
<title>Generative Image Dynamics. (arXiv:2309.07906v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07906</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach to modeling an image-space prior on scene dynamics.
Our prior is learned from a collection of motion trajectories extracted from
real video sequences containing natural, oscillating motion such as trees,
flowers, candles, and clothes blowing in the wind. Given a single image, our
trained model uses a frequency-coordinated diffusion sampling process to
predict a per-pixel long-term motion representation in the Fourier domain,
which we call a neural stochastic motion texture. This representation can be
converted into dense motion trajectories that span an entire video. Along with
an image-based rendering module, these trajectories can be used for a number of
downstream applications, such as turning still images into seamlessly looping
dynamic videos, or allowing users to realistically interact with objects in
real pictures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1&quot;&gt;Richard Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1&quot;&gt;Noah Snavely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holynski_A/0/1/0/all/0/1&quot;&gt;Aleksander Holynski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07907">
<title>Physically Plausible Full-Body Hand-Object Interaction Synthesis. (arXiv:2309.07907v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07907</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a physics-based method for synthesizing dexterous hand-object
interactions in a full-body setting. While recent advancements have addressed
specific facets of human-object interactions, a comprehensive physics-based
approach remains a challenge. Existing methods often focus on isolated segments
of the interaction process and rely on data-driven techniques that may result
in artifacts. In contrast, our proposed method embraces reinforcement learning
(RL) and physics simulation to mitigate the limitations of data-driven
approaches. Through a hierarchical framework, we first learn skill priors for
both body and hand movements in a decoupled setting. The generic skill priors
learn to decode a latent skill embedding into the motion of the underlying
part. A high-level policy then controls hand-object interactions in these
pretrained latent spaces, guided by task objectives of grasping and 3D target
trajectory following. It is trained using a novel reward function that combines
an adversarial style term with a task reward, encouraging natural motions while
fulfilling the task incentives. Our method successfully accomplishes the
complete interaction task, from approaching an object to grasping and
subsequent manipulation. We compare our approach against kinematics-based
baselines and show that it leads to more physically plausible motions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_J/0/1/0/all/0/1&quot;&gt;Jona Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1&quot;&gt;Sammy Christen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1&quot;&gt;Muhammed Kocabas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksan_E/0/1/0/all/0/1&quot;&gt;Emre Aksan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07909">
<title>Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07909</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised contrastive learning methods have recently seen significant
improvements, particularly through data augmentation strategies that aim to
produce robust and generalizable representations. However, prevailing data
augmentation methods, whether hand designed or based on foundation models, tend
to rely heavily on prior knowledge or external data. This dependence often
compromises their effectiveness and efficiency. Furthermore, the applicability
of most existing data augmentation strategies is limited when transitioning to
other research domains, especially science-related data. This limitation stems
from the paucity of prior knowledge and labeled data available in these
domains. To address these challenges, we introduce DiffAug-a novel and
efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure
that the augmented and original data share a smoothed latent space, which is
achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug
first mines sufficient prior semantic knowledge about the neighborhood. This
provides a constraint to guide the diffusion steps, eliminating the need for
labels, external data/models, or prior knowledge. Designed as an
architecture-agnostic framework, DiffAug provides consistent improvements.
Specifically, it improves image classification and clustering accuracy by
1.6%~4.5%. When applied to biological data, DiffAug improves performance by up
to 10.1%, with an average improvement of 5.8%. DiffAug shows good performance
in both vision and biological domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1&quot;&gt;Zelin Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Panpan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan.Z Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07910">
<title>TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting. (arXiv:2309.07910v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07910</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing volumetric methods for predicting 3D human pose estimation are
accurate, but computationally expensive and optimized for single time-step
prediction. We present TEMPO, an efficient multi-view pose estimation model
that learns a robust spatiotemporal representation, improving pose accuracy
while also tracking and forecasting human pose. We significantly reduce
computation compared to the state-of-the-art by recurrently computing
per-person 2D pose features, fusing both spatial and temporal information into
a single representation. In doing so, our model is able to use spatiotemporal
context to predict more accurate human poses without sacrificing efficiency. We
further use this representation to track human poses over time as well as
predict future poses. Finally, we demonstrate that our model is able to
generalize across datasets without scene-specific fine-tuning. TEMPO achieves
10$\%$ better MPJPE with a 33$\times$ improvement in FPS compared to TesseTrack
on the challenging CMU Panoptic Studio dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_R/0/1/0/all/0/1&quot;&gt;Rohan Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris Kitani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1&quot;&gt;Laszlo A. Jeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07911">
<title>Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning. (arXiv:2309.07911v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07911</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, large-scale pre-trained language-image models like CLIP have shown
extraordinary capabilities for understanding spatial contents, but naively
transferring such models to video recognition still suffers from unsatisfactory
temporal modeling capabilities. Existing methods insert tunable structures into
or in parallel with the pre-trained model, which either requires
back-propagation through the whole pre-trained model and is thus
resource-demanding, or is limited by the temporal reasoning capability of the
pre-trained structure. In this work, we present DiST, which disentangles the
learning of spatial and temporal aspects of videos. Specifically, DiST uses a
dual-encoder structure, where a pre-trained foundation model acts as the
spatial encoder, and a lightweight network is introduced as the temporal
encoder. An integration branch is inserted between the encoders to fuse
spatio-temporal information. The disentangled spatial and temporal learning in
DiST is highly efficient because it avoids the back-propagation of massive
pre-trained parameters. Meanwhile, we empirically show that disentangled
learning with an extra network for integration benefits both spatial and
temporal understanding. Extensive experiments on five benchmarks show that DiST
delivers better performance than existing state-of-the-art methods by
convincing gaps. When pre-training on the large-scale Kinetics-710, we achieve
89.7% on Kinetics-400 with a frozen ViT-L model, which verifies the scalability
of DiST. Codes and models can be found in
https://github.com/alibaba-mmai-research/DiST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1&quot;&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Changxin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Deli Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1&quot;&gt;Nong Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07914">
<title>ALWOD: Active Learning for Weakly-Supervised Object Detection. (arXiv:2309.07914v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07914</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection (OD), a crucial vision task, remains challenged by the lack
of large training datasets with precise object localization labels. In this
work, we propose ALWOD, a new framework that addresses this problem by fusing
active learning (AL) with weakly and semi-supervised object detection
paradigms. Because the performance of AL critically depends on the model
initialization, we propose a new auxiliary image generator strategy that
utilizes an extremely small labeled set, coupled with a large weakly tagged set
of images, as a warm-start for AL. We then propose a new AL acquisition
function, another critical factor in AL success, that leverages the
student-teacher OD pair disagreement and uncertainty to effectively propose the
most informative images to annotate. Finally, to complete the AL loop, we
introduce a new labeling task delegated to human annotators, based on selection
and correction of model-proposed detections, which is both rapid and effective
in labeling the informative images. We demonstrate, across several challenging
benchmarks, that ALWOD significantly narrows the gap between the ODs trained on
few partially labeled but strategically selected image instances and those that
rely on the fully-labeled data. Our code is publicly available on
https://github.com/seqam-lab/ALWOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_V/0/1/0/all/0/1&quot;&gt;Velibor Ilic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiatong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kisacanin_B/0/1/0/all/0/1&quot;&gt;Branislav Kisacanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07915">
<title>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07915</link>
<description rdf:parseType="Literal">&lt;p&gt;Starting from the resurgence of deep learning, vision-language models (VLMs)
benefiting from large language models (LLMs) have never been so popular.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images. The issue can
traced back to the architectural design of VLMs or pre-training data.
Specifically, the current VLMs primarily emphasize utilizing multi-modal data
with a single image some, rather than multi-modal prompts with interleaved
multiple images and text. Even though some newly proposed VLMs could handle
user prompts with multiple images, pre-training data does not provide more
sophisticated multi-modal prompts than interleaved image and text crawled from
the web. We propose MMICL to address the issue by considering both the model
and data perspectives. We introduce a well-designed architecture capable of
seamlessly integrating visual and textual context in an interleaved manner and
MIC dataset to reduce the gap between the training data and the complex user
prompts in real-world applications, including: 1) multi-modal context with
interleaved images and text, 2) textual references for each image, and 3)
multi-image data with spatial, logical, or temporal relationships. Our
experiments confirm that MMICL achieves new stat-of-the-art zero-shot and
few-shot performance on a wide range of general vision-language tasks,
especially for complex reasoning benchmarks including MME and MMBench. Our
analysis demonstrates that MMICL effectively deals with the challenge of
complex multi-modal prompt understanding. The experiments on ScienceQA-IMG also
show that MMICL successfully alleviates the issue of language bias in VLMs,
which we believe is the reason behind the advanced performance of MMICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haozhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zefan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1&quot;&gt;Shuzheng Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1&quot;&gt;Kaikai An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zixuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wenjuan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1&quot;&gt;Baobao Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07917">
<title>Looking at words and points with attention: a benchmark for text-to-shape coherence. (arXiv:2309.07917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07917</link>
<description rdf:parseType="Literal">&lt;p&gt;While text-conditional 3D object generation and manipulation have seen rapid
progress, the evaluation of coherence between generated 3D shapes and input
textual descriptions lacks a clear benchmark. The reason is twofold: a) the low
quality of the textual descriptions in the only publicly available dataset of
text-shape pairs; b) the limited effectiveness of the metrics used to
quantitatively assess such coherence. In this paper, we propose a comprehensive
solution that addresses both weaknesses. Firstly, we employ large language
models to automatically refine textual descriptions associated with shapes.
Secondly, we propose a quantitative metric to assess text-to-shape coherence,
through cross-attention mechanisms. To validate our approach, we conduct a user
study and compare quantitatively our metric with existing ones. The refined
dataset, the new metric and a set of text-shape pairs validated by the user
study comprise a novel, fine-grained benchmark that we publicly release to
foster research on text-to-shape coherence of text-conditioned 3D generative
models. Benchmark available at
https://cvlab-unibo.github.io/CrossCoherence-Web/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amaduzzi_A/0/1/0/all/0/1&quot;&gt;Andrea Amaduzzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisanti_G/0/1/0/all/0/1&quot;&gt;Giuseppe Lisanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1&quot;&gt;Samuele Salti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1&quot;&gt;Luigi Di Stefano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07918">
<title>Unified Human-Scene Interaction via Prompted Chain-of-Contacts. (arXiv:2309.07918v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07918</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-Scene Interaction (HSI) is a vital component of fields like embodied AI
and virtual reality. Despite advancements in motion quality and physical
plausibility, two pivotal factors, versatile interaction control and the
development of a user-friendly interface, require further exploration before
the practical application of HSI. This paper presents a unified HSI framework,
UniHSI, which supports unified control of diverse interactions through language
commands. This framework is built upon the definition of interaction as Chain
of Contacts (CoC): steps of human joint-object part pairs, which is inspired by
the strong correlation between interaction types and human-object contact
regions. Based on the definition, UniHSI constitutes a Large Language Model
(LLM) Planner to translate language prompts into task plans in the form of CoC,
and a Unified Controller that turns CoC into uniform task execution. To
facilitate training and evaluation, we collect a new dataset named ScenePlan
that encompasses thousands of task plans generated by LLMs based on diverse
scenarios. Comprehensive experiments demonstrate the effectiveness of our
framework in versatile task execution and generalizability to real scanned
scenes. The project page is at https://github.com/OpenRobotLab/UniHSI .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zeqi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jinkun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07920">
<title>Large-Vocabulary 3D Diffusion Model with Transformer. (arXiv:2309.07920v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07920</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating diverse and high-quality 3D assets with an automatic generative
model is highly desirable. Despite extensive efforts on 3D generation, most
existing works focus on the generation of a single category or a few
categories. In this paper, we introduce a diffusion-based feed-forward
framework for synthesizing massive categories of real-world 3D objects with a
single generative model. Notably, there are three major challenges for this
large-vocabulary 3D generation: a) the need for expressive yet efficient 3D
representation; b) large diversity in geometry and texture across categories;
c) complexity in the appearances of real-world objects. To this end, we propose
a novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for
handling challenges via three aspects. 1) Considering efficiency and
robustness, we adopt a revised triplane representation and improve the fitting
speed and accuracy. 2) To handle the drastic variations in geometry and
texture, we regard the features of all 3D objects as a combination of
generalized 3D knowledge and specialized 3D features. To extract generalized 3D
knowledge from diverse categories, we propose a novel 3D-aware transformer with
shared cross-plane attention. It learns the cross-plane relations across
different planes and aggregates the generalized 3D knowledge with specialized
3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance
the generalized 3D knowledge in the encoded triplanes for handling categories
with complex appearances. Extensive experiments on ShapeNet and OmniObject3D
(over 200 diverse real-world categories) convincingly demonstrate that a single
DiffTF model achieves state-of-the-art large-vocabulary 3D object generation
performance with large diversity, rich semantics, and high quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Ziang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1&quot;&gt;Fangzhou Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07921">
<title>OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects. (arXiv:2309.07921v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07921</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce OpenIllumination, a real-world dataset containing over 108K
images of 64 objects with diverse materials, captured under 72 camera views and
a large number of different illuminations. For each image in the dataset, we
provide accurate camera parameters, illumination ground truth, and foreground
segmentation masks. Our dataset enables the quantitative evaluation of most
inverse rendering and material decomposition methods for real objects. We
examine several state-of-the-art inverse rendering methods on our dataset and
compare their performances. The dataset and code can be found on the project
page: https://oppo-us-research.github.io/OpenIllumination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1&quot;&gt;Isabella Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Linghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1&quot;&gt;Ziyang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Haian Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Chin Ming Ryan Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1&quot;&gt;Ravi Ramamoorthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zexiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.08790">
<title>P1AC: Revisiting Absolute Pose From a Single Affine Correspondence. (arXiv:2011.08790v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2011.08790</link>
<description rdf:parseType="Literal">&lt;p&gt;Affine correspondences have traditionally been used to improve feature
matching over wide baselines. While recent work has successfully used affine
correspondences to solve various relative camera pose estimation problems, less
attention has been given to their use in absolute pose estimation. We introduce
the first general solution to the problem of estimating the pose of a
calibrated camera given a single observation of an oriented point and an affine
correspondence. The advantage of our approach (P1AC) is that it requires only a
single correspondence, in comparison to the traditional point-based approach
(P3P), significantly reducing the combinatorics in robust estimation. P1AC
provides a general solution that removes restrictive assumptions made in prior
work and is applicable to large-scale image-based localization. We propose a
minimal solution to the P1AC problem and evaluate our novel solver on synthetic
data, showing its numerical stability and performance under various types of
noise. On standard image-based localization benchmarks we show that P1AC
achieves more accurate results than the widely used P3P algorithm. Code for our
method is available at https://github.com/jonathanventura/P1AC/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1&quot;&gt;Jonathan Ventura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kukelova_Z/0/1/0/all/0/1&quot;&gt;Zuzana Kukelova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1&quot;&gt;Torsten Sattler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel Bar&amp;#xe1;th&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.08797">
<title>Towards Language-guided Visual Recognition via Dynamic Convolutions. (arXiv:2110.08797v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.08797</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we are committed to establishing an unified and end-to-end
multi-modal network via exploring the language-guided visual recognition. To
approach this target, we first propose a novel multi-modal convolution module
called Language-dependent Convolution (LaConv). Its convolution kernels are
dynamically generated based on natural language information, which can help
extract differentiated visual features for different multi-modal examples.
Based on the LaConv module, we further build the first fully language-driven
convolution network, termed as LaConvNet, which can unify the visual
recognition and multi-modal reasoning in one forward structure. To validate
LaConv and LaConvNet, we conduct extensive experiments on four benchmark
datasets of two vision-and-language tasks, i.e., visual question answering
(VQA) and referring expression comprehension (REC). The experimental results
not only shows the performance gains of LaConv compared to the existing
multi-modal modules, but also witness the merits of LaConvNet as an unified
network, including compact network, high generalization ability and excellent
performance, e.g., +4.7% on RefCOCO+.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Gen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongjian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yue Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.01210">
<title>Co-Teaching for Unsupervised Domain Adaptation and Expansion. (arXiv:2204.01210v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.01210</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) essentially trades a model&apos;s performance
on a source domain for improving its performance on a target domain. To resolve
the issue, Unsupervised Domain Expansion (UDE) has been proposed recently. UDE
tries to adapt the model for the target domain as UDA does, and in the meantime
maintains its source-domain performance. In both UDA and UDE settings, a model
tailored to a given domain, let it be the source or the target domain, is
assumed to well handle samples from the given domain. We question the
assumption by reporting the existence of cross-domain visual ambiguity: Given
the lack of a crystally clear boundary between the two domains, samples from
one domain can be visually close to the other domain. Such sorts of samples are
typically in minority in their host domain, so they tend to be overlooked by
the domain-specific model, but can be better handled by a model from the other
domain. We exploit this finding, and accordingly propose Co-Teaching (CT). The
CT method is instantiated with knowledge distillation based CT (kdCT) plus
mixup based CT (miCT). Specifically, kdCT transfers knowledge from a
leading-teacher network and an assistant-teacher network to a student network,
so the cross-domain ambiguity will be better handled by the student. Meanwhile,
miCT further enhances the generalization ability of the student. Extensive
experiments on two image classification datasets and two driving-scene
segmentation datasets justify the viability of CT for UDA and UDE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1&quot;&gt;Kaibin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1&quot;&gt;Qijie Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xirong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07913">
<title>A Survivor in the Era of Large-Scale Pretraining: An Empirical Study of One-Stage Referring Expression Comprehension. (arXiv:2204.07913v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07913</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the existing work in one-stage referring expression comprehension
(REC) mainly focuses on multi-modal fusion and reasoning, while the influence
of other factors in this task lacks in-depth exploration. To fill this gap, we
conduct an empirical study in this paper. Concretely, we first build a very
simple REC network called SimREC, and ablate 42 candidate designs/settings,
which covers the entire process of one-stage REC from network design to model
training. Afterwards, we conduct over 100 experimental trials on three
benchmark datasets of REC. The extensive experimental results not only show the
key factors that affect REC performance in addition to multi-modal fusion,
e.g., multi-scale features and data augmentation, but also yield some findings
that run counter to conventional understanding. For example, as a vision and
language (V&amp;amp;L) task, REC does is less impacted by language prior. In addition,
with a proper combination of these findings, we can improve the performance of
SimREC by a large margin, e.g., +27.12% on RefCOCO+, which outperforms all
existing REC methods. But the most encouraging finding is that with much less
training overhead and parameters, SimREC can still achieve better performance
than a set of large-scale pre-trained models, e.g., UNITER and VILLA,
portraying the special role of REC in existing V&amp;amp;L research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Gen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiamu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11110">
<title>Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11110</link>
<description rdf:parseType="Literal">&lt;p&gt;Grasping inhomogeneous objects in real-world applications remains a
challenging task due to the unknown physical properties such as mass
distribution and coefficient of friction. In this study, we propose a
meta-learning algorithm called ConDex, which incorporates Conditional Neural
Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical
properties of objects using depth images. ConDex efficiently acquires physical
embeddings from limited trials, enabling precise grasping point estimation.
Furthermore, ConDex is capable of updating the predicted grasping quality
iteratively from new trials in an online fashion. To the best of our knowledge,
we are the first who generate two object datasets focusing on inhomogeneous
physical properties with varying mass distributions and friction coefficients.
Extensive evaluations in simulation demonstrate ConDex&apos;s superior performance
over DexNet-2.0 and existing meta-learning-based grasping pipelines.
Furthermore, ConDex shows robust generalization to previously unseen real-world
objects despite training solely in the simulation. The synthetic and real-world
datasets will be published as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1&quot;&gt;Ning Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1&quot;&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1&quot;&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00085">
<title>Machine Learning and Computer Vision Techniques in Continuous Beehive Monitoring Applications: A survey. (arXiv:2208.00085v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00085</link>
<description rdf:parseType="Literal">&lt;p&gt;Wide use and availability of the machine learning and computer vision
techniques allows development of relatively complex monitoring systems in many
domains. Besides the traditional industrial domain, new application appears
also in biology and agriculture, where we could speak about the detection of
infections, parasites and weeds, but also about automated monitoring and early
warning systems. This is also connected with the introduction of the easily
accessible hardware and development kits such as Arduino, or RaspberryPi
family. In this paper, we survey 50 existing papers focusing on the methods of
automated beehive monitoring methods using the computer vision techniques,
particularly on the pollen and Varroa mite detection together with the bee
traffic monitoring. Such systems could also be used for the monitoring of the
honeybee colonies and for the inspection of their health state, which could
identify potentially dangerous states before the situation is critical, or to
better plan periodic bee colony inspections and therefore save significant
costs. Later, we also include analysis of the research trends in this
application field and we outline the possible direction of the new
explorations. Our paper is aimed also at veterinary and apidology professionals
and experts, who might not be familiar with machine learning to introduce them
to its possibilities, therefore each family of applications is opened by a
brief theoretical introduction and motivation related to its base method. We
hope that this paper will inspire other scientists to use machine learning
techniques for other applications in beehive monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1&quot;&gt;Simon Bilik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemcik_T/0/1/0/all/0/1&quot;&gt;Tomas Zemcik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratochvila_L/0/1/0/all/0/1&quot;&gt;Lukas Kratochvila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricanek_D/0/1/0/all/0/1&quot;&gt;Dominik Ricanek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1&quot;&gt;Milos Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambanini_S/0/1/0/all/0/1&quot;&gt;Sebastian Zambanini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1&quot;&gt;Karel Horak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02048">
<title>Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02048</link>
<description rdf:parseType="Literal">&lt;p&gt;During image editing, existing deep generative models tend to re-synthesize
the entire output from scratch, including the unedited regions. This leads to a
significant waste of computation, especially for minor editing operations. In
this work, we present Spatially Sparse Inference (SSI), a general-purpose
technique that selectively performs computation for edited regions and
accelerates various generative models, including both conditional GANs and
diffusion models. Our key observation is that users prone to gradually edit the
input image. This motivates us to cache and reuse the feature maps of the
original image. Given an edited image, we sparsely apply the convolutional
filters to the edited regions while reusing the cached features for the
unedited areas. Based on our algorithm, we further propose Sparse Incremental
Generative Engine (SIGE) to convert the computation reduction to latency
reduction on off-the-shelf hardware. With about $1\%$-area edits, SIGE
accelerates DDPM by $3.0\times$ on NVIDIA RTX 3090 and $4.6\times$ on Apple M1
Pro GPU, Stable Diffusion by $7.2\times$ on 3090, and GauGAN by $5.6\times$ on
3090 and $5.2\times$ on M1 Pro GPU. Compared to our conference version, we
extend SIGE to accommodate attention layers and apply it to Stable Diffusion.
Additionally, we offer support for Apple M1 Pro GPU and include more results
with large and sequential edits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Muyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Ji Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1&quot;&gt;Chenlin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06660">
<title>Far Away in the Deep Space: Dense Nearest-Neighbor-Based Out-of-Distribution Detection. (arXiv:2211.06660v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06660</link>
<description rdf:parseType="Literal">&lt;p&gt;The key to out-of-distribution detection is density estimation of the
in-distribution data or of its feature representations. This is particularly
challenging for dense anomaly detection in domains where the in-distribution
data has a complex underlying structure. Nearest-Neighbors approaches have been
shown to work well in object-centric data domains, such as industrial
inspection and image classification. In this paper, we show that
nearest-neighbor approaches also yield state-of-the-art results on dense
novelty detection in complex driving scenes when working with an appropriate
feature representation. In particular, we find that transformer-based
architectures produce representations that yield much better similarity metrics
for the task. We identify the multi-head structure of these models as one of
the reasons, and demonstrate a way to transfer some of the improvements to
CNNs. Ultimately, the approach is simple and non-invasive, i.e., it does not
affect the primary segmentation performance, refrains from training on examples
of anomalies, and achieves state-of-the-art results on RoadAnomaly,
StreetHazards, and SegmentMeIfYouCan-Anomaly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galesso_S/0/1/0/all/0/1&quot;&gt;Silvio Galesso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1&quot;&gt;Max Argus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09597">
<title>Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v7 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09597</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, as an essential ability for complex problem-solving, can provide
back-end support for various real-world applications, such as medical
diagnosis, negotiation, etc. This paper provides a comprehensive survey of
cutting-edge research on reasoning with language model prompting. We introduce
research works with comparisons and summaries and provide systematic resources
to help beginners. We also discuss the potential reasons for emerging such
reasoning abilities and highlight future research directions. Resources are
available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated
periodically).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Shuofei Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yixin Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08657">
<title>Economical Quaternion Extraction from a Human Skeletal Pose Estimate using 2-D Cameras. (arXiv:2303.08657v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08657</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel algorithm to extract a quaternion from a
two dimensional camera frame for estimating a contained human skeletal pose.
The problem of pose estimation is usually tackled through the usage of stereo
cameras and intertial measurement units for obtaining depth and euclidean
distance for measurement of points in 3D space. However, the usage of these
devices comes with a high signal processing latency as well as a significant
monetary cost. By making use of MediaPipe, a framework for building perception
pipelines for human pose estimation, the proposed algorithm extracts a
quaternion from a 2-D frame capturing an image of a human object at a sub-fifty
millisecond latency while also being capable of deployment at edges with a
single camera frame and a generally low computational resource availability,
especially for use cases involving last-minute detection and reaction by
autonomous robots. The algorithm seeks to bypass the funding barrier and
improve accessibility for robotics researchers involved in designing control
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radhakrishna_S/0/1/0/all/0/1&quot;&gt;Sriram Radhakrishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanyam_A/0/1/0/all/0/1&quot;&gt;Adithya Balasubramanyam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09858">
<title>Preventing Unauthorized AI Over-Analysis by Medical Image Adversarial Watermarking. (arXiv:2303.09858v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09858</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement of deep learning has facilitated the integration of
Artificial Intelligence (AI) into clinical practices, particularly in
computer-aided diagnosis. Given the pivotal role of medical images in various
diagnostic procedures, it becomes imperative to ensure the responsible and
secure utilization of AI techniques. However, the unauthorized utilization of
AI for image analysis raises significant concerns regarding patient privacy and
potential infringement on the proprietary rights of data custodians.
Consequently, the development of pragmatic and cost-effective strategies that
safeguard patient privacy and uphold medical image copyrights emerges as a
critical necessity. In direct response to this pressing demand, we present a
pioneering solution named Medical Image Adversarial watermarking (MIAD-MARK).
Our approach introduces watermarks that strategically mislead unauthorized AI
diagnostic models, inducing erroneous predictions without compromising the
integrity of the visual content. Importantly, our method integrates an
authorization protocol tailored for legitimate users, enabling the removal of
the MIAD-MARK through encryption-generated keys. Through extensive experiments,
we validate the efficacy of MIAD-MARK across three prominent medical image
datasets. The empirical outcomes demonstrate the substantial impact of our
approach, notably reducing the accuracy of standard AI diagnostic models to a
mere 8.57% under white box conditions and 45.83% in the more challenging black
box scenario. Additionally, our solution effectively mitigates unauthorized
exploitation of medical images even in the presence of sophisticated watermark
removal networks. Notably, those AI diagnosis networks exhibit a meager average
accuracy of 38.59% when applied to images protected by MIAD-MARK, underscoring
the robustness of our safeguarding mechanism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pu_B/0/1/0/all/0/1&quot;&gt;Bangzheng Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiji Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_C/0/1/0/all/0/1&quot;&gt;Chen Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13867">
<title>Few Shot Medical Image Segmentation with Cross Attention Transformer. (arXiv:2303.13867v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13867</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation has made significant progress in recent years.
Deep learning-based methods are recognized as data-hungry techniques, requiring
large amounts of data with manual annotations. However, manual annotation is
expensive in the field of medical image analysis, which requires
domain-specific expertise. To address this challenge, few-shot learning has the
potential to learn new classes from only a few examples. In this work, we
propose a novel framework for few-shot medical image segmentation, termed
CAT-Net, based on cross masked attention Transformer. Our proposed network
mines the correlations between the support image and query image, limiting them
to focus only on useful foreground information and boosting the representation
capacity of both the support prototype and query features. We further design an
iterative refinement framework that refines the query image segmentation
iteratively and promotes the support feature in turn. We validated the proposed
method on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental
results demonstrate the superior performance of our method compared to
state-of-the-art methods and the effectiveness of each component. Code:
https://github.com/hust-linyi/CAT-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yufan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16617">
<title>NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination. (arXiv:2303.16617v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16617</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse rendering methods aim to estimate geometry, materials and
illumination from multi-view RGB images. In order to achieve better
decomposition, recent approaches attempt to model indirect illuminations
reflected from different materials via Spherical Gaussians (SG), which,
however, tends to blur the high-frequency reflection details. In this paper, we
propose an end-to-end inverse rendering pipeline that decomposes materials and
illumination from multi-view images, while considering near-field indirect
illumination. In a nutshell, we introduce the Monte Carlo sampling based path
tracing and cache the indirect illumination as neural radiance, enabling a
physics-faithful and easy-to-optimize inverse rendering method. To enhance
efficiency and practicality, we leverage SG to represent the smooth environment
illuminations and apply importance sampling techniques. To supervise indirect
illuminations from unobserved directions, we develop a novel radiance
consistency constraint between implicit neural radiance and path tracing
results of unobserved rays along with the joint optimization of materials and
illuminations, thus significantly improving the decomposition performance.
Extensive experiments demonstrate that our method outperforms the
state-of-the-art on multiple synthetic and real datasets, especially in terms
of inter-reflection decomposition.Our code and data are available at
https://woolseyyy.github.io/nefii/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Changjie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01029">
<title>Domain Generalization for Crop Segmentation with Knowledge Distillation. (arXiv:2304.01029v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01029</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, precision agriculture has gradually oriented farming closer
to automation processes to support all the activities related to field
management. Service robotics plays a predominant role in this evolution by
deploying autonomous agents that can navigate fields while performing tasks
without human intervention, such as monitoring, spraying, and harvesting. To
execute these precise actions, mobile robots need a real-time perception system
that understands their surroundings and identifies their targets in the wild.
Generalizing to new crops and environmental conditions is critical for
practical applications, as labeled samples are rarely available. In this paper,
we investigate the problem of crop segmentation and propose a novel approach to
enhance domain generalization using knowledge distillation. In the proposed
framework, we transfer knowledge from an ensemble of models individually
trained on source domains to a student model that can adapt to unseen target
domains. To evaluate the proposed method, we present a synthetic multi-domain
dataset for crop segmentation containing plants of variegate shapes and
covering different terrain styles, weather conditions, and light scenarios for
more than 50,000 samples. We demonstrate significant improvements in
performance over state-of-the-art methods and superior sim-to-real
generalization. Our approach provides a promising solution for domain
generalization in crop segmentation and has the potential to enhance a wide
variety of precision agriculture applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1&quot;&gt;Simone Angarano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1&quot;&gt;Mauro Martini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navone_A/0/1/0/all/0/1&quot;&gt;Alessandro Navone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1&quot;&gt;Marcello Chiaberge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07097">
<title>Interpretable Weighted Siamese Network to Predict the Time to Onset of Alzheimer&apos;s Disease from MRI Images. (arXiv:2304.07097v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07097</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s Disease (AD) is a progressive disease preceded by Mild Cognitive
Impairment (MCI). Early detection of AD is crucial for making treatment
decisions. However, most of the literature on computer-assisted detection of AD
focuses on classifying brain images into one of three major categories:
healthy, MCI, and AD; or categorizing MCI patients into (1) progressive: those
who progress from MCI to AD at a future examination time, and (2) stable: those
who stay as MCI and never progress to AD. This misses the opportunity to
accurately identify the trajectory of progressive MCI patients. In this paper,
we revisit the brain image classification task for AD identification and
re-frame it as an ordinal classification task to predict how close a patient is
to the severe AD stage. To this end, we select progressive MCI patients from
the Alzheimer&apos;s Disease Neuroimaging Initiative (ADNI) dataset and construct an
ordinal dataset with a prediction target that indicates the time to progression
to AD. We train a Siamese network model to predict the time to onset of AD
based on MRI brain images. We also propose a Weighted variety of Siamese
network and compare its performance to a baseline model. Our evaluations show
that incorporating a weighting factor to Siamese networks brings considerable
performance gain at predicting how close input brain MRI images are to
progressing to AD. Moreover, we complement our results with an interpretation
of the learned embedding space of the Siamese networks using a model
explainability technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hagos_M/0/1/0/all/0/1&quot;&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Belton_N/0/1/0/all/0/1&quot;&gt;Niamh Belton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Killeen_R/0/1/0/all/0/1&quot;&gt;Ronan P. Killeen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08981">
<title>MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. (arXiv:2304.08981v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08981</link>
<description rdf:parseType="Literal">&lt;p&gt;The first Multimodal Emotion Recognition Challenge (MER 2023) was
successfully held at ACM Multimedia. The challenge focuses on system robustness
and consists of three distinct tracks: (1) MER-MULTI, where participants are
required to recognize both discrete and dimensional emotions; (2) MER-NOISE, in
which noise is added to test videos for modality robustness evaluation; (3)
MER-SEMI, which provides a large amount of unlabeled samples for
semi-supervised learning. In this paper, we introduce the motivation behind
this challenge, describe the benchmark dataset, and provide some statistics
about participants. To continue using this dataset after MER 2023, please sign
a new End User License Agreement and send it to our official email address
merchallenge.contact@gmail.com. We believe this high-quality dataset can become
a new benchmark in multimodal emotion recognition, especially for the Chinese
research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1&quot;&gt;Zheng Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haiyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Licai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mingyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Ke Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Ying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jiangyan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guoying Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jianhua Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10520">
<title>Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10520</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),
efficiently learn a rich representation of the input. However, for adapting to
downstream tasks, they require a sufficient amount of labeled data since their
rich features code not only objects but also less relevant image background. In
contrast, Instance Discrimination (ID) methods focus on objects. In this work,
we study how to combine the efficiency and scalability of MIM with the ability
of ID to perform downstream classification in the absence of large amounts of
labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning
(MAE-CT), a sequential approach that utilizes the implicit clustering of the
Nearest Neighbor Contrastive Learning (NNCLR) objective to induce abstraction
in the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features such
that they form semantic clusters of objects without using any labels. Notably,
MAE-CT does not rely on hand-crafted augmentations and frequently achieves its
best performances while using only minimal augmentations (crop &amp;amp; flip).
Further, MAE-CT is compute efficient as it requires at most 10% overhead
compared to MAE re-training. Applied to large and huge Vision Transformer (ViT)
models, MAE-CT excels over previous self-supervised methods trained on ImageNet
in linear probing, k-NN and low-shot classification accuracy as well as in
unsupervised clustering accuracy. With ViT-H/16 MAE-CT achieves a new
state-of-the-art in linear probing of 82.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehner_J/0/1/0/all/0/1&quot;&gt;Johannes Lehner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkin_B/0/1/0/all/0/1&quot;&gt;Benedikt Alkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furst_A/0/1/0/all/0/1&quot;&gt;Andreas F&amp;#xfc;rst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rumetshofer_E/0/1/0/all/0/1&quot;&gt;Elisabeth Rumetshofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miklautz_L/0/1/0/all/0/1&quot;&gt;Lukas Miklautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14408">
<title>Autocharacterization: Automated and Scalable Semiconductor Property Estimation from High-throughput Experiments using Computer Vision. (arXiv:2304.14408v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14408</link>
<description rdf:parseType="Literal">&lt;p&gt;High-throughput materials synthesis methods have risen in popularity due to
their potential to accelerate the design and discovery of novel functional
materials, such as solution-processed semiconductors. After synthesis, key
material properties must be measured and characterized to validate discovery
and provide feedback to optimization cycles. However, with the boom in
development of high-throughput synthesis tools that champion production rates
up to $10^4$ samples per hour with flexible form factors, most sample
characterization methods are either slow (conventional rates of $10^1$ samples
per hour, approximately 1000x slower) or rigid (e.g., designed for
standard-size microplates), resulting in a bottleneck that impedes the
materials-design process. To overcome this challenge, we propose a set of
automated material property characterization (autocharacterization) tools that
leverage the adaptive, parallelizable, and scalable nature of computer vision
to accelerate the throughput of characterization by 85x compared to the
non-automated workflow. We demonstrate a generalizable composition mapping tool
for high-throughput synthesized binary material systems as well as two scalable
autocharacterization algorithms that (1) autonomously compute the band gap of
200 unique compositions in 6 minutes and (2) autonomously compute the degree of
degradation in 200 unique compositions in 20 minutes, generating ultra-high
compositional resolution trends of band gap and stability. We demonstrate that
the developed band gap and degradation detection autocharacterization methods
achieve 98.5% accuracy and 96.9% accuracy, respectively, on the
FA$_{1-x}$MA$_{x}$PbI$_3$, $0\leq x \leq 1$ perovskite semiconductor system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siemenn_A/0/1/0/all/0/1&quot;&gt;Alexander E. Siemenn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aissi_E/0/1/0/all/0/1&quot;&gt;Eunice Aissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheng_F/0/1/0/all/0/1&quot;&gt;Fang Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tiihonen_A/0/1/0/all/0/1&quot;&gt;Armi Tiihonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kavak_H/0/1/0/all/0/1&quot;&gt;Hamide Kavak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Das_B/0/1/0/all/0/1&quot;&gt;Basita Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Buonassisi_T/0/1/0/all/0/1&quot;&gt;Tonio Buonassisi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07180">
<title>Robust Saliency-Aware Distillation for Few-shot Fine-grained Visual Recognition. (arXiv:2305.07180v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07180</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing novel sub-categories with scarce samples is an essential and
challenging research topic in computer vision. Existing literature addresses
this challenge by employing local-based representation approaches, which may
not sufficiently facilitate meaningful object-specific semantic understanding,
leading to a reliance on apparent background correlations. Moreover, they
primarily rely on high-dimensional local descriptors to construct complex
embedding space, potentially limiting the generalization. To address the above
challenges, this article proposes a novel model called RSaG for few-shot
fine-grained visual recognition. RSaG introduces additional saliency-aware
supervision via saliency detection to guide the model toward focusing on the
intrinsic discriminative regions. Specifically, RSaG utilizes the saliency
detection model to emphasize the critical regions of each sub-category,
providing additional object-specific information for fine-grained prediction.
RSaG transfers such information with two symmetric branches in a mutual
learning paradigm. Furthermore, RSaG exploits inter-regional relationships to
enhance the informativeness of the representation and subsequently summarize
the highlighted details into contextual embeddings to facilitate the effective
transfer, enabling quick generalization to novel sub-categories. The proposed
approach is empirically evaluated on three widely used benchmarks,
demonstrating its superior performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haiqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C. L. Philip Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xinrong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15021">
<title>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15021</link>
<description rdf:parseType="Literal">&lt;p&gt;Embodied AI is a crucial frontier in robotics, capable of planning and
executing action sequences for robots to accomplish long-horizon tasks in
physical environments. In this work, we introduce EmbodiedGPT, an end-to-end
multi-modal foundation model for embodied AI, empowering embodied agents with
multi-modal understanding and execution capabilities. To achieve this, we have
made the following efforts: (i) We craft a large-scale embodied planning
dataset, termed EgoCOT. The dataset consists of carefully selected videos from
the Ego4D dataset, along with corresponding high-quality language instructions.
Specifically, we generate a sequence of sub-goals with the &quot;Chain of Thoughts&quot;
mode for effective embodied planning. (ii) We introduce an efficient training
approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B
large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We
introduce a paradigm for extracting task-related features from LLM-generated
planning queries to form a closed loop between high-level planning and
low-level control. Extensive experiments show the effectiveness of EmbodiedGPT
on embodied tasks, including embodied planning, embodied control, visual
captioning, and visual question answering. Notably, EmbodiedGPT significantly
enhances the success rate of the embodied control task by extracting more
effective features. It has achieved a remarkable 1.6 times increase in success
rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World
benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yao Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Mengkang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jun Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19862">
<title>Self-supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive. (arXiv:2305.19862v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19862</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern consumer cameras usually employ the rolling shutter (RS) mechanism,
where images are captured by scanning scenes row-by-row, yielding RS
distortions for dynamic scenes. To correct RS distortions, existing methods
adopt a fully supervised learning manner, where high framerate global shutter
(GS) images should be collected as ground-truth supervision. In this paper, we
propose a Self-supervised learning framework for Dual reversed RS distortions
Correction (SelfDRSC), where a DRSC network can be learned to generate a high
framerate GS video only based on dual RS images with reversed distortions. In
particular, a bidirectional distortion warping module is proposed for
reconstructing dual reversed RS images, and then a self-supervised loss can be
deployed to train DRSC network by enhancing the cycle consistency between input
and reconstructed dual reversed RS images. Besides start and end RS scanning
time, GS images at arbitrary intermediate scanning time can also be supervised
in SelfDRSC, thus enabling the learned DRSC network to generate a high
framerate GS video. Moreover, a simple yet effective self-distillation strategy
is introduced in self-supervised loss for mitigating boundary artifacts in
generated GS images. On synthetic dataset, SelfDRSC achieves better or
comparable quantitative metrics in comparison to state-of-the-art methods
trained in the full supervision manner. On real-world RS cases, our SelfDRSC
can produce high framerate GS videos with finer correction textures and better
temporary consistency. The source code and trained models are made publicly
available at https://github.com/shangwei5/SelfDRSC. We also provide an
implementation in HUAWEI Mindspore at
https://github.com/Hunter-Will/SelfDRSC-mindspore.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_W/0/1/0/all/0/1&quot;&gt;Wei Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1&quot;&gt;Dongwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chaoyu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaotao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1&quot;&gt;Lei Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10830">
<title>3D VR Sketch Guided 3D Shape Prototyping and Exploration. (arXiv:2306.10830v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10830</link>
<description rdf:parseType="Literal">&lt;p&gt;3D shape modeling is labor-intensive, time-consuming, and requires years of
expertise. To facilitate 3D shape modeling, we propose a 3D shape generation
network that takes a 3D VR sketch as a condition. We assume that sketches are
created by novices without art training and aim to reconstruct geometrically
realistic 3D shapes of a given category. To handle potential sketch ambiguity,
our method creates multiple 3D shapes that align with the original sketch&apos;s
structure. We carefully design our method, training the model step-by-step and
leveraging multi-modal 3D shape representation to support training with limited
training data. To guarantee the realism of generated 3D shapes we leverage the
normalizing flow that models the distribution of the latent space of 3D shapes.
To encourage the fidelity of the generated 3D shapes to an input sketch, we
propose a dedicated loss that we deploy at different stages of the training
process. The code is available at https://github.com/Rowl1ng/3Dsketch2shape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Ling Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1&quot;&gt;Pinaki Nath Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1&quot;&gt;Yulia Gryaditskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14565">
<title>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14565</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the promising progress in multi-modal tasks, current large
multi-modal models (LMM) are prone to hallucinating inconsistent descriptions
with respect to the associated image and human instructions. This paper
addresses this issue by introducing the first large and diverse visual
instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.
Our dataset consists of 120k visual instructions generated by GPT4, covering 16
vision-and-language tasks with open-ended instructions and answers. Unlike
existing studies that primarily focus on positive instruction samples, we
design LRV-Instruction to include both positive and negative instructions for
more robust visual instruction tuning. Our negative instructions are designed
at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent
Element Manipulation. To efficiently measure the hallucination generated by
LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel
approach to evaluate visual instruction tuning without the need for
human-annotated groundtruth answers and can adapt to diverse instruction
formats. We conduct comprehensive experiments to investigate the hallucination
of LMMs. Our results demonstrate that existing LMMs exhibit significant
hallucination when presented with our negative instructions, particularly with
Existent Element Manipulation instructions. Moreover, by finetuning MiniGPT4 on
LRV-Instruction, we successfully mitigate hallucination while improving
performance on public datasets using less training data compared to
state-of-the-art methods. Additionally, we observed that a balanced ratio of
positive and negative instances in the training data leads to a more robust
model. Updates of our project are available at
https://fuxiaoliu.github.io/LRV/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yacoob_Y/0/1/0/all/0/1&quot;&gt;Yaser Yacoob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06526">
<title>AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion. (arXiv:2307.06526v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06526</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-trained vision-language models allow for the zero-shot
text-based generation of 3D avatars. The previous state-of-the-art method
utilized CLIP to supervise neural implicit models that reconstructed a human
body mesh. However, this approach has two limitations. Firstly, the lack of
avatar-specific models can cause facial distortion and unrealistic clothing in
the generated avatars. Secondly, CLIP only provides optimization direction for
the overall appearance, resulting in less impressive results. To address these
limitations, we propose AvatarFusion, the first framework to use a latent
diffusion model to provide pixel-level guidance for generating human-realistic
avatars while simultaneously segmenting clothing from the avatar&apos;s body.
AvatarFusion includes the first clothing-decoupled neural implicit avatar model
that employs a novel Dual Volume Rendering strategy to render the decoupled
skin and clothing sub-models in one space. We also introduce a novel
optimization method, called Pixel-Semantics Difference-Sampling (PS-DS), which
semantically separates the generation of body and clothes, and generates a
variety of clothing styles. Moreover, we establish the first benchmark for
zero-shot text-to-avatar generation. Our experimental results demonstrate that
our framework outperforms previous approaches, with significant improvements
observed in all metrics. Additionally, since our model is clothing-decoupled,
we can exchange the clothes of avatars. Code are available on our project page
https://hansenhuang0823.github.io/AvatarFusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shuo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jia Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16834">
<title>Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16834</link>
<description rdf:parseType="Literal">&lt;p&gt;Innovative enhancement in embedded system platforms, specifically hardware
accelerations, significantly influence the application of deep learning in
real-world scenarios. These innovations translate human labor efforts into
automated intelligent systems employed in various areas such as autonomous
driving, robotics, Internet-of-Things (IoT), and numerous other impactful
applications. NVIDIA&apos;s Jetson platform is one of the pioneers in offering
optimal performance regarding energy efficiency and throughput in the execution
of deep learning algorithms. Previously, most benchmarking analysis was based
on 2D images with a single deep learning model for each comparison result. In
this paper, we implement an end-to-end video-based crime-scene anomaly
detection system inputting from surveillance videos and the system is deployed
and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin
Nano). The comparison analysis includes the integration of Torch-TensorRT as a
software developer kit from NVIDIA for the model performance optimisation. The
system is built based on the PySlowfast open-source project from Facebook as
the coding template. The end-to-end system process comprises the videos from
camera, data preprocessing pipeline, feature extractor and the anomaly
detection. We provide the experience of an AI-based system deployment on
various Jetson Edge devices with Docker technology. Regarding anomaly
detectors, a weakly supervised video-based deep learning model called Robust
Temporal Feature Magnitude Learning (RTFM) is applied in the system. The
approach system reaches 47.56 frames per second (FPS) inference speed on a
Jetson edge device with only 3.11 GB RAM usage total. We also discover the
promising Jetson device that the AI system achieves 15% better performance than
the previous version of Jetson devices while consuming 50% less energy power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hoang Viet Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thinh Gia Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1&quot;&gt;Chuong Dinh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1&quot;&gt;An Dinh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1&quot;&gt;Hien Bich Vo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08518">
<title>Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08518</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional geometric registration based estimation methods only exploit the
CAD model implicitly, which leads to their dependence on observation quality
and deficiency to occlusion. To address the problem,the paper proposes a
bidirectional correspondence prediction network with a point-wise
attention-aware mechanism. This network not only requires the model points to
predict the correspondence but also explicitly models the geometric
similarities between observations and the model prior. Our key insight is that
the correlations between each model point and scene point provide essential
information for learning point-pair matches. To further tackle the correlation
noises brought by feature distribution divergence, we design a simple but
effective pseudo-siamese network to improve feature homogeneity. Experimental
results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that
the proposed method achieves better performance than other state-of-the-art
methods under the same evaluation criteria. Its robustness in estimating poses
is greatly improved, especially in an environment with severe occlusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guangjian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Rong Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12966">
<title>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. (arXiv:2308.12966v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12966</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Qwen-VL series, a set of large-scale vision-language models
(LVLMs) designed to perceive and understand both text and images. Comprising
Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks
like image captioning, question answering, visual localization, and flexible
interaction. The evaluation covers a wide range of tasks including zero-shot
captioning, visual or document visual question answering, and grounding. We
demonstrate the Qwen-VL outperforms existing LVLMs. We present their
architecture, training, capabilities, and performance, highlighting their
contributions to advancing multimodal artificial intelligence. Code, demo and
models are available at https://github.com/QwenLM/Qwen-VL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinze Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shusheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Sinan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13437">
<title>Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models. (arXiv:2308.13437v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13437</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Multimodal Large Language Models (MLLMs) that enable Large Language
Models (LLMs) to interpret images through visual instruction tuning have
achieved significant success. However, existing visual instruction tuning
methods only utilize image-language instruction data to align the language and
image modalities, lacking a more fine-grained cross-modal alignment. In this
paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which
extends the functionality of MLLMs by integrating an additional region-level
vision encoder. This integration promotes a more detailed comprehension of
images for the MLLM. In addition, to efficiently achieve a fine-grained
alignment between the vision modules and the LLM, we design multiple data
generation strategies to construct an image-region-language instruction
dataset. Finally, we present both quantitative experiments and qualitative
analysis that demonstrate the superiority of the proposed model. Code and data
will be released at https://github.com/PVIT-official/PVIT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Ruoyu Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fuwen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13469">
<title>RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network. (arXiv:2308.13469v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13469</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-domain few-shot segmentation (CD-FSS) aims to achieve semantic
segmentation in previously unseen domains with a limited number of annotated
samples. Although existing CD-FSS models focus on cross-domain feature
transformation, relying exclusively on inter-domain knowledge transfer may lead
to the loss of critical intra-domain information. To this end, we propose a
novel residual transformation network (RestNet) that facilitates knowledge
transfer while retaining the intra-domain support-query feature information.
Specifically, we propose a Semantic Enhanced Anchor Transform (SEAT) module
that maps features to a stable domain-agnostic space using advanced semantics.
Additionally, an Intra-domain Residual Enhancement (IRE) module is designed to
maintain the intra-domain representation of the original discriminant space in
the new space. We also propose a mask prediction strategy based on prototype
fusion to help the model gradually learn how to segment. Our RestNet can
transfer cross-domain knowledge from both inter-domain and intra-domain without
requiring additional fine-tuning. Extensive experiments on ISIC, Chest X-ray,
and FSS-1000 show that our RestNet achieves state-of-the-art performance. Our
code will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chuang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenkai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00655">
<title>RigNet++: Efficient Repetitive Image Guided Network for Depth Completion. (arXiv:2309.00655v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00655</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth completion aims to recover dense depth maps from sparse ones, where
color images are often used to facilitate this task. Recent depth methods
primarily focus on image guided learning frameworks. However, blurry guidance
in the image and unclear structure in the depth still impede their performance.
To tackle these challenges, we explore an efficient repetitive design in our
image guided network to gradually and sufficiently recover depth values.
Specifically, the efficient repetition is embodied in both the image guidance
branch and depth generation branch. In the former branch, we design a dense
repetitive hourglass network to extract discriminative image features of
complex environments, which can provide powerful contextual instruction for
depth prediction. In the latter branch, we introduce a repetitive guidance
module based on dynamic convolution, in which an efficient convolution
factorization is proposed to reduce the complexity while modeling
high-frequency structures progressively. Extensive experiments indicate that
our approach achieves superior or competitive results on KITTI, VKITTI, NYUv2,
3D60, and Matterport3D datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00827">
<title>Few shot font generation via transferring similarity guided global style and quantization local style. (arXiv:2309.00827v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00827</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic few-shot font generation (AFFG), aiming at generating new fonts
with only a few glyph references, reduces the labor cost of manually designing
fonts. However, the traditional AFFG paradigm of style-content disentanglement
cannot capture the diverse local details of different fonts. So, many
component-based approaches are proposed to tackle this problem. The issue with
component-based approaches is that they usually require special pre-defined
glyph components, e.g., strokes and radicals, which is infeasible for AFFG of
different languages. In this paper, we present a novel font generation approach
by aggregating styles from character similarity-guided global features and
stylized component-level representations. We calculate the similarity scores of
the target character and the referenced samples by measuring the distance along
the corresponding channels from the content features, and assigning them as the
weights for aggregating the global style features. To better capture the local
styles, a cross-attention-based style transfer module is adopted to transfer
the styles of reference glyphs to the components, where the components are
self-learned discrete latent codes through vector quantization without manual
definition. With these designs, our AFFG method could obtain a complete set of
component-level style representations, and also control the global glyph
characteristics. The experimental results reflect the effectiveness and
generalization of the proposed method on different linguistic scripts, and also
show its superiority when compared with other state-of-the-art methods. The
source code can be found at https://github.com/awei669/VQ-Font.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Wei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1&quot;&gt;Anna Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1&quot;&gt;Brian Kenji Iwana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shilin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00923">
<title>GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning. (arXiv:2309.00923v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00923</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates a challenging problem of zero-shot learning in the
multi-label scenario (MLZSL), wherein, the model is trained to recognize
multiple unseen classes within a sample (e.g., an image) based on seen classes
and auxiliary knowledge, e.g., semantic information. Existing methods usually
resort to analyzing the relationship of various seen classes residing in a
sample from the dimension of spatial or semantic characteristics, and transfer
the learned model to unseen ones. But they ignore the effective integration of
local and global features. That is, in the process of inferring unseen classes,
global features represent the principal direction of the image in the feature
space, while local features should maintain uniqueness within a certain range.
This integrated neglect will make the model lose its grasp of the main
components of the image. Relying only on the local existence of seen classes
during the inference stage introduces unavoidable bias. In this paper, we
propose a novel and effective group bi-enhancement framework for MLZSL, dubbed
GBE-MLZSL, to fully make use of such properties and enable a more accurate and
robust visual-semantic projection. Specifically, we split the feature maps into
several feature groups, of which each feature group can be trained
independently with the Local Information Distinguishing Module (LID) to ensure
uniqueness. Meanwhile, a Global Enhancement Module (GEM) is designed to
preserve the principal direction. Besides, a static graph structure is designed
to construct the correlation of local features. Experiments on large-scale
MLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that the
proposed GBE-MLZSL outperforms other state-of-the-art methods with large
margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1&quot;&gt;Peiran Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiewei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02561">
<title>Physically Grounded Vision-Language Models for Robotic Manipulation. (arXiv:2309.02561v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02561</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in vision-language models (VLMs) have led to improved
performance on tasks such as visual question answering and image captioning.
Consequently, these models are now well-positioned to reason about the physical
world, particularly within domains such as robotic manipulation. However,
current VLMs are limited in their understanding of the physical concepts (e.g.,
material, fragility) of common objects, which restricts their usefulness for
robotic manipulation tasks that involve interaction and physical reasoning
about such objects. To address this limitation, we propose PhysObjects, an
object-centric dataset of 39.6K crowd-sourced and 417K automated physical
concept annotations of common household objects. We demonstrate that
fine-tuning a VLM on PhysObjects improves its understanding of physical object
concepts, including generalization to held-out concepts, by capturing human
priors of these concepts from visual appearance. We incorporate this
physically-grounded VLM in an interactive framework with a large language
model-based robotic planner, and show improved planning performance on tasks
that require reasoning about physical object concepts, compared to baselines
that do not leverage physically-grounded VLMs. We additionally illustrate the
benefits of our physically-grounded VLM on a real robot, where it improves task
success rates. We release our dataset and provide further details and
visualizations of our results at https://iliad.stanford.edu/pg-vlm/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jensen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_B/0/1/0/all/0/1&quot;&gt;Bidipta Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Anirudha Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03955">
<title>SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with Simpler Solutions. (arXiv:2309.03955v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03955</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) show impressive performance for the
photorealistic free-view rendering of scenes. However, NeRFs require dense
sampling of images in the given scene, and their performance degrades
significantly when only a sparse set of views are available. Researchers have
found that supervising the depth estimated by the NeRF helps train it
effectively with fewer views. The depth supervision is obtained either using
classical approaches or neural networks pre-trained on a large dataset. While
the former may provide only sparse supervision, the latter may suffer from
generalization issues. As opposed to the earlier approaches, we seek to learn
the depth supervision by designing augmented models and training them along
with the NeRF. We design augmented models that encourage simpler solutions by
exploring the role of positional encoding and view-dependent radiance in
training the few-shot NeRF. The depth estimated by these simpler models is used
to supervise the NeRF depth estimates. Since the augmented models can be
inaccurate in certain regions, we design a mechanism to choose only reliable
depth estimates for supervision. Finally, we add a consistency loss between the
coarse and fine multi-layer perceptrons of the NeRF to ensure better
utilization of hierarchical sampling. We achieve state-of-the-art
view-synthesis performance on two popular datasets by employing the above
regularizations. The source code for our model can be found on our project
page: https://nagabhushansn95.github.io/publications/2023/SimpleNeRF.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somraj_N/0/1/0/all/0/1&quot;&gt;Nagabhushan Somraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanayil_A/0/1/0/all/0/1&quot;&gt;Adithyan Karanayil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soundararajan_R/0/1/0/all/0/1&quot;&gt;Rajiv Soundararajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05406">
<title>Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction. (arXiv:2309.05406v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05406</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffuse gliomas are malignant brain tumors that grow widespread through the
brain. The complex interactions between neoplastic cells and normal tissue, as
well as the treatment-induced changes often encountered, make glioma tumor
growth modeling challenging. In this paper, we present a novel end-to-end
network capable of generating future tumor masks and realistic MRIs of how the
tumor will look at any future time points for different treatment plans. Our
approach is based on cutting-edge diffusion probabilistic models and
deep-segmentation neural networks. We included sequential multi-parametric
magnetic resonance images (MRI) and treatment information as conditioning
inputs to guide the generative diffusion process. This allows for tumor growth
estimates at any given time point. We trained the model using real-world
postoperative longitudinal MRI data with glioma tumor growth trajectories
represented as tumor segmentation maps over time. The model has demonstrated
promising performance across a range of tasks, including the generation of
high-quality synthetic MRIs with tumor masks, time-series tumor segmentations,
and uncertainty estimates. Combined with the treatment-aware generated MRIs,
the tumor growth predictions with uncertainty estimates can provide useful
information for clinical decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qinghui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fuster_Garcia_E/0/1/0/all/0/1&quot;&gt;Elies Fuster-Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hovden_I/0/1/0/all/0/1&quot;&gt;Ivar Thokle Hovden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sederevicius_D/0/1/0/all/0/1&quot;&gt;Donatas Sederevicius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Skogen_K/0/1/0/all/0/1&quot;&gt;Karoline Skogen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+MacIntosh_B/0/1/0/all/0/1&quot;&gt;Bradley J MacIntosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grodem_E/0/1/0/all/0/1&quot;&gt;Edvard Gr&amp;#xf8;dem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schellhorn_T/0/1/0/all/0/1&quot;&gt;Till Schellhorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brandal_P/0/1/0/all/0/1&quot;&gt;Petter Brandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bjornerud_A/0/1/0/all/0/1&quot;&gt;Atle Bj&amp;#xf8;rnerud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Emblem_K/0/1/0/all/0/1&quot;&gt;Kyrre Eeg Emblem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05446">
<title>A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET/CT Images. (arXiv:2309.05446v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05446</link>
<description rdf:parseType="Literal">&lt;p&gt;Fluorodeoxyglucose (FDG) positron emission tomography (PET) combined with
computed tomography (CT) is considered the primary solution for detecting some
cancers, such as lung cancer and melanoma. Automatic segmentation of tumors in
PET/CT images can help reduce doctors&apos; workload, thereby improving diagnostic
quality. However, precise tumor segmentation is challenging due to the small
size of many tumors and the similarity of high-uptake normal areas to the tumor
regions. To address these issues, this paper proposes a
localization-to-segmentation framework (L2SNet) for precise tumor segmentation.
L2SNet first localizes the possible lesions in the lesion localization phase
and then uses the location cues to shape the segmentation results in the lesion
segmentation phase. To further improve the segmentation performance of L2SNet,
we design an adaptive threshold scheme that takes the segmentation results of
the two phases into consideration. The experiments with the MICCAI 2023
Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge dataset show
that our method achieved a competitive result and was ranked in the top 7
methods on the preliminary test set. Our work is available at:
https://github.com/MedCAI/L2SNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Linghan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jinpeng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongbing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06670">
<title>ShaDocFormer: A Shadow-attentive Threshold Detector with Cascaded Fusion Refiner for document shadow removal. (arXiv:2309.06670v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06670</link>
<description rdf:parseType="Literal">&lt;p&gt;Document shadow is a common issue that arise when capturing documents using
mobile devices, which significantly impacts the readability. Current methods
encounter various challenges including inaccurate detection of shadow masks and
estimation of illumination. In this paper, we propose ShaDocFormer, a
Transformer-based architecture that integrates traditional methodologies and
deep learning techniques to tackle the problem of document shadow removal. The
ShaDocFormer architecture comprises two components: the Shadow-attentive
Threshold Detector (STD) and the Cascaded Fusion Refiner (CFR). The STD module
employs a traditional thresholding technique and leverages the attention
mechanism of the Transformer to gather global information, thereby enabling
precise detection of shadow masks. The cascaded and aggregative structure of
the CFR module facilitates a coarse-to-fine restoration process for the entire
image. As a result, ShaDocFormer excels in accurately detecting and capturing
variations in both shadow and illumination, thereby enabling effective removal
of shadows. Extensive experiments demonstrate that ShaDocFormer outperforms
current state-of-the-art methods in both qualitative and quantitative
measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shenghong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zinuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1&quot;&gt;Chi-Man Pun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06724">
<title>Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06724</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to provide a general framework of for computational photography that
recovers the real scene from imperfect images, via the Deep Nonparametric
Convexified Filtering (DNCF). It is consists of a nonparametric deep network to
resemble the physical equations behind the image formation, such as denoising,
super-resolution, inpainting, and flash. DNCF has no parameterization dependent
on training data, therefore has a strong generalization and robustness to
adversarial image manipulation. During inference, we also encourage the network
parameters to be nonnegative and create a bi-convex function on the input and
parameters, and this adapts to second-order optimization algorithms with
insufficient running time, having 10X acceleration over Deep Image Prior. With
these tools, we empirically verify its capability to defend image
classification deep networks against adversary attack algorithms in real-time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wangni_J/0/1/0/all/0/1&quot;&gt;Jianqiao Wangni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06745">
<title>VEATIC: Video-based Emotion and Affect Tracking in Context Dataset. (arXiv:2309.06745v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06745</link>
<description rdf:parseType="Literal">&lt;p&gt;Human affect recognition has been a significant topic in psychophysics and
computer vision. However, the currently published datasets have many
limitations. For example, most datasets contain frames that contain only
information about facial expressions. Due to the limitations of previous
datasets, it is very hard to either understand the mechanisms for affect
recognition of humans or generalize well on common cases for computer vision
models trained on those datasets. In this work, we introduce a brand new large
dataset, the Video-based Emotion and Affect Tracking in Context Dataset
(VEATIC), that can conquer the limitations of the previous datasets. VEATIC has
124 video clips from Hollywood movies, documentaries, and home videos with
continuous valence and arousal ratings of each frame via real-time annotation.
Along with the dataset, we propose a new computer vision task to infer the
affect of the selected character via both context and character information in
each video frame. Additionally, we propose a simple model to benchmark this new
computer vision task. We also compare the performance of the pretrained model
using our dataset with other similar datasets. Experiments show the competing
results of our pretrained model via VEATIC, indicating the generalizability of
VEATIC. Our dataset is available at https://veatic.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhihang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1&quot;&gt;Jefferson Ortega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhimin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunhui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Stella X. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitney_D/0/1/0/all/0/1&quot;&gt;David Whitney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06902">
<title>CCSPNet-Joint: Efficient Joint Training Method for Traffic Sign Detection Under Extreme Conditions. (arXiv:2309.06902v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06902</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic sign detection is an important research direction in intelligent
driving. Unfortunately, existing methods often overlook extreme conditions such
as fog, rain, and motion blur. Moreover, the end-to-end training strategy for
image denoising and object detection models fails to utilize inter-model
information effectively. To address these issues, we propose CCSPNet, an
efficient feature extraction module based on Transformers and CNNs, which
effectively leverages contextual information, achieves faster inference speed
and provides stronger feature enhancement capabilities. Furthermore, we
establish the correlation between object detection and image denoising tasks
and propose a joint training model, CCSPNet-Joint, to improve data efficiency
and generalization. Finally, to validate our approach, we create the CCTSDB-AUG
dataset for traffic sign detection in extreme scenarios. Extensive experiments
have shown that CCSPNet achieves state-of-the-art performance in traffic sign
detection under extreme conditions. Compared to end-to-end methods,
CCSPNet-Joint achieves a 5.32% improvement in precision and an 18.09%
improvement in mAP@.5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1&quot;&gt;Haoqin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiangfang Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06987">
<title>Instance Adaptive Prototypical Contrastive Embedding for Generalized Zero Shot Learning. (arXiv:2309.06987v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06987</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized zero-shot learning(GZSL) aims to classify samples from seen and
unseen labels, assuming unseen labels are not accessible during training.
Recent advancements in GZSL have been expedited by incorporating
contrastive-learning-based (instance-based) embedding in generative networks
and leveraging the semantic relationship between data points. However, existing
embedding architectures suffer from two limitations: (1) limited
discriminability of synthetic features&apos; embedding without considering
fine-grained cluster structures; (2) inflexible optimization due to restricted
scaling mechanisms on existing contrastive embedding networks, leading to
overlapped representations in the embedding space. To enhance the quality of
representations in the embedding space, as mentioned in (1), we propose a
margin-based prototypical contrastive learning embedding network that reaps the
benefits of prototype-data (cluster quality enhancement) and implicit data-data
(fine-grained representations) interaction while providing substantial cluster
supervision to the embedding network and the generator. To tackle (2), we
propose an instance adaptive contrastive loss that leads to generalized
representations for unseen labels with increased inter-class margin. Through
comprehensive experimental evaluation, we show that our method can outperform
the current state-of-the-art on three benchmark datasets. Our approach also
consistently achieves the best unseen performance in the GZSL setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_R/0/1/0/all/0/1&quot;&gt;Riti Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vora_S/0/1/0/all/0/1&quot;&gt;Sahil Vora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baoxin Li&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>