<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.13272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.08364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.07436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.12987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1406.6046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1409.7291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1501.01678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1503.08992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04916" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.00004">
<title>Informational non-reductionist theory of consciousness that providing maximum accuracy of reality prediction. (arXiv:2401.00004v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00004</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper considers a non-reductionist theory of consciousness, which is not
reducible to theories of reality and to physiological or psychological
theories. Following D.I.Dubrovsky&apos;s &quot;informational approach&quot; to the &quot;Mind-Brain
Problem&quot;, we consider the reality through the prism of information about
observed phenomena, which, in turn, is perceived by subjective reality through
sensations, perceptions, feelings, etc., which, in turn, are information about
the corresponding brain processes. Within this framework the following
principle of the Information Theory of Consciousness (ITS) development is put
forward: the brain discovers all possible causal relations in the external
world and makes all possible inferences by them. The paper shows that ITS built
on this principle: (1) also base on the information laws of the structure of
external world; (2) explains the structure and functioning of the brain
functional systems and cellular ensembles; (3) ensures maximum accuracy of
predictions and the anticipation of reality; (4) resolves emerging
contradictions and (5) is an information theory of the brain&apos;s reflection of
reality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vityaev_E/0/1/0/all/0/1&quot;&gt;E.E. Vityaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00005">
<title>Consciousness as a logically consistent and prognostic model of reality. (arXiv:2401.00005v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00005</link>
<description rdf:parseType="Literal">&lt;p&gt;The work demonstrates that brain might reflect the external world causal
relationships in the form of a logically consistent and prognostic model of
reality, which shows up as consciousness. The paper analyses and solves the
problem of statistical ambiguity and provides a formal model of causal
relationships as probabilistic maximally specific rules. We suppose that brain
makes all possible inferences from causal relationships. We prove that the
suggested formal model has a property of an unambiguous inference: from
consistent premises we infer a consistent conclusion. It enables a set of all
inferences to form a consistent model of the perceived world. Causal
relationships may create fixed points of cyclic inter-predictable properties.
We consider the &quot;natural&quot; classification introduced by John St. Mill and
demonstrate that a variety of fixed points of the objects&apos; attributes forms a
&quot;natural&quot; classification of the external world. Then we consider notions of
&quot;natural&quot; categories and causal models of categories, introduced by Eleanor
Rosch and Bob Rehder and demonstrate that fixed points of causal relationships
between objects attributes, which we perceive, formalize these notions. If the
&quot;natural&quot; classification describes the objects of the external world, and
&quot;natural&quot; concepts the perception of these objects, then the theory of
integrated information, introduced by G. Tononi, describes the information
processes of the brain for &quot;natural&quot; concepts formation that reflects the
&quot;natural&quot; classification. We argue that integrated information provides high
accuracy of the objects identification. A computer-based experiment is provided
that illustrates fixed points formation for coded digits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vityaev_E/0/1/0/all/0/1&quot;&gt;Evgenii Vityaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00006">
<title>Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation. (arXiv:2401.00006v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00006</link>
<description rdf:parseType="Literal">&lt;p&gt;Building open-ended learning agents involves challenges in pre-trained
language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle
with context-specific real-time interactions, while RL methods face efficiency
issues for exploration. To this end, we propose OpenContra, a co-training
framework that cooperates LLMs and GRL to construct an open-ended agent capable
of comprehending arbitrary human instructions. The implementation comprises two
stages: (1) fine-tuning an LLM to translate human instructions into structured
goals, and curriculum training a goal-conditioned RL policy to execute
arbitrary goals; (2) collaborative training to make the LLM and RL policy learn
to adapt each, achieving open-endedness on instruction space. We conduct
experiments on Contra, a battle royale FPS game with a complex and vast goal
space. The results show that an agent trained with OpenContra comprehends
arbitrary human instructions and completes goals with a high completion ratio,
which proves that OpenContra may be the first practical solution for
constructing open-ended embodied agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shaopeng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fuxian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jing Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00007">
<title>Modeling arousal potential of epistemic emotions using Bayesian information gain: Inquiry cycle driven by free energy fluctuations. (arXiv:2401.00007v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00007</link>
<description rdf:parseType="Literal">&lt;p&gt;Epistemic emotions, such as curiosity and interest, drive the inquiry
process. This study proposes a novel formulation of epistemic emotions such as
curiosity and interest using two types of information gain generated by the
principle of free energy minimization: Kullback-Leibler divergence(KLD) from
Bayesian posterior to prior, which represents free energy reduction in
recognition, and Bayesian surprise (BS), which represents the expected
information gain by Bayesian prior update. By applying a Gaussian generative
model with an additional uniform likelihood, we found that KLD and BS form an
upward-convex function of surprise (minimized free energy and prediction
error), similar to Berlyne&apos;s arousal potential functions, or the Wundt curve.
We consider that the alternate maximization of BS and KLD generates an ideal
inquiry cycle to approach the optimal arousal level with fluctuations in
surprise, and that curiosity and interest drive to facilitate the cyclic
process. We exhaustively analyzed the effects of prediction uncertainty (prior
variance) and observation uncertainty (likelihood variance) on the peaks of the
information gain function as optimal surprises. The results show that greater
prediction uncertainty, meaning an open-minded attitude, and less observational
uncertainty, meaning precise observation with attention, are expected to
provide greater information gains through a greater range of exploration. The
proposed mathematical framework unifies the free energy principle of the brain
and the arousal potential theory to explain the Wundt curve as an information
gain function and suggests an ideal inquiry process driven by epistemic
emotions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanagisawa_H/0/1/0/all/0/1&quot;&gt;Hideyoshi Yanagisawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honda_S/0/1/0/all/0/1&quot;&gt;Shimon Honda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00009">
<title>Turing&apos;s Test, a Beautiful Thought Experiment. (arXiv:2401.00009v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00009</link>
<description rdf:parseType="Literal">&lt;p&gt;In the wake of large language models, there has been a resurgence of claims
and questions about the Turing test and its value for AI, which are reminiscent
of decades of practical &quot;Turing&quot; tests. If AI were quantum physics, by now
several &quot;Schr\&quot;odinger&apos;s&quot; cats could have been killed. Better late than never,
it is time for a historical reconstruction of Turing&apos;s beautiful thought
experiment. In this paper I present a wealth of evidence, including new
archival sources, give original answers to several open questions about
Turing&apos;s 1950 paper, and address the core question of the value of Turing&apos;s
test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_B/0/1/0/all/0/1&quot;&gt;Bernardo Gon&amp;#xe7;alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00015">
<title>Distributional Reinforcement Learning-based Energy Arbitrage Strategies in Imbalance Settlement Mechanism. (arXiv:2401.00015v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00015</link>
<description rdf:parseType="Literal">&lt;p&gt;Growth in the penetration of renewable energy sources makes supply more
uncertain and leads to an increase in the system imbalance. This trend,
together with the single imbalance pricing, opens an opportunity for balance
responsible parties (BRPs) to perform energy arbitrage in the imbalance
settlement mechanism. To this end, we propose a battery control framework based
on distributional reinforcement learning (DRL). Our proposed control framework
takes a risk-sensitive perspective, allowing BRPs to adjust their risk
preferences: we aim to optimize a weighted sum of the arbitrage profit and a
risk measure while constraining the daily number of cycles for the battery. We
assess the performance of our proposed control framework using the Belgian
imbalance prices of 2022 and compare two state-of-the-art RL methods, deep Q
learning and soft actor-critic. Results reveal that the distributional soft
actor-critic method can outperform other methods. Moreover, we note that our
fully risk-averse agent appropriately learns to hedge against the risk related
to the unknown imbalance price by (dis)charging the battery only when the agent
is more certain about the price.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madahi_S/0/1/0/all/0/1&quot;&gt;Seyed Soroush Karimi Madahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claessens_B/0/1/0/all/0/1&quot;&gt;Bert Claessens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1&quot;&gt;Chris Develder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00020">
<title>AI-driven platform for systematic nomenclature and intelligent knowledge acquisition of natural medicinal materials. (arXiv:2401.00020v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00020</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural Medicinal Materials (NMMs) have a long history of global clinical
applications, accompanied by extensive informational records. Despite their
significant impact on healthcare, the field faces a major challenge: the
non-standardization of NMM knowledge, stemming from historical complexities and
causing limitations in broader applications. To address this, we introduce a
Systematic Nomenclature for NMMs, underpinned by ShennongAlpha, an AI-driven
platform designed for intelligent knowledge acquisition. This nomenclature
system enables precise identification and differentiation of NMMs.
ShennongAlpha, cataloging over ten thousand NMMs with standardized bilingual
information, enhances knowledge management and application capabilities,
thereby overcoming traditional barriers. Furthermore, it pioneers AI-empowered
conversational knowledge acquisition and standardized machine translation.
These synergistic innovations mark the first major advance in integrating
domain-specific NMM knowledge with AI, propelling research and applications
across both NMM and AI fields while establishing a groundbreaking precedent in
this crucial area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zijie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yongjing Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1&quot;&gt;Chaojun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1&quot;&gt;Tiange Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wufan Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tian Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00031">
<title>Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges. (arXiv:2401.00031v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00031</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-making is a dynamic process requiring perception, memory, and
reasoning to make choices and find optimal policies. Traditional approaches to
decision-making suffer from sample efficiency and generalization, while
large-scale self-supervised pretraining has enabled fast adaptation with
fine-tuning or few-shot learning in language and vision. We thus argue to
integrate knowledge acquired from generic large-scale self-supervised
pretraining into downstream decision-making problems. We propose
Pretrain-Then-Adapt pipeline and survey recent work on data collection,
pretraining objectives and adaptation strategies for decision-making
pretraining and downstream inference. Finally, we identify critical challenges
and future directions for developing decision foundation model with the help of
generic and flexible self-supervised pretraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jianbin Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00033">
<title>Hybrid Modeling Design Patterns. (arXiv:2401.00033v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00033</link>
<description rdf:parseType="Literal">&lt;p&gt;Design patterns provide a systematic way to convey solutions to recurring
modeling challenges. This paper introduces design patterns for hybrid modeling,
an approach that combines modeling based on first principles with data-driven
modeling techniques. While both approaches have complementary advantages there
are often multiple ways to combine them into a hybrid model, and the
appropriate solution will depend on the problem at hand. In this paper, we
provide four base patterns that can serve as blueprints for combining
data-driven components with domain knowledge into a hybrid approach. In
addition, we also present two composition patterns that govern the combination
of the base patterns into more complex hybrid models. Each design pattern is
illustrated by typical use cases from application areas such as climate
modeling, engineering, and physics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1&quot;&gt;Maja Rudolph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurz_S/0/1/0/all/0/1&quot;&gt;Stefan Kurz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakitsch_B/0/1/0/all/0/1&quot;&gt;Barbara Rakitsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00037">
<title>Messenger and Non-Coding RNA Design via Expected Partition Function and Continuous Optimization. (arXiv:2401.00037v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.00037</link>
<description rdf:parseType="Literal">&lt;p&gt;The tasks of designing messenger RNAs and non-coding RNAs are discrete
optimization problems, and several versions of these problems are NP-hard. As
an alternative to commonly used local search methods, we formulate these
problems as continuous optimization and develop a general framework for this
optimization based on a new concept of &quot;expected partition function&quot;. The basic
idea is to start with a distribution over all possible candidate sequences, and
extend the objective function from a sequence to a distribution. We then use
gradient descent-based optimization methods to improve the extended objective
function, and the distribution will gradually shrink towards a one-hot sequence
(i.e., a single sequence). We consider two important case studies within this
framework, the mRNA design problem optimizing for partition function (i.e.,
ensemble free energy) and the non-coding RNA design problem optimizing for
conditional (i.e., Boltzmann) probability. In both cases, our approach
demonstrate promising preliminary results. We make our code available at
https://github.com/KuNyaa/RNA_Design_codebase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dai_N/0/1/0/all/0/1&quot;&gt;Ning Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wei Yu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianshuo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathews_D/0/1/0/all/0/1&quot;&gt;David H. Mathews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Liang Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00062">
<title>Semantic Computing for Organizational Effectiveness: From Organization Theory to Practice through Semantics-Based Modelling. (arXiv:2401.00062v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00062</link>
<description rdf:parseType="Literal">&lt;p&gt;A critical function of an organization is to foster the level of integration
(coordination and cooperation) necessary to achieve its objectives. The need to
coordinate and motivation to cooperate emerges from the myriad dependencies
between an organization&apos;s members and their work. Therefore, to reason about
solutions to coordination and cooperation problems requires a robust
representation that includes the underlying dependencies. We find that such a
representation remains missing from formal organizational models, and we
leverage semantics to bridge this gap. Drawing on well-established
organizational research and our extensive fieldwork with one of North America&apos;s
largest municipalities, (1) we introduce an ontology, formalized in first-order
logic, that operationalizes concepts like outcome, reward, and epistemic
dependence, and their links to potential integration risks; and (2) present
real-world applications of this ontology to analyze and support integration in
complex government infrastructure projects. Our ontology is implemented and
validated in both Z3 and OWL. Key features of our model include inferable
dependencies, explainable coordination and cooperation risks, and actionable
insights on how dependency structures within an organization can be altered to
mitigate the risks. Conceptualizing real-world challenges like incentive
misalignment, free-riding, and subgoal optimization in terms of dependency
structures, our semantics-based approach represents a novel method for
modelling and enhancing coordination and cooperation. Integrated within a
decision-support system, our model may serve as an impactful aid for
organizational design and effectiveness. More broadly, our approach underscores
the transformative potential of semantics in deriving tangible, real-world
value from existing organization theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizk_M/0/1/0/all/0/1&quot;&gt;Mena Rizk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosu_D/0/1/0/all/0/1&quot;&gt;Daniela Rosu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_M/0/1/0/all/0/1&quot;&gt;Mark Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00104">
<title>Causal State Distillation for Explainable Reinforcement Learning. (arXiv:2401.00104v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00104</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) is a powerful technique for training intelligent
agents, but understanding why these agents make specific decisions can be quite
challenging. This lack of transparency in RL models has been a long-standing
problem, making it difficult for users to grasp the reasons behind an agent&apos;s
behaviour. Various approaches have been explored to address this problem, with
one promising avenue being reward decomposition (RD). RD is appealing as it
sidesteps some of the concerns associated with other methods that attempt to
rationalize an agent&apos;s behaviour in a post-hoc manner. RD works by exposing
various facets of the rewards that contribute to the agent&apos;s objectives during
training. However, RD alone has limitations as it primarily offers insights
based on sub-rewards and does not delve into the intricate cause-and-effect
relationships that occur within an RL agent&apos;s neural model. In this paper, we
present an extension of RD that goes beyond sub-rewards to provide more
informative explanations. Our approach is centred on a causal learning
framework that leverages information-theoretic measures for explanation
objectives that encourage three crucial properties of causal factors:
\emph{causal sufficiency}, \emph{sparseness}, and \emph{orthogonality}. These
properties help us distill the cause-and-effect relationships between the
agent&apos;s states and actions or rewards, allowing for a deeper understanding of
its decision-making processes. Our framework is designed to generate local
explanations and can be applied to a wide range of RL tasks with multiple
reward channels. Through a series of experiments, we demonstrate that our
approach offers more meaningful and insightful explanations for the agent&apos;s
action selections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenhao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xufeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fryen_T/0/1/0/all/0/1&quot;&gt;Thilo Fryen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Hee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengdi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magg_S/0/1/0/all/0/1&quot;&gt;Sven Magg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00110">
<title>Diffusion Model with Perceptual Loss. (arXiv:2401.00110v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00110</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shanchuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00125">
<title>LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning. (arXiv:2401.00125v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00125</link>
<description rdf:parseType="Literal">&lt;p&gt;Although planning is a crucial component of the autonomous driving stack,
researchers have yet to develop robust planning algorithms that are capable of
safely handling the diverse range of possible driving scenarios. Learning-based
planners suffer from overfitting and poor long-tail performance. On the other
hand, rule-based planners generalize well, but might fail to handle scenarios
that require complex driving maneuvers. To address these limitations, we
investigate the possibility of leveraging the common-sense reasoning
capabilities of Large Language Models (LLMs) such as GPT4 and Llama2 to
generate plans for self-driving vehicles. In particular, we develop a novel
hybrid planner that leverages a conventional rule-based planner in conjunction
with an LLM-based planner. Guided by commonsense reasoning abilities of LLMs,
our approach navigates complex scenarios which existing planners struggle with,
produces well-reasoned outputs while also remaining grounded through working
alongside the rule-based approach. Through extensive evaluation on the nuPlan
benchmark, we achieve state-of-the-art performance, outperforming all existing
pure learning- and rule-based methods across most metrics. Our code will be
available at https://llmassist.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharan_S/0/1/0/all/0/1&quot;&gt;S P Sharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittaluga_F/0/1/0/all/0/1&quot;&gt;Francesco Pittaluga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar B G&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00132">
<title>Contrastive learning-based agent modeling for deep reinforcement learning. (arXiv:2401.00132v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.00132</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent systems often require agents to collaborate with or compete
against other agents with diverse goals, behaviors, or strategies. Agent
modeling is essential when designing adaptive policies for intelligent machine
agents in multiagent systems, as this is the means by which the ego agent
understands other agents&apos; behavior and extracts their meaningful policy
representations. These representations can be used to enhance the ego agent&apos;s
adaptive policy which is trained by reinforcement learning. However, existing
agent modeling approaches typically assume the availability of local
observations from other agents (modeled agents) during training or a long
observation trajectory for policy adaption. To remove these constrictive
assumptions and improve agent modeling performance, we devised a Contrastive
Learning-based Agent Modeling (CLAM) method that relies only on the local
observations from the ego agent during training and execution. With these
observations, CLAM is capable of generating consistent high-quality policy
representations in real-time right from the beginning of each episode. We
evaluated the efficacy of our approach in both cooperative and competitive
multi-agent environments. Our experiments demonstrate that our approach
achieves state-of-the-art on both cooperative and competitive tasks,
highlighting the potential of contrastive learning-based agent modeling for
enhancing reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wenhao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yu-Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00139">
<title>Is Knowledge All Large Language Models Needed for Causal Reasoning?. (arXiv:2401.00139v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00139</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the causal reasoning of large language models (LLMs) to
enhance their interpretability and reliability in advancing artificial
intelligence. Despite the proficiency of LLMs in a range of tasks, their
potential for understanding causality requires further exploration. We propose
a novel causal attribution model that utilizes &quot;do-operators&quot; for constructing
counterfactual scenarios, allowing us to systematically quantify the influence
of input numerical data and LLMs&apos; pre-existing knowledge on their causal
reasoning processes. Our newly developed experimental setup assesses LLMs&apos;
reliance on contextual information and inherent knowledge across various
domains. Our evaluation reveals that LLMs&apos; causal reasoning ability depends on
the context and domain-specific knowledge provided, and supports the argument
that &quot;knowledge is, indeed, what LLMs principally require for sound causal
reasoning&quot;. On the contrary, in the absence of knowledge, LLMs still maintain a
degree of causal reasoning using the available numerical data, albeit with
limitations in the calculations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hengrui Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Rui Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00158">
<title>ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph. (arXiv:2401.00158v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00158</link>
<description rdf:parseType="Literal">&lt;p&gt;Question Answering over Knowledge Graph (KGQA) aims to seek answer entities
for the natural language question from a large-scale Knowledge Graph~(KG). To
better perform reasoning on KG, recent work typically adopts a pre-trained
language model~(PLM) to model the question, and a graph neural network~(GNN)
based module to perform multi-hop reasoning on the KG. Despite the
effectiveness, due to the divergence in model architecture, the PLM and GNN are
not closely integrated, limiting the knowledge sharing and fine-grained feature
interactions. To solve it, we aim to simplify the above two-module approach,
and develop a more capable PLM that can directly support subgraph reasoning for
KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware
self-attention mechanism to imitate the GNN for performing structured
reasoning, and also adopt an adaptation tuning strategy to adapt the model
parameters with 20,000 subgraphs with synthesized questions. After adaptation,
the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments
show that ReasoningLM surpasses state-of-the-art models by a large margin, even
with fewer updated parameters and less training data. Our codes and data are
publicly available at~\url{https://github.com/RUCAIBox/ReasoningLM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jinhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00161">
<title>DiffHybrid-UQ: Uncertainty Quantification for Differentiable Hybrid Neural Modeling. (arXiv:2401.00161v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00161</link>
<description rdf:parseType="Literal">&lt;p&gt;The hybrid neural differentiable models mark a significant advancement in the
field of scientific machine learning. These models, integrating numerical
representations of known physics into deep neural networks, offer enhanced
predictive capabilities and show great potential for data-driven modeling of
complex physical systems. However, a critical and yet unaddressed challenge
lies in the quantification of inherent uncertainties stemming from multiple
sources. Addressing this gap, we introduce a novel method, DiffHybrid-UQ, for
effective and efficient uncertainty propagation and estimation in hybrid neural
differentiable models, leveraging the strengths of deep ensemble Bayesian
learning and nonlinear transformations. Specifically, our approach effectively
discerns and quantifies both aleatoric uncertainties, arising from data noise,
and epistemic uncertainties, resulting from model-form discrepancies and data
sparsity. This is achieved within a Bayesian model averaging framework, where
aleatoric uncertainties are modeled through hybrid neural models. The unscented
transformation plays a pivotal role in enabling the flow of these uncertainties
through the nonlinear functions within the hybrid model. In contrast, epistemic
uncertainties are estimated using an ensemble of stochastic gradient descent
(SGD) trajectories. This approach offers a practical approximation to the
posterior distribution of both the network parameters and the physical
parameters. Notably, the DiffHybrid-UQ framework is designed for simplicity in
implementation and high scalability, making it suitable for parallel computing
environments. The merits of the proposed method have been demonstrated through
problems governed by both ordinary and partial differentiable equations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhare_D/0/1/0/all/0/1&quot;&gt;Deepak Akhare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tengfei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian-Xun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00209">
<title>AI and Tempo Estimation: A Review. (arXiv:2401.00209v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;The author&apos;s goal in this paper is to explore how artificial intelligence
(AI) has been utilised to inform our understanding of and ability to estimate
at scale a critical aspect of musical creativity - musical tempo. The central
importance of tempo to musical creativity can be seen in how it is used to
express specific emotions (Eerola and Vuoskoski 2013), suggest particular
musical styles (Li and Chan 2011), influence perception of expression (Webster
and Weir 2005) and mediate the urge to move one&apos;s body in time to the music
(Burger et al. 2014). Traditional tempo estimation methods typically detect
signal periodicities that reflect the underlying rhythmic structure of the
music, often using some form of autocorrelation of the amplitude envelope
(Lartillot and Toiviainen 2007). Recently, AI-based methods utilising
convolutional or recurrent neural networks (CNNs, RNNs) on spectral
representations of the audio signal have enjoyed significant improvements in
accuracy (Aarabi and Peeters 2022). Common AI-based techniques include those
based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)),
classification and statistical learning (e.g., support vector machines (SVM)),
and artificial neural networks (ANNs) (e.g., self-organising maps (SOMs), CNNs,
RNNs, deep learning (DL)). The aim here is to provide an overview of some of
the more common AI-based tempo estimation algorithms and to shine a light on
notable benefits and potential drawbacks of each. Limitations of AI in this
field in general are also considered, as is the capacity for such methods to
account for idiosyncrasies inherent in tempo perception, i.e., how well
AI-based approaches are able to think and act like humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luck_G/0/1/0/all/0/1&quot;&gt;Geoff Luck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00211">
<title>Open-TI: Open Traffic Intelligence with Augmented Language Model. (arXiv:2401.00211v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00211</link>
<description rdf:parseType="Literal">&lt;p&gt;Transportation has greatly benefited the cities&apos; development in the modern
civilization process. Intelligent transportation, leveraging advanced computer
algorithms, could further increase people&apos;s daily commuting efficiency.
However, intelligent transportation, as a cross-discipline, often requires
practitioners to comprehend complicated algorithms and obscure neural networks,
bringing a challenge for the advanced techniques to be trusted and deployed in
practical industries. Recognizing the expressiveness of the pre-trained large
language models, especially the potential of being augmented with abilities to
understand and execute intricate commands, we introduce Open-TI. Serving as a
bridge to mitigate the industry-academic gap, Open-TI is an innovative model
targeting the goal of Turing Indistinguishable Traffic Intelligence, it is
augmented with the capability to harness external traffic analysis packages
based on existing conversations. Marking its distinction, Open-TI is the first
method capable of conducting exhaustive traffic analysis from scratch -
spanning from map data acquisition to the eventual execution in complex
simulations. Besides, Open-TI is able to conduct task-specific embodiment like
training and adapting the traffic signal control policies (TSC), explore demand
optimizations, etc. Furthermore, we explored the viability of LLMs directly
serving as control agents, by understanding the expected intentions from
Open-TI, we designed an agent-to-agent communication mode to support Open-TI
conveying messages to ChatZero (control agent), and then the control agent
would choose from the action space to proceed the execution. We eventually
provide the formal implementation structure, and the open-ended design invites
further community-driven enhancements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_L/0/1/0/all/0/1&quot;&gt;Longchao Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liou_K/0/1/0/all/0/1&quot;&gt;Kuanru Liou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tiejin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuesong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiangyong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yezhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00225">
<title>Enhancing dysarthria speech feature representation with empirical mode decomposition and Walsh-Hadamard transform. (arXiv:2401.00225v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.00225</link>
<description rdf:parseType="Literal">&lt;p&gt;Dysarthria speech contains the pathological characteristics of vocal tract
and vocal fold, but so far, they have not yet been included in traditional
acoustic feature sets. Moreover, the nonlinearity and non-stationarity of
speech have been ignored. In this paper, we propose a feature enhancement
algorithm for dysarthria speech called WHFEMD. It combines empirical mode
decomposition (EMD) and fast Walsh-Hadamard transform (FWHT) to enhance
features. With the proposed algorithm, the fast Fourier transform of the
dysarthria speech is first performed and then followed by EMD to get intrinsic
mode functions (IMFs). After that, FWHT is used to output new coefficients and
to extract statistical features based on IMFs, power spectral density, and
enhanced gammatone frequency cepstral coefficients. To evaluate the proposed
approach, we conducted experiments on two public pathological speech databases
including UA Speech and TORGO. The results show that our algorithm performed
better than traditional features in classification. We achieved improvements of
13.8% (UA Speech) and 3.84% (TORGO), respectively. Furthermore, the
incorporation of an imbalanced classification algorithm to address data
imbalance has resulted in a 12.18% increase in recognition accuracy. This
algorithm effectively addresses the challenges of the imbalanced dataset and
non-linearity in dysarthric speech and simultaneously provides a robust
representation of the local pathological features of the vocal folds and
tracts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Ting Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shufei Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dingam_C/0/1/0/all/0/1&quot;&gt;Camille Dingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Huizhi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00230">
<title>Transformer Multivariate Forecasting: Less is More?. (arXiv:2401.00230v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00230</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of multivariate forecasting, transformer models stand out as
powerful apparatus, displaying exceptional capabilities in handling messy
datasets from real-world contexts. However, the inherent complexity of these
datasets, characterized by numerous variables and lengthy temporal sequences,
poses challenges, including increased noise and extended model runtime. This
paper focuses on reducing redundant information to elevate forecasting accuracy
while optimizing runtime efficiency. We propose a novel transformer forecasting
framework enhanced by Principal Component Analysis (PCA) to tackle this
challenge. The framework is evaluated by five state-of-the-art (SOTA) models
and four diverse real-world datasets. Our experimental results demonstrate the
framework&apos;s ability to minimize prediction errors across all models and
datasets while significantly reducing runtime. From the model perspective, one
of the PCA-enhanced models: PCA+Crossformer, reduces mean square errors (MSE)
by 33.3% and decreases runtime by 49.2% on average. From the dataset
perspective, the framework delivers 14.3% MSE and 76.6% runtime reduction on
Electricity datasets, as well as 4.8% MSE and 86.9% runtime reduction on
Traffic datasets. This study aims to advance various SOTA models and enhance
transformer-based time series forecasting for intricate data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Caesar Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouvry_P/0/1/0/all/0/1&quot;&gt;Pascal Bouvry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00238">
<title>How to Evaluate Coreference in Literary Texts?. (arXiv:2401.00238v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00238</link>
<description rdf:parseType="Literal">&lt;p&gt;In this short paper, we examine the main metrics used to evaluate textual
coreference and we detail some of their limitations. We show that a unique
score cannot represent the full complexity of the problem at stake, and is thus
uninformative, or even misleading. We propose a new way of evaluating
coreference, taking into account the context (in our case, the analysis of
fictions, esp. novels). More specifically, we propose to distinguish long
coreference chains (corresponding to main characters), from short ones
(corresponding to secondary characters), and singletons (isolated elements).
This way, we hope to get more interpretable and thus more informative results
through evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duron_Tejedor_A/0/1/0/all/0/1&quot;&gt;Ana-Isabel Duron-Tejedor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amsili_P/0/1/0/all/0/1&quot;&gt;Pascal Amsili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poibeau_T/0/1/0/all/0/1&quot;&gt;Thierry Poibeau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00248">
<title>Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation. (arXiv:2401.00248v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00248</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmenting any object represents a crucial step towards achieving artificial
general intelligence, and the &quot;Segment Anything Model&quot; (SAM) has significantly
advanced the development of foundational models in computer vision. We have
high expectations regarding whether SAM can enhance highly accurate dichotomous
image segmentation. In fact, the evidence presented in this article
demonstrates that by inputting SAM with simple prompt boxes and utilizing the
results output by SAM as input for IS5Net, we can greatly improve the
effectiveness of highly accurate dichotomous image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Keren Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qijun Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00285">
<title>BusReF: Infrared-Visible images registration and fusion focus on reconstructible area using one set of features. (arXiv:2401.00285v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00285</link>
<description rdf:parseType="Literal">&lt;p&gt;In a scenario where multi-modal cameras are operating together, the problem
of working with non-aligned images cannot be avoided. Yet, existing image
fusion algorithms rely heavily on strictly registered input image pairs to
produce more precise fusion results, as a way to improve the performance of
downstream high-level vision tasks. In order to relax this assumption, one can
attempt to register images first. However, the existing methods for registering
multiple modalities have limitations, such as complex structures and reliance
on significant semantic information. This paper aims to address the problem of
image registration and fusion in a single framework, called BusRef. We focus on
Infrared-Visible image registration and fusion task (IVRF). In this framework,
the input unaligned image pairs will pass through three stages: Coarse
registration, Fine registration and Fusion. It will be shown that the unified
approach enables more robust IVRF. We also propose a novel training and
evaluation strategy, involving the use of masks to reduce the influence of
non-reconstructible regions on the loss functions, which greatly improves the
accuracy and robustness of the fusion task. Last but not least, a
gradient-aware fusion network is designed to preserve the complementary
information. The advanced performance of this algorithm is demonstrated by
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1&quot;&gt;Josef Kittler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00288">
<title>Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit. (arXiv:2401.00288v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.00288</link>
<description rdf:parseType="Literal">&lt;p&gt;Code intelligence leverages machine learning techniques to extract knowledge
from extensive code corpora, with the aim of developing intelligent tools to
improve the quality and productivity of computer programming. Currently, there
is already a thriving research community focusing on code intelligence, with
efforts ranging from software engineering, machine learning, data mining,
natural language processing, and programming languages. In this paper, we
conduct a comprehensive literature review on deep learning for code
intelligence, from the aspects of code representation learning, deep learning
techniques, and application tasks. We also benchmark several state-of-the-art
neural models for code intelligence, and provide an open-source toolkit
tailored for the rapid prototyping of deep-learning-based code intelligence
models. In particular, we inspect the existing code intelligence models under
the basis of code representation learning, and provide a comprehensive overview
to enhance comprehension of the present state of code intelligence.
Furthermore, we publicly release the source code and data resources to provide
the community with a ready-to-use benchmark, which can facilitate the
evaluation and comparison of existing and future code intelligence models
(https://xcodemind.github.io). At last, we also point out several challenging
and promising directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yao Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhangqian Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yulei Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guandong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00290">
<title>Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks. (arXiv:2401.00290v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00290</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of red teaming LLMs on elementary calculations and
algebraic tasks to evaluate how various prompting techniques affect the quality
of outputs. We present a framework to procedurally generate numerical questions
and puzzles, and compare the results with and without the application of
several red teaming techniques. Our findings suggest that even though
structured reasoning and providing worked-out examples slow down the
deterioration of the quality of answers, the gpt-3.5-turbo and gpt-4 models are
not well suited for elementary calculations and reasoning tasks, also when
being red teamed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buszydlik_A/0/1/0/all/0/1&quot;&gt;Aleksander Buszydlik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobiczek_K/0/1/0/all/0/1&quot;&gt;Karol Dobiczek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okon_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Teodor Oko&amp;#x144;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skublicki_K/0/1/0/all/0/1&quot;&gt;Konrad Skublicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lippmann_P/0/1/0/all/0/1&quot;&gt;Philip Lippmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00298">
<title>Principal-Agent Reward Shaping in MDPs. (arXiv:2401.00298v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00298</link>
<description rdf:parseType="Literal">&lt;p&gt;Principal-agent problems arise when one party acts on behalf of another,
leading to conflicts of interest. The economic literature has extensively
studied principal-agent problems, and recent work has extended this to more
complex scenarios such as Markov Decision Processes (MDPs). In this paper, we
further explore this line of research by investigating how reward shaping under
budget constraints can improve the principal&apos;s utility. We study a two-player
Stackelberg game where the principal and the agent have different reward
functions, and the agent chooses an MDP policy for both players. The principal
offers an additional reward to the agent, and the agent picks their policy
selfishly to maximize their reward, which is the sum of the original and the
offered reward. Our results establish the NP-hardness of the problem and offer
polynomial approximation algorithms for two classes of instances: Stochastic
trees and deterministic decision processes with a finite horizon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Porat_O/0/1/0/all/0/1&quot;&gt;Omer Ben-Porat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1&quot;&gt;Yishay Mansour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moshkovitz_M/0/1/0/all/0/1&quot;&gt;Michal Moshkovitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taitler_B/0/1/0/all/0/1&quot;&gt;Boaz Taitler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00315">
<title>Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders for More Efficient Multi-Agent Path Finding Plan Execution. (arXiv:2401.00315v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00315</link>
<description rdf:parseType="Literal">&lt;p&gt;The Multi-Agent Path Finding (MAPF) problem involves planning collision-free
paths for multiple agents in a shared environment. The majority of MAPF solvers
rely on the assumption that an agent can arrive at a specific location at a
specific timestep. However, real-world execution uncertainties can cause agents
to deviate from this assumption, leading to collisions and deadlocks. Prior
research solves this problem by having agents follow a Temporal Plan Graph
(TPG), enforcing a consistent passing order at every location as defined in the
MAPF plan. However, we show that TPGs are overly strict because, in some
circumstances, satisfying the passing order requires agents to wait
unnecessarily, leading to longer execution time. To overcome this issue, we
introduce a new graphical representation called a Bidirectional Temporal Plan
Graph (BTPG), which allows switching passing orders during execution to avoid
unnecessary waiting time. We design two anytime algorithms for constructing a
BTPG: BTPG-na\&quot;ive and BTPG-optimized. Experimental results show that following
BTPGs consistently outperforms following TPGs, reducing unnecessary waits by
8-20%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yifan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veerapaneni_R/0/1/0/all/0/1&quot;&gt;Rishi Veerapaneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaoyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00330">
<title>Efficient Two-Phase Offline Deep Reinforcement Learning from Preference Feedback. (arXiv:2401.00330v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00330</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we consider the offline preference-based reinforcement learning
problem. We focus on the two-phase learning approach that is prevalent in
previous reinforcement learning from human preference works. We find a
challenge in applying two-phase learning in the offline PBRL setting that the
learned utility model can be too hard for the learning agent to optimize during
the second learning phase. To overcome the challenge, we propose a two-phasing
learning approach under behavior regularization through action clipping. The
insight is that the state-actions which are poorly covered by the dataset can
only provide limited information and increase the complexity of the problem in
the second learning phase. Our method ignores such state-actions during the
second learning phase to achieve higher learning efficiency. We empirically
verify that our method has high learning efficiency on a variety of datasets in
robotic control environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gagandeep Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00365">
<title>HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes. (arXiv:2401.00365v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00365</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector quantization (VQ) is a technique to deterministically learn features
with discrete codebook representations. It is commonly performed with a
variational autoencoding model, VQ-VAE, which can be further extended to
hierarchical structures for making high-fidelity reconstructions. However, such
hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse
issue, where the codebook is not efficiently used to express the data, and
hence degrades reconstruction accuracy. To mitigate this problem, we propose a
novel unified framework to stochastically learn hierarchical discrete
representation on the basis of the variational Bayes framework, called
hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally
generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and
residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training
scheme. Our comprehensive experiments on image datasets show that HQ-VAE
enhances codebook usage and improves reconstruction performance. We also
validated HQ-VAE in terms of its applicability to a different modality with an
audio dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takida_Y/0/1/0/all/0/1&quot;&gt;Yuhta Takida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikemiya_Y/0/1/0/all/0/1&quot;&gt;Yukara Ikemiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shibuya_T/0/1/0/all/0/1&quot;&gt;Takashi Shibuya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1&quot;&gt;Kazuki Shimada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1&quot;&gt;Woosung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Chieh-Hsin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murata_N/0/1/0/all/0/1&quot;&gt;Naoki Murata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uesaka_T/0/1/0/all/0/1&quot;&gt;Toshimitsu Uesaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchida_K/0/1/0/all/0/1&quot;&gt;Kengo Uchida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wei-Hsiang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1&quot;&gt;Yuki Mitsufuji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00379">
<title>DREAM: Debugging and Repairing AutoML Pipelines. (arXiv:2401.00379v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.00379</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning models have become an integrated component of modern software
systems. In response to the challenge of model design, researchers proposed
Automated Machine Learning (AutoML) systems, which automatically search for
model architecture and hyperparameters for a given task. Like other software
systems, existing AutoML systems suffer from bugs. We identify two common and
severe bugs in AutoML, performance bug (i.e., searching for the desired model
takes an unreasonably long time) and ineffective search bug (i.e., AutoML
systems are not able to find an accurate enough model). After analyzing the
workflow of AutoML, we observe that existing AutoML systems overlook potential
opportunities in search space, search method, and search feedback, which
results in performance and ineffective search bugs. Based on our analysis, we
design and implement DREAM, an automatic debugging and repairing system for
AutoML systems. It monitors the process of AutoML to collect detailed feedback
and automatically repairs bugs by expanding search space and leveraging a
feedback-driven search strategy. Our evaluation results show that DREAM can
effectively and efficiently repair AutoML bugs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1&quot;&gt;Juan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shiqing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00390">
<title>Horizontal Federated Computer Vision. (arXiv:2401.00390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00390</link>
<description rdf:parseType="Literal">&lt;p&gt;In the modern world, the amount of visual data recorded has been rapidly
increasing. In many cases, data is stored in geographically distinct locations
and thus requires a large amount of time and space to consolidate. Sometimes,
there are also regulations for privacy protection which prevent data
consolidation. In this work, we present federated implementations for object
detection and recognition using a federated Faster R-CNN (FRCNN) and image
segmentation using a federated Fully Convolutional Network (FCN). Our FRCNN was
trained on 5000 examples of the COCO2017 dataset while our FCN was trained on
the entire train set of the CamVid dataset. The proposed federated models
address the challenges posed by the increasing volume and decentralized nature
of visual data, offering efficient solutions in compliance with privacy
regulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1&quot;&gt;Paul K. Mandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leo_C/0/1/0/all/0/1&quot;&gt;Cole Leo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurley_C/0/1/0/all/0/1&quot;&gt;Connor Hurley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00391">
<title>Controllable Safety-Critical Closed-loop Traffic Simulation via Guided Diffusion. (arXiv:2401.00391v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.00391</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the performance of autonomous vehicle planning algorithms
necessitates simulating long-tail traffic scenarios. Traditional methods for
generating safety-critical scenarios often fall short in realism and
controllability. Furthermore, these techniques generally neglect the dynamics
of agent interactions. To mitigate these limitations, we introduce a novel
closed-loop simulation framework rooted in guided diffusion models. Our
approach yields two distinct advantages: 1) the generation of realistic
long-tail scenarios that closely emulate real-world conditions, and 2) enhanced
controllability, enabling more comprehensive and interactive evaluations. We
achieve this through novel guidance objectives that enhance road progress while
lowering collision and off-road rates. We develop a novel approach to simulate
safety-critical scenarios through an adversarial term in the denoising process,
which allows the adversarial agent to challenge a planner with plausible
maneuvers, while all agents in the scene exhibit reactive and realistic
behaviors. We validate our framework empirically using the NuScenes dataset,
demonstrating improvements in both realism and controllability. These findings
affirm that guided diffusion models provide a robust and versatile foundation
for safety-critical, interactive traffic simulation, extending their utility
across the broader landscape of autonomous driving. For additional resources
and demonstrations, visit our project page at https://safe-sim.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1&quot;&gt;Wei-Jer Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittaluga_F/0/1/0/all/0/1&quot;&gt;Francesco Pittaluga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00393">
<title>Generative Model-Driven Synthetic Training Image Generation: An Approach to Cognition in Rail Defect Detection. (arXiv:2401.00393v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00393</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in cognitive computing, with the integration of deep
learning techniques, have facilitated the development of intelligent cognitive
systems (ICS). This is particularly beneficial in the context of rail defect
detection, where the ICS would emulate human-like analysis of image data for
defect patterns. Despite the success of Convolutional Neural Networks (CNN) in
visual defect classification, the scarcity of large datasets for rail defect
detection remains a challenge due to infrequent accident events that would
result in defective parts and images. Contemporary researchers have addressed
this data scarcity challenge by exploring rule-based and generative data
augmentation models. Among these, Variational Autoencoder (VAE) models can
generate realistic data without extensive baseline datasets for noise modeling.
This study proposes a VAE-based synthetic image generation technique for rail
defects, incorporating weight decay regularization and image reconstruction
loss to prevent overfitting. The proposed method is applied to create a
synthetic dataset for the Canadian Pacific Railway (CPR) with just 50 real
samples across five classes. Remarkably, 500 synthetic samples are generated
with a minimal reconstruction loss of 0.021. A Visual Transformer (ViT) model
underwent fine-tuning using this synthetic CPR dataset, achieving high accuracy
rates (98%-99%) in classifying the five defect classes. This research offers a
promising solution to the data scarcity challenge in rail defect detection,
showcasing the potential for robust ICS development in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdousi_R/0/1/0/all/0/1&quot;&gt;Rahatara Ferdousi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chunsheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;M. Anwar Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laamarti_F/0/1/0/all/0/1&quot;&gt;Fedwa Laamarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;M. Shamim Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00409">
<title>A Two-stream Hybrid CNN-Transformer Network for Skeleton-based Human Interaction Recognition. (arXiv:2401.00409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00409</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Interaction Recognition is the process of identifying interactive
actions between multiple participants in a specific situation. The aim is to
recognise the action interactions between multiple entities and their meaning.
Many single Convolutional Neural Network has issues, such as the inability to
capture global instance interaction features or difficulty in training, leading
to ambiguity in action semantics. In addition, the computational complexity of
the Transformer cannot be ignored, and its ability to capture local information
and motion features in the image is poor. In this work, we propose a Two-stream
Hybrid CNN-Transformer Network (THCT-Net), which exploits the local specificity
of CNN and models global dependencies through the Transformer. CNN and
Transformer simultaneously model the entity, time and space relationships
between interactive entities respectively. Specifically, Transformer-based
stream integrates 3D convolutions with multi-head self-attention to learn
inter-token correlations; We propose a new multi-branch CNN framework for
CNN-based streams that automatically learns joint spatio-temporal features from
skeleton sequences. The convolutional layer independently learns the local
features of each joint neighborhood and aggregates the features of all joints.
And the raw skeleton coordinates as well as their temporal difference are
integrated with a dual-branch paradigm to fuse the motion features of the
skeleton. Besides, a residual structure is added to speed up training
convergence. Finally, the recognition results of the two branches are fused
using parallel splicing. Experimental results on diverse and challenging
datasets, demonstrate that the proposed method can better comprehend and infer
the meaning and context of various actions, outperforming state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1&quot;&gt;Ruoqi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianqin Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00420">
<title>SynCDR : Training Cross Domain Retrieval Models with Synthetic Data. (arXiv:2401.00420v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00420</link>
<description rdf:parseType="Literal">&lt;p&gt;In cross-domain retrieval, a model is required to identify images from the
same semantic category across two visual domains. For instance, given a sketch
of an object, a model needs to retrieve a real image of it from an online
store&apos;s catalog. A standard approach for such a problem is learning a feature
space of images where Euclidean distances reflect similarity. Even without
human annotations, which may be expensive to acquire, prior methods function
reasonably well using unlabeled images for training. Our problem constraint
takes this further to scenarios where the two domains do not necessarily share
any common categories in training data. This can occur when the two domains in
question come from different versions of some biometric sensor recording
identities of different people. We posit a simple solution, which is to
generate synthetic data to fill in these missing category examples across
domains. This, we do via category preserving translation of images from one
visual domain to another. We compare approaches specifically trained for this
translation for a pair of domains, as well as those that can use large-scale
pre-trained text-to-image diffusion models via prompts, and find that the
latter can generate better replacement synthetic data, leading to more accurate
cross-domain retrieval models. Code for our work is available at
https://github.com/samarth4149/SynCDR .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Samarth Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1&quot;&gt;Venkatesh Saligrama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00426">
<title>keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM. (arXiv:2401.00426v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00426</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have exhibited remarkable performance on various
natural language processing (NLP) tasks, especially for question answering.
However, in the face of problems beyond the scope of knowledge, these LLMs tend
to talk nonsense with a straight face, where the potential solution could be
incorporating an Information Retrieval (IR) module and generating response
based on these retrieved knowledge. In this paper, we present a novel framework
to assist LLMs, such as ChatGPT, to retrieve question-related structured
information on the knowledge graph, and demonstrate that Knowledge-based
question answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to
guide the LLM to sequentially find the answer entities of a complex question
through interpretable logical chains. Specifically, the workflow of Keqing will
execute decomposing a complex question according to predefined templates,
retrieving candidate entities on knowledge graph, reasoning answers of
sub-questions, and finally generating response with reasoning paths, which
greatly improves the reliability of LLM&apos;s response. The experimental results on
KBQA datasets show that Keqing can achieve competitive performance and
illustrate the logic of answering each question.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaojie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yishi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenxi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bo An&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00428">
<title>Training towards significance with the decorrelated event classifier transformer neural network. (arXiv:2401.00428v1 [hep-ex])</title>
<link>http://arxiv.org/abs/2401.00428</link>
<description rdf:parseType="Literal">&lt;p&gt;Experimental particle physics uses machine learning for many of tasks, where
one application is to classify signal and background events. The classification
can be used to bin an analysis region to enhance the expected significance for
a mass resonance search. In natural language processing, one of the leading
neural network architectures is the transformer. In this work, an event
classifier transformer is proposed to bin an analysis region, in which the
network is trained with special techniques. The techniques developed here can
enhance the significance and reduce the correlation between the network&apos;s
output and the reconstructed mass. It is found that this trained network can
perform better than boosted decision trees and feed-forward networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaebak Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00430">
<title>Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy. (arXiv:2401.00430v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00430</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of Artificial Intelligence Generated Content (AIGC), conditional
multimodal synthesis technologies (e.g., text-to-image, text-to-video,
text-to-audio, etc) are gradually reshaping the natural content in the real
world. The key to multimodal synthesis technology is to establish the mapping
relationship between different modalities. Brain signals, serving as potential
reflections of how the brain interprets external information, exhibit a
distinctive One-to-Many correspondence with various external modalities. This
correspondence makes brain signals emerge as a promising guiding condition for
multimodal content synthesis. Brian-conditional multimodal synthesis refers to
decoding brain signals back to perceptual experience, which is crucial for
developing practical brain-computer interface systems and unraveling complex
mechanisms underlying how the brain perceives and comprehends external stimuli.
This survey comprehensively examines the emerging field of AIGC-based
Brain-conditional Multimodal Synthesis, termed AIGC-Brain, to delineate the
current landscape and future directions. To begin, related brain neuroimaging
datasets, functional brain regions, and mainstream generative models are
introduced as the foundation of AIGC-Brain decoding and analysis. Next, we
provide a comprehensive taxonomy for AIGC-Brain decoding models and present
task-specific representative work and detailed implementation strategies to
facilitate comparison and in-depth analysis. Quality assessments are then
introduced for both qualitative and quantitative evaluation. Finally, this
survey explores insights gained, providing current challenges and outlining
prospects of AIGC-Brain. Being the inaugural survey in this domain, this paper
paves the way for the progress of AIGC-Brain research, offering a foundational
overview to guide future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_W/0/1/0/all/0/1&quot;&gt;Weijian Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1&quot;&gt;Pengfei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhijun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00435">
<title>Bidirectional Trained Tree-Structured Decoder for Handwritten Mathematical Expression Recognition. (arXiv:2401.00435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00435</link>
<description rdf:parseType="Literal">&lt;p&gt;The Handwritten Mathematical Expression Recognition (HMER) task is a critical
branch in the field of OCR. Recent studies have demonstrated that incorporating
bidirectional context information significantly improves the performance of
HMER models. However, existing methods fail to effectively utilize
bidirectional context information during the inference stage. Furthermore,
current bidirectional training methods are primarily designed for string
decoders and cannot adequately generalize to tree decoders, which offer
superior generalization capabilities and structural analysis capacity. In order
to overcome these limitations, we propose the Mirror-Flipped Symbol Layout Tree
(MF-SLT) and Bidirectional Asynchronous Training (BAT) structure. Our method
extends the bidirectional training strategy to the tree decoder, allowing for
more effective training by leveraging bidirectional information. Additionally,
we analyze the impact of the visual and linguistic perception of the HMER model
separately and introduce the Shared Language Modeling (SLM) mechanism. Through
the SLM, we enhance the model&apos;s robustness and generalization when dealing with
visual ambiguity, particularly in scenarios with abundant training data. Our
approach has been validated through extensive experiments, demonstrating its
ability to achieve new state-of-the-art results on the CROHME 2014, 2016, and
2019 datasets, as well as the HME100K dataset. The code used in our experiments
will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hanbo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Pengfei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiefeng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jun Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00477">
<title>Coding for Gaussian Two-Way Channels: Linear and Learning-Based Approaches. (arXiv:2401.00477v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.00477</link>
<description rdf:parseType="Literal">&lt;p&gt;Although user cooperation cannot improve the capacity of Gaussian two-way
channels (GTWCs) with independent noises, it can improve communication
reliability. In this work, we aim to enhance and balance the communication
reliability in GTWCs by minimizing the sum of error probabilities via joint
design of encoders and decoders at the users. We first formulate general
encoding/decoding functions, where the user cooperation is captured by the
coupling of user encoding processes. The coupling effect renders the
encoder/decoder design non-trivial, requiring effective decoding to capture
this effect, as well as efficient power management at the encoders within power
constraints. To address these challenges, we propose two different two-way
coding strategies: linear coding and learning-based coding. For linear coding,
we propose optimal linear decoding and discuss new insights on encoding
regarding user cooperation to balance reliability. We then propose an efficient
algorithm for joint encoder/decoder design. For learning-based coding, we
introduce a novel recurrent neural network (RNN)-based coding architecture,
where we propose interactive RNNs and a power control layer for encoding, and
we incorporate bi-directional RNNs with an attention mechanism for decoding.
Through simulations, we show that our two-way coding methodologies outperform
conventional channel coding schemes (that do not utilize user cooperation)
significantly in sum-error performance. We also demonstrate that our linear
coding excels at high signal-to-noise ratios (SNRs), while our RNN-based coding
performs best at low SNRs. We further investigate our two-way coding strategies
in terms of power distribution, two-way coding benefit, different coding rates,
and block-length gain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junghoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taejoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Anindya Bijoy Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1&quot;&gt;Seyyedali Hosseinalipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Love_D/0/1/0/all/0/1&quot;&gt;David J. Love&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00496">
<title>SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge. (arXiv:2401.00496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00496</link>
<description rdf:parseType="Literal">&lt;p&gt;Surgical tool segmentation and action recognition are fundamental building
blocks in many computer-assisted intervention applications, ranging from
surgical skills assessment to decision support systems. Nowadays,
learning-based action recognition and segmentation approaches outperform
classical methods, relying, however, on large, annotated datasets. Furthermore,
action recognition and tool segmentation algorithms are often trained and make
predictions in isolation from each other, without exploiting potential
cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we
release the first multimodal, publicly available, in-vivo, dataset for surgical
action recognition and semantic instrumentation segmentation, containing 50
suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The
aim of the challenge is twofold. First, to enable researchers to leverage the
scale of the provided dataset and develop robust and highly accurate
single-task action recognition and tool segmentation approaches in the surgical
domain. Second, to further explore the potential of multitask-based learning
approaches and determine their comparative advantage against their single-task
counterparts. A total of 12 teams participated in the challenge, contributing 7
action recognition methods, 9 instrument segmentation techniques, and 4
multitask approaches that integrated both action recognition and instrument
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psychogyios_D/0/1/0/all/0/1&quot;&gt;Dimitrios Psychogyios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colleoni_E/0/1/0/all/0/1&quot;&gt;Emanuele Colleoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amsterdam_B/0/1/0/all/0/1&quot;&gt;Beatrice Van Amsterdam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chih-Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shu-Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fucang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Baosheng Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guotai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boels_M/0/1/0/all/0/1&quot;&gt;Maxence Boels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1&quot;&gt;Jiayu Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1&quot;&gt;Rachel Sparks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1&quot;&gt;Prokar Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granados_A/0/1/0/all/0/1&quot;&gt;Alejandro Granados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengya Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;An Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Long Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_A/0/1/0/all/0/1&quot;&gt;Atsushi Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harai_Y/0/1/0/all/0/1&quot;&gt;Yuriko Harai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_Y/0/1/0/all/0/1&quot;&gt;Yuto Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1&quot;&gt;Kazuyuki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoens_J/0/1/0/all/0/1&quot;&gt;Jente Simoens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeBacker_P/0/1/0/all/0/1&quot;&gt;Pieter DeBacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cisternino_F/0/1/0/all/0/1&quot;&gt;Francesco Cisternino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnari_G/0/1/0/all/0/1&quot;&gt;Gabriele Furnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottrie_A/0/1/0/all/0/1&quot;&gt;Alex Mottrie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferraguti_F/0/1/0/all/0/1&quot;&gt;Federica Ferraguti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_S/0/1/0/all/0/1&quot;&gt;Satoshi Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasai_S/0/1/0/all/0/1&quot;&gt;Satoshi Kasai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirasawa_K/0/1/0/all/0/1&quot;&gt;Kousuke Hirasawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Soohee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyu Eun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;Hyoun-Joong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Shan An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krell_S/0/1/0/all/0/1&quot;&gt;Stefanie Krell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodenstedt_S/0/1/0/all/0/1&quot;&gt;Sebastian Bodenstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicolas Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Alejandra Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Santiago Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puentes_J/0/1/0/all/0/1&quot;&gt;Juanita Puentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbelaez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1&quot;&gt;Omid Mohareri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00499">
<title>Generating High-Precision Force Fields for Molecular Dynamics Simulations to Study Chemical Reaction Mechanisms using Molecular Configuration Transformer. (arXiv:2401.00499v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/2401.00499</link>
<description rdf:parseType="Literal">&lt;p&gt;Theoretical studies on chemical reaction mechanisms have been crucial in
organic chemistry. Traditionally, calculating the manually constructed
molecular conformations of transition states for chemical reactions using
quantum chemical calculations is the most commonly used method. However, this
way is heavily dependent on individual experience and chemical intuition. In
our previous study, we proposed a research paradigm that uses enhanced sampling
in QM/MM molecular dynamics simulations to study chemical reactions. This
approach can directly simulate the entire process of a chemical reaction.
However, the computational speed limits the use of high-precision potential
energy functions for simulations. To address this issue, we present a scheme
for training high-precision force fields for molecular modeling using our
developed graph-neural-network-based molecular model, molecular configuration
transformer. This potential energy function allows for highly accurate
simulations at a low computational cost, leading to more precise calculations
of the mechanism of chemical reactions. We have used this approach to study a
Cope rearrangement reaction and a Carbonyl insertion reaction catalyzed by
Manganese. This &quot;AI+Physics&quot; based simulation approach is expected to become a
new trend in the theoretical study of organic chemical reaction mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Sihao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Cheng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Issac Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yi Qin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00521">
<title>Multi-spatial Multi-temporal Air Quality Forecasting with Integrated Monitoring and Reanalysis Data. (arXiv:2401.00521v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00521</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate air quality forecasting is crucial for public health, environmental
monitoring and protection, and urban planning. However, existing methods fail
to effectively utilize multi-scale information, both spatially and temporally.
Spatially, there is a lack of integration between individual monitoring
stations and city-wide scales. Temporally, the periodic nature of air quality
variations is often overlooked or inadequately considered. To address these
limitations, we present a novel Multi-spatial Multi-temporal air quality
forecasting method based on Graph Convolutional Networks and Gated Recurrent
Units (M2G2), bridging the gap in air quality forecasting across spatial and
temporal scales. The proposed framework consists of two modules: Multi-scale
Spatial GCN (MS-GCN) for spatial information fusion and Multi-scale Temporal
GRU(MT-GRU) for temporal information integration. In the spatial dimension, the
MS-GCN module employs a bidirectional learnable structure and a residual
structure, enabling comprehensive information exchange between individual
monitoring stations and the city-scale graph. Regarding the temporal dimension,
the MT-GRU module adaptively combines information from different temporal
scales through parallel hidden states. Leveraging meteorological indicators and
four air quality indicators, we present comprehensive comparative analyses and
ablation experiments, showcasing the higher accuracy of M2G2 in comparison to
nine currently available advanced approaches across all aspects. The
improvements of M2G2 over the second-best method on RMSE of the 24h/48h/72h are
as follows: PM2.5: (7.72%, 6.67%, 10.45%); PM10: (6.43%, 5.68%, 7.73%); NO2:
(5.07%, 7.76%, 16.60%); O3: (6.46%, 6.86%, 9.79%). Furthermore, we demonstrate
the effectiveness of each module of M2G2 by ablation study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaodan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jinyue Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00525">
<title>Pack and Measure: An Effective Approach for Influence Propagation in Social Networks. (arXiv:2401.00525v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.00525</link>
<description rdf:parseType="Literal">&lt;p&gt;The Influence Maximization problem under the Independent Cascade model (IC)
is considered. The problem asks for a minimal set of vertices to serve as &quot;seed
set&quot; from which a maximum influence propagation is expected. New seed-set
selection methods are introduced based on the notions of a $d$-packing and
vertex centrality. In particular, we focus on selecting seed-vertices that are
far apart and whose influence-values are the highest in their local
communities. Our best results are achieved via an initial computation of a
$d$-Packing followed by selecting either vertices of high degree or high
centrality in their respective closed neighborhoods. This overall &quot;Pack and
Measure&quot; approach proves highly effective as a seed selection method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Khzam_F/0/1/0/all/0/1&quot;&gt;Faisal N. Abu-Khzam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matar_G/0/1/0/all/0/1&quot;&gt;Ghinwa Bou Matar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoumi_S/0/1/0/all/0/1&quot;&gt;Sergio Thoumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00529">
<title>GraphGPT: Graph Learning with Generative Pre-trained Transformers. (arXiv:2401.00529v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00529</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce \textit{GraphGPT}, a novel model for Graph learning by
self-supervised Generative Pre-training Transformers. Our model transforms each
graph or sampled subgraph into a sequence of tokens representing the node, edge
and attributes reversibly using the Eulerian path first. Then we feed the
tokens into a standard transformer decoder and pre-train it with the
next-token-prediction (NTP) task. Lastly, we fine-tune the GraphGPT model with
the supervised tasks. This intuitive, yet effective model achieves superior or
close results to the state-of-the-art methods for the graph-, edge- and
node-level tasks on the large scale molecular dataset PCQM4Mv2, the
protein-protein association dataset ogbl-ppa and the ogbn-proteins dataset from
the Open Graph Benchmark (OGB). Furthermore, the generative pre-training
enables us to train GraphGPT up to 400M+ parameters with consistently
increasing performance, which is beyond the capability of GNNs and previous
graph transformers. The source code and pre-trained checkpoints will be
released soon\footnote{\url{https://github.com/alibaba/graph-gpt}} to pave the
way for the graph foundation model research, and also to assist the scientific
discovery in pharmaceutical, chemistry, material and bio-informatics domains,
etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qifang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1&quot;&gt;Weidong Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00532">
<title>On the Necessity of Metalearning: Learning Suitable Parameterizations for Learning Processes. (arXiv:2401.00532v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00532</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we will discuss metalearning and how we can go beyond the
current classical learning paradigm. We will first address the importance of
inductive biases in the learning process and what is at stake: the quantities
of data necessary to learn. We will subsequently see the importance of choosing
suitable parameterizations to end up with well-defined learning processes.
Especially since in the context of real-world applications, we face numerous
biases due, e.g., to the specificities of sensors, the heterogeneity of data
sources, the multiplicity of points of view, etc. This will lead us to the idea
of exploiting the structuring of the concepts to be learned in order to
organize the learning process that we published previously. We conclude by
discussing the perspectives around parameter-tying schemes and the emergence of
universal aspects in the models thus learned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidi_M/0/1/0/all/0/1&quot;&gt;Massinissa Hamidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osmani_A/0/1/0/all/0/1&quot;&gt;Aomar Osmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00536">
<title>A Multi-Task, Multi-Modal Approach for Predicting Categorical and Dimensional Emotions. (arXiv:2401.00536v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00536</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech emotion recognition (SER) has received a great deal of attention in
recent years in the context of spontaneous conversations. While there have been
notable results on datasets like the well known corpus of naturalistic dyadic
conversations, IEMOCAP, for both the case of categorical and dimensional
emotions, there are few papers which try to predict both paradigms at the same
time. Therefore, in this work, we aim to highlight the performance contribution
of multi-task learning by proposing a multi-task, multi-modal system that
predicts categorical and dimensional emotions. The results emphasise the
importance of cross-regularisation between the two types of emotions. Our
approach consists of a multi-task, multi-modal architecture that uses parallel
feature refinement through self-attention for the feature of each modality. In
order to fuse the features, our model introduces a set of learnable bridge
tokens that merge the acoustic and linguistic features with the help of
cross-attention. Our experiments for categorical emotions on 10-fold validation
yield results comparable to the current state-of-the-art. In our configuration,
our multi-task approach provides better results compared to learning each
paradigm separately. On top of that, our best performing model achieves a high
result for valence compared to the previous multi-task experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ispas_A/0/1/0/all/0/1&quot;&gt;Alex-R&amp;#x103;zvan Ispas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deschamps_Berger_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;o Deschamps-Berger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devillers_L/0/1/0/all/0/1&quot;&gt;Laurence Devillers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00544">
<title>A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models. (arXiv:2401.00544v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00544</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores the integration of large language models (LLMs) into
scientific data assimilation, focusing on combustion science as a case study.
Leveraging foundational models integrated with Retrieval-Augmented Generation
(RAG) framework, the study introduces an approach to process diverse combustion
research data, spanning experimental studies, simulations, and literature. The
multifaceted nature of combustion research emphasizes the critical role of
knowledge processing in navigating and extracting valuable information from a
vast and diverse pool of sources. The developed approach minimizes
computational and economic expenses while optimizing data privacy and accuracy.
It incorporates prompt engineering and offline open-source LLMs, offering user
autonomy in selecting base models. The study provides a thorough examination of
text segmentation strategies, conducts comparative studies between LLMs, and
explores various optimized prompts to demonstrate the effectiveness of the
framework. By incorporating an external database, the framework outperforms a
conventional LLM in generating accurate responses and constructing robust
arguments. Additionally, the study delves into the investigation of optimized
prompt templates for the purpose of efficient extraction of scientific
literature. The research addresses concerns related to hallucinations and false
research articles by introducing a custom workflow developed with a detection
algorithm to filter out inaccuracies. Despite identified areas for improvement,
the framework consistently delivers accurate domain-specific responses with
minimal human oversight. The prompt-agnostic approach introduced holds promise
for future deliberations. The study underscores the significance of integrating
LLMs and knowledge processing techniques in scientific research, providing a
foundation for advancements in data assimilation and utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vansh Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1&quot;&gt;Venkat Raman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00546">
<title>AllSpark: a multimodal spatiotemporal general model. (arXiv:2401.00546v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00546</link>
<description rdf:parseType="Literal">&lt;p&gt;For a long time, due to the high heterogeneity in structure and semantics
among various spatiotemporal modal data, the joint interpretation of multimodal
spatiotemporal data has been an extremely challenging problem. The primary
challenge resides in striking a trade-off between the cohesion and autonomy of
diverse modalities, and this trade-off exhibits a progressively nonlinear
nature as the number of modalities expands. We introduce the Language as
Reference Framework (LaRF), a fundamental principle for constructing a
multimodal unified model, aiming to strike a trade-off between the cohesion and
autonomy among different modalities. We propose a multimodal spatiotemporal
general artificial intelligence model, called AllSpark. Our model integrates
thirteen different modalities into a unified framework, including 1D (text,
code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs,
trajectory, oblique photography), and 3D (point clouds, videos) modalities. To
achieve modal cohesion, AllSpark uniformly maps diverse modal features to the
language modality. In addition, we design modality-specific prompts to guide
multi-modal large language models in accurately perceiving multimodal data. To
maintain modality autonomy, AllSpark introduces modality-specific encoders to
extract the tokens of various spatiotemporal modalities. And modal bridge is
employed to achieve dimensional projection from each modality to the language
modality. Finally, observing a gap between the model&apos;s interpretation and
downstream tasks, we designed task heads to enhance the model&apos;s generalization
capability on specific downstream tasks. Experiments indicate that AllSpark
achieves competitive accuracy in modalities such as RGB and trajectory compared
to state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Run Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;YanSheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dapeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shizhong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiayi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haifeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00563">
<title>KernelGPT: Enhanced Kernel Fuzzing via Large Language Models. (arXiv:2401.00563v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.00563</link>
<description rdf:parseType="Literal">&lt;p&gt;Bugs in operating system kernels can affect billions of devices and users all
over the world. As a result, a large body of research has been focused on
kernel fuzzing, i.e., automatically generating syscall (system call) sequences
to detect potential kernel bugs or vulnerabilities. Syzkaller, one of the most
widely studied kernel fuzzers, aims to generate valid syscall sequences based
on predefined specifications written in syzlang, a domain-specific language for
defining syscalls, their arguments, and the relationships between them. While
there has been existing work trying to automate Syzkaller specification
generation, this still remains largely manual work and a large number of
important syscalls are still uncovered. In this paper, we propose KernelGPT,
the first approach to automatically inferring Syzkaller specifications via
Large Language Models (LLMs) for enhanced kernel fuzzing. Our basic insight is
that LLMs have seen massive kernel code, documentation, and use cases during
pre-training, and thus can automatically distill the necessary information for
making valid syscalls. More specifically, KernelGPT leverages an iterative
approach to automatically infer all the necessary specification components, and
further leverages the validation feedback to repair/refine the initial
specifications. Our preliminary results demonstrate that KernelGPT can help
Syzkaller achieve higher coverage and find multiple previously unknown bugs.
Moreover, we also received a request from the Syzkaller team to upstream
specifications inferred by KernelGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zijie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lingming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00579">
<title>Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing. (arXiv:2401.00579v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00579</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), particularly those similar to ChatGPT, have
significantly influenced the field of Natural Language Processing (NLP). While
these models excel in general language tasks, their performance in
domain-specific downstream tasks such as biomedical and clinical Named Entity
Recognition (NER), Relation Extraction (RE), and Medical Natural Language
Inference (NLI) is still evolving. In this context, our study investigates the
potential of instruction tuning for biomedical language processing, applying
this technique to two general LLMs of substantial scale. We present a
comprehensive, instruction-based model trained on a dataset that consists of
approximately $200,000$ instruction-focused samples. This dataset represents a
carefully curated compilation of existing data, meticulously adapted and
reformatted to align with the specific requirements of our instruction-based
tasks. This initiative represents an important step in utilising such models to
achieve results on par with specialised encoder-only models like BioBERT and
BioClinicalBERT for various classical biomedical NLP tasks. Our work includes
an analysis of the dataset&apos;s composition and its impact on model performance,
providing insights into the intricacies of instruction tuning. By sharing our
codes, models, and the distinctively assembled instruction-based dataset, we
seek to encourage ongoing research and development in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohanian_O/0/1/0/all/0/1&quot;&gt;Omid Rohanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nouriborji_M/0/1/0/all/0/1&quot;&gt;Mohammadmahdi Nouriborji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1&quot;&gt;David A. Clifton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00587">
<title>Brain Tumor Segmentation Based on Deep Learning, Attention Mechanisms, and Energy-Based Uncertainty Prediction. (arXiv:2401.00587v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.00587</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain tumors are one of the deadliest forms of cancer with a mortality rate
of over 80%. A quick and accurate diagnosis is crucial to increase the chance
of survival. However, in medical analysis, the manual annotation and
segmentation of a brain tumor can be a complicated task. Multiple MRI
modalities are typically analyzed as they provide unique information regarding
the tumor regions. Although these MRI modalities are helpful for segmenting
gliomas, they tend to increase overfitting and computation. This paper proposes
a region of interest detection algorithm that is implemented during data
preprocessing to locate salient features and remove extraneous MRI data. This
decreases the input size, allowing for more aggressive data augmentations and
deeper neural networks. Following the preprocessing of the MRI modalities, a
fully convolutional autoencoder with soft attention segments the different
brain MRIs. When these deep learning algorithms are implemented in practice,
analysts and physicians cannot differentiate between accurate and inaccurate
predictions. Subsequently, test time augmentations and an energy-based model
were used for voxel-based uncertainty predictions. Experimentation was
conducted on the BraTS benchmarks and achieved state-of-the-art segmentation
performance. Additionally, qualitative results were used to assess the
segmentation models and uncertainty predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schwehr_Z/0/1/0/all/0/1&quot;&gt;Zachary Schwehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Achanta_S/0/1/0/all/0/1&quot;&gt;Sriman Achanta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00588">
<title>Fairness in Serving Large Language Models. (arXiv:2401.00588v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00588</link>
<description rdf:parseType="Literal">&lt;p&gt;High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide
range of requests from short chat conversations to long document reading. To
ensure that all client requests are processed fairly, most major LLM inference
services have request rate limits, to ensure that no client can dominate the
request queue. However, this rudimentary notion of fairness also results in
under-utilization of the resources and poor client experience when there is
spare capacity. While there is a rich literature on fair scheduling, serving
LLMs presents new challenges due to their unpredictable request lengths and
their unique batching characteristics on parallel accelerators. This paper
introduces the definition of LLM serving fairness based on a cost function that
accounts for the number of input and output tokens processed. To achieve
fairness in serving, we propose a novel scheduling algorithm, the Virtual Token
Counter (VTC), a fair scheduler based on the continuous batching mechanism. We
prove a 2x tight upper bound on the service difference between two backlogged
clients, adhering to the requirement of work-conserving. Through extensive
experiments, we demonstrate the superior performance of VTC in ensuring
fairness, especially in contrast to other baseline methods, which exhibit
shortcomings under various conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Ying Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1&quot;&gt;Shiyi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dacheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_D/0/1/0/all/0/1&quot;&gt;Danyang Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00608">
<title>Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00608</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera traps are valuable tools in animal ecology for biodiversity monitoring
and conservation. However, challenges like poor generalization to deployment at
new unseen locations limit their practical application. Images are naturally
associated with heterogeneous forms of context possibly in different
modalities. In this work, we leverage the structured context associated with
the camera trap images to improve out-of-distribution generalization for the
task of species identification in camera traps. For example, a photo of a wild
animal may be associated with information about where and when it was taken, as
well as structured biology knowledge about the animal species. While typically
overlooked by existing work, bringing back such context offers several
potential benefits for better image understanding, such as addressing data
scarcity and enhancing generalization. However, effectively integrating such
heterogeneous context into the visual domain is a challenging problem. To
address this, we propose a novel framework that reformulates species
classification as link prediction in a multimodal knowledge graph (KG). This
framework seamlessly integrates various forms of multimodal context for visual
recognition. We apply this framework for out-of-distribution species
classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets
and achieve competitive performance with state-of-the-art approaches.
Furthermore, our framework successfully incorporates biological taxonomy for
improved generalization and enhances sample efficiency for recognizing
under-represented species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1&quot;&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weidi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Cheng-Hao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong-You Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1&quot;&gt;Tanya Berger-Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1&quot;&gt;Charles Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00609">
<title>A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots. (arXiv:2401.00609v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00609</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a review of personality in neural conversational agents (CAs),
also called chatbots. First, we define Personality, Persona, and Profile. We
explain all personality schemes which have been used in CAs, and list models
under the scheme(s) which they use. Second we describe 21 datasets which have
been developed in recent CA personality research. Third, we define the methods
used to embody personality in a CA, and review recent models using them.
Fourth, we survey some relevant reviews on CAs, personality, and related
topics. Finally, we draw conclusions and identify some research challenges for
this important emerging field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutcliffe_R/0/1/0/all/0/1&quot;&gt;Richard Sutcliffe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00611">
<title>A Compact Representation for Bayesian Neural Networks By Removing Permutation Symmetry. (arXiv:2401.00611v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.00611</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian neural networks (BNNs) are a principled approach to modeling
predictive uncertainties in deep learning, which are important in
safety-critical applications. Since exact Bayesian inference over the weights
in a BNN is intractable, various approximate inference methods exist, among
which sampling methods such as Hamiltonian Monte Carlo (HMC) are often
considered the gold standard. While HMC provides high-quality samples, it lacks
interpretable summary statistics because its sample mean and variance is
meaningless in neural networks due to permutation symmetry. In this paper, we
first show that the role of permutations can be meaningfully quantified by a
number of transpositions metric. We then show that the recently proposed
rebasin method allows us to summarize HMC samples into a compact representation
that provides a meaningful explicit uncertainty estimate for each weight in a
neural network, thus unifying sampling methods with variational inference. We
show that this compact representation allows us to compare trained BNNs
directly in weight space across sampling methods and variational inference, and
to efficiently prune neural networks trained without explicit Bayesian
frameworks by exploiting uncertainty estimates from HMC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tim Z. Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bamler_R/0/1/0/all/0/1&quot;&gt;Robert Bamler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00631">
<title>Coordinated Deep Neural Networks: A Versatile Edge Offloading Algorithm. (arXiv:2401.00631v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.00631</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) applications continue to expand, there is a
growing need for deep neural network (DNN) models. Although DNN models deployed
at the edge are promising to provide AI as a service with low latency, their
cooperation is yet to be explored. In this paper, we consider the DNN service
providers share their computing resources as well as their models&apos; parameters
and allow other DNNs to offload their computations without mirroring. We
propose a novel algorithm called coordinated DNNs on edge (\textbf{CoDE}) that
facilitates coordination among DNN services by creating multi-task DNNs out of
individual models. CoDE aims to find the optimal path that results in the
lowest possible cost, where the cost reflects the inference delay, model
accuracy, and local computation workload. With CoDE, DNN models can make new
paths for inference by using their own or other models&apos; parameters. We then
evaluate the performance of CoDE through numerical experiments. The results
demonstrate a $75\%$ reduction in the local service computation workload while
degrading the accuracy by only $2\%$ and having the same inference time in a
balanced load condition. Under heavy load, CoDE can further decrease the
inference time by $30\%$ while the accuracy is reduced by only $4\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maleki_A/0/1/0/all/0/1&quot;&gt;Alireza Maleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_Mansouri_H/0/1/0/all/0/1&quot;&gt;Hamed Shah-Mansouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalaj_B/0/1/0/all/0/1&quot;&gt;Babak H. Khalaj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00663">
<title>1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation. (arXiv:2401.00663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00663</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent transformer-based models have dominated the Referring Video Object
Segmentation (RVOS) task due to the superior performance. Most prior works
adopt unified DETR framework to generate segmentation masks in
query-to-instance manner. In this work, we integrate strengths of that leading
RVOS models to build up an effective paradigm. We first obtain binary mask
sequences from the RVOS models. To improve the consistency and quality of
masks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally
ensembles RVOS models based on framework design as well as training strategy,
and leverages different video object segmentation (VOS) models to enhance mask
coherence by object propagation mechanism. Our method achieves 75.7% J&amp;amp;F on
Ref-Youtube-VOS validation set and 70% J&amp;amp;F on test set, which ranks 1st place
on 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.
Code is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhuoyan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yicheng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00685">
<title>Communication-Efficient Federated Learning for LEO Constellations Integrated with HAPs Using Hybrid NOMA-OFDM. (arXiv:2401.00685v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00685</link>
<description rdf:parseType="Literal">&lt;p&gt;Space AI has become increasingly important and sometimes even necessary for
government, businesses, and society. An active research topic under this
mission is integrating federated learning (FL) with satellite communications
(SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively
train a machine learning model. However, the special communication environment
of SatCom leads to a very slow FL training process up to days and weeks. This
paper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO
satellites, that (1) utilizes high-altitude platforms (HAPs) as distributed
parameter servers (PS) to enhance satellite visibility, and (2) introduces
non-orthogonal multiple access (NOMA) into LEO to enable fast and
bandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a
new communication topology that exploits HAPs to bridge satellites among
different orbits to mitigate the Doppler shift, and (4) a new FL model
aggregation scheme that optimally balances models between different orbits and
shells. Moreover, we (5) derive a closed-form expression of the outage
probability for satellites in near and far shells, as well as for the entire
system. Our extensive simulations have validated the mathematical analysis and
demonstrated the superior performance of NomaFedHAP in achieving fast and
efficient FL model convergence with high accuracy as compared to the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmahallawy_M/0/1/0/all/0/1&quot;&gt;Mohamed Elmahallawy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramadan_K/0/1/0/all/0/1&quot;&gt;Khaled Ramadan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00689">
<title>Large language model for Bible sentiment analysis: Sermon on the Mount. (arXiv:2401.00689v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00689</link>
<description rdf:parseType="Literal">&lt;p&gt;The revolution of natural language processing via large language models has
motivated its use in multidisciplinary areas that include social sciences and
humanities and more specifically, comparative religion. Sentiment analysis
provides a mechanism to study the emotions expressed in text. Recently,
sentiment analysis has been used to study and compare translations of the
Bhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we
use sentiment analysis for studying selected chapters of the Bible. These
chapters are known as the Sermon on the Mount. We utilize a pre-trained
language model for sentiment analysis by reviewing five translations of the
Sermon on the Mount, which include the King James version, the New
International Version, the New Revised Standard Version, the Lamsa Version, and
the Basic English Version. We provide a chapter-by-chapter and verse-by-verse
comparison using sentiment and semantic analysis and review the major
sentiments expressed. Our results highlight the varying sentiments across the
chapters and verses. We found that the vocabulary of the respective
translations is significantly different. We detected different levels of
humour, optimism, and empathy in the respective chapters that were used by
Jesus to deliver his message.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vora_M/0/1/0/all/0/1&quot;&gt;Mahek Vora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blau_T/0/1/0/all/0/1&quot;&gt;Tom Blau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kachhwal_V/0/1/0/all/0/1&quot;&gt;Vansh Kachhwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solo_A/0/1/0/all/0/1&quot;&gt;Ashu M. G. Solo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1&quot;&gt;Rohitash Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00698">
<title>Large Language Models aren&apos;t all that you need. (arXiv:2401.00698v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00698</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the architecture and systems built towards solving the
SemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity
Recognition) [1]. We evaluate two approaches (a) a traditional Conditional
Random Fields model and (b) a Large Language Model (LLM) fine-tuned with a
customized head and compare the two approaches. The novel ideas explored are:
1) Decaying auxiliary loss (with residual) - where we train the model on an
auxiliary task of Coarse-Grained NER and include this task as a part of the
loss function 2) Triplet token blending - where we explore ways of blending the
embeddings of neighboring tokens in the final NER layer prior to prediction 3)
Task-optimal heads - where we explore a variety of custom heads and learning
rates for the final layer of the LLM. We also explore multiple LLMs including
GPT-3 and experiment with a variety of dropout and other hyperparameter
settings before arriving at our final model which achieves micro &amp;amp; macro f1 of
0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while
pre-trained LLMs, by themselves, bring about a large improvement in scores as
compared to traditional models, we also demonstrate that tangible improvements
to the Macro-F1 score can be made by augmenting the LLM with additional
feature/loss/model engineering techniques described above.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holla_K/0/1/0/all/0/1&quot;&gt;Kiran Voderhobli Holla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1&quot;&gt;Chaithanya Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aryan Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00700">
<title>An attempt to generate new bridge types from latent space of generative adversarial network. (arXiv:2401.00700v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00700</link>
<description rdf:parseType="Literal">&lt;p&gt;Try to generate new bridge types using generative artificial intelligence
technology. Symmetric structured image dataset of three-span beam bridge, arch
bridge, cable-stayed bridge and suspension bridge are used . Based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
as well as Wasserstein loss function and Lipschitz constraints, generative
adversarial network is constructed and trained. From the obtained low
dimensional bridge-type latent space sampling, new bridge types with asymmetric
structures can be generated. Generative adversarial network can create new
bridge types by organically combining different structural components on the
basis of human original bridge types. It has a certain degree of human original
ability. Generative artificial intelligence technology can open up imagination
space and inspire humanity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00711">
<title>Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute. (arXiv:2401.00711v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00711</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating 3D human models directly from text helps reduce the cost and time
of character modeling. However, achieving multi-attribute controllable and
realistic 3D human avatar generation is still challenging due to feature
coupling and the scarcity of realistic 3D human avatar datasets. To address
these issues, we propose Text2Avatar, which can generate realistic-style 3D
avatars based on the coupled text prompts. Text2Avatar leverages a discrete
codebook as an intermediate feature to establish a connection between text and
avatars, enabling the disentanglement of features. Furthermore, to alleviate
the scarcity of realistic style 3D human avatar data, we utilize a pre-trained
unconditional 3D human avatar generation model to obtain a large amount of 3D
avatar pseudo data, which allows Text2Avatar to achieve realistic style
generation. Experimental results demonstrate that our method can generate
realistic 3D avatars from coupled textual data, which is challenging for other
existing methods in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chaoqun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuqin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ronghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_A/0/1/0/all/0/1&quot;&gt;Achun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yachao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00719">
<title>Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition. (arXiv:2401.00719v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00719</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing availability of consumer depth sensors, 3D face
recognition (FR) has attracted more and more attention. However, the data
acquired by these sensors are often coarse and noisy, making them impractical
to use directly. In this paper, we introduce an innovative Depth map denoising
network (DMDNet) based on the Denoising Implicit Image Function (DIIF) to
reduce noise and enhance the quality of facial depth images for low-quality 3D
FR. After generating clean depth faces using DMDNet, we further design a
powerful recognition network called Lightweight Depth and Normal Fusion network
(LDNFNet), which incorporates a multi-branch fusion block to learn unique and
complementary features between different modalities such as depth and normal
images. Comprehensive experiments conducted on four distinct low-quality
databases demonstrate the effectiveness and robustness of our proposed methods.
Furthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art
results on the Lock3DFace database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ruizhuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1&quot;&gt;Chao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Junlan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Weihong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00736">
<title>Diffusion Models, Image Super-Resolution And Everything: A Survey. (arXiv:2401.00736v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00736</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Models (DMs) represent a significant advancement in image
Super-Resolution (SR), aligning technical image quality more closely with human
preferences and expanding SR applications. DMs address critical limitations of
previous methods, enhancing overall realism and details in SR images. However,
DMs suffer from color-shifting issues, and their high computational costs call
for efficient sampling alternatives, underscoring the challenge of balancing
computational efficiency and image quality. This survey gives an overview of
DMs applied to image SR and offers a detailed analysis that underscores the
unique characteristics and methodologies within this domain, distinct from
broader existing reviews in the field. It presents a unified view of DM
fundamentals and explores research directions, including alternative input
domains, conditioning strategies, guidance, corruption spaces, and zero-shot
methods. This survey provides insights into the evolution of image SR with DMs,
addressing current trends, challenges, and future directions in this rapidly
evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian B. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanbhag_A/0/1/0/all/0/1&quot;&gt;Arundhati S. Shanbhag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1&quot;&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1&quot;&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00737">
<title>Searching, fast and slow, through product catalogs. (arXiv:2401.00737v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.00737</link>
<description rdf:parseType="Literal">&lt;p&gt;String matching algorithms in the presence of abbreviations, such as in Stock
Keeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In
this paper, we present a unified architecture for SKU search that provides both
a real-time suggestion system (based on a Trie data structure) as well as a
lower latency search system (making use of character level TF-IDF in
combination with language model vector embeddings) where users initiate the
search process explicitly. We carry out ablation studies that justify designing
a complex search system composed of multiple components to address the delicate
trade-off between speed and accuracy. Using SKU search in the Dynamics CRM as
an example, we show how our system vastly outperforms, in all aspects, the
results provided by the default search engine. Finally, we show how SKU
descriptions may be enhanced via generative text models (using gpt-3.5-turbo)
so that the consumers of the search results may get more context and a
generally better experience when presented with the results of their SKU
search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1&quot;&gt;Dayananda Ubrangala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1&quot;&gt;Juhi Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangappa_S/0/1/0/all/0/1&quot;&gt;Sharath Kumar Rangappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_K/0/1/0/all/0/1&quot;&gt;Kiran R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1&quot;&gt;Ravi Prasad Kondapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boue_L/0/1/0/all/0/1&quot;&gt;Laurent Bou&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00739">
<title>DiffMorph: Text-less Image Morphing with Diffusion Models. (arXiv:2401.00739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00739</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-conditioned image generation models are a prevalent use of AI image
synthesis, yet intuitively controlling output guided by an artist remains
challenging. Current methods require multiple images and textual prompts for
each object to specify them as concepts to generate a single customized image.
&lt;/p&gt;
&lt;p&gt;On the other hand, our work, \verb|DiffMorph|, introduces a novel approach
that synthesizes images that mix concepts without the use of textual prompts.
Our work integrates a sketch-to-image module to incorporate user sketches as
input. \verb|DiffMorph| takes an initial image with conditioning artist-drawn
sketches to generate a morphed image.
&lt;/p&gt;
&lt;p&gt;We employ a pre-trained text-to-image diffusion model and fine-tune it to
reconstruct each image faithfully. We seamlessly merge images and concepts from
sketches into a cohesive composition. The image generation capability of our
work is demonstrated through our results and a comparison of these with
prompt-based image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Shounak Chatterjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00741">
<title>ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. (arXiv:2401.00741v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00741</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing evaluations of tool learning primarily focus on validating the
alignment of selected tools for large language models (LLMs) with expected
outcomes. However, these approaches rely on a limited set of scenarios where
answers can be pre-determined, diverging from genuine needs. Furthermore, a
sole emphasis on outcomes disregards the intricate capabilities essential for
LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a
fine-grained system tailored for the evaluation of the LLMs&apos; tool learning
capabilities in authentic scenarios. The system meticulously examines seven
real-world scenarios, analyzing five dimensions crucial to LLMs in tool
learning: format alignment, intent comprehension, behavior planning, tool
selection, and answer organization. Additionally, ToolEyes incorporates a tool
library boasting approximately 600 tools, serving as an intermediary between
LLMs and the physical world. Evaluations involving ten LLMs across three
categories reveal a preference for specific scenarios and limited cognitive
abilities in tool learning. Intriguingly, expanding the model size even
exacerbates the hindrance to tool learning. These findings offer instructive
insights aimed at advancing the field of tool learning. The data is available
att https://github.com/Junjie-Ye/ToolEyes.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Caishuang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yilong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sixian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaoran Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00756">
<title>MPRE: Multi-perspective Patient Representation Extractor for Disease Prediction. (arXiv:2401.00756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00756</link>
<description rdf:parseType="Literal">&lt;p&gt;Patient representation learning based on electronic health records (EHR) is a
critical task for disease prediction. This task aims to effectively extract
useful information on dynamic features. Although various existing works have
achieved remarkable progress, the model performance can be further improved by
fully extracting the trends, variations, and the correlation between the trends
and variations in dynamic features. In addition, sparse visit records limit the
performance of deep learning models. To address these issues, we propose the
Multi-perspective Patient Representation Extractor (MPRE) for disease
prediction. Specifically, we propose Frequency Transformation Module (FTM) to
extract the trend and variation information of dynamic features in the
time-frequency domain, which can enhance the feature representation. In the 2D
Multi-Extraction Network (2D MEN), we form the 2D temporal tensor based on
trend and variation. Then, the correlations between trend and variation are
captured by the proposed dilated operation. Moreover, we propose the
First-Order Difference Attention Mechanism (FODAM) to calculate the
contributions of differences in adjacent variations to the disease diagnosis
adaptively. To evaluate the performance of MPRE and baseline methods, we
conduct extensive experiments on two real-world public datasets. The experiment
results show that MPRE outperforms state-of-the-art baseline methods in terms
of AUROC and AUPRC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Ziyue Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiayi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Wuman Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tse_R/0/1/0/all/0/1&quot;&gt;Rita Tse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pau_G/0/1/0/all/0/1&quot;&gt;Giovanni Pau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00757">
<title>A &amp; B == B &amp; A: Triggering Logical Reasoning Failures in Large Language Models. (arXiv:2401.00757v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.00757</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have propelled Artificial
Intelligence (AI) to new heights, enabling breakthroughs in various tasks such
as writing assistance, code generation, and machine translation. A significant
distinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to
&quot;reason.&quot; However, evaluating the reasoning ability of LLMs remains a challenge
as most existing evaluations focus on their accuracy on the downstream tasks
rather than directly assessing their reasoning processes. Efforts have been
made to develop benchmarks and metrics to assess reasoning in LLMs, but they
suffer from data leakage or limited scope. In this paper, we introduce
LogicAsker, an automatic approach that comprehensively evaluates and improves
the logical reasoning abilities of LLMs under a set of atomic reasoning skills
based on propositional and predicate logic. The results provide insights into
LLMs&apos; reasoning abilities and reveal the logical rules the LLMs did not learn
well. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,
ChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases
from LogicAsker can find logical reasoning failures in different LLMs with a
rate of 25\% - 94\%. In addition, the test cases of LogicAsker can be further
used to design demonstration examples for in-context learning, which
effectively improves the logical reasoning ability of LLMs, e.g., 10\% for
GPT-4. As far as we know, our work is the first to create prompts based on
testing results to improve LLMs&apos; formal reasoning ability effectively. All the
code, data, and results will be released for reproduction and future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiliu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jen-tse Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pinjia He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1&quot;&gt;Wenxiang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael R. Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00761">
<title>The Earth is Flat? Unveiling Factual Errors in Large Language Models. (arXiv:2401.00761v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.00761</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) like ChatGPT are foundational in various
applications due to their extensive knowledge from pre-training and
fine-tuning. Despite this, they are prone to generating factual and commonsense
errors, raising concerns in critical areas like healthcare, journalism, and
education to mislead users. Current methods for evaluating LLMs&apos; veracity are
limited by test data leakage or the need for extensive human labor, hindering
efficient and accurate error detection. To tackle this problem, we introduce a
novel, automatic testing framework, FactChecker, aimed at uncovering factual
inaccuracies in LLMs. This framework involves three main steps: First, it
constructs a factual knowledge graph by retrieving fact triplets from a
large-scale knowledge database. Then, leveraging the knowledge graph,
FactChecker employs a rule-based approach to generates three types of questions
(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and
multi-hop relations, along with correct answers. Lastly, it assesses the LLMs&apos;
responses for accuracy using tailored matching strategies for each question
type. Our extensive tests on six prominent LLMs, including text-davinci-002,
text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal
that FactChecker can trigger factual errors in up to 45\% of questions in these
models. Moreover, we demonstrate that FactChecker&apos;s test cases can improve
LLMs&apos; factual accuracy through in-context learning and fine-tuning (e.g.,
llama-2-13b-chat&apos;s accuracy increase from 35.3\% to 68.5\%). We are making all
code, data, and results available for future research endeavors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Juluan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jen-tse Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1&quot;&gt;Wenxiang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael R. Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00763">
<title>New Job, New Gender? Measuring the Social Bias in Image Generation Models. (arXiv:2401.00763v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.00763</link>
<description rdf:parseType="Literal">&lt;p&gt;Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel metamorphic testing
framework that can accurately, automatically and comprehensively trigger social
bias in image generation models. BiasPainter uses a diverse range of seed
images of individuals and prompts the image generation models to edit these
images using gender, race, and age-neutral queries. These queries span 62
professions, 39 activities, 57 types of objects, and 70 personality traits. The
framework then compares the edited images to the original seed images, focusing
on any changes related to gender, race, and age. BiasPainter adopts a testing
oracle that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. To evaluate the
effectiveness of BiasPainter, we use BiasPainter to test five widely-used
commercial image generation software and models, such as stable diffusion and
Midjourney. Experimental results show that 100\% of the generated test cases
can successfully trigger social bias in image generation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Haonan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jen-tse Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Haoyi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael R. Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00773">
<title>Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.00773</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic mixture models are acknowledged as a valuable tool for
unsupervised outlier detection owing to their interpretability and intuitive
grounding in statistical principles. Within this framework, Dirichlet process
mixture models emerge as a compelling alternative to conventional finite
mixture models for both clustering and outlier detection tasks. However,
despite their evident advantages, the widespread adoption of Dirichlet process
mixture models in unsupervised outlier detection has been hampered by
challenges related to computational inefficiency and sensitivity to outliers
during the construction of detectors. To tackle these challenges, we propose a
novel outlier detection method based on ensembles of Dirichlet process Gaussian
mixtures. The proposed method is a fully unsupervised algorithm that
capitalizes on random subspace and subsampling ensembles, not only ensuring
efficient computation but also enhancing the robustness of the resulting
outlier detector. Moreover, the proposed method leverages variational inference
for Dirichlet process mixtures to ensure efficient and fast computation.
Empirical studies with benchmark datasets demonstrate that our method
outperforms existing approaches for unsupervised outlier detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongwook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Juyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hee Cheol Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Seonghyun Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00776">
<title>Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study in the Autism Spectrum Disorder Therapy. (arXiv:2401.00776v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.00776</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, edge computing has served as a paradigm that enables many
future technologies like AI, Robotics, IoT, and high-speed wireless sensor
networks (like 5G) by connecting cloud computing facilities and services to the
end users. Especially in medical and healthcare applications, it provides
remote patient monitoring and increases voluminous multimedia. From the
robotics angle, robot-assisted therapy (RAT) is an active-assistive robotic
technology in rehabilitation robotics, attracting many researchers to study and
benefit people with disability like autism spectrum disorder (ASD) children.
However, the main challenge of RAT is that the model capable of detecting the
affective states of ASD people exists and can recall individual preferences.
Moreover, involving expert diagnosis and recommendations to guide robots in
updating the therapy approach to adapt to different statuses and scenarios is a
crucial part of the ASD therapy process. This paper proposes the architecture
of edge cognitive computing by combining human experts and assisted robots
collaborating in the same framework to help ASD patients with long-term
support. By integrating the real-time computing and analysis of a new cognitive
robotic model for ASD therapy, the proposed architecture can achieve a seamless
remote diagnosis, round-the-clock symptom monitoring, emergency warning,
therapy alteration, and advanced assistance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00779">
<title>Temporal Validity Change Prediction. (arXiv:2401.00779v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00779</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal validity is an important property of text that is useful for many
downstream applications, such as recommender systems, conversational AI, or
story understanding. Existing benchmarking tasks often require models to
identify the temporal validity duration of a single statement. However, in many
cases, additional contextual information, such as sentences in a story or posts
on a social media profile, can be collected from the available text stream.
This contextual information may greatly alter the duration for which a
statement is expected to be valid. We propose Temporal Validity Change
Prediction, a natural language processing task benchmarking the capability of
machine learning models to detect contextual statements that induce such
change. We create a dataset consisting of temporal target statements sourced
from Twitter and crowdsource sample context statements. We then benchmark a set
of transformer-based language models on our dataset. Finally, we experiment
with temporal validity duration prediction as an auxiliary task to improve the
performance of the state-of-the-art model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenzel_G/0/1/0/all/0/1&quot;&gt;Georg Wenzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1&quot;&gt;Adam Jatowt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00788">
<title>Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models. (arXiv:2401.00788v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.00788</link>
<description rdf:parseType="Literal">&lt;p&gt;The high cost of full-parameter fine-tuning (FFT) of Large Language Models
(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.
However, it remains unclear which methods provide the best cost-performance
trade-off at different model scales. We introduce Astraios, a suite of 28
instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up
to 16 billion parameters. Through investigations across 5 tasks and 8 different
datasets encompassing both code comprehension and code generation tasks, we
find that FFT generally leads to the best downstream performance across all
scales, and PEFT methods differ significantly in their efficacy based on the
model scale. LoRA usually offers the most favorable trade-off between cost and
performance. Further investigation into the effects of these methods on both
model robustness and code security reveals that larger models tend to
demonstrate reduced robustness and less security. At last, we explore the
relationships among updated parameters, cross-entropy loss, and task
performance. We find that the tuning effectiveness observed in small models
generalizes well to larger models, and the validation loss in instruction
tuning can be a reliable indicator of overall downstream performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1&quot;&gt;Terry Yue Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zebaze_A/0/1/0/all/0/1&quot;&gt;Armel Zebaze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suppattarachai_N/0/1/0/all/0/1&quot;&gt;Nitchakarn Suppattarachai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werra_L/0/1/0/all/0/1&quot;&gt;Leandro von Werra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1&quot;&gt;Harm de Vries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1&quot;&gt;Niklas Muennighoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00832">
<title>Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education. (arXiv:2401.00832v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.00832</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of Artificial Intelligence (AI), particularly Large Language
Model (LLM)-based systems, in education has shown promise in enhancing teaching
and learning experiences. However, the advent of Multimodal Large Language
Models (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing
multimodal data including text, sound, and visual inputs, opens a new era of
enriched, personalized, and interactive learning landscapes in education.
Grounded in theory of multimedia learning, this paper explores the
transformative role of MLLMs in central aspects of science education by
presenting exemplary innovative learning scenarios. Possible applications for
MLLMs could range from content creation to tailored support for learning,
fostering competencies in scientific practices, and providing assessment and
feedback. These scenarios are not limited to text-based and uni-modal formats
but can be multimodal, increasing thus personalization, accessibility, and
potential learning effectiveness. Besides many opportunities, challenges such
as data protection and ethical considerations become more salient, calling for
robust frameworks to ensure responsible integration. This paper underscores the
necessity for a balanced approach in implementing MLLMs, where the technology
complements rather than supplants the educator&apos;s role, ensuring thus an
effective and ethical use of AI in science education. It calls for further
research to explore the nuanced implications of MLLMs on the evolving role of
educators and to extend the discourse beyond science education to other
disciplines. Through the exploration of potentials, challenges, and future
implications, we aim to contribute to a preliminary understanding of the
transformative trajectory of MLLMs in science education and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bewersdorff_A/0/1/0/all/0/1&quot;&gt;Arne Bewersdorff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartmann_C/0/1/0/all/0/1&quot;&gt;Christian Hartmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hornberger_M/0/1/0/all/0/1&quot;&gt;Marie Hornberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sessler_K/0/1/0/all/0/1&quot;&gt;Kathrin Se&amp;#xdf;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bannert_M/0/1/0/all/0/1&quot;&gt;Maria Bannert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1&quot;&gt;Gjergji Kasneci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaoming Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nerdel_C/0/1/0/all/0/1&quot;&gt;Claudia Nerdel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00850">
<title>Refining Pre-Trained Motion Models. (arXiv:2401.00850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.00850</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the difficulty of manually annotating motion in video, the current best
motion estimation methods are trained with synthetic data, and therefore
struggle somewhat due to a train/test gap. Self-supervised methods hold the
promise of training directly on real video, but typically perform worse. These
include methods trained with warp error (i.e., color constancy) combined with
smoothness terms, and methods that encourage cycle-consistency in the estimates
(i.e., tracking backwards should yield the opposite trajectory as tracking
forwards). In this work, we take on the challenge of improving state-of-the-art
supervised models with self-supervised training. We find that when the
initialization is supervised weights, most existing self-supervision techniques
actually make performance worse instead of better, which suggests that the
benefit of seeing the new data is overshadowed by the noise in the training
signal. Focusing on obtaining a ``clean&apos;&apos; training signal from real-world
unlabelled video, we propose to separate label-making and training into two
distinct stages. In the first stage, we use the pre-trained model to estimate
motion in a video, and then select the subset of motion estimates which we can
verify with cycle-consistency. This produces a sparse but accurate
pseudo-labelling of the video. In the second stage, we fine-tune the model to
reproduce these outputs, while also applying augmentations on the input. We
complement this boot-strapping method with simple techniques that densify and
re-balance the pseudo-labels, ensuring that we do not merely train on ``easy&apos;&apos;
tracks. We show that our method yields reliable gains over fully-supervised
methods in real videos, for both short-term (flow-based) and long-range
(multi-frame) pixel tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinglong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1&quot;&gt;Adam W. Harley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas J. Guibas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.13272">
<title>Accelerating Large Kernel Convolutions with Nested Winograd Transformation.pdf. (arXiv:2102.13272v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2102.13272</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent literature has shown that convolutional neural networks (CNNs) with
large kernels outperform vision transformers (ViTs) and CNNs with stacked small
kernels in many computer vision tasks, such as object detection and image
restoration. The Winograd transformation helps reduce the number of repetitive
multiplications in convolution and is widely supported by many commercial AI
processors. Researchers have proposed accelerating large kernel convolutions by
linearly decomposing them into many small kernel convolutions and then
sequentially accelerating each small kernel convolution with the Winograd
algorithm. This work proposes a nested Winograd algorithm that iteratively
decomposes a large kernel convolution into small kernel convolutions and proves
it to be more effective than the linear decomposition Winograd transformation
algorithm. Experiments show that compared to the linear decomposition Winograd
algorithm, the proposed algorithm reduces the total number of multiplications
by 1.4 to 10.5 times for computing 4x4 to 31x31 convolutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jingbo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xizi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsui_C/0/1/0/all/0/1&quot;&gt;Chi-Ying Tsui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.08364">
<title>Data Valuation for Vertical Federated Learning: A Model-free and Privacy-preserving Method. (arXiv:2112.08364v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.08364</link>
<description rdf:parseType="Literal">&lt;p&gt;Vertical Federated learning (VFL) is a promising paradigm for predictive
analytics, empowering an organization (i.e., task party) to enhance its
predictive models through collaborations with multiple data suppliers (i.e.,
data parties) in a decentralized and privacy-preserving way. Despite the
fast-growing interest in VFL, the lack of effective and secure tools for
assessing the value of data owned by data parties hinders the application of
VFL in business contexts. In response, we propose FedValue, a
privacy-preserving, task-specific but model-free data valuation method for VFL,
which consists of a data valuation metric and a federated computation method.
Specifically, we first introduce a novel data valuation metric, namely
MShapley-CMI. The metric evaluates a data party&apos;s contribution to a predictive
analytics task without the need of executing a machine learning model, making
it well-suited for real-world applications of VFL. Next, we develop an
innovative federated computation method that calculates the MShapley-CMI value
for each data party in a privacy-preserving manner. Extensive experiments
conducted on six public datasets validate the efficacy of FedValue for data
valuation in the context of VFL. In addition, we illustrate the practical
utility of FedValue with a case study involving federated movie
recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junjie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xiao Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.07436">
<title>SuperAnimal pretrained pose estimation models for behavioral analysis. (arXiv:2203.07436v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.07436</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantification of behavior is critical in applications ranging from
neuroscience, veterinary medicine and animal conservation efforts. A common key
step for behavioral analysis is first extracting relevant keypoints on animals,
known as pose estimation. However, reliable inference of poses currently
requires domain knowledge and manual labeling effort to build supervised
models. We present a series of technical innovations that enable a new method,
collectively called SuperAnimal, to develop unified foundation models that can
be used on over 45 species, without additional human labels. Concretely, we
introduce a method to unify the keypoint space across differently labeled
datasets (via our generalized data converter) and for training these diverse
datasets in a manner such that they don&apos;t catastrophically forget keypoints
given the unbalanced inputs (via our keypoint gradient masking and memory
replay approaches). These models show excellent performance across six pose
benchmarks. Then, to ensure maximal usability for end-users, we demonstrate how
to fine-tune the models on differently labeled data and provide tooling for
unsupervised video adaptation to boost performance and decrease jitter across
frames. If the models are fine-tuned, we show SuperAnimal models are
10-100$\times$ more data efficient than prior transfer-learning-based
approaches. We illustrate the utility of our models in behavioral
classification in mice and gait analysis in horses. Collectively, this presents
a data-efficient solution for animal pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Shaokai Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filippova_A/0/1/0/all/0/1&quot;&gt;Anastasiia Filippova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauer_J/0/1/0/all/0/1&quot;&gt;Jessy Lauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1&quot;&gt;Steffen Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_M/0/1/0/all/0/1&quot;&gt;Maxime Vidal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathis_A/0/1/0/all/0/1&quot;&gt;Alexander Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathis_M/0/1/0/all/0/1&quot;&gt;Mackenzie Weygandt Mathis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.12987">
<title>FlowX: Towards Explainable Graph Neural Networks via Message Flows. (arXiv:2206.12987v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.12987</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the explainability of graph neural networks (GNNs) as a step
toward elucidating their working mechanisms. While most current methods focus
on explaining graph nodes, edges, or features, we argue that, as the inherent
functional mechanism of GNNs, message flows are more natural for performing
explainability. To this end, we propose a novel method here, known as FlowX, to
explain GNNs by identifying important message flows. To quantify the importance
of flows, we propose to follow the philosophy of Shapley values from
cooperative game theory. To tackle the complexity of computing all coalitions&apos;
marginal contributions, we propose a flow sampling scheme to compute Shapley
value approximations as initial assessments of further training. We then
propose an information-controlled learning algorithm to train flow scores
toward diverse explanation targets: necessary or sufficient explanations.
Experimental studies on both synthetic and real-world datasets demonstrate that
our proposed FlowX and its variants lead to improved explainability of GNNs.
The code is available at https://github.com/divelab/DIG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1&quot;&gt;Shurui Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_Q/0/1/0/all/0/1&quot;&gt;Qicheng Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shuiwang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09894">
<title>Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09894</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing popularity of the federated learning (FL) framework due to its
success in a wide range of collaborative learning tasks also induces certain
security concerns. Among many vulnerabilities, the risk of Byzantine attacks is
of particular concern, which refers to the possibility of malicious clients
participating in the learning process. Hence, a crucial objective in FL is to
neutralize the potential impact of Byzantine attacks and to ensure that the
final model is trustable. It has been observed that the higher the variance
among the clients&apos; models/updates, the more space there is for Byzantine
attacks to be hidden. As a consequence, by utilizing momentum, and thus,
reducing the variance, it is possible to weaken the strength of known Byzantine
attacks. The centered clipping (CC) framework has further shown that the
momentum term from the previous iteration, besides reducing the variance, can
be used as a reference point to neutralize Byzantine attacks better. In this
work, we first expose vulnerabilities of the CC framework, and introduce a
novel attack strategy that can circumvent the defences of CC and other robust
aggregators and reduce their test accuracy up to %33 on best-case scenarios in
image classification tasks. Then, we propose a new robust and fast defence
mechanism that is effective against the proposed and other existing Byzantine
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1&quot;&gt;Kerem Ozfatura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1&quot;&gt;Emre Ozfatura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kupcu_A/0/1/0/all/0/1&quot;&gt;Alptekin Kupcu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz Gunduz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04030">
<title>Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks. (arXiv:2209.04030v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04030</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) provides an efficient paradigm to jointly train a
global model leveraging data from distributed users. As local training data
comes from different users who may not be trustworthy, several studies have
shown that FL is vulnerable to poisoning attacks. Meanwhile, to protect the
privacy of local users, FL is usually trained in a differentially private way
(DPFL). Thus, in this paper, we ask: What are the underlying connections
between differential privacy and certified robustness in FL against poisoning
attacks? Can we leverage the innate privacy property of DPFL to provide
certified robustness for FL? Can we further improve the privacy of FL to
improve such robustness certification? We first investigate both user-level and
instance-level privacy of FL and provide formal privacy analysis to achieve
improved instance-level privacy. We then provide two robustness certification
criteria: certified prediction and certified attack inefficacy for DPFL on both
user and instance levels. Theoretically, we provide the certified robustness of
DPFL based on both criteria given a bounded number of adversarial users or
instances. Empirically, we conduct extensive experiments to verify our theories
under a range of poisoning attacks on different datasets. We find that
increasing the level of privacy protection in DPFL results in stronger
certified attack inefficacy; however, it does not necessarily lead to a
stronger certified prediction. Thus, achieving the optimal certified prediction
requires a proper balance between privacy and utility loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chulin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1&quot;&gt;Yunhui Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qinbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourian_A/0/1/0/all/0/1&quot;&gt;Arash Nourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00392">
<title>Physical Computing: A Category Theoretic Perspective on Physical Computation and System Compositionality. (arXiv:2210.00392v4 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00392</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a category theory-based framework to redefine physical
computing in light of advancements in quantum computing and non-standard
computing systems. By integrating classical definitions within this broader
perspective, the paper rigorously recontextualizes what constitutes physical
computing devices and processes. It demonstrates how the compositional nature
and relational structures of physical computing systems can be coherently
formalized using category theory. This approach not only encapsulates recent
formalisms in physical computing but also offers a structured method to explore
the dynamic interactions within these systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dehghani_N/0/1/0/all/0/1&quot;&gt;Nima Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Caterina_G/0/1/0/all/0/1&quot;&gt;Gianluca Caterina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16212">
<title>A Multi-objective Complex Network Pruning Framework Based on Divide-and-conquer and Global Performance Impairment Ranking. (arXiv:2303.16212v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16212</link>
<description rdf:parseType="Literal">&lt;p&gt;Model compression plays a vital role in the practical deployment of deep
neural networks (DNNs), and evolutionary multi-objective (EMO) pruning is an
essential tool in balancing the compression rate and performance of the DNNs.
However, due to its population-based nature, EMO pruning suffers from the
complex optimization space and the resource-intensive structure verification
process, especially in complex networks. To this end, a multi-objective complex
network pruning framework based on divide-and-conquer and global performance
impairment ranking (EMO-DIR) is proposed in this paper. Firstly, a
divide-and-conquer EMO network pruning method is proposed, which decomposes the
complex task of EMO pruning on the entire network into easier sub-tasks on
multiple sub-networks. On the one hand, this decomposition narrows the pruning
optimization space and decreases the optimization difficulty; on the other
hand, the smaller network structure converges faster, so the proposed algorithm
consumes lower computational resources. Secondly, a sub-network training method
based on cross-network constraints is designed, which could bridge independent
EMO pruning sub-tasks, allowing them to collaborate better and improving the
overall performance of the pruned network. Finally, a multiple sub-networks
joint pruning method based on EMO is proposed. This method combines the Pareto
Fronts from EMO pruning results on multiple sub-networks through global
performance impairment ranking to design a joint pruning scheme. The rich
experiments on CIFAR-10/100 and ImageNet-100/1k are conducted. The proposed
algorithm achieves a comparable performance with the state-of-the-art pruning
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_R/0/1/0/all/0/1&quot;&gt;Ronghua Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Songling Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yinan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weitong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1&quot;&gt;Licheng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songhua Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02478">
<title>Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02478</link>
<description rdf:parseType="Literal">&lt;p&gt;English as foreign language_EFL_students&apos; use of text generated from
artificial intelligence_AI_natural language generation_NLG_tools may improve
their writing quality. However, it remains unclear to what extent AI-generated
text in these students&apos; writing might lead to higher-quality writing. We
explored 23 Hong Kong secondary school students&apos; attempts to write stories
comprising their own words and AI-generated text. Human experts scored the
stories for dimensions of content, language and organization. We analyzed the
basic organization and structure and syntactic complexity of the stories&apos;
AI-generated text and performed multiple linear regression and cluster
analyses. The results show the number of human words and the number of
AI-generated words contribute significantly to scores. Besides, students can be
grouped into competent and less competent writers who use more AI-generated
text or less AI-generated text compared to their peers. Comparisons of clusters
reveal some benefit of AI-generated text in improving the quality of both
high-scoring students&apos; and low-scoring students&apos; writing. The findings can
inform pedagogical strategies to use AI-generated text for EFL students&apos;
writing and to address digital divides. This study contributes designs of NLG
tools and writing activities to implement AI-generated text in schools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1&quot;&gt;David James Woo&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susanto_H/0/1/0/all/0/1&quot;&gt;Hengky Susanto&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_C/0/1/0/all/0/1&quot;&gt;Chi Ho Yeung&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Kai Guo&lt;/a&gt; (3), (4) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_A/0/1/0/all/0/1&quot;&gt;April Ka Yeng Fung&lt;/a&gt; ((1) Precious Blood Secondary School, Hong Kong, (2) Department of Science and Environmental Studies, The Education University of Hong Kong, Hong Kong, (3) Faculty of Education, The University of Hong Kong, Hong Kong, and (4) Hoi Ping Chamber of Commerce Secondary School, Hong Kong)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03365">
<title>Decision-Focused Model-based Reinforcement Learning for Reward Transfer. (arXiv:2304.03365v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03365</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-focused (DF) model-based reinforcement learning has recently been
introduced as a powerful algorithm that can focus on learning the MDP dynamics
that are most relevant for obtaining high returns. While this approach
increases the agent&apos;s performance by directly optimizing the reward, it does so
by learning less accurate dynamics from a maximum likelihood perspective. We
demonstrate that when the reward function is defined by preferences over
multiple objectives, the DF model may be sensitive to changes in the objective
preferences.In this work, we develop the robust decision-focused (RDF)
algorithm, which leverages the non-identifiability of DF solutions to learn
models that maximize expected returns while simultaneously learning models that
transfer to changes in the preference over multiple objectives. We demonstrate
the effectiveness of RDF on two synthetic domains and two healthcare
simulators, showing that it significantly improves the robustness of DF model
learning to changes in the reward function without compromising training-time
return.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Abhishek Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parbhoo_S/0/1/0/all/0/1&quot;&gt;Sonali Parbhoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottesman_O/0/1/0/all/0/1&quot;&gt;Omer Gottesman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05673">
<title>Precise localization of corneal reflections in eye images using deep learning trained on synthetic data. (arXiv:2304.05673v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05673</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a deep learning method for accurately localizing the center of a
single corneal reflection (CR) in an eye image. Unlike previous approaches, we
use a convolutional neural network (CNN) that was trained solely using
simulated data. Using only simulated data has the benefit of completely
sidestepping the time-consuming process of manual annotation that is required
for supervised training on real eye images. To systematically evaluate the
accuracy of our method, we first tested it on images with simulated CRs placed
on different backgrounds and embedded in varying levels of noise. Second, we
tested the method on high-quality videos captured from real eyes. Our method
outperformed state-of-the-art algorithmic methods on real eye images with a 35%
reduction in terms of spatial precision, and performed on par with
state-of-the-art on simulated images in terms of spatial accuracy.We conclude
that our method provides a precise method for CR center localization and
provides a solution to the data availability problem which is one of the
important common roadblocks in the development of deep learning models for gaze
estimation. Due to the superior CR center localization and ease of application,
our method has the potential to improve the accuracy and precision of CR-based
eye trackers
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_S/0/1/0/all/0/1&quot;&gt;Sean Anthony Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nystrom_M/0/1/0/all/0/1&quot;&gt;Marcus Nystr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maquiling_V/0/1/0/all/0/1&quot;&gt;Virmarie Maquiling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehorster_D/0/1/0/all/0/1&quot;&gt;Diederick C. Niehorster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08842">
<title>UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark Suite. (arXiv:2304.08842v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08842</link>
<description rdf:parseType="Literal">&lt;p&gt;In the nascent domain of urban digital twins (UDT), the prospects for
leveraging cutting-edge deep learning techniques are vast and compelling.
Particularly within the specialized area of intelligent road inspection (IRI),
a noticeable gap exists, underscored by the current dearth of dedicated
research efforts and the lack of large-scale well-annotated datasets. To foster
advancements in this burgeoning field, we have launched an online open-source
benchmark suite, referred to as UDTIRI. Along with this article, we introduce
the road pothole detection task, the first online competition published within
this benchmark suite. This task provides a well-annotated dataset, comprising
1,000 RGB images and their pixel/instance-level ground-truth annotations,
captured in diverse real-world scenarios under different illumination and
weather conditions. Our benchmark provides a systematic and thorough evaluation
of state-of-the-art object detection, semantic segmentation, and instance
segmentation networks, developed based on either convolutional neural networks
or Transformers. We anticipate that our benchmark will serve as a catalyst for
the integration of advanced UDT techniques into IRI. By providing algorithms
with a more comprehensive understanding of diverse road conditions, we seek to
unlock their untapped potential and foster innovation in this critical domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Sicen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dacheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Denghuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shuai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xingyi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qijun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rui Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00354">
<title>Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00354</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based generative models have achieved remarkable success in various
domains. It trains a shared model on denoising tasks that encompass different
noise levels simultaneously, representing a form of multi-task learning (MTL).
However, analyzing and improving diffusion models from an MTL perspective
remains under-explored. In particular, MTL can sometimes lead to the well-known
phenomenon of negative transfer, which results in the performance degradation
of certain tasks due to conflicts between tasks. In this paper, we first aim to
analyze diffusion training from an MTL standpoint, presenting two key
observations: (O1) the task affinity between denoising tasks diminishes as the
gap between noise levels widens, and (O2) negative transfer can arise even in
diffusion training. Building upon these observations, we aim to enhance
diffusion training by mitigating negative transfer. To achieve this, we propose
leveraging existing MTL methods, but the presence of a huge number of denoising
tasks makes this computationally expensive to calculate the necessary per-task
loss or gradient. To address this challenge, we propose clustering the
denoising tasks into small task clusters and applying MTL methods to them.
Specifically, based on (O2), we employ interval clustering to enforce temporal
proximity among denoising tasks within clusters. We show that interval
clustering can be solved using dynamic programming, utilizing signal-to-noise
ratio, timestep, and task affinity for clustering objectives. Through this, our
approach addresses the issue of negative transfer in diffusion models by
allowing for efficient computation of MTL methods. We validate the efficacy of
proposed clustering and its integration with MTL methods through various
experiments, demonstrating 1) improved generation quality and 2) faster
training convergence of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1&quot;&gt;Hyojun Go&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;JinYoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yunsung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seunghyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Shinhyeok Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1&quot;&gt;Hyeongdon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seungtaek Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05836">
<title>Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05836</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference is one of the hallmarks of human intelligence. While the
field of CausalNLP has attracted much interest in the recent years, existing
causal inference datasets in NLP primarily rely on discovering causality from
empirical knowledge (e.g., commonsense knowledge). In this work, we propose the
first benchmark dataset to test the pure causal inference skills of large
language models (LLMs). Specifically, we formulate a novel task Corr2Cause,
which takes a set of correlational statements and determines the causal
relationship between the variables. We curate a large-scale dataset of more
than 200K samples, on which we evaluate seventeen existing LLMs. Through our
experiments, we identify a key shortcoming of LLMs in terms of their causal
inference skills, and show that these models achieve almost close to random
performance on the task. This shortcoming is somewhat mitigated when we try to
re-purpose LLMs for this skill via finetuning, but we find that these models
still fail to generalize -- they can only perform causal inference in
in-distribution settings when variable names and textual expressions used in
the queries are similar to those in the training set, but fail in
out-of-distribution settings generated by perturbing these queries. Corr2Cause
is a challenging task for LLMs, and would be helpful in guiding future research
on improving LLMs&apos; pure reasoning skills and generalizability. Our data is at
https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at
https://github.com/causalNLP/corr2cause.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhijing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiarui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poff_S/0/1/0/all/0/1&quot;&gt;Spencer Poff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1&quot;&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1&quot;&gt;Mona Diab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15546">
<title>When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15546</link>
<description rdf:parseType="Literal">&lt;p&gt;The intersection of the Foundation Model (FM) and Federated Learning (FL)
provides mutual benefits, presents a unique opportunity to unlock new
possibilities in AI research, and address critical challenges in AI and
real-world applications. FL expands the availability of data for FMs and
enables computation sharing, distributing the training process and reducing the
burden on FL participants. It promotes collaborative FM development,
democratizing the process and fostering inclusivity and innovation. On the
other hand, FM, with its enormous size, pre-trained knowledge, and exceptional
performance, serves as a robust starting point for FL, facilitating faster
convergence and better performance under non-iid data. Additionally, leveraging
FM to generate synthetic data enriches data diversity, reduces overfitting, and
preserves privacy. By examining the interplay between FL and FM, this paper
aims to deepen the understanding of their synergistic relationship,
highlighting the motivations, challenges, and future directions. Through an
exploration of the challenges faced by FL and FM individually and their
interconnections, we aim to inspire future research directions that can further
enhance both fields, driving advancements and propelling the development of
privacy-preserving and scalable AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07931">
<title>Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07931</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised and language-supervised image models contain rich knowledge
of the world that is important for generalization. Many robotic tasks, however,
require a detailed understanding of 3D geometry, which is often lacking in 2D
image features. This work bridges this 2D-to-3D gap for robotic manipulation by
leveraging distilled feature fields to combine accurate 3D geometry with rich
semantics from 2D foundation models. We present a few-shot learning method for
6-DOF grasping and placing that harnesses these strong spatial and semantic
priors to achieve in-the-wild generalization to unseen objects. Using features
distilled from a vision-language model, CLIP, we present a way to designate
novel objects for manipulation via free-text natural language, and demonstrate
its ability to generalize to unseen expressions and novel categories of
objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;William Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Ge Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1&quot;&gt;Alan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1&quot;&gt;Jansen Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14550">
<title>ReMAV: Reward Modeling of Autonomous Vehicles for Finding Likely Failure Events. (arXiv:2308.14550v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14550</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles are advanced driving systems that are well known to be
vulnerable to various adversarial attacks, compromising vehicle safety and
posing a risk to other road users. Rather than actively training complex
adversaries by interacting with the environment, there is a need to first
intelligently find and reduce the search space to only those states where
autonomous vehicles are found to be less confident. In this paper, we propose a
black-box testing framework ReMAV that uses offline trajectories first to
analyze the existing behavior of autonomous vehicles and determine appropriate
thresholds to find the probability of failure events. To this end, we introduce
a three-step methodology which i) uses offline state action pairs of any
autonomous vehicle under test, ii) builds an abstract behavior representation
using our designed reward modeling technique to analyze states with uncertain
driving decisions, and iii) uses a disturbance model for minimal perturbation
attacks where the driving decisions are less confident. Our reward modeling
technique helps in creating a behavior representation that allows us to
highlight regions of likely uncertain behavior even when the standard
autonomous vehicle performs well. We perform our experiments in a high-fidelity
urban driving environment using three different driving scenarios containing
single- and multi-agent interactions. Our experiment shows an increase in 35,
23, 48, and 50% in the occurrences of vehicle collision, road object collision,
pedestrian collision, and offroad steering events, respectively by the
autonomous vehicle under test, demonstrating a significant increase in failure
events. We compare ReMAV with two baselines and show that ReMAV demonstrates
significantly better effectiveness in generating failure events compared to the
baselines in all evaluation metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharif_A/0/1/0/all/0/1&quot;&gt;Aizaz Sharif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marijan_D/0/1/0/all/0/1&quot;&gt;Dusica Marijan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15821">
<title>Federated Two Stage Decoupling With Adaptive Personalization Layers. (arXiv:2308.15821v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15821</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning has gained significant attention due to its groundbreaking
ability to enable distributed learning while maintaining privacy constraints.
However, as a consequence of data heterogeneity among decentralized devices, it
inherently experiences significant learning degradation and slow convergence
speed. Therefore, it is natural to employ the concept of clustering homogeneous
clients into the same group, allowing only the model weights within each group
to be aggregated. While most existing clustered federated learning methods
employ either model gradients or inference outputs as metrics for client
partitioning, with the goal of grouping similar devices together, may still
have heterogeneity within each cluster. Moreover, there is a scarcity of
research exploring the underlying reasons for determining the appropriate
timing for clustering, resulting in the common practice of assigning each
client to its own individual cluster, particularly in the context of highly non
independent and identically distributed (Non-IID) data. In this paper, we
introduce a two-stage decoupling federated learning algorithm with adaptive
personalization layers named FedTSDP, where client clustering is performed
twice according to inference outputs and model weights, respectively. Hopkins
amended sampling is adopted to determine the appropriate timing for clustering
and the sampling weight of public unlabeled data. In addition, a simple yet
effective approach is developed to adaptively adjust the personalization layers
based on varying degrees of data skew. Experimental results show that our
proposed method has reliable performance on both IID and non-IID scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hangyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhenping Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02332">
<title>Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations. (arXiv:2309.02332v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02332</link>
<description rdf:parseType="Literal">&lt;p&gt;In the intricate architecture of the mammalian central nervous system,
neurons form populations. Axonal bundles communicate between these clusters
using spike trains. However, these neuron populations&apos; precise encoding and
operations have yet to be discovered. In our analysis, the starting point is a
state-of-the-art mechanistic model of a generic neuron endowed with plasticity.
From this simple framework emerges a subtle mathematical construct: The
representation and manipulation of information can be precisely characterized
by an algebra of convex cones. Furthermore, these neuron populations are not
merely passive transmitters. They act as operators within this algebraic
structure, mirroring the functionality of a low-level programming language.
When these populations interconnect, they embody succinct yet potent algebraic
expressions. These networks allow them to implement many operations, such as
specialization, generalization, novelty detection, dimensionality reduction,
inverse modeling, prediction, and associative memory. In broader terms, this
work illuminates the potential of matrix embeddings in advancing our
understanding in fields like cognitive science and AI. These embeddings enhance
the capacity for concept processing and hierarchical description over their
vector counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nilsson_M/0/1/0/all/0/1&quot;&gt;Martin N. P. Nilsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08883">
<title>Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees. (arXiv:2309.08883v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08883</link>
<description rdf:parseType="Literal">&lt;p&gt;Satisfiability Modulo Counting (SMC) encompasses problems that require both
symbolic decision-making and statistical reasoning. Its general formulation
captures many real-world problems at the intersection of symbolic and
statistical Artificial Intelligence. SMC searches for policy interventions to
control probabilistic outcomes. Solving SMC is challenging because of its
highly intractable nature($\text{NP}^{\text{PP}}$-complete), incorporating
statistical inference and symbolic reasoning. Previous research on SMC solving
lacks provable guarantees and/or suffers from sub-optimal empirical
performance, especially when combinatorial constraints are present. We propose
XOR-SMC, a polynomial algorithm with access to NP-oracles, to solve highly
intractable SMC problems with constant approximation guarantees. XOR-SMC
transforms the highly intractable SMC into satisfiability problems, by
replacing the model counting in SMC with SAT formulae subject to randomized XOR
constraints. Experiments on solving important SMC problems in AI for social
good demonstrate that XOR-SMC finds solutions close to the true optimum,
outperforming several baselines which struggle to find good approximations for
the intractable model counting in SMC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinzhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yexiang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14181">
<title>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14181</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess preliminary low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs. Project Page:
https://q-future.github.io/Q-Bench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Erli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1&quot;&gt;Liang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Annan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenxiu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qiong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weisi Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03940">
<title>Hard View Selection for Self-Supervised Learning. (arXiv:2310.03940v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03940</link>
<description rdf:parseType="Literal">&lt;p&gt;Many Self-Supervised Learning (SSL) methods train their models to be
invariant to different &quot;views&quot; of an image input for which a good data
augmentation pipeline is crucial. While considerable efforts were directed
towards improving pre-text tasks, architectures, or robustness (e.g., Siamese
networks or teacher-softmax centering), the majority of these methods remain
strongly reliant on the random sampling of operations within the image
augmentation pipeline, such as the random resized crop or color distortion
operation. In this paper, we argue that the role of the view generation and its
effect on performance has so far received insufficient attention. To address
this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS)
strategy designed to extend the random view generation to expose the pretrained
model to harder samples during SSL training. It encompasses the following
iterative steps: 1) randomly sample multiple views and create pairs of two
views, 2) run forward passes for each view pair on the currently trained model,
3) adversarially select the pair yielding the worst loss, and 4) run the
backward pass with the selected pair. In our empirical analysis we show that
under the hood, HVS increases task difficulty by controlling the Intersection
over Union of views during pretraining. With only 300-epoch pretraining, HVS is
able to closely rival the 800-epoch DINO baseline which remains very favorable
even when factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.4% and 1.9% on linear evaluation and similar improvements on transfer
tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1&quot;&gt;Ivo Rapant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09130">
<title>Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09130</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) shows powerful capability in natural language
understanding by capturing hidden semantics in vector space. This process
enriches the value of the text embeddings for various downstream tasks, thereby
fostering the Embedding-as-a-Service (EaaS) business model. However, the direct
transmission of text to servers poses a largely unaddressed risk of privacy
leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an
innovative framework that split the model to execute the token embedding layer
on the client side at minimal computational cost. This allows the client to
introduce noise prior to transmitting the embeddings to the server, and
subsequently receive and denoise the perturbed output embeddings for downstream
tasks. Our approach is designed for the inference stage of LLMs and requires no
modifications to the model parameters. Extensive experiments demonstrate SnD&apos;s
effectiveness in optimizing the privacy-utility tradeoff across various LLM
architectures and diverse downstream tasks. The results reveal a significant
performance improvement under the same privacy budget compared to the baseline,
offering clients a privacy-preserving solution for local privacy protection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_P/0/1/0/all/0/1&quot;&gt;Peihua Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Ran Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Youjia Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yan Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09877">
<title>Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE). (arXiv:2310.09877v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09877</link>
<description rdf:parseType="Literal">&lt;p&gt;Accumulated Local Effects (ALE) is a model-agnostic approach for global
explanations of the results of black-box machine learning (ML) algorithms.
There are at least three challenges with conducting statistical inference based
on ALE: ensuring the reliability of ALE analyses, especially in the context of
small datasets; intuitively characterizing a variable&apos;s overall effect in ML;
and making robust inferences from ML data analysis. In response, we introduce
innovative tools and techniques for statistical inference using ALE,
establishing bootstrapped confidence intervals tailored to dataset size and
introducing ALE effect size measures that intuitively indicate effects on both
the outcome variable scale and a normalized scale. Furthermore, we demonstrate
how to use these tools to draw reliable statistical inferences, reflecting the
flexible patterns ALE adeptly highlights, with implementations available in the
&apos;ale&apos; package in R. This work propels the discourse on ALE and its
applicability in ML and statistical analysis forward, offering practical
solutions to prevailing challenges in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okoli_C/0/1/0/all/0/1&quot;&gt;Chitu Okoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20199">
<title>In Search of Lost Online Test-time Adaptation: A Survey. (arXiv:2310.20199v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20199</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a comprehensive survey on online test-time
adaptation (OTTA), a paradigm focused on adapting machine learning models to
novel data distributions upon batch arrival. Despite the proliferation of OTTA
methods recently, the field is mired in issues like ambiguous settings,
antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the
real challenges and making reproducibility elusive. For clarity and a rigorous
comparison, we classify OTTA techniques into three primary categories and
subject them to benchmarks using the potent Vision Transformer (ViT) backbone
to discover genuinely effective strategies. Our benchmarks span not only
conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also
real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating
variations across search engines and synthesized data by diffusion models. To
gauge efficiency in online scenarios, we introduce novel evaluation metrics,
inclusive of FLOPs, shedding light on the trade-offs between adaptation
accuracy and computational overhead. Our findings diverge from existing
literature, indicating: (1) transformers exhibit heightened resilience to
diverse domain shifts, (2) the efficacy of many OTTA methods hinges on ample
batch sizes, and (3) stability in optimization and resistance to perturbations
are critical during adaptation, especially when the batch size is 1. Motivated
by these insights, we pointed out promising directions for future research. The
source code is made available: https://github.com/Jo-wang/OTTA_ViT_survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Liang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01004">
<title>Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning. (arXiv:2311.01004v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01004</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of multimodality and large language models, the deep
learning-based technique for medical image captioning holds the potential to
offer valuable diagnostic recommendations. However, current generic text and
image pre-trained models do not yield satisfactory results when it comes to
describing intricate details within medical images. In this paper, we present a
novel medical image captioning method guided by the segment anything model
(SAM) to enable enhanced encoding with both general and detailed feature
extraction. In addition, our approach employs a distinctive pre-training
strategy with mixed semantic learning to simultaneously capture both the
overall information and finer details within medical images. We demonstrate the
effectiveness of this approach, as it outperforms the pre-trained BLIP2 model
on various evaluation metrics for generating descriptions of medical images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benlu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weijie Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xuechen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gaoang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03356">
<title>GLaMM: Pixel Grounding Large Multimodal Model. (arXiv:2311.03356v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03356</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Multimodal Models (LMMs) extend Large Language Models to the vision
domain. Initial LMMs used holistic images and text prompts to generate
ungrounded textual responses. Recently, region-level LMMs have been used to
generate visually grounded responses. However, they are limited to only
referring to a single object category at a time, require users to specify the
regions, or cannot offer dense pixel-wise object grounding. In this work, we
present Grounding LMM (GLaMM), the first model that can generate natural
language responses seamlessly intertwined with corresponding object
segmentation masks. GLaMM not only grounds objects appearing in the
conversations but is flexible enough to accept both textual and optional visual
prompts (region of interest) as input. This empowers users to interact with the
model at various levels of granularity, both in textual and visual domains. Due
to the lack of standard benchmarks for the novel setting of visually Grounded
Conversation Generation (GCG), we introduce a comprehensive evaluation protocol
with our curated grounded conversations. Our proposed GCG task requires densely
grounded concepts in natural scenes at a large-scale. To this end, we propose a
densely annotated Grounding-anything Dataset (GranD) using our proposed
automated annotation pipeline that encompasses 7.5M unique concepts grounded in
a total of 810M regions available with segmentation masks. Besides GCG, GLaMM
also performs effectively on several downstream tasks, e.g., referring
expression segmentation, image and region-level captioning and vision-language
conversations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1&quot;&gt;Hanoona Rasheed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Maaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullappilly_S/0/1/0/all/0/1&quot;&gt;Sahal Shaji Mullappilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Shaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao M. Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Erix Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad S. Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03380">
<title>An attempt to generate new bridge types from latent space of variational autoencoder. (arXiv:2311.03380v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04014">
<title>A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems. (arXiv:2311.04014v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04014</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel operator, termed the Y operator, to elevate
control performance in Actor-Critic(AC) based reinforcement learning for
systems governed by stochastic differential equations(SDEs). The Y operator
ingeniously integrates the stochasticity of a class of child-mother system into
the Critic network&apos;s loss function, yielding substantial advancements in the
control performance of RL algorithms.Additionally, the Y operator elegantly
reformulates the challenge of solving partial differential equations for the
state-value function into a parallel problem for the drift and diffusion
functions within the system&apos;s SDEs.A rigorous mathematical proof confirms the
operator&apos;s validity.This transformation enables the Y Operator-based
Reinforcement Learning(YORL) framework to efficiently tackle optimal control
problems in both model-based and data-driven systems.The superiority of YORL is
demonstrated through linear and nonlinear numerical examples showing its
enhanced performance over existing methods post convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Cheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04412">
<title>Human Conditional Reasoning in Answer Set Programming. (arXiv:2311.04412v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04412</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a conditional sentence &quot;P=&amp;gt;Q&quot; (if P then Q) and respective facts, four
different types of inferences are observed in human reasoning. Affirming the
antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent
(AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and
denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them,
AA and DC are logically valid, while AC and DA are logically invalid and often
called logical fallacies. Nevertheless, humans often perform AC or DA as
pragmatic inference in daily life. In this paper, we realize AC, DA and DC
inferences in answer set programming. Eight different types of completion are
introduced and their semantics are given by answer sets. We investigate formal
properties and characterize human reasoning tasks in cognitive psychology.
Those completions are also applied to commonsense reasoning in AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakama_C/0/1/0/all/0/1&quot;&gt;Chiaki Sakama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15487">
<title>Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning. (arXiv:2311.15487v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15487</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the gradient descent flow widely used for the minimization of the
$\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two
modified versions; one adapted for the overparametrized setting, and the other
for the underparametrized setting. Both have a clear and natural invariant
geometric meaning, taking into account the pullback vector bundle structure in
the overparametrized, and the pushforward vector bundle structure in the
underparametrized setting. In the overparametrized case, we prove that,
provided that a rank condition holds, all orbits of the modified gradient
descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform
exponential convergence rate; one thereby obtains an a priori stopping time for
any prescribed proximity to the global minimum. We point out relations of the
latter to sub-Riemannian geometry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Thomas Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16754">
<title>Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird&apos;s Eye View Segmentation for Connected and Autonomous Driving. (arXiv:2311.16754v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16754</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative perception has recently gained significant attention in
autonomous driving, improving perception quality by enabling the exchange of
additional information among vehicles. However, deploying collaborative
perception systems can lead to domain shifts due to diverse environmental
conditions and data heterogeneity among connected and autonomous vehicles
(CAVs). To address these challenges, we propose a unified domain generalization
framework applicable in both training and inference stages of collaborative
perception. In the training phase, we introduce an Amplitude Augmentation
(AmpAug) method to augment low-frequency image variations, broadening the
model&apos;s ability to learn across various domains. We also employ a
meta-consistency training scheme to simulate domain shifts, optimizing the
model with a carefully designed consistency loss to encourage domain-invariant
representations. In the inference phase, we introduce an intra-system domain
alignment mechanism to reduce or potentially eliminate the domain discrepancy
among CAVs prior to inference. Comprehensive experiments substantiate the
effectiveness of our method in comparison with the existing state-of-the-art
works. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Senkang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhengru Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xianhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuguang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1&quot;&gt;Sam Kwong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00209">
<title>On the Interplay Between Stepsize Tuning and Progressive Sharpening. (arXiv:2312.00209v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent empirical work has revealed an intriguing property of deep learning
models by which the sharpness (largest eigenvalue of the Hessian) increases
throughout optimization until it stabilizes around a critical value at which
the optimizer operates at the edge of stability, given a fixed stepsize (Cohen
et al, 2022). We investigate empirically how the sharpness evolves when using
stepsize-tuners, the Armijo linesearch and Polyak stepsizes, that adapt the
stepsize along the iterations to local quantities such as, implicitly, the
sharpness itself. We find that the surprisingly poor performance of a classical
Armijo linesearch in the deterministic setting may be well explained by its
tendency to ever-increase the sharpness of the objective. On the other hand, we
observe that Polyak stepsizes operate generally at the edge of stability or
even slightly beyond, outperforming its Armijo and constant stepsizes
counterparts in the deterministic setting. We conclude with an analysis that
suggests unlocking stepsize tuners requires an understanding of the joint
dynamics of the step size and the sharpness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roulet_V/0/1/0/all/0/1&quot;&gt;Vincent Roulet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwala_A/0/1/0/all/0/1&quot;&gt;Atish Agarwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1&quot;&gt;Fabian Pedregosa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00798">
<title>A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?. (arXiv:2312.00798v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00798</link>
<description rdf:parseType="Literal">&lt;p&gt;We administer a Turing Test to AI Chatbots. We examine how Chatbots behave in
a suite of classic behavioral games that are designed to elicit characteristics
such as trust, fairness, risk-aversion, cooperation, \textit{etc.}, as well as
how they respond to a traditional Big-5 psychological survey that measures
personality traits. ChatGPT-4 exhibits behavioral and personality traits that
are statistically indistinguishable from a random human from tens of thousands
of human subjects from more than 50 countries. Chatbots also modify their
behavior based on previous experience and contexts ``as if&apos;&apos; they were learning
from the interactions, and change their behavior in response to different
framings of the same strategic situation. Their behaviors are often distinct
from average and modal human behaviors, in which case they tend to behave on
the more altruistic and cooperative end of the distribution. We estimate that
they act as if they are maximizing an average of their own and partner&apos;s
payoffs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1&quot;&gt;Qiaozhu Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yutong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Walter Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jackson_M/0/1/0/all/0/1&quot;&gt;Matthew O. Jackson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04501">
<title>Graph Metanetworks for Processing Diverse Neural Architectures. (arXiv:2312.04501v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04501</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks efficiently encode learned information within their
parameters. Consequently, many tasks can be unified by treating neural networks
themselves as input data. When doing so, recent studies demonstrated the
importance of accounting for the symmetries and geometry of parameter spaces.
However, those works developed architectures tailored to specific networks such
as MLPs and CNNs without normalization layers, and generalizing such
architectures to other types of networks can be challenging. In this work, we
overcome these challenges by building new metanetworks - neural networks that
take weights from other neural networks as input. Put simply, we carefully
build graphs representing the input neural networks and process the graphs
using graph neural networks. Our approach, Graph Metanetworks (GMNs),
generalizes to neural architectures where competing methods struggle, such as
multi-head attention layers, normalization layers, convolutional layers, ResNet
blocks, and group-equivariant linear layers. We prove that GMNs are expressive
and equivariant to parameter permutation symmetries that leave the input neural
network functions unchanged. We validate the effectiveness of our method on
several metanetwork tasks over diverse neural network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1&quot;&gt;Derek Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1&quot;&gt;Haggai Maron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1&quot;&gt;Marc T. Law&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorraine_J/0/1/0/all/0/1&quot;&gt;Jonathan Lorraine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1&quot;&gt;James Lucas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07586">
<title>Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07586</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a sampling method that provides first-principle
non-linear correction for classifier-free guided DDPMs. Such correction forces
the guided DDPMs to respect the Fokker-Planck equation of their underlying
diffusion process, in a way that is training-free, derivative-free, and
compatible with existing sampling methods. Experiments show that characteristic
guidance enhances control and reduces color and exposure issues in image
generation, proving effective in diverse applications ranging from latent space
sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Candi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yuan Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08793">
<title>Forbidden Facts: An Investigation of Competing Objectives in Llama-2. (arXiv:2312.08793v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08793</link>
<description rdf:parseType="Literal">&lt;p&gt;LLMs often face competing pressures (for example helpfulness vs.
harmlessness). To understand how models resolve such conflicts, we study
Llama-2-chat models on the forbidden fact task. Specifically, we instruct
Llama-2 to truthfully complete a factual recall statement while forbidding it
from saying the correct answer. This often makes the model give incorrect
answers. We decompose Llama-2 into 1000+ components, and rank each one with
respect to how useful it is for forbidding the correct answer. We find that in
aggregate, around 35 components are enough to reliably implement the full
suppression behavior. However, these components are fairly heterogeneous and
many operate using faulty heuristics. We discover that one of these heuristics
can be exploited via a manually designed adversarial attack which we call The
California Attack. Our results highlight some roadblocks standing in the way of
being able to successfully interpret advanced ML systems. Project website
available at https://forbiddenfacts.github.io .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tony T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miles Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_K/0/1/0/all/0/1&quot;&gt;Kaivalya Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1&quot;&gt;Nir Shavit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10402">
<title>Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion. (arXiv:2312.10402v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10402</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Music Transcription (AMT) is a vital technology in the field of
music information processing. Despite recent enhancements in performance due to
machine learning techniques, current methods typically attain high accuracy in
domains where abundant annotated data is available. Addressing domains with low
or no resources continues to be an unresolved challenge. To tackle this issue,
we propose a transcription model that does not require any MIDI-audio paired
data through the utilization of scalable synthetic audio for pre-training and
adversarial domain confusion using unannotated real audio. In experiments, we
evaluate methods under the real-world application scenario where training
datasets do not include the MIDI annotation of audio in the target data domain.
Our proposed method achieved competitive performance relative to established
baseline methods, despite not utilizing any real datasets of paired MIDI-audio.
Additionally, ablation studies have provided insights into the scalability of
this approach and the forthcoming challenges in the field of AMT research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_G/0/1/0/all/0/1&quot;&gt;Gakusei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akama_T/0/1/0/all/0/1&quot;&gt;Taketo Akama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10472">
<title>Analyzing Generalization in Policy Networks: A Case Study with the Double-Integrator System. (arXiv:2312.10472v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10472</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive utilization of deep reinforcement learning (DRL) policy networks in
diverse continuous control tasks has raised questions regarding performance
degradation in expansive state spaces where the input state norm is larger than
that in the training environment. This paper aims to uncover the underlying
factors contributing to such performance deterioration when dealing with
expanded state spaces, using a novel analysis technique known as state
division. In contrast to prior approaches that employ state division merely as
a post-hoc explanatory tool, our methodology delves into the intrinsic
characteristics of DRL policy networks. Specifically, we demonstrate that the
expansion of state space induces the activation function $\tanh$ to exhibit
saturability, resulting in the transformation of the state division boundary
from nonlinear to linear. Our analysis centers on the paradigm of the
double-integrator system, revealing that this gradual shift towards linearity
imparts a control behavior reminiscent of bang-bang control. However, the
inherent linearity of the division boundary prevents the attainment of an ideal
bang-bang control, thereby introducing unavoidable overshooting. Our
experimental investigations, employing diverse RL algorithms, establish that
this performance phenomenon stems from inherent attributes of the DRL policy
network, remaining consistent across various optimization algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruining Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Haoran Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_M/0/1/0/all/0/1&quot;&gt;Maolong Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qisong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10661">
<title>Wikiformer: Pre-training with Structured Information of Wikipedia for Ad-hoc Retrieval. (arXiv:2312.10661v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10661</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of deep learning and natural language processing
techniques, pre-trained language models have been widely used to solve
information retrieval (IR) problems. Benefiting from the pre-training and
fine-tuning paradigm, these models achieve state-of-the-art performance. In
previous works, plain texts in Wikipedia have been widely used in the
pre-training stage. However, the rich structured information in Wikipedia, such
as the titles, abstracts, hierarchical heading (multi-level title) structure,
relationship between articles, references, hyperlink structures, and the
writing organizations, has not been fully explored. In this paper, we devise
four pre-training objectives tailored for IR tasks based on the structured
knowledge of Wikipedia. Compared to existing pre-training methods, our approach
can better capture the semantic knowledge in the training corpus by leveraging
the human-edited structured data from Wikipedia. Experimental results on
multiple IR benchmark datasets show the superior performance of our model in
both zero-shot and fine-tuning settings compared to existing strong retrieval
baselines. Besides, experimental results in biomedical and legal domains
demonstrate that our approach achieves better performance in vertical domains
compared to previous models, especially in scenarios where long text similarity
matching is needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weihang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1&quot;&gt;Qingyao Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiqun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1&quot;&gt;Shengluan Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10841">
<title>Online Boosting Adaptive Learning under Concept Drift for Multistream Classification. (arXiv:2312.10841v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10841</link>
<description rdf:parseType="Literal">&lt;p&gt;Multistream classification poses significant challenges due to the necessity
for rapid adaptation in dynamic streaming processes with concept drift. Despite
the growing research outcomes in this area, there has been a notable oversight
regarding the temporal dynamic relationships between these streams, leading to
the issue of negative transfer arising from irrelevant data. In this paper, we
propose a novel Online Boosting Adaptive Learning (OBAL) method that
effectively addresses this limitation by adaptively learning the dynamic
correlation among different streams. Specifically, OBAL operates in a
dual-phase mechanism, in the first of which we design an Adaptive COvariate
Shift Adaptation (AdaCOSA) algorithm to construct an initialized ensemble model
using archived data from various source streams, thus mitigating the covariate
shift while learning the dynamic correlations via an adaptive re-weighting
strategy. During the online process, we employ a Gaussian Mixture Model-based
weighting mechanism, which is seamlessly integrated with the acquired
correlations via AdaCOSA to effectively handle asynchronous drift. This
approach significantly improves the predictive performance and stability of the
target stream. We conduct comprehensive experiments on several synthetic and
real-world data streams, encompassing various drifting scenarios and types. The
results clearly demonstrate that OBAL achieves remarkable advancements in
addressing multistream classification problems by effectively leveraging
positive knowledge derived from multiple sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1&quot;&gt;En Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guangquan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11753">
<title>Poker Hand History File Format Specification. (arXiv:2312.11753v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11753</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the Poker Hand History (PHH) file format, designed to
standardize the recording of poker hands across different game variants.
Despite poker&apos;s widespread popularity in the mainstream culture as a mind sport
and its prominence in the field of artificial intelligence (AI) research as a
benchmark for imperfect information AI agents, it lacks a consistent format
that humans can use to document poker hands across different variants that can
also easily be parsed by machines. To address this gap in the literature, we
propose the PHH format which provides a concise human-readable machine-friendly
representation of hand history that comprehensively captures various details of
the hand, ranging from initial game parameters and actions to contextual
parameters including but not limited to the venue, players, and time control
information. In the supplementary, we provide over 10,000 hands covering 11
different variants in the PHH format. Building on our previous work on
PokerKit, a premier poker hand simulation tool, we demonstrate the usages of
our open-source Python implementation of the PHH parser. The source code of the
parser is available on GitHub: https://github.com/uoftcprg/pokerkit
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juho Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16127">
<title>Large Language Model Situational Awareness Based Planning. (arXiv:2312.16127v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16127</link>
<description rdf:parseType="Literal">&lt;p&gt;This work pioneers evaluating emergent planning capabilities based on
situational awareness in large language models. We contribute (i) novel
benchmarks and metrics for standardized assessment; (ii) a unique dataset to
spur progress; and (iii) demonstrations that prompting and multi-agent schemes
significantly enhance planning performance in context-sensitive planning tasks.
Positioning this within a situated agent and automated planning research, we
highlight inherent reliability challenges--efficiently mapping world states to
actions without environmental guidance remains open despite simulated domain
advances. Although out-of-scope, limitations around validation methodology and
data availability indicate exciting directions, including fine-tuning on
expanded planning corpora and optimizations for triggering fast latent
planning. By conclusively demonstrating current methods&apos; promise and
limitations via rigorous comparison, we catalyze investigating reliable
goal-directed reasoning for situated agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liman Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hanyang Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16256">
<title>DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision. (arXiv:2312.16256v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16256</link>
<description rdf:parseType="Literal">&lt;p&gt;We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_L/0/1/0/all/0/1&quot;&gt;Lu Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yichen Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhi Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wentian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1&quot;&gt;Cheng Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1&quot;&gt;Kun Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lantao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zixun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yawen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanmao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xingpeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_R/0/1/0/all/0/1&quot;&gt;Rohan Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Aniruddha Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangrui Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1&quot;&gt;Gang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benes_B/0/1/0/all/0/1&quot;&gt;Bedrich Benes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1&quot;&gt;Aniket Bera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16430">
<title>Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16430</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference is deterministic. IPO uses a root-finding MSE loss to solve the
ignoring KL-regularization problem. In this paper, we&apos;ll figure out, although
IPO fix the problem when preference is deterministic, but both DPO and IPO
fails the KL-regularization term because the support of preference distribution
not equal to reference distribution. Then, we design a simple and intuitive
off-policy preference optimization algorithm from an importance sampling view,
which we call Maximum Preference Optimization (MPO), and add off-policy
KL-regularization terms which makes KL-regularization truly effective. The
objective of MPO bears resemblance to RLHF&apos;s objective, and likes IPO, MPO is
off-policy. So, MPO attains the best of both worlds. To simplify the learning
process and save memory usage, MPO eliminates the needs for both reward model
and reference policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zaifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16477">
<title>Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding. (arXiv:2312.16477v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16477</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the results of view-based 3D shape recognition methods have
saturated, and models with excellent performance cannot be deployed on
memory-limited devices due to their huge size of parameters. To address this
problem, we introduce a compression method based on knowledge distillation for
this field, which largely reduces the number of parameters while preserving
model performance as much as possible. Specifically, to enhance the
capabilities of smaller models, we design a high-performing large model called
Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first
establishes relationships between view-level features. Additionally, to capture
deeper features, we employ the grouping module to enhance view-level features
into group-level features. Finally, the group-level ViT aggregates group-level
features into complete, well-formed 3D shape descriptors. Notably, in both
ViTs, we introduce spatial encoding of camera coordinates as innovative
position embeddings. Furthermore, we propose two compressed versions based on
GMViT, namely GMViT-simple and GMViT-mini. To enhance the training
effectiveness of the small models, we introduce a knowledge distillation method
throughout the GMViT process, where the key outputs of each GMViT component
serve as distillation targets. Extensive experiments demonstrate the efficacy
of the proposed method. The large model GMViT achieves excellent 3D
classification and retrieval results on the benchmark datasets ModelNet,
ShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,
reduce the parameter size by 8 and 17.6 times, respectively, and improve shape
recognition speed by 1.5 times on average, while preserving at least 90% of the
classification and retrieval performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lixiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1&quot;&gt;Qingzhe Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanyan Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16767">
<title>Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search. (arXiv:2312.16767v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16767</link>
<description rdf:parseType="Literal">&lt;p&gt;Anytime multi-agent path finding (MAPF) is a promising approach to scalable
path optimization in large-scale multi-agent systems. State-of-the-art anytime
MAPF is based on Large Neighborhood Search (LNS), where a fast initial solution
is iteratively optimized by destroying and repairing a fixed number of parts,
i.e., the neighborhood, of the solution, using randomized destroy heuristics
and prioritized planning. Despite their recent success in various MAPF
instances, current LNS-based approaches lack exploration and flexibility due to
greedy optimization with a fixed neighborhood size which can lead to low
quality solutions in general. So far, these limitations have been addressed
with extensive prior effort in tuning or offline machine learning beyond actual
planning. In this paper, we focus on online learning in LNS and propose
Bandit-based Adaptive LArge Neighborhood search Combined with Exploration
(BALANCE). BALANCE uses a bi-level multi-armed bandit scheme to adapt the
selection of destroy heuristics and neighborhood sizes on the fly during
search. We evaluate BALANCE on multiple maps from the MAPF benchmark set and
empirically demonstrate cost improvements of at least 50% compared to
state-of-the-art anytime MAPF in large-scale scenarios. We find that Thompson
Sampling performs particularly well compared to alternative multi-armed bandit
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thomy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Taoan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koenig_S/0/1/0/all/0/1&quot;&gt;Sven Koenig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17122">
<title>Large Language Model for Causal Decision Making. (arXiv:2312.17122v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17122</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown their success in language
understanding and reasoning on general topics. However, their capability to
inference based on user-specified structured data and knowledge in corpus-rare
concepts like causal decision-making is still limited. In this work, we explore
the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can
identify the causal task, execute a corresponding function, and interpret its
numerical results based on users&apos; queries and the provided dataset. Meanwhile,
we propose a data generation process for more controllable GPT prompting and
present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal
problem identification and input parameter extraction for causal function
calling and (2) Causal-Interpret-Bench for in-context causal interpretation.
With three case studies, we showed that LLM4Causal can deliver end-to-end
solutions for causal problems and provide easy-to-understand answers. Numerical
studies also reveal that it has a remarkable ability to identify the correct
causal task given a query.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haitao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1&quot;&gt;Lin Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuhe Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Rui Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17163">
<title>FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17163</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by human driving focus, this research pioneers networks augmented
with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN
architecture and Directional IoU Loss - targeted innovations addressing
obstacles to precise lane detection for autonomous driving. Experiments
demonstrate our Focusing Sampling strategy, emphasizing vital distant details
unlike uniform approaches, significantly boosts both benchmark and practical
curved/distant lane recognition accuracy essential for safety. While FENetV1
achieves state-of-the-art conventional metric performance via enhancements
isolating perspective-aware contexts mimicking driver vision, FENetV2 proves
most reliable on the proposed Partial Field analysis. Hence we specifically
recommend V2 for practical lane navigation despite fractional degradation on
standard entire-image measures. Future directions include collecting on-road
data and integrating complementary dual frameworks to further breakthroughs
guided by human perception principles. Code will be made available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liman Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hanyang Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1406.6046">
<title>Hybrid Epidemics - A Case Study on Computer Worm Conficker. (arXiv:1406.6046v3 [cs.CR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1406.6046</link>
<description rdf:parseType="Literal">&lt;p&gt;Conficker is a computer worm that erupted on the Internet in 2008. It is
unique in combining three different spreading strategies: local probing,
neighbourhood probing, and global probing. We propose a mathematical model that
combines three modes of spreading, local, neighbourhood and global to capture
the worm&apos;s spreading behaviour. The parameters of the model are inferred
directly from network data obtained during the first day of the Conifcker
epidemic. The model is then used to explore the trade-off between spreading
modes in determining the worm&apos;s effectiveness. Our results show that the
Conficker epidemic is an example of a critically hybrid epidemic, in which the
different modes of spreading in isolation do not lead to successful epidemics.
Such hybrid spreading strategies may be used beneficially to provide the most
effective strategies for promulgating information across a large population.
When used maliciously, however, they can present a dangerous challenge to
current internet security protocols.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chain_B/0/1/0/all/0/1&quot;&gt;Benjamin M. Chain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1409.7291">
<title>Optimizing Hybrid Spreading in Metapopulations. (arXiv:1409.7291v3 [physics.soc-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1409.7291</link>
<description rdf:parseType="Literal">&lt;p&gt;Epidemic spreading phenomena are ubiquitous in nature and society. Examples
include the spreading of diseases, information, and computer viruses. Epidemics
can spread by local spreading, where infected nodes can only infect a limited
set of direct target nodes and global spreading, where an infected node can
infect every other node. In reality, many epidemics spread using a hybrid
mixture of both types of spreading. In this study we develop a theoretical
framework for studying hybrid epidemics, and examine the optimum balance
between spreading mechanisms in terms of achieving the maximum outbreak size.
We show the existence of critically hybrid epidemics where neither spreading
mechanism alone can cause a noticeable spread but a combination of the two
spreading mechanisms would produce an enormous outbreak. Our results provide
new strategies for maximising beneficial epidemics and estimating the worst
outcome of damaging hybrid epidemics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Miller_J/0/1/0/all/0/1&quot;&gt;Joel C. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cox_I/0/1/0/all/0/1&quot;&gt;Ingemar J. Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chain_B/0/1/0/all/0/1&quot;&gt;Benjamin M. Chain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1501.01678">
<title>LeoTask: a fast, flexible and reliable framework for computational research. (arXiv:1501.01678v1 [cs.SE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1501.01678</link>
<description rdf:parseType="Literal">&lt;p&gt;LeoTask is a Java library for computation-intensive and time-consuming
research tasks. It automatically executes tasks in parallel on multiple CPU
cores on a computing facility. It uses a configuration file to enable automatic
exploration of parameter space and flexible aggregation of results, and
therefore allows researchers to focus on programming the key logic of a
computing task. It also supports reliable recovery from interruptions, dynamic
and cloneable networks, and integration with the plotting software Gnuplot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chain_B/0/1/0/all/0/1&quot;&gt;Benjamin M. Chain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1503.08992">
<title>Hybrid spreading mechanisms and T cell activation shape the dynamics of HIV-1 infection. (arXiv:1503.08992v1 [q-bio.PE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1503.08992</link>
<description rdf:parseType="Literal">&lt;p&gt;HIV-1 can disseminate between susceptible cells by two mechanisms: cell-free
infection following fluid-phase diffusion of virions and by highly-efficient
direct cell-to-cell transmission at immune cell contacts. The contribution of
this hybrid spreading mechanism, which is also a characteristic of some
important computer worm outbreaks, to HIV-1 progression in vivo remains
unknown. Here we present a new mathematical model that explicitly incorporates
the ability of HIV-1 to use hybrid spreading mechanisms and evaluate the
consequences for HIV-1 pathogenenesis. The model captures the major phases of
the HIV-1 infection course of a cohort of treatment naive patients and also
accurately predicts the results of the Short Pulse Anti-Retroviral Therapy at
Seroconversion (SPARTAC) trial. Using this model we find that hybrid spreading
is critical to seed and establish infection, and that cell-to-cell spread and
increased CD4+ T cell activation are important for HIV-1 progression. Notably,
the model predicts that cell-to-cell spread becomes increasingly effective as
infection progresses and thus may present a considerable treatment barrier.
Deriving predictions of various treatments&apos; influence on HIV-1 progression
highlights the importance of earlier intervention and suggests that treatments
effectively targeting cell-to-cell HIV-1 spread can delay progression to AIDS.
This study suggests that hybrid spreading is a fundamental feature of HIV
infection, and provides the mathematical framework incorporating this feature
with which to evaluate future therapeutic strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Groppelli_E/0/1/0/all/0/1&quot;&gt;Elisabetta Groppelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pellegrino_P/0/1/0/all/0/1&quot;&gt;Pierre Pellegrino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Williams_I/0/1/0/all/0/1&quot;&gt;Ian Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Borrow_P/0/1/0/all/0/1&quot;&gt;Persephone Borrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chain_B/0/1/0/all/0/1&quot;&gt;Benjamin M. Chain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jolly_C/0/1/0/all/0/1&quot;&gt;Clare Jolly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04430">
<title>Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method based on Fast Fourier Convolution and ConvNeXt. (arXiv:2305.04430v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.04430</link>
<description rdf:parseType="Literal">&lt;p&gt;Haze usually leads to deteriorated images with low contrast, color shift and
structural distortion. We observe that many deep learning based models exhibit
exceptional performance on removing homogeneous haze, but they usually fail to
address the challenge of non-homogeneous dehazing. Two main factors account for
this situation. Firstly, due to the intricate and non uniform distribution of
dense haze, the recovery of structural and chromatic features with high
fidelity is challenging, particularly in regions with heavy haze. Secondly, the
existing small scale datasets for non-homogeneous dehazing are inadequate to
support reliable learning of feature mappings between hazy images and their
corresponding haze-free counterparts by convolutional neural network
(CNN)-based models. To tackle these two challenges, we propose a novel two
branch network that leverages 2D discrete wavelete transform (DWT), fast
Fourier convolution (FFC) residual block and a pretrained ConvNeXt model.
Specifically, in the DWT-FFC frequency branch, our model exploits DWT to
capture more high-frequency features. Moreover, by taking advantage of the
large receptive field provided by FFC residual blocks, our model is able to
effectively explore global contextual information and produce images with
better perceptual quality. In the prior knowledge branch, an ImageNet
pretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model to
learn more supplementary information and acquire a stronger generalization
ability. The feasibility and effectiveness of the proposed method is
demonstrated via extensive experiments and ablation studies. The code is
available at https://github.com/zhouh115/DWT-FFC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yangyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00983">
<title>Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.00983</link>
<description rdf:parseType="Literal">&lt;p&gt;Inventory Routing Problem (IRP) is a crucial challenge in supply chain
management as it involves optimizing efficient route selection while
considering the uncertainty of inventory demand planning. To solve IRPs,
usually a two-stage approach is employed, where demand is predicted using
machine learning techniques first, and then an optimization algorithm is used
to minimize routing costs. Our experiment shows machine learning models fall
short of achieving perfect accuracy because inventory levels are influenced by
the dynamic business environment, which, in turn, affects the optimization
problem in the next stage, resulting in sub-optimal decisions. In this paper,
we formulate and propose a decision-focused learning-based approach to solving
real-world IRPs. This approach directly integrates inventory prediction and
routing optimization within an end-to-end system potentially ensuring a robust
supply chain strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;MD Shafikul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1&quot;&gt;Azmine Toushik Wasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04916">
<title>Explainable Identification of Hate Speech towards Islam using Graph Neural Networks. (arXiv:2311.04916v2 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.04916</link>
<description rdf:parseType="Literal">&lt;p&gt;Islamophobic language is a prevalent challenge on online social interaction
platforms. Identifying and eliminating such hatred is a crucial step towards a
future of harmony and peace. This study presents a novel paradigm for
identifying and explaining hate speech towards Islam using graph neural
networks. Utilizing the intrinsic ability of graph neural networks to find,
extract, and use relationships across disparate data points, our model
consistently achieves outstanding performance while offering explanations for
the underlying correlations and causation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1&quot;&gt;Azmine Toushik Wasi&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>