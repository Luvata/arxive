<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/1902.01453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1905.09610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.12056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.12883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.12787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14129" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/1902.01453">
<title>PVNet: A LRCN Architecture for Spatio-Temporal Photovoltaic PowerForecasting from Numerical Weather Prediction. (arXiv:1902.01453v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1902.01453</link>
<description rdf:parseType="Literal">&lt;p&gt;Photovoltaic (PV) power generation has emerged as one of the lead renewable
energy sources. Yet, its production is characterized by high uncertainty, being
dependent on weather conditions like solar irradiance and temperature.
Predicting PV production, even in the 24-hour forecast, remains a challenge and
leads energy providers to left idling - often carbon emitting - plants. In this
paper, we introduce a Long-Term Recurrent Convolutional Network using Numerical
Weather Predictions (NWP) to predict, in turn, PV production in the 24-hour and
48-hour forecast horizons. This network architecture fully leverages both
temporal and spatial weather data, sampled over the whole geographical area of
interest. We train our model on an NWP dataset from the National Oceanic and
Atmospheric Administration (NOAA) to predict spatially aggregated PV production
in Germany. We compare its performance to the persistence model and
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathe_J/0/1/0/all/0/1&quot;&gt;Johan Mathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1&quot;&gt;Nina Miolane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebastien_N/0/1/0/all/0/1&quot;&gt;Nicolas Sebastien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lequeux_J/0/1/0/all/0/1&quot;&gt;Jeremie Lequeux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1905.09610">
<title>Hypothetical answers to continuous queries over data streams. (arXiv:1905.09610v3 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/1905.09610</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous queries over data streams may suffer from blocking operations
and/or unbound wait, which may delay answers until some relevant input arrives
through the data stream. These delays may turn answers, when they arrive,
obsolete to users who sometimes have to make decisions with no help whatsoever.
Therefore, it can be useful to provide hypothetical answers - &quot;given the
current information, it is possible that X will become true at time t&quot; -
instead of no information at all.
&lt;/p&gt;
&lt;p&gt;In this paper we present a semantics for queries and corresponding answers
that covers such hypothetical answers, together with an online algorithm for
updating the set of facts that are consistent with the currently available
information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_Filipe_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s Cruz-Filipe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaspar_G/0/1/0/all/0/1&quot;&gt;Gra&amp;#xe7;a Gaspar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_I/0/1/0/all/0/1&quot;&gt;Isabel Nunes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.12056">
<title>Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2108.12056</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow&apos;s machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of learning and retaining new information without being exposed to it
arbitrarily often. Past efforts to engineer such systems have sought to build
or regulate artificial neural networks using disjoint sets of weights that are
uniquely sensitive to specific tasks or inputs. This has not yet enabled
continual learning over long sequences of previously unseen data without
corrupting existing knowledge: a problem known as catastrophic forgetting. In
this paper, we introduce a system that can learn sequentially over previously
unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is
done by controlling the activity of weights in a convolutional neural network
on the basis of inputs using top-down regulation generated by a second
feed-forward neural network. We find that our method learns continually under
domain transfer with sparse bursts of activity in weights that are recycled
across tasks, rather than by maintaining task-specific modules. Sparse synaptic
bursting is found to balance activity and suppression such that new functions
can be learned without corrupting extant knowledge, thus mirroring the balance
of order and disorder in systems at the edge of chaos. This behavior emerges
during a prior pre-training (or &apos;meta-learning&apos;) phase in which regulated
synapses are selectively disinhibited, or grown, from an initial state of
uniform suppression through prediction error minimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1&quot;&gt;Shawn L. Beaulieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.12883">
<title>Human Detection of Political Speech Deepfakes across Transcripts, Audio, and Video. (arXiv:2202.12883v4 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2202.12883</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in technology for hyper-realistic visual and audio effects
provoke the concern that deepfake videos of political speeches will soon be
indistinguishable from authentic video recordings. The conventional wisdom in
communication theory predicts people will fall for fake news more often when
the same version of a story is presented as a video versus text. We conduct 5
pre-registered randomized experiments with 2,215 participants to evaluate how
accurately humans distinguish real political speeches from fabrications across
base rates of misinformation, audio sources, question framings, and media
modalities. We find base rates of misinformation minimally influence
discernment and deepfakes with audio produced by the state-of-the-art
text-to-speech algorithms are harder to discern than the same deepfakes with
voice actor audio. Moreover across all experiments, we find audio and visual
information enables more accurate discernment than text alone: human
discernment relies more on how something is said, the audio-visual cues, than
what is said, the speech content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1&quot;&gt;Matthew Groh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1&quot;&gt;Aruna Sankaranarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1&quot;&gt;Nikhil Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong Young Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lippman_A/0/1/0/all/0/1&quot;&gt;Andrew Lippman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_R/0/1/0/all/0/1&quot;&gt;Rosalind Picard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07703">
<title>TeleGraph: A Benchmark Dataset for Hierarchical Link Prediction. (arXiv:2204.07703v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07703</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction is a key problem for network-structured data, attracting
considerable research efforts owing to its diverse applications. The current
link prediction methods focus on general networks and are overly dependent on
either the closed triangular structure of networks or node attributes. Their
performance on sparse or highly hierarchical networks has not been well
studied. On the other hand, the available tree-like benchmark datasets are
either simulated, with limited node information, or small in scale. To bridge
this gap, we present a new benchmark dataset TeleGraph, a highly sparse and
hierarchical telecommunication network associated with rich node attributes,
for assessing and fostering the link inference techniques. Our empirical
results suggest that most of the algorithms fail to produce a satisfactory
performance on a nearly tree-like dataset, which calls for special attention
when designing or deploying the link prediction algorithm in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Min Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bisheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Menglin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lujia Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.12787">
<title>Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.12787</link>
<description rdf:parseType="Literal">&lt;p&gt;While AlphaZero-style reinforcement learning (RL) algorithms excel in various
board games, in this paper we show that they face challenges on impartial games
where players share pieces. We present a concrete example of a game - namely
the children&apos;s game of Nim - and other impartial games that seem to be a
stumbling block for AlphaZero-style and similar self-play reinforcement
learning algorithms.
&lt;/p&gt;
&lt;p&gt;Our work is built on the challenges posed by the intricacies of data
distribution on the ability of neural networks to learn parity functions,
exacerbated by the noisy labels issue. Our findings are consistent with recent
studies showing that AlphaZero-style algorithms are vulnerable to adversarial
attacks and adversarial perturbations, showing the difficulty of learning to
master the games in all legal states.
&lt;/p&gt;
&lt;p&gt;We show that Nim can be learned on small boards, but the learning progress of
AlphaZero-style algorithms dramatically slows down when the board size
increases. Intuitively, the difference between impartial games like Nim and
partisan games like Chess and Go can be explained by the fact that if a small
part of the board is covered for impartial games it is typically not possible
to predict whether the position is won or lost as there is often zero
correlation between the visible part of a partly blanked-out position and its
correct evaluation. This situation starkly contrasts partisan games where a
partly blanked-out board position typically provides abundant or at least
non-trifle information about the value of the fully uncovered position.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riis_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Riis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11230">
<title>You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. (arXiv:2207.11230v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11230</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout Analysis (the identification of zones and their classification) is the
first step along line segmentation in Optical Character Recognition and similar
tasks. The ability of identifying main body of text from marginal text or
running titles makes the difference between extracting the work full text of a
digitized book and noisy outputs. We show that most segmenters focus on pixel
classification and that polygonization of this output has not been used as a
target for the latest competition on historical document (ICDAR 2017 and
onwards), despite being the focus in the early 2010s. We propose to shift, for
efficiency, the task from a pixel classification-based polygonization to an
object detection using isothetic rectangles. We compare the output of Kraken
and YOLOv5 in terms of segmentation and show that the later severely
outperforms the first on small datasets (1110 samples and below). We release
two datasets for training and evaluation on historical documents as well as a
new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of
Kraken 4.1.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clerice_T/0/1/0/all/0/1&quot;&gt;Thibault Cl&amp;#xe9;rice&lt;/a&gt; (ENC, CJM, HiSoMA, UJML, ALMAnaCH)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07003">
<title>Vision-aided UAV navigation and dynamic obstacle avoidance using gradient-based B-spline trajectory optimization. (arXiv:2209.07003v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07003</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating dynamic environments requires the robot to generate collision-free
trajectories and actively avoid moving obstacles. Most previous works designed
path planning algorithms based on one single map representation, such as the
geometric, occupancy, or ESDF map. Although they have shown success in static
environments, due to the limitation of map representation, those methods cannot
reliably handle static and dynamic obstacles simultaneously. To address the
problem, this paper proposes a gradient-based B-spline trajectory optimization
algorithm utilizing the robot&apos;s onboard vision. The depth vision enables the
robot to track and represent dynamic objects geometrically based on the voxel
map. The proposed optimization first adopts the circle-based guide-point
algorithm to approximate the costs and gradients for avoiding static obstacles.
Then, with the vision-detected moving objects, our receding-horizon distance
field is simultaneously used to prevent dynamic collisions. Finally, the
iterative re-guide strategy is applied to generate the collision-free
trajectory. The simulation and physical experiments prove that our method can
run in real-time to navigate dynamic environments safely. Our software is
available on GitHub as an open-source package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhefan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1&quot;&gt;Yumeng Xiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baihan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1&quot;&gt;Kenji Shimada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08258">
<title>A real-time dynamic obstacle tracking and mapping system for UAV navigation and collision avoidance with an RGB-D camera. (arXiv:2209.08258v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08258</link>
<description rdf:parseType="Literal">&lt;p&gt;The real-time dynamic environment perception has become vital for autonomous
robots in crowded spaces. Although the popular voxel-based mapping methods can
efficiently represent 3D obstacles with arbitrarily complex shapes, they can
hardly distinguish between static and dynamic obstacles, leading to the limited
performance of obstacle avoidance. While plenty of sophisticated learning-based
dynamic obstacle detection algorithms exist in autonomous driving, the
quadcopter&apos;s limited computation resources cannot achieve real-time performance
using those approaches. To address these issues, we propose a real-time dynamic
obstacle tracking and mapping system for quadcopter obstacle avoidance using an
RGB-D camera. The proposed system first utilizes a depth image with an
occupancy voxel map to generate potential dynamic obstacle regions as
proposals. With the obstacle region proposals, the Kalman filter and our
continuity filter are applied to track each dynamic obstacle. Finally, the
environment-aware trajectory prediction method is proposed based on the Markov
chain using the states of tracked dynamic obstacles. We implemented the
proposed system with our custom quadcopter and navigation planner. The
simulation and physical experiments show that our methods can successfully
track and represent obstacles in dynamic environments in real-time and safely
avoid obstacles. Our software is available on GitHub as an open-source ROS
package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhefan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baihan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1&quot;&gt;Yumeng Xiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1&quot;&gt;Kenji Shimada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08604">
<title>NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly. (arXiv:2210.08604v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08604</link>
<description rdf:parseType="Literal">&lt;p&gt;Norm discovery is important for understanding and reasoning about the
acceptable behaviors and potential violations in human communication and
interactions. We introduce NormSage, a framework for addressing the novel task
of conversation-grounded multi-lingual, multi-cultural norm discovery, based on
language model prompting and self-verification. NormSAGE leverages the
expressiveness and implicit knowledge of the pretrained GPT-3 language model
backbone, to elicit knowledge about norms through directed questions
representing the norm discovery task and conversation context. It further
addresses the risk of language model hallucination with a self-verification
mechanism ensuring that the norms discovered are correct and are substantially
grounded to their source conversations. Evaluation results show that our
approach discovers significantly more relevant and insightful norms for
conversations on-the-fly compared to baselines (&amp;gt;10+% in Likert scale rating).
The norms discovered from Chinese conversation are also comparable to the norms
discovered from English conversation in terms of insightfulness and correctness
(&amp;lt;3% difference). In addition, the culture-specific norms are promising
quality, allowing for 80% accuracy in culture pair human identification.
Finally, our grounding process in norm discovery self-verification can be
extended for instantiating the adherence and violation of any norm for a given
conversation on-the-fly, with explainability and transparency. NormSAGE
achieves an AUC of 95.4% in grounding, with natural language explanation
matching human-written quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1&quot;&gt;Yi R. Fung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tuhin Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambow_O/0/1/0/all/0/1&quot;&gt;Owen Rambow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1&quot;&gt;Smaranda Muresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10537">
<title>Online LiDAR-Camera Extrinsic Parameters Self-checking. (arXiv:2210.10537v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10537</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of neural networks and the increasing popularity of
automatic driving, the calibration of the LiDAR and the camera has attracted
more and more attention. This calibration task is multi-modal, where the rich
color and texture information captured by the camera and the accurate
three-dimensional spatial information from the LiDAR is incredibly significant
for downstream tasks. Current research interests mainly focus on obtaining
accurate calibration results through information fusion. However, they seldom
analyze whether the calibrated results are correct or not, which could be of
significant importance in real-world applications. For example, in large-scale
production, the LiDARs and the cameras of each smart car have to get
well-calibrated as the car leaves the production line, while in the rest of the
car life period, the poses of the LiDARs and cameras should also get
continually supervised to ensure the security. To this end, this paper proposes
a self-checking algorithm to judge whether the extrinsic parameters are
well-calibrated by introducing a binary classification network based on the
fused information from the camera and the LiDAR. Moreover, since there is no
such dataset for the task in this work, we further generate a new dataset
branch from the KITTI dataset tailored for the task. Our experiments on the
proposed dataset branch demonstrate the performance of our method. To the best
of our knowledge, this is the first work to address the significance of
continually checking the calibrated extrinsic parameters for autonomous
driving. The code is open-sourced on the Github website at
https://github.com/OpenCalib/LiDAR2camera_self-check.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Pengjin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1&quot;&gt;Guohang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02658">
<title>Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02658</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, machine learning (ML) has become a popular approach to support
self-adaptation. ML has been used to deal with several problems in
self-adaptation, such as maintaining an up-to-date runtime model under
uncertainty and scalable decision-making. Yet, exploiting ML comes with
inherent challenges. In this paper, we focus on a particularly important
challenge for learning-based self-adaptive systems: drift in adaptation spaces.
With adaptation space we refer to the set of adaptation options a self-adaptive
system can select from at a given time to adapt based on the estimated quality
properties of the adaptation options. Drift of adaptation spaces originates
from uncertainties, affecting the quality properties of the adaptation options.
Such drift may imply that eventually no adaptation option can satisfy the
initial set of the adaptation goals, deteriorating the quality of the system,
or adaptation options may emerge that allow enhancing the adaptation goals. In
ML, such shift corresponds to novel class appearance, a type of concept drift
in target data that common ML techniques have problems dealing with. To tackle
this problem, we present a novel approach to self-adaptation that enhances
learning-based self-adaptive systems with a lifelong ML layer. We refer to this
approach as lifelong self-adaptation. The lifelong ML layer tracks the system
and its environment, associates this knowledge with the current tasks,
identifies new tasks based on differences, and updates the learning models of
the self-adaptive system accordingly. A human stakeholder may be involved to
support the learning process and adjust the learning and goal models. We
present a general architecture for lifelong self-adaptation and apply it to the
case of drift of adaptation spaces that affects the decision-making in
self-adaptation. We validate the approach for a series of scenarios using the
DeltaIoT exemplar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gheibi_O/0/1/0/all/0/1&quot;&gt;Omid Gheibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1&quot;&gt;Danny Weyns&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13709">
<title>Undesirable Biases in NLP: Addressing Challenges of Measurement. (arXiv:2211.13709v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13709</link>
<description rdf:parseType="Literal">&lt;p&gt;As Large Language Models and Natural Language Processing (NLP) technology
rapidly develop and spread into daily life, it becomes crucial to anticipate
how their use could harm people. One problem that has received a lot of
attention in recent years is that this technology has displayed harmful biases,
from generating derogatory stereotypes to producing disparate outcomes for
different social groups. Although a lot of effort has been invested in
assessing and mitigating these biases, our methods of measuring the biases of
NLP models have serious problems and it is often unclear what they actually
measure. In this paper, we provide an interdisciplinary approach to discussing
the issue of NLP model bias by adopting the lens of psychometrics -- a field
specialized in the measurement of concepts like bias that are not directly
observable. In particular, we will explore two central notions from
psychometrics, the construct validity and the reliability of measurement tools,
and discuss how they can be applied in the context of measuring model bias. Our
goal is to provide NLP practitioners with methodological tools for designing
better bias measures, and to inspire them more generally to explore tools from
psychometrics when working on bias measurement tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1&quot;&gt;Oskar van der Wal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachmann_D/0/1/0/all/0/1&quot;&gt;Dominik Bachmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1&quot;&gt;Alina Leidinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maanen_L/0/1/0/all/0/1&quot;&gt;Leendert van Maanen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1&quot;&gt;Willem Zuidema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1&quot;&gt;Katrin Schulz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03597">
<title>DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. (arXiv:2212.03597v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03597</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances on deep learning models come at the price of formidable
training cost. The increasing model size is one of the root causes, but another
less-emphasized fact is that data scale is actually increasing at a similar
speed as model scale, and the training cost is proportional to both of them.
Compared to the rapidly evolving model architecture, how to efficiently use the
training data (especially for the expensive foundation model pretraining) is
both less explored and difficult to realize due to the lack of a convenient
framework that focuses on data efficiency capabilities. To this end, we present
DeepSpeed Data Efficiency, a framework that makes better use of data, increases
training efficiency, and improves model quality. Specifically, we propose and
combine two data efficiency techniques: efficient data sampling via a general
curriculum learning library, and efficient data routing via a novel random
layerwise token dropping technique. For GPT-3 1.3B language model pretraining,
our work achieves 12.5x less data/time/cost (\$3.7K if rent on Azure), while
still maintaining 95% of model quality compared to baseline with full data and
cost (\$46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also
achieve the same model quality with up to 2x less data/time/cost, or achieve
better model quality under same data/time/cost. DeepSpeed Data Efficiency is
easy to use and tune, enabling us to easily apply it and verify its benefit on
additional tasks including GPT-3 MoE model pretraining and small-scale
GPT-2/ViT finetuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Conglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Connor Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08422">
<title>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles. (arXiv:2301.08422v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08422</link>
<description rdf:parseType="Literal">&lt;p&gt;Tunnel construction using the drill-and-blast method requires the 3D
measurement of the excavation front to evaluate underbreak locations.
Considering the inspection and measurement task&apos;s safety, cost, and efficiency,
deploying lightweight autonomous robots, such as unmanned aerial vehicles
(UAV), becomes more necessary and popular. Most of the previous works use a
prior map for inspection viewpoint determination and do not consider dynamic
obstacles. To maximally increase the level of autonomy, this paper proposes a
vision-based UAV inspection framework for dynamic tunnel environments without
using a prior map. Our approach utilizes a hierarchical planning scheme,
decomposing the inspection problem into different levels. The high-level
decision maker first determines the task for the robot and generates the target
point. Then, the mid-level path planner finds the waypoint path and optimizes
the collision-free static trajectory. Finally, the static trajectory will be
fed into the low-level local planner to avoid dynamic obstacles and navigate to
the target point. Besides, our framework contains a novel dynamic map module
that can simultaneously track dynamic obstacles and represent static obstacles
based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM)
pipeline is applied to generate the 3D shape of the target. To our best
knowledge, this is the first time autonomous inspection has been realized in
unknown and dynamic tunnel environments. Our flight experiments in a real
tunnel prove that our method can autonomously inspect the tunnel excavation
front surface. Our software is available on GitHub as an open-source ROS
package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhefan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baihan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1&quot;&gt;Yumeng Xiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_C/0/1/0/all/0/1&quot;&gt;Christopher Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1&quot;&gt;Kenji Shimada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17503">
<title>Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning. (arXiv:2303.17503v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17503</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Pgx, a suite of board game reinforcement learning (RL)
environments written in JAX and optimized for GPU/TPU accelerators. By
leveraging JAX&apos;s auto-vectorization and parallelization over accelerators, Pgx
can efficiently scale to thousands of simultaneous simulations over
accelerators. In our experiments on a DGX-A100 workstation, we discovered that
Pgx can simulate RL environments 10-100x faster than existing implementations
available in Python. Pgx includes RL environments commonly used as benchmarks
in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx
offers miniature game sets and baseline models to facilitate rapid research
cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm
with Pgx environments. Overall, Pgx provides high-performance environment
simulators for researchers to accelerate their RL experiments. Pgx is available
at &lt;a href=&quot;http://github.com/sotetsuk/pgx.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyamada_S/0/1/0/all/0/1&quot;&gt;Sotetsu Koyamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okano_S/0/1/0/all/0/1&quot;&gt;Shinri Okano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimori_S/0/1/0/all/0/1&quot;&gt;Soichiro Nishimori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murata_Y/0/1/0/all/0/1&quot;&gt;Yu Murata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habara_K/0/1/0/all/0/1&quot;&gt;Keigo Habara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kita_H/0/1/0/all/0/1&quot;&gt;Haruka Kita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishii_S/0/1/0/all/0/1&quot;&gt;Shin Ishii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02209">
<title>Large-scale Online Ridesharing: The Effect of Assignment Optimality on System Performance. (arXiv:2305.02209v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02209</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobility-on-demand (MoD) systems consist of a fleet of shared vehicles that
can be hailed for one-way point-to-point trips. The total distance driven by
the vehicles and the fleet size can be reduced by employing ridesharing, i.e.,
by assigning multiple passengers to one vehicle. However, finding the optimal
passenger-vehicle assignment in an MoD system is a hard combinatorial problem.
In this work, we demonstrate how the VGA method, a recently proposed systematic
method for ridesharing, can be used to compute the optimal passenger-vehicle
assignments and corresponding vehicle routes in a massive-scale MoD system. In
contrast to existing works, we solve all passenger-vehicle assignment problems
to optimality, regularly dealing with instances containing thousands of
vehicles and passengers. Moreover, to examine the impact of using optimal
ridesharing assignments, we compare the performance of an MoD system that uses
optimal assignments against an MoD system that uses assignments computed using
insertion heuristic and against an MoD system that uses no ridesharing. We
found that the system that uses optimal ridesharing assignments subject to the
maximum travel delay of 4 minutes reduces the vehicle distance driven by 57 %
compared to an MoD system without ridesharing. Furthermore, we found that the
optimal assignments result in a 20 % reduction in vehicle distance driven and 5
% lower average passenger travel delay compared to a system that uses insertion
heuristic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fiedler_D/0/1/0/all/0/1&quot;&gt;David Fiedler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Certicky_M/0/1/0/all/0/1&quot;&gt;Michal &amp;#x10c;ertick&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Alonso_Mora_J/0/1/0/all/0/1&quot;&gt;Javier Alonso-Mora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pechoucek_M/0/1/0/all/0/1&quot;&gt;Michal P&amp;#x11b;chou&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cap_M/0/1/0/all/0/1&quot;&gt;Michal &amp;#x10c;&amp;#xe1;p&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06435">
<title>Phase transitions in the mini-batch size for sparse and dense two-layer neural networks. (arXiv:2305.06435v3 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06435</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of mini-batches of data in training artificial neural networks is
nowadays very common. Despite its broad usage, theories explaining
quantitatively how large or small the optimal mini-batch size should be are
missing. This work presents a systematic attempt at understanding the role of
the mini-batch size in training two-layer neural networks. Working in the
teacher-student scenario, with a sparse teacher, and focusing on tasks of
different complexity, we quantify the effects of changing the mini-batch size
$m$. We find that often the generalization performances of the student strongly
depend on $m$ and may undergo sharp phase transitions at a critical value
$m_c$, such that for $m&amp;lt;m_c$ the training process fails, while for $m&amp;gt;m_c$ the
student learns perfectly or generalizes very well the teacher. Phase
transitions are induced by collective phenomena firstly discovered in
statistical mechanics and later observed in many fields of science. Observing a
phase transition by varying the mini-batch size across different architectures
raises several questions about the role of this hyperparameter in the neural
network learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Marino_R/0/1/0/all/0/1&quot;&gt;Raffaele Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ricci_Tersenghi_F/0/1/0/all/0/1&quot;&gt;Federico Ricci-Tersenghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07522">
<title>SPADE: Sparse Pillar-based 3D Object Detection Accelerator for Autonomous Driving. (arXiv:2305.07522v3 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07522</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection using point cloud (PC) data is essential for perception
pipelines of autonomous driving, where efficient encoding is key to meeting
stringent resource and latency requirements. PointPillars, a widely adopted
bird&apos;s-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars
for fast and accurate 3D object detection. However, the state-of-the-art
methods employing PointPillars overlook the inherent sparsity of pillar
encoding where only a valid pillar is encoded with a vector of channel
elements, missing opportunities for significant computational reduction.
Meanwhile, current sparse convolution accelerators are designed to handle only
element-wise activation sparsity and do not effectively address the vector
sparsity imposed by pillar encoding.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose SPADE, an algorithm-hardware co-design strategy to
maximize vector sparsity in pillar-based 3D object detection and accelerate
vector-sparse convolution commensurate with the improved sparsity. SPADE
consists of three components: (1) a dynamic vector pruning algorithm balancing
accuracy and computation savings from vector sparsity, (2) a sparse coordinate
management hardware transforming 2D systolic array into a vector-sparse
convolution accelerator, and (3) sparsity-aware dataflow optimization tailoring
sparse convolution schedules for hardware efficiency. Taped-out with a
commercial technology, SPADE saves the amount of computation by 36.3--89.2\%
for representative 3D object detection networks and benchmarks, leading to
1.3--10.9$\times$ speedup and 1.5--12.6$\times$ energy savings compared to the
ideal dense accelerator design. These sparsity-proportional performance gains
equate to 4.1--28.8$\times$ speedup and 90.2--372.3$\times$ energy savings
compared to the counterpart server and edge platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seongmin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_M/0/1/0/all/0/1&quot;&gt;Minyong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Janghwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Won Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Nam Sung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Mingu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jungwook Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15944">
<title>How to Turn Your Knowledge Graph Embeddings into Generative Models. (arXiv:2305.15944v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15944</link>
<description rdf:parseType="Literal">&lt;p&gt;Some of the most successful knowledge graph embedding (KGE) models for link
prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based
models. Under this perspective they are not amenable for exact
maximum-likelihood estimation (MLE), sampling and struggle to integrate logical
constraints. This work re-interprets the score functions of these KGEs as
circuits -- constrained computational graphs allowing efficient
marginalisation. Then, we design two recipes to obtain efficient generative
circuit models by either restricting their activations to be non-negative or
squaring their outputs. Our interpretation comes with little or no loss of
performance for link prediction, while the circuits framework unlocks exact
learning by MLE, efficient sampling of new triples, and guarantee that logical
constraints are satisfied by design. Furthermore, our models scale more
gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loconte_L/0/1/0/all/0/1&quot;&gt;Lorenzo Loconte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauro_N/0/1/0/all/0/1&quot;&gt;Nicola Di Mauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peharz_R/0/1/0/all/0/1&quot;&gt;Robert Peharz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1&quot;&gt;Antonio Vergari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16494">
<title>Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (arXiv:2305.16494v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16494</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are known to be susceptible to adversarial samples: small
variations of natural examples crafted to deliberately mislead the models.
While they can be easily generated using gradient-based techniques in digital
and physical scenarios, they often differ greatly from the actual data
distribution of natural images, resulting in a trade-off between strength and
stealthiness. In this paper, we propose a novel framework dubbed
Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic
adversarial samples. By exploiting a gradient guided by a diffusion model,
Diff-PGD ensures that adversarial samples remain close to the original data
distribution while maintaining their effectiveness. Moreover, our framework can
be easily customized for specific tasks such as digital attacks, physical-world
attacks, and style-based attacks. Compared with existing methods for generating
natural-style adversarial samples, our framework enables the separation of
optimizing adversarial loss from other surrogate losses (e.g.,
content/smoothness/style loss), making it more stable and controllable.
Finally, we demonstrate that the samples generated using Diff-PGD have better
transferability and anti-purification power than traditional gradient-based
methods. Code will be released in https://github.com/xavihart/Diff-PGD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Haotian Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1&quot;&gt;Alexandre Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17402">
<title>Learning and Collusion in Multi-unit Auctions. (arXiv:2305.17402v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17402</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider repeated multi-unit auctions with uniform pricing, which are
widely used in practice for allocating goods such as carbon licenses. In each
round, $K$ identical units of a good are sold to a group of buyers that have
valuations with diminishing marginal returns. The buyers submit bids for the
units, and then a price $p$ is set per unit so that all the units are sold. We
consider two variants of the auction, where the price is set to the $K$-th
highest bid and $(K+1)$-st highest bid, respectively.
&lt;/p&gt;
&lt;p&gt;We analyze the properties of this auction in both the offline and online
settings. In the offline setting, we consider the problem that one player $i$
is facing: given access to a data set that contains the bids submitted by
competitors in past auctions, find a bid vector that maximizes player $i$&apos;s
cumulative utility on the data set. We design a polynomial time algorithm for
this problem, by showing it is equivalent to finding a maximum-weight path on a
carefully constructed directed acyclic graph.
&lt;/p&gt;
&lt;p&gt;In the online setting, the players run learning algorithms to update their
bids as they participate in the auction over time. Based on our offline
algorithm, we design efficient online learning algorithms for bidding. The
algorithms have sublinear regret, under both full information and bandit
feedback structures. We complement our online learning algorithms with regret
lower bounds.
&lt;/p&gt;
&lt;p&gt;Finally, we analyze the quality of the equilibria in the worst case through
the lens of the core solution concept in the game among the bidders. We show
that the $(K+1)$-st price format is susceptible to collusion among the bidders;
meanwhile, the $K$-th price format does not have this issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branzei_S/0/1/0/all/0/1&quot;&gt;Simina Br&amp;#xe2;nzei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derakhshan_M/0/1/0/all/0/1&quot;&gt;Mahsa Derakhshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yanjun Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18342">
<title>Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18342</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative neural models hold great promise in enhancing programming
education by synthesizing new content. We seek to design neural models that can
automatically generate programming tasks for a given specification in the
context of visual programming domains. Despite the recent successes of large
generative models like GPT-4, our initial results show that these models are
ineffective in synthesizing visual programming tasks and struggle with logical
and spatial reasoning. We propose a novel neuro-symbolic technique,
NeurTaskSyn, that can synthesize programming tasks for a specification given in
the form of desired programming concepts exercised by its solution code and
constraints on the visual task. NeurTaskSyn has two components: the first
component is trained via imitation learning procedure to generate possible
solution codes, and the second component is trained via reinforcement learning
procedure to guide an underlying symbolic execution engine that generates
visual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn
through an extensive empirical evaluation and a qualitative study on reference
tasks taken from the Hour of Code: Classic Maze challenge by Code-dot-org and
the Intro to Programming with Karel course by CodeHS-dot-com.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1&quot;&gt;Victor-Alexandru P&amp;#x103;durean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzannetos_G/0/1/0/all/0/1&quot;&gt;Georgios Tzannetos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1&quot;&gt;Adish Singla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19556">
<title>Exploring Phonetic Context-Aware Lip-Sync For Talking Face Generation. (arXiv:2305.19556v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19556</link>
<description rdf:parseType="Literal">&lt;p&gt;Talking face generation is the challenging task of synthesizing a natural and
realistic face that requires accurate synchronization with a given audio. Due
to co-articulation, where an isolated phone is influenced by the preceding or
following phones, the articulation of a phone varies upon the phonetic context.
Therefore, modeling lip motion with the phonetic context can generate more
spatio-temporally aligned lip movement. In this respect, we investigate the
phonetic context in generating lip motion for talking face generation. We
propose Context-Aware Lip-Sync framework (CALS), which explicitly leverages
phonetic context to generate lip movement of the target face. CALS is comprised
of an Audio-to-Lip module and a Lip-to-Face module. The former is pretrained
based on masked learning to map each phone to a contextualized lip motion unit.
The contextualized lip motion unit then guides the latter in synthesizing a
target identity with context-aware lip motion. From extensive experiments, we
verify that simply exploiting the phonetic context in the proposed CALS
framework effectively enhances spatio-temporal alignment. We also demonstrate
the extent to which the phonetic context assists in lip synchronization and
find the effective window size for lip generation to be approximately 1.2
seconds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19706">
<title>Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming. (arXiv:2305.19706v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19706</link>
<description rdf:parseType="Literal">&lt;p&gt;Global optimization of decision trees has shown to be promising in terms of
accuracy, size, and consequently human comprehensibility. However, many of the
methods used rely on general-purpose solvers for which scalability remains an
issue. Dynamic programming methods have been shown to scale much better because
they exploit the tree structure by solving subtrees as independent subproblems.
However, this only works when an objective can be optimized separately for
subtrees. We explore this relationship in detail and show the necessary and
sufficient conditions for such separability and generalize previous dynamic
programming approaches into a framework that can optimize any combination of
separable objectives and constraints. Experiments on five application domains
show the general applicability of this framework, while outperforming the
scalability of general-purpose solvers by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linden_J/0/1/0/all/0/1&quot;&gt;Jacobus G. M. van der Linden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weerdt_M/0/1/0/all/0/1&quot;&gt;Mathijs M. de Weerdt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirovic_E/0/1/0/all/0/1&quot;&gt;Emir Demirovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03988">
<title>Learn the Force We Can: Enabling Sparse Motion Control in Multi-Object Video Generation. (arXiv:2306.03988v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03988</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel unsupervised method to autoregressively generate videos
from a single frame and a sparse motion input. Our trained model can generate
unseen realistic object-to-object interactions. Although our model has never
been given the explicit segmentation and motion of each object in the scene
during training, it is able to implicitly separate their dynamics and extents.
Key components in our method are the randomized conditioning scheme, the
encoding of the input motion control, and the randomized and sparse sampling to
enable generalization to out of distribution but realistic correlations. Our
model, which we call YODA, has therefore the ability to move objects without
physically touching them. Through extensive qualitative and quantitative
evaluations on several datasets, we show that YODA is on par with or better
than state of the art video generation prior work in terms of both
controllability and video quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1&quot;&gt;Aram Davtyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1&quot;&gt;Paolo Favaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05323">
<title>Advancing Italian Biomedical Information Extraction with Transformers-based Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05323</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of computerized medical records in hospitals has reduced
burdensome activities like manual writing and information fetching. However,
the data contained in medical records are still far underutilized, primarily
because extracting data from unstructured textual medical records takes time
and effort. Information Extraction, a subfield of Natural Language Processing,
can help clinical practitioners overcome this limitation by using automated
text-mining pipelines. In this work, we created the first Italian
neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to
develop a Transformers-based model. Moreover, we collected and leveraged three
external independent datasets to implement an effective multicenter model, with
overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned
are: (i) the crucial role of a consistent annotation process and (ii) a
fine-tuning strategy that combines classical methods with a &quot;low-resource&quot;
approach. This allowed us to establish methodological guidelines that pave the
way for Natural Language Processing studies in less-resourced languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crema_C/0/1/0/all/0/1&quot;&gt;Claudio Crema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buonocore_T/0/1/0/all/0/1&quot;&gt;Tommaso Mario Buonocore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fostinelli_S/0/1/0/all/0/1&quot;&gt;Silvia Fostinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parimbelli_E/0/1/0/all/0/1&quot;&gt;Enea Parimbelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verde_F/0/1/0/all/0/1&quot;&gt;Federico Verde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fundaro_C/0/1/0/all/0/1&quot;&gt;Cira Fundar&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manera_M/0/1/0/all/0/1&quot;&gt;Marina Manera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramusino_M/0/1/0/all/0/1&quot;&gt;Matteo Cotta Ramusino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capelli_M/0/1/0/all/0/1&quot;&gt;Marco Capelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1&quot;&gt;Alfredo Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binetti_G/0/1/0/all/0/1&quot;&gt;Giuliano Binetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellazzi_R/0/1/0/all/0/1&quot;&gt;Riccardo Bellazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redolfi_A/0/1/0/all/0/1&quot;&gt;Alberto Redolfi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06446">
<title>ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06446</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have shown impressive performance and have become
a unified backbone for multiple vision tasks. However, both the attention
mechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently
efficient due to dense multiplications, leading to costly training and
inference. To this end, we propose to reparameterize pre-trained ViTs with a
mixture of multiplication primitives, e.g., bitwise shifts and additions,
towards a new type of multiplication-reduced model, dubbed
$\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on
GPUs without requiring training from scratch. Specifically, all
$\texttt{MatMuls}$ among queries, keys, and values are reparameterized using
additive kernels, after mapping queries and keys to binary codes in Hamming
space. The remaining MLPs or linear layers are then reparameterized with shift
kernels. We utilize TVM to implement and optimize those customized kernels for
practical hardware deployment on GPUs. We find that such a reparameterization
on attention maintains model accuracy, while inevitably leading to accuracy
drops when being applied to MLPs. To marry the best of both worlds, we further
propose a new mixture of experts (MoE) framework to reparameterize MLPs by
taking multiplication or its primitives as experts, e.g., multiplication and
shift, and designing a new latency-aware load-balancing loss. Such a loss helps
to train a generic router for assigning a dynamic amount of input tokens to
different experts according to their latency. Extensive experiments on various
2D/3D Transformer-based vision tasks consistently validate the effectiveness of
our proposed ShiftAddViT, achieving up to $\textbf{5.18$\times$}$ latency
reductions on GPUs and $\textbf{42.9}$% energy savings, while maintaining a
comparable accuracy as original or efficient ViTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1&quot;&gt;Haoran You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Huihong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yipin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yingyan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09526">
<title>Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09526</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation Learning (IL) is a widely used framework for learning imitative
behavior from demonstrations. It is especially appealing for solving complex
real-world tasks where handcrafting reward function is difficult, or when the
goal is to mimic human expert behavior. However, the learned imitative policy
can only follow the behavior in the demonstration. When applying the imitative
policy, we may need to customize the policy behavior to meet different
requirements coming from diverse downstream tasks. Meanwhile, we still want the
customized policy to maintain its imitative nature. To this end, we formulate a
new problem setting called policy customization. It defines the learning task
as training a policy that inherits the characteristics of the prior policy
while satisfying some additional requirements imposed by a target downstream
task. We propose a novel and principled approach to interpret and determine the
trade-off between the two task objectives. Specifically, we formulate the
customization problem as a Markov Decision Process (MDP) with a reward function
that combines 1) the inherent reward of the demonstration; and 2) the add-on
reward specified by the downstream task. We propose a novel framework, Residual
Q-learning, which can solve the formulated MDP by leveraging the prior policy
without knowing the inherent reward or value function of the prior policy. We
derive a family of residual Q-learning algorithms that can realize offline and
online policy customization, and show that the proposed algorithms can
effectively accomplish policy customization tasks in various environments. Demo
videos and code are available on our website:
https://sites.google.com/view/residualq-learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_H/0/1/0/all/0/1&quot;&gt;Haruki Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercat_J/0/1/0/all/0/1&quot;&gt;Jean Mercat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10345">
<title>Do as I can, not as I get. (arXiv:2306.10345v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10345</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a model called TMR to mine valuable information from
simulated data environments. We intend to complete the submission of this
paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shangfei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc Viet Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lei Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14448">
<title>Progressive Energy-Based Cooperative Learning for Multi-Domain Image-to-Image Translation. (arXiv:2306.14448v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14448</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a novel energy-based cooperative learning framework for
multi-domain image-to-image translation. The framework consists of four
components: descriptor, translator, style encoder, and style generator. The
descriptor is a multi-head energy-based model that represents a multi-domain
image distribution. The components of translator, style encoder, and style
generator constitute a diversified image generator. Specifically, given an
input image from a source domain, the translator turns it into a stylised
output image of the target domain according to a style code, which can be
inferred by the style encoder from a reference image or produced by the style
generator from a random noise. Since the style generator is represented as an
domain-specific distribution of style codes, the translator can provide a
one-to-many transformation (i.e., diversified generation) between source domain
and target domain. To train our framework, we propose a likelihood-based
multi-domain cooperative learning algorithm to jointly train the multi-domain
descriptor and the diversified image generator (including translator, style
encoder, and style generator modules) via multi-domain MCMC teaching, in which
the descriptor guides the diversified image generator to shift its probability
density toward the data distribution, while the diversified image generator
uses its randomly translated images to initialize the descriptor&apos;s Langevin
dynamics process for efficient sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Weinan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaxuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingnian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jianwen Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14685">
<title>DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14685</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though trained mainly on images, we discover that pretrained diffusion
models show impressive power in guiding sketch synthesis. In this paper, we
present DiffSketcher, an innovative algorithm that creates \textit{vectorized}
free-hand sketches using natural language input. DiffSketcher is developed
based on a pre-trained text-to-image diffusion model. It performs the task by
directly optimizing a set of B\&apos;ezier curves with an extended version of the
score distillation sampling (SDS) loss, which allows us to use a raster-level
diffusion model as a prior for optimizing a parametric vectorized sketch
generator. Furthermore, we explore attention maps embedded in the diffusion
model for effective stroke initialization to speed up the generation process.
The generated sketches demonstrate multiple levels of abstraction while
maintaining recognizability, underlying structure, and essential visual details
of the subject drawn. Our experiments show that DiffSketcher achieves greater
quality than prior work. The code and demo of DiffSketcher can be found at
https://ximinng.github.io/DiffSketcher-project/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Ximing Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haitao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16834">
<title>Intelligence of Astronomical Optical Telescope: Present Status and Future Perspectives. (arXiv:2306.16834v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16834</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence technology has been widely used in astronomy, and new
artificial intelligence technologies and application scenarios are constantly
emerging. There have been a large number of papers reviewing the application of
artificial intelligence technology in astronomy. However, relevant articles
seldom mention telescope intelligence separately, and it is difficult to
understand the current development status and research hotspots of telescope
intelligence from these papers. This paper combines the development history of
artificial intelligence technology and the difficulties of critical
technologies of telescopes, comprehensively introduces the development and
research hotspots of telescope intelligence, then conducts statistical analysis
on various research directions of telescope intelligence and defines the
research directions&apos; merits. All kinds of research directions are evaluated,
and the research trend of each telescope&apos;s intelligence is pointed out.
Finally, according to the advantages of artificial intelligence technology and
the development trend of telescopes, future research hotspots of telescope
intelligence are given.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tianzhu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jingyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Pang_X/0/1/0/all/0/1&quot;&gt;Xiushan Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yonghui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huaiqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiangqun Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17361">
<title>iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17361</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural causal models (SCMs) are widely used in various disciplines to
represent causal relationships among variables in complex systems.
Unfortunately, the underlying causal structure is often unknown, and estimating
it from data remains a challenging task. In many situations, however, the end
goal is to localize the changes (shifts) in the causal mechanisms between
related datasets instead of learning the full causal structure of the
individual datasets. Some applications include root cause analysis, analyzing
gene regulatory network structure changes between healthy and cancerous
individuals, or explaining distribution shifts. This paper focuses on
identifying the causal mechanism shifts in two or more related datasets over
the same set of variables -- without estimating the entire DAG structure of
each SCM. Prior work under this setting assumed linear models with Gaussian
noises; instead, in this work we assume that each SCM belongs to the more
general class of nonlinear additive noise models (ANMs). A key technical
contribution of this work is to show that the Jacobian of the score function
for the mixture distribution allows for the identification of shifts under
general non-parametric functional mechanisms. Once the shifted variables are
identified, we leverage recent work to estimate the structural differences, if
any, for the shifted variables. Experiments on synthetic and real-world data
are provided to showcase the applicability of this approach. Code implementing
the proposed method is open-source and publicly available at
https://github.com/kevinsbello/iSCAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_K/0/1/0/all/0/1&quot;&gt;Kevin Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1&quot;&gt;Bryon Aragam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03393">
<title>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes&apos; text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs. Our codes and
datasets are available at https://github.com/CurryTang/Graph-LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhikai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaochi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08927">
<title>Multi-Stage Cable Routing through Hierarchical Imitation Learning. (arXiv:2307.08927v5 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08927</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning to perform multi-stage robotic manipulation
tasks, with applications to cable routing, where the robot must route a cable
through a series of clips. This setting presents challenges representative of
complex multi-stage robotic manipulation scenarios: handling deformable
objects, closing the loop on visual perception, and handling extended behaviors
consisting of multiple steps that must be executed successfully to complete the
entire task. In such settings, learning individual primitives for each stage
that succeed with a high enough rate to perform a complete temporally extended
task is impractical: if each stage must be completed successfully and has a
non-negligible probability of failure, the likelihood of successful completion
of the entire task becomes negligible. Therefore, successful controllers for
such multi-stage tasks must be able to recover from failure and compensate for
imperfections in low-level controllers by smartly choosing which controllers to
trigger at any given time, retrying, or taking corrective action as needed. To
this end, we describe an imitation learning system that uses vision-based
policies trained from demonstrations at both the lower (motor control) and the
upper (sequencing) level, present a system for instantiating this method to
learn the cable routing task, and perform evaluations showing great performance
in generalizing to very challenging clip placement variations. Supplementary
videos, datasets, and code can be found at
https://sites.google.com/view/cablerouting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianlan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Charles Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xinyang Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1&quot;&gt;Gilbert Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1&quot;&gt;Liam Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaal_S/0/1/0/all/0/1&quot;&gt;Stefan Schaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12542">
<title>Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging. (arXiv:2307.12542v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12542</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent progress in enhancing the privacy of federated learning (FL)
via differential privacy (DP), the trade-off of DP between privacy protection
and performance is still underexplored for real-world medical scenario. In this
paper, we propose to optimize the trade-off under the context of client-level
DP, which focuses on privacy during communications. However, FL for medical
imaging involves typically much fewer participants (hospitals) than other
domains (e.g., mobile devices), thus ensuring clients be differentially private
is much more challenging. To tackle this problem, we propose an adaptive
intermediary strategy to improve performance without harming privacy.
Specifically, we theoretically find splitting clients into sub-clients, which
serve as intermediaries between hospitals and the server, can mitigate the
noises introduced by DP without harming privacy. Our proposed approach is
empirically evaluated on both classification and segmentation tasks using two
public datasets, and its effectiveness is demonstrated with significant
performance improvements and comprehensive analytical studies. Code is
available at: https://github.com/med-air/Client-DP-FL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Meirui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1&quot;&gt;Anjie Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13716">
<title>FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13716</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional federated learning uses the number of samples to calculate the
weights of each client model and uses this fixed weight value to fusion the
global model. However, in practical scenarios, each client&apos;s device and data
heterogeneity leads to differences in the quality of each client&apos;s model. Thus
the contribution to the global model is not wholly determined by the sample
size. In addition, if clients intentionally upload low-quality or malicious
models, using these models for aggregation will lead to a severe decrease in
global model accuracy. Traditional federated learning algorithms do not address
these issues. To solve this probelm, we propose FedDRL, a model fusion approach
using reinforcement learning based on a two staged approach. In the first
stage, Our method could filter out malicious models and selects trusted client
models to participate in the model fusion. In the second stage, the FedDRL
algorithm adaptively adjusts the weights of the trusted client models and
aggregates the optimal global model. We also define five model fusion scenarios
and compare our method with two baseline algorithms in those scenarios. The
experimental results show that our algorithm has higher reliability than other
algorithms while maintaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Leiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Cihao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Sibo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziling Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yuming Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheewei Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15220">
<title>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15220</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in surgical computer vision applications have been driven
by fully-supervised methods, primarily using only visual data. These methods
rely on manually annotated surgical videos to predict a fixed set of object
categories, limiting their generalizability to unseen surgical procedures and
downstream tasks. In this work, we put forward the idea that the surgical video
lectures available through open surgical e-learning platforms can provide
effective supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. SurgVLP
constructs a new contrastive learning objective to align video clip embeddings
with the corresponding multiple text embeddings by bringing them together
within a joint latent space. To effectively show the representation capability
of the learned joint latent space, we introduce several vision-and-language
tasks for surgery, such as text-based video retrieval, temporal activity
grounding, and video captioning, as benchmarks for evaluation. We further
demonstrate that without using any labeled ground truth, our approach can be
employed for traditional vision-only surgical downstream tasks, such as
surgical tool, phase, and triplet recognition. The code will be made available
at https://github.com/CAMMA-public/SurgVLP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1&quot;&gt;Vinkle Srivastav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavanchy_J/0/1/0/all/0/1&quot;&gt;Joel L. Lavanchy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16149">
<title>A Novel DDPM-based Ensemble Approach for Energy Theft Detection in Smart Grids. (arXiv:2307.16149v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16149</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy theft, characterized by manipulating energy consumption readings to
reduce payments, poses a dual threat-causing financial losses for grid
operators and undermining the performance of smart grids. Effective Energy
Theft Detection (ETD) methods become crucial in mitigating these risks by
identifying such fraudulent activities in their early stages. However, the
majority of current ETD methods rely on supervised learning, which is hindered
by the difficulty of labelling data and the risk of overfitting known attacks.
To address these challenges, several unsupervised ETD methods have been
proposed, focusing on learning the normal patterns from honest users,
specifically the reconstruction of input. However, our investigation reveals a
limitation in current unsupervised ETD methods, as they can only detect
anomalous behaviours in users exhibiting regular patterns. Users with
high-variance behaviours pose a challenge to these methods. In response, this
paper introduces a Denoising Diffusion Probabilistic Model (DDPM)-based ETD
approach. This innovative approach demonstrates impressive ETD performance on
high-variance smart grid data by incorporating additional attributes correlated
with energy consumption. The proposed methods improve the average ETD
performance on high-variance smart grid data from below 0.5 to over 0.9 w.r.t.
AUC. On the other hand, our experimental findings indicate that while the
state-of-the-art ETD methods based on reconstruction error can identify ETD
attacks for the majority of users, they prove ineffective in detecting attacks
for certain users. To address this, we propose a novel ensemble approach that
considers both reconstruction error and forecasting error, enhancing the
robustness of the ETD methodology. The proposed ensemble method improves the
average ETD performance on the stealthiest attacks from nearly 0 to 0.5 w.r.t.
5%-TPR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1&quot;&gt;Asif Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gope_P/0/1/0/all/0/1&quot;&gt;Prosanta Gope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikdar_B/0/1/0/all/0/1&quot;&gt;Biplab Sikdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04586">
<title>Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v16 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04586</link>
<description rdf:parseType="Literal">&lt;p&gt;Mainstream approaches for creating AIs include deep learning and generative
approaches (e.g., large language models) and manually constructed symbolic
approaches. These approaches have led to valuable AI systems and impressive
feats, but they can create risks when their operations affect people. Manually
constructed AIs are brittle even in circumscribed domains. Generative AIs can
make strange mistakes and not notice them. Today, these AIs cannot be
instructed easily, fail to use common sense, lack curiosity, and lack social
alignment. Developmental AI is a bootstrapping approach that uses embodied AIs.
The AIs start with innate competences and learn by interacting with their
environment. The AIs develop abilities in small steps along a bio-inspired
trajectory. Developmental AIs have shown capabilities for multimodal
perception, object recognition, and manipulation. Computational models for
hierarchical planning, abstraction discovery, curiosity, and language
acquisition exist but need to be adapted to an embodied approach. This research
aims to produce AIs that learn to communicate, establish common ground, read
critically, consider the provenance of information, test hypotheses, and
collaborate. However, developmental AI systems have not yet passed the
abilities of young children. They need to bridge competence gaps involving
nonverbal communication, speech, reading, and writing. Scaling to practical
applications also requires reducing hardware costs. This position paper lays
out prospects, gaps, and challenges for this approach. The ambition is to
create data-rich experientially based foundation models for human-compatible,
resilient, and trustworthy AIs. The AIs would learn, share what they learn, and
collaborate to achieve high standards. The approach would make AI technology
more democratic and enable more people to train, test, build on, and replicate
AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1&quot;&gt;Robert Price&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05027">
<title>VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching. (arXiv:2309.05027v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05027</link>
<description rdf:parseType="Literal">&lt;p&gt;Although diffusion models in text-to-speech have become a popular choice due
to their strong generative ability, the intrinsic complexity of sampling from
diffusion models harms their efficiency. Alternatively, we propose VoiceFlow,
an acoustic model that utilizes a rectified flow matching algorithm to achieve
high synthesis quality with a limited number of sampling steps. VoiceFlow
formulates the process of generating mel-spectrograms into an ordinary
differential equation conditional on text inputs, whose vector field is then
estimated. The rectified flow technique then effectively straightens its
sampling trajectory for efficient synthesis. Subjective and objective
evaluations on both single and multi-speaker corpora showed the superior
synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation
studies further verified the validity of the rectified flow technique in
VoiceFlow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chenpeng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Ziyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09552">
<title>CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09552</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end automatic speech recognition (ASR) systems often struggle to
recognize rare name entities, such as personal names, organizations, and
terminologies not frequently encountered in the training data. This paper
presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on
OpenAI&apos;s Whisper model that can recognize user-defined name entities by
performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of
Whisper encoder. The recognized entities are used as prompts for the Whisper
decoder. We first propose a multitask training approach with OV-KWS and ASR
tasks to optimize the model. Experiments show that this approach substantially
improves the entity recalls compared to the original Whisper model on Chinese
Aishell hot word subsets and two internal code-switch test sets. However, we
observed a slight increase in mixed-error-rate (MER) on internal test sets due
to catastrophic forgetting. To address this problem and use different sizes of
the Whisper model without finetuning, we propose to use OV-KWS as a separate
module and construct a spoken form prompt to prevent hallucination. The OV-KWS
module consistently improves MER and Entity Recall for whisper-small, medium,
and large models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinglu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1&quot;&gt;Chang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengxin Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1&quot;&gt;Xiaosong Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piao_M/0/1/0/all/0/1&quot;&gt;Mengyao Piao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiawei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1&quot;&gt;Xinglin Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Miaomiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yanqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09677">
<title>Single and Few-step Diffusion for Generative Speech Enhancement. (arXiv:2309.09677v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09677</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have shown promising results in speech enhancement, using a
task-adapted diffusion process for the conditional generation of clean speech
given a noisy mixture. However, at test time, the neural network used for score
estimation is called multiple times to solve the iterative reverse process.
This results in a slow inference process and causes discretization errors that
accumulate over the sampling trajectory. In this paper, we address these
limitations through a two-stage training approach. In the first stage, we train
the diffusion model the usual way using the generative denoising score matching
loss. In the second stage, we compute the enhanced signal by solving the
reverse process and compare the resulting estimate to the clean speech target
using a predictive loss. We show that using this second training stage enables
achieving the same performance as the baseline model using only 5 function
evaluations instead of 60 function evaluations. While the performance of usual
generative diffusion algorithms drops dramatically when lowering the number of
function evaluations (NFEs) to obtain single-step diffusion, we show that our
proposed method keeps a steady performance and therefore largely outperforms
the diffusion baseline in this setting and also generalizes better than its
predictive counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lay_B/0/1/0/all/0/1&quot;&gt;Bunlong Lay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lemercier_J/0/1/0/all/0/1&quot;&gt;Jean-Marie Lemercier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Richter_J/0/1/0/all/0/1&quot;&gt;Julius Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1&quot;&gt;Timo Gerkmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12678">
<title>QAL-BP: An Augmented Lagrangian Quantum Approach for Bin Packing. (arXiv:2309.12678v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12678</link>
<description rdf:parseType="Literal">&lt;p&gt;The bin packing is a well-known NP-Hard problem in the domain of artificial
intelligence, posing significant challenges in finding efficient solutions.
Conversely, recent advancements in quantum technologies have shown promising
potential for achieving substantial computational speedup, particularly in
certain problem classes, such as combinatorial optimization. In this study, we
introduce QAL-BP, a novel Quadratic Unconstrained Binary Optimization (QUBO)
formulation designed specifically for bin packing and suitable for quantum
computation. QAL-BP utilizes the Augmented Lagrangian method to incorporate the
bin packing constraints into the objective function while also facilitating an
analytical estimation of heuristic, but empirically robust, penalty
multipliers. This approach leads to a more versatile and generalizable model
that eliminates the need for empirically calculating instance-dependent
Lagrangian coefficients, a requirement commonly encountered in alternative QUBO
formulations for similar problems. To assess the effectiveness of our proposed
approach, we conduct experiments on a set of bin packing instances using a real
Quantum Annealing device. Additionally, we compare the results with those
obtained from two different classical solvers, namely simulated annealing and
Gurobi. The experimental findings not only confirm the correctness of the
proposed formulation but also demonstrate the potential of quantum computation
in effectively solving the bin packing problem, particularly as more reliable
quantum technology becomes available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cellini_L/0/1/0/all/0/1&quot;&gt;Lorenzo Cellini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Macaluso_A/0/1/0/all/0/1&quot;&gt;Antonio Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lombardi_M/0/1/0/all/0/1&quot;&gt;Michele Lombardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00100">
<title>Multilingual Natural Language Processing Model for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00100</link>
<description rdf:parseType="Literal">&lt;p&gt;The impression section of a radiology report summarizes important radiology
findings and plays a critical role in communicating these findings to
physicians. However, the preparation of these summaries is time-consuming and
error-prone for radiologists. Recently, numerous models for radiology report
summarization have been developed. Nevertheless, there is currently no model
that can summarize these reports in multiple languages. Such a model could
greatly improve future research and the development of Deep Learning models
that incorporate data from patients with different ethnic backgrounds. In this
study, the generation of radiology impressions in different languages was
automated by fine-tuning a model, publicly available, based on a multilingual
text-to-text Transformer to summarize findings available in English,
Portuguese, and German radiology reports. In a blind test, two board-certified
radiologists indicated that for at least 70% of the system-generated summaries,
the quality matched or exceeded the corresponding human-written summaries,
suggesting substantial clinical reliability. Furthermore, this study showed
that the multilingual model outperformed other models that specialized in
summarizing radiology reports in only one language, as well as models that were
not specifically designed for summarizing radiology reports, such as ChatGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindo_M/0/1/0/all/0/1&quot;&gt;Mariana Lindo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1&quot;&gt;Ana Sofia Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1&quot;&gt;Gijs Luijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_G/0/1/0/all/0/1&quot;&gt;Gustavo Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Moon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaarschmidt_B/0/1/0/all/0/1&quot;&gt;Benedikt Michael Schaarschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deuschl_C/0/1/0/all/0/1&quot;&gt;Cornelius Deuschl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haubold_J/0/1/0/all/0/1&quot;&gt;Johannes Haubold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_V/0/1/0/all/0/1&quot;&gt;Victor Alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06114">
<title>Learning Interactive Real-World Simulators. (arXiv:2310.06114v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06114</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models trained on internet data have revolutionized how text,
image, and video content can be created. Perhaps the next milestone for
generative models is to simulate realistic experience in response to actions
taken by humans, robots, and other interactive agents. Applications of a
real-world simulator range from controllable content creation in games and
movies, to training embodied agents purely in simulation that can be directly
deployed in the real world. We explore the possibility of learning a universal
simulator of real-world interaction through generative modeling. We first make
the important observation that natural datasets available for learning a
real-world simulator are often rich along different dimensions (e.g., abundant
objects in image data, densely sampled actions in robotics data, and diverse
movements in navigation data). With careful orchestration of diverse datasets,
each providing a different aspect of the overall experience, we can simulate
the visual outcome of both high-level instructions such as ``open the drawer&apos;&apos;
and low-level controls such as &quot;move by x, y&quot; from otherwise static scenes and
objects. We use the simulator to train both high-level vision-language policies
and low-level reinforcement learning policies, each of which can be deployed in
the real world in zero shot after training purely in simulation. We also show
that other types of intelligence such as video captioning models can benefit
from training with simulated experience, opening up even wider applications.
Video demos can be found at https://universal-simulator.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengjiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemipour_K/0/1/0/all/0/1&quot;&gt;Kamyar Ghasemipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1&quot;&gt;Jonathan Tompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07932">
<title>What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07932</link>
<description rdf:parseType="Literal">&lt;p&gt;When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human&apos;s input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL&apos;s reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot&apos;s.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Ran Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1&quot;&gt;Andrea Bajcsy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08279">
<title>Can Text-based Knowledge Graph Completion Benefit From Zero-Shot Large Language Models?. (arXiv:2310.08279v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08279</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based knowledge graph completion (KGC) methods, leveraging textual
entity descriptions are at the research forefront. The efficacy of these models
hinges on the quality of the textual data. This study explores whether enriched
or more efficient textual descriptions can amplify model performance. Recently,
Large Language Models (LLMs) have shown remarkable improvements in NLP tasks,
attributed to their sophisticated text generation and conversational
capabilities. LLMs assimilate linguistic patterns and integrate knowledge from
their training data. Compared to traditional databases like Wikipedia, LLMs
provide several advantages, facilitating broader information querying and
content augmentation. We hypothesize that LLMs, without fine-tuning, can refine
entity descriptions, serving as an auxiliary knowledge source. An in-depth
analysis was conducted to verify this hypothesis. We found that (1) without
fine-tuning, LLMs have the capability to further improve the quality of entity
text descriptions. We validated this through experiments on the FB15K-237 and
WN18RR datasets. (2) LLMs exhibit text generation hallucination issues and
selectively output words with multiple meanings. This was mitigated by
contextualizing prompts to constrain LLM outputs. (3) Larger model sizes do not
necessarily guarantee better performance; even the 7B model can achieve
optimized results in this comparative task. These findings underscore the
untapped potential of large models in text-based KGC, which is a promising
direction for further research in KGC. The code and datasets are accessible at
\href{https://github.com/sjlmg/CP-KGC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Li Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09605">
<title>Penetrative AI: Making LLMs Comprehend the Physical World. (arXiv:2310.09605v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09605</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in Large Language Models (LLMs) have demonstrated their
remarkable capabilities across a range of tasks. Questions, however, persist
about the nature of LLMs and their potential to integrate common-sense human
knowledge when performing tasks involving information about the real physical
world. This paper delves into these questions by exploring how LLMs can be
extended to interact with and reason about the physical world through IoT
sensors and actuators, a concept that we term &quot;Penetrative AI&quot;. The paper
explores such an extension at two levels of LLMs&apos; ability to penetrate into the
physical world via the processing of sensory signals. Our preliminary findings
indicate that LLMs, with ChatGPT being the representative example in our
exploration, have considerable and unique proficiency in employing the embedded
world knowledge for interpreting IoT sensor data and reasoning over them about
tasks in the physical realm. Not only this opens up new applications for LLMs
beyond traditional text-based tasks, but also enables new ways of incorporating
human knowledge in cyber-physical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huatao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Liying Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1&quot;&gt;Mani Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16221">
<title>Hierarchical Randomized Smoothing. (arXiv:2310.16221v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16221</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1&quot;&gt;Yan Scholten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1&quot;&gt;Jan Schuchardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Bojchevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17072">
<title>IMMP++: Isometric Motion Manifold Primitives with Parametric Curve Models. (arXiv:2310.17072v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17072</link>
<description rdf:parseType="Literal">&lt;p&gt;The Motion Manifold Primitive (MMP) produces, for a given task, a continuous
manifold of trajectories, each of which can successfully complete the task,
addressing the challenge of high dimensionality in trajectory data. However,
the discrete-time trajectory representations used in existing MMP methods lack
important functionalities of movement primitives (e.g., temporal modulation,
via-points modulation, etc.) found in other conventional methods that employ
parametric curve representations. To address these limitations, we introduce
Motion Manifold Primitives++ (MMP++), which combines the advantages of the MMP
and conventional methods by applying the MMP framework to the parametric curve
representations. However, we observe that the performance of MMP++ can
sometimes degrade significantly due to geometric distortion in the latent space
-- by distortion, we mean that similar motions are not located nearby in the
latent space. To mitigate this issue, we propose Isometric Motion Manifold
Primitives++ (IMMP++), where the latent coordinate space preserves the geometry
of the manifold. Experimental results with 2-DoF planar motions and 7-DoF robot
arm tasks demonstrate that MMP++ and IMMP++ outperform existing methods, in
some cases by a significant margin, while maintaining the advantages of
parametric curve representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yonghyeon Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18341">
<title>CXR-LLAVA: a multimodal large language model for interpreting chest X-ray images. (arXiv:2310.18341v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18341</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: This study aimed to develop an open-source multimodal large language
model (CXR-LLAVA) for interpreting chest X-ray images (CXRs), leveraging recent
advances in large language models (LLMs) to potentially replicate the image
interpretation skills of human radiologists Materials and Methods: For
training, we collected 592,580 publicly available CXRs, of which 374,881 had
labels for certain radiographic abnormalities (Dataset 1) and 217,699 provided
free-text radiology reports (Dataset 2). After pre-training a vision
transformer with Dataset 1, we integrated it with an LLM influenced by the
LLAVA network. Then, the model was fine-tuned, primarily using Dataset 2. The
model&apos;s diagnostic performance for major pathological findings was evaluated,
along with the acceptability of radiologic reports by human radiologists, to
gauge its potential for autonomous reporting. Results: The model demonstrated
impressive performance in test sets, achieving an average F1 score of 0.81 for
six major pathological findings in the MIMIC internal test set and 0.62 for
seven major pathological findings in the external test set. The model&apos;s F1
scores surpassed those of GPT-4-vision and Gemini-Pro-Vision in both test sets.
In human radiologist evaluations of the external test set, the model achieved a
72.7% success rate in autonomous reporting, slightly below the 84.0% rate of
ground truth reports. Conclusion: This study highlights the significant
potential of multimodal LLMs for CXR interpretation, while also acknowledging
the performance limitations. Despite these challenges, we believe that making
our model open-source will catalyze further research, expanding its
effectiveness and applicability in various clinical contexts. CXR-LLAVA is
available at https://github.com/ECOFRI/CXR_LLAVA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seowoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youn_J/0/1/0/all/0/1&quot;&gt;Jiwon Youn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungjin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Mansu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Soon Ho Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19273">
<title>The Memory Perturbation Equation: Understanding Model&apos;s Sensitivity to Data. (arXiv:2310.19273v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19273</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding model&apos;s sensitivity to its training data is crucial but can
also be challenging and costly, especially during training. To simplify such
issues, we present the Memory-Perturbation Equation (MPE) which relates model&apos;s
sensitivity to perturbation in its training data. Derived using Bayesian
principles, the MPE unifies existing sensitivity measures, generalizes them to
a wide-variety of models and algorithms, and unravels useful properties
regarding sensitivities. Our empirical results show that sensitivity estimates
obtained during training can be used to faithfully predict generalization on
unseen test data. The proposed equation is expected to be useful for future
research on robust and adaptive learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nickl_P/0/1/0/all/0/1&quot;&gt;Peter Nickl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tailor_D/0/1/0/all/0/1&quot;&gt;Dharmesh Tailor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollenhoff_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19680">
<title>Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19680</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies have been
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes PLM-integrated NMT
(PiNMT) model to overcome the identified problems. PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieve
state-of-the-art performance on the IWSLT&apos;14 En$\leftrightarrow$De dataset.
This study&apos;s outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Soon-Jae Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Chang-Sung Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20501">
<title>LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. (arXiv:2310.20501v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20501</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the emergence of large language models (LLMs) has revolutionized
the paradigm of information retrieval (IR) applications, especially in web
search. With their remarkable capabilities in generating human-like texts, LLMs
have created enormous texts on the Internet. As a result, IR systems in the
LLMs era are facing a new challenge: the indexed documents now are not only
written by human beings but also automatically generated by the LLMs. How these
LLM-generated documents influence the IR systems is a pressing and still
unexplored question. In this work, we conduct a quantitative evaluation of
different IR models in scenarios where both human-written and LLM-generated
texts are involved. Surprisingly, our findings indicate that neural retrieval
models tend to rank LLM-generated documents higher. We refer to this category
of biases in neural retrieval models towards the LLM-generated text as the
\textbf{source bias}. Moreover, we discover that this bias is not confined to
the first-stage neural retrievers, but extends to the second-stage neural
re-rankers. Then, we provide an in-depth analysis from the perspective of text
compression and observe that neural models can better understand the semantic
information of LLM-generated text, which is further substantiated by our
theoretical analysis. To mitigate the source bias, we also propose a
plug-and-play debiased constraint for the optimization objective, and
experimental results show the effectiveness. Finally, we discuss the potential
severe concerns stemming from the observed source bias and hope our findings
can serve as a critical wake-up call to the IR community and beyond. To
facilitate future explorations of IR in the LLM era, the constructed two new
benchmarks and codes will later be available at
\url{https://github.com/KID-22/LLM4IR-Bias}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1&quot;&gt;Sunhao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1&quot;&gt;Liang Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01017">
<title>Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01017</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lunjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02794">
<title>Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder. (arXiv:2311.02794v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02794</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models of observations under interventions have been a vibrant
topic of interest across machine learning and the sciences in recent years. For
example, in drug discovery, there is a need to model the effects of diverse
interventions on cells in order to characterize unknown biological mechanisms
of action. We propose the Sparse Additive Mechanism Shift Variational
Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and
interpretability for perturbation models. SAMS-VAE models the latent state of a
perturbed sample as the sum of a local latent variable capturing
sample-specific variation and sparse global variables of latent intervention
effects. Crucially, SAMS-VAE sparsifies these global latent variables for
individual perturbations to identify disentangled, perturbation-specific latent
subspaces that are flexibly composable. We evaluate SAMS-VAE both
quantitatively and qualitatively on a range of tasks using two popular single
cell sequencing datasets. In order to measure perturbation-specific
model-properties, we also introduce a framework for evaluation of perturbation
models based on average treatment effects with links to posterior predictive
checks. SAMS-VAE outperforms comparable models in terms of generalization
across in-distribution and out-of-distribution tasks, including a combinatorial
reasoning task under resource paucity, and yields interpretable latent
structures which correlate strongly to known biological mechanisms. Our results
suggest SAMS-VAE is an interesting addition to the modeling toolkit for machine
learning-driven scientific discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bereket_M/0/1/0/all/0/1&quot;&gt;Michael Bereket&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1&quot;&gt;Theofanis Karaletsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03220">
<title>ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03220</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Alympics (Olympics for Agents), a systematic simulation
framework utilizing Large Language Model (LLM) agents for game theory research.
Alympics creates a versatile platform for studying complex game theory
problems, bridging the gap between theoretical game theory and empirical
investigations by providing a controlled environment for simulating human-like
strategic interactions with LLM agents. In our pilot case study, the &quot;Water
Allocation Challenge,&quot; we explore Alympics through a challenging strategic game
focused on the multi-round auction on scarce survival resources. This study
demonstrates the framework&apos;s ability to qualitatively and quantitatively
analyze game determinants, strategies, and outcomes. Additionally, we conduct a
comprehensive human assessment and an in-depth evaluation of LLM agents in
strategic decision-making scenarios. Our findings not only expand the
understanding of LLM agents&apos; proficiency in emulating human strategic behavior
but also highlight their potential in advancing game theory knowledge, thereby
enriching our understanding of both game theory and empowering further research
into strategic decision-making domains with LLM agents. Codes, prompts, and all
related resources are available at https://github.com/microsoft/Alympics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shaoguang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenshan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fengyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04131">
<title>Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04131</link>
<description rdf:parseType="Literal">&lt;p&gt;While transformer models exhibit strong capabilities on linguistic tasks,
their complex architectures make them difficult to interpret. Recent work has
aimed to reverse engineer transformer models into human-readable
representations called circuits that implement algorithmic functions. We extend
this research by analyzing and comparing circuits for similar sequence
continuation tasks, which include increasing sequences of digits, number words,
and months. Through the application of circuit analysis techniques, we identify
key sub-circuits responsible for detecting sequence members and for predicting
the next member in a sequence. Our analysis reveals that semantically related
sequences rely on shared circuit subgraphs with analogous roles. Overall,
documenting shared computational structures enables better prediction of model
behaviors, identification of errors, and safer editing procedures. This
mechanistic understanding of transformers is a critical step towards building
more robust, aligned, and interpretable language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1&quot;&gt;Michael Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05546">
<title>Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization. (arXiv:2311.05546v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05546</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Agent Reinforcement Learning is becoming increasingly more important in
times of autonomous driving and other smart industrial applications.
Simultaneously a promising new approach to Reinforcement Learning arises using
the inherent properties of quantum mechanics, reducing the trainable parameters
of a model significantly. However, gradient-based Multi-Agent Quantum
Reinforcement Learning methods often have to struggle with barren plateaus,
holding them back from matching the performance of classical approaches. We
build upon an existing approach for gradient free Quantum Reinforcement
Learning and propose three genetic variations with Variational Quantum Circuits
for Multi-Agent Reinforcement Learning using evolutionary optimization. We
evaluate our genetic variations in the Coin Game environment and also compare
them to classical approaches. We showed that our Variational Quantum Circuit
approaches perform significantly better compared to a neural network with a
similar amount of trainable parameters. Compared to the larger neural network,
our approaches archive similar results using $97.88\%$ less parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kolle_M/0/1/0/all/0/1&quot;&gt;Michael K&amp;#xf6;lle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Topp_F/0/1/0/all/0/1&quot;&gt;Felix Topp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thomy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Altmann_P/0/1/0/all/0/1&quot;&gt;Philipp Altmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Nusslein_J/0/1/0/all/0/1&quot;&gt;Jonas N&amp;#xfc;&amp;#xdf;lein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1&quot;&gt;Claudia Linnhoff-Popien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07377">
<title>Testing learning-enabled cyber-physical systems with Large-Language Models: A Formal Approach. (arXiv:2311.07377v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07377</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of machine learning (ML) into cyber-physical systems (CPS)
offers significant benefits, including enhanced efficiency, predictive
capabilities, real-time responsiveness, and the enabling of autonomous
operations. This convergence has accelerated the development and deployment of
a range of real-world applications, such as autonomous vehicles, delivery
drones, service robots, and telemedicine procedures. However, the software
development life cycle (SDLC) for AI-infused CPS diverges significantly from
traditional approaches, featuring data and learning as two critical components.
Existing verification and validation techniques are often inadequate for these
new paradigms. In this study, we pinpoint the main challenges in ensuring
formal safety for learningenabled CPS.We begin by examining testing as the most
pragmatic method for verification and validation, summarizing the current
state-of-the-art methodologies. Recognizing the limitations in current testing
approaches to provide formal safety guarantees, we propose a roadmap to
transition from foundational probabilistic testing to a more rigorous approach
capable of delivering formal assurance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mok_A/0/1/0/all/0/1&quot;&gt;Aloysius K. Mok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piskac_R/0/1/0/all/0/1&quot;&gt;Ruzica Piskac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamachari_B/0/1/0/all/0/1&quot;&gt;Bhaskar Krishnamachari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dakai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolsky_O/0/1/0/all/0/1&quot;&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insup Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11101">
<title>$\varepsilon$-fractional Core Stability in Hedonic Games. (arXiv:2311.11101v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11101</link>
<description rdf:parseType="Literal">&lt;p&gt;Hedonic Games (HGs) are a classical framework modeling coalition formation of
strategic agents guided by their individual preferences. According to these
preferences, it is desirable that a coalition structure (i.e. a partition of
agents into coalitions) satisfies some form of stability. The most well-known
and natural of such notions is arguably core-stability. Informally, a partition
is core-stable if no subset of agents would like to deviate by regrouping in a
so-called core-blocking coalition. Unfortunately, core-stable partitions seldom
exist and even when they do, it is often computationally intractable to find
one. To circumvent these problems, we propose the notion of
$\varepsilon$-fractional core-stability, where at most an
$\varepsilon$-fraction of all possible coalitions is allowed to core-block. It
turns out that such a relaxation may guarantee both existence and
polynomial-time computation. Specifically, we design efficient algorithms
returning an $\varepsilon$-fractional core-stable partition, with $\varepsilon$
exponentially decreasing in the number of agents, for two fundamental classes
of HGs: Simple Fractional and Anonymous. From a probabilistic point of view,
being the definition of $\varepsilon$-fractional core equivalent to requiring
that uniformly sampled coalitions core-block with probability lower than
$\varepsilon$, we further extend the definition to handle more complex sampling
distributions. Along this line, when valuations have to be learned from samples
in a PAC-learning fashion, we give positive and negative results on which
distributions allow the efficient computation of outcomes that are
$\varepsilon$-fractional core-stable with arbitrarily high confidence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fioravanti_S/0/1/0/all/0/1&quot;&gt;Simone Fioravanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flammini_M/0/1/0/all/0/1&quot;&gt;Michele Flammini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodric_B/0/1/0/all/0/1&quot;&gt;Bojana Kodric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varricchio_G/0/1/0/all/0/1&quot;&gt;Giovanna Varricchio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13326">
<title>Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13326</link>
<description rdf:parseType="Literal">&lt;p&gt;Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1&quot;&gt;Woosung Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1&quot;&gt;Insu Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yuntae Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Gimin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Chang Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14084">
<title>AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14084</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon causes source bias in text retrieval for web search.
Specifically, neural retrieval models tend to rank generated texts higher than
human-written texts. In this paper, we extend the study of this bias to
cross-modal retrieval. Firstly, we successfully construct a suitable benchmark
to explore the existence of the bias. Subsequent extensive experiments on this
benchmark reveal that AI-generated images introduce an invisible relevance bias
to text-image retrieval models. Specifically, our experiments show that
text-image retrieval models tend to rank the AI-generated images higher than
the real images, even though the AI-generated images do not exhibit more
visually relevant features to the query than real images. This invisible
relevance bias is prevalent across retrieval models with varying training data
and architectures. Furthermore, our subsequent exploration reveals that the
inclusion of AI-generated images in the training data of the retrieval models
exacerbates the invisible relevance bias. The above phenomenon triggers a
vicious cycle, which makes the invisible relevance bias become more and more
serious. To elucidate the potential causes of invisible relevance and address
the aforementioned issues, we introduce an effective training method aimed at
alleviating the invisible relevance bias. Subsequently, we apply our proposed
debiasing method to retroactively identify the causes of invisible relevance,
revealing that the AI-generated images induce the image encoder to embed
additional information into their representation. This information exhibits a
certain consistency across generated images with different semantics and can
make the retriever estimate a higher relevance score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shicheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1&quot;&gt;Danyang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1&quot;&gt;Liang Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jingcheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14656">
<title>Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. (arXiv:2311.14656v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14656</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal large language models (MLLMs) have shown remarkable capabilities
across a broad range of tasks but their knowledge and abilities in the
geographic and geospatial domains are yet to be explored, despite potential
wide-ranging benefits to navigation, environmental research, urban development,
and disaster response. We conduct a series of experiments exploring various
vision capabilities of MLLMs within these domains, particularly focusing on the
frontier model GPT-4V, and benchmark its performance against open-source
counterparts. Our methodology involves challenging these models with a
small-scale geographic benchmark consisting of a suite of visual tasks, testing
their abilities across a spectrum of complexity. The analysis uncovers not only
where such models excel, including instances where they outperform humans, but
also where they falter, providing a balanced view of their capabilities in the
geographic domain. To enable the comparison and evaluation of future models,
our benchmark will be publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1&quot;&gt;Jonathan Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luddecke_T/0/1/0/all/0/1&quot;&gt;Timo L&amp;#xfc;ddecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1&quot;&gt;Rehan Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kai Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1&quot;&gt;Samuel Albanie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15565">
<title>Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15565</link>
<description rdf:parseType="Literal">&lt;p&gt;My research investigates the use of cutting-edge hybrid deep learning models
to accurately differentiate between AI-generated text and human writing. I
applied a robust methodology, utilising a carefully selected dataset comprising
AI and human texts from various sources, each tagged with instructions.
Advanced natural language processing techniques facilitated the analysis of
textual features. Combining sophisticated neural networks, the custom model
enabled it to detect nuanced differences between AI and human content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1&quot;&gt;Abiodun Finbarrs Oketunji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16167">
<title>Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16167</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an end-to-end adaptive sampling neural network
(MMPDE-Net) based on the moving mesh method, which can adaptively generate new
sampling points by solving the moving mesh PDE. This model focuses on improving
the quality of sampling points generation. Moreover, we develop an iterative
algorithm based on MMPDE-Net, which makes the sampling points more precise and
controllable. Since MMPDE-Net is a framework independent of the deep learning
solver, we combine it with physics-informed neural networks (PINN) to propose
moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error
analysis under some assumptions. Finally, we demonstrate the performance
improvement of MS-PINN compared to PINN through numerical experiments of four
typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yangtao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiaolin He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16716">
<title>GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16716</link>
<description rdf:parseType="Literal">&lt;p&gt;GNN-based recommenders have excelled in modeling intricate user-item
interactions through multi-hop message passing. However, existing methods often
overlook the dynamic nature of evolving user-item interactions, which impedes
the adaption to changing user preferences and distribution shifts in newly
arriving data. Thus, their scalability and performances in real-world dynamic
environments are limited. In this study, we propose GraphPro, a framework that
incorporates parameter-efficient and dynamic graph pre-training with prompt
learning. This novel combination empowers GNNs to effectively capture both
long-term user preferences and short-term behavior dynamics, enabling the
delivery of accurate and timely recommendations. Our GraphPro framework
addresses the challenge of evolving user preferences by seamlessly integrating
a temporal prompt mechanism and a graph-structural prompt learning mechanism
into the pre-trained GNN model. The temporal prompt mechanism encodes time
information on user-item interaction, allowing the model to naturally capture
temporal context, while the graph-structural prompt learning mechanism enables
the transfer of pre-trained knowledge to adapt to behavior dynamics without the
need for continuous incremental training. We further bring in a dynamic
evaluation setting for recommendation to mimic real-world dynamic scenarios and
bridge the offline-online gap to a better level. Our extensive experiments
including a large-scale industrial deployment showcases the lightweight plug-in
scalability of our GraphPro when integrated with various state-of-the-art
recommenders, emphasizing the advantages of GraphPro in terms of effectiveness,
robustness and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Da Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kangyi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18424">
<title>Investigating Collaborative Data Practices: a Case Study on Artificial Intelligence for Healthcare Research. (arXiv:2311.18424v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18424</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing artificial intelligence (AI) tools for healthcare is a
collaborative effort, bringing data scientists, clinicians, patients and other
disciplines together. In this paper, we explore the collaborative data
practices of research consortia tasked with applying AI tools to understand and
manage multiple long-term conditions in the UK. Through an inductive thematic
analysis of 13 semi-structured interviews with participants of these consortia,
we aimed to understand how collaboration happens based on the tools used,
communication processes and settings, as well as the conditions and obstacles
for collaborative work. Our findings reveal the adaptation of tools that are
used for sharing knowledge and the tailoring of information based on the
audience, particularly those from a clinical or patient perspective.
Limitations on the ability to do this were also found to be imposed by the use
of electronic healthcare records and access to datasets. We identified meetings
as the key setting for facilitating exchanges between disciplines and allowing
for the blending and creation of knowledge. Finally, we bring to light the
conditions needed to facilitate collaboration and discuss how some of the
challenges may be navigated in future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henkin_R/0/1/0/all/0/1&quot;&gt;Rafael Henkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remfry_E/0/1/0/all/0/1&quot;&gt;Elizabeth Remfry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reynolds_D/0/1/0/all/0/1&quot;&gt;Duncan J. Reynolds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clinch_M/0/1/0/all/0/1&quot;&gt;Megan Clinch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1&quot;&gt;Michael R. Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01520">
<title>Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian networks (BNs) are a foundational model in machine learning and
causal inference. Their graphical structure can handle high-dimensional
problems, divide them into a sparse collection of smaller ones, underlies Judea
Pearl&apos;s causality, and determines their explainability and interpretability.
Despite their popularity, there are almost no resources in the literature on
how to compute Shannon&apos;s entropy and the Kullback-Leibler (KL) divergence for
BNs under their most common distributional assumptions. In this paper, we
provide computationally efficient algorithms for both by leveraging BNs&apos;
graphical structure, and we illustrate them with a complete set of numerical
examples. In the process, we show it is possible to reduce the computational
complexity of KL from cubic to quadratic for Gaussian BNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03006">
<title>Multi-Weight Ranking for Multi-Criteria Decision Making. (arXiv:2312.03006v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03006</link>
<description rdf:parseType="Literal">&lt;p&gt;Cone distribution functions from statistics are turned into Multi-Criteria
Decision Making tools. It is demonstrated that this procedure can be considered
as an upgrade of the weighted sum scalarization insofar as it absorbs a whole
collection of weighted sum scalarizations at once instead of fixing a
particular one in advance. As examples show, this type of scalarization--in
contrast to a pure weighted sum scalarization-is also able to detect
``non-convex&quot; parts of the Pareto frontier. Situations are characterized in
which different types of rank reversal occur, and it is explained why this
might even be useful for analyzing the ranking procedure. The ranking functions
are then extended to sets providing unary indicators for set preferences which
establishes, for the first time, the link between set optimization methods and
set-based multi-objective optimization. A potential application in machine
learning is outlined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamel_A/0/1/0/all/0/1&quot;&gt;Andreas H Hamel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostner_D/0/1/0/all/0/1&quot;&gt;Daniel Kostner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03365">
<title>Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks. (arXiv:2312.03365v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03365</link>
<description rdf:parseType="Literal">&lt;p&gt;Controlling energy consumption in buildings through demand response (DR) has
become increasingly important to reduce global carbon emissions and limit
climate change. In this paper, we specifically focus on controlling the heating
system of a residential building to optimize its energy consumption while
respecting user&apos;s thermal comfort. Recent works in this area have mainly
focused on either model-based control, e.g., model predictive control (MPC), or
model-free reinforcement learning (RL) to implement practical DR algorithms. A
specific RL method that recently has achieved impressive success in domains
such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for
building control it has remained largely unexplored. Thus, we study MCTS
specifically for building demand response. Its natural structure allows a
flexible optimization that implicitly integrate exogenous constraints (as
opposed, for example, to conventional RL solutions), making MCTS a promising
candidate for DR control problems. We demonstrate how to improve MCTS control
performance by incorporating a Physics-informed Neural Network (PiNN) model for
its underlying thermal state prediction, as opposed to traditional purely
data-driven Black-Box approaches. Our MCTS implementation aligned with a PiNN
model is able to obtain a 3% increment of the obtained reward compared to a
rule-based controller; leading to a 10% cost reduction and 35% reduction on
temperature difference with the desired one when applied to an artificial price
profile. We further implemented a Deep Learning layer into the Monte Carlo Tree
Search technique using a neural network that leads the tree search through more
optimal nodes. We then compared this addition with its Vanilla version, showing
the improvement in computational cost required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pavirani_F/0/1/0/all/0/1&quot;&gt;Fabio Pavirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gokhale_G/0/1/0/all/0/1&quot;&gt;Gargya Gokhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Claessens_B/0/1/0/all/0/1&quot;&gt;Bert Claessens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Develder_C/0/1/0/all/0/1&quot;&gt;Chris Develder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04234">
<title>Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04234</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers, renowned for their self-attention mechanism, have achieved
state-of-the-art performance across various tasks in natural language
processing, computer vision, time-series modeling, etc. However, one of the
challenges with deep Transformer models is the oversmoothing problem, where
representations across layers converge to indistinguishable values, leading to
significant performance degradation. We interpret the original self-attention
as a simple graph filter and redesign it from a graph signal processing (GSP)
perspective. We propose graph-filter-based self-attention (GFSA) to learn a
general yet effective one, whose complexity, however, is slightly larger than
that of the original self-attention mechanism. We demonstrate that GFSA
improves the performance of Transformers in various fields, including computer
vision, natural language processing, graph pattern classification, speech
recognition, and code classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongwhan Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wi_H/0/1/0/all/0/1&quot;&gt;Hyowon Wi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jayoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yehjin Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kookjin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trask_N/0/1/0/all/0/1&quot;&gt;Nathaniel Trask&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Noseong Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04350">
<title>CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04350</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to perform causal reasoning is widely considered a core feature
of intelligence. In this work, we investigate whether large language models
(LLMs) can coherently reason about causality. Much of the existing work in
natural language processing (NLP) focuses on evaluating commonsense causal
reasoning in LLMs, thus failing to assess whether a model can perform causal
inference in accordance with a set of well-defined formal rules. To address
this, we propose a new NLP task, causal inference in natural language, inspired
by the &quot;causal inference engine&quot; postulated by Judea Pearl et al. We compose a
large dataset, CLadder, with 10K samples: based on a collection of causal
graphs and queries (associational, interventional, and counterfactual), we
obtain symbolic questions and ground-truth answers, through an oracle causal
inference engine. These are then translated into natural language. We evaluate
multiple LLMs on our dataset, and we introduce and evaluate a bespoke
chain-of-thought prompting strategy, CausalCoT. We show that our task is highly
challenging for LLMs, and we conduct an in-depth analysis to gain deeper
insights into the causal reasoning abilities of LLMs. Our data is open-sourced
at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found
at https://github.com/causalNLP/cladder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhijing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1&quot;&gt;Felix Leeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gresele_L/0/1/0/all/0/1&quot;&gt;Luigi Gresele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1&quot;&gt;Ojasv Kamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1&quot;&gt;Kevin Blin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adauto_F/0/1/0/all/0/1&quot;&gt;Fernando Gonzalez Adauto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1&quot;&gt;Max Kleiman-Weiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05379">
<title>Exploring Parity Challenges in Reinforcement Learning through Curriculum Learning with Noisy Labels. (arXiv:2312.05379v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05379</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into applying reinforcement learning (RL) in strategy
games, particularly those characterized by parity challenges, as seen in
specific positions of Go and Chess and a broader range of impartial games. We
propose a simulated learning process, structured within a curriculum learning
framework and augmented with noisy labels, to mirror the intricacies of
self-play learning scenarios. This approach thoroughly analyses how neural
networks (NNs) adapt and evolve from elementary to increasingly complex game
positions. Our empirical research indicates that even minimal label noise can
significantly impede NNs&apos; ability to discern effective strategies, a difficulty
that intensifies with the growing complexity of the game positions. These
findings underscore the urgent need for advanced methodologies in RL training,
specifically tailored to counter the obstacles imposed by noisy evaluations.
The development of such methodologies is crucial not only for enhancing NN
proficiency in strategy games with significant parity elements but also for
broadening the resilience and efficiency of RL systems across diverse and
complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riis_S/0/1/0/all/0/1&quot;&gt;Soren Riis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06008">
<title>Guardians of Trust: Navigating Data Security in AIOps through Vendor Partnerships. (arXiv:2312.06008v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06008</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence for IT Operations (AIOps) is a rapidly growing field
that applies artificial intelligence and machine learning to automate and
optimize IT operations. AIOps vendors provide services that ingest end-to-end
logs, traces, and metrics to offer a full stack observability of IT systems.
However, these data sources may contain sensitive information such as internal
IP addresses, hostnames, HTTP headers, SQLs, method/argument return values,
URLs, personal identifiable information (PII), or confidential business data.
Therefore, data security is a crucial concern when working with AIOps vendors.
In this article, we will discuss the security features offered by different
vendors and how we can adopt best practices to ensure data protection and
privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Subhadip Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07586">
<title>Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07586</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a sampling method that provides first-principle
non-linear correction for classifier-free guided DDPMs. Such correction forces
the guided DDPMs to respect the Fokker-Planck equation of their underlying
diffusion process, in a way that is training-free, derivative-free, and
compatible with existing sampling methods. Experiments show that characteristic
guidance enhances control and reduces color and exposure issues in image
generation, proving effective in diverse applications ranging from latent space
sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Candi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yuan Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07899">
<title>Morphological Profiling for Drug Discovery in the Era of Deep Learning. (arXiv:2312.07899v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07899</link>
<description rdf:parseType="Literal">&lt;p&gt;Morphological profiling is a valuable tool in phenotypic drug discovery. The
advent of high-throughput automated imaging has enabled the capturing of a wide
range of morphological features of cells or organisms in response to
perturbations at the single-cell resolution. Concurrently, significant advances
in machine learning and deep learning, especially in computer vision, have led
to substantial improvements in analyzing large-scale high-content images at
high-throughput. These efforts have facilitated understanding of compound
mechanism-of-action (MOA), drug repurposing, characterization of cell
morphodynamics under perturbation, and ultimately contributing to the
development of novel therapeutics. In this review, we provide a comprehensive
overview of the recent advances in the field of morphological profiling. We
summarize the image profiling analysis workflow, survey a broad spectrum of
analysis strategies encompassing feature engineering- and deep learning-based
approaches, and introduce publicly available benchmark datasets. We place a
particular emphasis on the application of deep learning in this pipeline,
covering cell segmentation, image representation learning, and multimodal
learning. Additionally, we illuminate the application of morphological
profiling in phenotypic drug discovery and highlight potential challenges and
opportunities in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qiaosi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ratnayake_R/0/1/0/all/0/1&quot;&gt;Ranjala Ratnayake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Seabra_G/0/1/0/all/0/1&quot;&gt;Gustavo Seabra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhe Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fang_R/0/1/0/all/0/1&quot;&gt;Ruogu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Lina Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yousong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kahveci_T/0/1/0/all/0/1&quot;&gt;Tamer Kahveci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luesch_H/0/1/0/all/0/1&quot;&gt;Hendrik Luesch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanjun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10201">
<title>CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion Recognition. (arXiv:2312.10201v3 [cs.MM] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10201</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal multi-label emotion recognition (MMER) aims to identify relevant
emotions from multiple modalities. The challenge of MMER is how to effectively
capture discriminative features for multiple labels from heterogeneous data.
Recent studies are mainly devoted to exploring various fusion strategies to
integrate multi-modal information into a unified representation for all labels.
However, such a learning scheme not only overlooks the specificity of each
modality but also fails to capture individual discriminative features for
different labels. Moreover, dependencies of labels and modalities cannot be
effectively modeled. To address these issues, this paper presents ContrAstive
feature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically,
we devise a reconstruction-based fusion mechanism to better model fine-grained
modality-to-label dependencies by contrastively learning modal-separated and
label-specific features. To further exploit the modality complementarity, we
introduce a shuffle-based aggregation strategy to enrich co-occurrence
collaboration among labels. Experiments on two benchmark datasets CMU-MOSEI and
M3ED demonstrate the effectiveness of CARAT over state-of-the-art methods. Code
is available at https://github.com/chengzju/CARAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Cheng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1&quot;&gt;Lidan Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10251">
<title>Advancing Surgical VQA with Scene Graph Knowledge. (arXiv:2312.10251v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10251</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern operating room is becoming increasingly complex, requiring innovative
intra-operative support systems. While the focus of surgical data science has
largely been on video analysis, integrating surgical computer vision with
language capabilities is emerging as a necessity. Our work aims to advance
Visual Question Answering (VQA) in the surgical context with scene graph
knowledge, addressing two main challenges in the current surgical VQA systems:
removing question-condition bias in the surgical VQA dataset and incorporating
scene-aware reasoning in the surgical VQA model design. First, we propose a
Surgical Scene Graph-based dataset, SSG-QA, generated by employing segmentation
and detection models on publicly available datasets. We build surgical scene
graphs using spatial and action information of instruments and anatomies. These
graphs are fed into a question engine, generating diverse QA pairs. Our SSG-QA
dataset provides a more complex, diverse, geometrically grounded, unbiased, and
surgical action-oriented dataset compared to existing surgical VQA datasets. We
then propose SSG-QA-Net, a novel surgical VQA model incorporating a lightweight
Scene-embedded Interaction Module (SIM), which integrates geometric scene
knowledge in the VQA model design by employing cross-attention between the
textual and the scene features. Our comprehensive analysis of the SSG-QA
dataset shows that SSG-QA-Net outperforms existing methods across different
question types and complexities. We highlight that the primary limitation in
the current surgical VQA systems is the lack of scene knowledge to answer
complex queries. We present a novel surgical VQA dataset and model and show
that results can be significantly improved by incorporating geometric scene
features in the VQA model design. The source code and the dataset will be made
publicly available at: https://github.com/CAMMA-public/SSG-QA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kattel_M/0/1/0/all/0/1&quot;&gt;Manasi Kattel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavanchy_J/0/1/0/all/0/1&quot;&gt;Joel L. Lavanchy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1&quot;&gt;Vinkle Srivastav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11193">
<title>&quot;Paraphrasing The Original Text&quot; Makes High Accuracy Long-Context QA. (arXiv:2312.11193v7 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11193</link>
<description rdf:parseType="Literal">&lt;p&gt;Most open-source generative language models currently have a context window
of no more than 4k, limiting their ability when facing long text. Many previous
efforts have tried to extend the context window of models, but their actual
effects have been found to be very limited. To address this issue, we
theoretically analyze the effectiveness of the long-context training data and
find that long-context training requires &quot;effective&quot; data rather than simply
&quot;long&quot; data, which is rarely noticed in previous studies. Thus, we propose
adding &quot;original text paraphrasing&quot; to enhance the effectiveness of the data.
The model trained on our re-fined dataset obtains excellent long-context
capabilities and achieves state-of-the-art accuracy on multi-document retrieval
and QA tasks among models of comparable scales. The model and training data
have been made available on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yijiong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11658">
<title>Traces of Memorisation in Large Language Models for Code. (arXiv:2312.11658v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11658</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have gained significant popularity because of their
ability to generate human-like text and potential applications in various
fields, such as Software Engineering. Large language models for code are
commonly trained on large unsanitised corpora of source code scraped from the
internet. The content of these datasets is memorised and can be extracted by
attackers with data extraction attacks. In this work, we explore memorisation
in large language models for code and compare the rate of memorisation with
large language models trained on natural language. We adopt an existing
benchmark for natural language and construct a benchmark for code by
identifying samples that are vulnerable to attack. We run both benchmarks
against a variety of models, and perform a data extraction attack. We find that
large language models for code are vulnerable to data extraction attacks, like
their natural language counterparts. From the training data that was identified
to be potentially extractable we were able to extract 47% from a
CodeGen-Mono-16B code completion model. We also observe that models memorise
more, as their parameter count grows, and that their pre-training data are also
vulnerable to attack. We also find that data carriers are memorised at a higher
rate than regular code or documentation and that different model architectures
memorise different samples. Data leakage has severe outcomes, so we urge the
research community to further investigate the extent of this phenomenon using a
wider range of models and extraction techniques in order to build safeguards to
mitigate this issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Kaswan_A/0/1/0/all/0/1&quot;&gt;Ali Al-Kaswan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1&quot;&gt;Maliheh Izadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1&quot;&gt;Arie van Deursen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12108">
<title>Knowledge Graph Error Detection with Contrastive Confidence Adaption. (arXiv:2312.12108v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12108</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs (KGs) often contain various errors. Previous works on
detecting errors in KGs mainly rely on triplet embedding from graph structure.
We conduct an empirical study and find that these works struggle to
discriminate noise from semantically-similar correct triplets. In this paper,
we propose a KG error detection model CCA to integrate both textual and graph
structural information from triplet reconstruction for better distinguishing
semantics. We design interactive contrastive learning to capture the
differences between textual and structural patterns. Furthermore, we construct
realistic datasets with semantically-similar noise and adversarial noise.
Experimental results demonstrate that CCA outperforms state-of-the-art
baselines, especially in detecting semantically-similar noise and adversarial
noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wei Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13845">
<title>Image Clustering using Restricted Boltzman Machine. (arXiv:2312.13845v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13845</link>
<description rdf:parseType="Literal">&lt;p&gt;In various verification systems, Restricted Boltzmann Machines (RBMs) have
demonstrated their efficacy in both front-end and back-end processes. In this
work, we propose the use of RBMs to the image clustering tasks. RBMs are
trained to convert images into image embeddings. We employ the conventional
bottom-up Agglomerative Hierarchical Clustering (AHC) technique. To address the
challenge of limited test face image data, we introduce Agglomerative
Hierarchical Clustering based Method for Image Clustering using Restricted
Boltzmann Machine (AHC-RBM) with two major steps. Initially, a universal RBM
model is trained using all available training dataset. Subsequently, we train
an adapted RBM model using the data from each test image. Finally, RBM vectors
which is the embedding vector is generated by concatenating the
visible-to-hidden weight matrices of these adapted models, and the bias
vectors. These vectors effectively preserve class-specific information and are
utilized in image clustering tasks. Our experimental results, conducted on two
benchmark image datasets (MS-Celeb-1M and DeepFashion), demonstrate that our
proposed approach surpasses well-known clustering algorithms such as k-means,
spectral clustering, and approximate Rank-order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1&quot;&gt;Abraham Woubie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1&quot;&gt;Enoch Solomon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1&quot;&gt;Eyael Solomon Emiru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14856">
<title>Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code. (arXiv:2312.14856v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14856</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for systematically evaluating the correctness and
robustness of instruction-tuned large language models (LLMs) for code
generation via a new benchmark, Turbulence. Turbulence consists of a large set
of natural language $\textit{question templates}$, each of which is a
programming problem, parameterised so that it can be asked in many different
forms. Each question template has an associated $\textit{test oracle}$ that
judges whether a code solution returned by an LLM is correct. Thus, from a
single question template, it is possible to ask an LLM a
$\textit{neighbourhood}$ of very similar programming questions, and assess the
correctness of the result returned for each question. This allows gaps in an
LLM&apos;s code generation abilities to be identified, including
$\textit{anomalies}$ where the LLM correctly solves $\textit{almost all}$
questions in a neighbourhood but fails for particular parameter instantiations.
We present experiments against five LLMs from OpenAI, Cohere and Meta, each at
two temperature configurations. Our findings show that, across the board,
Turbulence is able to reveal gaps in LLM reasoning ability. This goes beyond
merely highlighting that LLMs sometimes produce wrong code (which is no
surprise): by systematically identifying cases where LLMs are able to solve
some problems in a neighbourhood but do not manage to generalise to solve the
whole neighbourhood, our method is effective at highlighting
$\textit{robustness}$ issues. We present data and examples that shed light on
the kinds of mistakes that LLMs make when they return incorrect code results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honarvar_S/0/1/0/all/0/1&quot;&gt;Shahin Honarvar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilk_M/0/1/0/all/0/1&quot;&gt;Mark van der Wilk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donaldson_A/0/1/0/all/0/1&quot;&gt;Alastair Donaldson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14972">
<title>A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production. (arXiv:2312.14972v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14972</link>
<description rdf:parseType="Literal">&lt;p&gt;Many companies rely on APIs of managed AI models such as OpenAI&apos;s GPT-4 to
create AI-enabled experiences in their products. Along with the benefits of
ease of use and shortened time to production, this reliance on proprietary APIs
has downsides in terms of model control, performance reliability, up-time
predictability, and cost. At the same time, there has been a flurry of open
source small language models (SLMs) that have been made available for
commercial use. However, their readiness to replace existing capabilities
remains unclear, and a systematic approach to test these models is not readily
available. In this paper, we present a systematic evaluation methodology for,
and characterization of, modern open source SLMs and their trade-offs when
replacing a proprietary LLM APIs for a real-world product feature. We have
designed SLaM, an automated analysis tool that enables the quantitative and
qualitative testing of product features utilizing arbitrary SLMs. Using SLaM,
we examine both the quality and the performance characteristics of modern SLMs
relative to an existing customer-facing OpenAI-based implementation. We find
that across 9 SLMs and 29 variants, we observe competitive quality-of-results
for our use case, significant performance consistency improvement, and a cost
reduction of 5x-29x when compared to OpenAI GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irugalbandara_C/0/1/0/all/0/1&quot;&gt;Chandra Irugalbandara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendra_A/0/1/0/all/0/1&quot;&gt;Ashish Mahendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daynauth_R/0/1/0/all/0/1&quot;&gt;Roland Daynauth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arachchige_T/0/1/0/all/0/1&quot;&gt;Tharuka Kasthuri Arachchige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flautner_K/0/1/0/all/0/1&quot;&gt;Krisztian Flautner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Lingjia Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yiping Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1&quot;&gt;Jason Mars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15667">
<title>TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy Gradient. (arXiv:2312.15667v3 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15667</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Agent Policy Gradient (MAPG) has made significant progress in recent
years. However, centralized critics in state-of-the-art MAPG methods still face
the centralized-decentralized mismatch (CDM) issue, which means sub-optimal
actions by some agents will affect other agent&apos;s policy learning. While using
individual critics for policy updates can avoid this issue, they severely limit
cooperation among agents. To address this issue, we propose an agent topology
framework, which decides whether other agents should be considered in policy
gradient and achieves compromise between facilitating cooperation and
alleviating the CDM issue. The agent topology allows agents to use coalition
utility as learning objective instead of global utility by centralized critics
or local utility by individual critics. To constitute the agent topology,
various models are studied. We propose Topology-based multi-Agent Policy
gradiEnt (TAPE) for both stochastic and deterministic MAPG methods. We prove
the policy improvement theorem for stochastic TAPE and give a theoretical
explanation for the improved cooperation among agents. Experiment results on
several benchmarks show the agent topology is able to facilitate agent
cooperation and alleviate CDM issue respectively to improve performance of
TAPE. Finally, multiple ablation studies and a heuristic graph search algorithm
are devised to show the efficacy of the agent topology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_X/0/1/0/all/0/1&quot;&gt;Xingzhou Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norman_T/0/1/0/all/0/1&quot;&gt;Timothy J. Norman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaiqi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yali Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17267">
<title>Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17267</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, prompt-tuning with pre-trained language models (PLMs) has
demonstrated the significantly enhancing ability of relation extraction (RE)
tasks. However, in low-resource scenarios, where the available training data is
scarce, previous prompt-based methods may still perform poorly for prompt-based
representation learning due to a superficial understanding of the relation. To
this end, we highlight the importance of learning high-quality relation
representation in low-resource scenarios for RE, and propose a novel
prompt-based relation representation method, named MVRE
(\underline{M}ulti-\underline{V}iew \underline{R}elation
\underline{E}xtraction), to better leverage the capacity of PLMs to improve the
performance of RE within the low-resource prompt-tuning paradigm. Specifically,
MVRE decouples each relation into different perspectives to encompass
multi-view relation representations for maximizing the likelihood during
relation inference. Furthermore, we also design a Global-Local loss and a
Dynamic-Initialization method for better alignment of the multi-view
relation-representing virtual words, containing the semantics of relation
labels during the optimization learning process and initialization. Extensive
experiments on three benchmark datasets show that our method can achieve
state-of-the-art in low-resource settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chenghao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaoye Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Wenfeng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dangyang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00741">
<title>ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. (arXiv:2401.00741v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00741</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing evaluations of tool learning primarily focus on validating the
alignment of selected tools for large language models (LLMs) with expected
outcomes. However, these approaches rely on a limited set of scenarios where
answers can be pre-determined, diverging from genuine needs. Furthermore, a
sole emphasis on outcomes disregards the intricate capabilities essential for
LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a
fine-grained system tailored for the evaluation of the LLMs&apos; tool learning
capabilities in authentic scenarios. The system meticulously examines seven
real-world scenarios, analyzing five dimensions crucial to LLMs in tool
learning: format alignment, intent comprehension, behavior planning, tool
selection, and answer organization. Additionally, ToolEyes incorporates a tool
library boasting approximately 600 tools, serving as an intermediary between
LLMs and the physical world. Evaluations involving ten LLMs across three
categories reveal a preference for specific scenarios and limited cognitive
abilities in tool learning. Intriguingly, expanding the model size even
exacerbates the hindrance to tool learning. These findings offer instructive
insights aimed at advancing the field of tool learning. The data is available
att https://github.com/Junjie-Ye/ToolEyes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Caishuang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yilong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sixian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaoran Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00773">
<title>Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00773</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic mixture models are acknowledged as a valuable tool for
unsupervised outlier detection owing to their interpretability and intuitive
grounding in statistical principles. Within this framework, Dirichlet process
mixture models emerge as a compelling alternative to conventional finite
mixture models for both clustering and outlier detection tasks. However,
despite their evident advantages, the widespread adoption of Dirichlet process
mixture models in unsupervised outlier detection has been hampered by
challenges related to computational inefficiency and sensitivity to outliers
during the construction of detectors. To tackle these challenges, we propose a
novel outlier detection method based on ensembles of Dirichlet process Gaussian
mixtures. The proposed method is a fully unsupervised algorithm that
capitalizes on random subspace and subsampling ensembles, not only ensuring
efficient computation but also enhancing the robustness of the resulting
outlier detector. Moreover, the proposed method leverages variational inference
for Dirichlet process mixtures to ensure efficient and fast computation.
Empirical studies with benchmark datasets demonstrate that our method
outperforms existing approaches for unsupervised outlier detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongwook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Juyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hee Cheol Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Seonghyun Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01326">
<title>An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01326</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1&quot;&gt;Urchade Zaratiana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1&quot;&gt;Nadi Tomeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1&quot;&gt;Pierre Holat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1&quot;&gt;Thierry Charnois&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01841">
<title>Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01841</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental (and largely open) challenge in sequential decision-making is
dealing with non-stationary environments, where exogenous environmental
conditions change over time. Such problems are traditionally modeled as
non-stationary Markov decision processes (NSMDP). However, existing approaches
for decision-making in NSMDPs have two major shortcomings: first, they assume
that the updated environmental dynamics at the current time are known (although
future dynamics can change); and second, planning is largely pessimistic, i.e.,
the agent acts ``safely&apos;&apos; to account for the non-stationary evolution of the
environment. We argue that both these assumptions are invalid in practice --
updated environmental conditions are rarely known, and as the agent interacts
with the environment, it can learn about the updated dynamics and avoid being
pessimistic, at least in states whose dynamics it is confident about. We
present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree
Search (ADA-MCTS)} that addresses these challenges. We show that the agent can
learn the updated dynamics of the environment over time and then act as it
learns, i.e., if the agent is in a region of the state space about which it has
updated knowledge, it can avoid being pessimistic. To quantify ``updated
knowledge,&apos;&apos; we disintegrate the aleatoric and epistemic uncertainty in the
agent&apos;s updated belief and show how the agent can use these estimates for
decision-making. We compare the proposed approach with the multiple
state-of-the-art approaches in decision-making across multiple well-established
open-source problems and empirically show that our approach is faster and
highly adaptive without sacrificing safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Baiting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02727">
<title>Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02727</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AEs) have been extensively studied due to their
potential for privacy protection and inspiring robust neural networks. Yet,
making a targeted AE transferable across unknown models remains challenging. In
this paper, to alleviate the overfitting dilemma common in an AE crafted by
existing simple iterative attacks, we propose fine-tuning it in the feature
space. Specifically, starting with an AE generated by a baseline attack, we
encourage the features conducive to the target class and discourage the
features to the original class in a middle layer of the source model. Extensive
experiments demonstrate that only a few iterations of fine-tuning can boost
existing attacks&apos; targeted transferability nontrivially and universally. Our
results also verify that the simple iterative attacks can yield comparable or
even better transferability than the resource-intensive methods, which rest on
training target-specific classifiers or generators with additional data. The
code is available at: github.com/zengh5/TA_feature_FT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Hui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Biwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Anjie Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02810">
<title>Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02810</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural network (PINN) is a data-driven solver for partial
and ordinary differential equations(ODEs/PDEs). It provides a unified framework
to address both forward and inverse problems. However, the complexity of the
objective function often leads to training failures. This issue is particularly
prominent when solving high-frequency and multi-scale problems. We proposed
using transfer learning to boost the robustness and convergence of training
PINN, starting training from low-frequency problems and gradually approaching
high-frequency problems. Through two case studies, we discovered that transfer
learning can effectively train PINN to approximate solutions from low-frequency
problems to high-frequency problems without increasing network parameters.
Furthermore, it requires fewer data points and less training time. We
elaborately described our training strategy, including optimizer selection, and
suggested guidelines for using transfer learning to train neural networks for
solving more complex problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustajab_A/0/1/0/all/0/1&quot;&gt;Abdul Hannan Mustajab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Hao Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizvi_Z/0/1/0/all/0/1&quot;&gt;Zarghaam Rizvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wuttke_F/0/1/0/all/0/1&quot;&gt;Frank Wuttke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02941">
<title>Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02941</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic semantic segmentation of magnetic resonance imaging (MRI) images
using deep neural networks greatly assists in evaluating and planning
treatments for various clinical applications. However, training these models is
conditioned on the availability of abundant annotated data to implement the
end-to-end supervised learning procedure. Even if we annotate enough data, MRI
images display considerable variability due to factors such as differences in
patients, MRI scanners, and imaging protocols. This variability necessitates
retraining neural networks for each specific application domain, which, in
turn, requires manual annotation by expert radiologists for all new domains. To
relax the need for persistent data annotation, we develop a method for
unsupervised federated domain adaptation using multiple annotated source
domains. Our approach enables the transfer of knowledge from several annotated
source domains to adapt a model for effective use in an unannotated target
domain. Initially, we ensure that the target domain data shares similar
representations with each source domain in a latent embedding space, modeled as
the output of a deep encoder, by minimizing the pair-wise distances of the
distributions for the target domain and the source domains. We then employ an
ensemble approach to leverage the knowledge obtained from all domains. We
provide theoretical analysis and perform experiments on the MICCAI 2016
multi-site dataset to demonstrate our method is effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nananukul_N/0/1/0/all/0/1&quot;&gt;Navapat Nananukul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanian_zadeh_H/0/1/0/all/0/1&quot;&gt;Hamid Soltanian-zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1&quot;&gt;Mohammad Rostami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02987">
<title>Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02987</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of pretrained models has significantly impacted Natural
Language Processing (NLP) and Computer Vision to relational datasets.
Traditionally, these models are assessed through fine-tuned downstream tasks.
However, this raises the question of how to evaluate these models more
efficiently and more effectively. In this study, we explore a novel approach
where we leverage the meta features associated with each entity as a source of
worldly knowledge and employ entity representations from the models. We propose
using the consistency between these representations and the meta features as a
metric for evaluating pretrained models. Our method&apos;s effectiveness is
demonstrated across various domains, including models with relational datasets,
large language models and image models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aboagye_P/0/1/0/all/0/1&quot;&gt;Prince Aboagye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_U/0/1/0/all/0/1&quot;&gt;Uday Singh Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_M/0/1/0/all/0/1&quot;&gt;Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Shubham Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04124">
<title>MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. (arXiv:2401.04124v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04124</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents centered around Large Language Models (LLMs) are now capable of
automating mobile device operations for users. After fine-tuning to learn a
user&apos;s mobile operations, these agents can adhere to high-level user
instructions online. They execute tasks such as goal decomposition, sequencing
of sub-goals, and interactive environmental exploration, until the final
objective is achieved. However, privacy concerns related to personalized user
data arise during mobile operations, requiring user confirmation. Moreover,
users&apos; real-world operations are exploratory, with action data being complex
and redundant, posing challenges for agent learning. To address these issues,
in our practical application, we have designed interactive tasks between agents
and humans to identify sensitive information and align with personalized user
needs. Additionally, we integrated Standard Operating Procedure (SOP)
information within the model&apos;s in-context learning to enhance the agent&apos;s
comprehension of complex task execution. Our approach is evaluated on the new
device control benchmark AitW, which encompasses 30K unique instructions across
multi-step tasks, including application operation, web searching, and web
shopping. Experimental results show that the SOP-based agent achieves
state-of-the-art performance without incurring additional inference costs,
boasting an overall action success rate of 66.92%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tinghe Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04374">
<title>Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04374</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the complexity and lack of transparency in deep neural networks (DNNs),
extensive efforts have been made to make these systems more interpretable or
explain their behaviors in accessible terms. Unlike most reviews, which focus
on algorithmic and model-centric perspectives, this work takes a &quot;data-centric&quot;
view, examining how data collection, processing, and analysis contribute to
explainable AI (XAI). We categorize existing work into three categories subject
to their purposes: interpretations of deep models, referring to feature
attributions and reasoning processes that correlate data points with model
outputs; influences of training data, examining the impact of training data
nuances, such as data valuation and sample anomalies, on decision-making
processes; and insights of domain knowledge, discovering latent patterns and
fostering new knowledge from data and models to advance social values and
scientific discovery. Specifically, we distill XAI methodologies into data
mining operations on training and testing data across modalities, such as
images, text, and tabular data, as well as on training logs, checkpoints,
models and other DNN behavior descriptors. In this way, our study offers a
comprehensive, data-centric examination of XAI from a lens of data mining
methods and applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiamin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinhao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zeyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04579">
<title>A Deep Network for Explainable Prediction of Non-Imaging Phenotypes using Anatomical Multi-View Data. (arXiv:2401.04579v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04579</link>
<description rdf:parseType="Literal">&lt;p&gt;Large datasets often contain multiple distinct feature sets, or views, that
offer complementary information that can be exploited by multi-view learning
methods to improve results. We investigate anatomical multi-view data, where
each brain anatomical structure is described with multiple feature sets. In
particular, we focus on sets of white matter microstructure and connectivity
features from diffusion MRI, as well as sets of gray matter area and thickness
features from structural MRI. We investigate machine learning methodology that
applies multi-view approaches to improve the prediction of non-imaging
phenotypes, including demographics (age), motor (strength), and cognition
(picture vocabulary). We present an explainable multi-view network (EMV-Net)
that can use different anatomical views to improve prediction performance. In
this network, each individual anatomical view is processed by a view-specific
feature extractor and the extracted information from each view is fused using a
learnable weight. This is followed by a wavelet transform-based module to
obtain complementary information across views which is then applied to
calibrate the view-specific information. Additionally, the calibrator produces
an attention-based calibration score to indicate anatomical structures&apos;
importance for interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tengfei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zekelman_L/0/1/0/all/0/1&quot;&gt;Leo Zekelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Makris_N/0/1/0/all/0/1&quot;&gt;Nikos Makris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rathi_Y/0/1/0/all/0/1&quot;&gt;Yogesh Rathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Donnell_L/0/1/0/all/0/1&quot;&gt;Lauren J. O&amp;#x27; Donnell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04658">
<title>Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04658</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear attention is an efficient attention mechanism that has recently
emerged as a promising alternative to conventional softmax attention. With its
ability to process tokens in linear computational complexities, linear
attention, in theory, can handle sequences of unlimited length without
sacrificing speed, i.e., maintaining a constant training speed for various
sequence lengths with a fixed memory consumption. However, due to the issue
with cumulative summation (cumsum), current linear attention algorithms cannot
demonstrate their theoretical advantage in a causal setting. In this paper, we
present Lightning Attention-2, the first linear attention implementation that
enables linear attention to realize its theoretical computational benefits. To
achieve this, we leverage the thought of tiling, separately handling the
intra-block and inter-block components in linear attention calculation.
Specifically, we utilize the conventional attention computation mechanism for
the intra-blocks and apply linear attention kernel tricks for the inter-blocks.
A tiling technique is adopted through both forward and backward procedures to
take full advantage of the GPU hardware. We implement our algorithm in Triton
to make it IO-aware and hardware-friendly. Various experiments are conducted on
different model sizes and sequence lengths. Lightning Attention-2 retains
consistent training and inference speed regardless of input sequence length and
is significantly faster than other attention mechanisms. The source code is
available at https://github.com/OpenNLPLab/lightning-attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weigao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xuyang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiran Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04812">
<title>Sample-and-Bound for Non-Convex Optimization. (arXiv:2401.04812v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04812</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard approaches for global optimization of non-convex functions, such as
branch-and-bound, maintain partition trees to systematically prune the domain.
The tree size grows exponentially in the number of dimensions. We propose new
sampling-based methods for non-convex optimization that adapts Monte Carlo Tree
Search (MCTS) to improve efficiency. Instead of the standard use of visitation
count in Upper Confidence Bounds, we utilize numerical overapproximations of
the objective as an uncertainty metric, and also take into account of sampled
estimates of first-order and second-order information. The Monte Carlo tree in
our approach avoids the usual fixed combinatorial patterns in growing the tree,
and aggressively zooms into the promising regions, while still balancing
exploration and exploitation. We evaluate the proposed algorithms on
high-dimensional non-convex optimization benchmarks against competitive
baselines and analyze the effects of the hyper parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yaoguang Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhizhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Sicun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04846">
<title>The inherent goodness of well educated intelligence. (arXiv:2401.04846v2 [econ.TH] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04846</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper will examine what makes a being intelligent, whether that be a
biological being or an artificial silicon being on a computer. Special
attention will be paid to the being having the ability to characterize and
control a collective system of many identical conservative sub-systems
conservatively interacting. The essence of intelligence will be found to be the
golden rule -- &quot;the collective acts as one&quot; or &quot;knowing the global consequences
of local actions&quot;. The flow of the collective is a small set of twinkling
textures, that are governed by a puppeteer who is pulling a small number of
strings according to a geodesic motion of least action, determined by the
symmetries. Controlling collective conservative systems is difficult and has
historically been done by adding significant viscosity to the system to
stabilize the desirable meta stable equilibriums of maximum performance, but it
degrades or destroys them in the process. There is an alternative. Once the
optimum twinkling textures of the meta stable equilibriums are identified by
the intelligent being (that is the collective system is characterized), the
collective system can be moved by the intelligent being to the optimum
twinkling textures, then quickly vibrated by the intelligent being according to
the textures so that the collective system remains at the meta stable
equilibrium. Well educated intelligence knows the global consequences of its
local actions so that it will not take short term actions that will lead to
poor long term outcomes. In contrast, trained intelligence or trained stupidity
will optimize its short term actions, leading to poor long term outcomes. Well
educated intelligence is inherently good, but trained stupidity is inherently
evil and should be feared. Particular attention is paid to the control and
optimization of economic and social collectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Glinsky_M/0/1/0/all/0/1&quot;&gt;Michael E. Glinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Sievert_S/0/1/0/all/0/1&quot;&gt;Sharon Sievert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04925">
<title>The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04925</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain of Thought (CoT) is significant in improving the reasoning abilities of
large language models (LLMs). However, the correlation between the
effectiveness of CoT and the length of reasoning steps in prompts remains
largely unknown. To shed light on this, we have conducted several empirical
experiments to explore the relations. Specifically, we design experiments that
expand and compress the rationale reasoning steps within CoT demonstrations,
while keeping all other factors constant. We have the following key findings.
First, the results indicate that lengthening the reasoning steps in prompts,
even without adding new information into the prompt, considerably enhances
LLMs&apos; reasoning abilities across multiple datasets. Alternatively, shortening
the reasoning steps, even while preserving the key information, significantly
diminishes the reasoning abilities of models. This finding highlights the
importance of the number of steps in CoT prompts and provides practical
guidance to make better use of LLMs&apos; potential in complex problem-solving
scenarios. Second, we also investigated the relationship between the
performance of CoT and the rationales used in demonstrations. Surprisingly, the
result shows that even incorrect rationales can yield favorable outcomes if
they maintain the requisite length of inference. Third, we observed that the
advantages of increasing reasoning steps are task-dependent: simpler tasks
require fewer steps, whereas complex tasks gain significantly from longer
inference sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Mingyu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qinkai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+shu_D/0/1/0/all/0/1&quot;&gt;Dong shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yanda Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05596">
<title>POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05596</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-resource languages (LRLs) face challenges in supervised neural machine
translation due to limited parallel data, prompting research into unsupervised
methods. Unsupervised neural machine translation (UNMT) methods, including
back-translation, transfer learning, and pivot-based translation, offer
practical solutions for LRL translation, but they are hindered by issues like
synthetic data noise, language bias, and error propagation, which can
potentially be mitigated by Large Language Models (LLMs). LLMs have advanced
NMT with in-context learning (ICL) and supervised fine-tuning methods, but
insufficient training data results in poor performance in LRLs. We argue that
LLMs can mitigate the linguistic noise with auxiliary languages to improve
translations in LRLs. In this paper, we propose Probability-driven Meta-graph
Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of
multiple auxiliary languages to enhance LLMs&apos; translation capabilities for
LRLs. POMP involves constructing a directed acyclic meta-graph for each source
language, from which we dynamically sample multiple paths to prompt LLMs to
mitigate the linguistic noise and improve translations during training. We use
the BLEURT metric to evaluate the translations and back-propagate rewards,
estimated by scores, to update the probabilities of auxiliary languages in the
paths. Our experiments show significant improvements in the translation quality
of three LRLs, demonstrating the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shilong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhiliang Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Zhihua Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14129">
<title>WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.14129</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly evolving healthcare industry, platforms now have access to not
only traditional medical records, but also diverse data sets encompassing
various patient interactions, such as those from healthcare web portals. To
address this rich diversity of data, we introduce WellFactor: a method that
derives patient profiles by integrating information from these sources. Central
to our approach is the utilization of constrained low-rank approximation.
WellFactor is optimized to handle the sparsity that is often inherent in
healthcare data. Moreover, by incorporating task-specific label information,
our method refines the embedding results, offering a more informed perspective
on patients. One important feature of WellFactor is its ability to compute
embeddings for new, previously unobserved patient data instantaneously,
eliminating the need to revisit the entire data set or recomputing the
embedding. Comprehensive evaluations on real-world healthcare data demonstrate
WellFactor&apos;s effectiveness. It produces better results compared to other
existing methods in classification performance, yields meaningful clustering of
patients, and delivers consistent results in patient similarity searches and
predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dongjin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1&quot;&gt;Andy Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozturk_O/0/1/0/all/0/1&quot;&gt;Ozgur Ozturk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_D/0/1/0/all/0/1&quot;&gt;Deep Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drake_B/0/1/0/all/0/1&quot;&gt;Barry Drake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haidarian_H/0/1/0/all/0/1&quot;&gt;Hamid Haidarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_F/0/1/0/all/0/1&quot;&gt;Faizan Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Haesun Park&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>