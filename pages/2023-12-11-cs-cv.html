<!DOCTYPE html>
<html>
<head>
<title>2023-12-11-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.03763">Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing. (arXiv:2312.03763v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yushi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1">Feitong Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1">Di Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiangeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1">Kyle Genova</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1">Sean Fanello</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1">Rohit Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1">Thomas Funkhouser</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinda Zhang</a></p>
<p>We present a novel framework for generating photorealistic 3D human head and
subsequently manipulating and reposing them with remarkable flexibility. The
proposed approach leverages an implicit function representation of 3D human
heads, employing 3D Gaussians anchored on a parametric face model. To enhance
representational capabilities and encode spatial information, we embed a
lightweight tri-plane payload within each Gaussian rather than directly storing
color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space
via a 3DMM, enabling effective utilization of the diffusion model for 3D head
avatar generation. Our method facilitates the creation of diverse and realistic
3D human heads with fine-grained editing over facial features and expressions.
Extensive experiments demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03766">Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment. (arXiv:2312.03766v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gordon_B/0/1/0/all/0/1">Brian Gordon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1">Yonatan Bitton</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafir_Y/0/1/0/all/0/1">Yonatan Shafir</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1">Roopal Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1">Dani Lischinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1">Idan Szpektor</a></p>
<p>While existing image-text alignment models reach high quality binary
assessments, they fall short of pinpointing the exact source of misalignment.
In this paper, we present a method to provide detailed textual and visual
explanation of detected misalignments between text-image pairs. We leverage
large language models and visual grounding models to automatically construct a
training set that holds plausible misaligned captions for a given image and
corresponding textual explanations and visual indicators. We also publish a new
human curated test set comprising ground-truth textual and visual misalignment
annotations. Empirical results show that fine-tuning vision language models on
our training set enables them to articulate misalignments and visually indicate
them within images, outperforming strong baselines both on the binary alignment
classification and the explanation generation tasks. Our method code and human
curated test set are available at: https://mismatch-quest.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03767">Unknown Sample Discovery for Source Free Open Set Domain Adaptation. (arXiv:2312.03767v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jahan_C/0/1/0/all/0/1">Chowdhury Sadman Jahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1">Andreas Savakis</a></p>
<p>Open Set Domain Adaptation (OSDA) aims to adapt a model trained on a source
domain to a target domain that undergoes distribution shift and contains
samples from novel classes outside the source domain. Source-free OSDA
(SF-OSDA) techniques eliminate the need to access source domain samples, but
current SF-OSDA methods utilize only the known classes in the target domain for
adaptation, and require access to the entire target domain even during
inference after adaptation, to make the distinction between known and unknown
samples. In this paper, we introduce Unknown Sample Discovery (USD) as an
SF-OSDA method that utilizes a temporally ensembled teacher model to conduct
known-unknown target sample separation and adapts the student model to the
target domain over all classes using co-training and temporal consistency
between the teacher and the student. USD promotes Jensen-Shannon distance (JSD)
as an effective measure for known-unknown sample separation. Our
teacher-student framework significantly reduces error accumulation resulting
from imperfect known-unknown sample separation, while curriculum guidance helps
to reliably learn the distinction between target known and target unknown
subspaces. USD appends the target model with an unknown class node, thus
readily classifying a target sample into any of the known or unknown classes in
subsequent post-adaptation inference stages. Empirical results show that USD is
superior to existing SF-OSDA methods and is competitive with current OSDA
models that utilize both source and target domains during adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03771">DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models. (arXiv:2312.03771v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shaoan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zhisheng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kelvin C.K. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yandong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1">Tingbo Hou</a></p>
<p>This study introduces Text-Guided Subject-Driven Image Inpainting, a novel
task that combines text and exemplar images for image inpainting. While both
text and exemplar images have been used independently in previous efforts,
their combined utilization remains unexplored. Simultaneously accommodating
both conditions poses a significant challenge due to the inherent balance
required between editability and subject fidelity. To tackle this challenge, we
propose a two-step approach DreamInpainter. First, we compute dense subject
features to ensure accurate subject replication. Then, we employ a
discriminative token selection module to eliminate redundant subject details,
preserving the subject's identity while allowing changes according to other
conditions such as mask shape and text prompts. Additionally, we introduce a
decoupling regularization technique to enhance text control in the presence of
exemplar images. Our extensive experiments demonstrate the superior performance
of our method in terms of visual quality, identity preservation, and text
control, showcasing its effectiveness in the context of text-guided
subject-driven image inpainting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03772">DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing. (arXiv:2312.03772v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Shao-Yu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hwann-Tzong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tyng-Luh Liu</a></p>
<p>We present a diffusion-based video editing framework, namely DiffusionAtlas,
which can achieve both frame consistency and high fidelity in editing video
object appearance. Despite the success in image editing, diffusion models still
encounter significant hindrances when it comes to video editing due to the
challenge of maintaining spatiotemporal consistency in the object's appearance
across frames. On the other hand, atlas-based techniques allow propagating
edits on the layered representations consistently back to frames. However, they
often struggle to create editing effects that adhere correctly to the
user-provided textual or visual conditions due to the limitation of editing the
texture atlas on a fixed UV mapping field. Our method leverages a
visual-textual diffusion model to edit objects directly on the diffusion
atlases, ensuring coherent object identity across frames. We design a loss term
with atlas-based constraints and build a pretrained text-driven diffusion model
as pixel-wise guidance for refining shape distortions and correcting texture
deviations. Qualitative and quantitative experiments show that our method
outperforms state-of-the-art methods in achieving consistent high-fidelity
video-object editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03774">OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries. (arXiv:2312.03774v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuhang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xinge Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuexin Ma</a></p>
<p>Occupancy prediction has increasingly garnered attention in recent years for
its fine-grained understanding of 3D scenes. Traditional approaches typically
rely on dense, regular grid representations, which often leads to excessive
computational demands and a loss of spatial details for small objects. This
paper introduces OctreeOcc, an innovative 3D occupancy prediction framework
that leverages the octree representation to adaptively capture valuable
information in 3D, offering variable granularity to accommodate object shapes
and semantic regions of varying sizes and complexities. In particular, we
incorporate image semantic information to improve the accuracy of initial
octree structures and design an effective rectification mechanism to refine the
octree structure iteratively. Our extensive evaluations show that OctreeOcc not
only surpasses state-of-the-art methods in occupancy prediction, but also
achieves a 15%-24% reduction in computational overhead compared to
dense-grid-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03775">FAAC: Facial Animation Generation with Anchor Frame and Conditional Control for Superior Fidelity and Editability. (arXiv:2312.03775v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1">Sunqi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1">Hengjun Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1">Zhaodong Bing</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Tianzhu Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liangyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiajun Liang</a></p>
<p>Over recent years, diffusion models have facilitated significant advancements
in video generation. Yet, the creation of face-related videos still confronts
issues such as low facial fidelity, lack of frame consistency, limited
editability and uncontrollable human poses. To address these challenges, we
introduce a facial animation generation method that enhances both face identity
fidelity and editing capabilities while ensuring frame consistency. This
approach incorporates the concept of an anchor frame to counteract the
degradation of generative ability in original text-to-image models when
incorporating a motion module. We propose two strategies towards this
objective: training-free and training-based anchor frame methods. Our method's
efficacy has been validated on multiple representative DreamBooth and LoRA
models, delivering substantial improvements over the original outcomes in terms
of facial fidelity, text-to-image editability, and video motion. Moreover, we
introduce conditional control using a 3D parametric face model to capture
accurate facial movements and expressions. This solution augments the creative
possibilities for facial animation generation through the integration of
multiple control signals. For additional samples, please visit
https://anonymous.4open.science/r/FAAC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03777">On the Robustness of Large Multimodal Models Against Image Adversarial Attacks. (arXiv:2312.03777v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1">Xuanimng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Aparcedo_A/0/1/0/all/0/1">Alejandro Aparcedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Young Kyun Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a></p>
<p>Recent advances in instruction tuning have led to the development of
State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these
models, the impact of visual adversarial attacks on LMMs has not been
thoroughly examined. We conduct a comprehensive study of the robustness of
various LMMs against different adversarial attacks, evaluated across tasks
including image classification, image captioning, and Visual Question Answer
(VQA). We find that in general LMMs are not robust to visual adversarial
inputs. However, our findings suggest that context provided to the model via
prompts, such as questions in a QA pair helps to mitigate the effects of visual
adversarial inputs. Notably, the LMMs evaluated demonstrated remarkable
resilience to such attacks on the ScienceQA task with only an 8.10% drop in
performance compared to their visual counterparts which dropped 99.73%. We also
propose a new approach to real-world image classification which we term query
decomposition. By incorporating existence queries into our input prompt we
observe diminished attack effectiveness and improvements in image
classification accuracy. This research highlights a previously under-explored
facet of LMM robustness and sets the stage for future work aimed at
strengthening the resilience of multimodal systems in adversarial environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03781">Lite-Mind: Towards Efficient and Versatile Brain Representation Network. (arXiv:2312.03781v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zixuan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1">Duoqian Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1">Guangyin Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Liang Hu</a></p>
<p>Research in decoding visual information from the brain, particularly through
the non-invasive fMRI method, is rapidly progressing. The challenge arises from
the limited data availability and the low signal-to-noise ratio of fMRI
signals, leading to a low-precision task of fMRI-to-image retrieval.
State-of-the-art MindEye remarkably improves fMRI-to-image retrieval
performance by leveraging a deep MLP with a high parameter count orders of
magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to
the final hidden layer of CLIP's vision transformer. However, significant
individual variations exist among subjects, even within identical experimental
setups, mandating the training of subject-specific models. The substantial
parameters pose significant challenges in deploying fMRI decoding on practical
devices, especially with the necessitating of specific models for each subject.
To this end, we propose Lite-Mind, a lightweight, efficient, and versatile
brain representation network based on discrete Fourier transform, that
efficiently aligns fMRI voxels to fine-grained information of CLIP. Our
experiments demonstrate that Lite-Mind achieves an impressive 94.3%
fMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7%
fewer parameters than MindEye. Lite-Mind is also proven to be able to be
migrated to smaller brain datasets and establishes a new state-of-the-art for
zero-shot classification on the GOD dataset. The code is available at
https://github.com/gongzix/Lite-Mind.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03782">Novel class discovery meets foundation models for 3D semantic segmentation. (arXiv:2312.03782v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Riz_L/0/1/0/all/0/1">Luigi Riz</a>, <a href="http://arxiv.org/find/cs/1/au:+Saltori_C/0/1/0/all/0/1">Cristiano Saltori</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1">Elisa Ricci</a>, <a href="http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1">Fabio Poiesi</a></p>
<p>The task of Novel Class Discovery (NCD) in semantic segmentation entails
training a model able to accurately segment unlabelled (novel) classes, relying
on the available supervision from annotated (base) classes. Although
extensively investigated in 2D image data, the extension of the NCD task to the
domain of 3D point clouds represents a pioneering effort, characterized by
assumptions and challenges that are not present in the 2D case. This paper
represents an advancement in the analysis of point cloud data in four
directions. Firstly, it introduces the novel task of NCD for point cloud
semantic segmentation. Secondly, it demonstrates that directly transposing the
only existing NCD method for 2D image semantic segmentation to 3D data yields
suboptimal results. Thirdly, a new NCD approach based on online clustering,
uncertainty estimation, and semantic distillation is presented. Lastly, a novel
evaluation protocol is proposed to rigorously assess the performance of NCD in
point cloud semantic segmentation. Through comprehensive evaluations on the
SemanticKITTI, SemanticPOSS, and S3DIS datasets, the paper demonstrates
substantial superiority of the proposed method over the considered baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03790">Memory-Efficient Optical Flow via Radius-Distribution Orthogonal Cost Volume. (arXiv:2312.03790v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1">Gangwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shujun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1">Hao Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1">Miaojie Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a></p>
<p>The full 4D cost volume in Recurrent All-Pairs Field Transforms (RAFT) or
global matching by Transformer achieves impressive performance for optical flow
estimation. However, their memory consumption increases quadratically with
input resolution, rendering them impractical for high-resolution images. In
this paper, we present MeFlow, a novel memory-efficient method for
high-resolution optical flow estimation. The key of MeFlow is a recurrent local
orthogonal cost volume representation, which decomposes the 2D search space
dynamically into two 1D orthogonal spaces, enabling our method to scale
effectively to very high-resolution inputs. To preserve essential information
in the orthogonal space, we utilize self attention to propagate feature
information from the 2D space to the orthogonal space. We further propose a
radius-distribution multi-scale lookup strategy to model the correspondences of
large displacements at a negligible cost. We verify the efficiency and
effectiveness of our method on the challenging Sintel and KITTI benchmarks, and
real-world 4K ($2160\!\times\!3840$) images. Our method achieves competitive
performance on both Sintel and KITTI benchmarks, while maintaining the highest
memory efficiency on high-resolution inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03793">AnimateZero: Video Diffusion Models are Zero-Shot Image Animators. (arXiv:2312.03793v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiwen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1">Xiaodong Cun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1">Chenyang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jian Zhang</a></p>
<p>Large-scale text-to-video (T2V) diffusion models have great progress in
recent years in terms of visual quality, motion and temporal consistency.
However, the generation process is still a black box, where all attributes
(e.g., appearance, motion) are learned and generated jointly without precise
control ability other than rough text descriptions. Inspired by image animation
which decouples the video as one specific appearance with the corresponding
motion, we propose AnimateZero to unveil the pre-trained text-to-video
diffusion model, i.e., AnimateDiff, and provide more precise appearance and
motion control abilities for it. For appearance control, we borrow intermediate
latents and their features from the text-to-image (T2I) generation for ensuring
the generated first frame is equal to the given generated image. For temporal
control, we replace the global temporal attention of the original T2V model
with our proposed positional-corrected window attention to ensure other frames
align with the first frame well. Empowered by the proposed methods, AnimateZero
can successfully control the generating progress without further training. As a
zero-shot image animator for given images, AnimateZero also enables multiple
new applications, including interactive video generation and real image
animation. The detailed experiments demonstrate the effectiveness of the
proposed method in both T2V and related applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03795">AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and Reconstruction with Canonical Score Distillation. (arXiv:2312.03795v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinzhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yikai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Junliang Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengkun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Ling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Kai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bin He</a></p>
<p>Text-to-3D model adaptations have advanced static 3D model quality, but
sequential 3D model generation, particularly for animatable objects with large
motions, is still scarce. Our work proposes AnimatableDreamer, a text-to-4D
generation framework capable of generating diverse categories of non-rigid
objects while adhering to the object motions extracted from a monocular video.
At its core, AnimatableDreamer is equipped with our novel optimization design
dubbed Canonical Score Distillation (CSD), which simplifies the generation
dimension from 4D to 3D by denoising over different frames in the time-varying
camera spaces while conducting the distillation process in a unique canonical
space shared per video. Concretely, CSD ensures that score gradients
back-propagate to the canonical space through differentiable warping, hence
guaranteeing the time-consistent generation and maintaining morphological
plausibility across different poses. By lifting the 3D generator to 4D with
warping functions, AnimatableDreamer offers a novel perspective on non-rigid 3D
model generation and reconstruction. Besides, with inductive knowledge from a
multi-view consistent diffusion model, CSD regularizes reconstruction from
novel views, thus cyclically enhancing the generation process. Extensive
experiments demonstrate the capability of our method in generating
high-flexibility text-guided 3D models from the monocular video, while also
showing improved reconstruction performance over typical non-rigid
reconstruction methods. Project page https://AnimatableDreamer.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03798">Single Image Reflection Removal with Reflection Intensity Prior Knowledge. (arXiv:2312.03798v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">Dongshen Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seungkyu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1">Heechan Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1">Hyukmin Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">HyunCheol Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choo_H/0/1/0/all/0/1">HyonGon Choo</a></p>
<p>Single Image Reflection Removal (SIRR) in real-world images is a challenging
task due to diverse image degradations occurring on the glass surface during
light transmission and reflection. Many existing methods rely on specific prior
assumptions to resolve the problem. In this paper, we propose a general
reflection intensity prior that captures the intensity of the reflection
phenomenon and demonstrate its effectiveness. To learn the reflection intensity
prior, we introduce the Reflection Prior Extraction Network (RPEN). By
segmenting images into regional patches, RPEN learns non-uniform reflection
prior in an image. We propose Prior-based Reflection Removal Network (PRRN)
using a simple transformer U-Net architecture that adapts reflection prior fed
from RPEN. Experimental results on real-world benchmarks demonstrate the
effectiveness of our approach achieving state-of-the-art accuracy in SIRR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03799">Low-power, Continuous Remote Behavioral Localization with Event Cameras. (arXiv:2312.03799v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hamann_F/0/1/0/all/0/1">Friedhelm Hamann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Suman Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_I/0/1/0/all/0/1">Ignacio Juarez Martinez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hart_T/0/1/0/all/0/1">Tom Hart</a>, <a href="http://arxiv.org/find/cs/1/au:+Kacelnik_A/0/1/0/all/0/1">Alex Kacelnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1">Guillermo Gallego</a></p>
<p>Researchers in natural science need reliable methods for quantifying animal
behavior. Recently, numerous computer vision methods emerged to automate the
process. However, observing wild species at remote locations remains a
challenging task due to difficult lighting conditions and constraints on power
supply and data storage. Event cameras offer unique advantages for
battery-dependent remote monitoring due to their low power consumption and high
dynamic range capabilities. We use this novel sensor to quantify a behavior in
Chinstrap penguins called ecstatic display. We formulate the problem as a
temporal action detection task, determining the start and end times of the
behavior. For this purpose, we recorded a colony of breeding penguins in
Antarctica during several weeks and labeled event data on 16 nests. The
developed method consists of a generator of candidate time intervals
(proposals) and a classifier of the actions within them. The experiments show
that the event cameras' natural response to motion is effective for continuous
behavior monitoring and detection, reaching a mean average precision (mAP) of
58% (which increases to 63% in good weather conditions). The results also
demonstrate the robustness against various lighting conditions contained in the
challenging dataset. The low-power capabilities of the event camera allows to
record three times longer than with a conventional camera. This work pioneers
the use of event cameras for remote wildlife observation, opening new
interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03804">How Low Can You Go? Surfacing Prototypical In-Distribution Samples for Unsupervised Anomaly Detection. (arXiv:2312.03804v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meissen_F/0/1/0/all/0/1">Felix Meissen</a>, <a href="http://arxiv.org/find/cs/1/au:+Getzner_J/0/1/0/all/0/1">Johannes Getzner</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1">Alexander Ziller</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1">Georgios Kaissis</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a></p>
<p>Unsupervised anomaly detection (UAD) alleviates large labeling efforts by
training exclusively on unlabeled in-distribution data and detecting outliers
as anomalies. Generally, the assumption prevails that large training datasets
allow the training of higher-performing UAD models. However, in this work, we
show that using only very few training samples can already match - and in some
cases even improve - anomaly detection compared to training with the whole
training dataset. We propose three methods to identify prototypical samples
from a large dataset of in-distribution samples. We demonstrate that by
training with a subset of just ten such samples, we achieve an area under the
receiver operating characteristics curve (AUROC) of $96.37 \%$ on CIFAR10,
$92.59 \%$ on CIFAR100, $95.37 \%$ on MNIST, $95.38 \%$ on Fashion-MNIST,
$96.37 \%$ on MVTec-AD, $98.81 \%$ on BraTS, and $81.95 \%$ on RSNA pneumonia
detection, even exceeding the performance of full training in $25/67$ classes
we tested. Additionally, we show that the prototypical in-distribution samples
identified by our proposed methods translate well to different models and other
datasets and that using their characteristics as guidance allows for successful
manual selection of small subsets of high-performing samples. Our code is
available at https://anonymous.4open.science/r/uad_prototypical_samples/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03805">SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited Scenarios. (arXiv:2312.03805v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mushui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Weijie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Ziqian Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yunlong Yu</a></p>
<p>Prompt learning is a powerful technique for transferring Vision-Language
Models (VLMs) such as CLIP to downstream tasks. However, the prompt-based
methods that are fine-tuned solely with base classes may struggle to generalize
to novel classes in open-vocabulary scenarios, especially when data are
limited. To address this issue, we propose an innovative approach called
SYNC-CLIP that leverages SYNthetiC data for enhancing the generalization
capability of CLIP. Based on the observation of the distribution shift between
the real and synthetic samples, we treat real and synthetic samples as distinct
domains and propose to optimize separate domain prompts to capture
domain-specific information, along with the shared visual prompts to preserve
the semantic consistency between two domains. By aligning the cross-domain
features, the synthetic data from novel classes can provide implicit guidance
to rebalance the decision boundaries. Experimental results on three model
generalization tasks demonstrate that our method performs very competitively
across various benchmarks. Notably, SYNC-CLIP outperforms the state-of-the-art
competitor PromptSRC by an average improvement of 3.0% on novel classes across
11 datasets in open-vocabulary scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03806">XCube ($\mathcal{X}^3$): Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies. (arXiv:2312.03806v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xuanchi Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiahui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xiaohui Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Museth_K/0/1/0/all/0/1">Ken Museth</a>, <a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1">Sanja Fidler</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_F/0/1/0/all/0/1">Francis Williams</a></p>
<p>We present $\mathcal{X}^3$ (pronounced XCube), a novel generative model for
high-resolution sparse 3D voxel grids with arbitrary attributes. Our model can
generate millions of voxels with a finest effective resolution of up to
$1024^3$ in a feed-forward fashion without time-consuming test-time
optimization. To achieve this, we employ a hierarchical voxel latent diffusion
model which generates progressively higher resolution grids in a coarse-to-fine
manner using a custom framework built on the highly efficient VDB data
structure. Apart from generating high-resolution objects, we demonstrate the
effectiveness of XCube on large outdoor scenes at scales of 100m$\times$100m
with a voxel size as small as 10cm. We observe clear qualitative and
quantitative improvements over past approaches. In addition to unconditional
generation, we show that our model can be used to solve a variety of tasks such
as user-guided editing, scene completion from a single scan, and text-to-3D.
More results and details can be found at
https://research.nvidia.com/labs/toronto-ai/xcube/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03808">SurfaceAug: Closing the Gap in Multimodal Ground Truth Sampling. (arXiv:2312.03808v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rubel_R/0/1/0/all/0/1">Ryan Rubel</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_N/0/1/0/all/0/1">Nathan Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudash_A/0/1/0/all/0/1">Andrew Dudash</a></p>
<p>Despite recent advances in both model architectures and data augmentation,
multimodal object detectors still barely outperform their LiDAR-only
counterparts. This shortcoming has been attributed to a lack of sufficiently
powerful multimodal data augmentation. To address this, we present SurfaceAug,
a novel ground truth sampling algorithm. SurfaceAug pastes objects by
resampling both images and point clouds, enabling object-level transformations
in both modalities. We evaluate our algorithm by training a multimodal detector
on KITTI and compare its performance to previous works. We show experimentally
that SurfaceAug outperforms existing methods on car detection tasks and
establishes a new state of the art for multimodal ground truth sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03812">Seeing the random forest through the decision trees. Supporting learning health systems from histopathology with machine learning models: Challenges and opportunities. (arXiv:2312.03812v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_R/0/1/0/all/0/1">Ricardo Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1">Ashirbani Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_C/0/1/0/all/0/1">Clinton J.V. Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Nejat_P/0/1/0/all/0/1">Peyman Nejat</a>, <a href="http://arxiv.org/find/cs/1/au:+Lokker_C/0/1/0/all/0/1">Cynthia Lokker</a>, <a href="http://arxiv.org/find/cs/1/au:+Norgan_A/0/1/0/all/0/1">Andrew P. Norgan</a></p>
<p>This paper discusses some overlooked challenges faced when working with
machine learning models for histopathology and presents a novel opportunity to
support "Learning Health Systems" with them. Initially, the authors elaborate
on these challenges after separating them according to their mitigation
strategies: those that need innovative approaches, time, or future
technological capabilities and those that require a conceptual reappraisal from
a critical perspective. Then, a novel opportunity to support "Learning Health
Systems" by integrating hidden information extracted by ML models from
digitalized histopathology slides with other healthcare big data is presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03816">AVID: Any-Length Video Inpainting with Diffusion Model. (arXiv:2312.03816v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhixing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bichen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yaqiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Luxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yinan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1">Peter Vajda</a>, <a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1">Dimitris Metaxas</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Licheng Yu</a></p>
<p>Recent advances in diffusion models have successfully enabled text-guided
image inpainting. While it seems straightforward to extend such editing
capability into video domain, there has been fewer works regarding text-guided
video inpainting. Given a video, a masked region at its initial frame, and an
editing prompt, it requires a model to do infilling at each frame following the
editing guidance while keeping the out-of-mask region intact. There are three
main challenges in text-guided video inpainting: ($i$) temporal consistency of
the edited video, ($ii$) supporting different inpainting types at different
structural fidelity level, and ($iii$) dealing with variable video length. To
address these challenges, we introduce Any-Length Video Inpainting with
Diffusion Model, dubbed as AVID. At its core, our model is equipped with
effective motion modules and adjustable structure guidance, for fixed-length
video inpainting. Building on top of that, we propose a novel Temporal
MultiDiffusion sampling pipeline with an middle-frame attention guidance
mechanism, facilitating the generation of videos with any desired duration. Our
comprehensive experiments show our model can robustly deal with various
inpainting types at different video duration range, with high quality. More
visualization results is made publicly available at
https://zhang-zx.github.io/AVID/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03817">Diffusion Illusions: Hiding Images in Plain Sight. (arXiv:2312.03817v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Burgert_R/0/1/0/all/0/1">Ryan Burgert</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Leite_A/0/1/0/all/0/1">Abe Leite</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1">Kanchana Ranasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael S. Ryoo</a></p>
<p>We explore the problem of computationally generating special `prime' images
that produce optical illusions when physically arranged and viewed in a certain
way. First, we propose a formal definition for this problem. Next, we introduce
Diffusion Illusions, the first comprehensive pipeline designed to automatically
generate a wide range of these illusions. Specifically, we both adapt the
existing `score distillation loss' and propose a new `dream target loss' to
optimize a group of differentially parametrized prime images, using a frozen
text-to-image diffusion model. We study three types of illusions, each where
the prime images are arranged in different ways and optimized using the
aforementioned losses such that images derived from them align with user-chosen
text prompts or images. We conduct comprehensive experiments on these illusions
and verify the effectiveness of our proposed method qualitatively and
quantitatively. Additionally, we showcase the successful physical fabrication
of our illusions -- as they are all designed to work in the real world. Our
code and examples are publicly available at our interactive project website:
https://diffusionillusions.com
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03818">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want. (arXiv:2312.03818v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zeyi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Ye Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1">Yuhang Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1">Shu Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yuanjun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a></p>
<p>Contrastive Language-Image Pre-training (CLIP) plays an essential role in
extracting valuable content information from images across diverse tasks. It
aligns textual and visual modalities to comprehend the entire image, including
all the details, even those irrelevant to specific tasks. However, for a finer
understanding and controlled editing of images, it becomes crucial to focus on
specific regions of interest, which can be indicated as points, masks, or boxes
by humans or perception models. To fulfill the requirements, we introduce
Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to
suggest attentive regions and fine-tuned with constructed millions of RGBA
region-text pairs. Alpha-CLIP not only preserves the visual recognition ability
of CLIP but also enables precise control over the emphasis of image contents.
It demonstrates effectiveness in various tasks, including but not limited to
open-world recognition, multimodal large language models, and conditional 2D /
3D generation. It has a strong potential to serve as a versatile tool for
image-related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03849">LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning. (arXiv:2312.03849v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1">Bolin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xiaoliang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lawrence Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guan Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1">James M. Rehg</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Miao Liu</a></p>
<p>Generating instructional images of human daily actions from an egocentric
viewpoint serves a key step towards efficient skill transfer. In this paper, we
introduce a novel problem -- egocentric action frame generation. The goal is to
synthesize the action frame conditioning on the user prompt question and an
input egocentric image that captures user's environment. Notably, existing
egocentric datasets lack the detailed annotations that describe the execution
of actions. Additionally, the diffusion-based image manipulation models fail to
control the state change of an action within the corresponding egocentric image
pixel space. To this end, we finetune a visual large language model (VLLM) via
visual instruction tuning for curating the enriched action descriptions to
address our proposed problem. Moreover, we propose to Learn EGOcentric (LEGO)
action frame generation using image and text embeddings from VLLM as additional
conditioning. We validate our proposed model on two egocentric datasets --
Ego4D and Epic-Kitchens. Our experiments show prominent improvement over prior
image manipulation models in both quantitative and qualitative evaluation. We
also conduct detailed ablation studies and analysis to provide insights on our
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03869">Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion. (arXiv:2312.03869v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prabhu_K/0/1/0/all/0/1">Kira Prabhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jane Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_L/0/1/0/all/0/1">Lynn Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1">Peter Hedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1">Dan B Goldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1">Ben Poole</a>, <a href="http://arxiv.org/find/cs/1/au:+Broxton_M/0/1/0/all/0/1">Michael Broxton</a></p>
<p>This paper presents a novel approach to inpainting 3D regions of a scene,
given masked multi-view images, by distilling a 2D diffusion model into a
learned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods
that explicitly condition the diffusion model on camera pose or multi-view
information, our diffusion model is conditioned only on a single masked 2D
image. Nevertheless, we show that this 2D diffusion model can still serve as a
generative prior in a 3D multi-view reconstruction problem where we optimize a
NeRF using a combination of score distillation sampling and NeRF reconstruction
losses. Predicted depth is used as additional supervision to encourage accurate
geometry. We compare our approach to 3D inpainting methods that focus on object
removal. Because our method can generate content to fill any 3D masked region,
we additionally demonstrate 3D object completion, 3D object replacement, and 3D
scene completion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03884">WonderJourney: Going from Anywhere to Everywhere. (arXiv:2312.03884v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong-Xing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1">Haoyi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1">Junhwa Hur</a>, <a href="http://arxiv.org/find/cs/1/au:+Sargent_K/0/1/0/all/0/1">Kyle Sargent</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1">Michael Rubinstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1">William T. Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1">Forrester Cole</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1">Deqing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1">Noah Snavely</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1">Charles Herrmann</a></p>
<p>We introduce WonderJourney, a modularized framework for perpetual 3D scene
generation. Unlike prior work on view generation that focuses on a single type
of scenes, we start at any user-provided location (by a text description or an
image) and generate a journey through a long sequence of diverse yet coherently
connected 3D scenes. We leverage an LLM to generate textual descriptions of the
scenes in this journey, a text-driven point cloud generation pipeline to make a
compelling and coherent sequence of 3D scenes, and a large VLM to verify the
generated scenes. We show compelling, diverse visual results across various
scene types and styles, forming imaginary "wonderjourneys". Project website:
https://kovenyu.com/WonderJourney/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03913">Controllable Human-Object Interaction Synthesis. (arXiv:2312.03913v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaman Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1">Alexander Clegg</a>, <a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1">Roozbeh Mottaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1">Xavier Puig</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">C. Karen Liu</a></p>
<p>Synthesizing semantic-aware, long-horizon, human-object interaction is
critical to simulate realistic human behaviors. In this work, we address the
challenging problem of generating synchronized object motion and human motion
guided by language descriptions in 3D scenes. We propose Controllable
Human-Object Interaction Synthesis (CHOIS), an approach that generates object
motion and human motion simultaneously using a conditional diffusion model
given a language description, initial object and human states, and sparse
object waypoints. While language descriptions inform style and intent,
waypoints ground the motion in the scene and can be effectively extracted using
high-level planning methods. Naively applying a diffusion model fails to
predict object motion aligned with the input waypoints and cannot ensure the
realism of interactions that require precise hand-object contact and
appropriate contact grounded by the floor. To overcome these problems, we
introduce an object geometry loss as additional supervision to improve the
matching between generated object motion and input object waypoints. In
addition, we design guidance terms to enforce contact constraints during the
sampling process of the trained diffusion model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03936">The Potential of Vision-Language Models for Content Moderation of Children&#x27;s Videos. (arXiv:2312.03936v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Syed Hammad Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shengnan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukthankar_G/0/1/0/all/0/1">Gita Sukthankar</a></p>
<p>Natural language supervision has been shown to be effective for zero-shot
learning in many computer vision tasks, such as object detection and activity
recognition. However, generating informative prompts can be challenging for
more subtle tasks, such as video content moderation. This can be difficult, as
there are many reasons why a video might be inappropriate, beyond violence and
obscenity. For example, scammers may attempt to create junk content that is
similar to popular educational videos but with no meaningful information. This
paper evaluates the performance of several CLIP variations for content
moderation of children's cartoons in both the supervised and zero-shot setting.
We show that our proposed model (Vanilla CLIP with Projection Layer)
outperforms previous work conducted on the Malicious or Benign (MOB) benchmark
for video content moderation. This paper presents an in depth analysis of how
context-specific language prompts affect content moderation performance. Our
results indicate that it is important to include more context in content
moderation prompts, particularly for cartoon videos as they are not well
represented in the CLIP training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03938">Adapting HouseDiffusion for conditional Floor Plan generation on Modified Swiss Dwellings dataset. (arXiv:2312.03938v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuhn_E/0/1/0/all/0/1">Emanuel Kuhn</a></p>
<p>Automated floor plan generation has recently gained momentum with several
methods that have been proposed. The CVAAD Floor Plan Auto-Completion workshop
challenge introduced MSD, a new dataset that includes existing structural walls
of the building as an additional input constraint. This technical report
presents an approach for extending a recent work, HouseDiffusion
(<a href="/abs/2211.13287">arXiv:2211.13287</a> [cs.CV]), to the MSD dataset. The adaption involves modifying
the model's transformer layers to condition on a set of wall lines. The report
introduces a pre-processing pipeline to extract wall lines from the binary mask
of the building structure provided as input. Additionally, it was found that a
data processing procedure that simplifies all room polygons to rectangles leads
to better performance. This indicates that future work should explore better
representations of variable-length polygons in diffusion models. The code will
be made available at a later date.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03946">A Layer-Wise Tokens-to-Token Transformer Network for Improved Historical Document Image Enhancement. (arXiv:2312.03946v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biswas_R/0/1/0/all/0/1">Risab Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Swalpa Kumar Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1">Umapada Pal</a></p>
<p>Document image enhancement is a fundamental and important stage for attaining
the best performance in any document analysis assignment because there are many
degradation situations that could harm document images, making it more
difficult to recognize and analyze them. In this paper, we propose
\textbf{T2T-BinFormer} which is a novel document binarization encoder-decoder
architecture based on a Tokens-to-token vision transformer. Each image is
divided into a set of tokens with a defined length using the ViT model, which
is then applied several times to model the global relationship between the
tokens. However, the conventional tokenization of input data does not
adequately reflect the crucial local structure between adjacent pixels of the
input image, which results in low efficiency. Instead of using a simple ViT and
hard splitting of images for the document image enhancement task, we employed a
progressive tokenization technique to capture this local information from an
image to achieve more effective results. Experiments on various DIBCO and
H-DIBCO benchmarks demonstrate that the proposed model outperforms the existing
CNN and ViT-based state-of-the-art methods. In this research, the primary area
of examination is the application of the proposed architecture to the task of
document binarization. The source code will be made available at
https://github.com/RisabBiswas/T2T-BinFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03970">Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models. (arXiv:2312.03970v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shibin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zhiyu Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hairong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>Medical report generation demands automatic creation of coherent and precise
descriptions for medical images. However, the scarcity of labelled medical
image-report pairs poses formidable challenges in developing large-scale neural
networks capable of harnessing the potential of artificial intelligence,
exemplified by large language models. This study builds upon the
state-of-the-art vision-language pre-training and fine-tuning approach, BLIP-2,
to customize general large-scale foundation models. Integrating adapter tuning
and a medical knowledge enhancement loss, our model significantly improves
accuracy and coherence. Validation on the dataset of ImageCLEFmedical 2023
demonstrates our model's prowess, achieving the best-averaged results against
several state-of-the-art methods. Significant improvements in ROUGE and CIDEr
underscore our method's efficacy, highlighting promising outcomes for the rapid
medical-domain adaptation of the vision-language foundation models in
addressing challenges posed by data scarcity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03993">Style Transfer to Calvin and Hobbes comics using Stable Diffusion. (arXiv:2312.03993v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1">Sloke Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+S%2E_S/0/1/0/all/0/1">Sundar Sripada V. S.</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkataramanan_A/0/1/0/all/0/1">Asvin Venkataramanan</a></p>
<p>This project report summarizes our journey to perform stable diffusion
fine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to
convert any given input image into the comic style of Calvin and Hobbes,
essentially performing style transfer. We train stable-diffusion-v1.5 using Low
Rank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The
diffusion itself is handled by a Variational Autoencoder (VAE), which is a
U-net. Our results were visually appealing for the amount of training time and
the quality of input data that went into training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03996">Stable diffusion for Data Augmentation in COCO and Weed Datasets. (arXiv:2312.03996v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1">Boyang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuzhen Lu</a></p>
<p>Generative models have increasingly impacted relative tasks ranging from
image revision and object detection in computer vision to interior design and
idea illustration in more general fields. Stable diffusion is an outstanding
model series that paves the way for producing high-resolution images with
thorough details from text prompts or reference images. It will be an
interesting topic about how to leverage the capability of stable diffusion to
elevate the image variations of certain categories (e.g., vehicles, humans, and
daily objects); particularly, it has the potential to gain improvements for
small datasets with image-sparse categories. This study utilized seven
categories in the popular COCO dataset and three widespread weed species in
Michigan to evaluate the efficiency of a recent version of stable diffusion. In
detail, Stable diffusion was used to generate synthetic images belonging to
these classes; then, YOLOv8 models were trained based on these synthetic
images, whose performance was compared to the models trained on original
images. In addition, several techniques (e.g., Image-to-image translation,
Dreambooth, ControlNet) of Stable diffusion were leveraged for image generation
with different focuses. In spite of the overall results being disappointing,
promising results have been achieved in some classes, illustrating the
potential of stable diffusion models to improve the performance of detection
models, which represent more helpful information being conveyed into the models
by the generated images. This seminal study may expedite the adaption of stable
diffusion models to classification and detection tasks in different fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04000">LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures. (arXiv:2312.04000v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thilak_V/0/1/0/all/0/1">Vimal Thilak</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Saremi_O/0/1/0/all/0/1">Omid Saremi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinh_L/0/1/0/all/0/1">Laurent Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1">Hanlin Goh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1">Preetum Nakkiran</a>, <a href="http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1">Joshua M. Susskind</a>, <a href="http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1">Etai Littwin</a></p>
<p>Joint embedding (JE) architectures have emerged as a promising avenue for
acquiring transferable data representations. A key obstacle to using JE
methods, however, is the inherent challenge of evaluating learned
representations without access to a downstream task, and an annotated dataset.
Without efficient and reliable evaluation, it is difficult to iterate on
architectural and training choices for JE methods. In this paper, we introduce
LiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the
quality of representations within JE architectures. Our metric addresses
several shortcomings of recent approaches based on feature covariance rank by
discriminating between informative and uninformative features. In essence,
LiDAR quantifies the rank of the Linear Discriminant Analysis (LDA) matrix
associated with the surrogate SSL task -- a measure that intuitively captures
the information content as it pertains to solving the SSL task. We empirically
demonstrate that LiDAR significantly surpasses naive rank based approaches in
its predictive power of optimal hyperparameters. Our proposed criterion
presents a more robust and intuitive means of assessing the quality of
representations within JE architectures, which we hope facilitates broader
adoption of these powerful techniques in various domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04005">KOALA: Self-Attention Matters in Knowledge Distillation of Latent Diffusion Models for Memory-Efficient and Fast Image Synthesis. (arXiv:2312.04005v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Youngwan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1">Kwanyong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Yoorhim Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yong-Ju Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a></p>
<p>Stable diffusion is the mainstay of the text-to-image (T2I) synthesis in the
community due to its generation performance and open-source nature. Recently,
Stable Diffusion XL (SDXL), the successor of stable diffusion, has received a
lot of attention due to its significant performance improvements with a higher
resolution of 1024x1024 and a larger model. However, its increased computation
cost and model size require higher-end hardware(e.g., bigger VRAM GPU) for
end-users, incurring higher costs of operation. To address this problem, in
this work, we propose an efficient latent diffusion model for text-to-image
synthesis obtained by distilling the knowledge of SDXL. To this end, we first
perform an in-depth analysis of the denoising U-Net in SDXL, which is the main
bottleneck of the model, and then design a more efficient U-Net based on the
analysis. Secondly, we explore how to effectively distill the generation
capability of SDXL into an efficient U-Net and eventually identify four
essential factors, the core of which is that self-attention is the most
important part. With our efficient U-Net and self-attention-based knowledge
distillation strategy, we build our efficient T2I models, called KOALA-1B &amp;
-700M, while reducing the model size up to 54% and 69% of the original SDXL
model. In particular, the KOALA-700M is more than twice as fast as SDXL while
still retaining a decent generation quality. We hope that due to its balanced
speed-performance tradeoff, our KOALA models can serve as a cost-effective
alternative to SDXL in resource-constrained environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04008">Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes. (arXiv:2312.04008v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kairui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zihao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Gengjie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Haotian Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_D/0/1/0/all/0/1">Die Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jibin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhecheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fupeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1">Ziyun Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Di Lin</a></p>
<p>We advocate the idea of the natural-language-driven(NLD) simulation to
efficiently produce the object interactions between multiple objects in the
virtual road scenes, for teaching and testing the autonomous driving systems
that should take quick action to avoid collision with obstacles with
unpredictable motions. The NLD simulation allows the brief natural-language
description to control the object interactions, significantly reducing the
human efforts for creating a large amount of interaction data. To facilitate
the research of NLD simulation, we collect the Language-to-Interaction(L2I)
benchmark dataset with 120,000 natural-language descriptions of object
interactions in 6 common types of road topologies. Each description is
associated with the programming code, which the graphic render can use to
visually reconstruct the object interactions in the virtual scenes. As a
methodology contribution, we design SimCopilot to translate the interaction
descriptions to the renderable code. We use the L2I dataset to evaluate
SimCopilot's abilities to control the object motions, generate complex
interactions, and generalize interactions across road topologies. The L2I
dataset and the evaluation results motivate the relevant research of the NLD
simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04016">PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation. (arXiv:2312.04016v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Umam_A/0/1/0/all/0/1">Ardian Umam</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng-Kun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Min-Hung Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chuang_J/0/1/0/all/0/1">Jen-Hui Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yen-Yu Lin</a></p>
<p>This paper proposes a cross-modal distillation framework, PartDistill, which
transfers 2D knowledge from vision-language models (VLMs) to facilitate 3D
shape part segmentation. PartDistill addresses three major challenges in this
task: the lack of 3D segmentation in invisible or undetected regions in the 2D
projections, inaccurate and inconsistent 2D predictions by VLMs, and the lack
of knowledge accumulation across different 3D shapes. PartDistill consists of a
teacher network that uses a VLM to make 2D predictions and a student network
that learns from the 2D predictions while extracting geometrical features from
multiple 3D shapes to carry out 3D part segmentation. A bi-directional
distillation, including forward and backward distillations, is carried out
within the framework, where the former forward distills the 2D predictions to
the student network, and the latter improves the quality of the 2D predictions,
which subsequently enhances the final 3D part segmentation. Moreover,
PartDistill can exploit generative models that facilitate effortless 3D shape
creation for generating knowledge sources to be distilled. Through extensive
experiments, PartDistill boosts the existing methods with substantial margins
on widely used ShapeNetPart and PartE datasets, by more than 15% and 12% higher
mIoU scores, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04024">k* Distribution: Evaluating the Latent Space of Deep Neural Networks using Local Neighborhood Analysis. (arXiv:2312.04024v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tatsuya_U/0/1/0/all/0/1">Ueda Tatsuya</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a></p>
<p>Most examinations of neural networks' learned latent spaces typically employ
dimensionality reduction techniques such as t-SNE or UMAP. While these methods
effectively capture the overall sample distribution in the entire learned
latent space, they tend to distort the structure of sample distributions within
specific classes in the subset of the latent space. This distortion complicates
the task of easily distinguishing classes identifiable by neural networks. In
response to this challenge, we introduce the k* Distribution methodology. This
approach focuses on capturing the characteristics and structure of sample
distributions for individual classes within the subset of the learned latent
space using local neighborhood analysis. The key concept is to facilitate easy
comparison of different k* distributions, enabling analysis of how various
classes are processed by the same neural network. This provides a more profound
understanding of existing contemporary visualizations. Our study reveals three
distinct distributions of samples within the learned latent space subset: a)
Fractured, b) Overlapped, and c) Clustered. We note and demonstrate that the
distribution of samples within the network's learned latent space significantly
varies depending on the class. Furthermore, we illustrate that our analysis can
be applied to explore the latent space of diverse neural network architectures,
various layers within neural networks, transformations applied to input
samples, and the distribution of training and testing data for neural networks.
We anticipate that our approach will facilitate more targeted investigations
into neural networks by collectively examining the distribution of different
samples within the learned latent space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04028">ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with Implicit Neural Representations. (arXiv:2312.04028v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingwu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Di Huang</a></p>
<p>Accurate representations of 3D faces are of paramount importance in various
computer vision and graphics applications. However, the challenges persist due
to the limitations imposed by data discretization and model linearity, which
hinder the precise capture of identity and expression clues in current studies.
This paper presents a novel 3D morphable face model, named ImFace++, to learn a
sophisticated and continuous space with implicit neural representations.
ImFace++ first constructs two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
which simultaneously facilitate the automatic learning of correspondences
across diverse facial shapes. To capture more sophisticated facial details, a
refinement displacement field within the template space is further
incorporated, enabling a fine-grained learning of individual-specific facial
details. Furthermore, a Neural Blend-Field is designed to reinforce the
representation capabilities through adaptive blending of an array of local
fields. In addition to ImFace++, we have devised an improved learning strategy
to extend expression embeddings, allowing for a broader range of expression
variations. Comprehensive qualitative and quantitative evaluations demonstrate
that ImFace++ significantly advances the state-of-the-art in terms of both face
reconstruction fidelity and correspondence accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04029">Improved Face Representation via Joint Label Classification and Supervised Contrastive Clustering. (arXiv:2312.04029v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenduo Zhang</a></p>
<p>Face clustering tasks can learn hierarchical semantic information from
large-scale data, which has the potential to help facilitate face recognition.
However, there are few works on this problem. This paper explores it by
proposing a joint optimization task of label classification and supervised
contrastive clustering to introduce the cluster knowledge to the traditional
face recognition task in two ways. We first extend ArcFace with a
cluster-guided angular margin to adjust the within-class feature distribution
according to the hard level of face clustering. Secondly, we propose a
supervised contrastive clustering approach to pull the features to the cluster
center and propose the cluster-aligning procedure to align the cluster center
and the learnable class center in the classifier for joint training. Finally,
extensive qualitative and quantitative experiments on popular facial benchmarks
demonstrate the effectiveness of our paradigm and its superiority over the
existing approaches to face recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04036">DiffusionPhase: Motion Diffusion in Frequency Domain. (arXiv:2312.04036v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1">Weilin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiming Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shutong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1">Dinesh Jayaraman</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingjie Liu</a></p>
<p>In this study, we introduce a learning-based method for generating
high-quality human motion sequences from text descriptions (e.g., ``A person
walks forward"). Existing techniques struggle with motion diversity and smooth
transitions in generating arbitrary-length motion sequences, due to limited
text-to-motion datasets and the pose representations used that often lack
expressiveness or compactness. To address these issues, we propose the first
method for text-conditioned human motion generation in the frequency domain of
motions. We develop a network encoder that converts the motion space into a
compact yet expressive parameterized phase space with high-frequency details
encoded, capturing the local periodicity of motions in time and space with high
accuracy. We also introduce a conditional diffusion model for predicting
periodic motion parameters based on text descriptions and a start pose,
efficiently achieving smooth transitions between motion sequences associated
with different text descriptions. Experiments demonstrate that our approach
outperforms current methods in generating a broader variety of high-quality
motions, and synthesizing long sequences with natural transitions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04043">Doodle Your 3D: From Abstract Freehand Sketches to Precise 3D Shapes. (arXiv:2312.04043v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_H/0/1/0/all/0/1">Hmrishav Bandyopadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Koley_S/0/1/0/all/0/1">Subhadeep Koley</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Ayan Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1">Aneeshan Sain</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1">Pinaki Nath Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1">Ayan Kumar Bhunia</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yi-Zhe Song</a></p>
<p>In this paper, we democratise 3D content creation, enabling precise
generation of 3D shapes from abstract sketches while overcoming limitations
tied to drawing skills. We introduce a novel part-level modelling and alignment
framework that facilitates abstraction modelling and cross-modal
correspondence. Leveraging the same part-level decoder, our approach seamlessly
extends to sketch modelling by establishing correspondence between CLIPasso
edgemaps and projected 3D part regions, eliminating the need for a dataset
pairing human sketches and 3D shapes. Additionally, our method introduces a
seamless in-position editing process as a byproduct of cross-modal part-aligned
modelling. Operating in a low-dimensional implicit space, our approach
significantly reduces computational demands and processing time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04044">Residual Graph Convolutional Network for Bird&#x27;s-Eye-View Semantic Segmentation. (arXiv:2312.04044v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qiuxiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiaojun Qi</a></p>
<p>Retrieving spatial information and understanding the semantic information of
the surroundings are important for Bird's-Eye-View (BEV) semantic segmentation.
In the application of autonomous driving, autonomous vehicles need to be aware
of their surroundings to drive safely. However, current BEV semantic
segmentation techniques, deep Convolutional Neural Networks (CNNs) and
transformers, have difficulties in obtaining the global semantic relationships
of the surroundings at the early layers of the network. In this paper, we
propose to incorporate a novel Residual Graph Convolutional (RGC) module in
deep CNNs to acquire both the global information and the region-level semantic
relationship in the multi-view image domain. Specifically, the RGC module
employs a non-overlapping graph space projection to efficiently project the
complete BEV information into graph space. It then builds interconnected
spatial and channel graphs to extract spatial information between each node and
channel information within each node (i.e., extract contextual relationships of
the global features). Furthermore, it uses a downsample residual process to
enhance the coordinate feature reuse to maintain the global information. The
segmentation data augmentation and alignment module helps to simultaneously
augment and align BEV features and ground truth to geometrically preserve their
alignment to achieve better segmentation results. Our experimental results on
the nuScenes benchmark dataset demonstrate that the RGC network outperforms
four state-of-the-art networks and its four variants in terms of IoU and mIoU.
The proposed RGC network achieves a higher mIoU of 3.1% than the best
state-of-the-art network, BEVFusion. Code and models will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04060">Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching. (arXiv:2312.04060v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Junsheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1">Baorui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yi Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Shen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhizhong Han</a></p>
<p>Cross-modality registration between 2D images from cameras and 3D point
clouds from LiDARs is a crucial task in computer vision and robotic. Previous
methods estimate 2D-3D correspondences by matching point and pixel patterns
learned by neural networks, and use Perspective-n-Points (PnP) to estimate
rigid transformation during post-processing. However, these methods struggle to
map points and pixels to a shared latent space robustly since points and pixels
have very different characteristics with patterns learned in different manners
(MLP and CNN), and they also fail to construct supervision directly on the
transformation since the PnP is non-differentiable, which leads to unstable
registration results. To address these problems, we propose to learn a
structured cross-modality latent space to represent pixel features and 3D
features via a differentiable probabilistic PnP solver. Specifically, we design
a triplet network to learn VoxelPoint-to-Pixel matching, where we represent 3D
elements using both voxels and points to learn the cross-modality latent space
with pixels. We design both the voxel and pixel branch based on CNNs to operate
convolutions on voxels/pixels represented in grids, and integrate an additional
point branch to regain the information lost during voxelization. We train our
framework end-to-end by imposing supervisions directly on the predicted pose
distribution with a probabilistic PnP solver. To explore distinctive patterns
of cross-modality features, we design a novel loss with adaptive-weighted
optimization for cross-modality feature description. The experimental results
on KITTI and nuScenes datasets show significant improvements over the
state-of-the-art methods. The code and models are available at
https://github.com/junshengzhou/VP2P-Match.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04063">An unsupervised approach towards promptable defect segmentation in laser-based additive manufacturing by Segment Anything. (arXiv:2312.04063v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Era_I/0/1/0/all/0/1">Israt Zarin Era</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1">Imtiaz Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhichao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Srinjoy Das</a></p>
<p>Foundation models are currently driving a paradigm shift in computer vision
tasks for various fields including biology, astronomy, and robotics among
others, leveraging user-generated prompts to enhance their performance. In the
manufacturing domain, accurate image-based defect segmentation is imperative to
ensure product quality and facilitate real-time process control. However, such
tasks are often characterized by multiple challenges including the absence of
labels and the requirement for low latency inference among others. To address
these issues, we construct a framework for image segmentation using a
state-of-the-art Vision Transformer (ViT) based Foundation model (Segment
Anything Model) with a novel multi-point prompt generation scheme using
unsupervised clustering. We apply our framework to perform real-time porosity
segmentation in a case study of laser base powder bed fusion (L-PBF) and obtain
high Dice Similarity Coefficients (DSC) without the necessity for any
supervised fine-tuning in the model. Using such lightweight foundation model
inference in conjunction with unsupervised prompt generation, we envision the
construction of a real-time anomaly detection pipeline that has the potential
to revolutionize the current laser-based additive manufacturing processes,
thereby facilitating the shift towards Industry 4.0 and promoting defect-free
production along with operational efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04066">Combining inherent knowledge of vision-language models with unsupervised domain adaptation through self-knowledge distillation. (arXiv:2312.04066v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Westfechtel_T/0/1/0/all/0/1">Thomas Westfechtel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dexuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1">Tatsuya Harada</a></p>
<p>Unsupervised domain adaptation (UDA) tries to overcome the tedious work of
labeling data by leveraging a labeled source dataset and transferring its
knowledge to a similar but different target dataset. On the other hand, current
vision-language models exhibit astonishing zero-shot prediction capabilities.
In this work, we combine knowledge gained through UDA with the inherent
knowledge of vision-language models. In a first step, we generate the zero-shot
predictions of the source and target dataset using the vision-language model.
Since zero-shot predictions usually exhibit a large entropy, meaning that the
class probabilities are rather evenly distributed, we first adjust the
distribution to accentuate the winning probabilities. This is done using both
source and target data to keep the relative confidence between source and
target data. We then employ a conventional DA method, to gain the knowledge
from the source dataset, in combination with self-knowledge distillation, to
maintain the inherent knowledge of the vision-language model. We further
combine our method with a gradual source domain expansion strategy (GSDE) and
show that this strategy can also benefit by including zero-shot predictions. We
conduct experiments and ablation studies on three benchmarks (OfficeHome,
VisDA, and DomainNet) and outperform state-of-the-art methods. We further show
in ablation studies the contributions of different parts of our algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04076">Large Language Models are Good Prompt Learners for Low-Shot Image Classification. (arXiv:2312.04076v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhaoheng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jingmin Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xuefeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Haidong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1">Ram Nevatia</a></p>
<p>Low-shot image classification, where training images are limited or
inaccessible, has benefited from recent progress on pre-trained vision-language
(VL) models with strong generalizability, e.g. CLIP. Prompt learning methods
built with VL models generate text features from the class names that only have
confined class-specific information. Large Language Models (LLMs), with their
vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we
discuss the integration of LLMs to enhance pre-trained VL models, specifically
on low-shot classification. However, the domain gap between language and vision
blocks the direct application of LLMs. Thus, we propose LLaMP, Large Language
Models as Prompt learners, that produces adaptive prompts for the CLIP text
encoder, establishing it as the connecting bridge. Experiments show that,
compared with other state-of-the-art prompt learning methods, LLaMP yields
better performance on both zero-shot generalization and few-shot image
classification, over a spectrum of 11 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04086">MTVG : Multi-text Video Generation with Text-to-Video Models. (arXiv:2312.04086v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1">Gyeongrok Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1">Jaehwan Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sieun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1">Wonmin Byeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinkyu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungwoong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1">Hyeokmin Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangpil Kim</a></p>
<p>Recently, video generation has attracted massive attention and yielded
noticeable outcomes. Concerning the characteristics of video, multi-text
conditioning incorporating sequential events is necessary for next-step video
generation. In this work, we propose a novel multi-text video generation~(MTVG)
by directly utilizing a pre-trained diffusion-based text-to-video~(T2V)
generation model without additional fine-tuning. To generate consecutive video
segments, visual consistency generated by distinct prompts is necessary with
diverse variations, such as motion and content-related transitions. Our
proposed MTVG includes Dynamic Noise and Last Frame Aware Inversion which
reinitialize the noise latent to preserve visual coherence between videos of
different prompts and prevent repetitive motion or contents. Furthermore, we
present Structure Guiding Sampling to maintain the global appearance across the
frames in a single video clip, where we leverage iterative latent updates
across the preceding frame. Additionally, our Prompt Generator allows for
arbitrary format of text conditions consisting of diverse events. As a result,
our extensive experiments, including diverse transitions of descriptions,
demonstrate that our proposed methods show superior generated outputs in terms
of semantically coherent and temporally seamless video.Video examples are
available in our project page: https://kuai-lab.github.io/mtvg-page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04087">VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models. (arXiv:2312.04087v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaozheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chaowei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1">Pingchuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Daoyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Cuiyun Gao</a></p>
<p>With recent advancements in Large Multimodal Models (LMMs) across various
domains, a novel prompting method called visual referring prompting has
emerged, showing significant potential in enhancing human-computer interaction
within multimodal systems. This method offers a more natural and flexible
approach to human interaction with these systems compared to traditional text
descriptions or coordinates. However, the categorization of visual referring
prompting remains undefined, and its impact on the performance of LMMs has yet
to be formally examined. In this study, we conduct the first comprehensive
analysis of LMMs using a variety of visual referring prompting strategies. We
introduce a benchmark dataset called VRPTEST, comprising 3 different visual
tasks and 2,275 images, spanning diverse combinations of prompt strategies.
Using VRPTEST, we conduct a comprehensive evaluation of eight versions of
prominent open-source and proprietary foundation models, including two early
versions of GPT-4V. We develop an automated assessment framework based on
software metamorphic testing techniques to evaluate the accuracy of LMMs
without the need for human intervention or manual labeling. We find that the
current proprietary models generally outperform the open-source ones, showing
an average accuracy improvement of 22.70%; however, there is still potential
for improvement. Moreover, our quantitative analysis shows that the choice of
prompt strategy significantly affects the accuracy of LMMs, with variations
ranging from -17.5% to +7.3%. Further case studies indicate that an appropriate
visual referring prompting strategy can improve LMMs' understanding of context
and location information, while an unsuitable one might lead to answer
rejection. We also provide insights on minimizing the negative impact of visual
referring prompting on LMMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04089">Open-Vocabulary Segmentation with Semantic-Assisted Calibration. (arXiv:2312.04089v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Sule Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yitong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yansong Tang</a></p>
<p>This paper studies open-vocabulary segmentation (OVS) through calibrating
in-vocabulary and domain-biased embedding space with generalized contextual
prior of CLIP. As the core of open-vocabulary understanding, alignment of
visual content with the semantics of unbounded text has become the bottleneck
of this field. To address this challenge, recent works propose to utilize CLIP
as an additional classifier and aggregate model predictions with CLIP
classification results. Despite their remarkable progress, performance of OVS
methods in relevant scenarios is still unsatisfactory compared with supervised
counterparts. We attribute this to the in-vocabulary embedding and
domain-biased CLIP prediction. To this end, we present a Semantic-assisted
CAlibration Network (SCAN). In SCAN, we incorporate generalized semantic prior
of CLIP into proposal embedding to avoid collapsing on known categories.
Besides, a contextual shift strategy is applied to mitigate the lack of global
context and unnatural background noise. With above designs, SCAN achieves
state-of-the-art performance on all popular open-vocabulary segmentation
benchmarks. Furthermore, we also focus on the problem of existing evaluation
system that ignores semantic duplication across categories, and propose a new
metric called Semantic-Guided IoU (SG-IoU).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04095">Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning Interference with Gradient Projection. (arXiv:2312.04095v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Tuan Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1">Santu Rana</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Sunil Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1">Svetha Venkatesh</a></p>
<p>Recent data-privacy laws have sparked interest in machine unlearning, which
involves removing the effect of specific training samples from a learnt model
as if they were never present in the original training dataset. The challenge
of machine unlearning is to discard information about the ``forget'' data in
the learnt model without altering the knowledge about the remaining dataset and
to do so more efficiently than the naive retraining approach. To achieve this,
we adopt a projected-gradient based learning method, named as
Projected-Gradient Unlearning (PGU), in which the model takes steps in the
orthogonal direction to the gradient subspaces deemed unimportant for the
retaining dataset, so as to its knowledge is preserved. By utilizing Stochastic
Gradient Descent (SGD) to update the model weights, our method can efficiently
scale to any model and dataset size. We provide empirically evidence to
demonstrate that our unlearning method can produce models that behave similar
to models retrained from scratch across various metrics even when the training
dataset is no longer accessible. Our code is available at
https://github.com/hnanhtuan/projected_gradient_unlearning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04106">Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial Reconstruction. (arXiv:2312.04106v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1">Jiayi Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Baixin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xurui Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jun Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Ying He</a></p>
<p>Neural radiance fields (NeRF) typically require a complete set of images
taken from multiple camera perspectives to accurately reconstruct geometric
details. However, this approach raise significant privacy concerns in the
context of facial reconstruction. The critical need for privacy protection
often leads invidividuals to be reluctant in sharing their facial images, due
to fears of potential misuse or security risks. Addressing these concerns, we
propose a method that leverages privacy-preserving images for reconstructing 3D
head geometry within the NeRF framework. Our method stands apart from
traditional facial reconstruction techniques as it does not depend on RGB
information from images containing sensitive facial data. Instead, it
effectively generates plausible facial geometry using a series of
identity-obscured inputs, thereby protecting facial privacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04113">Multi-strategy Collaborative Optimized YOLOv5s and its Application in Distance Estimation. (arXiv:2312.04113v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zijian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_Z/0/1/0/all/0/1">Zhenping Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangxiang Li</a></p>
<p>The increasing accident rate brought about by the explosive growth of
automobiles has made the research on active safety systems of automobiles
increasingly important. The importance of improving the accuracy of vehicle
target detection is self-evident. To achieve the goals of vehicle detection and
distance estimation and provide safety warnings, a Distance Estimation Safety
Warning System (DESWS) based on a new neural network model (YOLOv5s-SE) by
replacing the IoU with DIoU, embedding SE attention module, and a distance
estimation method through using the principle of similar triangles was
proposed. In addition, a method that can give safety suggestions based on the
estimated distance using nonparametric testing was presented in this work.
Through the simulation experiment, it was verified that the mAP was improved by
5.5% and the purpose of giving safety suggestions based on the estimated
distance information can be achieved.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04117">Instance Tracking in 3D Scenes from Egocentric Videos. (arXiv:2312.04117v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yunhan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1">Shu Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1">Charless Fowlkes</a></p>
<p>Egocentric sensors such as AR/VR devices capture human-object interactions
and offer the potential to provide task-assistance by recalling 3D locations of
objects of interest in the surrounding environment. This capability requires
instance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We
explore this problem by first introducing a new benchmark dataset, consisting
of RGB and depth videos, per-frame camera pose, and instance-level annotations
in both 2D camera and 3D world coordinates. We present an evaluation protocol
which evaluates tracking performance in 3D coordinates with two settings for
enrolling instances to track: (1) single-view online enrollment where an
instance is specified on-the-fly based on the human wearer's interactions. and
(2) multi-view pre-enrollment where images of an instance to be tracked are
stored in memory ahead of time. To address IT3DEgo, we first re-purpose methods
from relevant areas, e.g., single object tracking (SOT) -- running SOT methods
to track instances in 2D frames and lifting them to 3D using camera pose and
depth. We also present a simple method that leverages pretrained segmentation
and detection models to generate proposals from RGB frames and match proposals
with enrolled instance images. Perhaps surprisingly, our extensive experiments
show that our method (with no finetuning) significantly outperforms SOT-based
approaches. We conclude by arguing that the problem of egocentric instance
tracking is made easier by leveraging camera pose and using a 3D allocentric
(world) coordinate representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04118">Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play. (arXiv:2312.04118v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schaumloffel_T/0/1/0/all/0/1">Timothy Schauml&#xf6;ffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Aubret_A/0/1/0/all/0/1">Arthur Aubret</a>, <a href="http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1">Gemma Roig</a>, <a href="http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1">Jochen Triesch</a></p>
<p>Infants' ability to recognize and categorize objects develops gradually. The
second year of life is marked by both the emergence of more semantic visual
representations and a better understanding of word meaning. This suggests that
language input may play an important role in shaping visual representations.
However, even in suitable contexts for word learning like dyadic play sessions,
caregivers utterances are sparse and ambiguous, often referring to objects that
are different from the one to which the child attends. Here, we systematically
investigate to what extent caregivers' utterances can nevertheless enhance
visual representations. For this we propose a computational model of visual
representation learning during dyadic play. We introduce a synthetic dataset of
ego-centric images perceived by a toddler-agent that moves and rotates toy
objects in different parts of its home environment while hearing caregivers'
utterances, modeled as captions. We propose to model toddlers' learning as
simultaneously aligning representations for 1) close-in-time images and 2)
co-occurring images and utterances. We show that utterances with statistics
matching those of real caregivers give rise to representations supporting
improved category recognition. Our analysis reveals that a small
decrease/increase in object-relevant naming frequencies can drastically impact
the learned representations. This affects the attention on object names within
an utterance, which is required for efficient visuo-linguistic alignment.
Overall, our results support the hypothesis that caregivers' naming utterances
can improve toddlers' visual representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04119">A Multilevel Guidance-Exploration Network and Behavior-Scene Matching Method for Human Behavior Anomaly Detection. (arXiv:2312.04119v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guoqing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhiming Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianzhe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1">Yingxin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yifan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaozi Li</a></p>
<p>Human behavior anomaly detection aims to identify unusual human actions,
playing a crucial role in intelligent surveillance and other areas. The current
mainstream methods still adopt reconstruction or future frame prediction
techniques. However, reconstructing or predicting low-level pixel features
easily enables the network to achieve overly strong generalization ability,
allowing anomalies to be reconstructed or predicted as effectively as normal
data. Different from their methods, inspired by the Student-Teacher Network, we
propose a novel framework called the Multilevel Guidance-Exploration
Network(MGENet), which detects anomalies through the difference in high-level
representation between the Guidance and Exploration network. Specifically, we
first utilize the pre-trained Normalizing Flow that takes skeletal keypoints as
input to guide an RGB encoder, which takes unmasked RGB frames as input, to
explore motion latent features. Then, the RGB encoder guides the mask encoder,
which takes masked RGB frames as input, to explore the latent appearance
feature. Additionally, we design a Behavior-Scene Matching Module(BSMM) to
detect scene-related behavioral anomalies. Extensive experiments demonstrate
that our proposed method achieves state-of-the-art performance on ShanghaiTech
and UBnormal datasets, with AUC of 86.9 % and 73.5 %, respectively. The code
will be available on https://github.com/molu-ggg/GENet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04125">Forensic Iris Image Synthesis. (arXiv:2312.04125v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhuiyan_R/0/1/0/all/0/1">Rasel Ahmed Bhuiyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1">Adam Czajka</a></p>
<p>Post-mortem iris recognition is an emerging application of iris-based human
identification in a forensic setup, able to correctly identify deceased
subjects even three weeks post-mortem. This technique thus is considered as an
important component of future forensic toolkits. The current advancements in
this field are seriously slowed down by exceptionally difficult data
collection, which can happen in mortuary conditions, at crime scenes, or in
``body farm'' facilities. This paper makes a novel contribution to facilitate
progress in post-mortem iris recognition by offering a conditional
StyleGAN-based iris synthesis model, trained on the largest-available dataset
of post-mortem iris samples acquired from more than 350 subjects, generating --
through appropriate exploration of StyleGAN latent space -- multiple
within-class (same identity) and between-class (different new identities)
post-mortem iris images, compliant with ISO/IEC 29794-6, and with decomposition
deformations controlled by the requested PMI (post mortem interval). Besides an
obvious application to enhance the existing, very sparse, post-mortem iris
datasets to advance -- among others -- iris presentation attack endeavors, we
anticipate it may be useful to generate samples that would expose professional
forensic human examiners to never-seen-before deformations for various PMIs,
increasing their training effectiveness. The source codes and model weights are
made available with the paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04140">Polarimetric Light Transport Analysis for Specular Inter-reflection. (arXiv:2312.04140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maeda_R/0/1/0/all/0/1">Ryota Maeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Hiura_S/0/1/0/all/0/1">Shinsaku Hiura</a></p>
<p>Polarization is well known for its ability to decompose diffuse and specular
reflections. However, the existing decomposition methods only focus on direct
reflection and overlook multiple reflections, especially specular
inter-reflection. In this paper, we propose a novel decomposition method for
handling specular inter-reflection of metal objects by using a unique
polarimetric feature: the rotation direction of linear polarization. This
rotation direction serves as a discriminative factor between direct and
inter-reflection on specular surfaces. To decompose the reflectance components,
we actively rotate the linear polarization of incident light and analyze the
rotation direction of the reflected light. We evaluate our method using both
synthetic and real data, demonstrating its effectiveness in decomposing
specular inter-reflections of metal objects. Furthermore, we demonstrate that
our method can be combined with other decomposition methods for a detailed
analysis of light transport. As a practical application, we show its
effectiveness in improving the accuracy of 3D measurement against strong
specular inter-reflection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04143">Towards 4D Human Video Stylization. (arXiv:2312.04143v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tiantian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1">Xinxin Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_F/0/1/0/all/0/1">Fangzhou Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>We present a first step towards 4D (3D and time) human video stylization,
which addresses style transfer, novel view synthesis and human animation within
a unified framework. While numerous video stylization methods have been
developed, they are often restricted to rendering images in specific viewpoints
of the input video, lacking the capability to generalize to novel views and
novel poses in dynamic scenes. To overcome these limitations, we leverage
Neural Radiance Fields (NeRFs) to represent videos, conducting stylization in
the rendered feature space. Our innovative approach involves the simultaneous
representation of both the human subject and the surrounding scene using two
NeRFs. This dual representation facilitates the animation of human subjects
across various poses and novel viewpoints. Specifically, we introduce a novel
geometry-guided tri-plane representation, significantly enhancing feature
representation robustness compared to direct tri-plane optimization. Following
the video reconstruction, stylization is performed within the NeRFs' rendered
feature space. Extensive experiments demonstrate that the proposed method
strikes a superior balance between stylized textures and temporal coherence,
surpassing existing approaches. Furthermore, our framework uniquely extends its
capabilities to accommodate novel poses and viewpoints, making it a versatile
tool for creative human video stylization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04145">Diffusing Colors: Image Colorization with Text Guided Diffusion. (arXiv:2312.04145v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zabari_N/0/1/0/all/0/1">Nir Zabari</a>, <a href="http://arxiv.org/find/cs/1/au:+Azulay_A/0/1/0/all/0/1">Aharon Azulay</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorkor_A/0/1/0/all/0/1">Alexey Gorkor</a>, <a href="http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1">Tavi Halperin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1">Ohad Fried</a></p>
<p>The colorization of grayscale images is a complex and subjective task with
significant challenges. Despite recent progress in employing large-scale
datasets with deep neural networks, difficulties with controllability and
visual quality persist. To tackle these issues, we present a novel image
colorization framework that utilizes image diffusion techniques with granular
text prompts. This integration not only produces colorization outputs that are
semantically appropriate but also greatly improves the level of control users
have over the colorization process. Our method provides a balance between
automation and control, outperforming existing techniques in terms of visual
quality and semantic coherence. We leverage a pretrained generative Diffusion
Model, and show that we can finetune it for the colorization task without
losing its generative power or attention to text prompts. Moreover, we present
a novel CLIP-based ranking model that evaluates color vividness, enabling
automatic selection of the most suitable level of vividness based on the
specific scene semantics. Our approach holds potential particularly for color
enhancement and historical image colorization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04152">EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering within Transformer. (arXiv:2312.04152v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Dan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a></p>
<p>Video Motion Magnification (VMM) aims to break the resolution limit of human
visual perception capability and reveal the imperceptible minor motion that
contains valuable information in the macroscopic domain. However, challenges
arise in this task due to photon noise inevitably introduced by photographic
devices and spatial inconsistency in amplification, leading to flickering
artifacts in static fields and motion blur and distortion in dynamic fields in
the video. Existing methods focus on explicit motion modeling without
emphasizing prioritized denoising during the motion magnification process. This
paper proposes a novel dynamic filtering strategy to achieve static-dynamic
field adaptive denoising. Specifically, based on Eulerian theory, we separate
texture and shape to extract motion representation through inter-frame shape
differences, expecting to leverage these subdivided features to solve this task
finely. Then, we introduce a novel dynamic filter that eliminates noise cues
and preserves critical features in the motion magnification and amplification
generation phases. Overall, our unified framework, EulerMormer, is a pioneering
effort to first equip with Transformer in learning-based VMM. The core of the
dynamic filter lies in a global dynamic sparse cross-covariance attention
mechanism that explicitly removes noise while preserving vital information,
coupled with a multi-scale dual-path gating mechanism that selectively
regulates the dependence on different frequency features to reduce spatial
attenuation and complement motion boundaries. We demonstrate extensive
experiments that EulerMormer achieves more robust video motion magnification
from the Eulerian perspective, significantly outperforming state-of-the-art
methods. The source code is available at
https://github.com/VUT-HFUT/EulerMormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04160">Text as Image: Learning Transferable Adapter for Multi-Label Classification. (arXiv:2312.04160v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xuelin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiuxin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+liu_J/0/1/0/all/0/1">Jian liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1">Dongqi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Furong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weijia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jiawei Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qingpei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyi Zhang</a></p>
<p>Pre-trained vision-language models have notably accelerated progress of
open-world concept recognition. Their impressive zero-shot ability has recently
been transferred to multi-label image classification via prompt tuning,
enabling to discover novel labels in an open-vocabulary manner. However, this
paradigm suffers from non-trivial training costs, and becomes computationally
prohibitive for a large number of candidate labels. To address this issue, we
note that vision-language pre-training aligns images and texts in a unified
embedding space, making it potential for an adapter network to identify labels
in visual modality while be trained in text modality. To enhance such
cross-modal transfer ability, a simple yet effective method termed random
perturbation is proposed, which enables the adapter to search for potential
visual embeddings by perturbing text embeddings with noise during training,
resulting in better performance in visual modality. Furthermore, we introduce
an effective approach to employ large language models for multi-label
instruction-following text generation. In this way, a fully automated pipeline
for visual label recognition is developed without relying on any manual data.
Extensive experiments on public benchmarks show the superiority of our method
in various multi-label classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04168">Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient Semantic Segmentation. (arXiv:2312.04168v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jiawei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaolong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Meina Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Anbang Yao</a></p>
<p>In recent years, knowledge distillation methods based on contrastive learning
have achieved promising results on image classification and object detection
tasks. However, in this line of research, we note that less attention is paid
to semantic segmentation. Existing methods heavily rely on data augmentation
and memory buffer, which entail high computational resource demands when
applying them to handle semantic segmentation that requires to preserve
high-resolution feature maps for making dense pixel-wise predictions. In order
to address this problem, we present Augmentation-free Dense Contrastive
Knowledge Distillation (Af-DCD), a new contrastive distillation learning
paradigm to train compact and accurate deep neural networks for semantic
segmentation applications. Af-DCD leverages a masked feature mimicking
strategy, and formulates a novel contrastive learning loss via taking advantage
of tactful feature partitions across both channel and spatial dimensions,
allowing to effectively transfer dense and structured local knowledge learnt by
the teacher model to a target student model while maintaining training
efficiency. Extensive experiments on five mainstream benchmarks with various
teacher-student network pairs demonstrate the effectiveness of our approach.
For instance, the DeepLabV3-Res18|DeepLabV3-MBV2 model trained by Af-DCD
reaches 77.03%|76.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101
as the teacher, setting new performance records. Besides that, Af-DCD achieves
an absolute mIOU improvement of 3.26%|3.04%|2.75%|2.30%|1.42% compared with
individually trained counterpart on Cityscapes|Pascal
VOC|Camvid|ADE20K|COCO-Stuff-164K. Code is available at
https://github.com/OSVAI/Af-DCD
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04189">Joint-Individual Fusion Structure with Fusion Attention Module for Multi-Modal Skin Cancer Classification. (arXiv:2312.04189v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1">Peng Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xintong Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_Y/0/1/0/all/0/1">Yang Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaobin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaobin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Krammer_B/0/1/0/all/0/1">Bjoern H Menzee.Sebastian Krammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lasser_T/0/1/0/all/0/1">Tobias Lasser</a></p>
<p>Most convolutional neural network (CNN) based methods for skin cancer
classification obtain their results using only dermatological images. Although
good classification results have been shown, more accurate results can be
achieved by considering the patient's metadata, which is valuable clinical
information for dermatologists. Current methods only use the simple joint
fusion structure (FS) and fusion modules (FMs) for the multi-modal
classification methods, there still is room to increase the accuracy by
exploring more advanced FS and FM. Therefore, in this paper, we design a new
fusion method that combines dermatological images (dermoscopy images or
clinical images) and patient metadata for skin cancer classification from the
perspectives of FS and FM. First, we propose a joint-individual fusion (JIF)
structure that learns the shared features of multi-modality data and preserves
specific features simultaneously. Second, we introduce a fusion attention (FA)
module that enhances the most relevant image and metadata features based on
both the self and mutual attention mechanism to support the decision-making
pipeline. We compare the proposed JIF-MMFA method with other state-of-the-art
fusion methods on three different public datasets. The results show that our
JIF-MMFA method improves the classification results for all tested CNN
backbones and performs better than the other fusion methods on the three public
datasets, demonstrating our method's effectiveness and robustness
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04197">SAMBA: A Trainable Segmentation Web-App with Smart Labelling. (arXiv:2312.04197v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Docherty_R/0/1/0/all/0/1">Ronan Docherty</a>, <a href="http://arxiv.org/find/cs/1/au:+Squires_I/0/1/0/all/0/1">Isaac Squires</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvakeros_A/0/1/0/all/0/1">Antonis Vamvakeros</a>, <a href="http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1">Samuel J. Cooper</a></p>
<p>Segmentation is the assigning of a semantic class to every pixel in an image
and is a prerequisite for various statistical analysis tasks in materials
science, like phase quantification, physics simulations or morphological
characterization. The wide range of length scales, imaging techniques and
materials studied in materials science means any segmentation algorithm must
generalise to unseen data and support abstract, user-defined semantic classes.
Trainable segmentation is a popular interactive segmentation paradigm where a
classifier is trained to map from image features to user drawn labels. SAMBA is
a trainable segmentation tool that uses Meta's Segment Anything Model (SAM) for
fast, high-quality label suggestions and a random forest classifier for robust,
generalizable segmentations. It is accessible in the browser
(https://www.sambasegment.com/) without the need to download any external
dependencies. The segmentation backend is run in the cloud, so does not require
the user to have powerful hardware.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04210">Constraint Model for the Satellite Image Mosaic Selection Problem. (arXiv:2312.04210v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simon_M/0/1/0/all/0/1">Manuel Combarro Sim&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Talbot_P/0/1/0/all/0/1">Pierre Talbot</a>, <a href="http://arxiv.org/find/cs/1/au:+Danoy_G/0/1/0/all/0/1">Gr&#xe9;goire Danoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Musial_J/0/1/0/all/0/1">Jedrzej Musial</a>, <a href="http://arxiv.org/find/cs/1/au:+Alswaitti_M/0/1/0/all/0/1">Mohammed Alswaitti</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouvry_P/0/1/0/all/0/1">Pascal Bouvry</a></p>
<p>Satellite imagery solutions are widely used to study and monitor different
regions of the Earth. However, a single satellite image can cover only a
limited area. In cases where a larger area of interest is studied, several
images must be stitched together to create a single larger image, called a
mosaic, that can cover the area. Today, with the increasing number of satellite
images available for commercial use, selecting the images to build the mosaic
is challenging, especially when the user wants to optimize one or more
parameters, such as the total cost and the cloud coverage percentage in the
mosaic. More precisely, for this problem the input is an area of interest,
several satellite images intersecting the area, a list of requirements relative
to the image and the mosaic, such as cloud coverage percentage, image
resolution, and a list of objectives to optimize. We contribute to the
constraint and mixed integer lineal programming formulation of this new
problem, which we call the \textit{satellite image mosaic selection problem},
which is a multi-objective extension of the polygon cover problem. We propose a
dataset of realistic and challenging instances, where the images were captured
by the satellite constellations SPOT, Pl\'eiades and Pl\'eiades Neo. We
evaluate and compare the two proposed models and show their efficiency for
large instances, up to 200 images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04215">Guided Reconstruction with Conditioned Diffusion Models for Unsupervised Anomaly Detection in Brain MRIs. (arXiv:2312.04215v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Behrendt_F/0/1/0/all/0/1">Finn Behrendt</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhattacharya_D/0/1/0/all/0/1">Debayan Bhattacharya</a>, <a href="http://arxiv.org/find/eess/1/au:+Mieling_R/0/1/0/all/0/1">Robin Mieling</a>, <a href="http://arxiv.org/find/eess/1/au:+Maack_L/0/1/0/all/0/1">Lennart Maack</a>, <a href="http://arxiv.org/find/eess/1/au:+Kruger_J/0/1/0/all/0/1">Julia Kr&#xfc;ger</a>, <a href="http://arxiv.org/find/eess/1/au:+Opfer_R/0/1/0/all/0/1">Roland Opfer</a>, <a href="http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1">Alexander Schlaefer</a></p>
<p>Unsupervised anomaly detection in Brain MRIs aims to identify abnormalities
as outliers from a healthy training distribution. Reconstruction-based
approaches that use generative models to learn to reconstruct healthy brain
anatomy are commonly used for this task. Diffusion models are an emerging class
of deep generative models that show great potential regarding reconstruction
fidelity. However, they face challenges in preserving intensity characteristics
in the reconstructed images, limiting their performance in anomaly detection.
To address this challenge, we propose to condition the denoising mechanism of
diffusion models with additional information about the image to reconstruct
coming from a latent representation of the noise-free input image. This
conditioning enables high-fidelity reconstruction of healthy brain structures
while aligning local intensity characteristics of input-reconstruction pairs.
We evaluate our method's reconstruction quality, domain adaptation features and
finally segmentation performance on publicly available data sets with various
pathologies. Using our proposed conditioning mechanism we can reduce the
false-positive predictions and enable a more precise delineation of anomalies
which significantly enhances the anomaly detection performance compared to
established state-of-the-art approaches to unsupervised anomaly detection in
brain MRI. Furthermore, our approach shows promise in domain adaptation across
different MRI acquisitions and simulated contrasts, a crucial property of
general anomaly detection methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04225">TLCE: Transfer-Learning Based Classifier Ensembles for Few-Shot Class-Incremental Learning. (arXiv:2312.04225v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuangmei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tieru Wu</a></p>
<p>Few-shot class-incremental learning (FSCIL) struggles to incrementally
recognize novel classes from few examples without catastrophic forgetting of
old classes or overfitting to new classes. We propose TLCE, which ensembles
multiple pre-trained models to improve separation of novel and old classes.
TLCE minimizes interference between old and new classes by mapping old class
images to quasi-orthogonal prototypes using episodic training. It then
ensembles diverse pre-trained models to better adapt to novel classes despite
data imbalance. Extensive experiments on various datasets demonstrate that our
transfer learning ensemble approach outperforms state-of-the-art FSCIL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04231">Adventures of Trustworthy Vision-Language Models: A Survey. (arXiv:2312.04231v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1">Mayank Vatsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Anubhooti Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Richa Singh</a></p>
<p>Recently, transformers have become incredibly popular in computer vision and
vision-language tasks. This notable rise in their usage can be primarily
attributed to the capabilities offered by attention mechanisms and the
outstanding ability of transformers to adapt and apply themselves to a variety
of tasks and domains. Their versatility and state-of-the-art performance have
established them as indispensable tools for a wide array of applications.
However, in the constantly changing landscape of machine learning, the
assurance of the trustworthiness of transformers holds utmost importance. This
paper conducts a thorough examination of vision-language transformers,
employing three fundamental principles of responsible AI: Bias, Robustness, and
Interpretability. The primary objective of this paper is to delve into the
intricacies and complexities associated with the practical use of transformers,
with the overarching goal of advancing our comprehension of how to enhance
their reliability and accountability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04233">Fine-tune vision foundation model for crack segmentation in civil infrastructures. (arXiv:2312.04233v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_K/0/1/0/all/0/1">Kang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yutao Guo</a></p>
<p>Large-scale foundation models have become the mainstream method in the field
of deep learning, while in civil engineering, the scale of AI models is
strictly limited. In this work, vision foundation model is introduced for crack
segmentation. Two Parameter-efficient fine-tuning methods, adapter and low-rank
adaptation, are adopted to fine-tune the foundation model in the field of
semantic segmentation: Segment Anything Model (SAM). The fine-tuned model
CrackSAM is much larger than all the existing crack segmentation models, but
shows excellent performance. To test the zero-shot performance of the proposed
method, two unique datasets related to road and exterior wall cracks are
collected, annotated and open-sourced, in total 810 images. Comparative
experiments are conducted with twelve mature semantic segmentation models. On
datasets with artificial noise and previously unseen datasets, the performance
of CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibits
remarkable superiority, particularly in challenging conditions such as dim
lighting, shadows, road markings, construction joints, and other interference
factors. Such cross-scenario results demonstrate the outstanding zero-shot
capability of foundation models, and provide new ideas for the development of
vision models in civil engineering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04236">Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images. (arXiv:2312.04236v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiqun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhenyue Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1">Dylan Campbell</a></p>
<p>We introduce a pipeline to address anatomical inaccuracies in Stable
Diffusion generated hand images. The initial step involves constructing a
specialized dataset, focusing on hand anomalies, to train our models
effectively. A finetuned detection model is pivotal for precise identification
of these anomalies, ensuring targeted correction. Body pose estimation aids in
understanding hand orientation and positioning, crucial for accurate anomaly
correction. The integration of ControlNet and InstructPix2Pix facilitates
sophisticated inpainting and pixel-level transformation, respectively. This
dual approach allows for high-fidelity image adjustments. This comprehensive
approach ensures the generation of images with anatomically accurate hands,
closely resembling real-world appearances. Our experimental results demonstrate
the pipeline's efficacy in enhancing hand image realism in Stable Diffusion
outputs. We provide an online demo at https://fixhand.yiqun.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04248">TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes. (arXiv:2312.04248v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Bo-Wen Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1">Qibin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a></p>
<p>Recent progress in the text-driven 3D stylization of a single object has been
considerably promoted by CLIP-based methods. However, the stylization of
multi-object 3D scenes is still impeded in that the image-text pairs used for
pre-training CLIP mostly consist of an object. Meanwhile, the local details of
multiple objects may be susceptible to omission due to the existing supervision
manner primarily relying on coarse-grained contrast of image-text pairs. To
overcome these challenges, we present a novel framework, dubbed TeMO, to parse
multi-object 3D scenes and edit their styles under the contrast supervision at
multiple levels. We first propose a Decoupled Graph Attention (DGA) module to
distinguishably reinforce the features of 3D surface points. Particularly, a
cross-modal graph is constructed to align the object points accurately and noun
phrases decoupled from the 3D mesh and textual description. Then, we develop a
Cross-Grained Contrast (CGC) supervision system, where a fine-grained loss
between the words in the textual description and the randomly rendered images
are constructed to complement the coarse-grained loss. Extensive experiments
show that our method can synthesize high-quality stylized content and
outperform the existing methods over a wide range of multi-object 3D meshes.
Our code and results will be made publicly available
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04265">Stronger, Fewer, &amp; Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation. (arXiv:2312.04265v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhixiang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaoxiao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianle Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1">Pengyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Ben Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huaian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jinjin Zheng</a></p>
<p>In this paper, we first assess and harness various Vision Foundation Models
(VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS).
Driven by the motivation that Leveraging Stronger pre-trained models and Fewer
trainable parameters for Superior generalizability, we introduce a robust
fine-tuning approach, namely Rein, to parameter-efficiently harness VFMs for
DGSS. Built upon a set of trainable tokens, each linked to distinct instances,
Rein precisely refines and forwards the feature maps from each layer to the
next layer within the backbone. This process produces diverse refinements for
different categories within a single image. With fewer trainable parameters,
Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full
parameter fine-tuning. Extensive experiments across various settings
demonstrate that Rein significantly outperforms state-of-the-art methods.
Remarkably, with just an extra 1% of trainable parameters within the frozen
backbone, Rein achieves a mIoU of 68.1% on the Cityscapes, without accessing
any real urban-scene datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04266">Activity Grammars for Temporal Action Segmentation. (arXiv:2312.04266v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1">Dayoung Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joonseok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1">Deunsol Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1">Suha Kwak</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1">Minsu Cho</a></p>
<p>Sequence prediction on temporal data requires the ability to understand
compositional structures of multi-level semantics beyond individual and
contextual properties. The task of temporal action segmentation, which aims at
translating an untrimmed activity video into a sequence of action segments,
remains challenging for this reason. This paper addresses the problem by
introducing an effective activity grammar to guide neural predictions for
temporal action segmentation. We propose a novel grammar induction algorithm
that extracts a powerful context-free grammar from action sequence data. We
also develop an efficient generalized parser that transforms frame-level
probability distributions into a reliable sequence of actions according to the
induced grammar with recursive rules. Our approach can be combined with any
neural network for temporal action segmentation to enhance the sequence
prediction and discover its compositional structure. Experimental results
demonstrate that our method significantly improves temporal action segmentation
in terms of both performance and interpretability on two standard benchmarks,
Breakfast and 50 Salads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04293">GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion Understanding. (arXiv:2312.04293v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zheng Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Licai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haiyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zhuofan Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jianhua Tao</a></p>
<p>Recently, GPT-4 with Vision (GPT-4V) has shown remarkable performance across
various multimodal tasks. However, its efficacy in emotion recognition remains
a question. This paper quantitatively evaluates GPT-4V's capabilities in
multimodal emotion understanding, encompassing tasks such as facial emotion
recognition, visual sentiment analysis, micro-expression recognition, dynamic
facial emotion recognition, and multimodal emotion recognition. Our experiments
show that GPT-4V exhibits impressive multimodal and temporal understanding
capabilities, even surpassing supervised systems in some tasks. Despite these
achievements, GPT-4V is currently tailored for general domains. It performs
poorly in micro-expression recognition that requires specialized expertise. The
main purpose of this paper is to present quantitative results of GPT-4V on
emotion understanding and establish a zero-shot benchmark for future research.
Code and evaluation results are available at:
https://github.com/zeroQiaoba/gpt4v-emotion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04296">Cross-codex Learning for Reliable Scribe Identification in Medieval Manuscripts. (arXiv:2312.04296v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weissmann_J/0/1/0/all/0/1">Julius Wei&#xdf;mann</a>, <a href="http://arxiv.org/find/cs/1/au:+Seidl_M/0/1/0/all/0/1">Markus Seidl</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1">Anya Dietrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Haltrich_M/0/1/0/all/0/1">Martin Haltrich</a></p>
<p>Historic scribe identification is a substantial task for obtaining
information about the past. Uniform script styles, such as the Carolingian
minuscule, make it a difficult task for classification to focus on meaningful
features. Therefore, we demonstrate in this paper the importance of cross-codex
training data for CNN based text-independent off-line scribe identification, to
overcome codex dependent overfitting. We report three main findings: First, we
found that preprocessing with masked grayscale images instead of RGB images
clearly increased the F1-score of the classification results. Second, we
trained different neural networks on our complex data, validating time and
accuracy differences in order to define the most reliable network architecture.
With AlexNet, the network with the best trade-off between F1-score and time, we
achieved for individual classes F1-scores of up to 0,96 on line level and up to
1.0 on page level in classification. Third, we could replicate the finding that
the CNN output can be further improved by implementing a reject option, giving
more stable results. We present the results on our large scale open source
dataset -- the Codex Claustroneoburgensis database (CCl-DB) -- containing a
significant number of writings from different scribes in several codices. We
demonstrate for the first time on a dataset with such a variety of codices that
paleographic decisions can be reproduced automatically and precisely with CNNs.
This gives manifold new and fast possibilities for paleographers to gain
insights into unlabeled material, but also to develop further hypotheses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04302">Prompt Highlighter: Interactive Control for Multi-Modal LLMs. (arXiv:2312.04302v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuechen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengju Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bohao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jiaya Jia</a></p>
<p>This study targets a critical aspect of multi-modal LLMs' (LLMs&amp;VLMs)
inference: explicit controllable text generation. Multi-modal LLMs empower
multi-modality understanding with the capability of semantic generation yet
bring less explainability and heavier reliance on prompt contents due to their
autoregressive generative nature. While manipulating prompt formats could
improve outputs, designing specific and precise prompts per task can be
challenging and ineffective. To tackle this issue, we introduce a novel
inference method, Prompt Highlighter, which enables users to highlight specific
prompt spans to interactively control the focus during generation. Motivated by
the classifier-free diffusion guidance, we form regular and unconditional
context pairs based on highlighted tokens, demonstrating that the
autoregressive generation in models can be guided in a classifier-free way.
Notably, we find that, during inference, guiding the models with highlighted
tokens through the attention weights leads to more desired outputs. Our
approach is compatible with current LLMs and VLMs, achieving impressive
customized generation results without training. Experiments confirm its
effectiveness in focusing on input contexts and generating reliable content.
Without tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and
1552.5 in MME-perception. The code is available at:
https://github.com/dvlab-research/Prompt-Highlighter/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04314">GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives. (arXiv:2312.04314v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zuyao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoxiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changwen Chen</a></p>
<p>Learning scene graphs from natural language descriptions has proven to be a
cheap and promising scheme for Scene Graph Generation (SGG). However, such
unstructured caption data and its processing are troubling the learning an
acurrate and complete scene graph. This dilema can be summarized as three
points. First, traditional language parsers often fail to extract meaningful
relationship triplets from caption data. Second, grounding unlocalized objects
in parsed triplets will meet ambiguity in visual-language alignment. Last,
caption data typically are sparse and exhibit bias to partial observations of
image content. These three issues make it hard for the model to generate
comprehensive and accurate scene graphs. To fill this gap, we propose a simple
yet effective framework, GPT4SGG, to synthesize scene graphs from holistic and
region-specific narratives. The framework discards traditional language parser,
and localize objects before obtaining relationship triplets. To obtain
relationship triplets, holistic and dense region-specific narratives are
generated from the image. With such textual representation of image data and a
task-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scene
graph as "pseudo labels". Experimental results showcase GPT4SGG significantly
improves the performance of SGG models trained on image-caption data. We
believe this pioneering work can motivate further research into mining the
visual reasoning capabilities of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04316">Towards Knowledge-driven Autonomous Driving. (arXiv:2312.04316v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yeqi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Pinlong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1">Licheng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Daocheng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuemeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xinyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianfei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xing Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1">Min Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Botian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>This paper explores the emerging knowledge-driven autonomous driving
technologies. Our investigation highlights the limitations of current
autonomous driving systems, in particular their sensitivity to data bias,
difficulty in handling long-tail scenarios, and lack of interpretability.
Conversely, knowledge-driven methods with the abilities of cognition,
generalization and life-long learning emerge as a promising way to overcome
these challenges. This paper delves into the essence of knowledge-driven
autonomous driving and examines its core components: dataset \&amp; benchmark,
environment, and driver agent. By leveraging large language models, world
models, neural rendering, and other advanced artificial intelligence
techniques, these components collectively contribute to a more holistic,
adaptive, and intelligent autonomous driving system. The paper systematically
organizes and reviews previous research efforts in this area, and provides
insights and guidance for future research and practical applications of
autonomous driving. We will continually share the latest updates on
cutting-edge developments in knowledge-driven autonomous driving along with the
relevant valuable open-source resources at:
\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04326">iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image Diffusion Model for Interior Design. (arXiv:2312.04326v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1">Ruyi Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaojun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Junyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuanhe Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dixiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Ziwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Renliang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pingjian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yan Song</a></p>
<p>With the open-sourcing of text-to-image models (T2I) such as stable diffusion
(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned
in specific domains based on the open-source SD model, such as in anime,
character portraits, etc. However, there are few specialized models in certain
domains, such as interior design, which is attributed to the complex textual
descriptions and detailed visual elements inherent in design, alongside the
necessity for adaptable resolution. Therefore, text-to-image models for
interior design are required to have outstanding prompt-following capabilities,
as well as iterative collaboration with design professionals to achieve the
desired outcome. In this paper, we collect and optimize text-image data in the
design field and continue training in both English and Chinese on the basis of
the open-source CLIP model. We also proposed a fine-tuning strategy with
curriculum learning and reinforcement learning from CLIP feedback to enhance
the prompt-following capabilities of our approach so as to improve the quality
of image generation. The experimental results on the collected dataset
demonstrate the effectiveness of the proposed approach, which achieves
impressive results and outperforms strong baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04328">A Multi-scale Information Integration Framework for Infrared and Visible Image Fusion. (arXiv:2312.04328v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1">Hanxiao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>Infrared and visible image fusion aims at generating a fused image containing
the intensity and detail information of source images, and the key issue is
effectively measuring and integrating the complementary information of
multi-modality images from the same scene. Existing methods mostly adopt a
simple weight in the loss function to decide the information retention of each
modality rather than adaptively measuring complementary information for
different image pairs. In this study, we propose a multi-scale dual attention
(MDA) framework for infrared and visible image fusion, which is designed to
measure and integrate complementary information in both structure and loss
function at the image and patch level. In our method, the residual downsample
block decomposes source images into three scales first. Then, dual attention
fusion block integrates complementary information and generates a spatial and
channel attention map at each scale for feature fusion. Finally, the output
image is reconstructed by the residual reconstruction block. Loss function
consists of image-level, feature-level and patch-level three parts, of which
the calculation of the image-level and patch-level two parts are based on the
weights generated by the complementary information measurement. Indeed, to
constrain the pixel intensity distribution between the output and infrared
image, a style loss is added. Our fusion results perform robust and informative
across different scenarios. Qualitative and quantitative results on two
datasets illustrate that our method is able to preserve both thermal radiation
and detailed information from two modalities and achieve comparable results
compared with the other state-of-the-art methods. Ablation experiments show the
effectiveness of our information integration architecture and adaptively
measure complementary information retention in the loss function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04334">Towards a Perceptual Evaluation Framework for Lighting Estimation. (arXiv:2312.04334v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giroux_J/0/1/0/all/0/1">Justine Giroux</a>, <a href="http://arxiv.org/find/cs/1/au:+Dastjerdi_M/0/1/0/all/0/1">Mohammad Reza Karimi Dastjerdi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hold_Geoffroy_Y/0/1/0/all/0/1">Yannick Hold-Geoffroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_Corral_J/0/1/0/all/0/1">Javier Vazquez-Corral</a>, <a href="http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1">Jean-Fran&#xe7;ois Lalonde</a></p>
<p>Progress in lighting estimation is tracked by computing existing image
quality assessment (IQA) metrics on images from standard datasets. While this
may appear to be a reasonable approach, we demonstrate that doing so does not
correlate to human preference when the estimated lighting is used to relight a
virtual scene into a real photograph. To study this, we design a controlled
psychophysical experiment where human observers must choose their preference
amongst rendered scenes lit using a set of lighting estimation algorithms
selected from the recent literature, and use it to analyse how these algorithms
perform according to human perception. Then, we demonstrate that none of the
most popular IQA metrics from the literature, taken individually, correctly
represent human perception. Finally, we show that by learning a combination of
existing IQA metrics, we can more accurately represent human preference. This
provides a new perceptual framework to help evaluate future lighting estimation
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04337">Multi-View Unsupervised Image Generation with Cross Attention Guidance. (arXiv:2312.04337v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cerkezi_L/0/1/0/all/0/1">Llukman Cerkezi</a>, <a href="http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1">Aram Davtyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sameni_S/0/1/0/all/0/1">Sepehr Sameni</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1">Paolo Favaro</a></p>
<p>The growing interest in novel view synthesis, driven by Neural Radiance Field
(NeRF) models, is hindered by scalability issues due to their reliance on
precisely annotated multi-view images. Recent models address this by
fine-tuning large text2image diffusion models on synthetic multi-view data.
Despite robust zero-shot generalization, they may need post-processing and can
face quality issues due to the synthetic-real domain gap. This paper introduces
a novel pipeline for unsupervised training of a pose-conditioned diffusion
model on single-category datasets. With the help of pretrained self-supervised
Vision Transformers (DINOv2), we identify object poses by clustering the
dataset through comparing visibility and locations of specific object parts.
The pose-conditioned diffusion model, trained on pose labels, and equipped with
cross-frame attention at inference time ensures cross-view consistency, that is
further aided by our novel hard-attention guidance. Our model, MIRAGE,
surpasses prior work in novel view synthesis on real images. Furthermore,
MIRAGE is robust to diverse textures and geometries, as demonstrated with our
experiments on synthetic images generated with pretrained Stable Diffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04344">Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pengcheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziyan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhongying Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yanzhou Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jin Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junjun He</a></p>
<p>OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued
considerable interest for its potential in medical applications. Despite its
promise, recent studies and internal reviews highlight its underperformance in
specialized medical tasks. This paper explores the boundary of GPT-4V's
capabilities in medicine, particularly in processing complex imaging data from
endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we
assessed its foundational competencies, identifying substantial areas for
enhancement. Our research emphasizes prompt engineering, an often-underutilized
strategy for improving AI responsiveness. Through iterative testing, we refined
the model's prompts, significantly improving its interpretative accuracy and
relevance in medical imaging. From our comprehensive evaluations, we distilled
10 effective prompt engineering techniques, each fortifying GPT-4V's medical
acumen. These methodical enhancements facilitate more reliable, precise, and
clinically valuable insights from GPT-4V, advancing its operability in critical
healthcare environments. Our findings are pivotal for those employing AI in
medicine, providing clear, actionable guidance on harnessing GPT-4V's full
diagnostic potential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04364">DemoCaricature: Democratising Caricature Generation with a Rough Sketch. (arXiv:2312.04364v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dar-Yen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Koley_S/0/1/0/all/0/1">Subhadeep Koley</a>, <a href="http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1">Aneeshan Sain</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1">Pinaki Nath Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1">Ayan Kumar Bhunia</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yi-Zhe Song</a></p>
<p>In this paper, we democratise caricature generation, empowering individuals
to effortlessly craft personalised caricatures with just a photo and a
conceptual sketch. Our objective is to strike a delicate balance between
abstraction and identity, while preserving the creativity and subjectivity
inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing
alongside single-image personalisation, selectively applying nuanced edits to
cross-attention layers for a seamless merge of identity and style.
Additionally, we propose Random Mask Reconstruction to enhance robustness,
directing the model to focus on distinctive identity and style features.
Crucially, our aim is not to replace artists but to eliminate accessibility
barriers, allowing enthusiasts to engage in the artistry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04369">SingingHead: A Large-scale 4D Dataset for Singing Head Animation. (arXiv:2312.04369v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Sijing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weitian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jun Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yucheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yichao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1">Guangtao Zhai</a></p>
<p>Singing, as a common facial movement second only to talking, can be regarded
as a universal language across ethnicities and cultures, plays an important
role in emotional communication, art, and entertainment. However, it is often
overlooked in the field of audio-driven facial animation due to the lack of
singing head datasets and the domain gap between singing and talking in rhythm
and amplitude. To this end, we collect a high-quality large-scale singing head
dataset, SingingHead, which consists of more than 27 hours of synchronized
singing video, 3D facial motion, singing audio, and background music from 76
individuals and 8 types of music. Along with the SingingHead dataset, we argue
that 3D and 2D facial animation tasks can be solved together, and propose a
unified singing facial animation framework named UniSinger to achieve both
singing audio-driven 3D singing head animation and 2D singing portrait video
synthesis. Extensive comparative experiments with both SOTA 3D facial animation
and 2D portrait animation methods demonstrate the necessity of singing-specific
datasets in singing head animation tasks and the promising performance of our
unified facial animation framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04385">AniRes2D: Anisotropic Residual-enhanced Diffusion for 2D MR Super-Resolution. (arXiv:2312.04385v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1">Zejun Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Remedios_S/0/1/0/all/0/1">Samuel W. Remedios</a>, <a href="http://arxiv.org/find/eess/1/au:+Dewey_B/0/1/0/all/0/1">Blake E. Dewey</a>, <a href="http://arxiv.org/find/eess/1/au:+Carass_A/0/1/0/all/0/1">Aaron Carass</a>, <a href="http://arxiv.org/find/eess/1/au:+Prince_J/0/1/0/all/0/1">Jerry L. Prince</a></p>
<p>Anisotropic low-resolution (LR) magnetic resonance (MR) images are fast to
obtain but hinder automated processing. We propose to use denoising diffusion
probabilistic models (DDPMs) to super-resolve these 2D-acquired LR MR slices.
This paper introduces AniRes2D, a novel approach combining DDPM with a residual
prediction for 2D super-resolution (SR). Results demonstrate that AniRes2D
outperforms several other DDPM-based models in quantitative metrics, visual
quality, and out-of-domain evaluation. We use a trained AniRes2D to
super-resolve 3D volumes slice by slice, where comparative quantitative results
and reduced skull aliasing are achieved compared to a recent state-of-the-art
self-supervised 3D super-resolution method. Furthermore, we explored the use of
noise conditioning augmentation (NCA) as an alternative augmentation technique
for DDPM-based SR models, but it was found to reduce performance. Our findings
contribute valuable insights to the application of DDPMs for SR of anisotropic
MR images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04393">PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction. (arXiv:2312.04393v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yinhuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jing Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Ailing Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhengyi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>Humans interact with objects all the time. Enabling a humanoid to learn
human-object interaction (HOI) is a key step for future smart animation and
intelligent robotics systems. However, recent progress in physics-based HOI
requires carefully designed task-specific rewards, making the system unscalable
and labor-intensive. This work focuses on dynamic HOI imitation: teaching
humanoid dynamic interaction skills through imitating kinematic HOI
demonstrations. It is quite challenging because of the complexity of the
interaction between body parts and objects and the lack of dynamic HOI data. To
handle the above issues, we present PhysHOI, the first physics-based whole-body
HOI imitation approach without task-specific reward designs. Except for the
kinematic HOI representations of humans and objects, we introduce the contact
graph to model the contact relations between body parts and objects explicitly.
A contact graph reward is also designed, which proved to be critical for
precise HOI imitation. Based on the key designs, PhysHOI can imitate diverse
HOI tasks simply yet effectively without prior knowledge. To make up for the
lack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset
that contains eight whole-body basketball skills. We validate PhysHOI on
diverse HOI tasks, including whole-body grasping and basketball skills.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04398">Intelligent Anomaly Detection for Lane Rendering Using Transformer with Self-Supervised Pre-Training and Customized Fine-Tuning. (arXiv:2312.04398v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yongqi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xingmin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1">Wei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Arem_B/0/1/0/all/0/1">Bart van Arem</a>, <a href="http://arxiv.org/find/cs/1/au:+Farah_H/0/1/0/all/0/1">Haneen Farah</a></p>
<p>The burgeoning navigation services using digital maps provide great
convenience to drivers. Nevertheless, the presence of anomalies in lane
rendering map images occasionally introduces potential hazards, as such
anomalies can be misleading to human drivers and consequently contribute to
unsafe driving conditions. In response to this concern and to accurately and
effectively detect the anomalies, this paper transforms lane rendering image
anomaly detection into a classification problem and proposes a four-phase
pipeline consisting of data pre-processing, self-supervised pre-training with
the masked image modeling (MiM) method, customized fine-tuning using
cross-entropy based loss with label smoothing, and post-processing to tackle it
leveraging state-of-the-art deep learning techniques, especially those
involving Transformer models. Various experiments verify the effectiveness of
the proposed pipeline. Results indicate that the proposed pipeline exhibits
superior performance in lane rendering image anomaly detection, and notably,
the self-supervised pre-training with MiM can greatly enhance the detection
accuracy while significantly reducing the total training time. For instance,
employing the Swin Transformer with Uniform Masking as self-supervised
pretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an
improved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin
Transformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an
AUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the
original 280. In conclusion, the proposed pipeline, with its incorporation of
self-supervised pre-training using MiM and other advanced deep learning
techniques, emerges as a robust solution for enhancing the accuracy and
efficiency of lane rendering image anomaly detection in digital navigation
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04403">OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization. (arXiv:2312.04403v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">Dongchen Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaojun Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yang Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xiaochun Cao</a></p>
<p>Vision-language pre-training (VLP) models demonstrate impressive abilities in
processing both images and text. However, they are vulnerable to multi-modal
adversarial examples (AEs). Investigating the generation of
high-transferability adversarial examples is crucial for uncovering VLP models'
vulnerabilities in practical scenarios. Recent works have indicated that
leveraging data augmentation and image-text modal interactions can enhance the
transferability of adversarial examples for VLP models significantly. However,
they do not consider the optimal alignment problem between dataaugmented
image-text pairs. This oversight leads to adversarial examples that are overly
tailored to the source model, thus limiting improvements in transferability. In
our research, we first explore the interplay between image sets produced
through data augmentation and their corresponding text sets. We find that
augmented image samples can align optimally with certain texts while exhibiting
less relevance to others. Motivated by this, we propose an Optimal
Transport-based Adversarial Attack, dubbed OT-Attack. The proposed method
formulates the features of image and text sets as two distinct distributions
and employs optimal transport theory to determine the most efficient mapping
between them. This optimal mapping informs our generation of adversarial
examples to effectively counteract the overfitting issues. Extensive
experiments across various network architectures and datasets in image-text
matching tasks reveal that our OT-Attack outperforms existing state-of-the-art
methods in terms of adversarial transferability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04410">Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models. (arXiv:2312.04410v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiayi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xingqian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yifan Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1">Zanlin Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaofei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1">Manushree Vasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shiji Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Humphrey Shi</a></p>
<p>Recently, diffusion models have made remarkable progress in text-to-image
(T2I) generation, synthesizing images with high fidelity and diverse contents.
Despite this advancement, latent space smoothness within diffusion models
remains largely unexplored. Smooth latent spaces ensure that a perturbation on
an input latent corresponds to a steady change in the output image. This
property proves beneficial in downstream tasks, including image interpolation,
inversion, and editing. In this work, we expose the non-smoothness of diffusion
latent spaces by observing noticeable visual fluctuations resulting from minor
latent variations. To tackle this issue, we propose Smooth Diffusion, a new
category of diffusion models that can be simultaneously high-performing and
smooth. Specifically, we introduce Step-wise Variation Regularization to
enforce the proportion between the variations of an arbitrary input latent and
that of the output image is a constant at any diffusion training step. In
addition, we devise an interpolation standard deviation (ISTD) metric to
effectively assess the latent space smoothness of a diffusion model. Extensive
quantitative and qualitative experiments demonstrate that Smooth Diffusion
stands out as a more desirable solution not only in T2I generation but also
across various downstream tasks. Smooth Diffusion is implemented as a
plug-and-play Smooth-LoRA to work with various community models. Code is
available at https://github.com/SHI-Labs/Smooth-Diffusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04424">Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views. (arXiv:2312.04424v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yabo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jiemin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuyang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1">Taoran Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaopeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lingxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinggang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Wenrui Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hongkai Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a></p>
<p>Synthesizing multi-view 3D from one single image is a significant and
challenging task. For this goal, Zero-1-to-3 methods aim to extend a 2D latent
diffusion model to the 3D scope. These approaches generate the target-view
image with a single-view source image and the camera pose as condition
information. However, the one-to-one manner adopted in Zero-1-to-3 incurs
challenges for building geometric and visual consistency across views,
especially for complex objects. We propose a cascade generation framework
constructed with two Zero-1-to-3 models, named Cascade-Zero123, to tackle this
issue, which progressively extracts 3D information from the source image.
Specifically, a self-prompting mechanism is designed to generate several nearby
views at first. These views are then fed into the second-stage model along with
the source image as generation conditions. With self-prompted multiple views as
the supplementary information, our Cascade-Zero123 generates more highly
consistent novel-view images than Zero-1-to-3. The promotion is significant for
various complex and challenging scenes, involving insects, humans, transparent
objects, and stacked multiple objects etc. The project page is at
https://cascadezero123.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04429">Approximate Caching for Efficiently Serving Diffusion Models. (arXiv:2312.04429v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Shubham Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Subrata Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Sarthak Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1">Srikrishna Karanam</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1">Koyel Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1">Shiv Saini</a></p>
<p>Text-to-image generation using diffusion models has seen explosive popularity
owing to their ability in producing high quality images adhering to text
prompts. However, production-grade diffusion model serving is a resource
intensive task that not only require high-end GPUs which are expensive but also
incurs considerable latency. In this paper, we introduce a technique called
approximate-caching that can reduce such iterative denoising steps for an image
generation based on a prompt by reusing intermediate noise states created
during a prior image generation for similar prompts. Based on this idea, we
present an end to end text-to-image system, Nirvana, that uses the
approximate-caching with a novel cache management-policy Least Computationally
Beneficial and Frequently Used (LCBFU) to provide % GPU compute savings, 19.8%
end-to-end latency reduction and 19% dollar savings, on average, on two real
production workloads. We further present an extensive characterization of real
production text-to-image prompts from the perspective of caching, popularity
and reuse of intermediate states in a large production environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04433">DreamVideo: Composing Your Dream Videos with Customized Subject and Motion. (arXiv:2312.04433v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yujie Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1">Zhiwu Qing</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Hangjie Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1">Hongming Shan</a></p>
<p>Customized generation using diffusion models has made impressive progress in
image generation, but remains unsatisfactory in the challenging video
generation task, as it requires the controllability of both subjects and
motions. To that end, we present DreamVideo, a novel approach to generating
personalized videos from a few static images of the desired subject and a few
videos of target motion. DreamVideo decouples this task into two stages,
subject learning and motion learning, by leveraging a pre-trained video
diffusion model. The subject learning aims to accurately capture the fine
appearance of the subject from provided images, which is achieved by combining
textual inversion and fine-tuning of our carefully designed identity adapter.
In motion learning, we architect a motion adapter and fine-tune it on the given
videos to effectively model the target motion pattern. Combining these two
lightweight and efficient adapters allows for flexible customization of any
subject with any motion. Extensive experimental results demonstrate the
superior performance of our DreamVideo over the state-of-the-art methods for
customized video generation. Our project page is at
https://dreamvideo-t2v.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.05642">Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v8 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Reinke_A/0/1/0/all/0/1">Annika Reinke</a>, <a href="http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1">Minu D. Tizabi</a>, <a href="http://arxiv.org/find/eess/1/au:+Sudre_C/0/1/0/all/0/1">Carole H. Sudre</a>, <a href="http://arxiv.org/find/eess/1/au:+Eisenmann_M/0/1/0/all/0/1">Matthias Eisenmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Radsch_T/0/1/0/all/0/1">Tim R&#xe4;dsch</a>, <a href="http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1">Michael Baumgartner</a>, <a href="http://arxiv.org/find/eess/1/au:+Acion_L/0/1/0/all/0/1">Laura Acion</a>, <a href="http://arxiv.org/find/eess/1/au:+Antonelli_M/0/1/0/all/0/1">Michela Antonelli</a>, <a href="http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1">Tal Arbel</a>, <a href="http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1">Spyridon Bakas</a>, <a href="http://arxiv.org/find/eess/1/au:+Bankhead_P/0/1/0/all/0/1">Peter Bankhead</a>, <a href="http://arxiv.org/find/eess/1/au:+Benis_A/0/1/0/all/0/1">Arriel Benis</a>, <a href="http://arxiv.org/find/eess/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew Blaschko</a>, <a href="http://arxiv.org/find/eess/1/au:+Buettner_F/0/1/0/all/0/1">Florian Buettner</a>, <a href="http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1">M. Jorge Cardoso</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jianxu Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1">Veronika Cheplygina</a>, <a href="http://arxiv.org/find/eess/1/au:+Christodoulou_E/0/1/0/all/0/1">Evangelia Christodoulou</a>, <a href="http://arxiv.org/find/eess/1/au:+Cimini_B/0/1/0/all/0/1">Beth Cimini</a>, <a href="http://arxiv.org/find/eess/1/au:+Collins_G/0/1/0/all/0/1">Gary S. Collins</a>, <a href="http://arxiv.org/find/eess/1/au:+Engelhardt_S/0/1/0/all/0/1">Sandy Engelhardt</a>, <a href="http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1">Keyvan Farahani</a>, <a href="http://arxiv.org/find/eess/1/au:+Ferrer_L/0/1/0/all/0/1">Luciana Ferrer</a>, <a href="http://arxiv.org/find/eess/1/au:+Galdran_A/0/1/0/all/0/1">Adrian Galdran</a>, <a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1">Bram van Ginneken</a>, <a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1">Ben Glocker</a>, <a href="http://arxiv.org/find/eess/1/au:+Godau_P/0/1/0/all/0/1">Patrick Godau</a>, <a href="http://arxiv.org/find/eess/1/au:+Haase_R/0/1/0/all/0/1">Robert Haase</a>, <a href="http://arxiv.org/find/eess/1/au:+Hamprecht_F/0/1/0/all/0/1">Fred Hamprecht</a>, <a href="http://arxiv.org/find/eess/1/au:+Hashimoto_D/0/1/0/all/0/1">Daniel A. Hashimoto</a>, <a href="http://arxiv.org/find/eess/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1">Doreen Heckmann-N&#xf6;tzel</a>, <a href="http://arxiv.org/find/eess/1/au:+Hirsch_P/0/1/0/all/0/1">Peter Hirsch</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoffman_M/0/1/0/all/0/1">Michael M. Hoffman</a>, <a href="http://arxiv.org/find/eess/1/au:+Huisman_M/0/1/0/all/0/1">Merel Huisman</a>, <a href="http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1">Fabian Isensee</a>, <a href="http://arxiv.org/find/eess/1/au:+Jannin_P/0/1/0/all/0/1">Pierre Jannin</a>, <a href="http://arxiv.org/find/eess/1/au:+Kahn_C/0/1/0/all/0/1">Charles E. Kahn</a>, <a href="http://arxiv.org/find/eess/1/au:+Kainmueller_D/0/1/0/all/0/1">Dagmar Kainmueller</a>, <a href="http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1">Bernhard Kainz</a>, <a href="http://arxiv.org/find/eess/1/au:+Karargyris_A/0/1/0/all/0/1">Alexandros Karargyris</a>, <a href="http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/eess/1/au:+Kavur_A/0/1/0/all/0/1">A. Emre Kavur</a>, <a href="http://arxiv.org/find/eess/1/au:+Kenngott_H/0/1/0/all/0/1">Hannes Kenngott</a>, <a href="http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1">Jens Kleesiek</a>, <a href="http://arxiv.org/find/eess/1/au:+Kleppe_A/0/1/0/all/0/1">Andreas Kleppe</a>, <a href="http://arxiv.org/find/eess/1/au:+Kohler_S/0/1/0/all/0/1">Sven Kohler</a>, <a href="http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1">Florian Kofler</a>, et al. (46 additional authors not shown)</p>
<p>While the importance of automatic image analysis is continuously increasing,
recent meta-research revealed major flaws with respect to algorithm validation.
Performance metrics are particularly key for meaningful, objective, and
transparent performance assessment and validation of the used automatic
algorithms, but relatively little attention has been given to the practical
pitfalls when using specific metrics for a given image analysis task. These are
typically related to (1) the disregard of inherent metric properties, such as
the behaviour in the presence of class imbalance or small target structures,
(2) the disregard of inherent data set properties, such as the non-independence
of the test cases, and (3) the disregard of the actual biomedical domain
interest that the metrics should reflect. This living dynamically document has
the purpose to illustrate important limitations of performance metrics commonly
applied in the field of image analysis. In this context, it focuses on
biomedical image analysis problems that can be phrased as image-level
classification, semantic segmentation, instance segmentation, or object
detection task. The current version is based on a Delphi process on metrics
conducted by an international consortium of image analysis experts from more
than 60 institutions worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.12928">If your data distribution shifts, use self-learning. (arXiv:2104.12928v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rusak_E/0/1/0/all/0/1">Evgenia Rusak</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1">Steffen Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Pachitariu_G/0/1/0/all/0/1">George Pachitariu</a>, <a href="http://arxiv.org/find/cs/1/au:+Eck_L/0/1/0/all/0/1">Luisa Eck</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1">Peter Gehler</a>, <a href="http://arxiv.org/find/cs/1/au:+Bringmann_O/0/1/0/all/0/1">Oliver Bringmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1">Wieland Brendel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a></p>
<p>We demonstrate that self-learning techniques like entropy minimization and
pseudo-labeling are simple and effective at improving performance of a deployed
computer vision model under systematic domain shifts. We conduct a wide range
of large-scale experiments and show consistent improvements irrespective of the
model architecture, the pre-training technique or the type of distribution
shift. At the same time, self-learning is simple to use in practice because it
does not require knowledge or access to the original training data or scheme,
is robust to hyperparameter choices, is straight-forward to implement and
requires only a few adaptation epochs. This makes self-learning techniques
highly attractive for any practitioner who applies machine learning algorithms
in the real world. We present state-of-the-art adaptation results on CIFAR10-C
(8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A
(14.8% error), theoretically study the dynamics of self-supervised adaptation
methods and propose a new classification dataset (ImageNet-D) which is
challenging even with adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.00428">Adv-4-Adv: Thwarting Changing Adversarial Perturbations via Adversarial Domain Adaptation. (arXiv:2112.00428v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianyue Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Shuya Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1">Chao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jun Luo</a></p>
<p>Whereas adversarial training can be useful against specific adversarial
perturbations, they have also proven ineffective in generalizing towards
attacks deviating from those used for training. However, we observe that this
ineffectiveness is intrinsically connected to domain adaptability, another
crucial issue in deep learning for which adversarial domain adaptation appears
to be a promising solution. Consequently, we proposed Adv-4-Adv as a novel
adversarial training method that aims to retain robustness against unseen
adversarial perturbations. Essentially, Adv-4-Adv treats attacks incurring
different perturbations as distinct domains, and by leveraging the power of
adversarial domain adaptation, it aims to remove the domain/attack-specific
features. This forces a trained model to learn a robust domain-invariant
representation, which in turn enhances its generalization ability. Extensive
evaluations on Fashion-MNIST, SVHN, CIFAR-10, and CIFAR-100 demonstrate that a
model trained by Adv-4-Adv based on samples crafted by simple attacks (e.g.,
FGSM) can be generalized to more advanced attacks (e.g., PGD), and the
performance exceeds state-of-the-art proposals on these datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.07921">Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks. (arXiv:2112.07921v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jaehui Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jun-Ho Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jong-Seok Lee</a></p>
<p>Recently, video-based action recognition methods using convolutional neural
networks (CNNs) achieve remarkable recognition performance. However, there is
still lack of understanding about the generalization mechanism of action
recognition models. In this paper, we suggest that action recognition models
rely on the motion information less than expected, and thus they are robust to
randomization of frame orders. Furthermore, we find that motion monotonicity
remaining after randomization also contributes to such robustness. Based on
this observation, we develop a novel defense method using temporal shuffling of
input videos against adversarial attacks for action recognition models. Another
observation enabling our defense method is that adversarial perturbations on
videos are sensitive to temporal destruction. To the best of our knowledge,
this is the first attempt to design a defense method without additional
training for 3D CNN-based video action recognition models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.04969">Assessing hierarchies by their consistent segmentations. (arXiv:2204.04969v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gutman_Z/0/1/0/all/0/1">Zeev Gutman</a>, <a href="http://arxiv.org/find/cs/1/au:+Vij_R/0/1/0/all/0/1">Ritvik Vij</a> (IIT Delhi), <a href="http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1">Laurent Najman</a> (LIGM), <a href="http://arxiv.org/find/cs/1/au:+Lindenbaum_M/0/1/0/all/0/1">Michael Lindenbaum</a></p>
<p>Current approaches to generic segmentation start by creating a hierarchy of
nested image partitions and then specifying a segmentation from it. Our first
contribution is to describe several ways, most of them new, for specifying
segmentations using the hierarchy elements. Then, we consider the best
hierarchy-induced segmentation specified by a limited number of hierarchy
elements. We focus on a common quality measure for binary segmentations, the
Jaccard index (also known as IoU). Optimizing the Jaccard index is highly
non-trivial, and yet we propose an efficient approach for doing exactly that.
This way we get algorithm-independent upper bounds on the quality of any
segmentation created from the hierarchy. We found that the obtainable
segmentation quality varies significantly depending on the way that the
segments are specified by the hierarchy elements, and that representing a
segmentation with only a few hierarchy elements is often possible. (Code is
available).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.08324">Exploring the Interactive Guidance for Unified and Effective Image Matting. (arXiv:2205.08324v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dinghao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yiqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Conghui He</a></p>
<p>Recent image matting studies are developing towards proposing trimap-free or
interactive methods for complete complex image matting tasks. Although avoiding
the extensive labors of trimap annotation, existing methods still suffer from
two limitations: (1) For the single image with multiple objects, it is
essential to provide extra interaction information to help determining the
matting target; (2) For transparent objects, the accurate regression of alpha
matte from RGB image is much more difficult compared with the opaque ones. In
this work, we propose a Unified Interactive image Matting method, named UIM,
which solves the limitations and achieves satisfying matting results for any
scenario. Specifically, UIM leverages multiple types of user interaction to
avoid the ambiguity of multiple matting targets, and we compare the pros and
cons of different annotation types in detail. To unify the matting performance
for transparent and opaque objects, we decouple image matting into two stages,
i.e., foreground segmentation and transparency prediction. Moreover, we design
a multi-scale attentive fusion module to alleviate the vagueness in the
boundary region. Experimental results demonstrate that UIM achieves
state-of-the-art performance on the Composition-1K test set and a synthetic
unified dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04979">Convolutional layers are equivariant to discrete shifts but not continuous translations. (arXiv:2206.04979v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McGreivy_N/0/1/0/all/0/1">Nick McGreivy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hakim_A/0/1/0/all/0/1">Ammar Hakim</a></p>
<p>The purpose of this short and simple note is to clarify a common
misconception about convolutional neural networks (CNNs). CNNs are made up of
convolutional layers which are shift equivariant due to weight sharing.
However, convolutional layers are not translation equivariant, even when
boundary effects are ignored and when pooling and subsampling are absent. This
is because shift equivariance is a discrete symmetry while translation
equivariance is a continuous symmetry. This fact is well known among
researchers in equivariant machine learning, but is usually overlooked among
non-experts. To minimize confusion, we suggest using the term `shift
equivariance' to refer to discrete shifts in pixels and `translation
equivariance' to refer to continuous translations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.13326">Point Cloud Attacks in Graph Spectral Domain: When 3D Geometry Meets Graph Signal Processing. (arXiv:2207.13326v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Daizong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a></p>
<p>With the increasing attention in various 3D safety-critical applications,
point cloud learning models have been shown to be vulnerable to adversarial
attacks. Although existing 3D attack methods achieve high success rates, they
delve into the data space with point-wise perturbation, which may neglect the
geometric characteristics. Instead, we propose point cloud attacks from a new
perspective -- the graph spectral domain attack, aiming to perturb graph
transform coefficients in the spectral domain that corresponds to varying
certain geometric structure. Specifically, leveraging on graph signal
processing, we first adaptively transform the coordinates of points onto the
spectral domain via graph Fourier transform (GFT) for compact representation.
Then, we analyze the influence of different spectral bands on the geometric
structure, based on which we propose to perturb the GFT coefficients via a
learnable graph spectral filter. Considering the low-frequency components
mainly contribute to the rough shape of the 3D object, we further introduce a
low-frequency constraint to limit perturbations within imperceptible
high-frequency components. Finally, the adversarial point cloud is generated by
transforming the perturbed spectral representation back to the data domain via
the inverse GFT. Experimental results demonstrate the effectiveness of the
proposed attack in terms of both the imperceptibility and attack success rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09943">Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition. (arXiv:2210.09943v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1">Samuel Dooley</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1">Rhea Sanjay Sukthanker</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P. Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1">Colin White</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1">Micah Goldblum</a></p>
<p>Face recognition systems are widely deployed in safety-critical applications,
including law enforcement, yet they exhibit bias across a range of
socio-demographic dimensions, such as gender and race. Conventional wisdom
dictates that model biases arise from biased training data. As a consequence,
previous works on bias mitigation largely focused on pre-processing the
training data, adding penalties to prevent bias from effecting the model during
training, or post-processing predictions to debias them, yet these approaches
have shown limited success on hard problems such as face recognition. In our
work, we discover that biases are actually inherent to neural network
architectures themselves. Following this reframing, we conduct the first neural
architecture search for fairness, jointly with a search for hyperparameters.
Our search outputs a suite of models which Pareto-dominate all other
high-performance architectures and existing bias mitigation methods in terms of
accuracy and fairness, often by large margins, on the two most widely used
datasets for face identification, CelebA and VGGFace2. Furthermore, these
models generalize to other datasets and sensitive attributes. We release our
code, models and raw data files at https://github.com/dooleys/FR-NAS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.11407">Similarity of Neural Architectures using Adversarial Attack Transferability. (arXiv:2210.11407v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jaehui Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">Dongyoon Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1">Byeongho Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Song Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1">Sanghyuk Chun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jong-Seok Lee</a></p>
<p>In recent years, many deep neural architectures have been developed for image
classification. Whether they are similar or dissimilar and what factors
contribute to their (dis)similarities remains curious. To address this
question, we aim to design a quantitative and scalable similarity measure
between neural architectures. We propose Similarity by Attack Transferability
(SAT) from the observation that adversarial attack transferability contains
information related to input gradients and decision boundaries widely used to
understand model behaviors. We conduct a large-scale analysis on 69
state-of-the-art ImageNet classifiers using our proposed similarity function to
answer the question. Moreover, we observe neural architecture-related phenomena
using model similarity that model diversity can lead to better performance on
model ensembles and knowledge distillation under specific conditions. Our
results provide insights into why developing diverse neural architectures with
distinct components is necessary.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.03894">visClust: A visual clustering algorithm based on orthogonal projections. (arXiv:2211.03894v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Breger_A/0/1/0/all/0/1">Anna Breger</a>, <a href="http://arxiv.org/find/cs/1/au:+Karner_C/0/1/0/all/0/1">Clemens Karner</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehler_M/0/1/0/all/0/1">Martin Ehler</a></p>
<p>We present a novel clustering algorithm, visClust, that is based on lower
dimensional data representations and visual interpretation. Thereto, we design
a transformation that allows the data to be represented by a binary integer
array enabling the use of image processing methods to select a partition.
Qualitative and quantitative analyses measured in accuracy and an adjusted
Rand-Index show that the algorithm performs well while requiring low runtime
and RAM. We compare the results to 6 state-of-the-art algorithms with available
code, confirming the quality of visClust by superior performance in most
experiments. Moreover, the algorithm asks for just one obligatory input
parameter while allowing optimization via optional parameters. The code is made
available on GitHub and straightforward to use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.05588">Watching the News: Towards VideoQA Models that can Read. (arXiv:2211.05588v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jahagirdar_S/0/1/0/all/0/1">Soumya Jahagirdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1">Minesh Mathew</a>, <a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1">Dimosthenis Karatzas</a>, <a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1">C. V. Jawahar</a></p>
<p>Video Question Answering methods focus on commonsense reasoning and visual
cognition of objects or persons and their interactions over time. Current
VideoQA approaches ignore the textual information present in the video.
Instead, we argue that textual information is complementary to the action and
provides essential contextualisation cues to the reasoning process. To this
end, we propose a novel VideoQA task that requires reading and understanding
the text in the video. To explore this direction, we focus on news videos and
require QA systems to comprehend and answer questions about the topics
presented by combining visual and textual cues in the video. We introduce the
``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$
news videos obtained from diverse news channels from around the world. We
demonstrate the limitations of current Scene Text VQA and VideoQA methods and
propose ways to incorporate scene text information into VideoQA methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10558">Internal Representations of Vision Models Through the Lens of Frames on Data Manifolds. (arXiv:2211.10558v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1">Henry Kvinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Jorgenson_G/0/1/0/all/0/1">Grayson Jorgenson</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1">Charles Godfrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1">Tegan Emerson</a></p>
<p>While the last five years have seen considerable progress in understanding
the internal representations of deep learning models, many questions remain.
This is especially true when trying to understand the impact of model design
choices, such as model architecture or training algorithm, on hidden
representation geometry and dynamics. In this work we present a new approach to
studying such representations inspired by the idea of a frame on the tangent
bundle of a manifold. Our construction, which we call a neural frame, is formed
by assembling a set of vectors representing specific types of perturbations of
a data point, for example infinitesimal augmentations, noise perturbations, or
perturbations produced by a generative model, and studying how these change as
they pass through a network. Using neural frames, we make observations about
the way that models process, layer-by-layer, specific modes of variation within
a small neighborhood of a datapoint. Our results provide new perspectives on a
number of phenomena, such as the manner in which training with augmentation
produces model invariance or the proposed trade-off between adversarial
training and model generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01506">Autonomous Apple Fruitlet Sizing and Growth Rate Tracking using Computer Vision. (arXiv:2212.01506v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freeman_H/0/1/0/all/0/1">Harry Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Qadri_M/0/1/0/all/0/1">Mohamad Qadri</a>, <a href="http://arxiv.org/find/cs/1/au:+Silwal_A/0/1/0/all/0/1">Abhisesh Silwal</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_P/0/1/0/all/0/1">Paul O&#x27;Connor</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_Z/0/1/0/all/0/1">Zachary Rubinstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Cooley_D/0/1/0/all/0/1">Daniel Cooley</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantor_G/0/1/0/all/0/1">George Kantor</a></p>
<p>In this paper, we present a computer vision-based approach to measure the
sizes and growth rates of apple fruitlets. Measuring the growth rates of apple
fruitlets is important because it allows apple growers to determine when to
apply chemical thinners to their crops in order to optimize yield. The current
practice of obtaining growth rates involves using calipers to record sizes of
fruitlets across multiple days. Due to the number of fruitlets needed to be
sized, this method is laborious, time-consuming, and prone to human error. With
images collected by a hand-held stereo camera, our system, segments, clusters,
and fits ellipses to fruitlets to measure their diameters. The growth rates are
then calculated by temporally associating clustered fruitlets across days. We
provide quantitative results on data collected in an apple orchard, and
demonstrate that our system is able to predict abscise rates within 3.5% of the
current method with a 6 times improvement in speed, while requiring
significantly less manual effort. Moreover, we provide results on images
captured by a robotic system in the field, and discuss the next steps required
to make the process fully autonomous.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04821">PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data. (arXiv:2212.04821v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1">Roei Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Abramovich_O/0/1/0/all/0/1">Ofir Abramovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Avraham_E/0/1/0/all/0/1">Elad Ben-Avraham</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbelle_A/0/1/0/all/0/1">Assaf Arbelle</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1">Leonid Karlinsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1">Ariel Shamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1">Amir Globerson</a></p>
<p>Action recognition models have achieved impressive results by incorporating
scene-level annotations, such as objects, their relations, 3D structure, and
more. However, obtaining annotations of scene structure for videos requires a
significant amount of effort to gather and annotate, making these methods
expensive to train. In contrast, synthetic datasets generated by graphics
engines provide powerful alternatives for generating scene-level annotations
across multiple tasks. In this work, we propose an approach to leverage
synthetic scene data for improving video understanding. We present a multi-task
prompt learning approach for video transformers, where a shared video
transformer backbone is enhanced by a small set of specialized parameters for
each task. Specifically, we add a set of "task prompts", each corresponding to
a different task, and let each prompt predict task-related annotations. This
design allows the model to capture information shared among synthetic scene
tasks as well as information shared between synthetic scene tasks and a real
video downstream task throughout the entire network. We refer to this approach
as "Promptonomy", since the prompts model task-related structure. We propose
the PromptonomyViT model (PViT), a video transformer that incorporates various
types of scene-level information from synthetic data using the "Promptonomy"
approach. PViT shows strong performance improvements on multiple video
understanding tasks and datasets. Project page:
\url{https://ofir1080.github.io/PromptonomyViT}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.07207">MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds. (arXiv:2212.07207v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krispel_G/0/1/0/all/0/1">Georg Krispel</a>, <a href="http://arxiv.org/find/cs/1/au:+Schinagl_D/0/1/0/all/0/1">David Schinagl</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruhwirth_Reisinger_C/0/1/0/all/0/1">Christian Fruhwirth-Reisinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1">Horst Possegger</a>, <a href="http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1">Horst Bischof</a></p>
<p>The sensing process of large-scale LiDAR point clouds inevitably causes large
blind spots, i.e. regions not visible to the sensor. We demonstrate how these
inherent sampling properties can be effectively utilized for self-supervised
representation learning by designing a highly effective pre-training framework
that considerably reduces the need for tedious 3D annotations to train
state-of-the-art object detectors. Our Masked AutoEncoder for LiDAR point
clouds (MAELi) intuitively leverages the sparsity of LiDAR point clouds in both
the encoder and decoder during reconstruction. This results in more expressive
and useful initialization, which can be directly applied to downstream
perception tasks, such as 3D object detection or semantic segmentation for
autonomous driving. In a novel reconstruction approach, MAELi distinguishes
between empty and occluded space and employs a new masking strategy that
targets the LiDAR's inherent spherical projection. Thereby, without any ground
truth whatsoever and trained on single frames only, MAELi obtains an
understanding of the underlying 3D scene geometry and semantics. To demonstrate
the potential of MAELi, we pre-train backbones in an end-to-end manner and show
the effectiveness of our unsupervised pre-trained weights on the tasks of 3D
object detection and semantic segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00752">Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v4 [cs.NI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ohta_S/0/1/0/all/0/1">Shoki Ohta</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1">Takayuki Nishio</a>, <a href="http://arxiv.org/find/cs/1/au:+Kudo_R/0/1/0/all/0/1">Riichi Kudo</a>, <a href="http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1">Kahoko Takahashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagata_H/0/1/0/all/0/1">Hisashi Nagata</a></p>
<p>This study demonstrates the feasibility of point cloud-based proactive link
quality prediction for millimeter-wave (mmWave) communications. Previous
studies have proposed machine learning-based methods to predict received signal
strength for future time periods using time series of depth images to mitigate
the line-of-sight (LOS) path blockage by pedestrians in mmWave communication.
However, these image-based methods have limited applicability due to privacy
concerns as camera images may contain sensitive information. This study
proposes a point cloud-based method for mmWave link quality prediction and
demonstrates its feasibility through experiments. Point clouds represent
three-dimensional (3D) spaces as a set of points and are sparser and less
likely to contain sensitive information than camera images. Additionally, point
clouds provide 3D position and motion information, which is necessary for
understanding the radio propagation environment involving pedestrians. This
study designs the mmWave link quality prediction method and conducts realistic
indoor experiments, where the link quality fluctuates significantly due to
human blockage, using commercially available IEEE 802.11ad-based 60 GHz
wireless LAN devices and Kinect v2 RGB-D camera and Velodyne VLP-16 light
detection and ranging (LiDAR) for point cloud acquisition. The experimental
results showed that our proposed method can predict future large attenuation of
mmWave received signal strength and throughput induced by the LOS path blockage
by pedestrians with comparable or superior accuracy to image-based prediction
methods. Hence, our point cloud-based method can serve as a viable alternative
to image-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12340">Incremental Value and Interpretability of Radiomics Features of Both Lung and Epicardial Adipose Tissue for Detecting the Severity of COVID-19 Infection. (arXiv:2301.12340v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_N/0/1/0/all/0/1">Ni Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1">Yanhui Tian</a>, <a href="http://arxiv.org/find/eess/1/au:+Neves_D/0/1/0/all/0/1">Daniel Gama das Neves</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Mesquita_C/0/1/0/all/0/1">Claudio Tinoco Mesquita</a>, <a href="http://arxiv.org/find/eess/1/au:+Martins_W/0/1/0/all/0/1">Wolney de Andrade Martins</a>, <a href="http://arxiv.org/find/eess/1/au:+Santos_A/0/1/0/all/0/1">Alair Augusto Sarmet Moreira Damas dos Santos</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yanting Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1">Chuang Han</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_F/0/1/0/all/0/1">Fubao Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_N/0/1/0/all/0/1">Neng Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1">Weihua Zhou</a></p>
<p>Epicardial adipose tissue (EAT) is known for its pro-inflammatory properties
and association with Coronavirus Disease 2019 (COVID-19) severity. However,
current EAT segmentation methods do not consider positional information.
Additionally, the detection of COVID-19 severity lacks consideration for EAT
radiomics features, which limits interpretability. This study investigates the
use of radiomics features from EAT and lungs to detect the severity of COVID-19
infections. A retrospective analysis of 515 patients with COVID-19 (Cohort1:
415, Cohort2: 100) was conducted using a proposed three-stage deep learning
approach for EAT extraction. Lung segmentation was achieved using a published
method. A hybrid model for detecting the severity of COVID-19 was built in a
derivation cohort, and its performance and uncertainty were evaluated in
internal (125, Cohort1) and external (100, Cohort2) validation cohorts. For EAT
extraction, the Dice similarity coefficients (DSC) of the two centers were
0.972 (+-0.011) and 0.968 (+-0.005), respectively. For severity detection, the
hybrid model with radiomics features of both lungs and EAT showed improvements
in AUC, net reclassification improvement (NRI), and integrated discrimination
improvement (IDI) compared to the model with only lung radiomics features. The
hybrid model exhibited an increase of 0.1 (p&lt;0.001), 19.3%, and 18.0%
respectively, in the internal validation cohort and an increase of 0.09
(p&lt;0.001), 18.0%, and 18.0%, respectively, in the external validation cohort
while outperforming existing detection methods. Uncertainty quantification and
radiomics features analysis confirmed the interpretability of case prediction
after inclusion of EAT features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01330">SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections. (arXiv:2302.01330v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhaoxi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangcong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziwei Liu</a></p>
<p>In this work, we present SceneDreamer, an unconditional generative model for
unbounded 3D scenes, which synthesizes large-scale 3D landscapes from random
noise. Our framework is learned from in-the-wild 2D image collections only,
without any 3D annotations. At the core of SceneDreamer is a principled
learning paradigm comprising 1) an efficient yet expressive 3D scene
representation, 2) a generative scene parameterization, and 3) an effective
renderer that can leverage the knowledge from 2D images. Our approach begins
with an efficient bird's-eye-view (BEV) representation generated from simplex
noise, which includes a height field for surface elevation and a semantic field
for detailed scene semantics. This BEV scene representation enables 1)
representing a 3D scene with quadratic complexity, 2) disentangled geometry and
semantics, and 3) efficient training. Moreover, we propose a novel generative
neural hash grid to parameterize the latent space based on 3D positions and
scene semantics, aiming to encode generalizable features across various scenes.
Lastly, a neural volumetric renderer, learned from 2D image collections through
adversarial training, is employed to produce photorealistic images. Extensive
experiments demonstrate the effectiveness of SceneDreamer and superiority over
state-of-the-art methods in generating vivid yet diverse unbounded 3D worlds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04871">In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing. (arXiv:2302.04871v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yiran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1">Zhixin Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1">Cameron Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seoung Wug Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jia-Bin Huang</a></p>
<p>3D-aware GANs offer new capabilities for view synthesis while preserving the
editing functionalities of their 2D counterparts. GAN inversion is a crucial
step that seeks the latent code to reconstruct input images or videos,
subsequently enabling diverse editing tasks through manipulation of this latent
code. However, a model pre-trained on a particular dataset (e.g., FFHQ) often
has difficulty reconstructing images with out-of-distribution (OOD) objects
such as faces with heavy make-up or occluding objects. We address this issue by
explicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea
is to represent the image using two individual neural radiance fields: one for
the in-distribution content and the other for the out-of-distribution object.
The final reconstruction is achieved by optimizing the composition of these two
radiance fields with carefully designed regularization. We demonstrate that our
explicit decomposition alleviates the inherent trade-off between reconstruction
fidelity and editability. We evaluate reconstruction accuracy and editability
of our method on challenging real face images and videos and showcase favorable
results against other baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11196">Invariant Target Detection in Images through the Normalized 2-D Correlation Technique. (arXiv:2302.11196v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Al_Obaidi_F/0/1/0/all/0/1">Fatin E. M. Al-Obaidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Saleh_A/0/1/0/all/0/1">Anwar H. Al-Saleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kafi_S/0/1/0/all/0/1">Shaymaa H. Kafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Karam_A/0/1/0/all/0/1">Ali J.Karam</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Zuky_A/0/1/0/all/0/1">Ali A. D. Al-Zuky</a></p>
<p>The normalized 2-D correlation technique is a robust method for detecting
targets in images due to its ability to remain invariant under rotation,
translation, and scaling. This paper examines the impact of translation, and
scaling on target identification in images. The results indicate a high level
of accuracy in detecting targets, even when they are exhibit variations in
location and size. The results indicate that the similarity between the image
and the two used targets improves as the resize ratio increases. All
statistical estimators demonstrate a strong similarity between the original and
extracted targets. The elapsed time for all scenarios falls within the range
(44.75-44.85), (37.48-37.73) seconds for bird and children targets
respectively, and the correlation coefficient displays stable relationships
with values that fall within the range of (0.90-0.98) and (0.87-0.93) for bird
and children targets respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00491">Pose Impact Estimation on Face Recognition using 3D-Aware Synthetic Data with Application to Quality Assessment. (arXiv:2303.00491v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grimmer_M/0/1/0/all/0/1">Marcel Grimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1">Christian Rathgeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1">Christoph Busch</a></p>
<p>Evaluating the quality of facial images is essential for operating face
recognition systems with sufficient accuracy. The recent advances in face
quality standardisation (ISO/IEC CD3 29794-5) recommend the usage of component
quality measures for breaking down face quality into its individual factors,
hence providing valuable feedback for operators to re-capture low-quality
images. In light of recent advances in 3D-aware generative adversarial
networks, we propose a novel dataset, Syn-YawPitch, comprising 1000 identities
with varying yaw-pitch angle combinations. Utilizing this dataset, we
demonstrate that pitch angles beyond 30 degrees have a significant impact on
the biometric performance of current face recognition systems. Furthermore, we
propose a lightweight and explainable pose quality predictor that adheres to
the draft international standard of ISO/IEC CD3 29794-5 and benchmark it
against state-of-the-art face image quality assessment algorithms
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06458">ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yuexian Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1">David A. Clifton</a></p>
<p>Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13472">Plotting Behind the Scenes: Towards Learnable Game Engines. (arXiv:2303.13472v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Menapace_W/0/1/0/all/0/1">Willi Menapace</a>, <a href="http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1">Aliaksandr Siarohin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1">St&#xe9;phane Lathuili&#xe8;re</a>, <a href="http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1">Panos Achlioptas</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1">Elisa Ricci</a></p>
<p>Neural video game simulators emerged as powerful tools to generate and edit
videos. Their idea is to represent games as the evolution of an environment's
state driven by the actions of its agents. While such a paradigm enables users
to play a game action-by-action, its rigidity precludes more semantic forms of
control. To overcome this limitation, we augment game models with prompts
specified as a set of natural language actions and desired states. The result-a
Promptable Game Model (PGM)-makes it possible for a user to play the game by
prompting it with high- and low-level action sequences. Most captivatingly, our
PGM unlocks the director's mode, where the game is played by specifying goals
for the agents in the form of a prompt. This requires learning "game AI",
encapsulated by our animation model, to navigate the scene using high-level
constraints, play against an adversary, and devise a strategy to win a point.
To render the resulting state, we use a compositional NeRF representation
encapsulated in our synthesis model. To foster future research, we present
newly collected, annotated and calibrated Tennis and Minecraft datasets. Our
method significantly outperforms existing neural video game simulators in terms
of rendering quality and unlocks applications beyond the capabilities of the
current state of the art. Our framework, data, and models are available at
https://snap-research.github.io/promptable-game-models/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16343">Facial recognition technology and human raters can predict political orientation from images of expressionless faces even when controlling for demographics and self-presentation. (arXiv:2303.16343v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1">Michal Kosinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Khambatta_P/0/1/0/all/0/1">Poruz Khambatta</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yilun Wang</a></p>
<p>Carefully standardized facial images of 591 participants were taken in the
laboratory, while controlling for self-presentation, facial expression, head
orientation, and image properties. They were presented to human raters and a
facial recognition algorithm: both humans (r=.21) and the algorithm (r=.22)
could predict participants' scores on a political orientation scale (Cronbach's
alpha=.94) decorrelated with age, gender, and ethnicity. These effects are on
par with how well job interviews predict job success, or alcohol drives
aggressiveness. Algorithm's predictive accuracy was even higher (r=.31) when it
leveraged information on participants' age, gender, and ethnicity. Moreover,
the associations between facial appearance and political orientation seem to
generalize beyond our sample: The predictive model derived from standardized
images (while controlling for age, gender, and ethnicity) could predict
political orientation (r=.13) from naturalistic images of 3,401 politicians
from the U.S., UK, and Canada. The analysis of facial features associated with
political orientation revealed that conservatives tended to have larger lower
faces. The predictability of political orientation from standardized images has
critical implications for privacy, the regulation of facial recognition
technology, and understanding the origins and consequences of political
orientation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17859">MapFormer: Boosting Change Detection by Using Pre-change Information. (arXiv:2303.17859v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bernhard_M/0/1/0/all/0/1">Maximilian Bernhard</a>, <a href="http://arxiv.org/find/cs/1/au:+Strauss_N/0/1/0/all/0/1">Niklas Strau&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1">Matthias Schubert</a></p>
<p>Change detection in remote sensing imagery is essential for a variety of
applications such as urban planning, disaster management, and climate research.
However, existing methods for identifying semantically changed areas overlook
the availability of semantic information in the form of existing maps
describing features of the earth's surface. In this paper, we leverage this
information for change detection in bi-temporal images. We show that the simple
integration of the additional information via concatenation of latent
representations suffices to significantly outperform state-of-the-art change
detection methods. Motivated by this observation, we propose the new task of
*Conditional Change Detection*, where pre-change semantic information is used
as input next to bi-temporal images. To fully exploit the extra information, we
propose *MapFormer*, a novel architecture based on a multi-modal feature fusion
module that allows for feature processing conditioned on the available semantic
information. We further employ a supervised, cross-modal contrastive loss to
guide the learning of visual representations. Our approach outperforms existing
change detection methods by an absolute 11.7\% and 18.4\% in terms of binary
change IoU on DynamicEarthNet and HRSCD, respectively. Furthermore, we
demonstrate the robustness of our approach to the quality of the pre-change
semantic information and the absence pre-change imagery. The code is available
at https://github.com/mxbh/mapformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10306">FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits. (arXiv:2304.10306v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karpikova_P/0/1/0/all/0/1">Polina Karpikova</a>, <a href="http://arxiv.org/find/cs/1/au:+Ekaterina_R/0/1/0/all/0/1">Radionova Ekaterina</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaschenko_A/0/1/0/all/0/1">Anastasia Yaschenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Spiridonov_A/0/1/0/all/0/1">Andrei Spiridonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostyushko_L/0/1/0/all/0/1">Leonid Kostyushko</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabbricatore_R/0/1/0/all/0/1">Riccardo Fabbricatore</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivakhnenko_A/0/1/0/all/0/1">Aleksei Ivakhnenko</a></p>
<p>Generative DNNs are a powerful tool for image synthesis, but they are limited
by their computational load. On the other hand, given a trained model and a
task, e.g. faces generation within a range of characteristics, the output image
quality will be unevenly distributed among images with different
characteristics. It follows, that we might restrain the models complexity on
some instances, maintaining a high quality. We propose a method for diminishing
computations by adding so-called early exit branches to the original
architecture, and dynamically switching the computational path depending on how
difficult it will be to render the output. We apply our method on two different
SOTA models performing generative tasks: generation from a semantic map, and
cross-reenactment of face expressions; showing it is able to output images with
custom lower-quality thresholds. For a threshold of LPIPS &lt;=0.1, we diminish
their computations by up to a half. This is especially relevant for real-time
applications such as synthesis of faces, when quality loss needs to be
contained, but most of the inputs need fewer computations than the complex
instances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14484">OriCon3D: Effective 3D Object Detection using Orientation and Confidence. (arXiv:2304.14484v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rajani_D/0/1/0/all/0/1">Dhyey Manish Rajani</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Surya Pratap Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Swayampakula_R/0/1/0/all/0/1">Rahul Kashyap Swayampakula</a></p>
<p>We introduce a technique for detecting 3D objects and estimating their
position from a single image. Our method is built on top of a similar
state-of-the-art technique [1], but with improved accuracy. The approach
followed in this research first estimates common 3D properties of an object
using a Deep Convolutional Neural Network (DCNN), contrary to other frameworks
that only leverage centre-point predictions. We then combine these estimates
with geometric constraints provided by a 2D bounding box to produce a complete
3D bounding box. The first output of our network estimates the 3D object
orientation using a discrete-continuous loss [1]. The second output predicts
the 3D object dimensions with minimal variance. Here we also present our
extensions by augmenting light-weight feature extractors and a customized
multibin architecture. By combining these estimates with the geometric
constraints of the 2D bounding box, we can accurately (or comparatively)
determine the 3D object pose better than our baseline [1] on the KITTI 3D
detection benchmark [2].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03907">Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation. (arXiv:2305.03907v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1">Bolin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryan_F/0/1/0/all/0/1">Fiona Ryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1">Wenqi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Miao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1">James M. Rehg</a></p>
<p>Egocentric gaze anticipation serves as a key building block for the emerging
capability of Augmented Reality. Notably, gaze behavior is driven by both
visual cues and audio signals during daily activities. Motivated by this
observation, we introduce the first model that leverages both the video and
audio modalities for egocentric gaze anticipation. Specifically, we propose a
Contrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two
modules to separately capture audio-visual correlations in spatial and temporal
dimensions, and applies a contrastive loss on the re-weighted audio-visual
features from fusion modules for representation learning. We conduct extensive
ablation studies and thorough analysis using two egocentric video datasets:
Ego4D and Aria, to validate our model design. We also demonstrate our model
outperforms prior state-of-the-art methods by at least +1.9% and +1.6%.
Moreover, we provide visualizations to show the gaze anticipation results and
provide additional insights into audio-visual representation learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09132">DualGenerator: Information Interaction-based Generative Network for Point Cloud Completion. (arXiv:2305.09132v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1">Pengcheng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Haozhe Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jihua Zhu</a></p>
<p>Point cloud completion estimates complete shapes from incomplete point clouds
to obtain higher-quality point cloud data. Most existing methods only consider
global object features, ignoring spatial and semantic information of adjacent
points. They cannot distinguish structural information well between different
object parts, and the robustness of models is poor. To tackle these challenges,
we propose an information interaction-based generative network for point cloud
completion ($\mathbf{DualGenerator}$). It contains an adversarial generation
path and a variational generation path, which interact with each other and
share weights. DualGenerator introduces a local refinement module in generation
paths, which captures general structures from partial inputs, and then refines
shape details of the point cloud. It promotes completion in the unknown region
and makes a distinction between different parts more obvious. Moreover, we
design DGStyleGAN to improve the generation quality further. It promotes the
robustness of this network combined with fusion analysis of dual-path
completion results. Qualitative and quantitative evaluations demonstrate that
our method is superior on MVP and Completion3D datasets. The performance will
not degrade significantly after adding noise interference or sparse sampling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13840">Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yatai Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hefeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Pan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiashi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1">Xin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xuefeng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>Recent advancements in diffusion models have unlocked unprecedented abilities
in visual creation. However, current text-to-video generation models struggle
with the trade-off among movement range, action coherence and object
consistency. To mitigate this issue, we present a controllable text-to-video
(T2V) diffusion model, called Control-A-Video, capable of maintaining
consistency while customizable video synthesis. Based on a pre-trained
conditional text-to-image (T2I) diffusion model, our model aims to generate
videos conditioned on a sequence of control signals, such as edge or depth
maps. For the purpose of improving object consistency, Control-A-Video
integrates motion priors and content priors into video generation. We propose
two motion-adaptive noise initialization strategies, which are based on pixel
residual and optical flow, to introduce motion priors from input videos,
producing more coherent videos. Moreover, a first-frame conditioned controller
is proposed to generate videos from content priors of the first frame, which
facilitates the semantic alignment with text and allows longer video generation
in an auto-regressive manner. With the proposed architecture and strategies,
our model achieves resource-efficient convergence and generate consistent and
coherent videos with fine-grained control. Extensive experiments demonstrate
its success in various video generative tasks such as video editing and video
style transfer, outperforming previous methods in terms of consistency and
quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16225">ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models. (arXiv:2305.16225v3 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1">Weiming Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1">Fan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1">Nisha Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haibin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chongyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1">Tong-Yee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1">Oliver Deussen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Changsheng Xu</a></p>
<p>Personalizing generative models offers a way to guide image generation with
user-provided references. Current personalization methods can invert an object
or concept into the textual conditioning space and compose new natural
sentences for text-to-image diffusion models. However, representing and editing
specific visual attributes such as material, style, and layout remains a
challenge, leading to a lack of disentanglement and editability. To address
this problem, we propose a novel approach that leverages the step-by-step
generation process of diffusion models, which generate images from low to high
frequency information, providing a new perspective on representing, generating,
and editing images. We develop the Prompt Spectrum Space P*, an expanded
textual conditioning space, and a new image representation method called
\sysname. ProSpect represents an image as a collection of inverted textual
token embeddings encoded from per-stage prompts, where each prompt corresponds
to a specific generation stage (i.e., a group of consecutive steps) of the
diffusion model. Experimental results demonstrate that P* and ProSpect offer
better disentanglement and controllability compared to existing methods. We
apply ProSpect in various personalized attribute-aware image generation
applications, such as image-guided or text-driven manipulations of materials,
style, and layout, achieving previously unattainable results from a single
image input without fine-tuning the diffusion models. Our source code is
available athttps://github.com/zyxElsa/ProSpect.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00245">From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1">Peter Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1">Mandar Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_J/0/1/0/all/0/1">James Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1">Panupong Pasupat</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1">Urvashi Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kenton Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1">Kristina Toutanova</a></p>
<p>Much of the previous work towards digital agents for graphical user
interfaces (GUIs) has relied on text-based representations (derived from HTML
or other structured data sources), which are not always readily available.
These input representations have been often coupled with custom, task-specific
action spaces. This paper focuses on creating agents that interact with the
digital world using the same conceptual interface that humans commonly use --
via pixel-based screenshots and a generic action space corresponding to
keyboard and mouse actions. Building upon recent progress in pixel-based
pretraining, we show, for the first time, that it is possible for such agents
to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based
instruction following tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00971">ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation. (arXiv:2306.00971v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1">Shaozhe Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shihao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1">Kwan-Yee K. Wong</a></p>
<p>Personalized text-to-image generation using diffusion models has recently
emerged and garnered significant interest. This task learns a novel concept
(e.g., a unique toy), illustrated in a handful of images, into a generative
model that captures fine visual details and generates photorealistic images
based on textual embeddings. In this paper, we present ViCo, a novel
lightweight plug-and-play method that seamlessly integrates visual condition
into personalized text-to-image generation. ViCo stands out for its unique
feature of not requiring any fine-tuning of the original diffusion model
parameters, thereby facilitating more flexible and scalable model deployment.
This key advantage distinguishes ViCo from most existing models that
necessitate partial or full diffusion fine-tuning. ViCo incorporates an image
attention module that conditions the diffusion process on patch-wise visual
semantics, and an attention-based object mask that comes at no extra cost from
the attention module. Despite only requiring light parameter training (~6%
compared to the diffusion U-Net), ViCo delivers performance that is on par
with, or even surpasses, all state-of-the-art models, both qualitatively and
quantitatively. This underscores the efficacy of ViCo, making it a highly
promising solution for personalized text-to-image generation without the need
for diffusion model fine-tuning. Code: https://github.com/haoosz/ViCo
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03881">Emergent Correspondence from Image Diffusion. (arXiv:2306.03881v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Luming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1">Menglin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1">Cheng Perng Phoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1">Bharath Hariharan</a></p>
<p>Finding correspondences between images is a fundamental problem in computer
vision. In this paper, we show that correspondence emerges in image diffusion
models without any explicit supervision. We propose a simple strategy to
extract this implicit knowledge out of diffusion networks as image features,
namely DIffusion FeaTures (DIFT), and use them to establish correspondences
between real images. Without any additional fine-tuning or supervision on the
task-specific data or annotations, DIFT is able to outperform both
weakly-supervised methods and competitive off-the-shelf features in identifying
semantic, geometric, and temporal correspondences. Particularly for semantic
correspondence, DIFT from Stable Diffusion is able to outperform DINO and
OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k
benchmark. It even outperforms the state-of-the-art supervised methods on 9 out
of 18 categories while remaining on par for the overall performance. Project
page: https://diffusionfeatures.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12816">XAI-TRIS: Non-linear image benchmarks to quantify false positive post-hoc attribution of feature importance. (arXiv:2306.12816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clark_B/0/1/0/all/0/1">Benedict Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilming_R/0/1/0/all/0/1">Rick Wilming</a>, <a href="http://arxiv.org/find/cs/1/au:+Haufe_S/0/1/0/all/0/1">Stefan Haufe</a></p>
<p>The field of 'explainable' artificial intelligence (XAI) has produced highly
cited methods that seek to make the decisions of complex machine learning (ML)
methods 'understandable' to humans, for example by attributing 'importance'
scores to input features. Yet, a lack of formal underpinning leaves it unclear
as to what conclusions can safely be drawn from the results of a given XAI
method and has also so far hindered the theoretical verification and empirical
validation of XAI methods. This means that challenging non-linear problems,
typically solved by deep neural networks, presently lack appropriate remedies.
Here, we craft benchmark datasets for three different non-linear classification
scenarios, in which the important class-conditional features are known by
design, serving as ground truth explanations. Using novel quantitative metrics,
we benchmark the explanation performance of a wide set of XAI methods across
three deep learning model architectures. We show that popular XAI methods are
often unable to significantly outperform random performance baselines and edge
detection methods. Moreover, we demonstrate that explanations derived from
different model architectures can be vastly different; thus, prone to
misinterpretation even under controlled conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08417">Divide&amp;Classify: Fine-Grained Classification for City-Wide Visual Place Recognition. (arXiv:2307.08417v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trivigno_G/0/1/0/all/0/1">Gabriele Trivigno</a>, <a href="http://arxiv.org/find/cs/1/au:+Berton_G/0/1/0/all/0/1">Gabriele Berton</a>, <a href="http://arxiv.org/find/cs/1/au:+Aragon_J/0/1/0/all/0/1">Juan Aragon</a>, <a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1">Barbara Caputo</a>, <a href="http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1">Carlo Masone</a></p>
<p>Visual Place recognition is commonly addressed as an image retrieval problem.
However, retrieval methods are impractical to scale to large datasets, densely
sampled from city-wide maps, since their dimension impact negatively on the
inference time. Using approximate nearest neighbour search for retrieval helps
to mitigate this issue, at the cost of a performance drop. In this paper we
investigate whether we can effectively approach this task as a classification
problem, thus bypassing the need for a similarity search. We find that existing
classification methods for coarse, planet-wide localization are not suitable
for the fine-grained and city-wide setting. This is largely due to how the
dataset is split into classes, because these methods are designed to handle a
sparse distribution of photos and as such do not consider the visual aliasing
problem across neighbouring classes that naturally arises in dense scenarios.
Thus, we propose a partitioning scheme that enables a fast and accurate
inference, preserving a simple learning procedure, and a novel inference
pipeline based on an ensemble of novel classifiers that uses the prototypes
learned via an angular margin loss. Our method, Divide&amp;Classify (D&amp;C), enjoys
the fast inference of classification solutions and an accuracy competitive with
retrieval methods on the fine-grained, city-wide setting. Moreover, we show
that D&amp;C can be paired with existing retrieval pipelines to speed up
computations by over 20 times while increasing their recall, leading to new
state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11086">PAPR: Proximity Attention Point Rendering. (arXiv:2307.11086v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanshu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Shichong Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1">Alireza Moazeni</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a></p>
<p>Learning accurate and parsimonious point cloud representations of scene
surfaces from scratch remains a challenge in 3D representation learning.
Existing point-based methods often suffer from the vanishing gradient problem
or require a large number of points to accurately model scene geometry and
texture. To address these limitations, we propose Proximity Attention Point
Rendering (PAPR), a novel method that consists of a point-based scene
representation and a differentiable renderer. Our scene representation uses a
point cloud where each point is characterized by its spatial position,
influence score, and view-independent feature vector. The renderer selects the
relevant points for each ray and produces accurate colours using their
associated features. PAPR effectively learns point cloud positions to represent
the correct scene geometry, even when the initialization drastically differs
from the target geometry. Notably, our method captures fine texture details
while using only a parsimonious set of points. We also demonstrate four
practical applications of our method: zero-shot geometry editing, object
manipulation, texture transfer, and exposure control. More results and code are
available on our project website at https://zvict.github.io/papr/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15250">D2S: Representing local descriptors and global scene coordinates for camera relocalization. (arXiv:2307.15250v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1">Bach-Thuan Bui</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1">Dinh-Tuan Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joo-Ho Lee</a></p>
<p>State-of-the-art visual localization methods mostly rely on complex
procedures to match local descriptors and 3D point clouds. However, these
procedures can incur significant cost in terms of inference, storage, and
updates over time. In this study, we propose a direct learning-based approach
that utilizes a simple network named D2S to represent local descriptors and
their scene coordinates. Our method is characterized by its simplicity and
cost-effectiveness. It solely leverages a single RGB image for localization
during the testing phase and only requires a lightweight model to encode a
complex sparse scene. The proposed D2S employs a combination of a simple loss
function and graph attention to selectively focus on robust descriptors while
disregarding areas such as clouds, trees, and several dynamic objects. This
selective attention enables D2S to effectively perform a binary-semantic
classification for sparse descriptors. Additionally, we propose a new outdoor
dataset to evaluate the capabilities of visual localization methods in terms of
scene generalization and self-updating from unlabeled observations. Our
approach outperforms the state-of-the-art CNN-based methods in scene coordinate
regression in indoor and outdoor environments. It demonstrates the ability to
generalize beyond training data, including scenarios involving transitions from
day to night and adapting to domain shifts, even in the absence of the labeled
data sources. The source code, trained models, dataset, and demo videos are
available at the following link: https://thpjp.github.io/d2s
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09778">Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rajabi_N/0/1/0/all/0/1">Navid Rajabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1">Jana Kosecka</a></p>
<p>With pre-training of vision-and-language models (VLMs) on large-scale
datasets of image-text pairs, several recent works showed that these
pre-trained models lack fine-grained understanding, such as the ability to
count and recognize verbs, attributes, or relationships. The focus of this work
is to study the ability of these models to understand spatial relations.
Previously, this has been tackled using image-text matching (e.g., Visual
Spatial Reasoning benchmark) or visual question answering (e.g., GQA or VQAv2),
both showing poor performance and a large gap compared to human performance. In
this work, we use explainability tools to understand the causes of poor
performance better and present an alternative fine-grained, compositional
approach for ranking spatial clauses. We combine the evidence from grounding
noun phrases corresponding to objects and their locations to compute the final
rank of the spatial clause. We demonstrate the approach on representative VLMs
(such as LXMERT, GPV, and MDETR) and compare and highlight their abilities to
reason about spatial relationships.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10158">HODN: Disentangling Human-Object Feature for HOI Detection. (arXiv:2308.10158v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Shuman Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiwen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Ke Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xianming Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>The task of Human-Object Interaction (HOI) detection is to detect humans and
their interactions with surrounding objects, where transformer-based methods
show dominant advances currently. However, these methods ignore the
relationship among humans, objects, and interactions: 1) human features are
more contributive than object ones to interaction prediction; 2) interactive
information disturbs the detection of objects but helps human detection. In
this paper, we propose a Human and Object Disentangling Network (HODN) to model
the HOI relationships explicitly, where humans and objects are first detected
by two disentangling decoders independently and then processed by an
interaction decoder. Considering that human features are more contributive to
interaction, we propose a Human-Guide Linking method to make sure the
interaction decoder focuses on the human-centric regions with human features as
the positional embeddings. To handle the opposite influences of interactions on
humans and objects, we propose a Stop-Gradient Mechanism to stop interaction
gradients from optimizing the object detection but to allow them to optimize
the human detection. Our proposed method achieves competitive performance on
both the V-COCO and the HICO-Det datasets. It can be combined with existing
methods easily for state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01860">Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hakim_Z/0/1/0/all/0/1">Zaber Ibn Abdul Hakim</a>, <a href="http://arxiv.org/find/cs/1/au:+Swargo_R/0/1/0/all/0/1">Rasman Mubtasim Swargo</a>, <a href="http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1">Muhammad Abdullah Adnan</a></p>
<p>In this paper, we devise a mechanism for the addition of multi-modal
information with an existing pipeline for continuous sign language recognition
and translation. In our procedure, we have incorporated optical flow
information with RGB images to enrich the features with movement-related
information. This work studies the feasibility of such modality inclusion using
a cross-modal encoder. The plugin we have used is very lightweight and doesn't
need to include a separate feature extractor for the new modality in an
end-to-end manner. We have applied the changes in both sign language
recognition and translation, improving the result in each case. We have
evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language
recognition and the RWTH-PHOENIX-2014T dataset for translation. On the
recognition task, our approach reduced the WER by 0.9, and on the translation
task, our approach increased most of the BLEU scores by ~0.6 on the test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02437">EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields. (arXiv:2310.02437v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1">Anish Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Madaan_R/0/1/0/all/0/1">Ratnesh Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cladera_F/0/1/0/all/0/1">Fernando Cladera</a>, <a href="http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1">Sai Vemprala</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1">Rogerio Bonatti</a>, <a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1">Kostas Daniilidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1">Ashish Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vijay Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1">Nikolai Matni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1">Jayesh K. Gupta</a></p>
<p>We present EvDNeRF, a pipeline for generating event data and training an
event-based dynamic NeRF, for the purpose of faithfully reconstructing
eventstreams on scenes with rigid and non-rigid deformations that may be too
fast to capture with a standard camera. Event cameras register asynchronous
per-pixel brightness changes at MHz rates with high dynamic range, making them
ideal for observing fast motion with almost no motion blur. Neural radiance
fields (NeRFs) offer visual-quality geometric-based learnable rendering, but
prior work with events has only considered reconstruction of static scenes. Our
EvDNeRF can predict eventstreams of dynamic scenes from a static or moving
viewpoint between any desired timestamps, thereby allowing it to be used as an
event-based simulator for a given scene. We show that by training on varied
batch sizes of events, we can improve test-time predictions of events at fine
time resolutions, outperforming baselines that pair standard dynamic NeRFs with
event generators. We release our simulated and real datasets, as well as code
for multi-view event-based data generation and the training and evaluation of
EvDNeRF models (https://github.com/anish-bhattacharya/EvDNeRF).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06641">How (not) to ensemble LVLMs for VQA. (arXiv:2310.06641v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alazraki_L/0/1/0/all/0/1">Lisa Alazraki</a>, <a href="http://arxiv.org/find/cs/1/au:+Castrejon_L/0/1/0/all/0/1">Lluis Castrejon</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mostafa Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Huot_F/0/1/0/all/0/1">Fantine Huot</a>, <a href="http://arxiv.org/find/cs/1/au:+Uijlings_J/0/1/0/all/0/1">Jasper Uijlings</a>, <a href="http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1">Thomas Mensink</a></p>
<p>This paper studies ensembling in the era of Large Vision-Language Models
(LVLMs). Ensembling is a classical method to combine different models to get
increased performance. In the recent work on Encyclopedic-VQA the authors
examine a wide variety of models to solve their task: from vanilla LVLMs, to
models including the caption as extra context, to models augmented with
Lens-based retrieval of Wikipedia pages. Intuitively these models are highly
complementary, which should make them ideal for ensembling. Indeed, an oracle
experiment shows potential gains from 48.8% accuracy (the best single model)
all the way up to 67% (best possible ensemble). So it is a trivial exercise to
create an ensemble with substantial real gains. Or is it?
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08528">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering. (arXiv:2310.08528v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Guanjun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1">Taoran Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jiemin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lingxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaopeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinggang Wang</a></p>
<p>Representing and rendering dynamic scenes has been an important but
challenging task. Especially, to accurately model complex motions, high
efficiency is usually hard to guarantee. To achieve real-time dynamic scene
rendering while also enjoying high training and storage efficiency, we propose
4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes
rather than applying 3D-GS for each individual frame. In 4D-GS, a novel
explicit representation containing both 3D Gaussians and 4D neural voxels is
proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is
proposed to efficiently build Gaussian features from 4D neural voxels and then
a lightweight MLP is applied to predict Gaussian deformations at novel
timestamps. Our 4D-GS method achieves real-time rendering under high
resolutions, 82 FPS at an 800$\times$800 resolution on an RTX 3090 GPU while
maintaining comparable or better quality than previous state-of-the-art
methods. More demos and code are available at
https://guanjunwu.github.io/4dgs/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13255">Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds. (arXiv:2310.13255v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Sipeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiazheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yicheng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zongqing Lu</a></p>
<p>Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to "a blindfolded text-based game."
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13355">SILC: Improving Vision Language Pretraining with Self-Distillation. (arXiv:2310.13355v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naeem_M/0/1/0/all/0/1">Muhammad Ferjad Naeem</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1">Yongqin Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaohua Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoyer_L/0/1/0/all/0/1">Lukas Hoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>, <a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1">Federico Tombari</a></p>
<p>Image-Text pretraining on web-scale image caption datasets has become the
default recipe for open vocabulary classification and retrieval models thanks
to the success of CLIP and its variants. Several works have also used CLIP
features for dense prediction tasks and have shown the emergence of open-set
abilities. However, the contrastive objective used by these models only focuses
on image-text alignment and does not incentivise image feature learning for
dense prediction tasks. In this work, we introduce SILC, a novel framework for
vision language pretraining. SILC improves image-text contrastive learning with
the simple addition of local-to-global correspondence learning by
self-distillation. We show that distilling local image features from an
exponential moving average (EMA) teacher model significantly improves model
performance on dense predictions tasks like detection and segmentation, while
also providing improvements on image-level tasks such as classification and
retrieval. SILC models sets a new state of the art for zero-shot
classification, few shot classification, image and text retrieval, zero-shot
segmentation, and open vocabulary segmentation. We further show that SILC
features greatly benefit open vocabulary detection, captioning and visual
question answering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14702">BM2CP: Efficient Collaborative Perception with LiDAR-Camera Modalities. (arXiv:2310.14702v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Binyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zhaonian Zou</a></p>
<p>Collaborative perception enables agents to share complementary perceptual
information with nearby agents. This would improve the perception performance
and alleviate the issues of single-view perception, such as occlusion and
sparsity. Most existing approaches mainly focus on single modality (especially
LiDAR), and not fully exploit the superiority of multi-modal perception. We
propose a collaborative perception paradigm, BM2CP, which employs LiDAR and
camera to achieve efficient multi-modal perception. It utilizes LiDAR-guided
modal fusion, cooperative depth generation and modality-guided intermediate
fusion to acquire deep interactions among modalities of different agents,
Moreover, it is capable to cope with the special case where one of the sensors,
same or different type, of any agent is missing. Extensive experiments validate
that our approach outperforms the state-of-the-art methods with 50X lower
communication volumes in both simulated and real-world autonomous driving
scenarios. Our code is available at https://github.com/byzhaoAI/BM2CP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19784">CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models. (arXiv:2310.19784v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Ziyang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1">Mingdeng Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhongang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Incorporating a customized object into image generation presents an
attractive feature in text-to-image generation. However, existing
optimization-based and encoder-based methods are hindered by drawbacks such as
time-consuming optimization, insufficient identity preservation, and a
prevalent copy-pasting effect. To overcome these limitations, we introduce
CustomNet, a novel object customization approach that explicitly incorporates
3D novel view synthesis capabilities into the object customization process.
This integration facilitates the adjustment of spatial position relationships
and viewpoints, yielding diverse outputs while effectively preserving object
identity. Moreover, we introduce delicate designs to enable location control
and flexible background control through textual descriptions or specific
user-defined images, overcoming the limitations of existing 3D novel view
synthesis methods. We further leverage a dataset construction pipeline that can
better handle real-world objects and complex backgrounds. Equipped with these
designs, our method facilitates zero-shot object customization without
test-time optimization, offering simultaneous control over the viewpoints,
location, and background. As a result, our CustomNet ensures enhanced identity
preservation and generates diverse, harmonious outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00979">Overhead Line Defect Recognition Based on Unsupervised Semantic Segmentation. (arXiv:2311.00979v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weixi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1">Xichen Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xun Ma</a></p>
<p>Overhead line inspection greatly benefits from defect recognition using
visible light imagery. Addressing the limitations of existing feature
extraction techniques and the heavy data dependency of deep learning
approaches, this paper introduces a novel defect recognition framework. This is
built on the Faster RCNN network and complemented by unsupervised semantic
segmentation. The approach involves identifying the type and location of the
target equipment, utilizing semantic segmentation to differentiate between the
device and its backdrop, and finally employing similarity measures and logical
rules to categorize the type of defect. Experimental results indicate that this
methodology focuses more on the equipment rather than the defects when
identifying issues in overhead lines. This leads to a notable enhancement in
accuracy and exhibits impressive adaptability. Thus, offering a fresh
perspective for automating the inspection of distribution network equipment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02183">A New Fine-grained Alignment Method for Image-text Matching. (arXiv:2311.02183v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a></p>
<p>Image-text retrieval is a widely studied topic in the field of computer
vision due to the exponential growth of multimedia data, whose core concept is
to measure the similarity between images and text. However, most existing
retrieval methods heavily rely on cross-attention mechanisms for cross-modal
fine-grained alignment, which takes into account excessive irrelevant regions
and treats prominent and non-significant words equally, thereby limiting
retrieval accuracy. This paper aims to investigate an alignment approach that
reduces the involvement of non-significant fragments in images and text while
enhancing the alignment of prominent segments. For this purpose, we introduce
the Cross-Modal Prominent Fragments Enhancement Aligning Network(CPFEAN), which
achieves improved retrieval accuracy by diminishing the participation of
irrelevant regions during alignment and relatively increasing the alignment
similarity of prominent words. Additionally, we incorporate prior textual
information into image regions to reduce misalignment occurrences. In practice,
we first design a novel intra-modal fragments relationship reasoning method,
and subsequently employ our proposed alignment mechanism to compute the
similarity between images and text. Extensive quantitative comparative
experiments on MS-COCO and Flickr30K datasets demonstrate that our approach
outperforms state-of-the-art methods by about 5% to 10% in the rSum metric.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03518">High-resolution power equipment recognition based on improved self-attention. (arXiv:2311.03518v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Siyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xin Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhen Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xun Ma</a></p>
<p>The current trend of automating inspections at substations has sparked a
surge in interest in the field of transformer image recognition. However, due
to restrictions in the number of parameters in existing models, high-resolution
images can't be directly applied, leaving significant room for enhancing
recognition accuracy. Addressing this challenge, the paper introduces a novel
improvement on deep self-attention networks tailored for this issue. The
proposed model comprises four key components: a foundational network, a region
proposal network, a module for extracting and segmenting target areas, and a
final prediction network. The innovative approach of this paper differentiates
itself by decoupling the processes of part localization and recognition,
initially using low-resolution images for localization followed by
high-resolution images for recognition. Moreover, the deep self-attention
network's prediction mechanism uniquely incorporates the semantic context of
images, resulting in substantially improved recognition performance.
Comparative experiments validate that this method outperforms the two other
prevalent target recognition models, offering a groundbreaking perspective for
automating electrical equipment inspections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04464">Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning. (arXiv:2311.04464v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuefeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1">Xiaofeng Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xiu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhigang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+lu_W/0/1/0/all/0/1">Wang lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1">Xiangyang Ji</a></p>
<p>Learning generalized representations from limited training samples is crucial
for applying deep neural networks in low-resource scenarios. Recently, methods
based on Contrastive Language-Image Pre-training (CLIP) have exhibited
promising performance in few-shot adaptation tasks. To avoid catastrophic
forgetting and overfitting caused by few-shot fine-tuning, existing works
usually freeze the parameters of CLIP pre-trained on large-scale datasets,
overlooking the possibility that some parameters might not be suitable for
downstream tasks. To this end, we revisit CLIP's visual encoder with a specific
focus on its distinctive attention pooling layer, which performs a spatial
weighted-sum of the dense feature maps. Given that dense feature maps contain
meaningful semantic information, and different semantics hold varying
importance for diverse downstream tasks (such as prioritizing semantics like
ears and eyes in pet classification tasks rather than side mirrors), using the
same weighted-sum operation for dense features across different few-shot tasks
might not be appropriate. Hence, we propose fine-tuning the parameters of the
attention pooling layer during the training process to encourage the model to
focus on task-specific semantics. In the inference process, we perform residual
blending between the features pooled by the fine-tuned and the original
attention pooling layers to incorporate both the few-shot knowledge and the
pre-trained CLIP's prior knowledge. We term this method as Semantic-Aware
FinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot
CLIP and is compatible with the existing adapter approach (termed SAFE-A).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07956">Robust Learning Based Condition Diagnosis Method for Distribution Network Switchgear. (arXiv:2311.07956v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1">Wenxi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zhe Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1">Weixi Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1">Weisi Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xinyi Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a></p>
<p>This paper introduces a robust, learning-based method for diagnosing the
state of distribution network switchgear, which is crucial for maintaining the
power quality for end users. Traditional diagnostic models often rely heavily
on expert knowledge and lack robustness. To address this, our method
incorporates an expanded feature vector that includes environmental data,
temperature readings, switch position, motor operation, insulation conditions,
and local discharge information. We tackle the issue of high dimensionality
through feature mapping. The method introduces a decision radius to categorize
unlabeled samples and updates the model parameters using a combination of
supervised and unsupervised loss, along with a consistency regularization
function. This approach ensures robust learning even with a limited number of
labeled samples. Comparative analysis demonstrates that this method
significantly outperforms existing models in both accuracy and robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09257">UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. (arXiv:2311.09257v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zhisheng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1">Tingbo Hou</a></p>
<p>Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11642">Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging. (arXiv:2311.11642v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muqeet_A/0/1/0/all/0/1">Abdul Muqeet</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyuchul Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Bumsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yohan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungrae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woonggon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">KwangHee Lee</a></p>
<p>Video face re-aging deals with altering the apparent age of a person to the
target age in videos. This problem is challenging due to the lack of paired
video datasets maintaining temporal consistency in identity and age. Most
re-aging methods process each image individually without considering the
temporal consistency of videos. While some existing works address the issue of
temporal coherence through video facial attribute manipulation in latent space,
they often fail to deliver satisfactory performance in age transformation. To
tackle the issues, we propose (1) a novel synthetic video dataset that features
subjects across a diverse range of age groups; (2) a baseline architecture
designed to validate the effectiveness of our proposed dataset, and (3) the
development of three novel metrics tailored explicitly for evaluating the
temporal consistency of video re-aging techniques. Our comprehensive
experiments on public datasets, such as VFHQ and CelebV-HQ, show that our
method outperforms the existing approaches in terms of both age transformation
and temporal consistency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14388">A Parameterized Generative Adversarial Network Using Cyclic Projection for Explainable Medical Image Classification. (arXiv:2311.14388v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1">Xiangyu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yue Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaohong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_C/0/1/0/all/0/1">Chan-Tong Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_T/0/1/0/all/0/1">Tong Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qinquan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1">Wei Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tao Tan</a></p>
<p>Although current data augmentation methods are successful to alleviate the
data insufficiency, conventional augmentation are primarily intra-domain while
advanced generative adversarial networks (GANs) generate images remaining
uncertain, particularly in small-scale datasets. In this paper, we propose a
parameterized GAN (ParaGAN) that effectively controls the changes of synthetic
samples among domains and highlights the attention regions for downstream
classification. Specifically, ParaGAN incorporates projection distance
parameters in cyclic projection and projects the source images to the decision
boundary to obtain the class-difference maps. Our experiments show that ParaGAN
can consistently outperform the existing augmentation methods with explainable
classification on two small-scale medical datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16447">TopoSemiSeg: Enforcing Topological Consistency for Semi-Supervised Segmentation of Histopathology Images. (arXiv:2311.16447v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1">Meilong Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1">Xiaoling Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1">Saumya Gupta</a>, <a href="http://arxiv.org/find/eess/1/au:+Abousamra_S/0/1/0/all/0/1">Shahira Abousamra</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a></p>
<p>In computational pathology, segmenting densely distributed objects like
glands and nuclei is crucial for downstream analysis. To alleviate the burden
of obtaining pixel-wise annotations, semi-supervised learning methods learn
from large amounts of unlabeled data. Nevertheless, existing semi-supervised
methods overlook the topological information hidden in the unlabeled images and
are thus prone to topological errors, e.g., missing or incorrectly
merged/separated glands or nuclei. To address this issue, we propose
TopoSemiSeg, the first semi-supervised method that learns the topological
representation from unlabeled data. In particular, we propose a topology-aware
teacher-student approach in which the teacher and student networks learn shared
topological representations. To achieve this, we introduce topological
consistency loss, which contains signal consistency and noise removal losses to
ensure the learned representation is robust and focuses on true topological
signals. Extensive experiments on public pathology image datasets show the
superiority of our method, especially on topology-wise evaluation metrics. Code
is available at https://github.com/Melon-Xu/TopoSemiSeg.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17663">Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications. (arXiv:2311.17663v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Junyi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xieyuanli Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiawei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingyi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jintao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1">Weihao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_R/0/1/0/all/0/1">Rui Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hesheng Wang</a></p>
<p>Understanding how the surrounding environment changes is crucial for
performing downstream tasks safely and reliably in autonomous driving
applications. Recent occupancy estimation techniques using only camera images
as input can provide dense occupancy representations of large-scale scenes
based on the current observation. However, they are mostly limited to
representing the current 3D space and do not consider the future state of
surrounding objects along the time axis. To extend camera-only occupancy
estimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark
for camera-only 4D occupancy forecasting, evaluating the surrounding scene
changes in a near future. We build our benchmark based on multiple publicly
available datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5,
which provides sequential occupancy states of general movable and static
objects, as well as their 3D backward centripetal flow. To establish this
benchmark for future research with comprehensive comparisons, we introduce four
baseline types from diverse camera-based perception and prediction
implementations, including a static-world occupancy model, voxelization of
point cloud prediction, 2D-3D instance-based prediction, and our proposed novel
end-to-end 4D occupancy forecasting network. Furthermore, the standardized
evaluation protocol for preset multiple tasks is also provided to compare the
performance of all the proposed baselines on present and future occupancy
estimation with respect to objects of interest in autonomous driving scenarios.
The dataset and our implementation of all four baselines in the proposed
Cam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00065">Unsupervised Keypoints from Pretrained Diffusion Models. (arXiv:2312.00065v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hedlin_E/0/1/0/all/0/1">Eric Hedlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1">Gopal Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1">Shweta Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xingzhe He</a>, <a href="http://arxiv.org/find/cs/1/au:+Isack_H/0/1/0/all/0/1">Hossam Isack</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhodin_A/0/1/0/all/0/1">Abhishek Kar Helge Rhodin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1">Andrea Tagliasacchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kwang Moo Yi</a></p>
<p>Unsupervised learning of keypoints and landmarks has seen significant
progress with the help of modern neural network architectures, but performance
is yet to match the supervised counterpart, making their practicability
questionable. We leverage the emergent knowledge within text-to-image diffusion
models, towards more robust unsupervised keypoints. Our core idea is to find
text embeddings that would cause the generative model to consistently attend to
compact regions in images (i.e. keypoints). To do so, we simply optimize the
text embedding such that the cross-attention maps within the denoising network
are localized as Gaussians with small standard deviations. We validate our
performance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,
DeepFashion, and Human3.6m datasets. We achieve significantly improved
accuracy, sometimes even outperforming supervised ones, particularly for data
that is non-aligned and less curated. Our code is publicly available and can be
found through our project page: https://ubc-vision.github.io/StableKeypoints/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00351">Manipulating the Label Space for In-Context Classification. (arXiv:2312.00351v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haokun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuhang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zihan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a></p>
<p>After pre-training by generating the next word conditional on previous words,
the Language Model (LM) acquires the ability of In-Context Learning (ICL) that
can learn a new task conditional on the context of the given in-context
examples (ICEs). Similarly, visually-conditioned Language Modelling is also
used to train Vision-Language Models (VLMs) with ICL ability. However, such
VLMs typically exhibit weaker classification abilities compared to contrastive
learning-based models like CLIP, since the Language Modelling objective does
not directly contrast whether an object is paired with a text. To improve the
ICL of classification, using more ICEs to provide more knowledge is a
straightforward way. However, this may largely increase the selection time, and
more importantly, the inclusion of additional in-context images tends to extend
the length of the in-context sequence beyond the processing capacity of a VLM.
To alleviate these limitations, we propose to manipulate the label space of
each ICE to increase its knowledge density, allowing for fewer ICEs to convey
as much information as a larger set would. Specifically, we propose two
strategies which are Label Distribution Enhancement and Visual Descriptions
Enhancement to improve In-context classification performance on diverse
datasets, including the classic ImageNet and more fine-grained datasets like
CUB-200. Specifically, using our approach on ImageNet, we increase accuracy
from 74.70\% in a 4-shot setting to 76.21\% with just 2 shots. surpassing CLIP
by 0.67\%. On CUB-200, our method raises 1-shot accuracy from 48.86\% to
69.05\%, 12.15\% higher than CLIP. The code is given in
https://anonymous.4open.science/r/MLS_ICC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00690">Open-vocabulary object 6D pose estimation. (arXiv:2312.00690v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Corsetti_J/0/1/0/all/0/1">Jaime Corsetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1">Davide Boscaini</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1">Changjae Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1">Andrea Cavallaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1">Fabio Poiesi</a></p>
<p>We introduce the new setting of open-vocabulary object 6D pose estimation, in
which a textual prompt is used to specify the object of interest. In contrast
to existing approaches, in our setting (i) the object of interest is specified
solely through the textual prompt, (ii) no object model (e.g. CAD or video
sequence) is required at inference, (iii) the object is imaged from two
different viewpoints of two different scenes, and (iv) the object was not
observed during the training phase. To operate in this setting, we introduce a
novel approach that leverages a Vision-Language Model to segment the object of
interest from two distinct scenes and to estimate its relative 6D pose. The key
of our approach is a carefully devised strategy to fuse object-level
information provided by the prompt with local image features, resulting in a
feature space that can generalize to novel concepts. We validate our approach
on a new benchmark based on two popular datasets, REAL275 and Toyota-Light,
which collectively encompass 39 object instances appearing in four thousand
image pairs. The results demonstrate that our approach outperforms both a
well-established hand-crafted method and a recent deep learning-based baseline
in estimating the relative 6D pose of objects in different scenes. Project
page: https://jcorsetti.github.io/oryon/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00786">Dense Optical Tracking: Connecting the Dots. (arXiv:2312.00786v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1">Guillaume Le Moing</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1">Jean Ponce</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a></p>
<p>Recent approaches to point tracking are able to recover the trajectory of any
scene point through a large portion of a video despite the presence of
occlusions. They are, however, too slow in practice to track every point
observed in a single frame in a reasonable amount of time. This paper
introduces DOT, a novel, simple and efficient method for solving this problem.
It first extracts a small set of tracks from key regions at motion boundaries
using an off-the-shelf point tracking algorithm. Given source and target
frames, DOT then computes rough initial estimates of a dense flow field and
visibility mask through nearest-neighbor interpolation, before refining them
using a learnable optical flow estimator that explicitly handles occlusions and
can be trained on synthetic data with ground-truth correspondences. We show
that DOT is significantly more accurate than current optical flow techniques,
outperforms sophisticated "universal" trackers like OmniMotion, and is on par
with, or better than, the best point tracking algorithms like CoTracker while
being at least two orders of magnitude faster. Quantitative and qualitative
experiments with synthetic and real videos validate the promise of the proposed
approach. Code, data, and videos showcasing the capabilities of our approach
are available in the project webpage: https://16lemoing.github.io/dot .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01623">Universal Segmentation at Arbitrary Granularity with Language Instruction. (arXiv:2312.01623v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cairong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yitong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiahao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yujiu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yansong Tang</a></p>
<p>This paper aims to achieve universal segmentation of arbitrary semantic
level. Despite significant progress in recent years, specialist segmentation
approaches are limited to specific tasks and data distribution. Retraining a
new model for adaptation to new scenarios or settings takes expensive
computation and time cost, which raises the demand for versatile and universal
segmentation model that can cater to various granularity. Although some
attempts have been made for unifying different segmentation tasks or
generalization to various scenarios, limitations in the definition of paradigms
and input-output spaces make it difficult for them to achieve accurate
understanding of content at arbitrary granularity. To this end, we present
UniLSeg, a universal segmentation model that can perform segmentation at any
semantic level with the guidance of language instructions. For training
UniLSeg, we reorganize a group of tasks from original diverse distributions
into a unified data format, where images with texts describing segmentation
targets as input and corresponding masks are output. Combined with a automatic
annotation engine for utilizing numerous unlabeled data, UniLSeg achieves
excellent performance on various tasks and settings, surpassing both specialist
and unified segmentation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01841">VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior. (arXiv:2312.01841v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xusen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Longhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1">Xinya Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kangneng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1">Daiheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1">Liefeng Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xun Cao</a></p>
<p>Audio-driven talking head generation has drawn much attention in recent
years, and many efforts have been made in lip-sync, expressive facial
expressions, natural head pose generation, and high video quality. However, no
model has yet led or tied on all these metrics due to the one-to-many mapping
between audio and motion. In this paper, we propose VividTalk, a two-stage
generic framework that supports generating high-visual quality talking head
videos with all the above properties. Specifically, in the first stage, we map
the audio to mesh by learning two motions, including non-rigid expression
motion and rigid head motion. For expression motion, both blendshape and vertex
are adopted as the intermediate representation to maximize the representation
ability of the model. For natural head motion, a novel learnable head pose
codebook with a two-phase training mechanism is proposed. In the second stage,
we proposed a dual branch motion-vae and a generator to transform the meshes
into dense motion and synthesize high-quality video frame-by-frame. Extensive
experiments show that the proposed VividTalk can generate high-visual quality
talking head videos with lip-sync and realistic enhanced by a large margin, and
outperforms previous state-of-the-art works in objective and subjective
comparisons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01860">Unveiling Objects with SOLA: An Annotation-Free Image Search on the Object Level for Automotive Data Sets. (arXiv:2312.01860v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rigoll_P/0/1/0/all/0/1">Philipp Rigoll</a>, <a href="http://arxiv.org/find/cs/1/au:+Langner_J/0/1/0/all/0/1">Jacob Langner</a>, <a href="http://arxiv.org/find/cs/1/au:+Sax_E/0/1/0/all/0/1">Eric Sax</a></p>
<p>Huge image data sets are the fundament for the development of the perception
of automated driving systems. A large number of images is necessary to train
robust neural networks that can cope with diverse situations. A sufficiently
large data set contains challenging situations and objects. For testing the
resulting functions, it is necessary that these situations and objects can be
found and extracted from the data set. While it is relatively easy to record a
large amount of unlabeled data, it is far more difficult to find demanding
situations and objects. However, during the development of perception systems,
it must be possible to access challenging data without having to perform
lengthy and time-consuming annotations. A developer must therefore be able to
search dynamically for specific situations and objects in a data set. Thus, we
designed a method which is based on state-of-the-art neural networks to search
for objects with certain properties within an image. For the ease of use, the
query of this search is described using natural language. To determine the time
savings and performance gains, we evaluated our method qualitatively and
quantitatively on automotive data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02190">Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D. (arXiv:2312.02190v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1">Karran Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1">Paul Guerrero</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1">Matheus Gadelha</a>, <a href="http://arxiv.org/find/cs/1/au:+Hold_Geoffroy_Y/0/1/0/all/0/1">Yannick Hold-Geoffroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1">Karan Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1">Niloy Mitra</a></p>
<p>Diffusion Handles is a novel approach to enabling 3D object edits on
diffusion images. We accomplish these edits using existing pre-trained
diffusion models, and 2D image depth estimation, without any fine-tuning or 3D
object retrieval. The edited results remain plausible, photo-real, and preserve
object identity. Diffusion Handles address a critically missing facet of
generative image based creative design, and significantly advance the
state-of-the-art in generative image editing. Our key insight is to lift
diffusion activations for an object to 3D using a proxy depth, 3D-transform the
depth and associated activations, and project them back to image space. The
diffusion process applied to the manipulated activations with identity control,
produces plausible edited images showing complex 3D occlusion and lighting
effects. We evaluate Diffusion Handles: quantitatively, on a large synthetic
data benchmark; and qualitatively by a user study, showing our output to be
more plausible, and better than prior art at both, 3D editing and identity
control. Project Webpage: https://diffusionhandles.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02209">AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing. (arXiv:2312.02209v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaosheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Si Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a></p>
<p>Editable 3D-aware generation, which supports user-interacted editing, has
witnessed rapid development recently. However, existing editable 3D GANs either
fail to achieve high-accuracy local editing or suffer from huge computational
costs. We propose AttriHuman-3D, an editable 3D human generation model, which
address the aforementioned problems with attribute decomposition and indexing.
The core idea of the proposed model is to generate all attributes (e.g. human
body, hair, clothes and so on) in an overall attribute space with six feature
planes, which are then decomposed and manipulated with different attribute
indexes. To precisely extract features of different attributes from the
generated feature planes, we propose a novel attribute indexing method as well
as an orthogonal projection regularization to enhance the disentanglement. We
also introduce a hyper-latent training strategy and an attribute-specific
sampling strategy to avoid style entanglement and misleading punishment from
the discriminator. Our method allows users to interactively edit selected
attributes in the generated 3D human avatars while keeping others fixed. Both
qualitative and quantitative experiments demonstrate that our model provides a
strong disentanglement between different attributes, allows fine-grained image
editing and generates high-quality 3D human avatars.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02238">X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model. (arXiv:2312.02238v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ran_L/0/1/0/all/0/1">Lingmin Ran</a>, <a href="http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1">Xiaodong Cun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jia-Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zijie_S/0/1/0/all/0/1">Song Zijie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Keppo_J/0/1/0/all/0/1">Jussi Keppo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a></p>
<p>We introduce X-Adapter, a universal upgrader to enable the pretrained
plug-and-play modules (e.g., ControlNet, LoRA) to work directly with the
upgraded text-to-image diffusion model (e.g., SDXL) without further retraining.
We achieve this goal by training an additional network to control the frozen
upgraded model with the new text-image data pairs. In detail, X-Adapter keeps a
frozen copy of the old model to preserve the connectors of different plugins.
Additionally, X-Adapter adds trainable mapping layers that bridge the decoders
from models of different versions for feature remapping. The remapped features
will be used as guidance for the upgraded model. To enhance the guidance
ability of X-Adapter, we employ a null-text training strategy for the upgraded
model. After training, we also introduce a two-stage denoising strategy to
align the initial latents of X-Adapter and the upgraded model. Thanks to our
strategies, X-Adapter demonstrates universal compatibility with various plugins
and also enables plugins of different versions to work together, thereby
expanding the functionalities of diffusion community. To verify the
effectiveness of the proposed method, we conduct extensive experiments and the
results show that X-Adapter may facilitate wider application in the upgraded
foundational diffusion model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02914">Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training. (arXiv:2312.02914v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1">Arun Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1">William Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1">Corban Rivera</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1">Ketul Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Melo_C/0/1/0/all/0/1">Celso M. de Melo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1">Rama Chellappa</a></p>
<p>In this work, we tackle the problem of unsupervised domain adaptation (UDA)
for video action recognition. Our approach, which we call UNITE, uses an image
teacher model to adapt a video student model to the target domain. UNITE first
employs self-supervised pre-training to promote discriminative feature learning
on target domain videos using a teacher-guided masked distillation objective.
We then perform self-training on masked target data, using the video student
model and image teacher model together to generate improved pseudolabels for
unlabeled target videos. Our self-training process successfully leverages the
strengths of both models to achieve strong transfer performance across domains.
We evaluate our approach on multiple video domain adaptation benchmarks and
observe significant improvements upon previously reported results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03289">Class Incremental Learning for Adversarial Robustness. (arXiv:2312.03289v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Seungju Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hongsin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changick Kim</a></p>
<p>Adversarial training integrates adversarial examples during model training to
enhance robustness. However, its application in fixed dataset settings differs
from real-world dynamics, where data accumulates incrementally. In this study,
we investigate Adversarially Robust Class Incremental Learning (ARCIL), a
method that combines adversarial robustness with incremental learning. We
observe that combining incremental learning with naive adversarial training
easily leads to a loss of robustness. We discover that this is attributed to
the disappearance of the flatness of the loss function, a characteristic of
adversarial training. To address this issue, we propose the Flatness Preserving
Distillation (FPD) loss that leverages the output difference between
adversarial and clean examples. Additionally, we introduce the Logit Adjustment
Distillation (LAD) loss, which adapts the model's knowledge to perform well on
new tasks. Experimental results demonstrate the superiority of our method over
approaches that apply adversarial training to existing incremental learning
methods, which provides a strong baseline for incremental learning on
adversarial robustness in the future. Our method achieves AutoAttack accuracy
that is 5.99\%p, 5.27\%p, and 3.90\%p higher on average than the baseline on
split CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively. The code will be
made available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03406">SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting. (arXiv:2312.03406v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yanjun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Liang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Rong Jin</a></p>
<p>Spatiotemporal forecasting tasks, such as weather forecasting and traffic
prediction, offer significant societal benefits. These tasks can be effectively
approached as image forecasting problems using computer vision models. Vector
quantization (VQ) is a well-known method for discrete representation that
improves the latent space, leading to enhanced generalization and transfer
learning capabilities. One of the main challenges in using VQ for
spatiotemporal forecasting is how to balance between keeping enough details and
removing noises from the original patterns for better generalization. We
address this challenge by developing sparse vector quantization, or {\bf SVQ}
for short, that leverages sparse regression to make better trade-off between
the two objectives. The main innovation of this work is to approximate sparse
regression by a two-layer MLP and a randomly fixed or learnable matrix,
dramatically improving its computational efficiency. Through experiments
conducted on diverse datasets in multiple fields including weather forecasting,
traffic flow prediction, and video forecasting, we unequivocally demonstrate
that our proposed method consistently enhances the performance of base models
and achieves state-of-the-art results across all benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03461">HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting. (arXiv:2312.03461v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuheng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhehao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Penghao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhuo Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yu Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jingyi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lan Xu</a></p>
<p>We have recently seen tremendous progress in photo-real human modeling and
rendering. Yet, efficiently rendering realistic human performance and
integrating it into the rasterization pipeline remains challenging. In this
paper, we present HiFi4G, an explicit and compact Gaussian-based approach for
high-fidelity human performance rendering from dense footage. Our core
intuition is to marry the 3D Gaussian representation with non-rigid tracking,
achieving a compact and compression-friendly representation. We first propose a
dual-graph mechanism to obtain motion priors, with a coarse deformation graph
for effective initialization and a fine-grained Gaussian graph to enforce
subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with
adaptive spatial-temporal regularizers to effectively balance the non-rigid
prior and Gaussian updating. We also present a companion compression scheme
with residual compensation for immersive experiences on various platforms. It
achieves a substantial compression rate of approximately 25 times, with less
than 2MB of storage per frame. Extensive experiments demonstrate the
effectiveness of our approach, which significantly outperforms existing
approaches in terms of optimization speed, rendering quality, and storage
overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03594">A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting. (arXiv:2312.03594v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1">Junhao Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yanhong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenran Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a></p>
<p>Achieving high-quality versatile image inpainting, where user-specified
regions are filled with plausible content according to user intent, presents a
significant challenge. Existing methods face difficulties in simultaneously
addressing context-aware image inpainting and text-guided object inpainting due
to the distinct optimal training strategies required. To overcome this
challenge, we introduce PowerPaint, the first high-quality and versatile
inpainting model that excels in both tasks. First, we introduce learnable task
prompts along with tailored fine-tuning strategies to guide the model's focus
on different inpainting targets explicitly. This enables PowerPaint to
accomplish various inpainting tasks by utilizing different task prompts,
resulting in state-of-the-art performance. Second, we demonstrate the
versatility of the task prompt in PowerPaint by showcasing its effectiveness as
a negative prompt for object removal. Additionally, we leverage prompt
interpolation techniques to enable controllable shape-guided object inpainting.
Finally, we extensively evaluate PowerPaint on various inpainting benchmarks to
demonstrate its superior performance for versatile image inpainting. We release
our codes and models on our project page: https://powerpaint.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03698">Intrinsic Harmonization for Illumination-Aware Compositing. (arXiv:2312.03698v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Careaga_C/0/1/0/all/0/1">Chris Careaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Miangoleh_S/0/1/0/all/0/1">S. Mahdi H. Miangoleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksoy_Y/0/1/0/all/0/1">Ya&#x11f;&#x131;z Aksoy</a></p>
<p>Despite significant advancements in network-based image harmonization
techniques, there still exists a domain disparity between typical training
pairs and real-world composites encountered during inference. Most existing
methods are trained to reverse global edits made on segmented image regions,
which fail to accurately capture the lighting inconsistencies between the
foreground and background found in composited images. In this work, we
introduce a self-supervised illumination harmonization approach formulated in
the intrinsic image domain. First, we estimate a simple global lighting model
from mid-level vision representations to generate a rough shading for the
foreground region. A network then refines this inferred shading to generate a
harmonious re-shading that aligns with the background scene. In order to match
the color appearance of the foreground and background, we utilize ideas from
prior harmonization approaches to perform parameterized image edits in the
albedo domain. To validate the effectiveness of our approach, we present
results from challenging real-world composites and conduct a user study to
objectively measure the enhanced realism achieved compared to state-of-the-art
harmonization methods.
</p>
</p>
</div>

    </div>
    </body>
    