<!DOCTYPE html>
<html>
<head>
<title>2024-01-06-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.01943">Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Excoffier_J/0/1/0/all/0/1">Jean-Baptiste Excoffier</a>, <a href="http://arxiv.org/find/cs/1/au:+Roehr_T/0/1/0/all/0/1">Tom Roehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Figueroa_A/0/1/0/all/0/1">Alexei Figueroa</a>, <a href="http://arxiv.org/find/cs/1/au:+Papaaioannou_M/0/1/0/all/0/1">Michalis Papaaioannou</a>, <a href="http://arxiv.org/find/cs/1/au:+Bressem_K/0/1/0/all/0/1">Keno Bressem</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortala_M/0/1/0/all/0/1">Matthieu Ortala</a></p>
<p>The increasing use of tools and solutions based on Large Language Models
(LLMs) for various tasks in the medical domain has become a prominent trend.
Their use in this highly critical and sensitive domain has thus raised
important questions about their robustness, especially in response to
variations in input, and the reliability of the generated outputs. This study
addresses these questions by constructing a textual dataset based on the
ICD-10-CM code descriptions, widely used in US hospitals and containing many
clinical terms, and their easily reproducible rephrasing. We then benchmarked
existing embedding models, either generalist or specialized in the clinical
domain, in a semantic search task where the goal was to correctly match the
rephrased text to the original description. Our results showed that generalist
models performed better than clinical models, suggesting that existing clinical
specialized models are more sensitive to small changes in input that confuse
them. The highlighted problem of specialized models may be due to the fact that
they have not been trained on sufficient data, and in particular on datasets
that are not diverse enough to have a reliable global language understanding,
which is still necessary for accurate handling of medical documents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01951">Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1">Mehran Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1">Peyman Hosseini</a></p>
<p>The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01952">Instruct-Imagen: Image Generation with Multi-modal Instruction. (arXiv:2401.01952v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kelvin C.K. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yu-Chuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yandong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_X/0/1/0/all/0/1">Xue Ben</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Boqing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1">William Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Ming-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xuhui Jia</a></p>
<p>This paper presents instruct-imagen, a model that tackles heterogeneous image
generation tasks and generalizes across unseen tasks. We introduce *multi-modal
instruction* for image generation, a task representation articulating a range
of generation intents with precision. It uses natural language to amalgamate
disparate modalities (e.g., text, edge, style, subject, etc.), such that
abundant generation intents can be standardized in a uniform format.
</p>
<p>We then build instruct-imagen by fine-tuning a pre-trained text-to-image
diffusion model with a two-stage framework. First, we adapt the model using the
retrieval-augmented training, to enhance model's capabilities to ground its
generation on external multimodal context. Subsequently, we fine-tune the
adapted model on diverse image generation tasks that requires vision-language
understanding (e.g., subject-driven generation, etc.), each paired with a
multi-modal instruction encapsulating the task's essence. Human evaluation on
various image generation datasets reveals that instruct-imagen matches or
surpasses prior task-specific models in-domain and demonstrates promising
generalization to unseen and more complex tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01967">A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity. (arXiv:2401.01967v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1">Andrew Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiaoyan Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Pres_I/0/1/0/all/0/1">Itamar Pres</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1">Martin Wattenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1">Jonathan K. Kummerfeld</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1">Rada Mihalcea</a></p>
<p>While alignment algorithms are now commonly used to tune pre-trained language
models towards a user's preferences, we lack explanations for the underlying
mechanisms in which models become ``aligned'', thus making it difficult to
explain phenomena like jailbreaks. In this work we study a popular algorithm,
direct preference optimization (DPO), and the mechanisms by which it reduces
toxicity. Namely, we first study how toxicity is represented and elicited in a
pre-trained language model, GPT2-medium. We then apply DPO with a carefully
crafted pairwise dataset to reduce toxicity. We examine how the resulting model
averts toxic outputs, and find that capabilities learned from pre-training are
not removed, but rather bypassed. We use this insight to demonstrate a simple
method to un-align the model, reverting it back to its toxic behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01970">FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding. (arXiv:2401.01970v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1">Xingxing Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Samangouei_P/0/1/0/all/0/1">Pouya Samangouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yunwen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1">Yan Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingyang Li</a></p>
<p>Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present \algfull{} (\algname{}), which
incorporates vision-language embeddings of foundation models into 3D Gaussian
Splatting (GS). The key contribution of this work is an efficient method to
reconstruct and represent 3D vision-language models. This is achieved by
distilling feature maps generated from image-based foundation models into those
rendered from our 3D model. To ensure high-quality rendering and fast training,
we introduce a novel scene representation by integrating strengths from both GS
and multi-resolution hash encodings (MHE). Our effective training procedure
also introduces a pixel alignment loss that makes the rendered feature distance
of same semantic entities close, following the pixel-level semantic boundaries.
Our results demonstrate remarkable multi-view semantic consistency,
facilitating diverse downstream tasks, beating state-of-the-art methods by
$\mathbf{10.2}$ percent on open-vocabulary language-based object detection,
despite that we are $\mathbf{851\times}$ faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code upon paper acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01974">Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stanic_A/0/1/0/all/0/1">Aleksandar Stani&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Caelles_S/0/1/0/all/0/1">Sergi Caelles</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1">Michael Tschannen</a></p>
<p>Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01989">Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1">Anshuman Chhabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1">Hadi Askari</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1">Prasant Mohapatra</a></p>
<p>We characterize and study zero-shot abstractive summarization in Large
Language Models (LLMs) by measuring position bias, which we propose as a
general formulation of the more restrictive lead bias phenomenon studied
previously in the literature. Position bias captures the tendency of a model
unfairly prioritizing information from certain parts of the input text over
others, leading to undesirable behavior. Through numerous experiments on four
diverse real-world datasets, we study position bias in multiple LLM models such
as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained
encoder-decoder abstractive summarization models such as Pegasus and BART. Our
findings lead to novel insights and discussion on performance and position bias
of models for zero-shot summarization tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01990">GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feizi_A/0/1/0/all/0/1">Aarash Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1">Randall Balestriero</a>, <a href="http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1">Adriana Romero-Soriano</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1">Reihaneh Rabbany</a></p>
<p>We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01993">On Time-Indexing as Inductive Bias in Deep RL for Sequential Manipulation Tasks. (arXiv:2401.01993v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qureshi_M/0/1/0/all/0/1">M. Nomaan Qureshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1">Ben Eisner</a>, <a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1">David Held</a></p>
<p>While solving complex manipulation tasks, manipulation policies often need to
learn a set of diverse skills to accomplish these tasks. The set of skills is
often quite multimodal - each one may have a quite distinct distribution of
actions and states. Standard deep policy-learning algorithms often model
policies as deep neural networks with a single output head (deterministic or
stochastic). This structure requires the network to learn to switch between
modes internally, which can lead to lower sample efficiency and poor
performance. In this paper we explore a simple structure which is conducive to
skill learning required for so many of the manipulation tasks. Specifically, we
propose a policy architecture that sequentially executes different action heads
for fixed durations, enabling the learning of primitive skills such as reaching
and grasping. Our empirical evaluation on the Metaworld tasks reveals that this
simple structure outperforms standard policy learning methods, highlighting its
potential for improved skill acquisition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02009">Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yongliang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Linjuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1">Qiuying Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weiming Lu</a></p>
<p>The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02051">An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search. (arXiv:2401.02051v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1">Xialiang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1">Mingxuan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fu Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenkun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhichao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingfu Zhang</a></p>
<p>It is often very tedious for human experts to design efficient algorithms.
Recently, we have proposed a novel Algorithm Evolution using Large Language
Model (AEL) framework for automatic algorithm design. AEL combines the power of
a large language model and the paradigm of evolutionary computation to design,
combine, and modify algorithms automatically. In this paper, we use AEL to
design the guide algorithm for guided local search (GLS) to solve the
well-known traveling salesman problem (TSP). AEL automatically evolves elite
GLS algorithms in two days, with minimal human effort and no model training.
Experimental results on 1,000 TSP20-TSP100 instances and TSPLib instances show
that AEL-designed GLS outperforms state-of-the-art human-designed GLS with the
same iteration budget. It achieves a 0% gap on TSP20 and TSP50 and a 0.032% gap
on TSP100 in 1,000 iterations. Our findings mark the emergence of a new era in
automatic algorithm design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02092">k-Winners-Take-All Ensemble Neural Network. (arXiv:2401.02092v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1">Abien Fred Agarap</a>, <a href="http://arxiv.org/find/cs/1/au:+Azcarraga_A/0/1/0/all/0/1">Arnulfo P. Azcarraga</a></p>
<p>Ensembling is one approach that improves the performance of a neural network
by combining a number of independent neural networks, usually by either
averaging or summing up their individual outputs. We modify this ensembling
approach by training the sub-networks concurrently instead of independently.
This concurrent training of sub-networks leads them to cooperate with each
other, and we refer to them as "cooperative ensemble". Meanwhile, the
mixture-of-experts approach improves a neural network performance by dividing
up a given dataset to its sub-networks. It then uses a gating network that
assigns a specialization to each of its sub-networks called "experts". We
improve on these aforementioned ways for combining a group of neural networks
by using a k-Winners-Take-All (kWTA) activation function, that acts as the
combination method for the outputs of each sub-network in the ensemble. We
refer to this proposed model as "kWTA ensemble neural networks" (kWTA-ENN).
With the kWTA activation function, the losing neurons of the sub-networks are
inhibited while the winning neurons are retained. This results in sub-networks
having some form of specialization but also sharing knowledge with one another.
We compare our approach with the cooperative ensemble and mixture-of-experts,
where we used a feed-forward neural network with one hidden layer having 100
neurons as the sub-network architecture. Our approach yields a better
performance compared to the baseline models, reaching the following test
accuracies on benchmark datasets: 98.34% on MNIST, 88.06% on Fashion-MNIST,
91.56% on KMNIST, and 95.97% on WDBC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02117">Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation. (arXiv:2401.02117v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zipeng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tony Z. Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a></p>
<p>Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02124">ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach. (arXiv:2401.02124v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kilimci_Z/0/1/0/all/0/1">Zeynep Hilal Kilimci</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yalcin_M/0/1/0/all/0/1">Mustafa Yalcin</a></p>
<p>Anticancer peptides (ACPs) are a class of molecules that have gained
significant attention in the field of cancer research and therapy. ACPs are
short chains of amino acids, the building blocks of proteins, and they possess
the ability to selectively target and kill cancer cells. One of the key
advantages of ACPs is their ability to selectively target cancer cells while
sparing healthy cells to a greater extent. This selectivity is often attributed
to differences in the surface properties of cancer cells compared to normal
cells. That is why ACPs are being investigated as potential candidates for
cancer therapy. ACPs may be used alone or in combination with other treatment
modalities like chemotherapy and radiation therapy. While ACPs hold promise as
a novel approach to cancer treatment, there are challenges to overcome,
including optimizing their stability, improving selectivity, and enhancing
their delivery to cancer cells, continuous increasing in number of peptide
sequences, developing a reliable and precise prediction model. In this work, we
propose an efficient transformer-based framework to identify anticancer
peptides for by performing accurate a reliable and precise prediction model.
For this purpose, four different transformer models, namely ESM, ProtBert,
BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid
sequences. To demonstrate the contribution of the proposed framework, extensive
experiments are carried on widely-used datasets in the literature, two versions
of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of
proposed model enhances classification accuracy when compared to the
state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of
accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and
88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02132">DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models. (arXiv:2401.02132v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1">Wendi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuohang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Damien_L/0/1/0/all/0/1">Lopez Damien</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1">Kamalika Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Malin_B/0/1/0/all/0/1">Bradley Malin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sricharan Kumar</a></p>
<p>Evaluating the quality and variability of text generated by Large Language
Models (LLMs) poses a significant, yet unresolved research challenge.
Traditional evaluation methods, such as ROUGE and BERTScore, which measure
token similarity, often fail to capture the holistic semantic equivalence. This
results in a low correlation with human judgments and intuition, which is
especially problematic in high-stakes applications like healthcare and finance
where reliability, safety, and robust decision-making are highly critical. This
work proposes DCR, an automated framework for evaluating and improving the
consistency of LLM-generated texts using a divide-conquer-reasoning approach.
Unlike existing LLM-based evaluators that operate at the paragraph level, our
method employs a divide-and-conquer evaluator (DCE) that breaks down the
paragraph-to-paragraph comparison between two generated responses into
individual sentence-to-paragraph comparisons, each evaluated based on
predefined criteria. To facilitate this approach, we introduce an automatic
metric converter (AMC) that translates the output from DCE into an
interpretable numeric score. Beyond the consistency evaluation, we further
present a reason-assisted improver (RAI) that leverages the analytical reasons
with explanations identified by DCE to generate new responses aimed at reducing
these inconsistencies. Through comprehensive and systematic empirical analysis,
we show that our approach outperforms state-of-the-art methods by a large
margin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the
consistency of LLM generation across multiple benchmarks in semantic, factual,
and summarization consistency tasks. Our approach also substantially reduces
nearly 90% of output inconsistencies, showing promise for effective
hallucination mitigation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02137">SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment. (arXiv:2401.02137v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Ziping Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Furong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qingpei Guo</a></p>
<p>Multimodal alignment between language and vision is the fundamental topic in
current vision-language model research. Contrastive Captioners (CoCa), as a
representative method, integrates Contrastive Language-Image Pretraining (CLIP)
and Image Caption (IC) into a unified framework, resulting in impressive
results. CLIP imposes a bidirectional constraints on global representation of
entire images and sentences. Although IC conducts an unidirectional
image-to-text generation on local representation, it lacks any constraint on
local text-to-image reconstruction, which limits the ability to understand
images at a fine-grained level when aligned with texts. To achieve multimodal
alignment from both global and local perspectives, this paper proposes
Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional
interactions on images and texts across the global and local representation
levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)
head based on ITC and IC heads. The improved SyCoCa can further leverage
textual cues to reconstruct contextual images and visual cues to predict
textual contents. When implementing bidirectional local interactions, the local
contents of images tend to be cluttered or unrelated to their textual
descriptions. Thus, we employ an attentive masking strategy to select effective
image patches for interaction. Extensive experiments on five vision-language
tasks, including image-text retrieval, image-captioning, visual question
answering, and zero-shot/finetuned image classification, validate the
effectiveness of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02143">Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions. (arXiv:2401.02143v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cheng-Te Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yu-Che Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chih-Yao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1">Jay Chiehen Liao</a></p>
<p>In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural
Networks (GNNs), a domain where deep learning-based approaches have
increasingly shown superior performance in both classification and regression
tasks compared to traditional methods. The survey highlights a critical gap in
deep neural TDL methods: the underrepresentation of latent correlations among
data instances and feature values. GNNs, with their innate capability to model
intricate relationships and interactions between diverse elements of tabular
data, have garnered significant interest and application across various TDL
domains. Our survey provides a systematic review of the methods involved in
designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed
investigation into the foundational aspects and an overview of GNN-based TDL
methods, offering insights into their evolving landscape. We present a
comprehensive taxonomy focused on constructing graph structures and
representation learning within GNN-based TDL methods. In addition, the survey
examines various training plans, emphasizing the integration of auxiliary tasks
to enhance the effectiveness of instance representations. A critical part of
our discussion is dedicated to the practical application of GNNs across a
spectrum of GNN4TDL scenarios, demonstrating their versatility and impact.
Lastly, we discuss the limitations and propose future research directions,
aiming to spur advancements in GNN4TDL. This survey serves as a resource for
researchers and practitioners, offering a thorough understanding of GNNs' role
in revolutionizing TDL and pointing towards future innovations in this
promising area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02153">Unit Testing in ASP Revisited: Language and Test-Driven Development Environment. (arXiv:2401.02153v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amendola_G/0/1/0/all/0/1">Giovanni Amendola</a>, <a href="http://arxiv.org/find/cs/1/au:+Berei_T/0/1/0/all/0/1">Tobias Berei</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazzotta_G/0/1/0/all/0/1">Giuseppe Mazzotta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricca_F/0/1/0/all/0/1">Francesco Ricca</a></p>
<p>Unit testing frameworks are nowadays considered a best practice, included in
almost all modern software development processes, to achieve rapid development
of correct specifications. Knowledge representation and reasoning paradigms
such as Answer Set Programming (ASP), that have been used in industry-level
applications, are not an exception. Indeed, the first unit testing
specification language for ASP was proposed in 2011 as a feature of the ASPIDE
development environment. Later, a more portable unit testing language was
included in the LANA annotation language. In this paper we revisit both
languages and tools for unit testing in ASP. We propose a new unit test
specification language that allows one to inline tests within ASP programs, and
we identify the computational complexity of the tasks associated with checking
the various program-correctness assertions. Test-case specifications are
transparent to the traditional evaluation, but can be interpreted by a specific
testing tool. Thus, we present a novel environment supporting test driven
development of ASP programs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02154">Disentangle Estimation of Causal Effects from Cross-Silo Data. (arXiv:2401.02154v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuxuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haozhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhiming He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenchao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jialiang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a></p>
<p>Estimating causal effects among different events is of great importance to
critical fields such as drug development. Nevertheless, the data features
associated with events may be distributed across various silos and remain
private within respective parties, impeding direct information exchange between
them. This, in turn, can result in biased estimations of local causal effects,
which rely on the characteristics of only a subset of the covariates. To tackle
this challenge, we introduce an innovative disentangle architecture designed to
facilitate the seamless cross-silo transmission of model parameters, enriched
with causal mechanisms, through a combination of shared and private branches.
Besides, we introduce global constraints into the equation to effectively
mitigate bias within the various missing domains, thereby elevating the
accuracy of our causal effect estimation. Extensive experiments conducted on
new semi-synthetic datasets show that our method outperforms state-of-the-art
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02158">Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and LightGBM models. (arXiv:2401.02158v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chavda_R/0/1/0/all/0/1">Rushi Chavda</a>, <a href="http://arxiv.org/find/cs/1/au:+Makwana_D/0/1/0/all/0/1">Darshan Makwana</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1">Vraj Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1">Anupam Shukla</a></p>
<p>This paper describes approaches and results for shared Task 1 and 4 of
SMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english
tweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary
classification of English Reddit posts self-reporting a social anxiety disorder
diagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all
participants. We have leveraged the Transformer model (BERT) in combination
with the LightGBM model for both tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02173">Prompt Decoupling for Text-to-Image Person Re-identification. (arXiv:2401.02173v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weihao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1">Lei Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1">Pingyang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yan Zhang</a></p>
<p>Text-to-image person re-identification (TIReID) aims to retrieve the target
person from an image gallery via a textual description query. Recently,
pre-trained vision-language models like CLIP have attracted significant
attention and have been widely utilized for this task due to their robust
capacity for semantic concept learning and rich multi-modal knowledge. However,
recent CLIP-based TIReID methods commonly rely on direct fine-tuning of the
entire network to adapt the CLIP model for the TIReID task. Although these
methods show competitive performance on this topic, they are suboptimal as they
necessitate simultaneous domain adaptation and task adaptation. To address this
issue, we attempt to decouple these two processes during the training stage.
Specifically, we introduce the prompt tuning strategy to enable domain
adaptation and propose a two-stage training approach to disentangle domain
adaptation from task adaptation. In the first stage, we freeze the two encoders
from CLIP and solely focus on optimizing the prompts to alleviate domain gap
between the original training data of CLIP and downstream tasks. In the second
stage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize
capturing fine-grained information, which is more suitable for TIReID task.
Finally, we evaluate the effectiveness of our method on three widely used
datasets. Compared to the directly fine-tuned approach, our method achieves
significant improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02183">FairGridSearch: A Framework to Compare Fairness-Enhancing Models. (arXiv:2401.02183v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shih-Chi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermakova_T/0/1/0/all/0/1">Tatiana Ermakova</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabian_B/0/1/0/all/0/1">Benjamin Fabian</a></p>
<p>Machine learning models are increasingly used in critical decision-making
applications. However, these models are susceptible to replicating or even
amplifying bias present in real-world data. While there are various bias
mitigation methods and base estimators in the literature, selecting the optimal
model for a specific application remains challenging.
</p>
<p>This paper focuses on binary classification and proposes FairGridSearch, a
novel framework for comparing fairness-enhancing models. FairGridSearch enables
experimentation with different model parameter combinations and recommends the
best one. The study applies FairGridSearch to three popular datasets (Adult,
COMPAS, and German Credit) and analyzes the impacts of metric selection, base
estimator choice, and classification threshold on model fairness.
</p>
<p>The results highlight the significance of selecting appropriate accuracy and
fairness metrics for model evaluation. Additionally, different base estimators
and classification threshold values affect the effectiveness of bias mitigation
methods and fairness stability respectively, but the effects are not consistent
across all datasets. Based on these findings, future research on fairness in
machine learning should consider a broader range of factors when building fair
models, going beyond bias mitigation methods alone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02199">LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System. (arXiv:2401.02199v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Patel_A/0/1/0/all/0/1">Anil Ranjitbhai Patel</a>, <a href="http://arxiv.org/find/eess/1/au:+Liggesmeyer_P/0/1/0/all/0/1">Peter Liggesmeyer</a></p>
<p>As the horizon of intelligent transportation expands with the evolution of
Automated Driving Systems (ADS), ensuring paramount safety becomes more
imperative than ever. Traditional risk assessment methodologies, primarily
crafted for human-driven vehicles, grapple to adequately adapt to the
multifaceted, evolving environments of ADS. This paper introduces a framework
for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of
Artificial Neural Networks (ANNs).
</p>
<p>Our proposed solution transcends these limitations, drawing upon ANNs, a
cornerstone of deep learning, to meticulously analyze and categorize risk
dimensions using real-time On-board Sensor (OBS) data. This learning-centric
approach not only elevates the ADS's situational awareness but also enriches
its understanding of immediate operational contexts. By dissecting OBS data,
the system is empowered to pinpoint its current risk profile, thereby enhancing
safety prospects for onboard passengers and the broader traffic ecosystem.
</p>
<p>Through this framework, we chart a direction in risk assessment, bridging the
conventional voids and enhancing the proficiency of ADS. By utilizing ANNs, our
methodology offers a perspective, allowing ADS to adeptly navigate and react to
potential risk factors, ensuring safer and more informed autonomous journeys.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02212">Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph. (arXiv:2401.02212v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Rikui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1">Xiaoye Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wenfeng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1">Xianling Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dangyang Chen</a></p>
<p>Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by
attaching the time scope. Existing temporal knowledge graph question answering
(TKGQA) models solely approach simple questions, owing to the prior assumption
that each question only contains a single temporal fact with explicit/implicit
temporal constraints. Hence, they perform poorly on questions which own
multiple temporal facts. In this paper, we propose \textbf{\underline{J}}oint
\textbf{\underline{M}}ulti \textbf{\underline{F}}acts
\textbf{\underline{R}}easoning \textbf{\underline{N}}etwork (JMFRN), to jointly
reasoning multiple temporal facts for accurately answering \emph{complex}
temporal questions. Specifically, JMFRN first retrieves question-related
temporal facts from TKG for each entity of the given complex question. For
joint reasoning, we design two different attention (\ie entity-aware and
time-aware) modules, which are suitable for universal settings, to aggregate
entities and timestamps information of retrieved facts. Moreover, to filter
incorrect type answers, we introduce an additional answer type discrimination
task. Extensive experiments demonstrate our proposed method significantly
outperforms the state-of-art on the well-known complex temporal question
benchmark TimeQuestions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02244">Policy-regularized Offline Multi-objective Reinforcement Learning. (arXiv:2401.02244v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qian Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zongkai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zifan Wu</a></p>
<p>In this paper, we aim to utilize only offline trajectory data to train a
policy for multi-objective RL. We extend the offline policy-regularized method,
a widely-adopted approach for single-objective offline RL problems, into the
multi-objective setting in order to achieve the above goal. However, such
methods face a new challenge in offline MORL settings, namely the
preference-inconsistent demonstration problem. We propose two solutions to this
problem: 1) filtering out preference-inconsistent demonstrations via
approximating behavior preferences, and 2) adopting regularization techniques
with high policy expressiveness. Moreover, we integrate the
preference-conditioned scalarized update method into policy-regularized offline
RL, in order to simultaneously learn a set of policies using a single policy
network, thus reducing the computational cost induced by the training of a
large number of individual policies for various preferences. Finally, we
introduce Regularization Weight Adaptation to dynamically determine appropriate
regularization weights for arbitrary target preferences during deployment.
Empirical results on various multi-objective datasets demonstrate the
capability of our approach in solving offline MORL problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02258">Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation. (arXiv:2401.02258v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1">Linglong Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1">Zina Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1">Richard Dobson</a></p>
<p>Missingness is ubiquitous in multivariate time series and poses an obstacle
to reliable downstream analysis. Although recurrent network imputation achieved
the SOTA, existing models do not scale to deep architectures that can
potentially alleviate issues arising in complex data. Moreover, imputation
carries the risk of biased estimations of the ground truth. Yet, confidence in
the imputed values is always unmeasured or computed post hoc from model output.
We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates
missing values and their associated uncertainty in heterogeneous multivariate
time series. By jointly representing feature-wise correlations and temporal
dynamics, we adopt a self attention mechanism, along with an effective residual
component, to achieve a deep recurrent neural network with good imputation
performance and stable convergence. We also leverage self-supervised metric
learning to boost performance by optimizing sample similarity. Finally, we
transform DEARI into a Bayesian neural network through a novel Bayesian
marginalization strategy to produce stochastic DEARI, which outperforms its
deterministic equivalent. Experiments show that DEARI surpasses the SOTA in
diverse imputation tasks using real-world datasets, namely air quality control,
healthcare and traffic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02290">Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Heng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jiangnan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1">Alejo Lopez Avila</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jinhua Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph
Completion (KGC) by modelling how entities and relations interact in recent
years. However, the explanation of the predicted facts has not caught the
necessary attention. Proper explanations for the results of GNN-based KGC
models increase model transparency and help researchers develop more reliable
models. Existing practices for explaining KGC tasks rely on
instance/subgraph-based approaches, while in some scenarios, paths can provide
more user-friendly and interpretable explanations. Nonetheless, the methods for
generating path-based explanations for KGs have not been well-explored. To
address this gap, we propose Power-Link, the first path-based KGC explainer
that explores GNN-based models. We design a novel simplified graph-powering
technique, which enables the generation of path-based explanations with a fully
parallelisable and memory-efficient training scheme. We further introduce three
new metrics for quantitative evaluation of the explanations, together with a
qualitative human evaluation. Extensive experiments demonstrate that Power-Link
outperforms the SOTA baselines in interpretability, efficiency, and
scalability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02347">Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training. (arXiv:2401.02347v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Longtian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_S/0/1/0/all/0/1">Shan Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuming He</a></p>
<p>Image captioning aims at generating descriptive and meaningful textual
descriptions of images, enabling a broad range of vision-language applications.
Prior works have demonstrated that harnessing the power of Contrastive Image
Language Pre-training (CLIP) offers a promising approach to achieving zero-shot
captioning, eliminating the need for expensive caption annotations. However,
the widely observed modality gap in the latent space of CLIP harms the
performance of zero-shot captioning by breaking the alignment between paired
image-text features. To address this issue, we conduct an analysis on the CLIP
latent space which leads to two findings. Firstly, we observe that the CLIP's
visual feature of image subregions can achieve closer proximity to the paired
caption due to the inherent information loss in text descriptions. In addition,
we show that the modality gap between a paired image-text can be empirically
modeled as a zero-mean Gaussian distribution. Motivated by the findings, we
propose a novel zero-shot image captioning framework with text-only training to
reduce the modality gap. In particular, we introduce a subregion feature
aggregation to leverage local region information, which produces a compact
visual representation for matching text representation. Moreover, we
incorporate a noise injection and CLIP reranking strategy to boost captioning
performance. We also extend our framework to build a zero-shot VQA pipeline,
demonstrating its generality. Through extensive experiments on common
captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that
our method achieves remarkable performance improvements. Code is available at
https://github.com/Artanic30/MacCap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02349">A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Korkmaz_E/0/1/0/all/0/1">Ezgi Korkmaz</a></p>
<p>Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to self driving vehicles, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will outline the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
robustness and generalization capabilities. Furthermore, we will formalize and
unify the diverse solution approaches to increase generalization, and overcome
overfitting in state-action value functions. We believe our study can provide a
compact systematic unified analysis for the current advancements in deep
reinforcement learning, and help to construct robust deep neural policies with
improved generalization abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02383">Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications. (arXiv:2401.02383v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkatrayappa_D/0/1/0/all/0/1">Darshan Venkatrayappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Tremeau_A/0/1/0/all/0/1">Alain Tremeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Muselet_D/0/1/0/all/0/1">Damien Muselet</a>, <a href="http://arxiv.org/find/cs/1/au:+Colantoni_P/0/1/0/all/0/1">Philippe Colantoni</a></p>
<p>3D human body shape and pose estimation from RGB images is a challenging
problem with potential applications in augmented/virtual reality, healthcare
and fitness technology and virtual retail. Recent solutions have focused on
three types of inputs: i) single images, ii) multi-view images and iii) videos.
In this study, we surveyed and compared 3D body shape and pose estimation
methods for contemporary dance and performing arts, with a special focus on
human body pose and dressing, camera viewpoint, illumination conditions and
background conditions. We demonstrated that multi-frame methods, such as PHALP,
provide better results than single-frame method for pose estimation when
dancers are performing contemporary dances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02385">TinyLlama: An Open-Source Small Language Model. (arXiv:2401.02385v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1">Guangtao Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianduo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wei Lu</a></p>
<p>We present TinyLlama, a compact 1.1B language model pretrained on around 1
trillion tokens for approximately 3 epochs. Building on the architecture and
tokenizer of Llama 2, TinyLlama leverages various advances contributed by the
open-source community (e.g., FlashAttention), achieving better computational
efficiency. Despite its relatively small size, TinyLlama demonstrates
remarkable performance in a series of downstream tasks. It significantly
outperforms existing open-source language models with comparable sizes. Our
model checkpoints and code are publicly available on GitHub at
https://github.com/jzhang38/TinyLlama.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02403">Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks. (arXiv:2401.02403v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sajadi_P/0/1/0/all/0/1">Pouyan Sajadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehaghani_M/0/1/0/all/0/1">Mostafa Rahmani Dehaghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yifan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">G. Gary Wang</a></p>
<p>Accurately predicting the temperature field in metal additive manufacturing
(AM) processes is critical to preventing overheating, adjusting process
parameters, and ensuring process stability. While physics-based computational
models offer precision, they are often time-consuming and unsuitable for
real-time predictions and online control in iterative design scenarios.
Conversely, machine learning models rely heavily on high-quality datasets,
which can be costly and challenging to obtain within the metal AM domain. Our
work addresses this by introducing a physics-informed neural network framework
specifically designed for temperature field prediction in metal AM. This
framework incorporates a physics-informed input, physics-informed loss
function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.
Utilizing real-time temperature data from the process, our model predicts 2D
temperature fields for future timestamps across diverse geometries, deposition
patterns, and process parameters. We validate the proposed framework in two
scenarios: full-field temperature prediction for a thin wall and 2D temperature
field prediction for cylinder and cubic parts, demonstrating errors below 3%
and 1%, respectively. Our proposed framework exhibits the flexibility to be
applied across diverse scenarios with varying process parameters, geometries,
and deposition patterns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02411">What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs. (arXiv:2401.02411v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trevithick_A/0/1/0/all/0/1">Alex Trevithick</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1">Matthew Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Takikawa_T/0/1/0/all/0/1">Towaki Takikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1">Umar Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1">Shalini De Mello</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1">Manmohan Chandraker</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1">Ravi Ramamoorthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1">Koki Nagano</a></p>
<p>3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02412">LLM Augmented LLMs: Expanding Capabilities through Composition. (arXiv:2401.02412v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1">Rachit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1">Bidisha Samanta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1">Siddharth Dalmia</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1">Nitish Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1">Shikhar Vashishth</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1">Sriram Ganapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1">Abhishek Bapna</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1">Prateek Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1">Partha Talukdar</a></p>
<p>Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02416">ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Ayush Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Katara_P/0/1/0/all/0/1">Pushkal Katara</a>, <a href="http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1">Nikolaos Gkanatsios</a>, <a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1">Adam W. Harley</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarch_G/0/1/0/all/0/1">Gabriel Sarch</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1">Kriti Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1">Vishrav Chaudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1">Katerina Fragkiadaki</a></p>
<p>State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.03275">Sample-efficient Reinforcement Learning in Robotic Table Tennis. (arXiv:2011.03275v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tebbe_J/0/1/0/all/0/1">Jonas Tebbe</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauch_L/0/1/0/all/0/1">Lukas Krauch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yapeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1">Andreas Zell</a></p>
<p>Reinforcement learning (RL) has achieved some impressive recent successes in
various computer games and simulations. Most of these successes are based on
having large numbers of episodes from which the agent can learn. In typical
robotic applications, however, the number of feasible attempts is very limited.
In this paper we present a sample-efficient RL algorithm applied to the example
of a table tennis robot. In table tennis every stroke is different, with
varying placement, speed and spin. An accurate return therefore has to be found
depending on a high-dimensional continuous state space. To make learning in few
trials possible the method is embedded into our robot system. In this way we
can use a one-step environment. The state space depends on the ball at hitting
time (position, velocity, spin) and the action is the racket state
(orientation, velocity) at hitting. An actor-critic based deterministic policy
gradient algorithm was developed for accelerated learning. Our approach
performs competitively both in a simulation and on the real robot in a number
of challenging scenarios. Accurate results are obtained without pre-training in
under $200$ episodes of training. The video presenting our experiments is
available at https://youtu.be/uRAtdoL6Wpw.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.14956">Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongquan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jiayi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhongxi Zheng</a></p>
<p>Learning from noisy labels is an important concern in plenty of real-world
scenarios. Various approaches for this concern first make corrections
corresponding to potentially noisy-labeled instances, and then update
predictive model with information of the made corrections. However, in specific
areas, such as medical histopathology whole slide image analysis (MHWSIA), it
is often difficult or impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. For the problem
1), we present one-step abductive multi-target learning (OSAMTL) that imposes a
one-step logical reasoning upon machine learning via a multi-target learning
procedure to constrain the predictions of the learning model to be subject to
our prior knowledge about the true target. For the problem 2), we propose a
logical assessment formula (LAF) that evaluates the logical rationality of the
outputs of an approach by estimating the consistencies between the predictions
of the learning model and the logical facts narrated from the results of the
one-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.
pylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine
learning model achieving logically more rational predictions, which is beyond
various state-of-the-art approaches in handling complex noisy labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08364">Data Valuation for Vertical Federated Learning: A Model-free and Privacy-preserving Method. (arXiv:2112.08364v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Leye Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Junjie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1">Xiao Fang</a></p>
<p>Vertical Federated learning (VFL) is a promising paradigm for predictive
analytics, empowering an organization (i.e., task party) to enhance its
predictive models through collaborations with multiple data suppliers (i.e.,
data parties) in a decentralized and privacy-preserving way. Despite the
fast-growing interest in VFL, the lack of effective and secure tools for
assessing the value of data owned by data parties hinders the application of
VFL in business contexts. In response, we propose FedValue, a
privacy-preserving, task-specific but model-free data valuation method for VFL,
which consists of a data valuation metric and a federated computation method.
Specifically, we first introduce a novel data valuation metric, namely
MShapley-CMI. The metric evaluates a data party's contribution to a predictive
analytics task without the need of executing a machine learning model, making
it well-suited for real-world applications of VFL. Next, we develop an
innovative federated computation method that calculates the MShapley-CMI value
for each data party in a privacy-preserving manner. Extensive experiments
conducted on six public datasets validate the efficacy of FedValue for data
valuation in the context of VFL. In addition, we illustrate the practical
utility of FedValue with a case study involving federated movie
recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.13750">A First Runtime Analysis of the NSGA-II on a Multimodal Problem. (arXiv:2204.13750v5 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1">Benjamin Doerr</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1">Zhongdi Qu</a></p>
<p>Very recently, the first mathematical runtime analyses of the multi-objective
evolutionary optimizer NSGA-II have been conducted. We continue this line of
research with a first runtime analysis of this algorithm on a benchmark problem
consisting of two multimodal objectives. We prove that if the population size
$N$ is at least four times the size of the Pareto front, then the NSGA-II with
four different ways to select parents and bit-wise mutation optimizes the
OneJumpZeroJump benchmark with jump size~$2 \le k \le n/4$ in time $O(N n^k)$.
When using fast mutation, a recently proposed heavy-tailed mutation operator,
this guarantee improves by a factor of $k^{\Omega(k)}$. Overall, this work
shows that the NSGA-II copes with the local optima of the OneJumpZeroJump
problem at least as well as the global SEMO algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11329">Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1">Malte Hoffmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoopes_A/0/1/0/all/0/1">Andrew Hoopes</a>, <a href="http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1">Douglas N. Greve</a>, <a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1">Bruce Fischl</a>, <a href="http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1">Adrian V. Dalca</a></p>
<p>Affine image registration is a cornerstone of medical-image analysis. While
classical algorithms can achieve excellent accuracy, they solve a
time-consuming optimization for every image pair. Deep-learning (DL) methods
learn a function that maps an image pair to an output transform. Evaluating the
function is fast, but capturing large transforms can be challenging, and
networks tend to struggle if a test-image characteristic shifts from the
training domain, such as resolution. Most affine methods are agnostic to
anatomy, meaning the registration will be inaccurate if algorithms consider all
structures in the image.
</p>
<p>We address these shortcomings with SynthMorph, an easy-to-use DL tool for
joint affine-deformable registration of any brain image without preprocessing,
right off the MRI scanner. First, we leverage a strategy to train networks with
wildly varying images synthesized from label maps, yielding robust performance
across acquisition specifics unseen at training. Second, we optimize the
spatial overlap of select anatomical labels. This enables networks to
distinguish anatomy of interest from irrelevant structures, removing the need
for preprocessing that excludes content which would impinge on anatomy-specific
registration. Third, we combine the affine model with a deformable hypernetwork
that lets users choose the optimal deformation-field regularity for their
specific data, at registration time, in a fraction of the time required by
classical methods.
</p>
<p>We rigorously analyze how competing architectures learn affine transforms and
compare state-of-the-art registration tools across an extremely diverse set of
neuroimaging data, aiming to truly capture the behavior of methods in the real
world. SynthMorph demonstrates consistent and improved accuracy. It is
available at https://w3id.org/synthmorph, as a single complete end-to-end
solution for registration of brain MRI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13991">Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zunaed_M/0/1/0/all/0/1">Mohammad Zunaed</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1">Md. Aynal Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1">Taufiq Hasan</a></p>
<p>Performance degradation due to distribution discrepancy is a longstanding
challenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent
studies have demonstrated that CNNs are biased toward styles (e.g.,
uninformative textures) rather than content (e.g., shape), in stark contrast to
the human vision system. Radiologists tend to learn visual cues from CXRs and
thus perform well across multiple domains. Motivated by this, we employ the
novel on-the-fly style randomization modules at both image (SRM-IL) and feature
(SRM-FL) levels to create rich style perturbed features while keeping the
content intact for robust cross-domain performance. Previous methods simulate
unseen domains by constructing new styles via interpolation or swapping styles
from existing data, limiting them to available source domains during training.
However, SRM-IL samples the style statistics from the possible value range of a
CXR image instead of the training data to achieve more diversified
augmentations. Moreover, we utilize pixel-wise learnable parameters in the
SRM-FL compared to pre-defined channel-wise mean and standard deviations as
style embeddings for capturing more representative style features.
Additionally, we leverage consistency regularizations on global semantic
features and predictive distributions from with and without style-perturbed
versions of the same CXR to tweak the model's sensitivity toward content
markers for accurate predictions. Our proposed method, trained on CheXpert and
MIMIC-CXR datasets, achieves 77.32$\pm$0.35, 88.38$\pm$0.19, 82.63$\pm$0.13
AUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH
chest X-ray14, respectively, compared to 75.56$\pm$0.80, 87.57$\pm$0.46,
82.07$\pm$0.19 from state-of-the-art models on five-fold cross-validation with
statistically significant results in thoracic disease classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07520">STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning. (arXiv:2304.07520v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sirui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yali Du</a></p>
<p>Centralized Training with Decentralized Execution (CTDE) has been proven to
be an effective paradigm in cooperative multi-agent reinforcement learning
(MARL). One of the major challenges is credit assignment, which aims to credit
agents by their contributions. While prior studies have shown great success,
their methods typically fail to work in episodic reinforcement learning
scenarios where global rewards are revealed only at the end of the episode.
They lack the functionality to model complicated relations of the delayed
global reward in the temporal dimension and suffer from inefficiencies. To
tackle this, we introduce Spatial-Temporal Attention with Shapley (STAS), a
novel method that learns credit assignment in both temporal and spatial
dimensions. It first decomposes the global return back to each time step, then
utilizes the Shapley Value to redistribute the individual payoff from the
decomposed global reward. To mitigate the computational complexity of the
Shapley Value, we introduce an approximation of marginal contribution and
utilize Monte Carlo sampling to estimate it. We evaluate our method on an Alice
&amp; Bob example and MPE environments across different scenarios. Our results
demonstrate that our method effectively assigns spatial-temporal credit,
outperforming all state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00876">Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karimi_H/0/1/0/all/0/1">Hamed Karimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Samavi_R/0/1/0/all/0/1">Reza Samavi</a></p>
<p>Precise estimation of predictive uncertainty in deep neural networks is a
critical requirement for reliable decision-making in machine learning and
statistical modeling, particularly in the context of medical AI. Conformal
Prediction (CP) has emerged as a promising framework for representing the model
uncertainty by providing well-calibrated confidence levels for individual
predictions. However, the quantification of model uncertainty in conformal
prediction remains an active research area, yet to be fully addressed. In this
paper, we explore state-of-the-art CP methodologies and their theoretical
foundations. We propose a probabilistic approach in quantifying the model
uncertainty derived from the produced prediction sets in conformal prediction
and provide certified boundaries for the computed uncertainty. By doing so, we
allow model uncertainty measured by CP to be compared by other uncertainty
quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and
Evidential approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04502">Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sedova_A/0/1/0/all/0/1">Anastasiia Sedova</a>, <a href="http://arxiv.org/find/cs/1/au:+Zellinger_L/0/1/0/all/0/1">Lena Zellinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1">Benjamin Roth</a></p>
<p>An accurate and substantial dataset is essential for training a reliable and
well-performing model. However, even manually annotated datasets contain label
errors, not to mention automatically labeled ones. Previous methods for label
denoising have primarily focused on detecting outliers and their permanent
removal - a process that is likely to over- or underfilter the dataset. In this
work, we propose AGRA: a new method for learning with noisy labels by using
Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior
to model training, the dataset is dynamically adjusted during the training
process. By comparing the aggregated gradient of a batch of samples and an
individual example gradient, our method dynamically decides whether a
corresponding example is helpful for the model at this point or is
counter-productive and should be left out for the current update. Extensive
evaluation on several datasets demonstrates AGRA's effectiveness, while a
comprehensive results analysis supports our initial hypothesis: permanent hard
outlier removal is not always what model benefits the most from.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10759">SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qitian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wentao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chenxiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1">Fan Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haitian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1">Yatao Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a></p>
<p>Learning representations on large-sized graphs is a long-standing challenge
due to the inter-dependence nature involved in massive data points.
Transformers, as an emerging class of foundation encoders for graph-structured
data, have shown promising performance on small graphs due to its global
attention capable of capturing all-pair influence beyond neighboring nodes.
Even so, existing approaches tend to inherit the spirit of Transformers in
language and vision tasks, and embrace complicated models by stacking deep
multi-head attentions. In this paper, we critically demonstrate that even using
a one-layer attention can bring up surprisingly competitive performance across
node property prediction benchmarks where node numbers range from
thousand-level to billion-level. This encourages us to rethink the design
philosophy for Transformers on large graphs, where the global attention is a
computation overhead hindering the scalability. We frame the proposed scheme as
Simplified Graph Transformers (SGFormer), which is empowered by a simple
attention model that can efficiently propagate information among arbitrary
nodes in one layer. SGFormer requires none of positional encodings,
feature/graph pre-processing or augmented loss. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M and yields up to
141x inference acceleration over SOTA Transformers on medium-sized graphs.
Beyond current results, we believe the proposed methodology alone enlightens a
new technical path of independent interest for building Transformers on large
graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11586">Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Egressy_B/0/1/0/all/0/1">B&#xe9;ni Egressy</a>, <a href="http://arxiv.org/find/cs/1/au:+Niederhausern_L/0/1/0/all/0/1">Luc von Niederh&#xe4;usern</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanusa_J/0/1/0/all/0/1">Jovan Blanusa</a>, <a href="http://arxiv.org/find/cs/1/au:+Altman_E/0/1/0/all/0/1">Erik Altman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1">Roger Wattenhofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Atasu_K/0/1/0/all/0/1">Kubilay Atasu</a></p>
<p>This paper analyses a set of simple adaptations that transform standard
message-passing Graph Neural Networks (GNN) into provably powerful directed
multigraph neural networks. The adaptations include multigraph port numbering,
ego IDs, and reverse message passing. We prove that the combination of these
theoretically enables the detection of any directed subgraph pattern. To
validate the effectiveness of our proposed adaptations in practice, we conduct
experiments on synthetic subgraph detection tasks, which demonstrate
outstanding performance with almost perfect results. Moreover, we apply our
proposed adaptations to two financial crime analysis tasks. We observe dramatic
improvements in detecting money laundering transactions, improving the
minority-class F1 score of a standard message-passing GNN by up to 30%, and
closely matching or outperforming tree-based and GNN baselines. Similarly
impressive results are observed on a real-world phishing detection dataset,
boosting three standard GNNs' F1 scores by around 15% and outperforming all
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05300">Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenhailong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenshan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a></p>
<p>Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12517">Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1">Hyunsik Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jeonghyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jinhyeok Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1">Gwanghyeon Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1">Moonkyu Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Youm_D/0/1/0/all/0/1">Donghoon Youm</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1">Jemin Hwangbo</a></p>
<p>Several earlier studies have shown impressive control performance in complex
robotic systems by designing the controller using a neural network and training
it with model-free reinforcement learning. However, these outstanding
controllers with natural motion style and high task performance are developed
through extensive reward engineering, which is a highly laborious and
time-consuming process of designing numerous reward terms and determining
suitable reward coefficients. In this work, we propose a novel reinforcement
learning framework for training neural network controllers for complex robotic
systems consisting of both rewards and constraints. To let the engineers
appropriately reflect their intent to constraints and handle them with minimal
computation overhead, two constraint types and an efficient policy optimization
algorithm are suggested. The learning framework is applied to train locomotion
controllers for several legged robots with different morphology and physical
attributes to traverse challenging terrains. Extensive simulation and
real-world experiments demonstrate that performant controllers can be trained
with significantly less reward engineering, by tuning only a single reward
coefficient. Furthermore, a more straightforward and intuitive engineering
process can be utilized, thanks to the interpretability and generalizability of
constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15256">Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Ji-Hoon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Jaehun Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Chung_J/0/1/0/all/0/1">Joon Son Chung</a></p>
<p>The goal of this work is to reconstruct high quality speech from lip motions
alone, a task also known as lip-to-speech. A key challenge of lip-to-speech
systems is the one-to-many mapping caused by (1) the existence of homophenes
and (2) multiple speech variations, resulting in a mispronounced and
over-smoothed speech. In this paper, we propose a novel lip-to-speech system
that significantly improves the generation quality by alleviating the
one-to-many mapping problem from multiple perspectives. Specifically, we
incorporate (1) self-supervised speech representations to disambiguate
homophenes, and (2) acoustic variance information to model diverse speech
styles. Additionally, to better solve the aforementioned problem, we employ a
flow based post-net which captures and refines the details of the generated
speech. We perform extensive experiments on two datasets, and demonstrate that
our method achieves the generation quality close to that of real human
utterance, outperforming existing methods in terms of speech naturalness and
intelligibility by a large margin. Synthesised samples are available at our
demo page: https://mm.kaist.ac.kr/projects/LTBS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02742">MLN-net: A multi-source medical image segmentation method for clustered microcalcifications using multiple layer normalization. (arXiv:2309.02742v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zanting Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1">Haidong Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Banteng Liu</a></p>
<p>Accurate segmentation of clustered microcalcifications in mammography is
crucial for the diagnosis and treatment of breast cancer. Despite exhibiting
expert-level accuracy, recent deep learning advancements in medical image
segmentation provide insufficient contribution to practical applications, due
to the domain shift resulting from differences in patient postures, individual
gland density, and imaging modalities of mammography etc. In this paper, a
novel framework named MLN-net, which can accurately segment multi-source images
using only single source images, is proposed for clustered microcalcification
segmentation. We first propose a source domain image augmentation method to
generate multi-source images, leading to improved generalization. And a
structure of multiple layer normalization (LN) layers is used to construct the
segmentation network, which can be found efficient for clustered
microcalcification segmentation in different domains. Additionally, a branch
selection strategy is designed for measuring the similarity of the source
domain data and the target domain data. To validate the proposed MLN-net,
extensive analyses including ablation experiments are performed, comparison of
12 baseline methods. Extensive experiments validate the effectiveness of
MLN-net in segmenting clustered microcalcifications from different domains and
the its segmentation accuracy surpasses state-of-the-art methods. Code will be
available at https://github.com/yezanting/MLN-NET-VERSON1.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09431">FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pretraining. (arXiv:2309.09431v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1">Shaheer Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Haghighat_M/0/1/0/all/0/1">Maryam Haghighat</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1">Tharindu Fernando</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1">Sridha Sridharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1">Clinton Fookes</a>, <a href="http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1">Peyman Moghadam</a></p>
<p>Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pretraining, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pretraining procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pretraining, we also devise efficient
masking strategies for pretraining each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12081">DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Haoran Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Dixin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongteng Xu</a></p>
<p>Graph matching is one of the most significant graph analytic tasks in
practice, which aims to find the node correspondence across different graphs.
Most existing approaches rely on adjacency matrices or node embeddings when
matching graphs, whose performances are often sub-optimal because of not fully
leveraging the multi-modal information hidden in graphs, such as node
attributes, subgraph structures, etc. In this study, we propose a novel and
effective graph matching method based on a differentiable hierarchical optimal
transport (HOT) framework, called DHOT-GM. Essentially, our method represents
each graph as a set of relational matrices corresponding to the information of
different modalities. Given two graphs, we enumerate all relational matrix
pairs and obtain their matching results, and accordingly, infer the node
correspondence by the weighted averaging of the matching results. This method
can be implemented as computing the HOT distance between the two graphs -- each
matching result is an optimal transport plan associated with the
Gromov-Wasserstein (GW) distance between two relational matrices, and the
weights of all matching results are the elements of an upper-level optimal
transport plan defined on the matrix sets. We propose a bi-level optimization
algorithm to compute the HOT distance in a differentiable way, making the
significance of the relational matrices adjustable. Experiments on various
graph matching tasks demonstrate the superiority and robustness of our method
compared to state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19251">Pre-trained Recommender Systems: A Causal Debiasing Perspective. (arXiv:2310.19251v3 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Ziqian Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Hao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1">Nghia Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a>, <a href="http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1">Anoop Deoras</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>Recent studies on pre-trained vision/language models have demonstrated the
practical benefit of a new, promising solution-building paradigm in AI where
models can be pre-trained on broad data describing a generic task space and
then adapted successfully to solve a wide range of downstream tasks, even when
training data is severely limited (e.g., in zero- or few-shot learning
scenarios). Inspired by such progress, we investigate in this paper the
possibilities and challenges of adapting such a paradigm to the context of
recommender systems, which is less investigated from the perspective of
pre-trained model. In particular, we propose to develop a generic recommender
that captures universal interaction patterns by training on generic user-item
interaction data extracted from different domains, which can then be fast
adapted to improve few-shot learning performance in unseen new domains (with
limited data).
</p>
<p>However, unlike vision/language data which share strong conformity in the
semantic space, universal patterns underlying recommendation data collected
across different domains (e.g., different countries or different E-commerce
platforms) are often occluded by both in-domain and cross-domain biases
implicitly imposed by the cultural differences in their user and item bases, as
well as their uses of different e-commerce platforms. As shown in our
experiments, such heterogeneous biases in the data tend to hinder the
effectiveness of the pre-trained model. To address this challenge, we further
introduce and formalize a causal debiasing perspective, which is substantiated
via a hierarchical Bayesian deep learning model, named PreRec. Our empirical
studies on real-world data show that the proposed model could significantly
improve the recommendation performance in zero- and few-shot learning settings
under both cross-market and cross-platform scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04589">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingxue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the
interactions among multi-modal inputs and the generation in non-textual
modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an
approach to treat the input from any modality as a token sequence and learn a
joint embedding space for all modalities. Specifically, for the input from any
modality, TEAL first discretizes it into a token sequence with the
off-the-shelf tokenizer and embeds the token sequence into a joint embedding
space with a learnable embedding matrix. MM-LLMs just need to predict the
multi-modal tokens autoregressively as the textual LLMs do. Finally, the
corresponding de-tokenizer is applied to generate the output in each modality
based on the predicted token sequence. With the joint embedding space, TEAL
enables the frozen LLMs to perform both understanding and generation tasks
involving non-textual modalities, such as image and audio. Thus, the textual
LLM can just work as an interface and maintain its high performance in textual
understanding and generation. Experiments show that TEAL achieves substantial
improvements in multi-modal understanding, and implements a simple scheme for
multi-modal generations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16124">DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification. (arXiv:2311.16124v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Mintong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Diffusion-based purification defenses leverage diffusion models to remove
crafted perturbations of adversarial examples and achieve state-of-the-art
robustness. Recent studies show that even advanced attacks cannot break such
defenses effectively, since the purification process induces an extremely deep
computational graph which poses the potential problem of gradient obfuscation,
high memory cost, and unbounded randomness. In this paper, we propose a unified
framework DiffAttack to perform effective and efficient attacks against
diffusion-based purification defenses, including both DDPM and score-based
approaches. In particular, we propose a deviated-reconstruction loss at
intermediate diffusion steps to induce inaccurate density gradient estimation
to tackle the problem of vanishing/exploding gradients. We also provide a
segment-wise forwarding-backwarding algorithm, which leads to memory-efficient
gradient backpropagation. We validate the attack effectiveness of DiffAttack
compared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that
DiffAttack decreases the robust accuracy of models compared with SOTA attacks
by over 20% on CIFAR-10 under $\ell_\infty$ attack $(\epsilon=8/255)$, and over
10% on ImageNet under $\ell_\infty$ attack $(\epsilon=4/255)$. We conduct a
series of ablations studies, and we find 1) DiffAttack with the
deviated-reconstruction loss added over uniformly sampled time steps is more
effective than that added over only initial/final steps, and 2) diffusion-based
purification with a moderate diffusion length is more robust under DiffAttack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01520">Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1">Marco Scutari</a></p>
<p>Bayesian networks (BNs) are a foundational model in machine learning and
causal inference. Their graphical structure can handle high-dimensional
problems, divide them into a sparse collection of smaller ones, underlies Judea
Pearl's causality, and determines their explainability and interpretability.
Despite their popularity, there are almost no resources in the literature on
how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for
BNs under their most common distributional assumptions. In this paper, we
provide computationally efficient algorithms for both by leveraging BNs'
graphical structure, and we illustrate them with a complete set of numerical
examples. In the process, we show it is possible to reduce the computational
complexity of KL from cubic to quadratic for Gaussian BNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10302">One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunshui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1">Binyuan Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1">Xiaobo Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiaxi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Shuzheng Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junhao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a></p>
<p>Aligning large language models(LLMs) with human is a critical step in
effectively utilizing their pre-trained capabilities across a wide array of
language tasks. Current instruction tuning practices often rely on expanding
dataset size without a clear strategy for ensuring data quality, which can
inadvertently introduce noise and degrade model performance. To address this
challenge, we introduce Nuggets, a novel and efficient methodology that employs
one shot learning to select high-quality instruction data from expansive
datasets. Nuggets assesses the potential of individual instruction examples to
act as effective one shot examples, thereby identifying those that can
significantly enhance diverse task performance. Nuggets utilizes a scoring
system based on the impact of candidate examples on the perplexity of a diverse
anchor set, facilitating the selection of the most beneficial data for
instruction tuning. Through rigorous testing on two benchmarks, including
MT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top
1% of Nuggets-curated examples substantially outperforms conventional methods
that use the full dataset. These findings advocate for a data selection
paradigm that prioritizes quality, offering a more efficient pathway to align
LLMs with humans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11234">Perceptual Musical Features for Interpretable Audio Tagging. (arXiv:2312.11234v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyberatos_V/0/1/0/all/0/1">Vassilis Lyberatos</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantarelis_S/0/1/0/all/0/1">Spyridon Kantarelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Dervakos_E/0/1/0/all/0/1">Edmund Dervakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamou_G/0/1/0/all/0/1">Giorgos Stamou</a></p>
<p>In the age of music streaming platforms, the task of automatically tagging
music audio has garnered significant attention, driving researchers to devise
methods aimed at enhancing performance metrics on standard datasets. Most
recent approaches rely on deep neural networks, which, despite their impressive
performance, possess opacity, making it challenging to elucidate their output
for a given input. While the issue of interpretability has been emphasized in
other fields like medicine, it has not received attention in music-related
tasks. In this study, we explored the relevance of interpretability in the
context of automatic music tagging. We constructed a workflow that incorporates
three different information extraction techniques: a) leveraging symbolic
knowledge, b) utilizing auxiliary deep neural networks, and c) employing signal
processing to extract perceptual features from audio files. These features were
subsequently used to train an interpretable machine-learning model for tag
prediction. We conducted experiments on two datasets, namely the MTG-Jamendo
dataset and the GTZAN dataset. Our method surpassed the performance of baseline
models in both tasks and, in certain instances, demonstrated competitiveness
with the current state-of-the-art. We conclude that there are use cases where
the deterioration in performance is outweighed by the value of
interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11671">Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kinniment_M/0/1/0/all/0/1">Megan Kinniment</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_L/0/1/0/all/0/1">Lucas Jun Koba Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1">Haoxing Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodrich_B/0/1/0/all/0/1">Brian Goodrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasin_M/0/1/0/all/0/1">Max Hasin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1">Lawrence Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Miles_L/0/1/0/all/0/1">Luke Harold Miles</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tao R. Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wijk_H/0/1/0/all/0/1">Hjalmar Wijk</a>, <a href="http://arxiv.org/find/cs/1/au:+Burget_J/0/1/0/all/0/1">Joel Burget</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1">Aaron Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1">Elizabeth Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1">Paul Christiano</a></p>
<p>In this report, we explore the ability of language model agents to acquire
resources, create copies of themselves, and adapt to novel challenges they
encounter in the wild. We refer to this cluster of capabilities as "autonomous
replication and adaptation" or ARA. We believe that systems capable of ARA
could have wide-reaching and hard-to-anticipate consequences, and that
measuring and forecasting ARA may be useful for informing measures around
security, monitoring, and alignment. Additionally, once a system is capable of
ARA, placing bounds on a system's capabilities may become significantly more
difficult.
</p>
<p>We construct four simple example agents that combine language models with
tools that allow them to take actions in the world. We then evaluate these
agents on 12 tasks relevant to ARA. We find that these language model agents
can only complete the easiest tasks from this list, although they make some
progress on the more challenging tasks. Unfortunately, these evaluations are
not adequate to rule out the possibility that near-future agents will be
capable of ARA. In particular, we do not think that these evaluations provide
good assurance that the ``next generation'' of language models (e.g. 100x
effective compute scaleup on existing models) will not yield agents capable of
ARA, unless intermediate evaluations are performed during pretraining.
Relatedly, we expect that fine-tuning of the existing models could produce
substantially more competent agents, even if the fine-tuning is not directly
targeted at ARA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11973">Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Haeyong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a></p>
<p>Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12728">Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhitian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1">Chenyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a></p>
<p>As Large Language Models (LLMs) have made significant advancements across
various tasks, such as question answering, translation, text summarization, and
dialogue systems, the need for accuracy in information becomes crucial,
especially for serious financial products serving billions of users like
Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation
(RAG) system that grounds LLMs on the most accurate and up-to-date information.
However, for a real-world product serving millions of users, the inference
speed of LLMs becomes a critical factor compared to a mere experimental model.
</p>
<p>Hence, this paper presents a generic framework for accelerating the inference
process, resulting in a substantial increase in speed and cost reduction for
our RAG system, with lossless generation accuracy. In the traditional inference
process, each token is generated sequentially by the LLM, leading to a time
consumption proportional to the number of generated tokens. To enhance this
process, our framework, named \textit{lookahead}, introduces a
\textit{multi-branch} strategy. Instead of generating a single token at a time,
we propose a \textit{Trie-based Retrieval} (TR) process that enables the
generation of multiple branches simultaneously, each of which is a sequence of
tokens. Subsequently, for each branch, a \textit{Verification and Accept} (VA)
process is performed to identify the longest correct sub-sequence as the final
output. Our strategy offers two distinct advantages: (1) it guarantees absolute
correctness of the output, avoiding any approximation algorithms, and (2) the
worst-case performance of our approach is equivalent to the conventional
process. We conduct extensive experiments to demonstrate the significant
improvements achieved by applying our inference acceleration framework. Code is
avaliable: https://github.com/alipay/PainlessInferenceAcceleration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16430">Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zaifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chao Wei</a></p>
<p>Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference near deterministic. IPO uses a root-finding pairwise MSE loss to
solve the ignoring KL-regularization problem, and learning an optimal policy.
But IPO's pairwise loss still can't s make the KL-regularization to work. In
this paper, we design a simple and intuitive off-policy preferences
optimization algorithm from an importance sampling view, and add an off-policy
KL-regularization term which makes KL-regularization truly effective. To
simplify the learning process and save memory usage, we can generate
regularization data in advance, which eliminate the needs for both reward model
and reference policy in the stage of optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16713">Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1">Linglong Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1">Zina Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_H/0/1/0/all/0/1">Hugh Logan Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuezhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1">Richard Dobson</a></p>
<p>This study presents a novel approach to addressing the challenge of missing
data in multivariate time series, with a particular focus on the complexities
of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model,
grounded in a transformer-based framework, introduces a conditional hidden
state initialization tailored to the intricacies of medical time series data.
This methodology diverges from traditional imputation techniques by
specifically targeting the imbalance in missing data distribution, a crucial
aspect often overlooked in healthcare datasets. By integrating advanced
knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to
the distinct patterns of missing data in Electronic Health Records (EHRs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00926">Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Ben Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yifei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changmiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xianjun Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yuxing Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1">Feiwei Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yong Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yu Gao</a></p>
<p>In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01078">Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1">Triet Minh Huynh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1">Quan Le Bao</a></p>
<p>Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems of various genres from natural language
prompts, thereby facilitating an intuitive process with enhanced content
control. Our most efficacious model, the GPT-3 Babbage variant, achieves a
custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of
Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems
into normal text prompts and yield a relatively high score of 0.781 in the "luc
bat" genre. This experiment presents the potential for cross-Language
poem-to-poem translation with translated poems as the inputs while concurrently
maintaining complete control over the generated content.
</p>
</p>
</div>

    </div>
    </body>
    