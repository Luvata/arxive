<!DOCTYPE html>
<html>
<head>
<title>2024-01-13-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.05338">STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers. (arXiv:2401.05338v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_D/0/1/0/all/0/1">Daqian Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fesser_L/0/1/0/all/0/1">Lukas Fesser</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1">Marta Kwiatkowska</a></p>
<p>Robustness certification, which aims to formally certify the predictions of
neural networks against adversarial inputs, has become an integral part of
important tool for safety-critical applications. Despite considerable progress,
existing certification methods are limited to elementary architectures, such as
convolutional networks, recurrent networks and recently Transformers, on
benchmark datasets such as MNIST. In this paper, we focus on the robustness
certification of scene text recognition (STR), which is a complex and
extensively deployed image-based sequence prediction problem. We tackle three
types of STR model architectures, including the standard STR pipelines and the
Vision Transformer. We propose STR-Cert, the first certification method for STR
models, by significantly extending the DeepPoly polyhedral verification
framework via deriving novel polyhedral bounds and algorithms for key STR model
components. Finally, we certify and compare STR models on six datasets,
demonstrating the efficiency and scalability of robustness certification,
particularly for the Vision Transformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05339">MicroGlam: Microscopic Skin Image Dataset with Cosmetics. (arXiv:2401.05339v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chong_T/0/1/0/all/0/1">Toby Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chadwick_A/0/1/0/all/0/1">Alina Chadwick</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_I/0/1/0/all/0/1">I-chao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Haoran Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Igarashi_T/0/1/0/all/0/1">Takeo Igarashi</a></p>
<p>In this paper, we present a cosmetic-specific skin image dataset. It consists
of skin images from $45$ patches ($5$ skin patches each from $9$ participants)
of size $8mm^*8mm$ under three cosmetic products (i.e., foundation, blusher,
and highlighter). We designed a novel capturing device inspired by Light Stage.
Using the device, we captured over $600$ images of each skin patch under
diverse lighting conditions in $30$ seconds. We repeated the process for the
same skin patch under three cosmetic products. Finally, we demonstrate the
viability of the dataset with an image-to-image translation-based pipeline for
cosmetic rendering and compared our data-driven approach to an existing
cosmetic rendering method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05345">DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines. (arXiv:2401.05345v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Durvasula_S/0/1/0/all/0/1">Sankeerth Durvasula</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1">Adrian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1">Ruofan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanjaya_P/0/1/0/all/0/1">Pawan Kumar Sanjaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Vijaykumar_N/0/1/0/all/0/1">Nandita Vijaykumar</a></p>
<p>Differentiable rendering is a technique used in an important emerging class
of visual computing applications that involves representing a 3D scene as a
model that is trained from 2D images using gradient descent. Recent works (e.g.
3D Gaussian Splatting) use a rasterization pipeline to enable rendering high
quality photo-realistic imagery at high speeds from these learned 3D models.
These methods have been demonstrated to be very promising, providing
state-of-art quality for many important tasks. However, training a model to
represent a scene is still a time-consuming task even when using powerful GPUs.
In this work, we observe that the gradient computation phase during training is
a significant bottleneck on GPUs due to the large number of atomic operations
that need to be processed. These atomic operations overwhelm atomic units in
the L2 partitions causing stalls. To address this challenge, we leverage the
observations that during the gradient computation: (1) for most warps, all
threads atomically update the same memory locations; and (2) warps generate
varying amounts of atomic traffic (since some threads may be inactive). We
propose DISTWAR, a software-approach to accelerate atomic operations based on
two key ideas: First, we enable warp-level reduction of threads at the SM
sub-cores using registers to leverage the locality in intra-warp atomic
updates. Second, we distribute the atomic computation between the warp-level
reduction at the SM and the L2 atomic units to increase the throughput of
atomic computation. Warps with many threads performing atomic updates to the
same memory locations are scheduled at the SM, and the rest using L2 atomic
units. We implement DISTWAR using existing warp-level primitives. We evaluate
DISTWAR on widely used raster-based differentiable rendering workloads. We
demonstrate significant speedups of 2.44x on average (up to 5.7x).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05352">Generalized Categories Discovery for Long-tailed Recognition. (arXiv:2401.05352v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1">Christoph Meinel</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haojin Yang</a></p>
<p>Generalized Class Discovery (GCD) plays a pivotal role in discerning both
known and unknown categories from unlabeled datasets by harnessing the insights
derived from a labeled set comprising recognized classes. A significant
limitation in prevailing GCD methods is their presumption of an equitably
distributed category occurrence in unlabeled data. Contrary to this assumption,
visual classes in natural environments typically exhibit a long-tailed
distribution, with known or prevalent categories surfacing more frequently than
their rarer counterparts. Our research endeavors to bridge this disconnect by
focusing on the long-tailed Generalized Category Discovery (Long-tailed GCD)
paradigm, which echoes the innate imbalances of real-world unlabeled datasets.
In response to the unique challenges posed by Long-tailed GCD, we present a
robust methodology anchored in two strategic regularizations: (i) a reweighting
mechanism that bolsters the prominence of less-represented, tail-end
categories, and (ii) a class prior constraint that aligns with the anticipated
class distribution. Comprehensive experiments reveal that our proposed method
surpasses previous state-of-the-art GCD methods by achieving an improvement of
approximately 6 - 9% on ImageNet100 and competitive performance on CIFAR100.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05353">ImbaGCD: Imbalanced Generalized Category Discovery. (arXiv:2401.05353v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1">Ben Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Simsek_F/0/1/0/all/0/1">Furkan Simsek</a>, <a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1">Christoph Meinel</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haojin Yang</a></p>
<p>Generalized class discovery (GCD) aims to infer known and unknown categories
in an unlabeled dataset leveraging prior knowledge of a labeled set comprising
known classes. Existing research implicitly/explicitly assumes that the
frequency of occurrence for each category, whether known or unknown, is
approximately the same in the unlabeled data. However, in nature, we are more
likely to encounter known/common classes than unknown/uncommon ones, according
to the long-tailed property of visual classes. Therefore, we present a
challenging and practical problem, Imbalanced Generalized Category Discovery
(ImbaGCD), where the distribution of unlabeled data is imbalanced, with known
classes being more frequent than unknown ones. To address these issues, we
propose ImbaGCD, A novel optimal transport-based expectation maximization
framework that accomplishes generalized category discovery by aligning the
marginal class prior distribution. ImbaGCD also incorporates a systematic
mechanism for estimating the imbalanced class prior distribution under the GCD
setup. Our comprehensive experiments reveal that ImbaGCD surpasses previous
state-of-the-art GCD methods by achieving an improvement of approximately 2 -
4% on CIFAR-100 and 15 - 19% on ImageNet-100, indicating its superior
effectiveness in solving the Imbalanced GCD problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05355">Developing a Resource-Constraint EdgeAI model for Surface Defect Detection. (arXiv:2401.05355v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mih_A/0/1/0/all/0/1">Atah Nuh Mih</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Hung Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawnine_A/0/1/0/all/0/1">Asfia Kawnine</a>, <a href="http://arxiv.org/find/cs/1/au:+Wachowicz_M/0/1/0/all/0/1">Monica Wachowicz</a></p>
<p>Resource constraints have restricted several EdgeAI applications to machine
learning inference approaches, where models are trained on the cloud and
deployed to the edge device. This poses challenges such as bandwidth, latency,
and privacy associated with storing data off-site for model building. Training
on the edge device can overcome these challenges by eliminating the need to
transfer data to another device for storage and model development. On-device
training also provides robustness to data variations as models can be retrained
on newly acquired data to improve performance. We, therefore, propose a
lightweight EdgeAI architecture modified from Xception, for on-device training
in a resource-constraint edge environment. We evaluate our model on a PCB
defect detection task and compare its performance against existing lightweight
models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our
experiment show that our model has a remarkable performance with a test
accuracy of 73.45% without pre-training. This is comparable to the test
accuracy of non-pre-trained MobileViT-XXS (75.40%) and much better than other
non-pre-trained models (MobileNetV2 - 50.05%, EfficientNetV2B0 - 54.30%). The
test accuracy of our model without pre-training is comparable to pre-trained
MobileNetV2 model - 75.45% and better than pre-trained EfficientNetV2B0 model -
58.10%. In terms of memory efficiency, our model performs better than
EfficientNetV2B0 and MobileViT-XXS. We find that the resource efficiency of
machine learning models does not solely depend on the number of parameters but
also depends on architectural considerations. Our method can be applied to
other resource-constraint applications while maintaining significant
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05362">DualTeacher: Bridging Coexistence of Unlabelled Classes for Semi-supervised Incremental Object Detection. (arXiv:2401.05362v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Ziqi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenbo Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1">Jiachen Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1">Jianyong Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianmin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>In real-world applications, an object detector often encounters object
instances from new classes and needs to accommodate them effectively. Previous
work formulated this critical problem as incremental object detection (IOD),
which assumes the object instances of new classes to be fully annotated in
incremental data. However, as supervisory signals are usually rare and
expensive, the supervised IOD may not be practical for implementation. In this
work, we consider a more realistic setting named semi-supervised IOD (SSIOD),
where the object detector needs to learn new classes incrementally from a few
labelled data and massive unlabelled data without catastrophic forgetting of
old classes. A commonly-used strategy for supervised IOD is to encourage the
current model (as a student) to mimic the behavior of the old model (as a
teacher), but it generally fails in SSIOD because a dominant number of object
instances from old and new classes are coexisting and unlabelled, with the
teacher only recognizing a fraction of them. Observing that learning only the
classes of interest tends to preclude detection of other classes, we propose to
bridge the coexistence of unlabelled classes by constructing two teacher models
respectively for old and new classes, and using the concatenation of their
predictions to instruct the student. This approach is referred to as
DualTeacher, which can serve as a strong baseline for SSIOD with limited
resource overhead and no extra hyperparameters. We build various benchmarks for
SSIOD and perform extensive experiments to demonstrate the superiority of our
approach (e.g., the performance lead is up to 18.28 AP on MS-COCO). Our code is
available at \url{https://github.com/chuxiuhong/DualTeacher}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05379">AutoVisual Fusion Suite: A Comprehensive Evaluation of Image Segmentation and Voice Conversion Tools on HuggingFace Platform. (arXiv:2401.05379v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1">Amirreza Hashemi</a></p>
<p>This study presents a comprehensive evaluation of tools available on the
HuggingFace platform for two pivotal applications in artificial intelligence:
image segmentation and voice conversion. The primary objective was to identify
the top three tools within each category and subsequently install and configure
these tools on Linux systems. We leveraged the power of pre-trained
segmentation models such as SAM and DETR Model with ResNet-50 backbone for
image segmentation, and the so-vits-svc-fork model for voice conversion. This
paper delves into the methodologies and challenges encountered during the
implementation process, and showcases the successful combination of video
segmentation and voice conversion in a unified project named AutoVisual Fusion
Suite.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05390">Generation of BIM data based on the automatic detection, identification and localization of lamps in buildings. (arXiv:2401.05390v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Troncoso_Pastoriza_F/0/1/0/all/0/1">Francisco Troncoso-Pastoriza</a>, <a href="http://arxiv.org/find/cs/1/au:+Eguia_Oller_P/0/1/0/all/0/1">Pablo Egu&#xed;a-Oller</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Redondo_R/0/1/0/all/0/1">Rebeca P. D&#xed;az-Redondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Granada_Alvarez_E/0/1/0/all/0/1">Enrique Granada-&#xc1;lvarez</a></p>
<p>In this paper we introduce a method that supports the detection,
identification and localization of lamps in a building, with the main goal of
automatically feeding its energy model by means of Building Information
Modeling (BIM) methods. The proposed method, thus, provides useful information
to apply energy-saving strategies to reduce energy consumption in the building
sector through the correct management of the lighting infrastructure. Based on
the unique geometry and brightness of lamps and the use of only greyscale
images, our methodology is able to obtain accurate results despite its low
computational needs, resulting in near-real-time processing. The main novelty
is that the focus of the candidate search is not over the entire image but
instead only on a limited region that summarizes the specific characteristics
of the lamp. The information obtained from our approach was used on the Green
Building XML Schema to illustrate the automatic generation of BIM data from the
results of the algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05392">AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted with Salt-and-Pepper. (arXiv:2401.05392v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1">Vikas Singh</a></p>
<p>Noise is inevitably common in digital images, leading to visual image
deterioration. Therefore, a suitable filtering method is required to lessen the
noise while preserving the image features (edges, corners, etc.). This paper
presents the efficient type-2 fuzzy weighted mean filter with an adaptive
threshold to remove the SAP noise. The present filter has two primary steps:
The first stage categorizes images as lightly, medium, and heavily corrupted
based on an adaptive threshold by comparing the M-ALD of processed pixels with
the upper and lower MF of the type-2 fuzzy identifier. The second stage
eliminates corrupted pixels by computing the appropriate weight using GMF with
the mean and variance of the uncorrupted pixels in the filter window.
Simulation results vividly show that the obtained denoised images preserve
image features, i.e., edges, corners, and other sharp structures, compared with
different filtering methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05396">Loss it right: Euclidean and Riemannian Metrics in Learning-based Visual Odometry. (arXiv:2401.05396v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alvarez_Tunon_O/0/1/0/all/0/1">Olaya &#xc1;lvarez-Tu&#xf1;&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Brodskiy_Y/0/1/0/all/0/1">Yury Brodskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kayacan_E/0/1/0/all/0/1">Erdal Kayacan</a></p>
<p>This paper overviews different pose representations and metric functions in
visual odometry (VO) networks. The performance of VO networks heavily relies on
how their architecture encodes the information. The choice of pose
representation and loss function significantly impacts network convergence and
generalization. We investigate these factors in the VO network DeepVO by
implementing loss functions based on Euler, quaternion, and chordal distance
and analyzing their influence on performance. The results of this study provide
insights into how loss functions affect the designing of efficient and accurate
VO networks for camera motion estimation. The experiments illustrate that a
distance that complies with the mathematical requirements of a metric, such as
the chordal distance, provides better generalization and faster convergence.
The code for the experiments can be found at
https://github.com/remaro-network/Loss_VO_right
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05401">Domain Similarity-Perceived Label Assignment for Domain Generalized Underwater Object Detection. (arXiv:2401.05401v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xisheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1">Pinhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>The inherent characteristics and light fluctuations of water bodies give rise
to the huge difference between different layers and regions in underwater
environments. When the test set is collected in a different marine area from
the training set, the issue of domain shift emerges, significantly compromising
the model's ability to generalize. The Domain Adversarial Learning (DAL)
training strategy has been previously utilized to tackle such challenges.
However, DAL heavily depends on manually one-hot domain labels, which implies
no difference among the samples in the same domain. Such an assumption results
in the instability of DAL. This paper introduces the concept of Domain
Similarity-Perceived Label Assignment (DSP). The domain label for each image is
regarded as its similarity to the specified domains. Through domain-specific
data augmentation techniques, we achieved state-of-the-art results on the
underwater cross-domain object detection benchmark S-UODAC2020. Furthermore, we
validated the effectiveness of our method in the Cityscapes dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05407">Machine Learning and Feature Ranking for Impact Fall Detection Event Using Multisensor Data. (arXiv:2401.05407v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Koffi_T/0/1/0/all/0/1">Tresor Y. Koffi</a>, <a href="http://arxiv.org/find/eess/1/au:+Mourchid_Y/0/1/0/all/0/1">Youssef Mourchid</a>, <a href="http://arxiv.org/find/eess/1/au:+Hindawi_M/0/1/0/all/0/1">Mohammed Hindawi</a>, <a href="http://arxiv.org/find/eess/1/au:+Dupuis_Y/0/1/0/all/0/1">Yohan Dupuis</a></p>
<p>Falls among individuals, especially the elderly population, can lead to
serious injuries and complications. Detecting impact moments within a fall
event is crucial for providing timely assistance and minimizing the negative
consequences. In this work, we aim to address this challenge by applying
thorough preprocessing techniques to the multisensor dataset, the goal is to
eliminate noise and improve data quality. Furthermore, we employ a feature
selection process to identify the most relevant features derived from the
multisensor UP-FALL dataset, which in turn will enhance the performance and
efficiency of machine learning models. We then evaluate the efficiency of
various machine learning models in detecting the impact moment using the
resulting data information from multiple sensors. Through extensive
experimentation, we assess the accuracy of our approach using various
evaluation metrics. Our results achieve high accuracy rates in impact
detection, showcasing the power of leveraging multisensor data for fall
detection tasks. This highlights the potential of our approach to enhance fall
detection systems and improve the overall safety and well-being of individuals
at risk of falls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05412">Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics. (arXiv:2401.05412v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xueyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1">Chao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ban_X/0/1/0/all/0/1">Xiaojuan Ban</a></p>
<p>Leveraging wearable devices for motion reconstruction has emerged as an
economical and viable technique. Certain methodologies employ sparse Inertial
Measurement Units (IMUs) on the human body and harness data-driven strategies
to model human poses. However, the reconstruction of motion based solely on
sparse IMUs data is inherently fraught with ambiguity, a consequence of
numerous identical IMU readings corresponding to different poses. In this
paper, we explore the spatial importance of multiple sensors, supervised by
text that describes specific actions. Specifically, uncertainty is introduced
to derive weighted features for each IMU. We also design a Hierarchical
Temporal Transformer (HTT) and apply contrastive learning to achieve precise
temporal and feature alignment of sensor data with textual semantics.
Experimental results demonstrate our proposed approach achieves significant
improvements in multiple metrics compared to existing methods. Notably, with
textual supervision, our method not only differentiates between ambiguous
actions such as sitting and standing but also produces more precise and natural
motion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05465">D3GU: Multi-Target Active Domain Adaptation via Enhancing Domain Alignment. (arXiv:2401.05465v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Linghan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Motamed_S/0/1/0/all/0/1">Saman Motamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Shayok Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1">Fernando De la Torre</a></p>
<p>Unsupervised domain adaptation (UDA) for image classification has made
remarkable progress in transferring classification knowledge from a labeled
source domain to an unlabeled target domain, thanks to effective domain
alignment techniques. Recently, in order to further improve performance on a
target domain, many Single-Target Active Domain Adaptation (ST-ADA) methods
have been proposed to identify and annotate the salient and exemplar target
samples. However, it requires one model to be trained and deployed for each
target domain and the domain label associated with each test sample. This
largely restricts its application in the ubiquitous scenarios with multiple
target domains. Therefore, we propose a Multi-Target Active Domain Adaptation
(MT-ADA) framework for image classification, named D3GU, to simultaneously
align different domains and actively select samples from them for annotation.
This is the first research effort in this field to our best knowledge. D3GU
applies Decomposed Domain Discrimination (D3) during training to achieve both
source-target and target-target domain alignments. Then during active sampling,
a Gradient Utility (GU) score is designed to weight every unlabeled target
image by its contribution towards classification and domain alignment tasks,
and is further combined with KMeans clustering to form GU-KMeans for diverse
image sampling. Extensive experiments on three benchmark datasets, Office31,
OfficeHome, and DomainNet, have been conducted to validate consistently
superior performance of D3GU for MT-ADA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05481">Transformer-CNN Fused Architecture for Enhanced Skin Lesion Segmentation. (arXiv:2401.05481v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tiwari_S/0/1/0/all/0/1">Siddharth Tiwari</a></p>
<p>The segmentation of medical images is important for the improvement and
creation of healthcare systems, particularly for early disease detection and
treatment planning. In recent years, the use of convolutional neural networks
(CNNs) and other state-of-the-art methods has greatly advanced medical image
segmentation. However, CNNs have been found to struggle with learning
long-range dependencies and capturing global context due to the limitations of
convolution operations. In this paper, we explore the use of transformers and
CNNs for medical image segmentation and propose a hybrid architecture that
combines the ability of transformers to capture global dependencies with the
ability of CNNs to capture low-level spatial details. We compare various
architectures and configurations and conduct multiple experiments to evaluate
their effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05516">FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields. (arXiv:2401.05516v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">GeonU Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1">Kim Youwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1">Tae-Hyun Oh</a></p>
<p>We present FPRF, a feed-forward photorealistic style transfer method for
large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with
arbitrary, multiple style reference images without additional optimization
while preserving multi-view appearance consistency. Prior arts required tedious
per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF
efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D
neural radiance field, which inherits AdaIN's feed-forward stylization
machinery, supporting arbitrary style reference images. Furthermore, FPRF
supports multi-reference stylization with the semantic correspondence matching
and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also
preserves multi-view consistency by applying semantic matching and style
transfer processes directly onto queried features in 3D space. In experiments,
we demonstrate that FPRF achieves favorable photorealistic quality 3D scene
stylization for large-scale scenes with diverse reference images. Project page:
https://kim-geonu.github.io/FPRF/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05520">From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\&#x27;ucho Heritage. (arXiv:2401.05520v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amadeus_M/0/1/0/all/0/1">Marcellus Amadeus</a>, <a href="http://arxiv.org/find/cs/1/au:+Castaneda_W/0/1/0/all/0/1">William Alberto Cruz Casta&#xf1;eda</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1">Andr&#xe9; Felipe Zanella</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahlow_F/0/1/0/all/0/1">Felipe Rodrigues Perche Mahlow</a></p>
<p>Generative AI has become pervasive in society, witnessing significant
advancements in various domains. Particularly in the realm of Text-to-Image
(TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities
in generating visual content based on textual prompts. This paper addresses the
potential of LDMs in representing local cultural concepts, historical figures,
and endangered species. In this study, we use the cultural heritage of Rio
Grande do Sul (RS), Brazil, as an illustrative case. Our objective is to
contribute to the broader understanding of how generative models can help to
capture and preserve the cultural and historical identity of regions. The paper
outlines the methodology, including subject selection, dataset creation, and
the fine-tuning process. The results showcase the images generated, alongside
the challenges and feasibility of each concept. In conclusion, this work shows
the power of these models to represent and preserve unique aspects of diverse
regions and communities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05530">Consensus Focus for Object Detection and minority classes. (arXiv:2401.05530v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salgado_E/0/1/0/all/0/1">Erik Isai Valle Salgado</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yaqi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Linchao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinghui Li</a></p>
<p>Ensemble methods exploit the availability of a given number of classifiers or
detectors trained in single or multiple source domains and tasks to address
machine learning problems such as domain adaptation or multi-source transfer
learning. Existing research measures the domain distance between the sources
and the target dataset, trains multiple networks on the same data with
different samples per class, or combines predictions from models trained under
varied hyperparameters and settings. Their solutions enhanced the performance
on small or tail categories but hurt the rest. To this end, we propose a
modified consensus focus for semi-supervised and long-tailed object detection.
We introduce a voting system based on source confidence that spots the
contribution of each model in a consensus, lets the user choose the relevance
of each class in the target label space so that it relaxes minority bounding
boxes suppression, and combines multiple models' results without discarding the
poisonous networks. Our tests on synthetic driving datasets retrieved higher
confidence and more accurate bounding boxes than the NMS, soft-NMS, and WBF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05570">Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms. (arXiv:2401.05570v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vorst_K/0/1/0/all/0/1">Kevin Van Vorst</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a></p>
<p>Self-supervised learning has become a popular way to pretrain a deep learning
model and then transfer it to perform downstream tasks. However, most of these
methods are developed on large-scale image datasets that contain natural
objects with clear textures, outlines, and distinct color contrasts. It remains
uncertain whether these methods are equally effective for medical imaging,
where the regions of interest often blend subtly and indistinctly with the
surrounding tissues. In this study, we propose an alternative method that uses
contralateral mammograms to train a neural network to encode similar embeddings
when a pair contains both normal images and different embeddings when a pair
contains normal and abnormal images. Our approach leverages the natural
symmetry of human body as weak labels to learn to distinguish abnormal lesions
from background tissues in a fully unsupervised manner. Our findings suggest
that it's feasible by incorporating soft labels derived from the Euclidean
distances between the embeddings of the image pairs into the Siamese network
loss. Our method demonstrates superior performance in mammogram patch
classification compared to existing self-supervised learning methods. This
approach not only leverages a vast amount of image data effectively but also
minimizes reliance on costly labels, a significant advantage particularly in
the field of medical imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05577">VLP: Vision Language Planning for Autonomous Driving. (arXiv:2401.05577v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chenbin Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaman_B/0/1/0/all/0/1">Burhaneddin Yaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Nesti_T/0/1/0/all/0/1">Tommaso Nesti</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallik_A/0/1/0/all/0/1">Abhirup Mallik</a>, <a href="http://arxiv.org/find/cs/1/au:+Allievi_A/0/1/0/all/0/1">Alessandro G Allievi</a>, <a href="http://arxiv.org/find/cs/1/au:+Velipasalar_S/0/1/0/all/0/1">Senem Velipasalar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1">Liu Ren</a></p>
<p>Autonomous driving is a complex and challenging task that aims at safe motion
planning through scene understanding and reasoning. While vision-only
autonomous driving methods have recently achieved notable performance, through
enhanced scene understanding, several key issues, including lack of reasoning,
low generalization performance and long-tail scenarios, still need to be
addressed. In this paper, we present VLP, a novel Vision-Language-Planning
framework that exploits language models to bridge the gap between linguistic
understanding and autonomous driving. VLP enhances autonomous driving systems
by strengthening both the source memory foundation and the self-driving car's
contextual understanding. VLP achieves state-of-the-art end-to-end planning
performance on the challenging NuScenes dataset by achieving 35.9\% and 60.5\%
reduction in terms of average L2 error and collision rates, respectively,
compared to the previous best method. Moreover, VLP shows improved performance
in challenging long-tail scenarios and strong generalization capabilities when
faced with new urban environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05583">Diffusion Priors for Dynamic View Synthesis from Monocular Videos. (arXiv:2401.05583v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1">Peiye Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1">Aliaksandr Siarohin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Junli Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1">Guocheng Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hsin-Ying Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a></p>
<p>Dynamic novel view synthesis aims to capture the temporal evolution of visual
content within videos. Existing methods struggle to distinguishing between
motion and structure, particularly in scenarios where camera poses are either
unknown or constrained compared to object motion. Furthermore, with information
solely from reference images, it is extremely challenging to hallucinate unseen
regions that are occluded or partially observed in the given videos. To address
these issues, we first finetune a pretrained RGB-D diffusion model on the video
frames using a customization technique. Subsequently, we distill the knowledge
from the finetuned model to a 4D representations encompassing both dynamic and
static Neural Radiance Fields (NeRF) components. The proposed pipeline achieves
geometric consistency while preserving the scene identity. We perform thorough
experiments to evaluate the efficacy of the proposed method qualitatively and
quantitatively. Our results demonstrate the robustness and utility of our
approach in challenging cases, further advancing dynamic novel view synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05584">FourCastNeXt: Improving FourCastNet Training with Limited Compute. (arXiv:2401.05584v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1">Edison Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Maruf Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yue Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahendru_R/0/1/0/all/0/1">Rahul Mahendru</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Rui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cook_H/0/1/0/all/0/1">Harrison Cook</a>, <a href="http://arxiv.org/find/cs/1/au:+Leeuwenburg_T/0/1/0/all/0/1">Tennessee Leeuwenburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1">Ben Evans</a></p>
<p>Recently, the FourCastNet Neural Earth System Model (NESM) has shown
impressive results on predicting various atmospheric variables, trained on the
ERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memory
complexity in sequence length compared to quadratic complexity in vanilla
transformers, training FourCastNet on ERA5 from scratch still requires large
amount of compute resources, which is expensive or even inaccessible to most
researchers. In this work, we will show improved methods that can train
FourCastNet using only 1% of the compute required by the baseline, while
maintaining model performance or par or even better than the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05593">Reverse Projection: Real-Time Local Space Texture Mapping. (arXiv:2401.05593v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1">Adrian Xuan Wei Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1">Lynnette Hui Xian Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffin_C/0/1/0/all/0/1">Conor Griffin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kyger_N/0/1/0/all/0/1">Nicholas Kyger</a>, <a href="http://arxiv.org/find/cs/1/au:+Baghernezhad_F/0/1/0/all/0/1">Faraz Baghernezhad</a></p>
<p>We present Reverse Projection, a novel projective texture mapping technique
for painting a decal directly to the texture of a 3D object. Designed to be
used in games, this technique works in real-time. By using projection
techniques that are computed in local space textures and outward-looking, users
using low-end android devices to high-end gaming desktops are able to enjoy the
personalization of their assets. We believe our proposed pipeline is a step in
improving the speed and versatility of model painting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05594">Wasserstein Distance-based Expansion of Low-Density Latent Regions for Unknown Class Detection. (arXiv:2401.05594v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mallick_P/0/1/0/all/0/1">Prakash Mallick</a>, <a href="http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1">Feras Dayoub</a>, <a href="http://arxiv.org/find/cs/1/au:+Sherrah_J/0/1/0/all/0/1">Jamie Sherrah</a></p>
<p>This paper addresses the significant challenge in open-set object detection
(OSOD): the tendency of state-of-the-art detectors to erroneously classify
unknown objects as known categories with high confidence. We present a novel
approach that effectively identifies unknown objects by distinguishing between
high and low-density regions in latent space. Our method builds upon the
Open-Det (OD) framework, introducing two new elements to the loss function.
These elements enhance the known embedding space's clustering and expand the
unknown space's low-density regions. The first addition is the Class
Wasserstein Anchor (CWA), a new function that refines the classification
boundaries. The second is a spectral normalisation step, improving the
robustness of the model. Together, these augmentations to the existing
Contrastive Feature Learner (CFL) and Unknown Probability Learner (UPL) loss
functions significantly improve OSOD performance. Our proposed OpenDet-CWA
(OD-CWA) method demonstrates: a) a reduction in open-set errors by
approximately 17%-22%, b) an enhancement in novelty detection capability by
1.5%-16%, and c) a decrease in the wilderness index by 2%-20% across various
open-set scenarios. These results represent a substantial advancement in the
field, showcasing the potential of our approach in managing the complexities of
open-set object detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05602">Nucleus subtype classification using inter-modality learning. (arXiv:2401.05602v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Remedios_L/0/1/0/all/0/1">Lucas W. Remedios</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1">Shunxing Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Remedios_S/0/1/0/all/0/1">Samuel W. Remedios</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Ho Hin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1">Leon Y. Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Thomas Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1">Ruining Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Can Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_K/0/1/0/all/0/1">Ken S. Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Roland_J/0/1/0/all/0/1">Joseph T. Roland</a>, <a href="http://arxiv.org/find/cs/1/au:+Washington_M/0/1/0/all/0/1">Mary K. Washington</a>, <a href="http://arxiv.org/find/cs/1/au:+Coburn_L/0/1/0/all/0/1">Lori A. Coburn</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_K/0/1/0/all/0/1">Keith T. Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1">Yuankai Huo</a>, <a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1">Bennett A. Landman</a></p>
<p>Understanding the way cells communicate, co-locate, and interrelate is
essential to understanding human physiology. Hematoxylin and eosin (H&amp;E)
staining is ubiquitously available both for clinical studies and research. The
Colon Nucleus Identification and Classification (CoNIC) Challenge has recently
innovated on robust artificial intelligence labeling of six cell types on H&amp;E
stains of the colon. However, this is a very small fraction of the number of
potential cell classification types. Specifically, the CoNIC Challenge is
unable to classify epithelial subtypes (progenitor, endocrine, goblet),
lymphocyte subtypes (B, helper T, cytotoxic T), or connective subtypes
(fibroblasts, stromal). In this paper, we propose to use inter-modality
learning to label previously un-labelable cell types on virtual H&amp;E. We
leveraged multiplexed immunofluorescence (MxIF) histology imaging to identify
14 subclasses of cell types. We performed style transfer to synthesize virtual
H&amp;E from MxIF and transferred the higher density labels from MxIF to these
virtual H&amp;E images. We then evaluated the efficacy of learning in this
approach. We identified helper T and progenitor nuclei with positive predictive
values of $0.34 \pm 0.15$ (prevalence $0.03 \pm 0.01$) and $0.47 \pm 0.1$
(prevalence $0.07 \pm 0.02$) respectively on virtual H&amp;E. This approach
represents a promising step towards automating annotation in digital pathology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05604">REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gritsevskiy_A/0/1/0/all/0/1">Andrew Gritsevskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Panickssery_A/0/1/0/all/0/1">Arjun Panickssery</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirtland_A/0/1/0/all/0/1">Aaron Kirtland</a>, <a href="http://arxiv.org/find/cs/1/au:+Kauffman_D/0/1/0/all/0/1">Derik Kauffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Gundlach_H/0/1/0/all/0/1">Hans Gundlach</a>, <a href="http://arxiv.org/find/cs/1/au:+Gritsevskaya_I/0/1/0/all/0/1">Irina Gritsevskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavanagh_J/0/1/0/all/0/1">Joe Cavanagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1">Jonathan Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Roux_L/0/1/0/all/0/1">Lydia La Roux</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_M/0/1/0/all/0/1">Michelle Hung</a></p>
<p>We propose a new benchmark evaluating the performance of multimodal large
language models on rebus puzzles. The dataset covers 333 original examples of
image-based wordplay, cluing 13 categories such as movies, composers, major
cities, and food. To achieve good performance on the benchmark of identifying
the clued word or phrase, models must combine image recognition and string
manipulation with hypothesis testing, multi-step reasoning, and an
understanding of human cognition, making for a complex, multimodal evaluation
of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro
significantly outperform all other tested models. However, even the best model
has a final accuracy of just 24%, highlighting the need for substantial
improvements in reasoning. Further, models rarely understand all parts of a
puzzle, and are almost always incapable of retroactively explaining the correct
answer. Our benchmark can therefore be used to identify major shortcomings in
the knowledge and reasoning of multimodal large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05625">Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle Dynamics in Videos. (arXiv:2401.05625v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Juni Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhikang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1">Pawel Polak</a></p>
<p>We introduce a novel method that combines differential geometry, kernels
smoothing, and spectral analysis to quantify facial muscle activity from widely
accessible video recordings, such as those captured on personal smartphones.
Our approach emphasizes practicality and accessibility. It has significant
potential for applications in national security and plastic surgery.
Additionally, it offers remote diagnosis and monitoring for medical conditions
such as stroke, Bell's palsy, and acoustic neuroma. Moreover, it is adept at
detecting and classifying emotions, from the overt to the subtle. The proposed
face muscle analysis technique is an explainable alternative to deep learning
methods and a non-invasive substitute to facial electromyography (fEMG).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05633">Transforming Image Super-Resolution: A ConvFormer-based Efficient Approach. (arXiv:2401.05633v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Gang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junjun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junpeng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianming Liu</a></p>
<p>Recent progress in single-image super-resolution (SISR) has achieved
remarkable performance, yet the computational costs of these methods remain a
challenge for deployment on resource-constrained devices. Especially for
transformer-based methods, the self-attention mechanism in such models brings
great breakthroughs while incurring substantial computational costs. To tackle
this issue, we introduce the Convolutional Transformer layer (ConvFormer) and
the ConvFormer-based Super-Resolution network (CFSR), which offer an effective
and efficient solution for lightweight image super-resolution tasks. In detail,
CFSR leverages the large kernel convolution as the feature mixer to replace the
self-attention module, efficiently modeling long-range dependencies and
extensive receptive fields with a slight computational cost. Furthermore, we
propose an edge-preserving feed-forward network, simplified as EFN, to obtain
local feature aggregation and simultaneously preserve more high-frequency
information. Extensive experiments demonstrate that CFSR can achieve an
advanced trade-off between computational cost and performance when compared to
existing lightweight SR methods. Compared to state-of-the-art methods, e.g.
ShuffleMixer, the proposed CFSR achieves 0.39 dB gains on Urban100 dataset for
x2 SR task while containing 26% and 31% fewer parameters and FLOPs,
respectively. Code and pre-trained models are available at
https://github.com/Aitical/CFSR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05638">MatSAM: Efficient Materials Microstructure Extraction via Visual Large Model. (arXiv:2401.05638v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changtai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1">Chao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ban_X/0/1/0/all/0/1">Xiaojuan Ban</a></p>
<p>Accurate and efficient extraction of microstructures in microscopic images of
materials plays a critical role in the exploration of structure-property
relationships and the optimization of process parameters. Deep learning-based
image segmentation techniques that rely on manual annotation are time-consuming
and labor-intensive and hardly meet the demand for model transferability and
generalization. Segment Anything Model (SAM), a large visual model with
powerful deep feature representation and zero-shot generalization capabilities,
has provided new solutions for image segmentation. However, directly applying
SAM to segmenting microstructures in microscopic images of materials without
human annotation cannot achieve the expected results, as the difficulty of
adapting its native prompt engineering to the dense and dispersed
characteristics of key microstructures in materials microscopy images. In this
paper, we propose MatSAM, a general and efficient microstructure extraction
solution based on SAM. A new point-based prompts generation strategy is
designed, grounded on the distribution and shape of materials microstructures.
It generates prompts for different microscopic images, fuses the prompts of the
region of interest (ROI) key points and grid key points, and integrates
post-processing methods for quantitative characterization of materials
microstructures. For common microstructures including grain boundary and phase,
MatSAM achieves superior segmentation performance to conventional methods and
is even preferable to supervised learning methods evaluated on 18 materials
microstructures imaged by the optical microscope (OM) and scanning electron
microscope (SEM). We believe that MatSAM can significantly reduce the cost of
quantitative characterization of materials microstructures and accelerate the
design of new materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05646">Masked Attribute Description Embedding for Cloth-Changing Person Re-identification. (arXiv:2401.05646v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chunlei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Decheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Ruimin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>Cloth-changing person re-identification (CC-ReID) aims to match persons who
change clothes over long periods. The key challenge in CC-ReID is to extract
clothing-independent features, such as face, hairstyle, body shape, and gait.
Current research mainly focuses on modeling body shape using multi-modal
biological features (such as silhouettes and sketches). However, it does not
fully leverage the personal description information hidden in the original RGB
image. Considering that there are certain attribute descriptions which remain
unchanged after the changing of cloth, we propose a Masked Attribute
Description Embedding (MADE) method that unifies personal visual appearance and
attribute description for CC-ReID. Specifically, handling variable
clothing-sensitive information, such as color and type, is challenging for
effective modeling. To address this, we mask the clothing and color information
in the personal attribute description extracted through an attribute detection
model. The masked attribute description is then connected and embedded into
Transformer blocks at various levels, fusing it with the low-level to
high-level features of the image. This approach compels the model to discard
clothing information. Experiments are conducted on several CC-ReID benchmarks,
including PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE
effectively utilizes attribute description, enhancing cloth-changing person
re-identification performance, and compares favorably with state-of-the-art
methods. The code is available at https://github.com/moon-wh/MADE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05675">Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation. (arXiv:2401.05675v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seung Hyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1">Junjie Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_I/0/1/0/all/0/1">Innfarn Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiahui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1">Fei Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Entis_G/0/1/0/all/0/1">Glenn Entis</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junfeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangpil Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1">Irfan Essa</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Feng Yang</a></p>
<p>Recent works demonstrate that using reinforcement learning (RL) with quality
rewards can enhance the quality of generated images in text-to-image (T2I)
generation. However, a simple aggregation of multiple rewards may cause
over-optimization in certain metrics and degradation in others, and it is
challenging to manually find the optimal weights. An effective strategy to
jointly optimize multiple rewards in RL for T2I generation is highly desirable.
This paper introduces Parrot, a novel multi-reward RL framework for T2I
generation. Through the use of the batch-wise Pareto optimal selection, Parrot
automatically identifies the optimal trade-off among different rewards during
the RL optimization of the T2I generation. Additionally, Parrot employs a joint
optimization approach for the T2I model and the prompt expansion network,
facilitating the generation of quality-aware text prompts, thus further
enhancing the final image quality. To counteract the potential catastrophic
forgetting of the original user prompt due to prompt expansion, we introduce
original prompt centered guidance at inference time, ensuring that the
generated image remains faithful to the user input. Extensive experiments and a
user study demonstrate that Parrot outperforms several baseline methods across
various quality criteria, including aesthetics, human preference, image
sentiment, and text-image alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05676">Exploring Self- and Cross-Triplet Correlations for Human-Object Interaction Detection. (arXiv:2401.05676v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Weibo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Weihong Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Jiandong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Liangqiong Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Honghai Liu</a></p>
<p>Human-Object Interaction (HOI) detection plays a vital role in scene
understanding, which aims to predict the HOI triplet in the form of &lt;human,
object, action&gt;. Existing methods mainly extract multi-modal features (e.g.,
appearance, object semantics, human pose) and then fuse them together to
directly predict HOI triplets. However, most of these methods focus on seeking
for self-triplet aggregation, but ignore the potential cross-triplet
dependencies, resulting in ambiguity of action prediction. In this work, we
propose to explore Self- and Cross-Triplet Correlations (SCTC) for HOI
detection. Specifically, we regard each triplet proposal as a graph where
Human, Object represent nodes and Action indicates edge, to aggregate
self-triplet correlation. Also, we try to explore cross-triplet dependencies by
jointly considering instance-level, semantic-level, and layout-level relations.
Besides, we leverage the CLIP model to assist our SCTC obtain interaction-aware
feature by knowledge distillation, which provides useful action clues for HOI
detection. Extensive experiments on HICO-DET and V-COCO datasets verify the
effectiveness of our proposed SCTC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05686">Self Expanding Convolutional Neural Networks. (arXiv:2401.05686v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Appolinary_B/0/1/0/all/0/1">Blaise Appolinary</a>, <a href="http://arxiv.org/find/cs/1/au:+Deaconu_A/0/1/0/all/0/1">Alex Deaconu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sophia Yang</a></p>
<p>In this paper, we present a novel method for dynamically expanding
Convolutional Neural Networks (CNNs) during training, aimed at meeting the
increasing demand for efficient and sustainable deep learning models. Our
approach, drawing from the seminal work on Self-Expanding Neural Networks
(SENN), employs a natural expansion score as an expansion criteria to address
the common issue of over-parameterization in deep convolutional neural
networks, thereby ensuring that the model's complexity is finely tuned to the
task's specific needs. A significant benefit of this method is its eco-friendly
nature, as it obviates the necessity of training multiple models of different
sizes. We employ a strategy where a single model is dynamically expanded,
facilitating the extraction of checkpoints at various complexity levels,
effectively reducing computational resource use and energy consumption while
also expediting the development cycle by offering diverse model complexities
from a single training session. We evaluate our method on the CIFAR-10 dataset
and our experimental results validate this approach, demonstrating that
dynamically adding layers not only maintains but also improves CNN performance,
underscoring the effectiveness of our expansion criteria. This approach marks a
considerable advancement in developing adaptive, scalable, and environmentally
considerate neural network architectures, addressing key challenges in the
field of deep learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05698">HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition. (arXiv:2401.05698v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Licai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zheng Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jianhua Tao</a></p>
<p>Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in
recent years for its critical role in creating emotion-ware intelligent
machines. Previous efforts in this area are dominated by the supervised
learning paradigm. Despite significant progress, supervised learning is meeting
its bottleneck due to the longstanding data scarcity issue in AVER. Motivated
by recent advances in self-supervised learning, we propose Hierarchical
Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that
leverages large-scale self-supervised pre-training on vast unlabeled
audio-visual data to promote the advancement of AVER. Following prior arts in
self-supervised audio-visual representation learning, HiCMAE adopts two primary
forms of self-supervision for pre-training, namely masked data modeling and
contrastive learning. Unlike them which focus exclusively on top-layer
representations while neglecting explicit guidance of intermediate layers,
HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual
feature learning and improve the overall quality of learned representations. To
verify the effectiveness of HiCMAE, we conduct extensive experiments on 9
datasets covering both categorical and dimensional AVER tasks. Experimental
results show that our method significantly outperforms state-of-the-art
supervised and self-supervised audio-visual methods, which indicates that
HiCMAE is a powerful audio-visual emotion representation learner. Codes and
models will be publicly available at https://github.com/sunlicai/HiCMAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05702">Video Anomaly Detection and Explanation via Large Language Models. (arXiv:2401.05702v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1">Hui Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qianru Sun</a></p>
<p>Video Anomaly Detection (VAD) aims to localize abnormal events on the
timeline of long-range surveillance videos. Anomaly-scoring-based methods have
been prevailing for years but suffer from the high complexity of thresholding
and low explanability of detection results. In this paper, we conduct pioneer
research on equipping video-based large language models (VLLMs) in the
framework of VAD, making the VAD model free from thresholds and able to explain
the reasons for the detected anomalies. We introduce a novel network module
Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range
context modeling. We design a three-phase training method to improve the
efficiency of fine-tuning VLLMs by substantially minimizing the requirements
for VAD data and lowering the costs of annotating instruction-tuning data. Our
trained model achieves the top performance on the anomaly videos of the
UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\% and +4.96\%,
respectively. More impressively, our approach can provide textual explanations
for detected anomalies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05730">Enhancing Contrastive Learning with Efficient Combinatorial Positive Pairing. (arXiv:2401.05730v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jaeill Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1">Duhun Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Eunjung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1">Jangwon Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jimyeong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1">Wonjong Rhee</a></p>
<p>In the past few years, contrastive learning has played a central role for the
success of visual unsupervised representation learning. Around the same time,
high-performance non-contrastive learning methods have been developed as well.
While most of the works utilize only two views, we carefully review the
existing multi-view methods and propose a general multi-view strategy that can
improve learning speed and performance of any contrastive or non-contrastive
method. We first analyze CMC's full-graph paradigm and empirically show that
the learning speed of $K$-views can be increased by $_{K}\mathrm{C}_{2}$ times
for small learning rate and early training. Then, we upgrade CMC's full-graph
by mixing views created by a crop-only augmentation, adopting small-size views
as in SwAV multi-crop, and modifying the negative sampling. The resulting
multi-view strategy is called ECPP (Efficient Combinatorial Positive Pairing).
We investigate the effectiveness of ECPP by applying it to SimCLR and assessing
the linear evaluation performance for CIFAR-10 and ImageNet-100. For each
benchmark, we achieve a state-of-the-art performance. In case of ImageNet-100,
ECPP boosted SimCLR outperforms supervised learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05735">Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kahatapitiya_K/0/1/0/all/0/1">Kumara Kahatapitiya</a>, <a href="http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1">Adil Karjauv</a>, <a href="http://arxiv.org/find/cs/1/au:+Abati_D/0/1/0/all/0/1">Davide Abati</a>, <a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1">Fatih Porikli</a>, <a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1">Yuki M. Asano</a>, <a href="http://arxiv.org/find/cs/1/au:+Habibian_A/0/1/0/all/0/1">Amirhossein Habibian</a></p>
<p>Diffusion-based video editing have reached impressive quality and can
transform either the global style, local structure, and attributes of given
video inputs, following textual edit prompts. However, such solutions typically
incur heavy memory and computational costs to generate temporally-coherent
frames, either in the form of diffusion inversion and/or cross-frame attention.
In this paper, we conduct an analysis of such inefficiencies, and suggest
simple yet effective modifications that allow significant speed-ups whilst
maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as
OCD, to further reduce latency by allocating computations more towards
foreground edited regions that are arguably more important for perceptual
quality. We achieve this by two novel proposals: i) Object-Centric Sampling,
decoupling the diffusion steps spent on salient regions or background,
allocating most of the model capacity to the former, and ii) Object-Centric 3D
Token Merging, which reduces cost of cross-frame attention by fusing redundant
tokens in unimportant background regions. Both techniques are readily
applicable to a given video editing model \textit{without} retraining, and can
drastically reduce its memory and computational cost. We evaluate our proposals
on inversion-based and control-signal-based editing pipelines, and show a
latency reduction up to 10x for a comparable synthesis quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05738">LKCA: Large Kernel Convolutional Attention. (arXiv:2401.05738v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenghao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1">Boheng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1">Pengbo Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingzi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jirui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lingyun Zhu</a></p>
<p>We revisit the relationship between attention mechanisms and large kernel
ConvNets in visual transformers and propose a new spatial attention named Large
Kernel Convolutional Attention (LKCA). It simplifies the attention operation by
replacing it with a single large kernel convolution. LKCA combines the
advantages of convolutional neural networks and visual transformers, possessing
a large receptive field, locality, and parameter sharing. We explained the
superiority of LKCA from both convolution and attention perspectives, providing
equivalent code implementations for each view. Experiments confirm that LKCA
implemented from both the convolutional and attention perspectives exhibit
equivalent performance. We extensively experimented with the LKCA variant of
ViT in both classification and segmentation tasks. The experiments demonstrated
that LKCA exhibits competitive performance in visual tasks. Our code will be
made publicly available at https://github.com/CatworldLee/LKCA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05745">Surface Normal Estimation with Transformers. (arXiv:2401.05745v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Barry Shichen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Siyun Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Paetzold_J/0/1/0/all/0/1">Johannes Paetzold</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1">Isao Echizen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiapeng Tang</a></p>
<p>We propose the use of a Transformer to accurately predict normals from point
clouds with noise and density variations. Previous learning-based methods
utilize PointNet variants to explicitly extract multi-scale features at
different input scales, then focus on a surface fitting method by which local
point cloud neighborhoods are fitted to a geometric surface approximated by
either a polynomial function or a multi-layer perceptron (MLP). However,
fitting surfaces to fixed-order polynomial functions can suffer from
overfitting or underfitting, and learning MLP-represented hyper-surfaces
requires pre-generated per-point weights. To avoid these limitations, we first
unify the design choices in previous works and then propose a simplified
Transformer-based model to extract richer and more robust geometric features
for the surface normal estimation task. Through extensive experiments, we
demonstrate that our Transformer-based method achieves state-of-the-art
performance on both the synthetic shape dataset PCPNet, and the real-world
indoor scene dataset SceneNN, exhibiting more noise-resilient behavior and
significantly faster inference. Most importantly, we demonstrate that the
sophisticated hand-designed modules in existing works are not necessary to
excel at the task of surface normal estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05750">GO-NeRF: Generating Virtual Objects in Neural Radiance Fields. (arXiv:2401.05750v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1">Peng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1">Feitong Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinda Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiaojuan Qi</a></p>
<p>Despite advances in 3D generation, the direct creation of 3D objects within
an existing 3D scene represented as NeRF remains underexplored. This process
requires not only high-quality 3D object generation but also seamless
composition of the generated 3D content into the existing NeRF. To this end, we
propose a new method, GO-NeRF, capable of utilizing scene context for
high-quality and harmonious 3D object generation within an existing NeRF. Our
method employs a compositional rendering formulation that allows the generated
3D objects to be seamlessly composited into the scene utilizing learned
3D-aware opacity maps without introducing unintended scene modification.
Moreover, we also develop tailored optimization objectives and training
strategies to enhance the model's ability to exploit scene context and mitigate
artifacts, such as floaters, originating from 3D object generation within a
scene. Extensive experiments on both feed-forward and $360^o$ scenes show the
superior performance of our proposed GO-NeRF in generating objects harmoniously
composited with surrounding scenes and synthesizing high-quality novel view
images. Project page at {\url{https://daipengwa.github.io/GO-NeRF/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05752">Learning Generalizable Models via Disentangling Spurious and Enhancing Potential Correlations. (arXiv:2401.05752v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Na Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lei Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jintao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yinghuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a></p>
<p>Domain generalization (DG) intends to train a model on multiple source
domains to ensure that it can generalize well to an arbitrary unseen target
domain. The acquisition of domain-invariant representations is pivotal for DG
as they possess the ability to capture the inherent semantic information of the
data, mitigate the influence of domain shift, and enhance the generalization
capability of the model. Adopting multiple perspectives, such as the sample and
the feature, proves to be effective. The sample perspective facilitates data
augmentation through data manipulation techniques, whereas the feature
perspective enables the extraction of meaningful generalization features. In
this paper, we focus on improving the generalization ability of the model by
compelling it to acquire domain-invariant representations from both the sample
and feature perspectives by disentangling spurious correlations and enhancing
potential correlations. 1) From the sample perspective, we develop a frequency
restriction module, guiding the model to focus on the relevant correlations
between object features and labels, thereby disentangling spurious
correlations. 2) From the feature perspective, the simple Tail Interaction
module implicitly enhances potential correlations among all samples from all
source domains, facilitating the acquisition of domain-invariant
representations across multiple domains for the model. The experimental results
show that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons
(MLPs) with a strong baseline embedded with these two modules can achieve
superior results, e.g., an average accuracy of 92.30% on Digits-DG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05768">Evaluating Data Augmentation Techniques for Coffee Leaf Disease Classification. (arXiv:2401.05768v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gheorghiu_A/0/1/0/all/0/1">Adrian Gheorghiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Taiatu_I/0/1/0/all/0/1">Iulian-Marius T&#x103;iatu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1">Dumitru-Clementin Cercel</a>, <a href="http://arxiv.org/find/cs/1/au:+Marin_I/0/1/0/all/0/1">Iuliana Marin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pop_F/0/1/0/all/0/1">Florin Pop</a></p>
<p>The detection and classification of diseases in Robusta coffee leaves are
essential to ensure that plants are healthy and the crop yield is kept high.
However, this job requires extensive botanical knowledge and much wasted time.
Therefore, this task and others similar to it have been extensively researched
subjects in image classification. Regarding leaf disease classification, most
approaches have used the more popular PlantVillage dataset while completely
disregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset. As
the RoCoLe dataset is imbalanced and does not have many samples, fine-tuning of
pre-trained models and multiple augmentation techniques need to be used. The
current paper uses the RoCoLe dataset and approaches based on deep learning for
classifying coffee leaf diseases from images, incorporating the pix2pix model
for segmentation and cycle-generative adversarial network (CycleGAN) for
augmentation. Our study demonstrates the effectiveness of Transformer-based
models, online augmentations, and CycleGAN augmentation in improving leaf
disease classification. While synthetic data has limitations, it complements
real data, enhancing model performance. These findings contribute to developing
robust techniques for plant disease detection and classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05771">Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image Classification. (arXiv:2401.05771v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1">Kunpeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhiying Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yongxin Guo</a></p>
<p>Accurate lesion classification in Wireless Capsule Endoscopy (WCE) images is
vital for early diagnosis and treatment of gastrointestinal (GI) cancers.
However, this task is confronted with challenges like tiny lesions and
background interference. Additionally, WCE images exhibit higher intra-class
variance and inter-class similarities, adding complexity. To tackle these
challenges, we propose Decoupled Supervised Contrastive Learning for WCE image
classification, learning robust representations from zoomed-in WCE images
generated by Saliency Augmentor. Specifically, We use uniformly down-sampled
WCE images as anchors and WCE images from the same class, especially their
zoomed-in images, as positives. This approach empowers the Feature Extractor to
capture rich representations from various views of the same image, facilitated
by Decoupled Supervised Contrastive Learning. Training a linear Classifier on
these representations within 10 epochs yields an impressive 92.01% overall
accuracy, surpassing the prior state-of-the-art (SOTA) by 0.72% on a blend of
two publicly accessible WCE datasets. Code is available at:
https://github.com/Qiukunpeng/DSCL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05772">Knowledge Translation: A New Pathway for Model Compression. (arXiv:2401.05772v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wujie Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Defang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Can Wang</a></p>
<p>Deep learning has witnessed significant advancements in recent years at the
cost of increasing training, inference, and model storage overhead. While
existing model compression methods strive to reduce the number of model
parameters while maintaining high accuracy, they inevitably necessitate the
re-training of the compressed model or impose architectural constraints. To
overcome these limitations, this paper presents a novel framework, termed
\textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model
is trained to receive the parameters of a larger model and generate compressed
parameters. The concept of KT draws inspiration from language translation,
which effectively employs neural networks to convert different languages,
maintaining identical meaning. Accordingly, we explore the potential of neural
networks to convert models of disparate sizes, while preserving their
functionality. We propose a comprehensive framework for KT, introduce data
augmentation strategies to enhance model performance despite restricted
training data, and successfully demonstrate the feasibility of KT on the MNIST
dataset. Code is available at \url{https://github.com/zju-SWJ/KT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05779">EraseDiff: Erasing Data Influence in Diffusion Models. (arXiv:2401.05779v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Trung Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1">Munawar Hayat</a>, <a href="http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1">Mehrtash Harandi</a></p>
<p>In response to data protection regulations and the ``right to be forgotten'',
in this work, we introduce an unlearning algorithm for diffusion models. Our
algorithm equips a diffusion model with a mechanism to mitigate the concerns
related to data memorization. To achieve this, we formulate the unlearning
problem as a bi-level optimization problem, wherein the outer objective is to
preserve the utility of the diffusion model on the remaining data. The inner
objective aims to scrub the information associated with forgetting data by
deviating the learnable generative process from the ground-truth denoising
procedure. To solve the resulting bi-level problem, we adopt a first-order
method, having superior practical performance while being vigilant about the
diffusion process and solving a bi-level problem therein. Empirically, we
demonstrate that our algorithm can preserve the model utility, effectiveness,
and efficiency while removing across two widely-used diffusion models and in
both conditional and unconditional image generation scenarios. In our
experiments, we demonstrate the unlearning of classes, attributes, and even a
race from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and
CIFAR10.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05806">CLIP-Driven Semantic Discovery Network for Visible-Infrared Person Re-Identification. (arXiv:2401.05806v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiaoyan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1">Neng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liehuang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dapeng Tao</a></p>
<p>Visible-infrared person re-identification (VIReID) primarily deals with
matching identities across person images from different modalities. Due to the
modality gap between visible and infrared images, cross-modality identity
matching poses significant challenges. Recognizing that high-level semantics of
pedestrian appearance, such as gender, shape, and clothing style, remain
consistent across modalities, this paper intends to bridge the modality gap by
infusing visual features with high-level semantics. Given the capability of
CLIP to sense high-level semantic information corresponding to visual
representations, we explore the application of CLIP within the domain of
VIReID. Consequently, we propose a CLIP-Driven Semantic Discovery Network
(CSDN) that consists of Modality-specific Prompt Learner, Semantic Information
Integration (SII), and High-level Semantic Embedding (HSE). Specifically,
considering the diversity stemming from modality discrepancies in language
descriptions, we devise bimodal learnable text tokens to capture
modality-private semantic information for visible and infrared images,
respectively. Additionally, acknowledging the complementary nature of semantic
details across different modalities, we integrate text features from the
bimodal language descriptions to achieve comprehensive semantics. Finally, we
establish a connection between the integrated text features and the visual
features across modalities. This process embed rich high-level semantic
information into visual representations, thereby promoting the modality
invariance of visual representations. The effectiveness and superiority of our
proposed CSDN over existing methods have been substantiated through
experimental evaluations on multiple widely used benchmarks. The code will be
released at \url{https://github.com/nengdong96/CSDN}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05807">On the representation and methodology for wide and short range head pose estimation. (arXiv:2401.05807v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cobo_A/0/1/0/all/0/1">Alejandro Cobo</a>, <a href="http://arxiv.org/find/cs/1/au:+Valle_R/0/1/0/all/0/1">Roberto Valle</a>, <a href="http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1">Jos&#xe9; M. Buenaposada</a>, <a href="http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1">Luis Baumela</a></p>
<p>Head pose estimation (HPE) is a problem of interest in computer vision to
improve the performance of face processing tasks in semi-frontal or profile
settings. Recent applications require the analysis of faces in the full
360{\deg} rotation range. Traditional approaches to solve the semi-frontal and
profile cases are not directly amenable for the full rotation case. In this
paper we analyze the methodology for short- and wide-range HPE and discuss
which representations and metrics are adequate for each case. We show that the
popular Euler angles representation is a good choice for short-range HPE, but
not at extreme rotations. However, the Euler angles' gimbal lock problem
prevents them from being used as a valid metric in any setting. We also revisit
the current cross-data set evaluation methodology and note that the lack of
alignment between the reference systems of the training and test data sets
negatively biases the results of all articles in the literature. We introduce a
procedure to quantify this misalignment and a new methodology for cross-data
set HPE that establishes new, more accurate, SOTA for the 300W-LP|Biwi
benchmark. We also propose a generalization of the geodesic angular distance
metric that enables the construction of a loss that controls the contribution
of each training sample to the optimization of the model. Finally, we introduce
a wide range HPE benchmark based on the CMU Panoptic data set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05820">Implications of Noise in Resistive Memory on Deep Neural Networks for Image Classification. (arXiv:2401.05820v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emonds_Y/0/1/0/all/0/1">Yannick Emonds</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_K/0/1/0/all/0/1">Kai Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Froning_H/0/1/0/all/0/1">Holger Fr&#xf6;ning</a></p>
<p>Resistive memory is a promising alternative to SRAM, but is also an
inherently unstable device that requires substantial effort to ensure correct
read and write operations. To avoid the associated costs in terms of area, time
and energy, the present work is concerned with exploring how much noise in
memory operations can be tolerated by image classification tasks based on
neural networks. We introduce a special noisy operator that mimics the noise in
an exemplary resistive memory unit, explore the resilience of convolutional
neural networks on the CIFAR-10 classification task, and discuss a couple of
countermeasures to improve this resilience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05827">Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinge Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Honghan Wu</a></p>
<p>The recent success of large language and vision models on vision question
answering (VQA), particularly their applications in medicine (Med-VQA), has
shown a great potential of realizing effective visual assistants for
healthcare. However, these models are not extensively tested on the
hallucination phenomenon in clinical settings. Here, we created a hallucination
benchmark of medical images paired with question-answer sets and conducted a
comprehensive evaluation of the state-of-the-art models. The study provides an
in-depth analysis of current models limitations and reveals the effectiveness
of various prompting strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05870">HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models. (arXiv:2401.05870v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanzhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinze Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhongrui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zeke Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1">Lei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xinyan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junjun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Mingming Sun</a></p>
<p>The goal of Arbitrary Style Transfer (AST) is injecting the artistic features
of a style reference into a given image/video. Existing methods usually focus
on pursuing the balance between style and content, whereas ignoring the
significant demand for flexible and customized stylization results and thereby
limiting their practical application. To address this critical issue, a novel
AST approach namely HiCAST is proposed, which is capable of explicitly
customizing the stylization results according to various source of semantic
clues. In the specific, our model is constructed based on Latent Diffusion
Model (LDM) and elaborately designed to absorb content and style instance as
conditions of LDM. It is characterized by introducing of \textit{Style
Adapter}, which allows user to flexibly manipulate the output results by
aligning multi-level style information and intrinsic knowledge in LDM. Lastly,
we further extend our model to perform video AST. A novel learning objective is
leveraged for video diffusion model training, which significantly improve
cross-frame temporal consistency in the premise of maintaining stylization
strength. Qualitative and quantitative comparisons as well as comprehensive
user studies demonstrate that our HiCAST outperforms the existing SoTA methods
in generating visually plausible stylization results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05879">YOIO: You Only Iterate Once by mining and fusing multiple necessary global information in the optical flow estimation. (arXiv:2401.05879v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1">Yu Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Yujuan_T/0/1/0/all/0/1">Tan Yujuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ao_R/0/1/0/all/0/1">Ren Ao</a>, <a href="http://arxiv.org/find/cs/1/au:+Duo_L/0/1/0/all/0/1">Liu Duo</a></p>
<p>Occlusions pose a significant challenge to optical flow algorithms that even
rely on global evidences. We consider an occluded point to be one that is
imaged in the reference frame but not in the next. Estimating the motion of
these points is extremely difficult, particularly in the two-frame setting.
Previous work only used the current frame as the only input, which could not
guarantee providing correct global reference information for occluded points,
and had problems such as long calculation time and poor accuracy in predicting
optical flow at occluded points. To enable both high accuracy and efficiency,
We fully mine and utilize the spatiotemporal information provided by the frame
pair, design a loopback judgment algorithm to ensure that correct global
reference information is obtained, mine multiple necessary global information,
and design an efficient refinement module that fuses these global information.
Specifically, we propose a YOIO framework, which consists of three main
components: an initial flow estimator, a multiple global information extraction
module, and a unified refinement module. We demonstrate that optical flow
estimates in the occluded regions can be significantly improved in only one
iteration without damaging the performance in non-occluded regions. Compared
with GMA, the optical flow prediction accuracy of this method in the occluded
area is improved by more than 10%, and the occ_out area exceeds 15%, while the
calculation time is 27% shorter. This approach, running up to 18.9fps with
436*1024 image resolution, obtains new state-of-the-art results on the
challenging Sintel dataset among all published and unpublished approaches that
can run in real-time, suggesting a new paradigm for accurate and efficient
optical flow estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05891">LiDAR data acquisition and processing for ecology applications. (arXiv:2401.05891v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ciobotari_I/0/1/0/all/0/1">Ion Ciobotari</a>, <a href="http://arxiv.org/find/cs/1/au:+Principe_A/0/1/0/all/0/1">Adriana Pr&#xed;ncipe</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1">Maria Alexandra Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1">Jo&#xe3;o Nuno Silva</a></p>
<p>The collection of ecological data in the field is essential to diagnose,
monitor and manage ecosystems in a sustainable way. Since acquisition of this
information through traditional methods are generally time-consuming, due to
the capability of recording large volumes of data in short time periods,
automation of data acquisition sees a growing trend. Terrestrial laser scanners
(TLS), particularly LiDAR sensors, have been used in ecology, allowing to
reconstruct the 3D structure of vegetation, and thus, infer ecosystem
characteristics based on the spatial variation of the density of points.
However, the low amount of information obtained per beam, lack of data analysis
tools and the high cost of the equipment limit their use. This way, a low-cost
TLS (&lt;10k$) was developed along with data acquisition and processing mechanisms
applicable in two case studies: an urban garden and a target area for
ecological restoration. The orientation of LiDAR was modified to make
observations in the vertical plane and a motor was integrated for its rotation,
enabling the acquisition of 360 degree data with high resolution. Motion and
location sensors were also integrated for automatic error correction and
georeferencing. From the data generated, histograms of point density variation
along the vegetation height were created, where shrub stratum was easily
distinguishable from tree stratum, and maximum tree height and shrub cover were
calculated. These results agreed with the field data, whereby the developed TLS
has proved to be effective in calculating metrics of structural complexity of
vegetation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05901">ConKeD: Multiview contrastive descriptor learning for keypoint-based retinal image registration. (arXiv:2401.05901v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rivas_Villar_D/0/1/0/all/0/1">David Rivas-Villar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hervella_A/0/1/0/all/0/1">&#xc1;lvaro S. Hervella</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouco_J/0/1/0/all/0/1">Jos&#xe9; Rouco</a>, <a href="http://arxiv.org/find/cs/1/au:+Novo_J/0/1/0/all/0/1">Jorge Novo</a></p>
<p>Retinal image registration is of utmost importance due to its wide
applications in medical practice. In this context, we propose ConKeD, a novel
deep learning approach to learn descriptors for retinal image registration. In
contrast to current registration methods, our approach employs a novel
multi-positive multi-negative contrastive learning strategy that enables the
utilization of additional information from the available training samples. This
makes it possible to learn high quality descriptors from limited training data.
To train and evaluate ConKeD, we combine these descriptors with domain-specific
keypoints, particularly blood vessel bifurcations and crossovers, that are
detected using a deep neural network. Our experimental results demonstrate the
benefits of the novel multi-positive multi-negative strategy, as it outperforms
the widely used triplet loss technique (single-positive and single-negative) as
well as the single-positive multi-negative alternative. Additionally, the
combination of ConKeD with the domain-specific keypoints produces comparable
results to the state-of-the-art methods for retinal image registration, while
offering important advantages such as avoiding pre-processing, utilizing fewer
training samples, and requiring fewer detected keypoints, among others.
Therefore, ConKeD shows a promising potential towards facilitating the
development and application of deep learning-based methods for retinal image
registration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05906">PartSTAD: 2D-to-3D Part Segmentation Task Adaptation. (arXiv:2401.05906v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunjin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1">Minhyuk Sung</a></p>
<p>We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D
segmentation lifting. Recent studies have highlighted the advantages of
utilizing 2D segmentation models to achieve high-quality 3D segmentation
through few-shot adaptation. However, previous approaches have focused on
adapting 2D segmentation models for domain shift to rendered images and
synthetic text descriptions, rather than optimizing the model specifically for
3D segmentation. Our proposed task adaptation method finetunes a 2D bounding
box prediction model with an objective function for 3D segmentation. We
introduce weights for 2D bounding boxes for adaptive merging and learn the
weights using a small additional neural network. Additionally, we incorporate
SAM, a foreground segmentation model on a bounding box, to improve the
boundaries of 2D segments and consequently those of 3D segmentation. Our
experiments on the PartNet-Mobility dataset show significant improvements with
our task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p
improvement in mAP_50 for semantic and instance segmentation compared to the
SotA few-shot 3D segmentation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05907">Efficient Image Deblurring Networks based on Diffusion Models. (arXiv:2401.05907v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanjie Liu</a></p>
<p>This article introduces a sliding window model for defocus deblurring that
achieves the best performance to date with extremely low memory usage. Named
Swintormer, the method utilizes a diffusion model to generate latent prior
features that assist in restoring more detailed images. It also extends the
sliding window strategy to specialized Transformer blocks for efficient
inference. Additionally, we have further optimized Multiply-Accumulate
operations (Macs). Compared to the currently top-performing GRL method, our
Swintormer model drastically reduces computational complexity from 140.35 GMACs
to 8.02 GMacs, while also improving the Signal-to-Noise Ratio (SNR) for defocus
deblurring from 27.04 dB to 27.07 dB. This new method allows for the processing
of higher resolution images on devices with limited memory, significantly
expanding potential application scenarios. The article concludes with an
ablation study that provides an in-depth analysis of the impact of each network
module on final performance. The source code and model will be available at the
following website: https://github.com/bnm6900030/swintormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05925">CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dou_B/0/1/0/all/0/1">Bin Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yongjia Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaohui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zejian Yuan</a></p>
<p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a
method for compact 3D-consistent scene segmentation at fast rendering speed
with only RGB images input. Previous NeRF-based 3D segmentation methods have
relied on implicit or voxel neural scene representation and ray-marching volume
rendering which are time consuming. Recent 3D Gaussian Splatting significantly
improves the rendering speed, however, existing Gaussians-based segmentation
methods(eg: Gaussian Grouping) fail to provide compact segmentation masks
especially in zero-shot segmentation, which is mainly caused by the lack of
robustness and compactness for straightforwardly assigning learnable parameters
to each Gaussian when encountering inconsistent 2D machine-generated labels.
Our method aims to achieve compact and reliable zero-shot scene segmentation
swiftly by mapping fused spatial and semantically meaningful features for each
Gaussian point with a shallow decoding network. Specifically, our method
firstly optimizes Gaussian points' position, convariance and color attributes
under the supervision of RGB images. After Gaussian Locating, we distill
multi-scale DINO features extracted from images through unprojection to each
Gaussian, which is then incorporated with spatial features from the fast point
features processing network, i.e. RandLA-Net. Then the shallow decoding MLP is
applied to the multi-scale fused features to obtain compact segmentation.
Experimental results show that our model can perform high-quality zero-shot
scene segmentation, as our model outperforms other segmentation methods on both
semantic and panoptic segmentation task, meanwhile consumes approximately only
10% segmenting time compared to NeRF-based segmentation. Code and more results
will be available at https://David-Dou.github.io/CoSSegGaussians
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05964">An attempt to generate new bridge types from latent space of PixelCNN. (arXiv:2401.05964v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongjun Zhang</a></p>
<p>Try to generate new bridge types using generative artificial intelligence
technology. Using symmetric structured image dataset of three-span beam bridge,
arch bridge, cable-stayed bridge and suspension bridge , based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
PixelCNN is constructed and trained. The model can capture the statistical
structure of the images and calculate the probability distribution of the next
pixel when the previous pixels are given. From the obtained latent space
sampling, new bridge types different from the training dataset can be
generated. PixelCNN can organically combine different structural components on
the basis of human original bridge types, creating new bridge types that have a
certain degree of human original ability. Autoregressive models cannot
understand the meaning of the sequence, while multimodal models combine
regression and autoregressive models to understand the sequence. Multimodal
models should be the way to achieve artificial general intelligence in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05968">A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd Counting. (arXiv:2401.05968v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_Y/0/1/0/all/0/1">Yashwardhan Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Ankit Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Phukan_O/0/1/0/all/0/1">Orchid Chetia Phukan</a>, <a href="http://arxiv.org/find/cs/1/au:+Buduru_A/0/1/0/all/0/1">Arun Balaji Buduru</a></p>
<p>Crowd counting finds direct applications in real-world situations, making
computational efficiency and performance crucial. However, most of the previous
methods rely on a heavy backbone and a complex downstream architecture that
restricts the deployment. To address this challenge and enhance the versatility
of crowd-counting models, we introduce two lightweight models. These models
maintain the same downstream architecture while incorporating two distinct
backbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion to
extract diverse scale features from a Pre-Trained Model (PTM) and subsequently
combine these features seamlessly. This approach empowers our models to achieve
improved performance while maintaining a compact and efficient design. With the
comparison of our proposed models with previously available state-of-the-art
(SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, it
achieves comparable results while being the most computationally efficient
model. Finally, we present a comparative study, an extensive ablation study,
along with pruning to show the effectiveness of our models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05971">UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization. (arXiv:2401.05971v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Rouwan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiaoya Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Juelin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuxiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Maojun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shen Yan</a></p>
<p>Despite significant progress in global localization of Unmanned Aerial
Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained
by the availability of datasets. Current datasets often focus on small-scale
scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV
build-in sensor data. To address these limitations, we introduce a large-scale
6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF
localization pipeline (UAVLoc), which consists of offline synthetic data
generation and online visual localization. Additionally, based on the 6-DoF
estimator, we design a hierarchical system for tracking ground target in 3D
space. Experimental results on the new dataset demonstrate the effectiveness of
the proposed approach. Code and dataset are available at
https://github.com/RingoWRW/UAVD4L
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05994">MGARD: A multigrid framework for high-performance, error-controlled data compression and refactoring. (arXiv:2401.05994v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1">Qian Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jieyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Whitney_B/0/1/0/all/0/1">Ben Whitney</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xin Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Reshniak_V/0/1/0/all/0/1">Viktor Reshniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1">Tania Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaemoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1">Anand Rangarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1">Lipeng Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_N/0/1/0/all/0/1">Nicolas Vidal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gainaru_A/0/1/0/all/0/1">Ana Gainaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Podhorszki_N/0/1/0/all/0/1">Norbert Podhorszki</a>, <a href="http://arxiv.org/find/cs/1/au:+Archibald_R/0/1/0/all/0/1">Richard Archibald</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1">Sanjay Ranka</a>, <a href="http://arxiv.org/find/cs/1/au:+Klasky_S/0/1/0/all/0/1">Scott Klasky</a></p>
<p>We describe MGARD, a software providing MultiGrid Adaptive Reduction for
floating-point scientific data on structured and unstructured grids. With
exceptional data compression capability and precise error control, MGARD
addresses a wide range of requirements, including storage reduction,
high-performance I/O, and in-situ data analysis. It features a unified
application programming interface (API) that seamlessly operates across diverse
computing architectures. MGARD has been optimized with highly-tuned GPU kernels
and efficient memory and device management mechanisms, ensuring scalable and
rapid operations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06000">Body-Area Capacitive or Electric Field Sensing for Human Activity Recognition and Human-Computer Interaction: A Comprehensive Survey. (arXiv:2401.06000v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bian_S/0/1/0/all/0/1">Sizhen Bian</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1">Mengxi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Lukowicz_P/0/1/0/all/0/1">Paul Lukowicz</a>, <a href="http://arxiv.org/find/eess/1/au:+Magno_M/0/1/0/all/0/1">Michele Magno</a></p>
<p>Due to the fact that roughly sixty percent of the human body is essentially
composed of water, the human body is inherently a conductive object, being able
to, firstly, form an inherent electric field from the body to the surroundings
and secondly, deform the distribution of an existing electric field near the
body. Body-area capacitive sensing, also called body-area electric field
sensing, is becoming a promising alternative for wearable devices to accomplish
certain tasks in human activity recognition and human-computer interaction.
Over the last decade, researchers have explored plentiful novel sensing systems
backed by the body-area electric field. On the other hand, despite the
pervasive exploration of the body-area electric field, a comprehensive survey
does not exist for an enlightening guideline. Moreover, the various hardware
implementations, applied algorithms, and targeted applications result in a
challenging task to achieve a systematic overview of the subject. This paper
aims to fill in the gap by comprehensively summarizing the existing works on
body-area capacitive sensing so that researchers can have a better view of the
current exploration status. To this end, we first sorted the explorations into
three domains according to the involved body forms: body-part electric field,
whole-body electric field, and body-to-body electric field, and enumerated the
state-of-art works in the domains with a detailed survey of the backed sensing
tricks and targeted applications. We then summarized the three types of sensing
frontends in circuit design, which is the most critical part in body-area
capacitive sensing, and analyzed the data processing pipeline categorized into
three kinds of approaches. Finally, we described the challenges and outlooks of
body-area electric sensing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06003">TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering. (arXiv:2401.06003v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franke_L/0/1/0/all/0/1">Linus Franke</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1">Darius R&#xfc;ckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_L/0/1/0/all/0/1">Laura Fink</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamminger_M/0/1/0/all/0/1">Marc Stamminger</a></p>
<p>Point-based radiance field rendering has demonstrated impressive results for
novel view synthesis, offering a compelling blend of rendering quality and
computational efficiency. However, also latest approaches in this domain are
not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.
2023] struggles when tasked with rendering highly detailed scenes, due to
blurring and cloudy artifacts. On the other hand, ADOP [R\"uckert et al. 2022]
can accommodate crisper images, but the neural reconstruction network decreases
performance, it grapples with temporal instability and it is unable to
effectively address large gaps in the point cloud.
</p>
<p>In this paper, we present TRIPS (Trilinear Point Splatting), an approach that
combines ideas from both Gaussian Splatting and ADOP. The fundamental concept
behind our novel technique involves rasterizing points into a screen-space
image pyramid, with the selection of the pyramid layer determined by the
projected point size. This approach allows rendering arbitrarily large points
using a single trilinear write. A lightweight neural network is then used to
reconstruct a hole-free image including detail beyond splat resolution.
Importantly, our render pipeline is entirely differentiable, allowing for
automatic optimization of both point sizes and positions.
</p>
<p>Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art
methods in terms of rendering quality while maintaining a real-time frame rate
of 60 frames per second on readily available hardware. This performance extends
to challenging scenarios, such as scenes featuring intricate geometry,
expansive landscapes, and auto-exposed footage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06005">How does the primate brain combine generative and discriminative computations in vision?. (arXiv:2401.06005v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Peters_B/0/1/0/all/0/1">Benjamin Peters</a>, <a href="http://arxiv.org/find/q-bio/1/au:+DiCarlo_J/0/1/0/all/0/1">James J. DiCarlo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gureckis_T/0/1/0/all/0/1">Todd Gureckis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Haefner_R/0/1/0/all/0/1">Ralf Haefner</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Isik_L/0/1/0/all/0/1">Leyla Isik</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua Tenenbaum</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Konkle_T/0/1/0/all/0/1">Talia Konkle</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Naselaris_T/0/1/0/all/0/1">Thomas Naselaris</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stachenfeld_K/0/1/0/all/0/1">Kimberly Stachenfeld</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tavares_Z/0/1/0/all/0/1">Zenna Tavares</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tsao_D/0/1/0/all/0/1">Doris Tsao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yildirim_I/0/1/0/all/0/1">Ilker Yildirim</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kriegeskorte_N/0/1/0/all/0/1">Nikolaus Kriegeskorte</a></p>
<p>Vision is widely understood as an inference problem. However, two contrasting
conceptions of the inference process have each been influential in research on
biological vision as well as the engineering of machine vision. The first
emphasizes bottom-up signal flow, describing vision as a largely feedforward,
discriminative inference process that filters and transforms the visual
information to remove irrelevant variation and represent behaviorally relevant
information in a format suitable for downstream functions of cognition and
behavioral control. In this conception, vision is driven by the sensory data,
and perception is direct because the processing proceeds from the data to the
latent variables of interest. The notion of "inference" in this conception is
that of the engineering literature on neural networks, where feedforward
convolutional neural networks processing images are said to perform inference.
The alternative conception is that of vision as an inference process in
Helmholtz's sense, where the sensory evidence is evaluated in the context of a
generative model of the causal processes giving rise to it. In this conception,
vision inverts a generative model through an interrogation of the evidence in a
process often thought to involve top-down predictions of sensory data to
evaluate the likelihood of alternative hypotheses. The authors include
scientists rooted in roughly equal numbers in each of the conceptions and
motivated to overcome what might be a false dichotomy between them and engage
the other perspective in the realm of theory and experiment. The primate brain
employs an unknown algorithm that may combine the advantages of both
conceptions. We explain and clarify the terminology, review the key empirical
evidence, and propose an empirical research program that transcends the
dichotomy and sets the stage for revealing the mysterious hybrid algorithm of
primate vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06009">Sea ice detection using concurrent multispectral and synthetic aperture radar imagery. (arXiv:2401.06009v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rogers_M/0/1/0/all/0/1">Martin S J Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_M/0/1/0/all/0/1">Maria Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_A/0/1/0/all/0/1">Andrew Fleming</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeeland_L/0/1/0/all/0/1">Louisa van Zeeland</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilkinson_J/0/1/0/all/0/1">Jeremy Wilkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosking_J/0/1/0/all/0/1">J. Scott Hosking</a></p>
<p>Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea
ice mapping due to its spatio-temporal coverage and the ability to detect sea
ice independent of cloud and lighting conditions. Automatic sea ice detection
using SAR imagery remains problematic due to the presence of ambiguous signal
and noise within the image. Conversely, ice and water are easily
distinguishable using multispectral imagery (MSI), but in the polar regions the
ocean's surface is often occluded by cloud or the sun may not appear above the
horizon for many months. To address some of these limitations, this paper
proposes a new tool trained using concurrent multispectral Visible and SAR
imagery for sea Ice Detection (ViSual\_IceD). ViSual\_IceD is a convolution
neural network (CNN) that builds on the classic U-Net architecture by
containing two parallel encoder stages, enabling the fusion and concatenation
of MSI and SAR imagery containing different spatial resolutions. The
performance of ViSual\_IceD is compared with U-Net models trained using
concatenated MSI and SAR imagery as well as models trained exclusively on MSI
or SAR imagery. ViSual\_IceD outperforms the other networks, with a F1 score
1.60\% points higher than the next best network, and results indicate that
ViSual\_IceD is selective in the image type it uses during image segmentation.
Outputs from ViSual\_IceD are compared to sea ice concentration products
derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how
ViSual\_IceD is a useful tool to use in conjunction with PMW data, particularly
in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery
continues to increase, ViSual\_IceD provides a new opportunity for robust,
accurate sea ice coverage detection in polar regions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06010">Attention to detail: inter-resolution knowledge distillation. (arXiv:2401.06010v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amor_R/0/1/0/all/0/1">Roc&#xed;o del Amor</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1">Julio Silva-Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Colomer_A/0/1/0/all/0/1">Adri&#xe1;n Colomer</a>, <a href="http://arxiv.org/find/cs/1/au:+Naranjo_V/0/1/0/all/0/1">Valery Naranjo</a></p>
<p>The development of computer vision solutions for gigapixel images in digital
pathology is hampered by significant computational limitations due to the large
size of whole slide images. In particular, digitizing biopsies at high
resolutions is a time-consuming process, which is necessary due to the
worsening results from the decrease in image detail. To alleviate this issue,
recent literature has proposed using knowledge distillation to enhance the
model performance at reduced image resolutions. In particular, soft labels and
features extracted at the highest magnification level are distilled into a
model that takes lower-magnification images as input. However, this approach
fails to transfer knowledge about the most discriminative image regions in the
classification process, which may be lost when the resolution is decreased. In
this work, we propose to distill this information by incorporating attention
maps during training. In particular, our formulation leverages saliency maps of
the target class via grad-CAMs, which guides the lower-resolution Student model
to match the Teacher distribution by minimizing the l2 distance between them.
Comprehensive experiments on prostate histology image grading demonstrate that
the proposed approach substantially improves the model performance across
different image resolutions compared to previous literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06013">Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery. (arXiv:2401.06013v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beilei_C/0/1/0/all/0/1">Cui Beilei</a>, <a href="http://arxiv.org/find/cs/1/au:+Mobarakol_I/0/1/0/all/0/1">Islam Mobarakol</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1">Bai Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Hongliang_R/0/1/0/all/0/1">Ren Hongliang</a></p>
<p>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,
surgical navigation and augmented reality visualization. Although the
foundation model exhibits outstanding performance in many vision tasks,
including depth estimation (e.g., DINOv2), recent works observed its
limitations in medical and surgical domain-specific applications. This work
presents a low-ranked adaptation (LoRA) of the foundation model for surgical
depth estimation. Methods: We design a foundation model-based depth estimation
method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for
depth estimation in endoscopic surgery. We build LoRA layers and integrate them
into DINO to adapt with surgery-specific domain knowledge instead of
conventional fine-tuning. During training, we freeze the DINO image encoder,
which shows excellent visual representation capacity, and only optimize the
LoRA layers and depth decoder to integrate features from the surgical scene.
Results: Our model is extensively validated on a MICCAI challenge dataset of
SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically
show that Surgical-DINO significantly outperforms all the state-of-the-art
models in endoscopic depth estimation tasks. The analysis with ablation studies
has shown evidence of the remarkable effect of our LoRA layers and adaptation.
Conclusion: Surgical-DINO shed some light on the successful adaptation of the
foundation models into the surgical domain for depth estimation. There is clear
evidence in the results that zero-shot prediction on pre-trained weights in
computer vision datasets or naive fine-tuning is not sufficient to use the
foundation model in the surgical domain directly. Code is available at
https://github.com/BeileiCui/SurgicalDINO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06019">Automatic UAV-based Airport Pavement Inspection Using Mixed Real and Virtual Scenarios. (arXiv:2401.06019v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alonso_P/0/1/0/all/0/1">Pablo Alonso</a>, <a href="http://arxiv.org/find/cs/1/au:+Gordoa_J/0/1/0/all/0/1">Jon Ander I&#xf1;iguez de Gordoa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1">Juan Diego Ortega</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1">Sara Garc&#xed;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Iriarte_F/0/1/0/all/0/1">Francisco Javier Iriarte</a>, <a href="http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1">Marcos Nieto</a></p>
<p>Runway and taxiway pavements are exposed to high stress during their
projected lifetime, which inevitably leads to a decrease in their condition
over time. To make sure airport pavement condition ensure uninterrupted and
resilient operations, it is of utmost importance to monitor their condition and
conduct regular inspections. UAV-based inspection is recently gaining
importance due to its wide range monitoring capabilities and reduced cost. In
this work, we propose a vision-based approach to automatically identify
pavement distress using images captured by UAVs. The proposed method is based
on Deep Learning (DL) to segment defects in the image. The DL architecture
leverages the low computational capacities of embedded systems in UAVs by using
an optimised implementation of EfficientNet feature extraction and Feature
Pyramid Network segmentation. To deal with the lack of annotated data for
training we have developed a synthetic dataset generation methodology to extend
available distress datasets. We demonstrate that the use of a mixed dataset
composed of synthetic and real training images yields better results when
testing the training models in real application scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06031">GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model. (arXiv:2401.06031v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhiyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huaming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiayu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhibo Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1">Kim-Kwang Raymond Choo</a></p>
<p>Adversarial generative models, such as Generative Adversarial Networks
(GANs), are widely applied for generating various types of data, i.e., images,
text, and audio. Accordingly, its promising performance has led to the
GAN-based adversarial attack methods in the white-box and black-box attack
scenarios. The importance of transferable black-box attacks lies in their
ability to be effective across different models and settings, more closely
aligning with real-world applications. However, it remains challenging to
retain the performance in terms of transferable adversarial examples for such
methods. Meanwhile, we observe that some enhanced gradient-based transferable
adversarial attack algorithms require prolonged time for adversarial sample
generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to
enhance the transferability of adversarial samples whilst improving the
algorithm's efficiency. The main approach is via optimising the training
process of the generator parameters. With the functional and characteristic
similarity analysis, we introduce a novel gradient editing (GE) mechanism and
verify its feasibility in generating transferable samples on various models.
Moreover, by exploring the frequency domain information to determine the
gradient editing direction, GE-AdvGAN can generate highly transferable
adversarial samples while minimizing the execution time in comparison to the
state-of-the-art transferable adversarial attack algorithms. The performance of
GE-AdvGAN is comprehensively evaluated by large-scale experiments on different
datasets, which results demonstrate the superiority of our algorithm. The code
for our algorithm is available at: https://github.com/LMBTough/GE-advGAN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06035">RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks. (arXiv:2401.06035v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1">Partha Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Soubhik Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>We present a novel unconditional video generative model designed to address
long-term spatial and temporal dependencies. To capture these dependencies, our
approach incorporates a hybrid explicit-implicit tri-plane representation
inspired by 3D-aware generative frameworks developed for three-dimensional
object representation and employs a singular latent code to model an entire
video sequence. Individual video frames are then synthesized from an
intermediate tri-plane representation, which itself is derived from the primary
latent code. This novel strategy reduces computational complexity by a factor
of $2$ as measured in FLOPs. Consequently, our approach facilitates the
efficient and temporally coherent generation of videos. Moreover, our joint
frame modeling approach, in contrast to autoregressive methods, mitigates the
generation of visual artifacts. We further enhance the model's capabilities by
integrating an optical flow-based module within our Generative Adversarial
Network (GAN) based generator architecture, thereby compensating for the
constraints imposed by a smaller generator size. As a result, our model is
capable of synthesizing high-fidelity video clips at a resolution of
$256\times256$ pixels, with durations extending to more than $5$ seconds at a
frame rate of 30 fps. The efficacy and versatility of our approach are
empirically validated through qualitative and quantitative assessments across
three different datasets comprising both synthetic and real video clips.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06052">Fast High Dynamic Range Radiance Fields for Dynamic Scenes. (arXiv:2401.06052v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Guanjun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1">Taoran Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jiemin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinggang Wang</a></p>
<p>Neural Radiances Fields (NeRF) and their extensions have shown great success
in representing 3D scenes and synthesizing novel-view images. However, most
NeRF methods take in low-dynamic-range (LDR) images, which may lose details,
especially with nonuniform illumination. Some previous NeRF methods attempt to
introduce high-dynamic-range (HDR) techniques but mainly target static scenes.
To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF
framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images
captured with various exposures. A learnable exposure mapping function is
constructed to obtain adaptive exposure values for each image. Based on the
monotonically increasing prior, a camera response function is designed for
stable learning. With the proposed model, high-quality novel-view images at any
time point can be rendered with any desired exposure. We further construct a
dataset containing multiple dynamic scenes captured with diverse exposures for
evaluation. All the datasets and code are available at
\url{https://guanjunwu.github.io/HDR-HexPlane/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06056">MatSynth: A Modern PBR Materials Dataset. (arXiv:2401.06056v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vecchio_G/0/1/0/all/0/1">Giuseppe Vecchio</a>, <a href="http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1">Valentin Deschaintre</a></p>
<p>We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR
materials. Materials are crucial components of virtual relightable assets,
defining the interaction of light at the surface of geometries. Given their
importance, significant research effort was dedicated to their representation,
creation and acquisition. However, in the past 6 years, most research in
material acquisiton or generation relied either on the same unique dataset, or
on company-owned huge library of procedural materials. With this dataset we
propose a significantly larger, more diverse, and higher resolution set of
materials than previously publicly available. We carefully discuss the data
collection process and demonstrate the benefits of this dataset on material
acquisition and generation applications. The complete data further contains
metadata with each material's origin, license, category, tags, creation method
and, when available, descriptions and physical size, as well as 3M+ renderings
of the augmented materials, in 1K, under various environment lightings. The
MatSynth dataset is released through the project page at:
https://www.gvecchio.com/matsynth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06071">LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhaowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yiqing Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1">Qi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1">Ran Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Junting Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zefeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1">Van Tu Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhida Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a></p>
<p>Multi-modal large language models have demonstrated impressive performance
across various tasks in different modalities. However, existing multi-modal
models primarily emphasize capturing global information within each modality
while neglecting the importance of perceiving local information across
modalities. Consequently, these models lack the ability to effectively
understand the fine-grained details of input data, limiting their performance
in tasks that require a more nuanced understanding. To address this limitation,
there is a compelling need to develop models that enable fine-grained
understanding across multiple modalities, thereby enhancing their applicability
to a wide range of tasks. In this paper, we propose LEGO, a language enhanced
multi-modal grounding model. Beyond capturing global information like other
multi-modal models, our proposed model excels at tasks demanding a detailed
understanding of local information within the input. It demonstrates precise
identification and localization of specific regions in images or moments in
videos. To achieve this objective, we design a diversified dataset construction
pipeline, resulting in a multi-modal, multi-granularity dataset for model
training. The code, dataset, and demo of our model can be found at https:
//github.com/lzw-lzw/LEGO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06105">PALP: Prompt Aligned Personalization of Text-to-Image Models. (arXiv:2401.06105v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1">Moab Arar</a>, <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1">Amir Hertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1">Omri Avrahami</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1">Shlomi Fruchter</a>, <a href="http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1">Yael Pritch</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1">Ariel Shamir</a></p>
<p>Content creators often aim to create personalized images using personal
subjects that go beyond the capabilities of conventional text-to-image models.
Additionally, they may want the resulting image to encompass a specific
location, style, ambiance, and more. Existing personalization methods may
compromise personalization ability or the alignment to complex textual prompts.
This trade-off can impede the fulfillment of user prompts and subject fidelity.
We propose a new approach focusing on personalization methods for a
\emph{single} prompt to address this issue. We term our approach prompt-aligned
personalization. While this may seem restrictive, our method excels in
improving text alignment, enabling the creation of images with complex and
intricate prompts, which may pose a challenge for current techniques. In
particular, our method keeps the personalized model aligned with a target
prompt using an additional score distillation sampling term. We demonstrate the
versatility of our method in multi- and single-shot settings and further show
that it can compose multiple subjects or use inspiration from reference images,
such as artworks. We compare our approach quantitatively and qualitatively with
existing baselines and state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06116">Gaussian Shadow Casting for Neural Characters. (arXiv:2401.06116v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1">Luis Bolanos</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Shih-Yang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1">Helge Rhodin</a></p>
<p>Neural character models can now reconstruct detailed geometry and texture
from video, but they lack explicit shadows and shading, leading to artifacts
when generating novel views and poses or during relighting. It is particularly
difficult to include shadows as they are a global effect and the required
casting of secondary rays is costly. We propose a new shadow model using a
Gaussian density proxy that replaces sampling with a simple analytic formula.
It supports dynamic motion and is tailored for shadow computation, thereby
avoiding the affine projection approximation and sorting required by the
closely related Gaussian splatting. Combined with a deferred neural rendering
model, our Gaussian shadows enable Lambertian shading and shadow casting with
minimal overhead. We demonstrate improved reconstructions, with better
separation of albedo, shading, and shadows in challenging outdoor scenes with
direct sun light and hard shadows. Our method is able to optimize the light
direction without any input from the user. As a result, novel poses have fewer
shadow artifacts and relighting in novel scenes is more realistic compared to
the state-of-the-art methods, providing new ways to pose neural characters in
novel environments, increasing their applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06122">Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bareeva_D/0/1/0/all/0/1">Dilyara Bareeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a>, <a href="http://arxiv.org/find/cs/1/au:+Warnecke_A/0/1/0/all/0/1">Alexander Warnecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirch_L/0/1/0/all/0/1">Lukas Pirch</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_K/0/1/0/all/0/1">Konrad Rieck</a>, <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a></p>
<p>Deep Neural Networks (DNNs) are capable of learning complex and versatile
representations, however, the semantic nature of the learned concepts remains
unknown. A common method used to explain the concepts learned by DNNs is
Activation Maximization (AM), which generates a synthetic input signal that
maximally activates a particular neuron in the network. In this paper, we
investigate the vulnerability of this approach to adversarial model
manipulations and introduce a novel method for manipulating feature
visualization without altering the model architecture or significantly
impacting the model's decision-making process. We evaluate the effectiveness of
our method on several neural network models and demonstrate its capabilities to
hide the functionality of specific neurons by masking the original explanations
of neurons with chosen target explanations during model auditing. As a remedy,
we propose a protective measure against such manipulations and provide
quantitative evidence which substantiates our findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06126">Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors. (arXiv:2401.06126v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saunders_J/0/1/0/all/0/1">Jack Saunders</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1">Vinay Namboodiri</a></p>
<p>Visual dubbing is the process of generating lip motions of an actor in a
video to synchronise with given audio. Recent advances have made progress
towards this goal but have not been able to produce an approach suitable for
mass adoption. Existing methods are split into either person-generic or
person-specific models. Person-specific models produce results almost
indistinguishable from reality but rely on long training times using large
single-person datasets. Person-generic works have allowed for the visual
dubbing of any video to any audio without further training, but these fail to
capture the person-specific nuances and often suffer from visual artefacts. Our
method, based on data-efficient neural rendering priors, overcomes the
limitations of existing approaches. Our pipeline consists of learning a
deferred neural rendering prior network and actor-specific adaptation using
neural textures. This method allows for $\textbf{high-quality visual dubbing
with just a few seconds of data}$, that enables video dubbing for any actor -
from A-list celebrities to background actors. We show that we achieve
state-of-the-art in terms of $\textbf{visual quality}$ and
$\textbf{recognisability}$ both quantitatively, and qualitatively through two
user studies. Our prior learning and adaptation method $\textbf{generalises to
limited data}$ better and is more $\textbf{scalable}$ than existing
person-specific models. Our experiments on real-world, limited data scenarios
find that our model is preferred over all others. The project page may be found
at https://dubbingforeveryone.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06127">E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yifan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1">Zheng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Idelbayev_Y/0/1/0/all/0/1">Yerlan Idelbayev</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zharkov_A/0/1/0/all/0/1">Andrey Zharkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1">Kfir Aberman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jian Ren</a></p>
<p>One highly promising direction for enabling flexible real-time on-device
image editing is utilizing data distillation by leveraging large-scale
text-to-image diffusion models, such as Stable Diffusion, to generate paired
datasets used for training generative adversarial networks (GANs). This
approach notably alleviates the stringent requirements typically imposed by
high-end commercial GPUs for performing image editing with diffusion models.
However, unlike text-to-image diffusion models, each distilled GAN is
specialized for a specific image editing task, necessitating costly training
efforts to obtain models for various concepts. In this work, we introduce and
address a novel research direction: can the process of distilling GANs from
diffusion models be made significantly more efficient? To achieve this goal, we
propose a series of innovative techniques. First, we construct a base GAN model
with generalized features, adaptable to different concepts through fine-tuning,
eliminating the need for training from scratch. Second, we identify crucial
layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a
simple yet effective rank search process, rather than fine-tuning the entire
base model. Third, we investigate the minimal amount of data necessary for
fine-tuning, further reducing the overall training time. Extensive experiments
show that we can efficiently empower GANs with the ability to perform real-time
high-quality image editing on mobile devices with remarkable reduced training
cost and storage for each concept.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06129">Distilling Vision-Language Models on Millions of Videos. (arXiv:2401.06129v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Long Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xingyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jialin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1">Chun-Te Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1">Hui Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1">Florian Schroff</a>, <a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1">Hartwig Adam</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1">Boqing Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1">Philipp Kr&#xe4;henb&#xfc;hl</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Liangzhe Yuan</a></p>
<p>The recent advance in vision-language models is largely attributed to the
abundance of image-text data. We aim to replicate this success for
video-language models, but there simply is not enough human-curated video-text
data available. We thus resort to fine-tuning a video-language model from a
strong image-language baseline with synthesized instructional data. The
resulting video-language model is then used to auto-label millions of videos to
generate high-quality captions. We show the adapted video-language model
performs well on a wide range of video-language benchmarks. For instance, it
surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our
model generates detailed descriptions for previously unseen videos, which
provide better textual supervision than existing methods. Experiments show that
a video-language dual-encoder model contrastively trained on these
auto-generated captions is 3.8% better than the strongest baseline that also
leverages vision-language models. Our best model outperforms state-of-the-art
methods on MSR-VTT zero-shot text-to-video retrieval by 6%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.00232">Improving deep neural network generalization and robustness to background bias via layer-wise relevance propagation optimization. (arXiv:2202.00232v7 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1">Pedro R. A. S. Bassi</a>, <a href="http://arxiv.org/find/eess/1/au:+Dertkigil_S/0/1/0/all/0/1">Sergio S. J. Dertkigil</a>, <a href="http://arxiv.org/find/eess/1/au:+Cavalli_A/0/1/0/all/0/1">Andrea Cavalli</a></p>
<p>Features in images' backgrounds can spuriously correlate with the images'
classes, representing background bias. They can influence the classifier's
decisions, causing shortcut learning (Clever Hans effect). The phenomenon
generates deep neural networks (DNNs) that perform well on standard evaluation
datasets but generalize poorly to real-world data. Layer-wise Relevance
Propagation (LRP) explains DNNs' decisions. Here, we show that the optimization
of LRP heatmaps can minimize the background bias influence on deep classifiers,
hindering shortcut learning. By not increasing run-time computational cost, the
approach is light and fast. Furthermore, it applies to virtually any
classification architecture. After injecting synthetic bias in images'
backgrounds, we compared our approach (dubbed ISNet) to eight state-of-the-art
DNNs, quantitatively demonstrating its superior robustness to background bias.
Mixed datasets are common for COVID-19 and tuberculosis classification with
chest X-rays, fostering background bias. By focusing on the lungs, the ISNet
reduced shortcut learning. Thus, its generalization performance on external
(out-of-distribution) test databases significantly surpassed all implemented
benchmark models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.02692">Gait Recognition in the Wild: A Large-scale Benchmark and NAS-based Baseline. (arXiv:2205.02692v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xianda Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Beibei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Junjie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jiankang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Guan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiwen Lu</a></p>
<p>Gait benchmarks empower the research community to train and evaluate
high-performance gait recognition systems. Even though growing efforts have
been devoted to cross-view recognition, academia is restricted by current
existing databases captured in the controlled environment. In this paper, we
contribute a new benchmark and strong baseline for Gait REcognition in the Wild
(GREW). The GREW dataset is constructed from natural videos, which contain
hundreds of cameras and thousands of hours of streams in open systems. With
tremendous manual annotations, the GREW consists of 26K identities and 128K
sequences with rich attributes for unconstrained gait recognition. Moreover, we
add a distractor set of over 233K sequences, making it more suitable for
real-world applications. Compared with prevailing predefined cross-view
datasets, the GREW has diverse and practical view variations, as well as more
naturally challenging factors. To the best of our knowledge, this is the first
large-scale dataset for gait recognition in the wild. Equipped with this
benchmark, we dissect the unconstrained gait recognition problem, where
representative appearance-based and model-based methods are explored. The
proposed GREW benchmark proves to be essential for both training and evaluating
gait recognizers in unconstrained scenarios. In addition, we propose the Single
Path One-Shot neural architecture search with uniform sampling for Gait
recognition, named SPOSGait, which is the first NAS-based gait recognition
model. In experiments, SPOSGait achieves state-of-the-art performance on the
CASIA-B, OU-MVLP, Gait3D, and GREW benchmarks, outperforming existing
approaches by a large margin. The code will be released at
https://github.com/XiandaGuo/SPOSGait.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05289">Localized adversarial artifacts for compressed sensing MRI. (arXiv:2206.05289v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Alaifari_R/0/1/0/all/0/1">Rima Alaifari</a>, <a href="http://arxiv.org/find/eess/1/au:+Alberti_G/0/1/0/all/0/1">Giovanni S. Alberti</a>, <a href="http://arxiv.org/find/eess/1/au:+Gauksson_T/0/1/0/all/0/1">Tandri Gauksson</a></p>
<p>As interest in deep neural networks (DNNs) for image reconstruction tasks
grows, their reliability has been called into question (Antun et al., 2020;
Gottschling et al., 2020). However, recent work has shown that, compared to
total variation (TV) minimization, when appropriately regularized, DNNs show
similar robustness to adversarial noise in terms of $\ell^2$-reconstruction
error (Genzel et al., 2022). We consider a different notion of robustness,
using the $\ell^\infty$-norm, and argue that localized reconstruction artifacts
are a more relevant defect than the $\ell^2$-error. We create adversarial
perturbations to undersampled magnetic resonance imaging measurements (in the
frequency domain) which induce severe localized artifacts in the TV-regularized
reconstruction. Notably, the same attack method is not as effective against DNN
based reconstruction. Finally, we show that this phenomenon is inherent to
reconstruction methods for which exact recovery can be guaranteed, as with
compressed sensing reconstructions with $\ell^1$- or TV-minimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.04847">Deep Lossy Plus Residual Coding for Lossless and Near-lossless Image Compression. (arXiv:2209.04847v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bai_Y/0/1/0/all/0/1">Yuanchao Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xianming Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ji_X/0/1/0/all/0/1">Xiangyang Ji</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xiaolin Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1">Wen Gao</a></p>
<p>Lossless and near-lossless image compression is of paramount importance to
professional users in many technical fields, such as medicine, remote sensing,
precision engineering and scientific research. But despite rapidly growing
research interests in learning-based image compression, no published method
offers both lossless and near-lossless modes. In this paper, we propose a
unified and powerful deep lossy plus residual (DLPR) coding framework for both
lossless and near-lossless image compression. In the lossless mode, the DLPR
coding system first performs lossy compression and then lossless coding of
residuals. We solve the joint lossy and residual compression problem in the
approach of VAEs, and add autoregressive context modeling of the residuals to
enhance lossless compression performance. In the near-lossless mode, we
quantize the original residuals to satisfy a given $\ell_\infty$ error bound,
and propose a scalable near-lossless compression scheme that works for variable
$\ell_\infty$ bounds instead of training multiple networks. To expedite the
DLPR coding, we increase the degree of algorithm parallelization by a novel
design of coding context, and accelerate the entropy coding with adaptive
residual interval. Experimental results demonstrate that the DLPR coding system
achieves both the state-of-the-art lossless and near-lossless image compression
performance with competitive coding speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.08159">Dynamics-aware Adversarial Attack of Adaptive Neural Networks. (arXiv:2210.08159v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1">An Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yueqi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiwen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>In this paper, we investigate the dynamics-aware adversarial attack problem
of adaptive neural networks. Most existing adversarial attack algorithms are
designed under a basic assumption -- the network architecture is fixed
throughout the attack process. However, this assumption does not hold for many
recently proposed adaptive neural networks, which adaptively deactivate
unnecessary execution units based on inputs to improve computational
efficiency. It results in a serious issue of lagged gradient, making the
learned attack at the current step ineffective due to the architecture change
afterward. To address this issue, we propose a Leaded Gradient Method (LGM) and
show the significant effects of the lagged gradient. More specifically, we
reformulate the gradients to be aware of the potential dynamic changes of
network architectures, so that the learned attack better "leads" the next step
than the dynamics-unaware methods when network architecture changes
dynamically. Extensive experiments on representative types of adaptive neural
networks for both 2D images and 3D point clouds show that our LGM achieves
impressive adversarial attack performance compared with the dynamic-unaware
attack methods. Code is available at https://github.com/antao97/LGM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14154">Interaction Region Visual Transformer for Egocentric Action Anticipation. (arXiv:2211.14154v7 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1">Debaditya Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendiran_R/0/1/0/all/0/1">Ramanathan Rajendiran</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1">Basura Fernando</a></p>
<p>Human-object interaction is one of the most important visual cues and we
propose a novel way to represent human-object interactions for egocentric
action anticipation. We propose a novel transformer variant to model
interactions by computing the change in the appearance of objects and human
hands due to the execution of the actions and use those changes to refine the
video representation. Specifically, we model interactions between hands and
objects using Spatial Cross-Attention (SCA) and further infuse contextual
information using Trajectory Cross-Attention to obtain environment-refined
interaction tokens. Using these tokens, we construct an interaction-centric
video representation for action anticipation. We term our model InAViT which
achieves state-of-the-art action anticipation performance on large-scale
egocentric datasets EPICKTICHENS100 (EK100) and EGTEA Gaze+. InAViT outperforms
other visual transformer-based methods including object-centric video
representation. On the EK100 evaluation server, InAViT is the top-performing
method on the public leaderboard (at the time of submission) where it
outperforms the second-best model by 3.3% on mean-top5 recall.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.13023">Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal. (arXiv:2212.13023v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_R/0/1/0/all/0/1">Ronglai Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mak_B/0/1/0/all/0/1">Brian Mak</a></p>
<p>Most deep-learning-based continuous sign language recognition (CSLR) models
share a similar backbone consisting of a visual module, a sequential module,
and an alignment module. However, due to limited training samples, a
connectionist temporal classification loss may not train such CSLR backbones
sufficiently. In this work, we propose three auxiliary tasks to enhance the
CSLR backbones. The first task enhances the visual module, which is sensitive
to the insufficient training problem, from the perspective of consistency.
Specifically, since the information of sign languages is mainly included in
signers' facial expressions and hand movements, a keypoint-guided spatial
attention module is developed to enforce the visual module to focus on
informative regions, i.e., spatial attention consistency. Second, noticing that
both the output features of the visual and sequential modules represent the
same sentence, to better exploit the backbone's power, a sentence embedding
consistency constraint is imposed between the visual and sequential modules to
enhance the representation power of both features. We name the CSLR model
trained with the above auxiliary tasks as consistency-enhanced CSLR, which
performs well on signer-dependent datasets in which all signers appear during
both training and testing. To make it more robust for the signer-independent
setting, a signer removal module based on feature disentanglement is further
proposed to remove signer information from the backbone. Extensive ablation
studies are conducted to validate the effectiveness of these auxiliary tasks.
More remarkably, with a transformer-based backbone, our model achieves
state-of-the-art or competitive performance on five benchmarks, PHOENIX-2014,
PHOENIX-2014-T, PHOENIX-2014-SI, CSL, and CSL-Daily. Code and Models are
available at https://github.com/2000ZRL/LCSA_C2SLR_SRM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08898">Recurrent Generic Contour-based Instance Segmentation with Progressive Learning. (arXiv:2301.08898v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Keyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wengang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yufei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jiajun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Houqiang Li</a></p>
<p>Contour-based instance segmentation has been actively studied, thanks to its
flexibility and elegance in processing visual objects within complex
backgrounds. In this work, we propose a novel deep network architecture, i.e.,
PolySnake, for generic contour-based instance segmentation. Motivated by the
classic Snake algorithm, the proposed PolySnake achieves superior and robust
segmentation performance with an iterative and progressive contour refinement
strategy. Technically, PolySnake introduces a recurrent update operator to
estimate the object contour iteratively. It maintains a single estimate of the
contour that is progressively deformed toward the object boundary. At each
iteration, PolySnake builds a semantic-rich representation for the current
contour and feeds it to the recurrent operator for further contour adjustment.
Through the iterative refinements, the contour progressively converges to a
stable status that tightly encloses the object instance. Beyond the scope of
general instance segmentation, extensive experiments are conducted to validate
the effectiveness and generalizability of our PolySnake in two additional
specific task scenarios, including scene text detection and lane detection. The
results demonstrate that the proposed PolySnake outperforms the existing
advanced methods on several multiple prevalent benchmarks across the three
tasks. The codes and pre-trained models are available at
https://github.com/fh2019ustc/PolySnake
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13330">Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference. (arXiv:2301.13330v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bablani_D/0/1/0/all/0/1">Deepika Bablani</a>, <a href="http://arxiv.org/find/cs/1/au:+Mckinstry_J/0/1/0/all/0/1">Jeffrey L. Mckinstry</a>, <a href="http://arxiv.org/find/cs/1/au:+Esser_S/0/1/0/all/0/1">Steven K. Esser</a>, <a href="http://arxiv.org/find/cs/1/au:+Appuswamy_R/0/1/0/all/0/1">Rathinakumar Appuswamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Modha_D/0/1/0/all/0/1">Dharmendra S. Modha</a></p>
<p>For efficient neural network inference, it is desirable to achieve
state-of-the-art accuracy with the simplest networks requiring the least
computation, memory, and power. Quantizing networks to lower precision is a
powerful technique for simplifying networks. As each layer of a network may
have different sensitivity to quantization, mixed precision quantization
methods selectively tune the precision of individual layers to achieve a
minimum drop in task performance (e.g., accuracy). To estimate the impact of
layer precision choice on task performance, two methods are introduced: i)
Entropy Approximation Guided Layer selection (EAGL) is fast and uses the
entropy of the weight distribution, and ii) Accuracy-aware Layer Precision
Selection (ALPS) is straightforward and relies on single epoch fine-tuning
after layer precision reduction. Using EAGL and ALPS for layer precision
selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit
layers for ResNet-50, ResNet-101 and BERT-base transformer networks,
demonstrating enhanced performance across the entire accuracy-throughput
frontier. The techniques demonstrate better performance than existing
techniques in several commensurate comparisons. Notably, this is accomplished
with significantly lesser computational time required to reach a solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14383">Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1">Matthew Trager</a>, <a href="http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1">Pramuditha Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1">Luca Zancato</a>, <a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1">Alessandro Achille</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1">Parminder Bhatia</a>, <a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1">Stefano Soatto</a></p>
<p>We investigate compositional structures in data embeddings from pre-trained
vision-language models (VLMs). Traditionally, compositionality has been
associated with algebraic operations on embeddings of words from a pre-existing
vocabulary. In contrast, we seek to approximate representations from an encoder
as combinations of a smaller set of vectors in the embedding space. These
vectors can be seen as "ideal words" for generating concepts directly within
the embedding space of the model. We first present a framework for
understanding compositional structures from a geometric perspective. We then
explain what these compositional structures entail probabilistically in the
case of VLM embeddings, providing intuitions for why they arise in practice.
Finally, we empirically explore these structures in CLIP's embeddings and we
evaluate their usefulness for solving different vision-language tasks such as
classification, debiasing, and retrieval. Our results show that simple linear
algebraic operations on embedding vectors can be used as compositional and
interpretable methods for regulating the behavior of VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08500">The Devil&#x27;s Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dolatabadi_H/0/1/0/all/0/1">Hadi M. Dolatabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1">Sarah Erfani</a>, <a href="http://arxiv.org/find/cs/1/au:+Leckie_C/0/1/0/all/0/1">Christopher Leckie</a></p>
<p>Protecting personal data against exploitation of machine learning models is
crucial. Recently, availability attacks have shown great promise to provide an
extra layer of protection against the unauthorized use of data to train neural
networks. These methods aim to add imperceptible noise to clean data so that
the neural networks cannot extract meaningful patterns from the protected data,
claiming that they can make personal data "unexploitable." This paper provides
a strong countermeasure against such approaches, showing that unexploitable
data might only be an illusion. In particular, we leverage the power of
diffusion models and show that a carefully designed denoising process can
counteract the effectiveness of the data-protecting perturbations. We
rigorously analyze our algorithm, and theoretically prove that the amount of
required denoising is directly related to the magnitude of the data-protecting
perturbations. Our approach, called AVATAR, delivers state-of-the-art
performance against a suite of recent availability attacks in various
scenarios, outperforming adversarial training even under distribution mismatch
between the diffusion model and the protected data. Our findings call for more
research into making personal data unexploitable, showing that this goal is far
from over. Our implementation is available at this repository:
https://github.com/hmdolatabadi/AVATAR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08929">SDFReg: Learning Signed Distance Functions for Point Cloud Registration. (arXiv:2304.08929v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leida Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhengda Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiqun Wang</a></p>
<p>Learning-based point cloud registration methods can handle clean point clouds
well, while it is still challenging to generalize to noisy, partial, and
density-varying point clouds. To this end, we propose a novel point cloud
registration framework for these imperfect point clouds. By introducing a
neural implicit representation, we replace the problem of rigid registration
between point clouds with a registration problem between the point cloud and
the neural implicit function. We then propose to alternately optimize the
implicit function and the registration between the implicit function and point
cloud. In this way, point cloud registration can be performed in a
coarse-to-fine manner. By fully capitalizing on the capabilities of the neural
implicit function without computing point correspondences, our method showcases
remarkable robustness in the face of challenges such as noise, incompleteness,
and density changes of point clouds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04665">Riesz networks: scale invariant neural networks in a single forward pass. (arXiv:2305.04665v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barisin_T/0/1/0/all/0/1">Tin Barisin</a>, <a href="http://arxiv.org/find/cs/1/au:+Schladitz_K/0/1/0/all/0/1">Katja Schladitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Redenbach_C/0/1/0/all/0/1">Claudia Redenbach</a></p>
<p>Scale invariance of an algorithm refers to its ability to treat objects
equally independently of their size. For neural networks, scale invariance is
typically achieved by data augmentation. However, when presented with a scale
far outside the range covered by the training set, neural networks may fail to
generalize.
</p>
<p>Here, we introduce the Riesz network, a novel scale invariant neural network.
Instead of standard 2d or 3d convolutions for combining spatial information,
the Riesz network is based on the Riesz transform which is a scale equivariant
operation. As a consequence, this network naturally generalizes to unseen or
even arbitrary scales in a single forward pass. As an application example, we
consider detecting and segmenting cracks in tomographic images of concrete. In
this context, 'scale' refers to the crack thickness which may vary strongly
even within the same sample. To prove its scale invariance, the Riesz network
is trained on one fixed crack width. We then validate its performance in
segmenting simulated and real tomographic images featuring a wide range of
crack widths. An additional experiment is carried out on the MNIST Large Scale
data set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06822">Implicit Neural Networks with Fourier-Feature Inputs for Free-breathing Cardiac MRI Reconstruction. (arXiv:2305.06822v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kunz_J/0/1/0/all/0/1">Johannes F. Kunz</a>, <a href="http://arxiv.org/find/eess/1/au:+Ruschke_S/0/1/0/all/0/1">Stefan Ruschke</a>, <a href="http://arxiv.org/find/eess/1/au:+Heckel_R/0/1/0/all/0/1">Reinhard Heckel</a></p>
<p>Cardiac magnetic resonance imaging (MRI) requires reconstructing a real-time
video of a beating heart from continuous highly under-sampled measurements.
This task is challenging since the object to be reconstructed (the heart) is
continuously changing during signal acquisition. In this paper, we propose a
reconstruction approach based on representing the beating heart with an
implicit neural network and fitting the network so that the representation of
the heart is consistent with the measurements. The network in the form of a
multi-layer perceptron with Fourier-feature inputs acts as an effective signal
prior and enables adjusting the regularization strength in both the spatial and
temporal dimensions of the signal. We study the proposed approach for 2D
free-breathing cardiac real-time MRI in different operating regimes, i.e., for
different image resolutions, slice thicknesses, and acquisition lengths. Our
method achieves reconstruction quality on par with or slightly better than
state-of-the-art untrained convolutional neural networks and superior image
quality compared to a recent method that fits an implicit representation
directly to Fourier-domain measurements. However, this comes at a relatively
high computational cost. Our approach does not require any additional patient
data or biosensors including electrocardiography, making it potentially
applicable in a wide range of clinical scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16829">BEV-IO: Enhancing Bird&#x27;s-Eye-View 3D Detection with Instance Occupancy. (arXiv:2305.16829v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zaibin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuanhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Huchuan Lu</a></p>
<p>A popular approach for constructing bird's-eye-view (BEV) representation in
3D detection is to lift 2D image features onto the viewing frustum space based
on explicitly predicted depth distribution. However, depth distribution can
only characterize the 3D geometry of visible object surfaces but fails to
capture their internal space and overall geometric structure, leading to sparse
and unsatisfactory 3D representations. To mitigate this issue, we present
BEV-IO, a new 3D detection paradigm to enhance BEV representation with instance
occupancy information. At the core of our method is the newly-designed instance
occupancy prediction (IOP) module, which aims to infer point-level occupancy
status for each instance in the frustum space. To ensure training efficiency
while maintaining representational flexibility, it is trained using the
combination of both explicit and implicit supervision. With the predicted
occupancy, we further design a geometry-aware feature propagation mechanism
(GFP), which performs self-attention based on occupancy distribution along each
ray in frustum and is able to enforce instance-level feature consistency. By
integrating the IOP module with GFP mechanism, our BEV-IO detector is able to
render highly informative 3D scene structures with more comprehensive BEV
representations. Experimental results demonstrate that BEV-IO can outperform
state-of-the-art methods while only adding a negligible increase in parameters
(0.2%) and computational overhead (0.24%in GFLOPs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18211">WiFi-TCN: Temporal Convolution for Human Interaction Recognition based on WiFi signal. (arXiv:2305.18211v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1">Chih-Yang Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1">Chia-Yu Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Tso Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Shih_T/0/1/0/all/0/1">Timothy K. Shih</a></p>
<p>The utilization of Wi-Fi based human activity recognition has gained
considerable interest in recent times, primarily owing to its applications in
various domains such as healthcare for monitoring breath and heart rate,
security, elderly care. These Wi-Fi-based methods exhibit several advantages
over conventional state-of-the-art techniques that rely on cameras and sensors,
including lower costs and ease of deployment. However, a significant challenge
associated with Wi-Fi-based HAR is the significant decline in performance when
the scene or subject changes. To mitigate this issue, it is imperative to train
the model using an extensive dataset. In recent studies, the utilization of
CNN-based models or sequence-to-sequence models such as LSTM, GRU, or
Transformer has become prevalent. While sequence-to-sequence models can be more
precise, they are also more computationally intensive and require a larger
amount of training data. To tackle these limitations, we propose a novel
approach that leverages a temporal convolution network with augmentations and
attention, referred to as TCN-AA. Our proposed method is computationally
efficient and exhibits improved accuracy even when the data size is increased
threefold through our augmentation techniques. Our experiments on a publicly
available dataset indicate that our approach outperforms existing
state-of-the-art methods, with a final accuracy of 99.42%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08836">Probabilistic-based Feature Embedding of 4-D Light Fields for Compressive Imaging and Denoising. (arXiv:2306.08836v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lyu_X/0/1/0/all/0/1">Xianqiang Lyu</a>, <a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a></p>
<p>The high-dimensional nature of the 4-D light field (LF) poses great
challenges in achieving efficient and effective feature embedding, that
severely impacts the performance of downstream tasks. To tackle this crucial
issue, in contrast to existing methods with empirically-designed architectures,
we propose a probabilistic-based feature embedding (PFE), which learns a
feature embedding architecture by assembling various low-dimensional
convolution patterns in a probability space for fully capturing spatial-angular
information. Building upon the proposed PFE, we then leverage the intrinsic
linear imaging model of the coded aperture camera to construct a
cycle-consistent 4-D LF reconstruction network from coded measurements.
Moreover, we incorporate PFE into an iterative optimization framework for 4-D
LF denoising. Our extensive experiments demonstrate the significant superiority
of our methods on both real-world and synthetic 4-D LF images, both
quantitatively and qualitatively, when compared with state-of-the-art methods.
The source code will be publicly available at
https://github.com/lyuxianqiang/LFCA-CR-NET.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09086">Relation-Aware Diffusion Model for Controllable Poster Layout Generation. (arXiv:2306.09086v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fengheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">An Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Wei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Honghe Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaoyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jingjing Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Junjie Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhangang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jingping Shao</a></p>
<p>Poster layout is a crucial aspect of poster design. Prior methods primarily
focus on the correlation between visual content and graphic elements. However,
a pleasant layout should also consider the relationship between visual and
textual contents and the relationship between elements. In this study, we
introduce a relation-aware diffusion model for poster layout generation that
incorporates these two relationships in the generation process. Firstly, we
devise a visual-textual relation-aware module that aligns the visual and
textual representations across modalities, thereby enhancing the layout's
efficacy in conveying textual information. Subsequently, we propose a geometry
relation-aware module that learns the geometry relationship between elements by
comprehensively considering contextual information. Additionally, the proposed
method can generate diverse layouts based on user constraints. To advance
research in this field, we have constructed a poster layout dataset named
CGL-Dataset V2. Our proposed method outperforms state-of-the-art methods on
CGL-Dataset V2. The data and code will be available at
https://github.com/liuan0803/RADM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01849">Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Belagali_V/0/1/0/all/0/1">Varun Belagali</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jinghuan Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael S. Ryoo</a></p>
<p>Sequence modeling approaches have shown promising results in robot imitation
learning. Recently, diffusion models have been adopted for behavioral cloning
in a sequence modeling fashion, benefiting from their exceptional capabilities
in modeling complex data distributions. The standard diffusion-based policy
iteratively generates action sequences from random noise conditioned on the
input states. Nonetheless, the model for diffusion policy can be further
improved in terms of visual representations. In this work, we propose Crossway
Diffusion, a simple yet effective method to enhance diffusion-based visuomotor
policy learning via a carefully designed state decoder and an auxiliary
self-supervised learning (SSL) objective. The state decoder reconstructs raw
image pixels and other state information from the intermediate representations
of the reverse diffusion process. The whole model is jointly optimized by the
SSL objective and the original diffusion loss. Our experiments demonstrate the
effectiveness of Crossway Diffusion in various simulated and real-world robot
tasks, confirming its consistent advantages over the standard diffusion-based
policy and substantial improvements over the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02730">Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. (arXiv:2307.02730v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng-Lan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yu-Ning Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1">Gang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Si-Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jin-Rong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen-Yue Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1">Ning Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xue-Hai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a></p>
<p>The fine-grained action analysis of the existing action datasets is
challenged by insufficient action categories, low fine granularities, limited
modalities, and tasks. In this paper, we propose a Multi-modality and
Multi-task dataset of Figure Skating (MMFS) which was collected from the World
Figure Skating Championships. MMFS, which possesses action recognition and
action quality assessment, captures RGB, skeleton, and is collected the score
of actions from 11671 clips with 256 categories including spatial and temporal
labels. The key contributions of our dataset fall into three aspects as
follows. (1) Independently spatial and temporal categories are first proposed
to further explore fine-grained action recognition and quality assessment. (2)
MMFS first introduces the skeleton modality for complex fine-grained action
quality assessment. (3) Our multi-modality and multi-task dataset encourage
more action analysis models. To benchmark our dataset, we adopt RGB-based and
skeleton-based baseline methods for action recognition and action quality
assessment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07643">Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) for Automated Structural Condition Assessment in Visual Inspection. (arXiv:2307.07643v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhaozheng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1">Ruwen Qin</a></p>
<p>Efficiently monitoring the condition of civil infrastructure requires
automating the structural condition assessment in visual inspection. This paper
proposes an Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) for
automatic structural condition assessment in visual bridge inspection.
AECIF-Net can simultaneously parse structural elements and segment surface
defects on the elements in inspection images. It integrates two task-specific
relearning subnets to extract task-specific features from an overall feature
embedding. A co-interactive feature fusion module further captures the spatial
correlation and facilitates information sharing between tasks. Experimental
results demonstrate that the proposed AECIF-Net outperforms the current
state-of-the-art approaches, achieving promising performance with 92.11% mIoU
for element segmentation and 87.16% mIoU for corrosion segmentation on the test
set of the new benchmark dataset Steel Bridge Condition Inspection Visual
(SBCIV). An ablation study verifies the merits of the designs for AECIF-Net,
and a case study demonstrates its capability to automate structural condition
assessment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08467">Riesz feature representation: scale equivariant scattering network for classification tasks. (arXiv:2307.08467v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barisin_T/0/1/0/all/0/1">Tin Barisin</a>, <a href="http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1">Jesus Angulo</a>, <a href="http://arxiv.org/find/cs/1/au:+Schladitz_K/0/1/0/all/0/1">Katja Schladitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Redenbach_C/0/1/0/all/0/1">Claudia Redenbach</a></p>
<p>Scattering networks yield powerful and robust hierarchical image descriptors
which do not require lengthy training and which work well with very few
training data. However, they rely on sampling the scale dimension. Hence, they
become sensitive to scale variations and are unable to generalize to unseen
scales. In this work, we define an alternative feature representation based on
the Riesz transform. We detail and analyze the mathematical foundations behind
this representation. In particular, it inherits scale equivariance from the
Riesz transform and completely avoids sampling of the scale dimension.
Additionally, the number of features in the representation is reduced by a
factor four compared to scattering networks. Nevertheless, our representation
performs comparably well for texture classification with an interesting
addition: scale equivariance. Our method yields superior performance when
dealing with scales outside of those covered by the training dataset. The
usefulness of the equivariance property is demonstrated on the digit
classification task, where accuracy remains stable even for scales four times
larger than the one chosen for training. As a second example, we consider
classification of textures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08994">Human Action Recognition in Still Images Using ConViT. (arXiv:2307.08994v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosseyni_S/0/1/0/all/0/1">Seyed Rohollah Hosseyni</a>, <a href="http://arxiv.org/find/cs/1/au:+Seyedin_S/0/1/0/all/0/1">Sanaz Seyedin</a>, <a href="http://arxiv.org/find/cs/1/au:+Taheri_H/0/1/0/all/0/1">Hasan Taheri</a></p>
<p>Understanding the relationship between different parts of an image is crucial
in a variety of applications, including object recognition, scene
understanding, and image classification. Despite the fact that Convolutional
Neural Networks (CNNs) have demonstrated impressive results in classifying and
detecting objects, they lack the capability to extract the relationship between
different parts of an image, which is a crucial factor in Human Action
Recognition (HAR). To address this problem, this paper proposes a new module
that functions like a convolutional layer that uses Vision Transformer (ViT).
In the proposed model, the Vision Transformer can complement a convolutional
neural network in a variety of tasks by helping it to effectively extract the
relationship among various parts of an image. It is shown that the proposed
model, compared to a simple CNN, can extract meaningful parts of an image and
suppress the misleading parts. The proposed model has been evaluated on the
Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5% mean
Average Precision (mAP) and 91.5% mAP results, respectively, which are
promising compared to other state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16518">MS23D: : A 3D Object Detection Method Using Multi-Scale Semantic Feature Points to Construct 3D Feature Layer. (arXiv:2308.16518v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1">Yongxin Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1">Aihong Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Binrui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1">Tianhong Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhetao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaxin Liu</a></p>
<p>LiDAR point clouds can effectively depict the motion and posture of objects
in three-dimensional space. Many studies accomplish the 3D object detection by
voxelizing point clouds. However, in autonomous driving scenarios, the sparsity
and hollowness of point clouds create some difficulties for voxel-based
methods. The sparsity of point clouds makes it challenging to describe the
geometric features of objects. The hollowness of point clouds poses
difficulties for the aggregation of 3D features. We propose a two-stage 3D
object detection framework, called MS23D. (1) We propose a method using voxel
feature points from multi-branch to construct the 3D feature layer. Using voxel
feature points from different branches, we construct a relatively compact 3D
feature layer with rich semantic features. Additionally, we propose a
distance-weighted sampling method, reducing the loss of foreground points
caused by downsampling and allowing the 3D feature layer to retain more
foreground points. (2) In response to the hollowness of point clouds, we
predict the offsets between deep-level feature points and the object's
centroid, making them as close as possible to the object's centroid. This
enables the aggregation of these feature points with abundant semantic
features. For feature points from shallow-level, we retain them on the object's
surface to describe the geometric features of the object. To validate our
approach, we evaluated its effectiveness on both the KITTI and ONCE datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16875">Holistic Processing of Colour Images Using Novel Quaternion-Valued Wavelets on the Plane. (arXiv:2308.16875v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dizon_N/0/1/0/all/0/1">Neil D. Dizon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hogan_J/0/1/0/all/0/1">Jeffrey A. Hogan</a></p>
<p>Recently, novel quaternion-valued wavelets on the plane were constructed
using an optimisation approach. These wavelets are compactly supported, smooth,
orthonormal, non-separable and truly quaternionic. However, they have not been
tested in application. In this paper, we introduce a methodology for
decomposing and reconstructing colour images using quaternionic wavelet filters
associated to recently developed quaternion-valued wavelets on the plane. We
investigate its applicability in compression, enhancement, segmentation, and
denoising of colour images. Our results demonstrate these wavelets as promising
tools for an end-to-end quaternion processing of colour images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01036">SEPAL: Spatial Gene Expression Prediction from Local Graphs. (arXiv:2309.01036v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mejia_G/0/1/0/all/0/1">Gabriel Mejia</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardenas_P/0/1/0/all/0/1">Paula C&#xe1;rdenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_D/0/1/0/all/0/1">Daniela Ruiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Castillo_A/0/1/0/all/0/1">Angela Castillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1">Pablo Arbel&#xe1;ez</a></p>
<p>Spatial transcriptomics is an emerging technology that aligns histopathology
images with spatially resolved gene expression profiling. It holds the
potential for understanding many diseases but faces significant bottlenecks
such as specialized equipment and domain expertise. In this work, we present
SEPAL, a new model for predicting genetic profiles from visual tissue
appearance. Our method exploits the biological biases of the problem by
directly supervising relative differences with respect to mean expression, and
leverages local visual context at every coordinate to make predictions using a
graph neural network. This approach closes the gap between complete locality
and complete globality in current methods. In addition, we propose a novel
benchmark that aims to better define the task by following current best
practices in transcriptomics and restricting the prediction variables to only
those with clear spatial patterns. Our extensive evaluation in two different
human breast cancer datasets indicates that SEPAL outperforms previous
state-of-the-art methods and other mechanisms of including spatial context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09571">Heterogeneous Generative Knowledge Distillation with Masked Image Modeling. (arXiv:2309.09571v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shumin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaodi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jing Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xianbin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Baochang Zhang</a></p>
<p>Small CNN-based models usually require transferring knowledge from a large
model before they are deployed in computationally resource-limited edge
devices. Masked image modeling (MIM) methods achieve great success in various
visual tasks but remain largely unexplored in knowledge distillation for
heterogeneous deep models. The reason is mainly due to the significant
discrepancy between the Transformer-based large model and the CNN-based small
network. In this paper, we develop the first Heterogeneous Generative Knowledge
Distillation (H-GKD) based on MIM, which can efficiently transfer knowledge
from large Transformer models to small CNN-based models in a generative
self-supervised fashion. Our method builds a bridge between Transformer-based
models and CNNs by training a UNet-style student with sparse convolution, which
can effectively mimic the visual representation inferred by a teacher over
masked modeling. Our method is a simple yet effective learning paradigm to
learn the visual representation and distribution of data from heterogeneous
teacher models, which can be pre-trained using advanced generative methods.
Extensive experiments show that it adapts well to various models and sizes,
consistently achieving state-of-the-art performance in image classification,
object detection, and semantic segmentation tasks. For example, in the Imagenet
1K dataset, H-GKD improves the accuracy of Resnet50 (sparse) from 76.98% to
80.01%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12009">Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision. (arXiv:2309.12009v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yiping Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1">Kunyu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1">Alina Roitberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Junwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruiping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yufan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>Self-supervised representation learning for human action recognition has
developed rapidly in recent years. Most of the existing works are based on
skeleton data while using a multi-modality setup. These works overlooked the
differences in performance among modalities, which led to the propagation of
erroneous knowledge between modalities while only three fundamental modalities,
i.e., joints, bones, and motions are used, hence no additional modalities are
explored.
</p>
<p>In this work, we first propose an Implicit Knowledge Exchange Module (IKEM)
which alleviates the propagation of erroneous knowledge between low-performance
modalities. Then, we further propose three new modalities to enrich the
complementary information between modalities. Finally, to maintain efficiency
when introducing new modalities, we propose a novel teacher-student framework
to distill the knowledge from the secondary modalities into the mandatory
modalities considering the relationship constrained by anchors, positives, and
negatives, named relational cross-modality knowledge distillation. The
experimental results demonstrate the effectiveness of our approach, unlocking
the efficient use of skeleton-based multi-modality data. Source code will be
made publicly available at https://github.com/desehuileng0o0/IKEM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00530">Multi-tiling Neural Radiance Field (NeRF) -- Geometric Assessment on Large-scale Aerial Datasets. (arXiv:2310.00530v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Ningli Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1">Rongjun Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Debao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Remondino_F/0/1/0/all/0/1">Fabio Remondino</a></p>
<p>Neural Radiance Fields (NeRF) offer the potential to benefit 3D
reconstruction tasks, including aerial photogrammetry. However, the scalability
and accuracy of the inferred geometry are not well-documented for large-scale
aerial assets,since such datasets usually result in very high memory
consumption and slow convergence.. In this paper, we aim to scale the NeRF on
large-scael aerial datasets and provide a thorough geometry assessment of NeRF.
Specifically, we introduce a location-specific sampling technique as well as a
multi-camera tiling (MCT) strategy to reduce memory consumption during image
loading for RAM, representation training for GPU memory, and increase the
convergence rate within tiles. MCT decomposes a large-frame image into multiple
tiled images with different camera models, allowing these small-frame images to
be fed into the training process as needed for specific locations without a
loss of accuracy. We implement our method on a representative approach,
Mip-NeRF, and compare its geometry performance with threephotgrammetric MVS
pipelines on two typical aerial datasets against LiDAR reference data. Both
qualitative and quantitative results suggest that the proposed NeRF approach
produces better completeness and object details than traditional approaches,
although as of now, it still falls short in terms of accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00723">HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count. (arXiv:2310.00723v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wiederhold_N/0/1/0/all/0/1">Noah Wiederhold</a>, <a href="http://arxiv.org/find/cs/1/au:+Megyeri_A/0/1/0/all/0/1">Ava Megyeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Paris_D/0/1/0/all/0/1">DiMaggio Paris</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1">Sean Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Banerjee_N/0/1/0/all/0/1">Natasha Kholgade Banerjee</a></p>
<p>We present the HOH (Human-Object-Human) Handover Dataset, a large object
count dataset with 136 objects, to accelerate data-driven research on handover
studies, human-robot handover implementation, and artificial intelligence (AI)
on handover parameter estimation from 2D and 3D data of person interactions.
HOH contains multi-view RGB and depth data, skeletons, fused point clouds,
grasp type and handedness labels, object, giver hand, and receiver hand 2D and
3D segmentations, giver and receiver comfort ratings, and paired object
metadata and aligned 3D models for 2,720 handover interactions spanning 136
objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40
participants. We also show experimental results of neural networks trained
using HOH to perform grasp, orientation, and trajectory prediction. As the only
fully markerless handover capture dataset, HOH represents natural human-human
handover interactions, overcoming challenges with markered datasets that
require specific suiting for body tracking, and lack high-resolution hand
tracking. To date, HOH is the largest handover dataset in number of objects,
participants, pairs with role reversal accounted for, and total interactions
captured.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01376">Towards Distribution-Agnostic Generalized Category Discovery. (arXiv:2310.01376v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jianhong Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuozhu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hualiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ruizhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_L/0/1/0/all/0/1">Lianrui Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaomeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Haoji Hu</a></p>
<p>Data imbalance and open-ended distribution are two intrinsic characteristics
of the real visual world. Though encouraging progress has been made in tackling
each challenge separately, few works dedicated to combining them towards
real-world scenarios. While several previous works have focused on classifying
close-set samples and detecting open-set samples during testing, it's still
essential to be able to classify unknown subjects as human beings. In this
paper, we formally define a more realistic task as distribution-agnostic
generalized category discovery (DA-GCD): generating fine-grained predictions
for both close- and open-set classes in a long-tailed open-world setting. To
tackle the challenging problem, we propose a Self-Balanced Co-Advice
contrastive framework (BaCon), which consists of a contrastive-learning branch
and a pseudo-labeling branch, working collaboratively to provide interactive
supervision to resolve the DA-GCD task. In particular, the contrastive-learning
branch provides reliable distribution estimation to regularize the predictions
of the pseudo-labeling branch, which in turn guides contrastive learning
through self-balanced knowledge transfer and a proposed novel contrastive loss.
We compare BaCon with state-of-the-art methods from two closely related fields:
imbalanced semi-supervised learning and generalized category discovery. The
effectiveness of BaCon is demonstrated with superior performance over all
baselines and comprehensive analysis across various datasets. Our code is
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05012">Detecting Abnormal Health Conditions in Smart Home Using a Drone. (arXiv:2310.05012v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barman_P/0/1/0/all/0/1">Pronob Kumar Barman</a></p>
<p>Nowadays, detecting aberrant health issues is a difficult process. Falling,
especially among the elderly, is a severe concern worldwide. Falls can result
in deadly consequences, including unconsciousness, internal bleeding, and often
times, death. A practical and optimal, smart approach of detecting falling is
currently a concern. The use of vision-based fall monitoring is becoming more
common among scientists as it enables senior citizens and those with other
health conditions to live independently. For tracking, surveillance, and
rescue, unmanned aerial vehicles use video or image segmentation and object
detection methods. The Tello drone is equipped with a camera and with this
device we determined normal and abnormal behaviors among our participants. The
autonomous falling objects are classified using a convolutional neural network
(CNN) classifier. The results demonstrate that the systems can identify falling
objects with a precision of 0.9948.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13183">Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Weizhi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongkuan Xu</a></p>
<p>It is widely acknowledged that large and sparse models have higher accuracy
than small and dense models under the same model size constraints. This
motivates us to train a large model and then remove its redundant neurons or
weights by pruning. Most existing works pruned the networks in a deterministic
way, the performance of which solely depends on a single pruning criterion and
thus lacks variety. Instead, in this paper, we propose a model pruning strategy
that first generates several pruning masks in a designed random way.
Subsequently, along with an effective mask-selection rule, the optimal mask is
chosen from the pool of mask candidates. To further enhance efficiency, we
introduce an early mask evaluation strategy, mitigating the overhead associated
with training multiple masks. Our extensive experiments demonstrate that this
approach achieves state-of-the-art performance across eight datasets from GLUE,
particularly excelling at high levels of sparsity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15043">CalibrationPhys: Self-supervised Video-based Heart and Respiratory Rate Measurements by Calibrating Between Multiple Cameras. (arXiv:2310.15043v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Akamatsu_Y/0/1/0/all/0/1">Yusuke Akamatsu</a>, <a href="http://arxiv.org/find/eess/1/au:+Umematsu_T/0/1/0/all/0/1">Terumi Umematsu</a>, <a href="http://arxiv.org/find/eess/1/au:+Imaoka_H/0/1/0/all/0/1">Hitoshi Imaoka</a></p>
<p>Video-based heart and respiratory rate measurements using facial videos are
more useful and user-friendly than traditional contact-based sensors. However,
most of the current deep learning approaches require ground-truth pulse and
respiratory waves for model training, which are expensive to collect. In this
paper, we propose CalibrationPhys, a self-supervised video-based heart and
respiratory rate measurement method that calibrates between multiple cameras.
CalibrationPhys trains deep learning models without supervised labels by using
facial videos captured simultaneously by multiple cameras. Contrastive learning
is performed so that the pulse and respiratory waves predicted from the
synchronized videos using multiple cameras are positive and those from
different videos are negative. CalibrationPhys also improves the robustness of
the models by means of a data augmentation technique and successfully leverages
a pre-trained model for a particular camera. Experimental results utilizing two
datasets demonstrate that CalibrationPhys outperforms state-of-the-art heart
and respiratory rate measurement methods. Since we optimize camera-specific
models using only videos from multiple cameras, our approach makes it easy to
use arbitrary cameras for heart and respiratory rate measurements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01459">Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization. (arXiv:2311.01459v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hassan_J/0/1/0/all/0/1">Jameel Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gani_H/0/1/0/all/0/1">Hanan Gani</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussein_N/0/1/0/all/0/1">Noor Hussein</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1">Muhammad Uzair Khattak</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a></p>
<p>The promising zero-shot generalization of vision-language models such as CLIP
has led to their adoption using prompt learning for numerous downstream tasks.
Previous works have shown test-time prompt tuning using entropy minimization to
adapt text prompts for unseen domains. While effective, this overlooks the key
cause for performance degradation to unseen domains -- distribution shift. In
this work, we explicitly handle this problem by aligning the
out-of-distribution (OOD) test sample statistics to those of the source data
using prompt tuning. We use a single test sample to adapt multi-modal prompts
at test time by minimizing the feature distribution shift to bridge the gap in
the test domain. Evaluating against the domain generalization benchmark, our
method improves zero-shot top- 1 accuracy beyond existing prompt-learning
techniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset
generalization with unseen categories across 10 datasets, our method improves
consistently across all datasets compared to the existing state-of-the-art. Our
source code and models are available at
https://jameelhassan.github.io/promptalign.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03774">Meta-Adapter: An Online Few-shot Learner for Vision-Language Model. (arXiv:2311.03774v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Cheng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Lin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1">Ruoyi Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hongbin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yixiao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>The contrastive vision-language pre-training, known as CLIP, demonstrates
remarkable potential in perceiving open-world visual concepts, enabling
effective zero-shot image recognition. Nevertheless, few-shot learning methods
based on CLIP typically require offline fine-tuning of the parameters on
few-shot samples, resulting in longer inference time and the risk of
over-fitting in certain domains. To tackle these challenges, we propose the
Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features
guided by the few-shot samples in an online manner. With a few training
samples, our method can enable effective few-shot learning capabilities and
generalize to unseen data or tasks without additional fine-tuning, achieving
competitive performance and high efficiency. Without bells and whistles, our
approach outperforms the state-of-the-art online few-shot learning method by an
average of 3.6\% on eight image classification datasets with higher inference
speed. Furthermore, our model is simple and flexible, serving as a
plug-and-play module directly applicable to downstream tasks. Without further
fine-tuning, Meta-Adapter obtains notable performance improvements in
open-vocabulary object detection and segmentation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15830">A-JEPA: Joint-Embedding Predictive Architecture Can Listen. (arXiv:2311.15830v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1">Zhengcong Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1">Mingyuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Junshi Huang</a></p>
<p>This paper presents that the masked-modeling principle driving the success of
large foundational vision models can be effectively applied to audio by making
predictions in a latent space. We introduce Audio-based Joint-Embedding
Predictive Architecture (A-JEPA), a simple extension method for self-supervised
learning from the audio spectrum. Following the design of I-JEPA, our A-JEPA
encodes visible audio spectrogram patches with a curriculum masking strategy
via context encoder, and predicts the representations of regions sampled at
well-designed locations. The target representations of those regions are
extracted by the exponential moving average of context encoder, \emph{i.e.},
target encoder, on the whole spectrogram. We find it beneficial to transfer
random block masking into time-frequency aware masking in a curriculum manner,
considering the complexity of highly correlated in local time and frequency in
audio spectrograms. To enhance contextual semantic understanding and
robustness, we fine-tune the encoder with a regularized masking on target
datasets, instead of input dropping or zero. Empirically, when built with
Vision Transformers structure, we find A-JEPA to be highly scalable and sets
new state-of-the-art performance on multiple audio and speech classification
tasks, outperforming other recent models that use externally supervised
pre-training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16163">IODeep: an IOD for the introduction of deep learning in the DICOM standard. (arXiv:2311.16163v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Contino_S/0/1/0/all/0/1">Salvatore Contino</a>, <a href="http://arxiv.org/find/eess/1/au:+Cruciata_L/0/1/0/all/0/1">Luca Cruciata</a>, <a href="http://arxiv.org/find/eess/1/au:+Gambino_O/0/1/0/all/0/1">Orazio Gambino</a>, <a href="http://arxiv.org/find/eess/1/au:+Pirrone_R/0/1/0/all/0/1">Roberto Pirrone</a></p>
<p>Background and Objective: In recent years, Artificial Intelligence (AI) and
in particular Deep Neural Networks (DNN) became a relevant research topic in
biomedical image segmentation due to the availability of more and more data
sets along with the establishment of well known competitions. Despite the
popularity of DNN based segmentation on the research side, these techniques are
almost unused in the daily clinical practice even if they could support
effectively the physician during the diagnostic process. Apart from the issues
related to the explainability of the predictions of a neural model, such
systems are not integrated in the diagnostic workflow, and a standardization of
their use is needed to achieve this goal. Methods: This paper presents IODeep a
new DICOM Information Object Definition (IOD) aimed at storing both the weights
and the architecture of a DNN already trained on a particular image dataset
that is labeled as regards the acquisition modality, the anatomical region, and
the disease under investigation. Results: The IOD architecture is presented
along with a DNN selection algorithm from the PACS server based on the labels
outlined above, and a simple PACS viewer purposely designed for demonstrating
the effectiveness of the DICOM integration, while no modifications are required
on the PACS server side. Also a service based architecture in support of the
entire workflow has been implemented. Conclusion: IODeep ensures full
integration of a trained AI model in a DICOM infrastructure, and it is also
enables a scenario where a trained model can be either fine-tuned with hospital
data or trained in a federated learning scheme shared by different hospitals.
In this way AI models can be tailored to the real data produced by a Radiology
ward thus improving the physician decision making process. Source code is
freely available at https://github.com/CHILab1/IODeep.git
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00840">Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">LianLi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a></p>
<p>Catastrophic Forgetting (CF) is a prominent issue in continual learning.
Parameter isolation addresses this challenge by masking a sub-network for each
task to mitigate interference with old tasks. However, these sub-networks are
constructed relying on weight magnitude, which does not necessarily correspond
to the importance of weights, resulting in maintaining unimportant weights and
constructing redundant sub-networks. To overcome this limitation, inspired by
information bottleneck, which removes redundancy between adjacent network
layers, we propose \textbf{\underline{I}nformation \underline{B}ottleneck
\underline{M}asked sub-network (IBM)} to eliminate redundancy within
sub-networks. Specifically, IBM accumulates valuable information into essential
weights to construct redundancy-free sub-networks, not only effectively
mitigating CF by freezing the sub-networks but also facilitating new tasks
training through the transfer of valuable knowledge. Additionally, IBM
decomposes hidden representations to automate the construction process and make
it flexible. Extensive experiments demonstrate that IBM consistently
outperforms state-of-the-art methods. Notably, IBM surpasses the
state-of-the-art parameter isolation method with a 70\% reduction in the number
of parameters within sub-networks and an 80\% decrease in training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02133">Style Aligned Image Generation via Shared Attention. (arXiv:2312.02133v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1">Amir Hertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1">Shlomi Fruchter</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a></p>
<p>Large-scale Text-to-Image (T2I) models have rapidly gained prominence across
creative fields, generating visually compelling outputs from textual prompts.
However, controlling these models to ensure consistent style remains
challenging, with existing methods necessitating fine-tuning and manual
intervention to disentangle content and style. In this paper, we introduce
StyleAligned, a novel technique designed to establish style alignment among a
series of generated images. By employing minimal `attention sharing' during the
diffusion process, our method maintains style consistency across images within
T2I models. This approach allows for the creation of style-consistent images
using a reference style through a straightforward inversion operation. Our
method's evaluation across diverse styles and text prompts demonstrates
high-quality synthesis and fidelity, underscoring its efficacy in achieving
consistent style across various inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08048">Compositional Inversion for Stable Diffusion Models. (arXiv:2312.08048v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xulu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiao-Yong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jinlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoxiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a></p>
<p>Inversion methods, such as Textual Inversion, generate personalized images by
incorporating concepts of interest provided by user images. However, existing
methods often suffer from overfitting issues, where the dominant presence of
inverted concepts leads to the absence of other desired concepts. It stems from
the fact that during inversion, the irrelevant semantics in the user images are
also encoded, forcing the inverted concepts to occupy locations far from the
core distribution in the embedding space. To address this issue, we propose a
method that guides the inversion process towards the core distribution for
compositional embeddings. Additionally, we introduce a spatial regularization
approach to balance the attention on the concepts being composed. Our method is
designed as a post-training approach and can be seamlessly integrated with
other inversion methods. Experimental results demonstrate the effectiveness of
our proposed approach in mitigating the overfitting problem and generating more
diverse and balanced compositions of concepts in the synthesized images. The
source code is available at
https://github.com/zhangxulu1996/Compositional-Inversion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08567">ConFormer: A Novel Collection of Deep Learning Models to Assist Cardiologists in the Assessment of Cardiac Function. (arXiv:2312.08567v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Thomas_E/0/1/0/all/0/1">Ethan Thomas</a>, <a href="http://arxiv.org/find/eess/1/au:+Aslam_S/0/1/0/all/0/1">Salman Aslam</a></p>
<p>Cardiovascular diseases, particularly heart failure, are a leading cause of
death globally. The early detection of heart failure through routine
echocardiogram screenings is often impeded by the high cost and labor-intensive
nature of these procedures, a barrier that can mean the difference between life
and death. This paper presents ConFormer, a novel deep learning model designed
to automate the estimation of Ejection Fraction (EF) and Left Ventricular Wall
Thickness from echocardiograms. The implementation of ConFormer has the
potential to enhance preventative cardiology by enabling cost-effective,
accessible, and comprehensive heart health monitoring, thereby saving countless
lives. The source code is available at https://github.com/Aether111/ConFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08952">UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation. (arXiv:2312.08952v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kefu Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1">Kai Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiaolei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiangui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rongdong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1">Wei Hao</a></p>
<p>Multi-object tracking (MOT) in video sequences remains a challenging task,
especially in scenarios with significant camera movements. This is because
targets can drift considerably on the image plane, leading to erroneous
tracking outcomes. Addressing such challenges typically requires supplementary
appearance cues or Camera Motion Compensation (CMC). While these strategies are
effective, they also introduce a considerable computational burden, posing
challenges for real-time MOT. In response to this, we introduce UCMCTrack, a
novel motion model-based tracker robust to camera movements. Unlike
conventional CMC that computes compensation parameters frame-by-frame,
UCMCTrack consistently applies the same compensation parameters throughout a
video sequence. It employs a Kalman filter on the ground plane and introduces
the Mapped Mahalanobis Distance (MMD) as an alternative to the traditional
Intersection over Union (IoU) distance measure. By leveraging projected
probability distributions on the ground plane, our approach efficiently
captures motion patterns and adeptly manages uncertainties introduced by
homography projections. Remarkably, UCMCTrack, relying solely on motion cues,
achieves state-of-the-art performance across a variety of challenging datasets,
including MOT17, MOT20, DanceTrack and KITTI. More details and code are
available at https://github.com/corfyi/UCMCTrack
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09507">WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Huy Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Kieu_T/0/1/0/all/0/1">Tung Kieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>Text-video retrieval, a prominent sub-field within the domain of multimodal
information retrieval, has witnessed remarkable growth in recent years.
However, existing methods assume video scenes are consistent with unbiased
descriptions. These limitations fail to align with real-world scenarios since
descriptions can be influenced by annotator biases, diverse writing styles, and
varying textual perspectives. To overcome the aforementioned problems, we
introduce $\texttt{WAVER}$, a cross-domain knowledge distillation framework via
vision-language models through open-vocabulary knowledge designed to tackle the
challenge of handling different writing styles in video descriptions.
$\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie in
pre-trained vision-language models and employs an implicit knowledge
distillation approach to transfer text-based knowledge from a teacher model to
a vision-based student. Empirical studies conducted across four standard
benchmark datasets, encompassing various settings, provide compelling evidence
that $\texttt{WAVER}$ can achieve state-of-the-art performance in text-video
retrieval task while handling writing-style variations. The code is available
at: https://github.com/Fsoft-AIC/WAVER
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10813">Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1">Tianxiang Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1">Mengyao Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sicheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jungong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1">Guiguang Ding</a></p>
<p>With the development of large pre-trained vision-language models, how to
effectively transfer the knowledge of such foundational models to downstream
tasks becomes a hot topic, especially in a data-deficient scenario. Recently,
prompt tuning has become a popular solution. When adapting the vision-language
models, researchers freeze the parameters in the backbone and only design and
tune the prompts. On the one hand, the delicate design of prompt tuning
exhibits strong performance. On the other hand, complicated structures and
update rules largely increase the computation and storage cost. Motivated by
the observation that the evolution pattern of the generalization capability in
visual-language models aligns harmoniously with the trend of rank variations in
the prompt matrix during adaptation, we design a new type of prompt,
Re-parameterized Low-rank Prompt (RLP), for both efficient and effective
adaptation. Our method could largely reduce the number of tunable parameters
and storage space, which is quite beneficial in resource-limited scenarios.
Extensive experiments further demonstrate the superiority of RLP. In
particular, RLP shows comparable or even stronger performance than the latest
state-of-the-art methods with an extremely small number of parameters. On a
series of tasks over 11 datasets, RLP significantly increases the average
downstream accuracy of classic prompt tuning by up to 5.25% using merely 0.5K
parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13941">Controllable 3D Face Generation with Conditional Style Code Diffusion. (arXiv:2312.13941v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiaolong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianxin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zongxin Yang</a></p>
<p>Generating photorealistic 3D faces from given conditions is a challenging
task. Existing methods often rely on time-consuming one-by-one optimization
approaches, which are not efficient for modeling the same distribution content,
e.g., faces. Additionally, an ideal controllable 3D face generation model
should consider both facial attributes and expressions. Thus we propose a novel
approach called TEx-Face(TExt &amp; Expression-to-Face) that addresses these
challenges by dividing the task into three components, i.e., 3D GAN Inversion,
Conditional Style Code Diffusion, and 3D Face Decoding. For 3D GAN inversion,
we introduce two methods which aim to enhance the representation of style codes
and alleviate 3D inconsistencies. Furthermore, we design a style code denoiser
to incorporate multiple conditions into the style code and propose a data
augmentation strategy to address the issue of insufficient paired
visual-language data. Extensive experiments conducted on FFHQ, CelebA-HQ, and
CelebA-Dialog demonstrate the promising performance of our TEx-Face in
achieving the efficient and controllable generation of photorealistic 3D faces.
The code will be available at https://github.com/sxl142/TEx-Face.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02501">The cell signaling structure function. (arXiv:2401.02501v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aho_L/0/1/0/all/0/1">Layton Aho</a>, <a href="http://arxiv.org/find/cs/1/au:+Winter_M/0/1/0/all/0/1">Mark Winter</a>, <a href="http://arxiv.org/find/cs/1/au:+DeCarlo_M/0/1/0/all/0/1">Marc DeCarlo</a>, <a href="http://arxiv.org/find/cs/1/au:+Frismantiene_A/0/1/0/all/0/1">Agne Frismantiene</a>, <a href="http://arxiv.org/find/cs/1/au:+Blum_Y/0/1/0/all/0/1">Yannick Blum</a>, <a href="http://arxiv.org/find/cs/1/au:+Gagliardi_P/0/1/0/all/0/1">Paolo Armando Gagliardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pertz_O/0/1/0/all/0/1">Olivier Pertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1">Andrew R. Cohen</a></p>
<p>Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here an approach
to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell
microscopy movies unique in requiring no a priori knowledge of expected pattern
dynamics, and no training data. The proposed cell signaling structure function
(SSF) is a Kolmogorov structure function that optimally measures cell signaling
state as nuclear intensity w.r.t. surrounding cytoplasm, a significant
improvement compared to the current state-of-the-art cytonuclear ratio. SSF
kymographs store at each spatiotemporal cell centroid the SSF value, or a
functional output such as velocity. Patterns of similarity are identified via
the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input SSF kymographs as points
in a low dimensional embedding that optimally captures the pattern similarity
identified by the NCD throughout the space. The only parameter is the expected
cell radii ($\mu m$). A new formulation of the cluster structure function
optimally estimates how meaningful an embedding from the RKHS representation.
Results are presented quantifying the impact of ERK and AKT signaling between
different oncogenic mutations, and by the relation between ERK signaling and
cellular velocity patterns for movies of 2-D monolayers of human breast
epithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation
of ERK, and human induced pluripotent stem cells .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03302">Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hashemi_S/0/1/0/all/0/1">Seyed Mohammad Hossein Hashemi</a>, <a href="http://arxiv.org/find/eess/1/au:+Safari_L/0/1/0/all/0/1">Leila Safari</a>, <a href="http://arxiv.org/find/eess/1/au:+Taromi_A/0/1/0/all/0/1">Amirhossein Dadashzade Taromi</a></p>
<p>In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03621">Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI. (arXiv:2401.03621v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ellethy_H/0/1/0/all/0/1">Hanem Ellethy</a>, <a href="http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1">Shekhar S. Chandra</a>, <a href="http://arxiv.org/find/eess/1/au:+Vegh_V/0/1/0/all/0/1">Viktor Vegh</a></p>
<p>Traumatic Brain Injury (TBI) poses a significant global public health
challenge, contributing to high morbidity and mortality rates and placing a
substantial economic burden on healthcare systems worldwide. The diagnosis of
TBI relies on clinical information along with Computed Tomography (CT) scans.
Addressing the multifaceted challenges posed by TBI has seen the development of
innovative, data-driven approaches, for this complex condition. Particularly
noteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority
of TBI cases where conventional methods often fall short. As such, we review
the state-of-the-art Machine Learning (ML) techniques applied to clinical
information and CT scans in TBI, with a particular focus on mTBI. We categorize
ML applications based on their data sources, and there is a spectrum of ML
techniques used to date. Most of these techniques have primarily focused on
diagnosis, with relatively few attempts at predicting the prognosis. This
review may serve as a source of inspiration for future research studies aimed
at improving the diagnosis of TBI using data-driven approaches and standard
diagnostic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03854">TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment. (arXiv:2401.03854v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiquan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xinyan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_J/0/1/0/all/0/1">Jinming Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qinyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Sen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Wei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jinlong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xixin Cao</a></p>
<p>Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the
quality of AI-generated images (AIGIs) from a human perception perspective, has
emerged as a new topic in computer vision. Unlike common image quality
assessment tasks where images are derived from original ones distorted by
noise, blur, and compression, \textit{etc.}, in AIGCIQA tasks, images are
typically generated by generative models using text prompts. Considerable
efforts have been made in the past years to advance AIGCIQA. However, most
existing AIGCIQA methods regress predicted scores directly from individual
generated images, overlooking the information contained in the text prompts of
these images. This oversight partially limits the performance of these AIGCIQA
methods. To address this issue, we propose a text-image encoder-based
regression (TIER) framework. Specifically, we process the generated images and
their corresponding text prompts as inputs, utilizing a text encoder and an
image encoder to extract features from these text prompts and generated images,
respectively. To demonstrate the effectiveness of our proposed TIER method, we
conduct extensive experiments on several mainstream AIGCIQA databases,
including AGIQA-1K, AGIQA-3K, and AIGCIQA2023. The experimental results
indicate that our proposed TIER method generally demonstrates superior
performance compared to baseline in most cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04718">Jump Cut Smoothing for Talking Heads. (arXiv:2401.04718v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaojuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1">Taesung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1">Eli Shechtman</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Richard Zhang</a></p>
<p>A jump cut offers an abrupt, sometimes unwanted change in the viewing
experience. We present a novel framework for smoothing these jump cuts, in the
context of talking head videos. We leverage the appearance of the subject from
the other source frames in the video, fusing it with a mid-level representation
driven by DensePose keypoints and face landmarks. To achieve motion, we
interpolate the keypoints and landmarks between the end frames around the cut.
We then use an image translation network from the keypoints and source frames,
to synthesize pixels. Because keypoints can contain errors, we propose a
cross-modal attention scheme to select and pick the most appropriate source
amongst multiple options for each key point. By leveraging this mid-level
representation, our method can achieve stronger results than a strong video
interpolation baseline. We demonstrate our method on various jump cuts in the
talking head videos, such as cutting filler words, pauses, and even random
cuts. Our experiments show that we can achieve seamless transitions, even in
the challenging cases where the talking head rotates or moves drastically in
the jump cut.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05167">Watermark Text Pattern Spotting in Document Images. (arXiv:2401.05167v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krubinski_M/0/1/0/all/0/1">Mateusz Krubi&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Matcovici_S/0/1/0/all/0/1">Stefan Matcovici</a>, <a href="http://arxiv.org/find/cs/1/au:+Grigore_D/0/1/0/all/0/1">Diana Grigore</a>, <a href="http://arxiv.org/find/cs/1/au:+Voinea_D/0/1/0/all/0/1">Daniel Voinea</a>, <a href="http://arxiv.org/find/cs/1/au:+Popa_A/0/1/0/all/0/1">Alin-Ionut Popa</a></p>
<p>Watermark text spotting in document images can offer access to an often
unexplored source of information, providing crucial evidence about a record's
scope, audience and sometimes even authenticity. Stemming from the problem of
text spotting, detecting and understanding watermarks in documents inherits the
same hardships - in the wild, writing can come in various fonts, sizes and
forms, making generic recognition a very difficult problem. To address the lack
of resources in this field and propel further research, we propose a novel
benchmark (K-Watermark) containing 65,447 data samples generated using Wrender,
a watermark text patterns rendering procedure. A validity study using humans
raters yields an authenticity score of 0.51 against pre-generated watermarked
documents. To prove the usefulness of the dataset and rendering technique, we
developed an end-to-end solution (Wextract) for detecting the bounding box
instances of watermark text, while predicting the depicted text. To deal with
this specific task, we introduce a variance minimization loss and a
hierarchical self-attention mechanism. To the best of our knowledge, we are the
first to propose an evaluation benchmark and a complete solution for retrieving
watermarks from documents surpassing baselines by 5 AP points in detection and
4 points in character accuracy.
</p>
</p>
</div>

    </div>
    </body>
    