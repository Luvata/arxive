<!DOCTYPE html>
<html>
<head>
<title>2023-08-12-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.05219">Decoding Layer Saliency in Language Transformers. (arXiv:2308.05219v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_E/0/1/0/all/0/1">Elizabeth M. Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Castanon_G/0/1/0/all/0/1">Gregory Castanon</a></p>
<p>In this paper, we introduce a strategy for identifying textual saliency in
large-scale language models applied to classification tasks. In visual networks
where saliency is more well-studied, saliency is naturally localized through
the convolutional layers of the network; however, the same is not true in
modern transformer-stack networks used to process natural language. We adapt
gradient-based saliency methods for these networks, propose a method for
evaluating the degree of semantic coherence of each layer, and demonstrate
consistent improvement over numerous other methods for textual saliency on
multiple benchmark classification datasets. Our approach requires no additional
training or access to labelled data, and is comparatively very computationally
efficient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05269">A Novel Self-training Approach for Low-resource Speech Recognition. (arXiv:2308.05269v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Satwinder Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1">Feng Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruili Wang</a></p>
<p>In this paper, we propose a self-training approach for automatic speech
recognition (ASR) for low-resource settings. While self-training approaches
have been extensively developed and evaluated for high-resource languages such
as English, their applications to low-resource languages like Punjabi have been
limited, despite the language being spoken by millions globally. The scarcity
of annotated data has hindered the development of accurate ASR systems,
especially for low-resource languages (e.g., Punjabi and M\=aori languages). To
address this issue, we propose an effective self-training approach that
generates highly accurate pseudo-labels for unlabeled low-resource speech. Our
experimental analysis demonstrates that our approach significantly improves
word error rate, achieving a relative improvement of 14.94% compared to a
baseline model across four real speech datasets. Further, our proposed approach
reports the best results on the Common Voice Punjabi dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05281">Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zihui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1">Libby Hemphill</a>, <a href="http://arxiv.org/find/cs/1/au:+Baecher_G/0/1/0/all/0/1">Gregory B. Baecher</a></p>
<p>Effective disaster response is critical for affected communities. Responders
and decision-makers would benefit from reliable, timely measures of the issues
impacting their communities during a disaster, and social media offers a
potentially rich data source. Social media can reflect public concerns and
demands during a disaster, offering valuable insights for decision-makers to
understand evolving situations and optimize resource allocation. We used
Bidirectional Encoder Representations from Transformers (BERT) topic modeling
to cluster topics from Twitter data. Then, we conducted a temporal-spatial
analysis to examine the distribution of these topics across different regions
during the 2020 western U.S. wildfire season. Our results show that Twitter
users mainly focused on three topics:"health impact," "damage," and
"evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to
explore the magnitude and velocity of topic diffusion on Twitter. The results
displayed a clear relationship between topic trends and wildfire propagation
patterns. The estimated parameters obtained from the SIR model in selected
cities revealed that residents exhibited a high level of several concerns
during the wildfire. Our study details how the SIR model and topic modeling
using social media data can provide decision-makers with a quantitative
approach to measure disaster response and support their decision-making
processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05317">Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source Learning. (arXiv:2308.05317v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Alexander Hanbo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1">Mingyue Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Spiliopoulou_E/0/1/0/all/0/1">Evangelia Spiliopoulou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jie Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1">Patrick Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiguo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1">Bonan Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1">Kathleen McKeown</a>, <a href="http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1">Vittorio Castelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1">Bing Xiang</a></p>
<p>We present a novel approach for structured data-to-text generation that
addresses the limitations of existing methods that primarily focus on specific
types of structured data. Our proposed method aims to improve performance in
multi-task training, zero-shot and few-shot scenarios by providing a unified
representation that can handle various forms of structured data such as tables,
knowledge graph triples, and meaning representations. We demonstrate that our
proposed approach can effectively adapt to new structured forms, and can
improve performance in comparison to current methods. For example, our method
resulted in a 66% improvement in zero-shot BLEU scores when transferring models
trained on table inputs to a knowledge graph dataset. Our proposed method is an
important step towards a more general data-to-text generation framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05336">Developing an Informal-Formal Persian Corpus. (arXiv:2308.05336v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tajalli_V/0/1/0/all/0/1">Vahide Tajalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalantari_F/0/1/0/all/0/1">Fateme Kalantari</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1">Mehrnoush Shamsfard</a></p>
<p>Informal language is a style of spoken or written language frequently used in
casual conversations, social media, weblogs, emails and text messages. In
informal writing, the language faces some lexical and/or syntactic changes
varying among different languages. Persian is one of the languages with many
differences between its formal and informal styles of writing, thus developing
informal language processing tools for this language seems necessary. Such a
converter needs a large aligned parallel corpus of colloquial-formal sentences
which can be useful for linguists to extract a regulated grammar and
orthography for colloquial Persian as is done for the formal language. In this
paper we explain our methodology in building a parallel corpus of 50,000
sentence pairs with alignments in the word/phrase level. The sentences were
attempted to cover almost all kinds of lexical and syntactic changes between
informal and formal Persian, therefore both methods of exploring and collecting
from the different resources of informal scripts and following the phonological
and morphological patterns of changes were applied to find as much instances as
possible. The resulting corpus has about 530,000 alignments and a dictionary
containing 49,397 word and phrase pairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05341">Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT. (arXiv:2308.05341v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mindner_L/0/1/0/all/0/1">Lorenz Mindner</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlippe_T/0/1/0/all/0/1">Tim Schlippe</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaaff_K/0/1/0/all/0/1">Kristina Schaaff</a></p>
<p>Recently, generative AIs like ChatGPT have become available to the wide
public. These tools can for instance be used by students to generate essays or
whole theses. But how does a teacher know whether a text is written by a
student or an AI? In our work, we explore traditional and new features to (1)
detect text generated by AI from scratch and (2) text rephrased by AI. Since we
found that classification is more difficult when the AI has been instructed to
create the text in a way that a human would not recognize that it was generated
by an AI, we also investigate this more advanced case. For our experiments, we
produced a new text corpus covering 10 school topics. Our best systems to
classify basic and advanced human-generated/AI-generated texts have F1-scores
of over 96%. Our best systems for classifying basic and advanced
human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems
use a combination of perplexity, semantic, list lookup, error-based,
readability, AI feedback, and text vector features. Our results show that the
new features substantially help to improve the performance of many classifiers.
Our best basic text rephrasing detection system even outperforms GPTZero by
183.8% relative in F1-score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05342">Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yun Zhao</a></p>
<p>In Large Language Models (LLMs), there have been consistent advancements in
task-specific performance, largely influenced by effective prompt design. While
recent research on prompting has enhanced the reasoning capabilities of LLMs, a
gap remains in further improving their understanding abilities. In this study,
we introduce metacognitive prompting (MP), a strategy inspired by human
introspective reasoning processes. Using MP, LLMs undergo a systematic series
of structured, self-aware evaluations, drawing on both their vast inherent
knowledge and new insights. Our experiments involve five prevalent LLMs:
Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general
natural language understanding (NLU) tasks from the GLUE and SuperGLUE
benchmarks. Results indicate that, although GPT-4 consistently excels in most
tasks, PaLM, when equipped with MP, approaches its performance level.
Furthermore, across models and datasets, MP consistently outperforms existing
prompting methods, including standard and chain-of-thought prompting. This
study underscores the potential to amplify the understanding abilities of LLMs
and highlights the benefits of mirroring human introspective reasoning in NLU
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05361">WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1">Siqiao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Fan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hongyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shuo Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Caigao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">James Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Peng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiu_D/0/1/0/all/0/1">Dacheng Xiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Hongyuan Mei</a></p>
<p>We present WeaverBird, an intelligent dialogue system designed specifically
for the finance domain. Our system harnesses a large language model of GPT
architecture that has been tuned using extensive corpora of finance-related
text. As a result, our system possesses the capability to understand complex
financial queries, such as "How should I manage my investments during
inflation?", and provide informed responses. Furthermore, our system
incorporates a local knowledge base and a search engine to retrieve relevant
information. The final responses are conditioned on the search results and
include proper citations to the sources, thus enjoying an enhanced credibility.
Through a range of finance-related questions, we have demonstrated the superior
performance of our system compared to other models. To experience our system
firsthand, users can interact with our live demo at
https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at
https://www.youtube.com/watch?v=yofgeqnlrMc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05476">Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1">Anusuya Krishnan</a></p>
<p>Deceptive text classification is a critical task in natural language
processing that aims to identify deceptive or fraudulent content. This study
presents a comparative analysis of machine learning and transformer-based
approaches for deceptive text classification. We investigate the effectiveness
of traditional machine learning algorithms and state-of-the-art transformer
models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive
text. A labeled dataset consisting of deceptive and non-deceptive texts is used
for training and evaluation purposes. Through extensive experimentation, we
compare the performance metrics, including accuracy, precision, recall, and F1
score, of the different approaches. The results of this study shed light on the
strengths and limitations of machine learning and transformer-based methods for
deceptive text classification, enabling researchers and practitioners to make
informed decisions when dealing with deceptive content
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05481">LLM As DBA. (arXiv:2308.05481v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xuanhe Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guoliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a></p>
<p>Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
github.com/TsinghuaDatabaseGroup/DB-GPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05502">Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1">Candida M. Greco</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagarelli_A/0/1/0/all/0/1">Andrea Tagarelli</a></p>
<p>Transformer-based language models (TLMs) have widely been recognized to be a
cutting-edge technology for the successful development of deep-learning-based
solutions to problems and applications that require natural language processing
and understanding. Like for other textual domains, TLMs have indeed pushed the
state-of-the-art of AI approaches for many tasks of interest in the legal
domain. Despite the first Transformer model being proposed about six years ago,
there has been a rapid progress of this technology at an unprecedented rate,
whereby BERT and related models represent a major reference, also in the legal
domain. This article provides the first systematic overview of TLM-based
methods for AI-driven problems and tasks in the legal sphere. A major goal is
to highlight research advances in this field so as to understand, on the one
hand, how the Transformers have contributed to the success of AI in supporting
legal processes, and on the other hand, what are the current limitations and
opportunities for further research development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05574">Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual Translation of Dravidian Languages. (arXiv:2308.05574v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebadulla_D/0/1/0/all/0/1">Danish Ebadulla</a>, <a href="http://arxiv.org/find/cs/1/au:+Raman_R/0/1/0/all/0/1">Rahul Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1">S. Natarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_H/0/1/0/all/0/1">Hridhay Kiran Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1">Ashish Harish Shenoy</a></p>
<p>Current research in zero-shot translation is plagued by several issues such
as high compute requirements, increased training time and off target
translations. Proposed remedies often come at the cost of additional data or
compute requirements. Pivot based neural machine translation is preferred over
a single-encoder model for most settings despite the increased training and
evaluation time. In this work, we overcome the shortcomings of zero-shot
translation by taking advantage of transliteration and linguistic similarity.
We build a single encoder-decoder neural machine translation system for
Dravidian-Dravidian multilingual translation and perform zero-shot translation.
We compare the data vs zero-shot accuracy tradeoff and evaluate the performance
of our vanilla method against the current state of the art pivot based method.
We also test the theory that morphologically rich languages require large
vocabularies by restricting the vocabulary using an optimal transport based
technique. Our model manages to achieves scores within 3 BLEU of large-scale
pivot-based models when it is trained on 50\% of the language directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05576">Do Language Models Refer?. (arXiv:2308.05576v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mandelkern_M/0/1/0/all/0/1">Matthew Mandelkern</a>, <a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1">Tal Linzen</a></p>
<p>What do language models (LMs) do with language? Everyone agrees that they
produce sequences of (mostly) coherent sentences. But are they saying anything
with those strings or simply babbling in a convincing simulacrum of language
use? This is a vague question, and there are many ways of making it precise.
Here we will address one aspect of the question, namely, whether LMs' words
refer: that is, whether the outputs of LMs achieve "word-to-world" connections.
There is prima facie reason to think they do not since LMs do not interact with
the world in the way that ordinary language users do. Drawing on insights from
the externalist tradition in philosophy of language, we argue that appearances
are misleading and that there is good reason to think that LMs can refer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05596">You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content. (arXiv:2308.05596v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xinlei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zannettou_S/0/1/0/all/0/1">Savvas Zannettou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a></p>
<p>The spread of toxic content online is an important problem that has adverse
effects on user experience online and in our society at large. Motivated by the
importance and impact of the problem, research focuses on developing solutions
to detect toxic content, usually leveraging machine learning (ML) models
trained on human-annotated datasets. While these efforts are important, these
models usually do not generalize well and they can not cope with new trends
(e.g., the emergence of new toxic terms). Currently, we are witnessing a shift
in the approach to tackling societal issues online, particularly leveraging
large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora
and have strong generalizability. In this work, we investigate how we can use
LLMs and prompt learning to tackle the problem of toxic content, particularly
focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection,
and 3) Detoxification. We perform an extensive evaluation over five model
architectures and eight datasets demonstrating that LLMs with prompt learning
can achieve similar or even better performance compared to models trained on
these specific tasks. We find that prompt learning achieves around 10\%
improvement in the toxicity classification task compared to the baselines,
while for the toxic span detection task we find better performance to the best
baseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the
detoxification task, we find that prompt learning can successfully reduce the
average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05609">LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition. (arXiv:2308.05609v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruas_P/0/1/0/all/0/1">Pedro Ruas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sousa_D/0/1/0/all/0/1">Diana F. Sousa</a>, <a href="http://arxiv.org/find/cs/1/au:+Neves_A/0/1/0/all/0/1">Andr&#xe9; Neves</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_C/0/1/0/all/0/1">Carlos Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Couto_F/0/1/0/all/0/1">Francisco M. Couto</a></p>
<p>Biomedical Natural Language Processing (NLP) tends to become cumbersome for
most researchers, frequently due to the amount and heterogeneity of text to be
processed. To address this challenge, the industry is continuously developing
highly efficient tools and creating more flexible engineering solutions. This
work presents the integration between industry data engineering solutions for
efficient data processing and academic systems developed for Named Entity
Recognition (LasigeUnicage\_NER) and Relation Extraction (BiOnt). Our design
reflects an integration of those components with external knowledge in the form
of additional training data from other datasets and biomedical ontologies. We
used this pipeline in the 2022 LitCoin NLP Challenge, where our team
LasigeUnicage was awarded the 7th Prize out of approximately 200 participating
teams, reflecting a successful collaboration between the academia (LASIGE) and
the industry (Unicage). The software supporting this work is available at
\url{https://github.com/lasigeBioTM/Litcoin-Lasige_Unicage}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05633">IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer. (arXiv:2308.05633v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1">Keqiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiaohao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1">Mahesan Niranjan</a></p>
<p>Automated medical report generation has become increasingly important in
medical analysis. It can produce computer-aided diagnosis descriptions and thus
significantly alleviate the doctors' work. Inspired by the huge success of
neural machine translation and image captioning, various deep learning methods
have been proposed for medical report generation. However, due to the inherent
properties of medical data, including data imbalance and the length and
correlation between report sequences, the generated reports by existing methods
may exhibit linguistic fluency but lack adequate clinical accuracy. In this
work, we propose an image-to-indicator hierarchical transformer (IIHT)
framework for medical report generation. It consists of three modules, i.e., a
classifier module, an indicator expansion module and a generator module. The
classifier module first extracts image features from the input medical images
and produces disease-related indicators with their corresponding states. The
disease-related indicators are subsequently utilised as input for the indicator
expansion module, incorporating the "data-text-data" strategy. The
transformer-based generator then leverages these extracted features along with
image features as auxiliary information to generate final reports. Furthermore,
the proposed IIHT method is feasible for radiologists to modify disease
indicators in real-world scenarios and integrate the operations into the
indicator expansion module for fluent and accurate medical report generation.
Extensive experiments and comparisons with state-of-the-art methods under
various evaluation metrics demonstrate the great performance of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05646">AST-MHSA : Code Summarization using Multi-Head Self-Attention. (arXiv:2308.05646v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagaraj_Y/0/1/0/all/0/1">Yeshwanth Nagaraj</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1">Ujjwal Gupta</a></p>
<p>Code summarization aims to generate concise natural language descriptions for
source code. The prevailing approaches adopt transformer-based encoder-decoder
architectures, where the Abstract Syntax Tree (AST) of the source code is
utilized for encoding structural information. However, ASTs are much longer
than the corresponding source code, and existing methods ignore this size
constraint by directly feeding the entire linearized AST into the encoders.
This simplistic approach makes it challenging to extract truly valuable
dependency relations from the overlong input sequence and leads to significant
computational overhead due to self-attention applied to all nodes in the AST.
</p>
<p>To address this issue effectively and efficiently, we present a model,
AST-MHSA that uses multi-head attention to extract the important semantic
information from the AST. The model consists of two main components: an encoder
and a decoder. The encoder takes as input the abstract syntax tree (AST) of the
code and generates a sequence of hidden states. The decoder then takes these
hidden states as input and generates a natural language summary of the code.
</p>
<p>The multi-head attention mechanism allows the model to learn different
representations of the input code, which can be combined to generate a more
comprehensive summary. The model is trained on a dataset of code and summaries,
and the parameters of the model are optimized to minimize the loss between the
generated summaries and the ground-truth summaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05680">Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning. (arXiv:2308.05680v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1">Iknoor Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1">Carolina Scarton</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xingyi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1">Kalina Bontcheva</a></p>
<p>The task of retrieving already debunked narratives aims to detect stories
that have already been fact-checked. The successful detection of claims that
have already been debunked not only reduces the manual efforts of professional
fact-checkers but can also contribute to slowing the spread of misinformation.
Mainly due to the lack of readily available data, this is an understudied
problem, particularly when considering the cross-lingual task, i.e. the
retrieval of fact-checking articles in a language different from the language
of the online post being checked. This paper fills this gap by (i) creating a
novel dataset to enable research on cross-lingual retrieval of already debunked
narratives, using tweets as queries to a database of fact-checking articles;
(ii) presenting an extensive experiment to benchmark fine-tuned and
off-the-shelf multilingual pre-trained Transformer models for this task; and
(iii) proposing a novel multistage framework that divides this cross-lingual
debunk retrieval task into refinement and re-ranking stages. Results show that
the task of cross-lingual retrieval of already debunked narratives is
challenging and off-the-shelf Transformer models fail to outperform a strong
lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework
is robust, outperforming BM25 in most scenarios and enabling cross-domain and
zero-shot learning, without significantly harming the model's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05696">A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment. (arXiv:2308.05696v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yingxiu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bowen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1">Binyuan Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haiyang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Nevin L. Zhang</a></p>
<p>Training large language models (LLMs) with open-domain instruction data has
yielded remarkable success in aligning to end tasks and user preferences.
Extensive research has highlighted that enhancing the quality and diversity of
instruction data consistently improves performance. However, the impact of data
complexity, as a crucial metric, remains relatively unexplored in three
aspects: (1) scaling law, where the sustainability of performance improvements
with increasing complexity is uncertain, (2) additional tokens, whether the
improvement brought by complexity comes from introducing more training tokens,
and (3) curriculum tuning, where the potential advantages of incorporating
instructions ranging from easy to difficult are not yet fully understood. In
this paper, we propose \textit{tree-instruct} to systematically enhance the
complexity of instruction data in a controllable manner. This approach adds a
specified number of nodes into the instruction semantic tree, yielding new
instruction data based on the modified tree. By adjusting the number of added
nodes, we can control the difficulty level in the modified instruction data.
Our preliminary experiments reveal the following insights: (1) Increasing
complexity consistently leads to sustained performance improvements. For
instance, using 1,000 instruction data and 10 nodes resulted in a substantial
24\% increase in win rate. (2) Under the same token budget, a few complex
instructions outperform diverse yet simple instructions. (3) Curriculum
instruction tuning might not yield the anticipated results; focusing on
increasing complexity appears to be the key.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05725">EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis. (arXiv:2308.05725v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tu Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">Wei-Ning Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+DAvirro_A/0/1/0/all/0/1">Antony D&#x27;Avirro</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bowen Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1">Itai Gat</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazel_Zarani_M/0/1/0/all/0/1">Maryam Fazel-Zarani</a>, <a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1">Tal Remez</a>, <a href="http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1">Jade Copet</a>, <a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1">Gabriel Synnaeve</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1">Michael Hassid</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1">Felix Kreuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1">Yossi Adi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1">Emmanuel Dupoux</a></p>
<p>Recent work has shown that it is possible to resynthesize high-quality speech
based, not on text, but on low bitrate discrete units that have been learned in
a self-supervised fashion and can therefore capture expressive aspects of
speech that are hard to transcribe (prosody, voice styles, non-verbal
vocalization). The adoption of these methods is still limited by the fact that
most speech synthesis datasets are read, severely limiting spontaneity and
expressivity. Here, we introduce Expresso, a high-quality expressive speech
dataset for textless speech synthesis that includes both read speech and
improvised dialogues rendered in 26 spontaneous expressive styles. We
illustrate the challenges and potentials of this dataset with an expressive
resynthesis benchmark where the task is to encode the input in low-bitrate
units and resynthesize it in a target voice while preserving content and style.
We evaluate resynthesis quality with automatic metrics for different
self-supervised discrete encoders, and explore tradeoffs between quality,
bitrate and invariance to speaker and style. All the dataset, evaluation
metrics and baseline models are open source
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.02399">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Longtian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Ziyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Ziyao Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zilu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yafeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guangnan Zhang</a></p>
<p>Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention
recently for its transferable visual representation learning. However, due to
the semantic gap within datasets, CLIP's pre-trained image-text alignment
becomes sub-optimal on downstream tasks, which severely harms its transferring
performance. To better adapt the cross-modality embedding space, we propose to
enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide
textual features of different categories to adaptively explore informative
regions on the image and aggregate visual features by attention mechanisms. In
this way, the texts become visual-guided, namely, more semantically correlated
with downstream images, which greatly benefits the category-wise matching
process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known
classification datasets to demonstrate its effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01944">Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v5 [cs.FL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yunhao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1">Jean-Rapha&#xeb;l Gaglione</a>, <a href="http://arxiv.org/find/cs/1/au:+Neary_C/0/1/0/all/0/1">Cyrus Neary</a>, <a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1">Ufuk Topcu</a></p>
<p>Automaton-based representations of task knowledge play an important role in
control and planning for sequential decision-making problems. However,
obtaining the high-level task knowledge required to build such automata is
often difficult. Meanwhile, large-scale generative language models (GLMs) can
automatically generate relevant task knowledge. However, the textual outputs
from GLMs cannot be formally verified or used for sequential decision-making.
We propose a novel algorithm named GLM2FSA, which constructs a finite state
automaton (FSA) encoding high-level task knowledge from a brief
natural-language description of the task goal. GLM2FSA first sends queries to a
GLM to extract task knowledge in textual form, and then it builds an FSA to
represent this text-based knowledge. The proposed algorithm thus fills the gap
between natural-language task descriptions and automaton-based representations,
and the constructed FSA can be formally verified against user-defined
specifications. We accordingly propose a method to iteratively refine the
queries to the GLM based on the outcomes, e.g., counter-examples, from
verification. We demonstrate GLM2FSA's ability to build and refine
automaton-based representations of everyday tasks (e.g., crossing a road), and
also of tasks that require highly-specialized knowledge (e.g., executing secure
multi-party computation).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13817">Let&#x27;s have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1">Sakib Shahriar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1">Kadhim Hayawi</a></p>
<p>The emergence of an AI-powered chatbot that can generate human-like sentences
and write coherent essays has caught the world's attention. This paper
discusses the historical overview of chatbots and the technology behind Chat
Generative Pre-trained Transformer, better known as ChatGPT. Moreover,
potential applications of ChatGPT in various domains, including healthcare,
education, and research, are highlighted. Despite promising results, there are
several privacy and ethical concerns surrounding ChatGPT. In addition, we
highlight some of the important limitations of the current version of ChatGPT.
We also ask ChatGPT to provide its point of view and present its responses to
several questions we attempt to answer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14679">Synthesizing Mixed-type Electronic Health Records using Diffusion Models. (arXiv:2302.14679v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ceritli_T/0/1/0/all/0/1">Taha Ceritli</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosheh_G/0/1/0/all/0/1">Ghadeer O. Ghosheh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1">Vinod Kumar Chauhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1">Tingting Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Creagh_A/0/1/0/all/0/1">Andrew P. Creagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1">David A. Clifton</a></p>
<p>Electronic Health Records (EHRs) contain sensitive patient information, which
presents privacy concerns when sharing such data. Synthetic data generation is
a promising solution to mitigate these risks, often relying on deep generative
models such as Generative Adversarial Networks (GANs). However, recent studies
have shown that diffusion models offer several advantages over GANs, such as
generation of more realistic synthetic data and stable training in generating
data modalities, including image, text, and sound. In this work, we investigate
the potential of diffusion models for generating realistic mixed-type tabular
EHRs, comparing TabDDPM model with existing methods on four datasets in terms
of data quality, utility, privacy, and augmentation. Our experiments
demonstrate that TabDDPM outperforms the state-of-the-art models across all
evaluation metrics, except for privacy, which confirms the trade-off between
privacy and utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03531">From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shulin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shirong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yangning Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hai-Tao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yong Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hong-Gee Kim</a></p>
<p>Entity Set Expansion (ESE) is a critical task aiming to expand entities of
the target semantic class described by a small seed entity set. Most existing
ESE methods are retrieval-based frameworks that need to extract the contextual
features of entities and calculate the similarity between seed entities and
candidate entities. To achieve the two purposes, they should iteratively
traverse the corpus and the entity vocabulary provided in the datasets,
resulting in poor efficiency and scalability. The experimental results indicate
that the time consumed by the retrieval-based ESE methods increases linearly
with entity vocabulary and corpus size. In this paper, we firstly propose a
generative ESE framework, Generative Entity Set Expansion (GenExpan), which
utilizes a generative pre-trained language model to accomplish ESE task.
Specifically, a prefix tree is employed to guarantee the validity of entity
generation, and automatically generated class names are adopted to guide the
model to generate target entities. Moreover, we propose Knowledge Calibration
and Generative Ranking to further bridge the gap between generic knowledge of
the language model and the goal of ESE task. Experiments on publicly available
datasets show that GenExpan is efficient and effective. For efficiency,
expansion time consumed by GenExpan is independent of entity vocabulary and
corpus size, and GenExpan achieves an average 600% speedup compared to strong
baselines. For expansion performance, our framework outperforms previous
state-of-the-art ESE methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09797">Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a></p>
<p>The performance of Large Language Models (LLMs) in reasoning tasks depends
heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency
being critical methods that enhance this ability. However, these methods do not
fully exploit the answers generated by the LLM to guide subsequent responses.
This paper proposes a new prompting method, named Progressive-Hint Prompting
(PHP), that enables automatic multiple interactions between users and LLMs by
using previously generated answers as hints to progressively guide toward the
correct answers. PHP is orthogonal to CoT and self-consistency, making it easy
to combine with state-of-the-art techniques to further improve performance. We
conducted extensive and comprehensive experiments on seven benchmarks. The
results show that PHP significantly improves accuracy while remaining highly
efficient. For instance, with text-davinci-003, we observed a 4.2% improvement
on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction
in sample paths with self-consistency. With GPT-4 and PHP, we achieve
state-of-the-art performances on SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%),
AQuA (76.4% -&gt; 79.9%) and MATH (50.3% -&gt; 53.9%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11679">Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release. (arXiv:2304.11679v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1">Zhouhong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaoxuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Haoning Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zhuozhi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zihan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qianyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Sihang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hongwei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a></p>
<p>Domain knowledge refers to the in-depth understanding, expertise, and
familiarity with a specific subject, industry, field, or area of special
interest. The existing benchmarks are all lack of an overall design for domain
knowledge evaluation. Holding the belief that the real ability of domain
language understanding can only be fairly evaluated by an comprehensive and
in-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa
targets at testing Large Language Models (LLMs) on their domain knowledge
understanding, it features extensive domain coverage, large data volume, and a
continually updated data set based on Chinese 112 first-level subject
classifications. DomMa consist of 100,000 questions in both Chinese and English
sourced from graduate entrance examinations and undergraduate exams in Chinese
college. We have also propose designs to make benchmark and evaluation process
more suitable to LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02130">Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions. (arXiv:2306.02130v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1">Jana Strakov&#xe1;</a>, <a href="http://arxiv.org/find/cs/1/au:+Fucikova_E/0/1/0/all/0/1">Eva Fu&#x10d;&#xed;kov&#xe1;</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajic_J/0/1/0/all/0/1">Jan Haji&#x10d;</a>, <a href="http://arxiv.org/find/cs/1/au:+Uresova_Z/0/1/0/all/0/1">Zde&#x148;ka Ure&#x161;ov&#xe1;</a></p>
<p>In this project, we have investigated the use of advanced machine learning
methods, specifically fine-tuned large language models, for pre-annotating data
for a lexical extension task, namely adding descriptive words (verbs) to an
existing (but incomplete, as of yet) ontology of event types. Several research
questions have been focused on, from the investigation of a possible heuristics
to provide at least hints to annotators which verbs to include and which are
outside the current version of the ontology, to the possible use of the
automatic scores to help the annotators to be more efficient in finding a
threshold for identifying verbs that cannot be assigned to any existing class
and therefore they are to be used as seeds for a new class. We have also
carefully examined the correlation of the automatic scores with the human
annotation. While the correlation turned out to be strong, its influence on the
annotation proper is modest due to its near linearity, even though the mere
fact of such pre-annotation leads to relatively short annotation times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02697">Strahler Number of Natural Language Sentences in Comparison with Random Trees. (arXiv:2307.02697v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tanaka_Ishii_K/0/1/0/all/0/1">Kumiko Tanaka-Ishii</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_A/0/1/0/all/0/1">Akira Tanaka</a></p>
<p>The Strahler number was originally proposed to characterize the complexity of
river bifurcation and has found various applications. This article proposes
computation of the Strahler number's upper and lower limits for natural
language sentence tree structures. Through empirical measurements across
grammatically annotated data, the Strahler number of natural language sentences
is shown to be almost 3 or 4, similarly to the case of river bifurcation as
reported by Strahler (1957). From the theory behind the number, we show that it
is one kind of lower limit on the amount of memory required to process
sentences. We consider the Strahler number to provide reasoning that explains
reports showing that the number of required memory areas to process sentences
is 3 to 4 for parsing (Abney and Johnson, 1991; Schuler et al., 2010), and
reports indicating a psychological "magical number" of 3 to 5 (Cowan, 2001). An
analytical and empirical analysis shows that the Strahler number is not
constant but grows logarithmically; therefore, the Strahler number of sentences
derives from the range of sentence lengths. Furthermore, the Strahler number is
not different for random trees, which could suggest that its origin is not
specific to natural language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15644">Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jialu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yicong Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1">Stephen Gould</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1">Hao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a></p>
<p>Recent research in language-guided visual navigation has demonstrated a
significant demand for the diversity of traversable environments and the
quantity of supervision for training generalizable agents. To tackle the common
data scarcity issue in existing vision-and-language navigation datasets, we
propose an effective paradigm for generating large-scale data for learning,
which applies 1200+ photo-realistic environments from HM3D and Gibson datasets
and synthesizes 4.9 million instruction trajectory pairs using fully-accessible
resources on the web. Importantly, we investigate the influence of each
component in this paradigm on the agent's performance and study how to
adequately apply the augmented data to pre-train and fine-tune an agent. Thanks
to our large-scale dataset, the performance of an existing agent can be pushed
up (+11% absolute with regard to previous SoTA) to a significantly new best of
80% single-run success rate on the R2R test split by simple imitation learning.
The long-lasting generalization gap between navigating in seen and unseen
environments is also reduced to less than 1% (versus 8% in the previous best
method). Moreover, our paradigm also facilitates different models to achieve
new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous
environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03131">Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xianfeng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yijin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely
utilized across a range of natural language generation (NLG) tasks. However,
recent studies have revealed a weak correlation between these matching-based
metrics and human evaluations, especially when compared with neural-based
metrics like BLEURT. In this paper, we conjecture that the performance
bottleneck in matching-based metrics may be caused by the limited diversity of
references. To address this issue, we propose to utilize \textit{multiple
references} to enhance the consistency between these metrics and human
evaluations. Within the WMT Metrics benchmarks, we observe that the
multi-references F200spBLEU surpasses the conventional single-reference one by
an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based
BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the
data leakage issue in large language models (LLMs) can be mitigated to a large
extent by our multi-reference metric. We release the code and data at
\url{https://github.com/SefaZeng/LLM-Ref}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03762">GPT-4 Can&#x27;t Reason. (arXiv:2308.03762v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1">Konstantine Arkoudas</a></p>
<p>GPT-4 was released in March 2023 to wide acclaim, marking a very substantial
improvement across the board over GPT-3.5 (OpenAI's previously best model,
which had powered the initial release of ChatGPT). However, despite the
genuinely impressive improvement, there are good reasons to be highly skeptical
of GPT-4's ability to reason. This position paper discusses the nature of
reasoning; criticizes the current formulation of reasoning problems in the NLP
community, as well as the way in which LLM reasoning performance is currently
evaluated; introduces a small collection of 21 diverse reasoning problems; and
performs a detailed qualitative evaluation of GPT-4's performance on those
problems. Based on this analysis, the paper concludes that, despite its
occasional flashes of analytical brilliance, GPT-4 at present is utterly
incapable of reasoning.
</p>
</p>
</div>

    </div>
    </body>
    