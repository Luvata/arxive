<!DOCTYPE html>
<html>
<head>
<title>2023-10-29-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2310.16897">Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks. (arXiv:2310.16897v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Helland_S/0/1/0/all/0/1">Solveig Helland</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavagnin_E/0/1/0/all/0/1">Elena Gavagnin</a>, <a href="http://arxiv.org/find/cs/1/au:+Spindler_A/0/1/0/all/0/1">Alexandre de Spindler</a></p>
<p>The growing capabilities of transformer models pave the way for solving
increasingly complex NLP tasks. A key to supporting application-specific
requirements is the ability to fine-tune. However, compiling a fine-tuning
dataset tailored to complex tasks is tedious and results in large datasets,
limiting the ability to control transformer output. We present an approach in
which complex tasks are divided into simpler subtasks. Multiple transformer
models are fine-tuned to one subtask each, and lined up to accomplish the
complex task. This simplifies the compilation of fine-tuning datasets and
increases overall controllability. Using the example of reducing gender bias as
a complex task, we demonstrate our approach and show that it performs better
than using a single model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16924">Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors. (arXiv:2310.16924v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehandru_N/0/1/0/all/0/1">Nikita Mehandru</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Sweta Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yimin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoong_E/0/1/0/all/0/1">Elaine C Khoong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1">Ge Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1">Marine Carpuat</a>, <a href="http://arxiv.org/find/cs/1/au:+Salehi_N/0/1/0/all/0/1">Niloufar Salehi</a></p>
<p>A major challenge in the practical use of Machine Translation (MT) is that
users lack guidance to make informed decisions about when to rely on outputs.
Progress in quality estimation research provides techniques to automatically
assess MT quality, but these techniques have primarily been evaluated in vitro
by comparison against human judgments outside of a specific context of use.
This paper evaluates quality estimation feedback in vivo with a human study
simulating decision-making in high-stakes medical settings. Using Emergency
Department discharge instructions, we study how interventions based on quality
estimation versus backtranslation assist physicians in deciding whether to show
MT outputs to a patient. We find that quality estimation improves appropriate
reliance on MT, but backtranslation helps physicians detect more clinically
harmful errors that QE alone often misses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16931">CL-MASR: A Continual Learning Benchmark for Multilingual ASR. (arXiv:2310.16931v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Libera_L/0/1/0/all/0/1">Luca Della Libera</a>, <a href="http://arxiv.org/find/cs/1/au:+Mousavi_P/0/1/0/all/0/1">Pooneh Mousavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaiem_S/0/1/0/all/0/1">Salah Zaiem</a>, <a href="http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1">Cem Subakan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1">Mirco Ravanelli</a></p>
<p>Modern multilingual automatic speech recognition (ASR) systems like Whisper
have made it possible to transcribe audio in multiple languages with a single
model. However, current state-of-the-art ASR models are typically evaluated on
individual languages or in a multi-task setting, overlooking the challenge of
continually learning new languages. There is insufficient research on how to
add new languages without losing valuable information from previous data.
Furthermore, existing continual learning benchmarks focus mostly on vision and
language tasks, leaving continual learning for multilingual ASR largely
unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for
studying multilingual ASR in a continual learning setting. CL-MASR provides a
diverse set of continual learning methods implemented on top of large-scale
pretrained ASR models, along with common metrics to assess the effectiveness of
learning new languages while addressing the issue of catastrophic forgetting.
To the best of our knowledge, CL-MASR is the first continual learning benchmark
for the multilingual ASR task. The code is available at
https://github.com/speechbrain/benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16937">Learning Transfers over Several Programming Languages. (arXiv:2310.16937v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baltaji_R/0/1/0/all/0/1">Razan Baltaji</a>, <a href="http://arxiv.org/find/cs/1/au:+Pujar_S/0/1/0/all/0/1">Saurabh Pujar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandel_L/0/1/0/all/0/1">Louis Mandel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirzel_M/0/1/0/all/0/1">Martin Hirzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Buratti_L/0/1/0/all/0/1">Luca Buratti</a>, <a href="http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1">Lav Varshney</a></p>
<p>Large language models (LLMs) have recently become remarkably good at
improving developer productivity for high-resource programming languages. These
models use two kinds of data: large amounts of unlabeled code samples for
pretraining and relatively smaller amounts of labeled code samples for
fine-tuning or in-context learning. Unfortunately, many programming languages
are low-resource, lacking labeled samples for most tasks and often even lacking
unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or
new languages) miss out on the benefits of LLMs. Cross-lingual transfer
learning uses data from a source language to improve model performance on a
target language. It has been well-studied for natural languages, but has
received little attention for programming languages. This paper reports
extensive experiments on four tasks using a transformer-based LLM and 11 to 41
programming languages to explore the following questions. First, how well
cross-lingual transfer works for a given task across different language pairs.
Second, given a task and target language, how to best choose a source language.
Third, the characteristics of a language pair that are predictive of transfer
performance, and fourth, how that depends on the given task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16944">Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1">Lewis Tunstall</a>, <a href="http://arxiv.org/find/cs/1/au:+Beeching_E/0/1/0/all/0/1">Edward Beeching</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_N/0/1/0/all/0/1">Nathan Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1">Nazneen Rajani</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1">Kashif Rasul</a>, <a href="http://arxiv.org/find/cs/1/au:+Belkada_Y/0/1/0/all/0/1">Younes Belkada</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shengyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Werra_L/0/1/0/all/0/1">Leandro von Werra</a>, <a href="http://arxiv.org/find/cs/1/au:+Fourrier_C/0/1/0/all/0/1">Cl&#xe9;mentine Fourrier</a>, <a href="http://arxiv.org/find/cs/1/au:+Habib_N/0/1/0/all/0/1">Nathan Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarrazin_N/0/1/0/all/0/1">Nathan Sarrazin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanseviero_O/0/1/0/all/0/1">Omar Sanseviero</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1">Thomas Wolf</a></p>
<p>We aim to produce a smaller language model that is aligned to user intent.
Previous research has shown that applying distilled supervised fine-tuning
(dSFT) on larger models significantly improves task accuracy; however, these
models are unaligned, i.e. they do not respond well to natural prompts. To
distill this property, we experiment with the use of preference data from AI
Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model,
we apply distilled direct preference optimization (dDPO) to learn a chat model
with significantly improved intent alignment. The approach requires only a few
hours of training without any additional sampling during fine-tuning. The final
result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B
parameter models, and requires no human annotation. In particular, results on
MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access
RLHF-based model. Code, models, data, and tutorials for the system are
available at https://github.com/huggingface/alignment-handbook.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16964">Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation. (arXiv:2310.16964v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lango_M/0/1/0/all/0/1">Mateusz Lango</a>, <a href="http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1">Ond&#x159;ej Du&#x161;ek</a></p>
<p>Hallucination of text ungrounded in the input is a well-known problem in
neural data-to-text generation. Many methods have been proposed to mitigate it,
but they typically require altering model architecture or collecting additional
data, and thus cannot be easily applied to an existing model. In this paper, we
explore a new way to mitigate hallucinations by combining the probabilistic
output of a generator language model (LM) with the output of a special "text
critic" classifier, which guides the generation by assessing the match between
the input data and the text generated so far. Our method does not need any
changes to the underlying LM's architecture or training procedure and can thus
be combined with any model and decoding operating on word probabilities. The
critic does not need any additional training data, using the base LM's training
data and synthetic negative examples. Our experimental results show that our
method improves over the baseline on the WebNLG and OpenDialKG benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16968">Understanding Social Structures from Contemporary Literary Fiction using Character Interaction Graph -- Half Century Chronology of Influential Bengali Writers. (arXiv:2310.16968v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tripto_N/0/1/0/all/0/1">Nafis Irtiza Tripto</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1">Mohammed Eunus Ali</a></p>
<p>Social structures and real-world incidents often influence contemporary
literary fiction. Existing research in literary fiction analysis explains these
real-world phenomena through the manual critical analysis of stories.
Conventional Natural Language Processing (NLP) methodologies, including
sentiment analysis, narrative summarization, and topic modeling, have
demonstrated substantial efficacy in analyzing and identifying similarities
within fictional works. However, the intricate dynamics of character
interactions within fiction necessitate a more nuanced approach that
incorporates visualization techniques. Character interaction graphs (or
networks) emerge as a highly suitable means for visualization and information
retrieval from the realm of fiction. Therefore, we leverage character
interaction graphs with NLP-derived features to explore a diverse spectrum of
societal inquiries about contemporary culture's impact on the landscape of
literary fiction. Our study involves constructing character interaction graphs
from fiction, extracting relevant graph features, and exploiting these features
to resolve various real-life queries. Experimental evaluation of influential
Bengali fiction over half a century demonstrates that character interaction
graphs can be highly effective in specific assessments and information
retrieval from literary fiction. Our data and codebase are available at
https://cutt.ly/fbMgGEM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16990">STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leon Liyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiarui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1">Joel Ruben Antony Moniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1">Aditya Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1">Dhivya Piraviperumal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Tien Dung Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzou_N/0/1/0/all/0/1">Nicholas Tzou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>In the context of a voice assistant system, steering refers to the phenomenon
in which a user issues a follow-up command attempting to direct or clarify a
previous turn. We propose STEER, a steering detection model that predicts
whether a follow-up turn is a user's attempt to steer the previous command.
Constructing a training dataset for steering use cases poses challenges due to
the cold-start problem. To overcome this, we developed heuristic rules to
sample opt-in usage data, approximating positive and negative samples without
any annotation. Our experimental results show promising performance in
identifying steering intent, with over 95% accuracy on our sampled data.
Moreover, STEER, in conjunction with our sampling strategy, aligns effectively
with real-world steering scenarios, as evidenced by its strong zero-shot
performance on a human-graded evaluation set. In addition to relying solely on
user transcripts as input, we introduce STEER+, an enhanced version of the
model. STEER+ utilizes a semantic parse tree to provide more context on
out-of-vocabulary words, such as named entities that often occur at the
sentence boundary. This further improves model performance, reducing error rate
in domains where entities frequently appear, such as messaging. Lastly, we
present a data analysis that highlights the improvement in user experience when
voice assistants support steering use cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16992">How well can machine-generated texts be identified and can language models be trained to avoid identification?. (arXiv:2310.16992v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1">Sinclair Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Steuber_F/0/1/0/all/0/1">Florian Steuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Joao A. G. Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodosek_G/0/1/0/all/0/1">Gabi Dreo Rodosek</a></p>
<p>With the rise of generative pre-trained transformer models such as GPT-3,
GPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated
ones has become important. We refined five separate language models to generate
synthetic tweets, uncovering that shallow learning classification algorithms,
like Naive Bayes, achieve detection accuracy between 0.6 and 0.8.
</p>
<p>Shallow learning classifiers differ from human-based detection, especially
when using higher temperature values during text generation, resulting in a
lower detection rate. Humans prioritize linguistic acceptability, which tends
to be higher at lower temperature values. In contrast, transformer-based
classifiers have an accuracy of 0.9 and above. We found that using a
reinforcement learning approach to refine our generative models can
successfully evade BERT-based classifiers with a detection accuracy of 0.15 or
less.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16995">Quality &gt; Quantity: Synthetic Corpora from Foundation Models for Closed-Domain Extractive Question Answering. (arXiv:2310.16995v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1">Saptarshi Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Heaton_C/0/1/0/all/0/1">Connor Heaton</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shreya Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1">Preslav Nakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1">Prasenjit Mitra</a></p>
<p>Domain adaptation, the process of training a model in one domain and applying
it to another, has been extensively explored in machine learning. While
training a domain-specific foundation model (FM) from scratch is an option,
recent methods have focused on adapting pre-trained FMs for domain-specific
tasks. However, our experiments reveal that either approach does not
consistently achieve state-of-the-art (SOTA) results in the target domain. In
this work, we study extractive question answering within closed domains and
introduce the concept of targeted pre-training. This involves determining and
generating relevant data to further pre-train our models, as opposed to the
conventional philosophy of utilizing domain-specific FMs trained on a wide
range of data. Our proposed framework uses Galactica to generate synthetic,
``targeted'' corpora that align with specific writing styles and topics, such
as research papers and radiology reports. This process can be viewed as a form
of knowledge distillation. We apply our method to two biomedical extractive
question answering datasets, COVID-QA and RadQA, achieving a new benchmark on
the former and demonstrating overall improvements on the latter. Code available
at https://github.com/saptarshi059/CDQA-v1-Targetted-PreTraining/tree/main.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17010">This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fanconi_C/0/1/0/all/0/1">Claudio Fanconi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vandenhirtz_M/0/1/0/all/0/1">Moritz Vandenhirtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Husmann_S/0/1/0/all/0/1">Severin Husmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia E. Vogt</a></p>
<p>Prototype learning, a popular machine learning method designed for inherently
interpretable decisions, leverages similarities to learned prototypes for
classifying new data. While it is mainly applied in computer vision, in this
work, we build upon prior research and further explore the extension of
prototypical networks to natural language processing. We introduce a learned
weighted similarity measure that enhances the similarity computation by
focusing on informative dimensions of pre-trained sentence embeddings.
Additionally, we propose a post-hoc explainability mechanism that extracts
prediction-relevant words from both the prototype and input sentences. Finally,
we empirically demonstrate that our proposed method not only improves
predictive performance on the AG News and RT Polarity datasets over a previous
prototype-based approach, but also improves the faithfulness of explanations
compared to rationale-based recurrent convolutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17015">Data Augmentation for Emotion Detection in Small Imbalanced Text Data. (arXiv:2310.17015v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koufakou_A/0/1/0/all/0/1">Anna Koufakou</a>, <a href="http://arxiv.org/find/cs/1/au:+Grisales_D/0/1/0/all/0/1">Diego Grisales</a>, <a href="http://arxiv.org/find/cs/1/au:+jesus_R/0/1/0/all/0/1">Ragy Costa de jesus</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_O/0/1/0/all/0/1">Oscar Fox</a></p>
<p>Emotion recognition in text, the task of identifying emotions such as joy or
anger, is a challenging problem in NLP with many applications. One of the
challenges is the shortage of available datasets that have been annotated with
emotions. Certain existing datasets are small, follow different emotion
taxonomies and display imbalance in their emotion distribution. In this work,
we studied the impact of data augmentation techniques precisely when applied to
small imbalanced datasets, for which current state-of-the-art models (such as
RoBERTa) under-perform. Specifically, we utilized four data augmentation
methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and
ProtAugment) on three datasets that come from different sources and vary in
size, emotion categories and distributions. Our experimental results show that
using the augmented data when training the classifier model leads to
significant improvements. Finally, we conducted two case studies: a) directly
using the popular chat-GPT API to paraphrase text using different prompts, and
b) using external data to augment the training set. Results show the promising
potential of these methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17017">An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives. (arXiv:2310.17017v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Young Min Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Rai_S/0/1/0/all/0/1">Sunny Rai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1">Lyle Ungar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1">Jo&#xe3;o Sedoc</a>, <a href="http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1">Sharath Chandra Guntuku</a></p>
<p>Mental health conversational agents (a.k.a. chatbots) are widely studied for
their potential to offer accessible support to those experiencing mental health
challenges. Previous surveys on the topic primarily consider papers published
in either computer science or medicine, leading to a divide in understanding
and hindering the sharing of beneficial knowledge between both domains. To
bridge this gap, we conduct a comprehensive literature review using the PRISMA
framework, reviewing 534 papers published in both computer science and
medicine. Our systematic review reveals 136 key papers on building mental
health-related conversational agents with diverse characteristics of modeling
and experimental design techniques. We find that computer science papers focus
on LLM techniques and evaluating response quality using automated metrics with
little attention to the application while medical papers use rule-based
conversational agents and outcome metrics to measure the health outcomes of
participants. Based on our findings on transparency, ethics, and cultural
heterogeneity in this review, we provide a few recommendations to help bridge
the disciplinary divide and enable the cross-disciplinary development of mental
health conversational agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17019">Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zentner_K/0/1/0/all/0/1">K.R. Zentner</a>, <a href="http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1">Ryan Julian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1">Brian Ichter</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1">Gaurav S. Sukhatme</a></p>
<p>This paper combines two contributions. First, we introduce an extension of
the Meta-World benchmark, which we call "Language-World," which allows a large
language model to operate in a simulated robotic environment using
semi-structured natural language queries and scripted skills described using
natural language. By using the same set of tasks as Meta-World, Language-World
results can be easily compared to Meta-World results, allowing for a point of
comparison between recent methods using Large Language Models (LLMs) and those
using Deep Reinforcement Learning. Second, we introduce a method we call Plan
Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of
high-level plans using end-to-end demonstrations. Using Language-World, we show
that PCBC is able to achieve strong performance in a variety of few-shot
regimes, often achieving task generalization with as little as a single
demonstration. We have made Language-World available as open-source software at
https://github.com/krzentner/language-world/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17022">Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1">Sidharth Mudgal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganapathy_H/0/1/0/all/0/1">Harish Ganapathy</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">YaGuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yanping Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Heng-Tze Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1">Michael Collins</a>, <a href="http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1">Trevor Strohman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jilin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1">Alex Beutel</a>, <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a></p>
<p>We propose controlled decoding (CD), a novel off-policy reinforcement
learning method to control the autoregressive generation from language models
towards high reward outcomes. CD solves an off-policy reinforcement learning
problem through a value function for the reward, which we call a prefix scorer.
The prefix scorer is used at inference time to steer the generation towards
higher reward outcomes. We show that the prefix scorer may be trained on
(possibly) off-policy data to predict the expected reward when decoding is
continued from a partially decoded response. We empirically demonstrate that CD
is effective as a control mechanism on Reddit conversations corpus. We also
show that the modularity of the design of CD makes it possible to control for
multiple rewards, effectively solving a multi-objective reinforcement learning
problem with no additional complexity. Finally, we show that CD can be applied
in a novel blockwise fashion at inference-time, again without the need for any
training-time changes, essentially bridging the gap between the popular
best-of-$K$ strategy and token-level reinforcement learning. This makes CD a
promising approach for alignment of language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17034">Follow-on Question Suggestion via Voice Hints for Voice Assistants. (arXiv:2310.17034v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1">Besnik Fetahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Faustini_P/0/1/0/all/0/1">Pedro Faustini</a>, <a href="http://arxiv.org/find/cs/1/au:+Castellucci_G/0/1/0/all/0/1">Giuseppe Castellucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1">Anjie Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1">Oleg Rokhlenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1">Shervin Malmasi</a></p>
<p>The adoption of voice assistants like Alexa or Siri has grown rapidly,
allowing users to instantly access information via voice search. Query
suggestion is a standard feature of screen-based search experiences, allowing
users to explore additional topics. However, this is not trivial to implement
in voice-based settings. To enable this, we tackle the novel task of suggesting
questions with compact and natural voice hints to allow users to ask follow-up
questions.
</p>
<p>We define the task, ground it in syntactic theory and outline linguistic
desiderata for spoken hints. We propose baselines and an approach using
sequence-to-sequence Transformers to generate spoken hints from a list of
questions. Using a new dataset of 6681 input questions and human written hints,
we evaluated the models with automatic metrics and human evaluation. Results
show that a naive approach of concatenating suggested questions creates poor
voice hints. Our approach, which applies a linguistically-motivated pretraining
task was strongly preferred by humans for producing the most natural hints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17041">On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lodha_A/0/1/0/all/0/1">Abhilasha Lodha</a>, <a href="http://arxiv.org/find/cs/1/au:+Belapurkar_G/0/1/0/all/0/1">Gayatri Belapurkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chalkapurkar_S/0/1/0/all/0/1">Saloni Chalkapurkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1">Yuanming Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1">Reshmi Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Samyadeep Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrov_D/0/1/0/all/0/1">Dmitrii Petrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Soundararajan Srinivasan</a></p>
<p>Fine-tuning all the layers of a pre-trained neural language encoder (either
using all the parameters or using parameter-efficient methods) is often the
de-facto way of adapting it to a new task. We show evidence that for different
downstream language tasks, fine-tuning only a subset of layers is sufficient to
obtain performance that is close to and often better than fine-tuning all the
layers in the language encoder. We propose an efficient metric based on the
diagonal of the Fisher information matrix (FIM score), to select the candidate
layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE
tasks and across distinct language encoders, that this metric can effectively
select layers leading to a strong downstream performance. Our work highlights
that task-specific information corresponding to a given downstream task is
often localized within a few layers, and tuning only those is sufficient for
strong performance. Additionally, we demonstrate the robustness of the FIM
score to rank layers in a manner that remains constant during the optimization
process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17054">BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs&#x27; Generation. (arXiv:2310.17054v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yufei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Felix Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Large language models (LLMs) such as GPT-3 have demonstrated a strong
capability to generate coherent and contextually relevant text. However, amidst
their successes, a crucial issue persists: their generated outputs still lack
commonsense at times. Moreover, fine-tuning the entire LLM towards more
commonsensical outputs is computationally expensive if not infeasible. In this
paper, we present a computation-efficient framework that steers a frozen
Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,
producing a plausible output that incorporates a list of concepts in a
meaningful way). Specifically, we first construct a reference-free evaluator
that assigns a sentence with a commonsensical score by grounding the sentence
to a dynamic commonsense knowledge base from four different relational aspects.
We then use the scorer as the oracle for commonsense knowledge, and extend the
controllable generation method called NADO to train an auxiliary head that
guides a fixed PTLM to better satisfy the oracle. We test our framework on a
series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two
constrained concept-to-sentence benchmarks. Human evaluation results
demonstrate that our method consistently leads to the most commonsensical
outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17064">math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saidi_H/0/1/0/all/0/1">Hassen Saidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Susmit Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahai_T/0/1/0/all/0/1">Tuhin Sahai</a></p>
<p>As artificial intelligence (AI) gains greater adoption in a wide variety of
applications, it has immense potential to contribute to mathematical discovery,
by guiding conjecture generation, constructing counterexamples, assisting in
formalizing mathematics, and discovering connections between different
mathematical areas, to name a few.
</p>
<p>While prior work has leveraged computers for exhaustive mathematical proof
search, recent efforts based on large language models (LLMs) aspire to position
computing platforms as co-contributors in the mathematical research process.
Despite their current limitations in logic and mathematical tasks, there is
growing interest in melding theorem proving systems with foundation models.
This work investigates the applicability of LLMs in formalizing advanced
mathematical concepts and proposes a framework that can critically review and
check mathematical reasoning in research papers. Given the noted reasoning
shortcomings of LLMs, our approach synergizes the capabilities of proof
assistants, specifically PVS, with LLMs, enabling a bridge between textual
descriptions in academic papers and formal specifications in PVS. By harnessing
the PVS environment, coupled with data ingestion and conversion mechanisms, we
envision an automated process, called \emph{math-PVS}, to extract and formalize
mathematical theorems from research papers, offering an innovative tool for
academic review and discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17086">Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Deqing Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tian-Qi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1">Vatsal Sharan</a></p>
<p>Transformers are remarkably good at in-context learning (ICL) -- learning
from demonstrations without parameter updates -- but how they perform ICL
remains a mystery. Recent work suggests that Transformers may learn in-context
by internally running Gradient Descent, a first-order optimization method. In
this paper, we instead demonstrate that Transformers learn to implement
higher-order optimization methods to perform ICL. Focusing on in-context linear
regression, we show that Transformers learn to implement an algorithm very
similar to Iterative Newton's Method, a higher-order optimization method,
rather than Gradient Descent. Empirically, we show that predictions from
successive Transformer layers closely match different iterations of Newton's
Method linearly, with each middle layer roughly computing 3 iterations. In
contrast, exponentially more Gradient Descent steps are needed to match an
additional Transformers layer; this suggests that Transformers have an
comparable rate of convergence with high-order methods such as Iterative
Newton, which are exponentially faster than Gradient Descent. We also show that
Transformers can learn in-context on ill-conditioned data, a setting where
Gradient Descent struggles but Iterative Newton succeeds. Finally, we show
theoretical results which support our empirical findings and have a close
correspondence with them: we prove that Transformers can implement $k$
iterations of Newton's method with $\mathcal{O}(k)$ layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17119">FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge. (arXiv:2310.17119v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bayat_F/0/1/0/all/0/1">Farima Fatahi Bayat</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1">Kun Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Benjamin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1">Yisi Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Belyi_A/0/1/0/all/0/1">Anton Belyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khorshidi_S/0/1/0/all/0/1">Samira Khorshidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1">Ihab F. Ilyas</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunyao Li</a></p>
<p>Detecting factual errors in textual information, whether generated by large
language models (LLM) or curated by humans, is crucial for making informed
decisions. LLMs' inability to attribute their claims to external knowledge and
their tendency to hallucinate makes it difficult to rely on their responses.
Humans, too, are prone to factual errors in their writing. Since manual
detection and correction of factual errors is labor-intensive, developing an
automatic approach can greatly reduce human effort. We present FLEEK, a
prototype tool that automatically extracts factual claims from text, gathers
evidence from external knowledge sources, evaluates the factuality of each
claim, and suggests revisions for identified errors using the collected
evidence. Initial empirical evaluation on fact error detection (77-85\% F1)
shows the potential of FLEEK. A video demo of FLEEK can be found at
https://youtu.be/NapJFUlkPdQ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17120">Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1">Reshmi Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kajal_H/0/1/0/all/0/1">Harjeet Singh Kajal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1">Sharanya Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_D/0/1/0/all/0/1">Dhuri Shrivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Samyadeep Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1">Hansi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Soundararajan Srinivasan</a></p>
<p>Breaking down a document or a conversation into multiple contiguous segments
based on its semantic structure is an important and challenging problem in NLP,
which can assist many downstream tasks. However, current works on topic
segmentation often focus on segmentation of structured texts. In this paper, we
comprehensively analyze the generalization capabilities of state-of-the-art
topic segmentation models on unstructured texts. We find that: (a) Current
strategies of pre-training on a large corpus of structured text such as
Wiki-727K do not help in transferability to unstructured conversational data.
(b) Training from scratch with only a relatively small-sized dataset of the
target unstructured domain improves the segmentation results by a significant
margin. We stress-test our proposed Topic Segmentation approach by
experimenting with multiple loss functions, in order to mitigate effects of
imbalance in unstructured conversational datasets. Our empirical evaluation
indicates that Focal Loss function is a robust alternative to Cross-Entropy and
re-weighted Cross-Entropy loss function when segmenting unstructured and
semi-structured chats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17121">Test-time Augmentation for Factual Probing. (arXiv:2310.17121v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamoda_G/0/1/0/all/0/1">Go Kamoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinzerling_B/0/1/0/all/0/1">Benjamin Heinzerling</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1">Keisuke Sakaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1">Kentaro Inui</a></p>
<p>Factual probing is a method that uses prompts to test if a language model
"knows" certain world knowledge facts. A problem in factual probing is that
small changes to the prompt can lead to large changes in model output. Previous
work aimed to alleviate this problem by optimizing prompts via text mining or
fine-tuning. However, such approaches are relation-specific and do not
generalize to unseen relation types. Here, we propose to use test-time
augmentation (TTA) as a relation-agnostic method for reducing sensitivity to
prompt variations by automatically augmenting and ensembling prompts at test
time. Experiments show improved model calibration, i.e., with TTA, model
confidence better reflects prediction accuracy. Improvements in prediction
accuracy are observed for some models, but for other models, TTA leads to
degradation. Error analysis identifies the difficulty of producing high-quality
prompt variations as the main challenge for TTA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17130">M2C: Towards Automatic Multimodal Manga Complement. (arXiv:2310.17130v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hongcheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jiaqi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhoujun Li</a></p>
<p>Multimodal manga analysis focuses on enhancing manga understanding with
visual and textual features, which has attracted considerable attention from
both natural language processing and computer vision communities. Currently,
most comics are hand-drawn and prone to problems such as missing pages, text
contamination, and aging, resulting in missing comic text content and seriously
hindering human comprehension. In other words, the Multimodal Manga Complement
(M2C) task has not been investigated, which aims to handle the aforementioned
issues by providing a shared semantic space for vision and language
understanding. To this end, we first propose the Multimodal Manga Complement
task by establishing a new M2C benchmark dataset covering two languages. First,
we design a manga argumentation method called MCoT to mine event knowledge in
comics with large language models. Then, an effective baseline FVP-M$^{2}$
using fine-grained visual prompts is proposed to support manga complement.
Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for
Multimodal Mange Complement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17133">Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1">Yuxin Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1">Chuanhao Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jingbo Zhu</a></p>
<p>This paper presents an in-depth study of multimodal machine translation
(MMT), examining the prevailing understanding that MMT systems exhibit
decreased sensitivity to visual information when text inputs are complete.
Instead, we attribute this phenomenon to insufficient cross-modal interaction,
rather than image information redundancy. A novel approach is proposed to
generate parallel Visual Question-Answering (VQA) style pairs from the source
text, fostering more robust cross-modal interaction. Using Large Language
Models (LLMs), we explicitly model the probing signal in MMT to convert it into
VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask
learning framework is introduced to incorporate explicit probing signals from
the dataset into the MMT training process. Experimental results on two
widely-used benchmarks demonstrate the effectiveness of this novel approach.
Our code and data would be available at:
\url{https://github.com/libeineu/MMT-VQA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17140">Symbolic Planning and Code Generation for Grounded Dialogue. (arXiv:2310.17140v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1">Justin T. Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenting Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Derek Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaduguru_S/0/1/0/all/0/1">Saujas Vaduguru</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1">Daniel Fried</a></p>
<p>Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code's output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system's performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17143">Supercharging academic writing with generative AI: framework, techniques, and caveats. (arXiv:2310.17143v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhicheng Lin</a></p>
<p>Academic writing is an indispensable yet laborious part of the research
enterprise. This Perspective maps out principles and methods for using
generative artificial intelligence (AI), specifically large language models
(LLMs), to elevate the quality and efficiency of academic writing. We introduce
a human-AI collaborative framework that delineates the rationale (why), process
(how), and nature (what) of AI engagement in writing. The framework pinpoints
both short-term and long-term reasons for engagement and their underlying
mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals
the role of AI throughout the writing process, conceptualized through a
two-stage model for human-AI collaborative writing, and the nature of AI
assistance in writing, represented through a model of writing-assistance types
and levels. Building on this framework, we describe effective prompting
techniques for incorporating AI into the writing routine (outlining, drafting,
and editing) as well as strategies for maintaining rigorous scholarship,
adhering to varied journal policies, and avoiding overreliance on AI.
Ultimately, the prudent integration of AI into academic writing can ease the
communication burden, empower authors, accelerate discovery, and promote
diversity in science.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17166">X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity. (arXiv:2310.17166v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1">Taejun Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinhyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Deokyeong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Seong Hoon Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jihoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taeuk Kim</a></p>
<p>Cross-lingual transfer (XLT) is an emergent ability of multilingual language
models that preserves their performance on a task to a significant extent when
evaluated in languages that were not included in the fine-tuning process. While
English, due to its widespread usage, is typically regarded as the primary
language for model adaption in various tasks, recent studies have revealed that
the efficacy of XLT can be amplified by selecting the most appropriate source
languages based on specific conditions. In this work, we propose the
utilization of sub-network similarity between two languages as a proxy for
predicting the compatibility of the languages in the context of XLT. Our
approach is model-oriented, better reflecting the inner workings of foundation
models. In addition, it requires only a moderate amount of raw text from
candidate languages, distinguishing it from the majority of previous methods
that rely on external resources. In experiments, we demonstrate that our method
is more effective than baselines across diverse tasks. Specifically, it shows
proficiency in ranking candidates for zero-shot XLT, achieving an improvement
of 4.6% on average in terms of NDCG@3. We also provide extensive analyses that
confirm the utility of sub-networks for XLT prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17191">How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiahai Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1">Jacob Steinhardt</a></p>
<p>To correctly use in-context information, language models (LMs) must bind
entities to their attributes. For example, given a context describing a "green
square" and a "blue circle", LMs must bind the shapes to their respective
colors. We analyze LM representations and identify the binding ID mechanism: a
general mechanism for solving the binding problem, which we observe in every
sufficiently large model from the Pythia and LLaMA families. Using causal
interventions, we show that LMs' internal activations represent binding
information by attaching binding ID vectors to corresponding entities and
attributes. We further show that binding ID vectors form a continuous subspace,
in which distances between binding ID vectors reflect their discernability.
Overall, our results uncover interpretable strategies in LMs for representing
symbolic knowledge in-context, providing a step towards understanding general
in-context reasoning in large-scale LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17207">Efficient Data Fusion using the Tsetlin Machine. (arXiv:2310.17207v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1">Rupsa Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Zadorozhny_V/0/1/0/all/0/1">Vladimir I. Zadorozhny</a>, <a href="http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1">Ole-Christoffer Granmo</a></p>
<p>We propose a novel way of assessing and fusing noisy dynamic data using a
Tsetlin Machine. Our approach consists in monitoring how explanations in form
of logical clauses that a TM learns changes with possible noise in dynamic
data. This way TM can recognize the noise by lowering weights of previously
learned clauses, or reflect it in the form of new clauses. We also perform a
comprehensive experimental study using notably different datasets that
demonstrated high performance of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17217">Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1">Chenze Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhengrui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Maximum likelihood estimation (MLE) is a statistical method used to estimate
the parameters of a probability distribution that best explain the observed
data. In the context of text generation, MLE is often used to train generative
language models, which can then be used to generate new text. However, we argue
that MLE is not always necessary and optimal, especially for closed-ended text
generation tasks like machine translation. In these tasks, the goal of model is
to generate the most appropriate response, which does not necessarily require
it to estimate the entire data distribution with MLE. To this end, we propose a
novel class of training objectives based on convex functions, which enables
text generation models to focus on highly probable outputs without having to
estimate the entire data distribution. We investigate the theoretical
properties of the optimal predicted distribution when applying convex functions
to the loss, demonstrating that convex functions can sharpen the optimal
distribution, thereby enabling the model to better capture outputs with high
probabilities. Experiments on various text generation tasks and models show the
effectiveness of our approach. It enables autoregressive models to bridge the
gap between greedy and beam search, and facilitates the learning of
non-autoregressive models with a maximum improvement of 9+ BLEU points.
Moreover, our approach also exhibits significant impact on large language
models (LLMs), substantially enhancing their generative capability on various
tasks. Source code is available at
\url{https://github.com/ictnlp/Convex-Learning}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17228">TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khatry_A/0/1/0/all/0/1">Anirudh Khatry</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1">Sumit Gulwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Priyanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1">Vu Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Singha_A/0/1/0/all/0/1">Ananya Singha</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mukul Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1">Gust Verbruggen</a></p>
<p>Target similarity tuning (TST) is a method of selecting relevant examples in
natural language (NL) to code generation through large language models (LLMs)
to improve performance. Its goal is to adapt a sentence embedding model to have
the similarity between two NL inputs match the similarity between their
associated code outputs. In this paper, we propose different methods to apply
and improve TST in the real world. First, we replace the sentence transformer
with embeddings from a larger model, which reduces sensitivity to the language
distribution and thus provides more flexibility in synthetic generation of
examples, and we train a tiny model that transforms these embeddings to a space
where embedding similarity matches code similarity, which allows the model to
remain a black box and only requires a few matrix multiplications at inference
time. Second, we how to efficiently select a smaller number of training
examples to train the TST model. Third, we introduce a ranking-based evaluation
for TST that does not require end-to-end code generation experiments, which can
be expensive to perform.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17230">Codebook Features: Sparse and Discrete Interpretability for Neural Networks. (arXiv:2310.17230v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1">Alex Tamkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Taufeeque_M/0/1/0/all/0/1">Mohammad Taufeeque</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a></p>
<p>Understanding neural networks is challenging in part because of the dense,
continuous nature of their hidden states. We explore whether we can train
neural networks to have hidden states that are sparse, discrete, and more
interpretable by quantizing their continuous features into what we call
codebook features. Codebook features are produced by finetuning neural networks
with vector quantization bottlenecks at each layer, producing a network whose
hidden features are the sum of a small number of discrete vector codes chosen
from a larger codebook. Surprisingly, we find that neural networks can operate
under this extreme bottleneck with only modest degradation in performance. This
sparse, discrete bottleneck also provides an intuitive way of controlling
neural network behavior: first, find codes that activate when the desired
behavior is present, then activate those same codes during generation to elicit
that behavior. We validate our approach by training codebook Transformers on
several different datasets. First, we explore a finite state machine dataset
with far more hidden states than neurons. In this setting, our approach
overcomes the superposition problem by assigning states to distinct codes, and
we find that we can make the neural network behave as if it is in a different
state by activating the code for that state. Second, we train Transformer
language models with up to 410M parameters on two natural language datasets. We
identify codes in these models representing diverse, disentangled concepts
(ranging from negative emotions to months of the year) and find that we can
guide the model to generate different topics by activating the appropriate
codes during inference. Overall, codebook features appear to be a promising
unit of analysis and control for neural networks and interpretability. Our
codebase and models are open-sourced at
https://github.com/taufeeque9/codebook-features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17233">EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning. (arXiv:2310.17233v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Ping Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiangpeng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Baosong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dayiheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jun Xie</a></p>
<p>Expressing universal semantics common to all languages is helpful in
understanding the meanings of complex and culture-specific sentences. The
research theme underlying this scenario focuses on learning universal
representations across languages with the usage of massive parallel corpora.
However, due to the sparsity and scarcity of parallel data, there is still a
big challenge in learning authentic ``universals'' for any two languages. In
this paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm,
to learn (X)Cross-lingual universals with the aid of excessive multilingual
non-parallel data. EMMA-X unifies the cross-lingual representation learning
task and an extra semantic relation prediction task within an EM framework.
Both the extra semantic classifier and the cross-lingual sentence encoder
approximate the semantic relation of two sentences, and supervise each other
until convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly
introduced benchmark containing 12 widely studied cross-lingual tasks that
fully depend on sentence-level representations. Results reveal that EMMA-X
achieves state-of-the-art performance. Further geometric analysis of the built
representation space with three requirements demonstrates the superiority of
EMMA-X over advanced models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17238">Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhaohui Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Songlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1">Kewei Tu</a></p>
<p>Entity and Relation Extraction (ERE) is an important task in information
extraction. Recent marker-based pipeline models achieve state-of-the-art
performance, but still suffer from the error propagation issue. Also, most of
current ERE models do not take into account higher-order interactions between
multiple entities and relations, while higher-order modeling could be
beneficial.In this work, we propose HyperGraph neural network for ERE
($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based
pipleline model). To alleviate error propagation,we use a high-recall pruner
mechanism to transfer the burden of entity identification and labeling from the
NER module to the joint module of our model. For higher-order modeling, we
build a hypergraph, where nodes are entities (provided by the span pruner) and
relations thereof, and hyperedges encode interactions between two different
relations or between a relation and its associated subject and object entities.
We then run a hypergraph neural network for higher-order inference by applying
message passing over the built hypergraph. Experiments on three widely used
benchmarks (\acef{}, \ace{} and \scierc{}) for ERE task show significant
improvements over the previous state-of-the-art PL-marker.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17271">Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?. (arXiv:2310.17271v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alajrami_A/0/1/0/all/0/1">Ahmed Alajrami</a>, <a href="http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1">Katerina Margatina</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a></p>
<p>Understanding how and what pre-trained language models (PLMs) learn about
language is an open challenge in natural language processing. Previous work has
focused on identifying whether they capture semantic and syntactic information,
and how the data or the pre-training objective affects their performance.
However, to the best of our knowledge, no previous work has specifically
examined how information loss in input token characters affects the performance
of PLMs. In this study, we address this gap by pre-training language models
using small subsets of characters from individual tokens. Surprisingly, we find
that pre-training even under extreme settings, i.e. using only one character of
each token, the performance retention in standard NLU benchmarks and probing
tasks compared to full-token models is high. For instance, a model pre-trained
only on single first characters from tokens achieves performance retention of
approximately $90$\% and $77$\% of the full-token model in SuperGLUE and GLUE
tasks, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17279">Automatic Logical Forms improve fidelity in Table-to-Text generation. (arXiv:2310.17279v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1">I&#xf1;igo Alonso</a>, <a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1">Eneko Agirre</a></p>
<p>Table-to-text systems generate natural language statements from structured
data like tables. While end-to-end techniques suffer from low factual
correctness (fidelity), a previous study reported gains when using manual
logical forms (LF) that represent the selected content and the semantics of the
target text. Given the manual step, it was not clear whether automatic LFs
would be effective, or whether the improvement came from content selection
alone. We present TlT which, given a table and a selection of the content,
first produces LFs and then the textual statement. We show for the first time
that automatic LFs improve quality, with an increase in fidelity of 30 points
over a comparable system not using LFs. Our experiments allow to quantify the
remaining challenges for high factual correctness, with automatic selection of
content coming first, followed by better Logic-to-Text generation and, to a
lesser extent, better Table-to-Logic parsing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17284">Learning to Abstract with Nonparametric Variational Information Bottleneck. (arXiv:2310.17284v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Behjati_M/0/1/0/all/0/1">Melika Behjati</a>, <a href="http://arxiv.org/find/cs/1/au:+Fehr_F/0/1/0/all/0/1">Fabio Fehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1">James Henderson</a></p>
<p>Learned representations at the level of characters, sub-words, words and
sentences, have each contributed to advances in understanding different NLP
tasks and linguistic phenomena. However, learning textual embeddings is costly
as they are tokenization specific and require different models to be trained
for each level of abstraction. We introduce a novel language representation
model which can learn to compress to different levels of abstraction at
different layers of the same model. We apply Nonparametric Variational
Information Bottleneck (NVIB) to stacked Transformer self-attention layers in
the encoder, which encourages an information-theoretic compression of the
representations through the model. We find that the layers within the model
correspond to increasing levels of abstraction and that their representations
are more linguistically informed. Finally, we show that NVIB compression
results in a model which is more robust to adversarial perturbations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17300">Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience. (arXiv:2310.17300v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Korre_D/0/1/0/all/0/1">Danai Korre</a></p>
<p>Embodied conversational agents (ECAs) are paradigms of conversational user
interfaces in the form of embodied characters. While ECAs offer various
manipulable features, this paper focuses on a study conducted to explore two
distinct levels of presentation realism. The two agent versions are
photorealistic and animated. The study aims to provide insights and design
suggestions for speech-enabled ECAs within serious game environments. A
within-subjects, two-by-two factorial design was employed for this research
with a cohort of 36 participants balanced for gender. The results showed that
both the photorealistic and the animated versions were perceived as highly
usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4
per cent of the participants stated they preferred the photorealistic version,
25 per cent stated they preferred the animated version and 5.6 per cent had no
stated preference. The photorealistic agents were perceived as more realistic
and human-like, while the animated characters made the task feel more like a
game. Even though the agents' realism had no significant effect on usability,
it positively influenced participants' perceptions of the agent. This research
aims to lay the groundwork for future studies on ECA realism's impact in
serious games across diverse contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17306">FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mukul Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1">Jos&#xe9; Cambronero</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1">Sumit Gulwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1">Vu Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Negreanu_C/0/1/0/all/0/1">Carina Negreanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nouri_E/0/1/0/all/0/1">Elnaz Nouri</a>, <a href="http://arxiv.org/find/cs/1/au:+Raza_M/0/1/0/all/0/1">Mohammad Raza</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1">Gust Verbruggen</a></p>
<p>Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17312">An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text. (arXiv:2310.17312v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liyanage_V/0/1/0/all/0/1">Vijini Liyanage</a>, <a href="http://arxiv.org/find/cs/1/au:+Buscaldi_D/0/1/0/all/0/1">Davide Buscaldi</a></p>
<p>Thanks to the state-of-the-art Large Language Models (LLMs), language
generation has reached outstanding levels. These models are capable of
generating high quality content, thus making it a challenging task to detect
generated text from human-written content. Despite the advantages provided by
Natural Language Generation, the inability to distinguish automatically
generated text can raise ethical concerns in terms of authenticity.
Consequently, it is important to design and develop methodologies to detect
artificial content. In our work, we present some classification models
constructed by ensembling transformer models such as Sci-BERT, DeBERTa and
XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate
that the considered ensemble architectures surpass the performance of the
individual transformer models for classification. Furthermore, the proposed
SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared
task 2023 data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17315">Nabra: Syrian Arabic Dialects with Morphological Annotations. (arXiv:2310.17315v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nayouf_A/0/1/0/all/0/1">Amal Nayouf</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammouda_T/0/1/0/all/0/1">Tymaa Hammouda</a>, <a href="http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1">Mustafa Jarrar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaraket_F/0/1/0/all/0/1">Fadi Zaraket</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurdy_M/0/1/0/all/0/1">Mohamad-Bassam Kurdy</a></p>
<p>This paper presents Nabra, a corpora of Syrian Arabic dialects with
morphological annotations. A team of Syrian natives collected more than 6K
sentences containing about 60K words from several sources including social
media posts, scripts of movies and series, lyrics of songs and local proverbs
to build Nabra. Nabra covers several local Syrian dialects including those of
Aleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and
Suwayda. A team of nine annotators annotated the 60K tokens with full
morphological annotations across sentence contexts. We trained the annotators
to follow methodological annotation guidelines to ensure unique morpheme
annotations, and normalized the annotations. F1 and kappa agreement scores
ranged between 74% and 98% across features, showing the excellent quality of
Nabra annotations. Our corpora are open-source and publicly available as part
of the Currasat portal https://sina.birzeit.edu/currasat.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17333">Arabic Fine-Grained Entity Recognition. (arXiv:2310.17333v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liqreina_H/0/1/0/all/0/1">Haneen Liqreina</a>, <a href="http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1">Mustafa Jarrar</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalilia_M/0/1/0/all/0/1">Mohammed Khalilia</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Shangiti_A/0/1/0/all/0/1">Ahmed Oumar El-Shangiti</a>, <a href="http://arxiv.org/find/cs/1/au:+AbdulMageed_M/0/1/0/all/0/1">Muhammad AbdulMageed</a></p>
<p>Traditional NER systems are typically trained to recognize coarse-grained
entities, and less attention is given to classifying entities into a hierarchy
of fine-grained lower-level subtypes. This article aims to advance Arabic NER
with fine-grained entities. We chose to extend Wojood (an open-source Nested
Arabic Named Entity Corpus) with subtypes. In particular, four main entity
types in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG),
and facility (FAC), are extended with 31 subtypes. To do this, we first revised
Wojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's
ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC,
ORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE
sub-types. We refer to this extended version of Wojood as WojoodF ine. To
evaluate our annotations, we measured the inter-annotator agreement (IAA) using
both Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively.
To compute the baselines of WojoodF ine, we fine-tune three pre-trained Arabic
BERT encoders in three settings: flat NER, nested NER and nested NER with
subtypes and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our
corpus and models are open-source and available at
https://sina.birzeit.edu/wojood/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17342">ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought. (arXiv:2310.17342v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanchong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1">Ruisheng Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongshen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a></p>
<p>Recently Large Language Models (LLMs) have been proven to have strong
abilities in various domains and tasks. We study the problem of prompt
designing in the text-to-SQL task and attempt to improve the LLMs' reasoning
ability when generating SQL queries. Besides the trivial few-shot in-context
learning setting, we design our chain-of-thought (CoT) prompt with a similar
method to schema linking. We provide a method named ACT-SQL to automatically
generate auto-CoT exemplars and thus the whole process doesn't need manual
labeling. Our approach is cost-saving since we only use the LLMs' API call once
when generating one SQL query. Furthermore, we extend our in-context learning
method to the multi-turn text-to-SQL task. The experiment results show that the
LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves
SOTA performance on the Spider dev set among existing in-context learning
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17353">Cultural Adaptation of Recipes. (arXiv:2310.17353v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kementchedjhieva_Y/0/1/0/all/0/1">Yova Kementchedjhieva</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1">Ruixiang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Karamolegkou_A/0/1/0/all/0/1">Antonia Karamolegkou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Li Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Dare_M/0/1/0/all/0/1">Megan Dare</a>, <a href="http://arxiv.org/find/cs/1/au:+Donatelli_L/0/1/0/all/0/1">Lucia Donatelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1">Daniel Hershcovich</a></p>
<p>Building upon the considerable advances in Large Language Models (LLMs), we
are now equipped to address more sophisticated tasks demanding a nuanced
understanding of cross-cultural contexts. A key example is recipe adaptation,
which goes beyond simple translation to include a grasp of ingredients,
culinary techniques, and dietary preferences specific to a given culture. We
introduce a new task involving the translation and cultural adaptation of
recipes between Chinese and English-speaking cuisines. To support this
investigation, we present CulturalRecipes, a unique dataset comprised of
automatically paired recipes written in Mandarin Chinese and English. This
dataset is further enriched with a human-written and curated test set. In this
intricate task of cross-cultural recipe adaptation, we evaluate the performance
of various methods, including GPT-4 and other LLMs, traditional machine
translation, and information retrieval techniques. Our comprehensive analysis
includes both automatic and human evaluation metrics. While GPT-4 exhibits
impressive abilities in adapting Chinese recipes into English, it still lags
behind human expertise when translating English recipes into Chinese. This
underscores the multifaceted nature of cultural adaptations. We anticipate that
these insights will significantly contribute to future research on
culturally-aware language models and their practical application in culturally
diverse contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17369">Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers. (arXiv:2310.17369v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teodorescu_D/0/1/0/all/0/1">Daniela Teodorescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1">Tiffany Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1">Alona Fyshe</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1">Saif M. Mohammad</a></p>
<p>Research in psychopathology has shown that, at an aggregate level, the
patterns of emotional change over time -- emotion dynamics -- are indicators of
one's mental health. One's patterns of emotion change have traditionally been
determined through self-reports of emotions; however, there are known issues
with accuracy, bias, and convenience. Recent approaches to determining emotion
dynamics from one's everyday utterances, addresses many of these concerns, but
it is not yet known whether these measures of utterance emotion dynamics (UED)
correlate with mental health diagnoses. Here, for the first time, we study the
relationship between tweet emotion dynamics and mental health disorders. We
find that each of the UED metrics studied varied by the user's self-disclosed
diagnosis. For example: average valence was significantly higher (i.e., more
positive text) in the control group compared to users with ADHD, MDD, and PTSD.
Valence variability was significantly lower in the control group compared to
ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but not PPD. Rise and
recovery rates of valence also exhibited significant differences from the
control. This work provides important early evidence for how linguistic cues
pertaining to emotion dynamics can play a crucial role as biosocial markers for
mental illnesses and aid in the understanding, diagnosis, and management of
mental health disorders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17372">Dialogue-based generation of self-driving simulation scenarios using Large Language Models. (arXiv:2310.17372v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miceli_Barone_A/0/1/0/all/0/1">Antonio Valerio Miceli-Barone</a>, <a href="http://arxiv.org/find/cs/1/au:+Lascarides_A/0/1/0/all/0/1">Alex Lascarides</a>, <a href="http://arxiv.org/find/cs/1/au:+Innes_C/0/1/0/all/0/1">Craig Innes</a></p>
<p>Simulation is an invaluable tool for developing and evaluating controllers
for self-driving cars. Current simulation frameworks are driven by
highly-specialist domain specific languages, and so a natural language
interface would greatly enhance usability. But there is often a gap, consisting
of tacit assumptions the user is making, between a concise English utterance
and the executable code that captures the user's intent. In this paper we
describe a system that addresses this issue by supporting an extended
multimodal interaction: the user can follow up prior instructions with
refinements or revisions, in reaction to the simulations that have been
generated from their utterances so far. We use Large Language Models (LLMs) to
map the user's English utterances in this interaction into domain-specific
code, and so we explore the extent to which LLMs capture the context
sensitivity that's necessary for computing the speaker's intended message in
discourse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17389">ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. (arXiv:2310.17389v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yongqi Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yangkun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuxin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yujia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a></p>
<p>Despite remarkable advances that large language models have achieved in
chatbots, maintaining a non-toxic user-AI interactive environment has become
increasingly critical nowadays. However, previous efforts in toxicity detection
have been mostly based on benchmarks derived from social media content, leaving
the unique challenges inherent to real-world user-AI interactions
insufficiently explored. In this work, we introduce ToxicChat, a novel
benchmark based on real user queries from an open-source chatbot. This
benchmark contains the rich, nuanced phenomena that can be tricky for current
toxicity detection models to identify, revealing a significant domain
difference compared to social media content. Our systematic evaluation of
models trained on existing toxicity datasets has shown their shortcomings when
applied to this unique domain of ToxicChat. Our work illuminates the
potentially overlooked challenges of toxicity detection in real-world user-AI
conversations. In the future, ToxicChat can be a valuable resource to drive
further advancements toward building a safe and healthy environment for user-AI
interactions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17407">Meaning and understanding in large language models. (arXiv:2310.17407v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Havlik_V/0/1/0/all/0/1">Vladim&#xed;r Havl&#xed;k</a></p>
<p>Can a machine understand the meanings of natural language? Recent
developments in the generative large language models (LLMs) of artificial
intelligence have led to the belief that traditional philosophical assumptions
about machine understanding of language need to be revised. This article
critically evaluates the prevailing tendency to regard machine language
performance as mere syntactic manipulation and the simulation of understanding,
which is only partial and very shallow, without sufficient referential
grounding in the world. The aim is to highlight the conditions crucial to
attributing natural language understanding to state-of-the-art LLMs, where it
can be legitimately argued that LLMs not only use syntax but also semantics,
their understanding not being simulated but duplicated; and determine how they
ground the meanings of linguistic expressions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17408">Tackling the Matrix Multiplication Micro-kernel Generation with Exo. (arXiv:2310.17408v1 [cs.MS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castello_A/0/1/0/all/0/1">Adri&#xe1;n Castell&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Bellavita_J/0/1/0/all/0/1">Julian Bellavita</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinh_G/0/1/0/all/0/1">Grace Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikarashi_Y/0/1/0/all/0/1">Yuka Ikarashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_H/0/1/0/all/0/1">H&#xe9;ctor Mart&#xed;nez</a></p>
<p>The optimization of the matrix multiplication (or GEMM) has been a need
during the last decades. This operation is considered the flagship of current
linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its
widespread use in a large variety of scientific applications. The GEMM is
usually implemented following the GotoBLAS philosophy, which tiles the GEMM
operands and uses a series of nested loops for performance improvement. These
approaches extract the maximum computational power of the architectures through
small pieces of hardware-oriented, high-performance code called micro-kernel.
However, this approach forces developers to generate, with a non-negligible
effort, a dedicated micro-kernel for each new hardware.
</p>
<p>In this work, we present a step-by-step procedure for generating
micro-kernels with the Exo compiler that performs close to (or even better
than) manually developed microkernels written with intrinsic functions or
assembly language. Our solution also improves the portability of the generated
code, since a hardware target is fully specified by a concise library-based
description of its instructions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17413">Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases. (arXiv:2310.17413v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Belfathi_A/0/1/0/all/0/1">Anas Belfathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_N/0/1/0/all/0/1">Nicolas Hernandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Monceaux_L/0/1/0/all/0/1">Laura Monceaux</a></p>
<p>We propose a comprehensive study of one-stage elicitation techniques for
querying a large pre-trained generative transformer (GPT-3.5-turbo) in the
rhetorical role prediction task of legal cases. This task is known as requiring
textual context to be addressed. Our study explores strategies such as zero-few
shots, task specification with definitions and clarification of annotation
ambiguities, textual context and reasoning with general prompts and specific
questions. We show that the number of examples, the definition of labels, the
presentation of the (labelled) textual context and specific questions about
this context have a positive influence on the performance of the model. Given
non-equivalent test set configurations, we observed that prompting with a few
labelled examples from direct context can lead the model to a better
performance than a supervised fined-tuned multi-class classifier based on the
BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach
the performance of the best systems = 86%) in the LegalEval 2023 task which, on
the other hand, require dedicated resources, architectures and training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17415">PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingchen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1">Pan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Ziyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Huiqun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1">Guisheng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Liang Hong</a></p>
<p>Large protein language models are adept at capturing the underlying
evolutionary information in primary structures, offering significant practical
value for protein engineering. Compared to natural language models, protein
amino acid sequences have a smaller data volume and a limited combinatorial
space. Choosing an appropriate vocabulary size to optimize the pre-trained
model is a pivotal issue. Moreover, despite the wealth of benchmarks and
studies in the natural language community, there remains a lack of a
comprehensive benchmark for systematically evaluating protein language model
quality. Given these challenges, PETA trained language models with 14 different
vocabulary sizes under three tokenization methods. It conducted thousands of
tests on 33 diverse downstream datasets to assess the models' transfer learning
capabilities, incorporating two classification heads and three random seeds to
mitigate potential biases. Extensive experiments indicate that vocabulary sizes
between 50 and 200 optimize the model, whereas sizes exceeding 800
detrimentally affect the model's representational performance. Our code, model
weights and datasets are available at
https://github.com/ginnm/ProteinPretraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17428">&#x27;&#x27;Fifty Shades of Bias&#x27;&#x27;: Normative Ratings of Gender Bias in GPT Generated English Text. (arXiv:2310.17428v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hada_R/0/1/0/all/0/1">Rishav Hada</a>, <a href="http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1">Agrima Seth</a>, <a href="http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1">Harshita Diddee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1">Kalika Bali</a></p>
<p>Language serves as a powerful tool for the manifestation of societal belief
systems. In doing so, it also perpetuates the prevalent biases in our society.
Gender bias is one of the most pervasive biases in our society and is seen in
online and offline discourses. With LLMs increasingly gaining human-like
fluency in text generation, gaining a nuanced understanding of the biases these
systems can generate is imperative. Prior work often treats gender bias as a
binary classification task. However, acknowledging that bias must be perceived
at a relative scale; we investigate the generation and consequent receptivity
of manual annotators to bias of varying degrees. Specifically, we create the
first dataset of GPT-generated English text with normative ratings of gender
bias. Ratings were obtained using Best--Worst Scaling -- an efficient
comparative annotation framework. Next, we systematically analyze the variation
of themes of gender biases in the observed ranking and show that
identity-attack is most closely related to gender bias. Finally, we show the
performance of existing automated models trained on related concepts on our
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17448">Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech Systems for the MADASR 2023 Challenge. (arXiv:2310.17448v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alumae_T/0/1/0/all/0/1">Tanel Alum&#xe4;e</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1">Jiaming Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Robnikov_D/0/1/0/all/0/1">Daniil Robnikov</a></p>
<p>This paper describes Tallinn University of Technology (TalTech) systems
developed for the ASRU MADASR 2023 Challenge. The challenge focuses on
automatic speech recognition of dialect-rich Indian languages with limited
training audio and text data. TalTech participated in two tracks of the
challenge: Track 1 that allowed using only the provided training data and Track
3 which allowed using additional audio data. In both tracks, we relied on
wav2vec2.0 models. Our methodology diverges from the traditional procedure of
finetuning pretrained wav2vec2.0 models in two key points: firstly, through the
implementation of the aligned data augmentation technique to enhance the
linguistic diversity of the training data, and secondly, via the application of
deep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks,
our approach yielded significant improvements over the provided baselines,
achieving the lowest word error rates across all participating teams.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17488">LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation. (arXiv:2310.17488v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1">Kai Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>This paper presents LightLM, a lightweight Transformer-based language model
for generative recommendation. While Transformer-based generative modeling has
gained importance in various AI sub-fields such as NLP and vision, generative
recommendation is still in its infancy due to its unique demand on personalized
generative modeling. Existing works on generative recommendation often use
NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are
heavy-weight and are not specifically designed for recommendation tasks.
LightLM tackles the issue by introducing a light-weight deep and narrow
Transformer architecture, which is specifically tailored for direct generation
of recommendation items. This structure is especially apt for straightforward
generative recommendation and stems from the observation that language model
does not have to be too wide for this task, as the input predominantly consists
of short tokens that are well-suited for the model's capacity. We also show
that our devised user and item ID indexing methods, i.e., Spectral
Collaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables
the deep and narrow Transformer architecture to outperform large-scale language
models for recommendation. Besides, to address the hallucination problem of
generating items as output, we propose the constrained generation process for
generative recommenders. Experiments on real-world datasets show that LightLM
outperforms various competitive baselines in terms of both recommendation
accuracy and efficiency. The code can be found at
https://github.com/dongyuanjushi/LightLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17490">Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sukmin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Jeong yeon Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Soyeong Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jong C. Park</a></p>
<p>Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17499">The IMS Toucan System for the Blizzard Challenge 2023. (arXiv:2310.17499v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1">Florian Lux</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1">Julia Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_S/0/1/0/all/0/1">Sarina Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bott_T/0/1/0/all/0/1">Thomas Bott</a>, <a href="http://arxiv.org/find/cs/1/au:+Schauffler_N/0/1/0/all/0/1">Nadja Schauffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1">Pavel Denisov</a>, <a href="http://arxiv.org/find/cs/1/au:+Schweitzer_A/0/1/0/all/0/1">Antje Schweitzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1">Ngoc Thang Vu</a></p>
<p>For our contribution to the Blizzard Challenge 2023, we improved on the
system we submitted to the Blizzard Challenge 2021. Our approach entails a
rule-based text-to-phoneme processing system that includes rule-based
disambiguation of homographs in the French language. It then transforms the
phonemes to spectrograms as intermediate representations using a fast and
efficient non-autoregressive synthesis architecture based on Conformer and
Glow. A GAN based neural vocoder that combines recent state-of-the-art
approaches converts the spectrogram to the final wave. We carefully designed
the data processing, training, and inference procedures for the challenge data.
Our system identifier is G. Open source code and demo are available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17512">CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qinlin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yiqiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Large language models (LLMs) have been widely used as agents to complete
different tasks, such as personal assistance or event planning. While most work
has focused on cooperation and collaboration between agents, little work
explores competition, another important mechanism that fosters the development
of society and economy. In this paper, we seek to examine the competition
behaviors in LLM-based agents. We first propose a general framework to study
the competition between agents. Then, we implement a practical competitive
environment using GPT-4 to simulate a virtual town with two types of agents,
including restaurant agents and customer agents. Specifically, restaurant
agents compete with each other to attract more customers, where the competition
fosters them to transform, such as cultivating new operating strategies. The
results of our experiments reveal several interesting findings ranging from
social learning to Matthew Effect, which aligns well with existing sociological
and economic theories. We believe that competition between agents deserves
further investigation to help us understand society better. The code will be
released soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17513">The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yuchen Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kangwook Lee</a></p>
<p>Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17514">The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks. (arXiv:2310.17514v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Kaiser Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Adina Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1">Dieuwke Hupkes</a></p>
<p>NLP models have progressed drastically in recent years, according to numerous
datasets proposed to evaluate performance. Questions remain, however, about how
particular dataset design choices may impact the conclusions we draw about
model capabilities. In this work, we investigate this question in the domain of
compositional generalization. We examine the performance of six modeling
approaches across 4 datasets, split according to 8 compositional splitting
strategies, ranking models by 18 compositional generalization splits in total.
Our results show that: i) the datasets, although all designed to evaluate
compositional generalization, rank modeling approaches differently; ii)
datasets generated by humans align better with each other than they with
synthetic datasets, or than synthetic datasets among themselves; iii)
generally, whether datasets are sampled from the same source is more predictive
of the resulting model ranking than whether they maintain the same
interpretation of compositionality; and iv) which lexical items are used in the
data can strongly impact conclusions. Overall, our results demonstrate that
much work remains to be done when it comes to assessing whether popular
evaluation datasets measure what they intend to measure, and suggest that
elucidating more rigorous standards for establishing the validity of evaluation
sets could benefit the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17526">Can large language models replace humans in the systematic review process? Evaluating GPT-4&#x27;s efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khraisha_Q/0/1/0/all/0/1">Qusai Khraisha</a>, <a href="http://arxiv.org/find/cs/1/au:+Put_S/0/1/0/all/0/1">Sophie Put</a>, <a href="http://arxiv.org/find/cs/1/au:+Kappenberg_J/0/1/0/all/0/1">Johanna Kappenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Warraitch_A/0/1/0/all/0/1">Azza Warraitch</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_K/0/1/0/all/0/1">Kristin Hadfield</a></p>
<p>Systematic reviews are vital for guiding practice, research, and policy, yet
they are often slow and labour-intensive. Large language models (LLMs) could
offer a way to speed up and automate systematic reviews, but their performance
in such tasks has not been comprehensively evaluated against humans, and no
study has tested GPT-4, the biggest LLM so far. This pre-registered study
evaluates GPT-4's capability in title/abstract screening, full-text review, and
data extraction across various literature types and languages using a
'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human
performance in most tasks, results were skewed by chance agreement and dataset
imbalance. After adjusting for these, there was a moderate level of performance
for data extraction, and - barring studies that used highly reliable prompts -
screening performance levelled at none to moderate for different stages and
languages. When screening full-text literature using highly reliable prompts,
GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key
studies using highly reliable prompts improved its performance even more. Our
findings indicate that, currently, substantial caution should be used if LLMs
are being used to conduct systematic reviews, but suggest that, for certain
systematic review tasks delivered under reliable prompts, LLMs can rival human
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17530">Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1">Laura Cabello</a>, <a href="http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1">Emanuele Bugliarello</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1">Stephanie Brandl</a>, <a href="http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1">Desmond Elliott</a></p>
<p>Pretrained machine learning models are known to perpetuate and even amplify
existing biases in data, which can result in unfair outcomes that ultimately
impact user experience. Therefore, it is crucial to understand the mechanisms
behind those prejudicial biases to ensure that model performance does not
result in discriminatory behaviour toward certain groups or populations. In
this work, we define gender bias as our case study. We quantify bias
amplification in pretraining and after fine-tuning on three families of
vision-and-language models. We investigate the connection, if any, between the
two learning stages, and evaluate how bias amplification reflects on model
performance. Overall, we find that bias amplification in pretraining and after
fine-tuning are independent. We then examine the effect of continued
pretraining on gender-neutral data, finding that this reduces group
disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without
significantly compromising task performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17551">Unpacking the Ethical Value Alignment in Big Models. (arXiv:2310.17551v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1">Xiaoyuan Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jing Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiting Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Big models have greatly advanced AI's ability to understand, generate, and
manipulate information and content, enabling numerous applications. However, as
these models become increasingly integrated into everyday life, their inherent
ethical values and potential biases pose unforeseen risks to society. This
paper provides an overview of the risks and challenges associated with big
models, surveys existing AI ethics guidelines, and examines the ethical
implications arising from the limitations of these models. Taking a normative
ethics perspective, we propose a reassessment of recent normative guidelines,
highlighting the importance of collaborative efforts in academia to establish a
unified and universal AI ethics framework. Furthermore, we investigate the
moral inclinations of current mainstream LLMs using the Moral Foundation
theory, analyze existing alignment algorithms, and outline the unique
challenges encountered in aligning ethical values within them. To address these
challenges, we introduce a novel conceptual paradigm for aligning the ethical
values of big models and discuss promising research directions for alignment
criteria, evaluation, and method, representing an initial step towards the
interdisciplinary construction of the ethically aligned AI
</p>
<p>This paper is a modified English version of our Chinese paper
https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended
to help non-Chinese native speakers better understand our work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17558">Towards Matching Phones and Speech Representations. (arXiv:2310.17558v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Gene-Ping Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hao Tang</a></p>
<p>Learning phone types from phone instances has been a long-standing problem,
while still being open. In this work, we revisit this problem in the context of
self-supervised learning, and pose it as the problem of matching cluster
centroids to phone embeddings. We study two key properties that enable
matching, namely, whether cluster centroids of self-supervised representations
reduce the variability of phone instances and respect the relationship among
phones. We then use the matching result to produce pseudo-labels and introduce
a new loss function for improving self-supervised representations. Our
experiments show that the matching result captures the relationship among
phones. Training the new loss function jointly with the regular self-supervised
losses, such as APC and CPC, significantly improves the downstream phone
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1901.05066">Investigating Antigram Behaviour using Distributional Semantics. (arXiv:1901.05066v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1">Saptarshi Sengupta</a></p>
<p>The field of computational linguistics constantly presents new challenges and
topics for research. Whether it be analyzing word usage changes over time or
identifying relationships between pairs of seemingly unrelated words. To this
point, we identify Anagrams and Antigrams as words possessing such unique
properties. The presented work is an exploration into generating anagrams from
a given word and determining whether there exists antigram (semantically
opposite anagrams) relationships between the pairs of generated anagrams using
GloVe embeddings. We propose a rudimentary, yet interpretable, rule-based
algorithm for detecting antigrams. On a small dataset of just 12 antigrams, our
approach yielded an accuracy of 39\% which shows that there is much work left
to be done in this space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.02397">On Classifying Continuous Constraint Satisfaction Problems. (arXiv:2106.02397v5 [cs.CC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1">Tillmann Miltzow</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1">Reinier F. Schmiermann</a></p>
<p>A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with an interval domain $U \subset \mathbb{R}$. We
engage in a systematic study to classify CCSPs that are complete of the
Existential Theory of the Reals, i.e., ER-complete. To define this class, we
first consider the problem ETR, which also stands for Existential Theory of the
Reals. In an instance of this problem we are given some sentence of the form
$\exists x_1, \ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where
$\Phi$ is a well-formed quantifier-free formula consisting of the symbols $\{0,
1, +, \cdot, \geq, &gt;, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time many-one reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.
</p>
<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.09462">pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks. (arXiv:2106.09462v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1">Juan Manuel P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajngewerc_M/0/1/0/all/0/1">Mariela Rajngewerc</a>, <a href="http://arxiv.org/find/cs/1/au:+Giudici_J/0/1/0/all/0/1">Juan Carlos Giudici</a>, <a href="http://arxiv.org/find/cs/1/au:+Furman_D/0/1/0/all/0/1">Dami&#xe1;n A. Furman</a>, <a href="http://arxiv.org/find/cs/1/au:+Luque_F/0/1/0/all/0/1">Franco Luque</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemany_L/0/1/0/all/0/1">Laura Alonso Alemany</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1">Mar&#xed;a Vanina Mart&#xed;nez</a></p>
<p>In recent years, the extraction of opinions and information from
user-generated text has attracted a lot of interest, largely due to the
unprecedented volume of content in Social Media. However, social researchers
face some issues in adopting cutting-edge tools for these tasks, as they are
usually behind commercial APIs, unavailable for other languages than English,
or very complex to use for non-experts. To address these issues, we present
pysentimiento, a comprehensive multilingual Python toolkit designed for opinion
mining and other Social NLP tasks. This open-source library brings
state-of-the-art models for Spanish, English, Italian, and Portuguese in an
easy-to-use Python library, allowing researchers to leverage these techniques.
We present a comprehensive assessment of performance for several pre-trained
language models across a variety of tasks, languages, and datasets, including
an evaluation of fairness in the results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.03018">Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users. (arXiv:2205.03018v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madhani_Y/0/1/0/all/0/1">Yash Madhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Parthan_S/0/1/0/all/0/1">Sushane Parthan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bedekar_P/0/1/0/all/0/1">Priyanka Bedekar</a>, <a href="http://arxiv.org/find/cs/1/au:+NC_G/0/1/0/all/0/1">Gokul NC</a>, <a href="http://arxiv.org/find/cs/1/au:+Khapra_R/0/1/0/all/0/1">Ruchi Khapra</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1">Anoop Kunchukuttan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1">Pratyush Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1">Mitesh M. Khapra</a></p>
<p>Transliteration is very important in the Indian language context due to the
usage of multiple scripts and the widespread use of romanized inputs. However,
few training and evaluation sets are publicly available. We introduce
Aksharantar, the largest publicly available transliteration dataset for Indian
languages created by mining from monolingual and parallel corpora, as well as
collecting data from human annotators. The dataset contains 26 million
transliteration pairs for 21 Indic languages from 3 language families using 12
scripts. Aksharantar is 21 times larger than existing datasets and is the first
publicly available dataset for 7 languages and 1 language family. We also
introduce the Aksharantar testset comprising 103k word pairs spanning 19
languages that enables a fine-grained analysis of transliteration models on
native origin words, foreign words, frequent words, and rare words. Using the
training set, we trained IndicXlit, a multilingual transliteration model that
improves accuracy by 15% on the Dakshina test set, and establishes strong
baselines on the Aksharantar testset introduced in this work. The models,
mining scripts, transliteration guidelines, and datasets are available at
https://github.com/AI4Bharat/IndicXlit under open-source licenses. We hope the
availability of these large-scale, open resources will spur innovation for
Indic language transliteration and downstream applications. We hope the
availability of these large-scale, open resources will spur innovation for
Indic language transliteration and downstream applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01488">Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kauf_C/0/1/0/all/0/1">Carina Kauf</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivanova_A/0/1/0/all/0/1">Anna A. Ivanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Rambelli_G/0/1/0/all/0/1">Giulia Rambelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1">Emmanuele Chersoni</a>, <a href="http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1">Jingyuan Selena She</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_Z/0/1/0/all/0/1">Zawad Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1">Evelina Fedorenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1">Alessandro Lenci</a></p>
<p>Word co-occurrence patterns in language corpora contain a surprising amount
of conceptual knowledge. Large language models (LLMs), trained to predict words
in context, leverage these patterns to achieve impressive performance on
diverse semantic tasks requiring world knowledge. An important but understudied
question about LLMs' semantic abilities is whether they acquire generalized
knowledge of common events. Here, we test whether five pre-trained LLMs (from
2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions
of agent-patient interactions than to minimally different implausible versions
of the same event. Using three curated sets of minimal sentence pairs (total
n=1,215), we found that pre-trained LLMs possess substantial event knowledge,
outperforming other distributional language models. In particular, they almost
always assign higher likelihood to possible vs. impossible events (The teacher
bought the laptop vs. The laptop bought the teacher). However, LLMs show less
consistent preferences for likely vs. unlikely events (The nanny tutored the
boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM
scores are driven by both plausibility and surface-level sentence features,
(ii) LLM scores generalize well across syntactic variants (active vs. passive
constructions) but less well across semantic variants (synonymous sentences),
(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence
plausibility serves as an organizing dimension in internal LLM representations.
Overall, our results show that important aspects of event knowledge naturally
emerge from distributional linguistic patterns, but also highlight a gap
between representations of possible/impossible and likely/unlikely events.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04089">ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1">Eldar Kurtic</a>, <a href="http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1">Elias Frantar</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>The breakthrough performance of large language models (LLMs) comes with major
computational footprints and high deployment costs. In this paper, we progress
towards resolving this problem by proposing a novel structured compression
approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art
accuracy-vs-speedup, while matching a set of desired target runtime speedups in
any given inference environment. Specifically, given a model, a dataset, an
inference environment, as well as a set of speedup targets, ZipLM iteratively
identifies and removes components with the worst loss-runtime trade-off. Unlike
prior methods that specialize in either the post-training/one-shot or the
gradual compression setting, and only for specific families of models such as
BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed
models across all these settings. Furthermore, ZipLM achieves superior results
for a fraction of the computational cost relative to prior distillation and
pruning techniques, making it a cost-effective approach for generating an
entire family of smaller, faster, and highly accurate models, guaranteed to
meet the desired inference specifications. In particular, ZipLM outperforms all
prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and
TinyBERT. Moreover, it matches the performance of the heavily optimized
MobileBERT model, obtained via extensive architecture search, by simply pruning
the baseline BERT-large model. When compressing GPT2, ZipLM outperforms
DistilGPT2 while being 60% smaller and 30% faster. Our code is available at:
https://github.com/IST-DASLab/ZipLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04449">Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yewen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1">Amos Azaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1">Tom M. Mitchell</a></p>
<p>High sample complexity has long been a challenge for RL. On the other hand,
humans learn to perform tasks not only from interaction or demonstrations, but
also by reading unstructured text documents, e.g., instruction manuals.
Instruction manuals and wiki pages are among the most abundant data that could
inform agents of valuable features and policies or task-specific environmental
dynamics and reward structures. Therefore, we hypothesize that the ability to
utilize human-written instruction manuals to assist learning policies for
specific tasks should lead to a more efficient and better-performing agent. We
propose the Read and Reward framework. Read and Reward speeds up RL algorithms
on Atari games by reading manuals released by the Atari game developers. Our
framework consists of a QA Extraction module that extracts and summarizes
relevant information from the manual and a Reasoning module that evaluates
object-agent interactions based on information from the manual. An auxiliary
reward is then provided to a standard A2C RL agent, when interaction is
detected. Experimentally, various RL algorithms obtain significant improvement
in performance and training speed when assisted by our design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04053">Describe me an Aucklet: Generating Grounded Perceptual Category Descriptions. (arXiv:2303.04053v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Noble_B/0/1/0/all/0/1">Bill Noble</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilinykh_N/0/1/0/all/0/1">Nikolai Ilinykh</a></p>
<p>Human speakers can generate descriptions of perceptual concepts, abstracted
from the instance-level. Moreover, such descriptions can be used by other
speakers to learn provisional representations of those concepts. Learning and
using abstract perceptual concepts is under-investigated in the
language-and-vision field. The problem is also highly relevant to the field of
representation learning in multi-modal NLP. In this paper, we introduce a
framework for testing category-level perceptual grounding in multi-modal
language models. In particular, we train separate neural networks to generate
and interpret descriptions of visual categories. We measure the communicative
success of the two models with the zero-shot classification performance of the
interpretation model, which we argue is an indicator of perceptual grounding.
Using this framework, we compare the performance of prototype- and
exemplar-based representations. Finally, we show that communicative success
exposes performance issues in the generation model, not captured by traditional
intrinsic NLG evaluation metrics, and argue that these issues stem from a
failure to properly ground language in vision at the category level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07914">Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference. (arXiv:2303.07914v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1">Biao Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1">Minpeng Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1">Kai Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhongqiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Boxing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yidong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaodong Shi</a></p>
<p>A popular approach to streaming speech translation is to employ a single
offline model with a wait-k policy to support different latency requirements,
which is simpler than training multiple online models with different latency
constraints. However, there is a mismatch problem in using a model trained with
complete utterances for streaming inference with partial input. We demonstrate
that speech representations extracted at the end of a streaming input are
significantly different from those extracted from a complete utterance. To
address this issue, we propose a new approach called Future-Aware Streaming
Translation (FAST) that adapts an offline ST model for streaming input. FAST
includes a Future-Aware Inference (FAI) strategy that incorporates future
context through a trainable masked embedding, and a Future-Aware Distillation
(FAD) framework that transfers future context from an approximation of full
speech to streaming input. Our experiments on the MuST-C EnDe, EnEs, and EnFr
benchmarks show that FAST achieves better trade-offs between translation
quality and latency than strong baselines. Extensive analyses suggest that our
methods effectively alleviate the aforementioned mismatch problem between
offline training and online inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13001">Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Mingyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haiyun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Songfang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shilong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huafeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1">Liping Jing</a></p>
<p>The emergence of ChatGPT has recently garnered significant attention from the
computational linguistics community. To demonstrate its capabilities as a
keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the
keyphrase generation task. We evaluate its performance in various aspects,
including keyphrase generation prompts, keyphrase generation diversity, and
long document understanding. Our evaluation is based on six benchmark datasets,
and we adopt the prompt suggested by OpenAI while extending it to six candidate
prompts. We find that ChatGPT performs exceptionally well on all six candidate
prompts, with minor performance differences observed across the datasets. Based
on our findings, we conclude that ChatGPT has great potential for keyphrase
generation. Moreover, we discover that ChatGPT still faces challenges when it
comes to generating absent keyphrases. Meanwhile, in the final section, we also
present some limitations and future expansions of this report.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00633">Self-Evaluation Guided Beam Search for Reasoning. (arXiv:2305.00633v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiran Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1">Min-Yen Kan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qizhe Xie</a></p>
<p>Breaking down a problem into intermediate steps has demonstrated impressive
performance in Large Language Model (LLM) reasoning. However, the growth of the
reasoning chain introduces uncertainty and error accumulation, making it
challenging to elicit accurate final results. To tackle this challenge of
uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation
mechanism to guide and calibrate the reasoning process of LLMs. We propose a
decoding algorithm integrating the self-evaluation guidance via stochastic beam
search. The self-evaluation guidance serves as a better-calibrated automatic
criterion, facilitating an efficient search in the reasoning space and
resulting in superior prediction quality. Stochastic beam search balances
exploitation and exploration of the search space with temperature-controlled
randomness. Our approach surpasses the corresponding Codex-backboned baselines
in few-shot accuracy by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQuA,
and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on
arithmetic reasoning demonstrate the efficiency of our method in outperforming
the baseline methods with comparable computational budgets. Further analysis in
multi-step reasoning finds our self-evaluation guidance pinpoints logic
failures and leads to higher consistency and robustness. Our code is publicly
available at https://guideddecoding.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02531">Can LLMs Capture Intertemporal Preferences?. (arXiv:2305.02531v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goli_A/0/1/0/all/0/1">Ali Goli</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Amandeep Singh</a></p>
<p>We explore the viability of Large Language Models (LLMs), specifically
OpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting
preferences, with a focus on intertemporal choices. Leveraging the extensive
literature on intertemporal discounting for benchmarking, we examine responses
from LLMs across various languages and compare them to human responses,
exploring preferences between smaller, sooner, and larger, later rewards. Our
findings reveal that both GPT models demonstrate less patience than humans,
with GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike
human decision-makers. Though GPT-4 does not display lexicographic preferences,
its measured discount rates are still considerably larger than those found in
humans. Interestingly, GPT models show greater patience in languages with weak
future tense references, such as German and Mandarin, aligning with existing
literature that suggests a correlation between language structure and
intertemporal preferences. We demonstrate how prompting GPT to explain its
decisions, a procedure we term ``chain-of-thought conjoint," can mitigate, but
does not eliminate, discrepancies between LLM and human responses. While
directly eliciting preferences using LLMs may yield misleading results,
combining chain-of-thought conjoint with topic modeling aids in hypothesis
generation, enabling researchers to explore the underpinnings of preferences.
Chain-of-thought conjoint provides a structured framework for marketers to use
LLMs to identify potential attributes or factors that can explain preference
heterogeneity across different customers and contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03598">NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jullien_M/0/1/0/all/0/1">Ma&#xeb;l Jullien</a>, <a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1">Marco Valentino</a>, <a href="http://arxiv.org/find/cs/1/au:+Frost_H/0/1/0/all/0/1">Hannah Frost</a>, <a href="http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1">Paul O&#x27;Regan</a>, <a href="http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1">Donal Landers</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1">Andr&#xe9; Freitas</a></p>
<p>How can we interpret and retrieve medical evidence to support clinical
decisions? Clinical trial reports (CTR) amassed over the years contain
indispensable information for the development of personalized medicine.
However, it is practically infeasible to manually inspect over 400,000+
clinical trial reports in order to find the best evidence for experimental
treatments. Natural Language Inference (NLI) offers a potential solution to
this problem, by allowing the scalable computation of textual entailment.
However, existing NLI models perform poorly on biomedical corpora, and
previously published datasets fail to capture the full complexity of inference
over CTRs. In this work, we present a novel resource to advance research on NLI
for reasoning on CTRs. The resource includes two main tasks. Firstly, to
determine the inference relation between a natural language statement, and a
CTR. Secondly, to retrieve supporting facts to justify the predicted relation.
We provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these
tasks. Baselines on this corpus expose the limitations of existing NLI models,
with 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To
the best of our knowledge, we are the first to design a task that covers the
interpretation of full CTRs. To encourage further work on this challenging
dataset, we make the corpus, competition leaderboard, website and code to
replicate the baseline experiments available at:
https://github.com/ai-systems/nli4ct
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04798">Multi-grained Hypergraph Interest Modeling for Conversational Recommendation. (arXiv:2305.04798v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1">Chenzhan Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Yupeng Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a></p>
<p>Conversational recommender system (CRS) interacts with users through
multi-turn dialogues in natural language, which aims to provide high-quality
recommendations for user's instant information need. Although great efforts
have been made to develop effective CRS, most of them still focus on the
contextual information from the current dialogue, usually suffering from the
data scarcity issue. Therefore, we consider leveraging historical dialogue data
to enrich the limited contexts of the current dialogue session.
</p>
<p>In this paper, we propose a novel multi-grained hypergraph interest modeling
approach to capture user interest beneath intricate historical data from
different perspectives. As the core idea, we employ hypergraph to represent
complicated semantic relations underlying historical dialogues. In our
approach, we first employ the hypergraph structure to model users' historical
dialogue sessions and form a session-based hypergraph, which captures
coarse-grained, session-level relations. Second, to alleviate the issue of data
scarcity, we use an external knowledge graph and construct a knowledge-based
hypergraph considering fine-grained, entity-level semantics. We further conduct
multi-grained hypergraph convolution on the two kinds of hypergraphs, and
utilize the enhanced representations to develop interest-aware CRS. Extensive
experiments on two benchmarks ReDial and TG-ReDial validate the effectiveness
of our approach on both recommendation and conversation tasks. Code is
available at: https://github.com/RUCAIBox/MHIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07224">Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xiaolei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianghong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haode Zhang</a></p>
<p>In natural language processing (NLP), deep neural networks (DNNs) could model
complex interactions between context and have achieved impressive results on a
range of NLP tasks. Prior works on feature interaction attribution mainly focus
on studying symmetric interaction that only explains the additional influence
of a set of words in combination, which fails to capture asymmetric influence
that contributes to model prediction. In this work, we propose an asymmetric
feature interaction attribution explanation model that aims to explore
asymmetric higher-order feature interactions in the inference of deep neural
NLP models. By representing our explanation with an directed interaction graph,
we experimentally demonstrate interpretability of the graph to discover
asymmetric feature interactions. Experimental results on two sentiment
classification datasets show the superiority of our model against the
state-of-the-art feature interaction attribution methods in identifying
influential features for model predictions. Our code is available at
https://github.com/StillLu/ASIV.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09758">A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot. (arXiv:2305.09758v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1">Aanisha Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1">Yaman K Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1">Balaji Krishnamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rajiv Ratn Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changyou Chen</a></p>
<p>Multimedia content, such as advertisements and story videos, exhibit a rich
blend of creativity and multiple modalities. They incorporate elements like
text, visuals, audio, and storytelling techniques, employing devices like
emotions, symbolism, and slogans to convey meaning. There is a dearth of large
annotated training datasets in the multimedia domain hindering the development
of supervised learning models with satisfactory performance for real-world
applications. On the other hand, the rise of large language models (LLMs) has
witnessed remarkable zero-shot performance in various natural language
processing (NLP) tasks, such as emotion classification, question-answering, and
topic classification. To leverage such advanced techniques to bridge this
performance gap in multimedia understanding, we propose verbalizing long videos
to generate their descriptions in natural language, followed by performing
video-understanding tasks on the generated story as opposed to the original
video. Through extensive experiments on fifteen video-understanding tasks, we
demonstrate that our method, despite being zero-shot, achieves significantly
better results than supervised baselines for video understanding. Furthermore,
to alleviate a lack of story understanding benchmarks, we publicly release the
first dataset on a crucial task in computational social science on persuasion
strategy identification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10355">Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yifan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a></p>
<p>Inspired by the superior language abilities of large language models (LLM),
large vision-language models (LVLM) have been recently explored by integrating
powerful LLMs for improving the performance on complex multimodal tasks.
Despite the promising progress on LVLMs, we find that LVLMs suffer from the
hallucination problem, i.e. they tend to generate objects that are inconsistent
with the target images in the descriptions. To investigate it, this work
presents the first systematic study on object hallucination of LVLMs. We
conduct the evaluation experiments on several representative LVLMs, and show
that they mostly suffer from severe object hallucination issue. We further
discuss that the visual instructions may influence the hallucination, and find
that: objects that frequently occur in the visual instructions or co-occur with
the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we
find that existing evaluation methods might be affected by the input
instructions and generation styles of LVLMs. Thus, we further design an
improved evaluation method for object hallucination by proposing a
polling-based query method called POPE. Experiment results demonstrate that our
POPE can evaluate the object hallucination in a more stable and flexible way.
Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11685">Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jang_K/0/1/0/all/0/1">Kangwook Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Sungnyun Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1">Hoirin Kim</a></p>
<p>Transformer-based speech self-supervised learning (SSL) models, such as
HuBERT, show surprising performance in various speech processing tasks.
However, huge number of parameters in speech SSL models necessitate the
compression to a more compact model for wider usage in academia or small
companies. In this study, we suggest to reuse attention maps across the
Transformer layers, so as to remove key and query parameters while retaining
the number of layers. Furthermore, we propose a novel masking distillation
strategy to improve the student model's speech representation quality. We
extend the distillation loss to utilize both masked and unmasked speech frames
to fully leverage the teacher model's high-quality representation. Our
universal compression strategy yields the student model that achieves phoneme
error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB
benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12945">ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination. (arXiv:2305.12945v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongfang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jindi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Baotian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a></p>
<p>As ChatGPT and GPT-4 spearhead the development of Large Language Models
(LLMs), more researchers are investigating their performance across various
tasks. But more research needs to be done on the interpretability capabilities
of LLMs, that is, the ability to generate reasons after an answer has been
given. Existing explanation datasets are mostly English-language general
knowledge questions, which leads to insufficient thematic and linguistic
diversity. To address the language bias and lack of medical resources in
generating rationales QA datasets, we present ExplainCPE (over 7k instances), a
challenging medical benchmark in Simplified Chinese. We analyzed the errors of
ChatGPT and GPT-4, pointing out the limitations of current LLMs in
understanding text and computational reasoning. During the experiment, we also
found that different LLMs have different preferences for in-context learning.
ExplainCPE presents a significant challenge, but its potential for further
investigation is promising, and it can be used to evaluate the ability of a
model to generate explanations. AI safety and trustworthiness need more
attention, and this work makes the first step to explore the medical
interpretability of LLMs.The dataset is available at
https://github.com/HITsz-TMG/ExplainCPE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13272">CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles. (arXiv:2305.13272v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sonkar_S/0/1/0/all/0/1">Shashank Sonkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Naiming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallick_D/0/1/0/all/0/1">Debshila Basu Mallick</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1">Richard G. Baraniuk</a></p>
<p>We present a design framework called Conversational Learning with Analytical
Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring
Systems (ITS) powered by high-performance Large Language Models (LLMs). The
CLASS framework empowers ITS with two key capabilities. First, through a
carefully curated scaffolding dataset, CLASS equips ITS with essential
problem-solving strategies, enabling it to provide tutor-like, step-by-step
guidance to students. Second, by using a dynamic conversational dataset, CLASS
assists ITS in facilitating natural language interactions, fostering engaging
student-tutor conversations. The CLASS framework also provides valuable
insights into ITS' internal decision-making process which allows seamless
integration of user feedback, thus enabling continuous refinement and
improvement. We also present a proof-of-concept ITS, referred to as SPOCK,
which is trained using the CLASS framework with a focus on introductory
college-level biology content. A carefully constructed protocol was developed
for SPOCK's preliminary evaluation, examining aspects such as the factual
accuracy and relevance of its responses. Experts in the field of biology
offered favorable remarks, particularly highlighting SPOCK's capability to
break down questions into manageable subproblems and provide encouraging
responses to students. Code and models are available at
https://github.com/luffycodes/Tutorbot-Spock.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13507">Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1">Mubashara Akhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1">Michael Schlichtkrull</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhijiang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1">Oana Cocarascu</a>, <a href="http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1">Elena Simperl</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1">Andreas Vlachos</a></p>
<p>Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned
image. Multimodal misinformation is perceived as more credible by humans, and
spreads faster than its text-only counterparts. While an increasing body of
research investigates automated fact-checking (AFC), previous surveys mostly
focus on text. In this survey, we conceptualise a framework for AFC including
subtasks unique to multimodal misinformation. Furthermore, we discuss related
terms used in different communities and map them to our framework. We focus on
four modalities prevalent in real-world fact-checking: text, image, audio, and
video. We survey benchmarks and models, and discuss limitations and promising
directions for future research
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13632">Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yifu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1">Yftah Ziser</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1">Edoardo M. Ponti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a></p>
<p>Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13850">Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiangnan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1">Qian Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juncheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1">Duo Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaozhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siliang Tang</a></p>
<p>Visual Relation Extraction (VRE) is a powerful means of discovering
relationships between entities within visually-rich documents. Existing methods
often focus on manipulating entity features to find pairwise relations, yet
neglect the more fundamental structural information that links disparate entity
pairs together. The absence of global structure information may make the model
struggle to learn long-range relations and easily predict conflicted results.
To alleviate such limitations, we propose a \textbf{G}l\textbf{O}bal
\textbf{S}tructure knowledge-guided relation \textbf{E}xtraction
(\textbf{\model}) framework. {\model} initiates by generating preliminary
relation predictions on entity pairs extracted from a scanned image of the
document. Subsequently, global structural knowledge is captured from the
preceding iterative predictions, which are then incorporated into the
representations of the entities. This ``generate-capture-incorporate'' cycle is
repeated multiple times, allowing entity representations and global structure
knowledge to be mutually reinforced. Extensive experiments validate that
{\model} not only outperforms existing methods in the standard fine-tuning
setting but also reveals superior cross-lingual learning capabilities; indeed,
even yields stronger data-efficient performance in the low-resource setting.
The code for GOSE will be available at https://github.com/chenxn2020/GOSE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14327">Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Da Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Fan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1">Ming Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a></p>
<p>Instruction tuning has emerged to enhance the capabilities of large language
models (LLMs) to comprehend instructions and generate appropriate responses.
Existing methods either manually annotate or employ LLM (e.g., GPT-series) to
generate data for instruction tuning. However, they often overlook associating
instructions with existing annotated datasets. In this paper, we propose
Dynosaur, a dynamic growth paradigm for the automatic curation of
instruction-tuning data. Based on the metadata of existing datasets, we use
LLMs to automatically construct instruction-tuning data by identifying relevant
data fields and generating appropriate instructions.
</p>
<p>By leveraging the existing annotated datasets, Dynosaur offers several
advantages: 1) it reduces the API cost for generating instructions (e.g., it
costs less than $12 USD by calling GPT-3.5-turbo for generating 800K
instruction tuning samples; 2) it provides high-quality data for instruction
tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform
with comparable data sizes); and 3) it supports the continuous improvement of
models by generating instruction-tuning data when a new annotated dataset
becomes available. We further investigate a continual learning scheme for
learning with the ever-growing instruction-tuning dataset, and demonstrate that
replaying tasks with diverse instruction embeddings not only helps mitigate
forgetting issues but generalizes to unseen tasks better.
</p>
<p>Code and data are available at https://github.com/WadeYin9712/Dynosaur.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14331">What Else Do I Need to Know? The Effect of Background Information on Users&#x27; Reliance on QA Systems. (arXiv:2305.14331v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1">Navita Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1">Eleftheria Briakou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Amanda Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Baumler_C/0/1/0/all/0/1">Connor Baumler</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonial_C/0/1/0/all/0/1">Claire Bonial</a>, <a href="http://arxiv.org/find/cs/1/au:+Micher_J/0/1/0/all/0/1">Jeffrey Micher</a>, <a href="http://arxiv.org/find/cs/1/au:+Voss_C/0/1/0/all/0/1">Clare R. Voss</a>, <a href="http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1">Marine Carpuat</a>, <a href="http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1">Hal Daum&#xe9; III</a></p>
<p>NLP systems have shown impressive performance at answering questions by
retrieving relevant context. However, with the increasingly large models, it is
impossible and often undesirable to constrain models' knowledge or reasoning to
only the retrieved context. This leads to a mismatch between the information
that the models access to derive the answer and the information that is
available to the user to assess the model predicted answer. In this work, we
study how users interact with QA systems in the absence of sufficient
information to assess their predictions. Further, we ask whether adding the
requisite background helps mitigate users' over-reliance on predictions. Our
study reveals that users rely on model predictions even in the absence of
sufficient information needed to assess the model's correctness. Providing the
relevant background, however, helps users better catch model errors, reducing
over-reliance on incorrect predictions. On the flip side, background
information also increases users' confidence in their accurate as well as
inaccurate judgments. Our work highlights that supporting users' verification
of QA predictions is an important, yet challenging, problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14702">DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yebowen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaiqiang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sangwoo Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1">Hassan Foroosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fei Liu</a></p>
<p>Human preference judgments are pivotal in guiding large language models
(LLMs) to produce outputs that align with human values. Human evaluations are
also used in summarization tasks to compare outputs from various systems,
complementing existing automatic metrics. Despite their significance, however,
there has been limited research probing these pairwise or $k$-wise comparisons.
The collective impact and relative importance of factors such as output length,
informativeness, fluency, and factual consistency are still not well
understood. It is also unclear if there are other hidden factors influencing
human judgments. In this paper, we conduct an in-depth examination of a
collection of pairwise human judgments released by OpenAI. Utilizing the
Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in
these human judgments. We find that the most favored factors vary across tasks
and genres, whereas the least favored factors tend to be consistent, e.g.,
outputs are too brief, contain excessive off-focus content or hallucinated
facts. Our findings have implications on the construction of balanced datasets
in human preference evaluations, which is a crucial step in shaping the
behaviors of future LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14802">Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Harvey Yiyun Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qinyuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1">Albert Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a></p>
<p>Large Language Models (LLMs) have the impressive ability to perform
in-context learning (ICL) from only a few examples, but the success of ICL
varies widely from task to task. Thus, it is important to quickly determine
whether ICL is applicable to a new task, but directly evaluating ICL accuracy
can be expensive in situations where test data is expensive to annotate -- the
exact situations where ICL is most appealing. In this paper, we propose the
task of ICL accuracy estimation, in which we predict the accuracy of an LLM
when doing in-context learning on a new task given only unlabeled test data for
that task. To perform ICL accuracy estimation, we propose a method that trains
a meta-model using LLM confidence scores as features. We compare our method to
several strong accuracy estimation baselines on a new benchmark that covers 4
LLMs and 3 task collections. The meta-model improves over all baselines across
8 out of 12 settings and achieves the same estimation performance as directly
evaluating on 40 collected labeled test examples per task. At the same time, no
existing approach provides an accurate and reliable ICL accuracy estimation in
every setting, highlighting the need for better ways to measure the uncertainty
of LLM predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14907">Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shivanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1">Matt Gardner</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a></p>
<p>In-context learning (ICL), the ability of large language models to perform
novel tasks by conditioning on a prompt with a few task examples, requires
these examples to be informative about the test instance. The standard approach
of independently ranking and selecting the most similar examples selects
redundant examples while omitting important information. In this work, we show
that BERTScore-Recall (BSR) selects better examples that demonstrate more of
the salient aspects, e.g. reasoning patterns, of the test input. We further
extend BSR and many standard metrics to easily optimizable set-level metrics,
giving still better coverage of those salient aspects. On 15 datasets spanning
6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric
for in-context example selection across the board, and (2) for compositional
tasks, set selection using Set-BSR outperforms independent ranking by up to 17
points on average and, despite being training-free, surpasses methods that
leverage task or LLM-specific training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14956">Editing Common Sense in Transformers. (arXiv:2305.14956v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Anshita Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1">Debanjan Mondal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheshadri_A/0/1/0/all/0/1">Akshay Krishna Sheshadri</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenlong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Lorraine Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1">Sarah Wiegreffe</a>, <a href="http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1">Niket Tandon</a></p>
<p>Editing model parameters directly in Transformers makes updating open-source
transformer-based models possible without re-training (Meng et al., 2023).
However, these editing methods have only been evaluated on statements about
encyclopedic knowledge with a single correct answer. Commonsense knowledge with
multiple correct answers, e.g., an apple can be green or red but not
transparent, has not been studied but is as essential for enhancing
transformers' reliability and usefulness. In this paper, we investigate whether
commonsense judgments are causally associated with localized, editable
parameters in Transformers, and we provide an affirmative answer. We find that
directly applying the MEMIT editing algorithm results in sub-par performance
and improve it for the commonsense domain by varying edit tokens and improving
the layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models
edited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and
10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel
evaluation dataset, PROBE SET, that contains unaffected and affected
neighborhoods, affected paraphrases, and affected reasoning challenges.
$MEMIT_{CSK}$ performs well across the metrics while fine-tuning baselines show
significant trade-offs between unaffected and affected metrics. These results
suggest a compelling future direction for incorporating feedback about common
sense into Transformers through direct model editing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15064">AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models. (arXiv:2305.15064v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1">Siqi Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a></p>
<p>Recent large language models (LLMs) are promising for making decisions in
grounded environments. However, LLMs frequently fail in complex decision-making
tasks due to the misalignment between the pre-trained knowledge in LLMs and the
actual rules in the environment. Existing methods require either costly
gradient computation or lengthy in-context demonstrations. In this paper, we
propose AutoPlan, an approach to guide LLM-based agents to accomplish
interactive decision-making tasks. AutoPlan augments the LLM prompt with a
task-solving plan and optimizes it through iterative experience collection and
reflection. Our experiments show that AutoPlan, though using no in-context
demonstrations, achieves success rates on par with the baselines using
human-written demonstrations on ALFWorld and even outperforms them by 8% on
HotpotQA. The code is available at https://github.com/owaski/AutoPlan.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15080">Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models. (arXiv:2305.15080v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Geewook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hodong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Daehee Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1">Haeji Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sanghee Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoonsik Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Sangdoo Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kil_T/0/1/0/all/0/1">Taeho Kil</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Bado Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Seunghyun Park</a></p>
<p>Recent advances in Large Language Models (LLMs) have stimulated a surge of
research aimed at extending their applications to the visual domain. While
these models exhibit promise in generating abstract image captions and
facilitating natural conversations, their performance on text-rich images still
requires improvement. In this paper, we introduce Contrastive Reading Model
(Cream), a novel neural architecture designed to enhance the language-image
understanding capability of LLMs by capturing intricate details that are often
overlooked in existing methods. Cream combines vision and auxiliary encoders,
fortified by a contrastive feature alignment technique, to achieve a more
effective comprehension of language information in visually situated contexts
within the images. Our approach bridges the gap between vision and language
understanding, paving the way for the development of more sophisticated
Document Intelligence Assistants. Through rigorous evaluations across diverse
visually-situated language understanding tasks that demand reasoning
capabilities, we demonstrate the compelling performance of Cream, positioning
it as a prominent model in the field of visual document understanding. We
provide our codebase and newly-generated datasets at
https://github.com/naver-ai/cream .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15814">Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages. (arXiv:2305.15814v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madhani_Y/0/1/0/all/0/1">Yash Madhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1">Mitesh M. Khapra</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1">Anoop Kunchukuttan</a></p>
<p>We create publicly available language identification (LID) datasets and
models in all 22 Indian languages listed in the Indian constitution in both
native-script and romanized text. First, we create Bhasha-Abhijnaanam, a
language identification test set for native-script as well as romanized text
which spans all 22 Indic languages. We also train IndicLID, a language
identifier for all the above-mentioned languages in both native and romanized
script. For native-script text, it has better language coverage than existing
LIDs and is competitive or better than other LIDs. IndicLID is the first LID
for romanized text in Indian languages. Two major challenges for romanized text
LID are the lack of training data and low-LID performance when languages are
similar. We provide simple and effective solutions to these problems. In
general, there has been limited work on romanized text in any language, and our
findings are relevant to other languages that need romanized language
identification. Our models are publicly available at
https://ai4bharat.iitm.ac.in/indiclid under open-source licenses. Our training
and test sets are also publicly available at
https://ai4bharat.iitm.ac.in/bhasha-abhijnaanam under open-source licenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16264">Scaling Data-Constrained Language Models. (arXiv:2305.16264v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1">Niklas Muennighoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1">Boaz Barak</a>, <a href="http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1">Teven Le Scao</a>, <a href="http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1">Aleksandra Piktus</a>, <a href="http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1">Nouamane Tazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1">Sampo Pyysalo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1">Thomas Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a></p>
<p>The current trend of scaling language models involves increasing both
parameter count and training dataset size. Extrapolating this trend suggests
that training dataset size may soon be limited by the amount of text data
available on the internet. Motivated by this limit, we investigate scaling
language models in data-constrained regimes. Specifically, we run a large set
of experiments varying the extent of data repetition and compute budget,
ranging up to 900 billion training tokens and 9 billion parameter models. We
find that with constrained data for a fixed compute budget, training with up to
4 epochs of repeated data yields negligible changes to loss compared to having
unique data. However, with more repetition, the value of adding compute
eventually decays to zero. We propose and empirically validate a scaling law
for compute optimality that accounts for the decreasing value of repeated
tokens and excess parameters. Finally, we experiment with approaches mitigating
data scarcity, including augmenting the training dataset with code data or
removing commonly used filters. Models and datasets from our 400 training runs
are freely available at https://github.com/huggingface/datablations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04618">Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lifan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1">Ganqu Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hongcheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_F/0/1/0/all/0/1">Fangyuan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xingyi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>This paper reexamines the research on out-of-distribution (OOD) robustness in
the field of NLP. We find that the distribution shift settings in previous
studies commonly lack adequate challenges, hindering the accurate evaluation of
OOD robustness. To address these issues, we propose a benchmark construction
protocol that ensures clear differentiation and challenging distribution
shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution
robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we
conduct a series of experiments on pre-trained language models for analysis and
evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the
relationship between in-distribution (ID) and OOD performance. We identify
three typical types that unveil the inner learning mechanism, which could
potentially facilitate the forecasting of OOD robustness, correlating with the
advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and
find that, despite exhibiting some effectiveness in specific cases, they do not
offer significant improvement compared to vanilla fine-tuning. Further, we
evaluate 5 LLMs with various adaptation paradigms and find that when sufficient
ID data is available, fine-tuning domain-specific models outperform LLMs on ID
examples significantly. However, in the case of OOD instances, prioritizing
LLMs with in-context learning yields better results. We identify that both
fine-tuned small models and LLMs face challenges in effectively addressing
downstream tasks. The code is public at
\url{https://github.com/lifan-yuan/OOD_NLP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08937">DocumentNet: Bridging the Data Gap in Document Pre-Training. (arXiv:2306.08937v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lijun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1">Jin Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaoyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiayi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1">Alexander G. Hauptmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1">Hanjun Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wei Wei</a></p>
<p>Document understanding tasks, in particular, Visually-rich Document Entity
Retrieval (VDER), have gained significant attention in recent years thanks to
their broad applications in enterprise AI. However, publicly available data
have been scarce for these tasks due to strict privacy constraints and high
annotation costs. To make things worse, the non-overlapping entity spaces from
different datasets hinder the knowledge transfer between document types. In
this paper, we propose a method to collect massive-scale and weakly labeled
data from the web to benefit the training of VDER models. The collected
dataset, named DocumentNet, does not depend on specific document types or
entity sets, making it universally applicable to all VDER tasks. The current
DocumentNet consists of 30M documents spanning nearly 400 document types
organized in a four-level ontology. Experiments on a set of broadly adopted
VDER tasks show significant improvements when DocumentNet is incorporated into
the pre-training for both classic and few-shot learning settings. With the
recent emergence of large language models (LLMs), DocumentNet provides a large
data source to extend their multi-modal capabilities for VDER.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01458">CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tong Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liangzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wangyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1">Mingbai Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1">Noa Garcia</a></p>
<p>The recent advances in natural language processing (NLP), have led to a new
trend of applying large language models (LLMs) to real-world scenarios. While
the latest LLMs are astonishingly fluent when interacting with humans, they
suffer from the misinformation problem by unintentionally generating factually
false statements. This can lead to harmful consequences, especially when
produced within sensitive contexts, such as healthcare. Yet few previous works
have focused on evaluating misinformation in the long-form (LF) generation of
LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have
been shown to perform well in different languages, misinformation evaluation
has been mostly conducted in English. To this end, we present a benchmark,
CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic,
specifically the maternity and infant care domain; and 2) a language other than
English, namely Chinese. Most importantly, we provide an innovative paradigm
for building LF generation evaluation benchmarks that can be transferred to
other knowledge-intensive domains and low-resourced languages. Our proposed
benchmark fills the gap between the extensive usage of LLMs and the lack of
datasets for assessing the misinformation generated by these models. It
contains 1,612 expert-checked questions, accompanied with human-selected
references. Using our benchmark, we conduct extensive experiments and found
that current Chinese LLMs are far from perfect in the topic of maternity and
infant care. In an effort to minimize the reliance on human resources for
performance evaluation, we offer off-the-shelf judgment models for
automatically assessing the LF output of LLMs given benchmark questions.
Moreover, we compare potential solutions for LF generation evaluation and
provide insights for building better automated metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04721">Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1">Suvir Mirchandani</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1">Pete Florence</a>, <a href="http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1">Brian Ichter</a>, <a href="http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1">Danny Driess</a>, <a href="http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1">Montserrat Gonzalez Arenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1">Kanishka Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Andy Zeng</a></p>
<p>We observe that pre-trained large language models (LLMs) are capable of
autoregressively completing complex token sequences -- from arbitrary ones
procedurally generated by probabilistic context-free grammars (PCFG), to more
rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a
general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern
completion proficiency can be partially retained even when the sequences are
expressed using tokens randomly sampled from the vocabulary. These results
suggest that without any additional training, LLMs can serve as general
sequence modelers, driven by in-context learning. In this work, we investigate
how these zero-shot capabilities may be applied to problems in robotics -- from
extrapolating sequences of numbers that represent states over time to complete
simple motions, to least-to-most prompting of reward-conditioned trajectories
that can discover and represent closed-loop policies (e.g., a stabilizing
controller for CartPole). While difficult to deploy today for real systems due
to latency, context size limitations, and compute costs, the approach of using
LLMs to drive low-level control may provide an exciting glimpse into how the
patterns among words could be transferred to actions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08701">AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lichang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunaratna_K/0/1/0/all/0/1">Kalpa Gunaratna</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1">Vikas Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zheng Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1">Vijay Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hongxia Jin</a></p>
<p>Large language models~(LLMs) strengthen instruction-following capability
through instruction-finetuning (IFT) on supervised instruction/response data.
However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly
contain many low-quality instances with incorrect or irrelevant responses,
which are misleading and detrimental to IFT. In this paper, we propose a simple
and effective data selection strategy that automatically identifies and filters
out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we
introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered
from the 52k Alpaca data. AlpaGasus significantly outperforms the original
Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human
evaluation. Its 13B variant matches $&gt;90\%$ performance of its teacher LLM
(i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also
provides 5.7x faster training, reducing the training time for a 7B variant from
80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the
efficacy of our method across diverse datasets, base models, and LLM filters.
Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be
generally applied to instruction-tuning data, leading to faster training and
better instruction-following models. Our project page is available at:
\url{https://lichang-chen.github.io/AlpaGasus/}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16200">A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zefa Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1">Ziyi Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jing Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bo Xu</a></p>
<p>This paper focuses on term-status pair extraction from medical dialogues
(MD-TSPE), which is essential in diagnosis dialogue systems and the automatic
scribe of electronic medical records (EMRs). In the past few years, works on
MD-TSPE have attracted increasing research attention, especially after the
remarkable progress made by generative methods. However, these generative
methods output a whole sequence consisting of term-status pairs in one stage
and ignore integrating prior knowledge, which demands a deeper understanding to
model the relationship between terms and infer the status of each term. This
paper presents a knowledge-enhanced two-stage generative framework (KTGF) to
address the above challenges. Using task-specific prompts, we employ a single
model to complete the MD-TSPE through two phases in a unified generative form:
we generate all terms the first and then generate the status of each generated
term. In this way, the relationship between terms can be learned more
effectively from the sequence containing only terms in the first phase, and our
designed knowledge-enhanced prompt in the second phase can leverage the
category and status candidates of the generated term for status generation.
Furthermore, our proposed special status "not mentioned" makes more terms
available and enriches the training data in the second phase, which is critical
in the low-resource setting. The experiments on the Chunyu and CMDD datasets
show that the proposed method achieves superior results compared to the
state-of-the-art models in the full training and low-resource settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13056">Lexical Diversity in Kinship Across Languages and Dialects. (arXiv:2308.13056v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalilia_H/0/1/0/all/0/1">Hadi Khalilia</a>, <a href="http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1">G&#xe1;bor Bella</a>, <a href="http://arxiv.org/find/cs/1/au:+Freihat_A/0/1/0/all/0/1">Abed Alhakim Freihat</a>, <a href="http://arxiv.org/find/cs/1/au:+Darma_S/0/1/0/all/0/1">Shandy Darma</a>, <a href="http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1">Fausto Giunchiglia</a></p>
<p>Languages are known to describe the world in diverse ways. Across lexicons,
diversity is pervasive, appearing through phenomena such as lexical gaps and
untranslatability. However, in computational resources, such as multilingual
lexical databases, diversity is hardly ever represented. In this paper, we
introduce a method to enrich computational lexicons with content relating to
linguistic diversity. The method is verified through two large-scale case
studies on kinship terminology, a domain known to be diverse across languages
and cultures: one case study deals with seven Arabic dialects, while the other
one with three Indonesian languages. Our results, made available as browseable
and downloadable computational resources, extend prior linguistics research on
kinship terminology, and provide insight into the extent of diversity even
within linguistically and culturally close communities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00359">Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1">Ashmit Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1">Aditya Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1">Aanisha Bhattacharyya</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1">Yaman K Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Somesh Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1">Uttaran Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1">Ishita Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrangeli_S/0/1/0/all/0/1">Stefano Petrangeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rajiv Ratn Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changyou Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1">Balaji Krishnamurthy</a></p>
<p>Shannon, in his seminal paper introducing information theory, divided the
communication into three levels: technical, semantic, and effectivenss. While
the technical level is concerned with accurate reconstruction of transmitted
symbols, the semantic and effectiveness levels deal with the inferred meaning
and its effect on the receiver. Thanks to telecommunications, the first level
problem has produced great advances like the internet. Large Language Models
(LLMs) make some progress towards the second goal, but the third level still
remains largely untouched. The third problem deals with predicting and
optimizing communication for desired receiver behavior. LLMs, while showing
wide generalization capabilities across a wide range of tasks, are unable to
solve for this. One reason for the underperformance could be a lack of
``behavior tokens'' in LLMs' training corpora. Behavior tokens define receiver
behavior over a communication, such as shares, likes, clicks, purchases,
retweets, etc. While preprocessing data for LLM training, behavior tokens are
often removed from the corpora as noise. Therefore, in this paper, we make some
initial progress towards reintroducing behavior tokens in LLM training. The
trained models, other than showing similar performance to LLMs on content
understanding tasks, show generalization capabilities on behavior simulation,
content simulation, behavior understanding, and behavior domain adaptation.
Using a wide range of tasks on two corpora, we show results on all these
capabilities. We call these models Large Content and Behavior Models (LCBMs).
Further, to spur more research on LCBMs, we release our new Content Behavior
Corpus (CBC), a repository containing communicator, message, and corresponding
receiver behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01522">What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You. (arXiv:2309.01522v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haiping Huang</a></p>
<p>The recently released artificial intelligence conversational agent, ChatGPT,
has gained significant attention in academia and real life. A multitude of
early ChatGPT users eagerly explore its capabilities and share their opinions
on it via social media. Both user queries and social media posts express public
concerns regarding this advanced dialogue system. To mine public concerns about
ChatGPT, a novel Self-Supervised neural Topic Model (SSTM), which formalizes
topic modeling as a representation learning procedure, is proposed in this
paper. Extensive experiments have been conducted on Twitter posts about ChatGPT
and queries asked by ChatGPT users. And experimental results demonstrate that
the proposed approach could extract higher quality public concerns with
improved interpretability and diversity, surpassing the performance of
state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00100">Multilingual Natural Language Processing Model for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lindo_M/0/1/0/all/0/1">Mariana Lindo</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1">Ana Sofia Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferreira_A/0/1/0/all/0/1">Andr&#xe9; Ferreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianning Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1">Gijs Luijten</a>, <a href="http://arxiv.org/find/cs/1/au:+Correia_G/0/1/0/all/0/1">Gustavo Correia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Moon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1">Jens Kleesiek</a>, <a href="http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1">Jan Egger</a>, <a href="http://arxiv.org/find/cs/1/au:+Alves_V/0/1/0/all/0/1">Victor Alves</a></p>
<p>The impression section of a radiology report summarizes important radiology
findings and plays a critical role in communicating these findings to
physicians. However, the preparation of these summaries is time-consuming and
error-prone for radiologists. Recently, numerous models for radiology report
summarization have been developed. Nevertheless, there is currently no model
that can summarize these reports in multiple languages. Such a model could
greatly improve future research and the development of Deep Learning models
that incorporate data from patients with different ethnic backgrounds. In this
study, the generation of radiology impressions in different languages was
automated by fine-tuning a model, publicly available, based on a multilingual
text-to-text Transformer to summarize findings available in English,
Portuguese, and German radiology reports. In a blind test, two board-certified
radiologists indicated that for at least 70% of the system-generated summaries,
the quality matched or exceeded the corresponding human-written summaries,
suggesting substantial clinical reliability. Furthermore, this study showed
that the multilingual model outperformed other models that specialized in
summarizing radiology reports in only one language, as well as models that were
not specifically designed for summarizing radiology reports, such as ChatGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02255">MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Pan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tony Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiacheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1">Michel Galley</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11191">Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Flores_L/0/1/0/all/0/1">Lorenzo Jaime Yu Flores</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heyuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1">Kejian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chheang_S/0/1/0/all/0/1">Sophie Chheang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>Text simplification has emerged as an increasingly useful application of AI
for bridging the communication gap in specialized fields such as medicine,
where the lexicon is often dominated by technical jargon and complex
constructs. Despite notable progress, methods in medical simplification
sometimes result in the generated text having lower quality and diversity. In
this work, we explore ways to further improve the readability of text
simplification in the medical domain. We propose (1) a new unlikelihood loss
that encourages generation of simpler terms and (2) a reranked beam search
decoding method that optimizes for simplicity, which achieve better performance
on readability metrics on three datasets. This study's findings offer promising
avenues for improving text simplification in the medical field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13588">Simultaneous Machine Translation with Tailored Reference. (arXiv:2310.13588v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shoutao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaolei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Simultaneous machine translation (SiMT) generates translation while reading
the whole source sentence. However, existing SiMT models are typically trained
using the same reference disregarding the varying amounts of available source
information at different latency. Training the model with ground-truth at low
latency may introduce forced anticipations, whereas utilizing reference
consistent with the source word order at high latency results in performance
degradation. Consequently, it is crucial to train the SiMT model with
appropriate reference that avoids forced anticipations during training while
maintaining high quality. In this paper, we propose a novel method that
provides tailored reference for the SiMT models trained at different latency by
rephrasing the ground-truth. Specifically, we introduce the tailor, induced by
reinforcement learning, to modify ground-truth to the tailored reference. The
SiMT model is trained with the tailored reference and jointly optimized with
the tailor to enhance performance. Importantly, our method is applicable to a
wide range of current SiMT approaches. Experiments on three translation tasks
demonstrate that our method achieves state-of-the-art performance in both fixed
and adaptive policies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14159">Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ko_D/0/1/0/all/0/1">Dayoon Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gunhee Kim</a></p>
<p>As short-form funny videos on social networks are gaining popularity, it
becomes demanding for AI models to understand them for better communication
with humans. Unfortunately, previous video humor datasets target specific
domains, such as speeches or sitcoms, and mostly focus on verbal cues. We
curate a user-generated dataset of 10K multimodal funny videos from YouTube,
called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both
verbal and visual elements contributing to humor. After filtering, we annotate
each video with timestamps and text explanations for funny moments. Our
ExFunTube is unique over existing datasets in that our videos cover a wide
range of domains with various types of humor that necessitate a multimodal
understanding of the content. Also, we develop a zero-shot video-to-text
prompting to maximize video humor understanding of large language models
(LLMs). With three different evaluation methods using automatic scores,
rationale quality experiments, and human evaluations, we show that our
prompting significantly improves LLMs' ability for humor explanation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15113">Counting the Bugs in ChatGPT&#x27;s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model. (arXiv:2310.15113v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weissweiler_L/0/1/0/all/0/1">Leonie Weissweiler</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1">Valentin Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantharuban_A/0/1/0/all/0/1">Anjali Kantharuban</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1">Anna Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1">Ritam Dutt</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengle_A/0/1/0/all/0/1">Amey Hengle</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabra_A/0/1/0/all/0/1">Anubha Kabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1">Atharva Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Vijayakumar_A/0/1/0/all/0/1">Abhishek Vijayakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haofei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a>, <a href="http://arxiv.org/find/cs/1/au:+Oflazer_K/0/1/0/all/0/1">Kemal Oflazer</a>, <a href="http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1">David R. Mortensen</a></p>
<p>Large language models (LLMs) have recently reached an impressive level of
linguistic capability, prompting comparisons with human language skills.
However, there have been relatively few systematic inquiries into the
linguistic capabilities of the latest generation of LLMs, and those studies
that do exist (i) ignore the remarkable ability of humans to generalize, (ii)
focus only on English, and (iii) investigate syntax or semantics and overlook
other capabilities that lie at the heart of human language, like morphology.
Here, we close these gaps by conducting the first rigorous analysis of the
morphological capabilities of ChatGPT in four typologically varied languages
(specifically, English, German, Tamil, and Turkish). We apply a version of
Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for
the four examined languages. We find that ChatGPT massively underperforms
purpose-built systems, particularly in English. Overall, our results -- through
the lens of morphology -- cast a new light on the linguistic capabilities of
ChatGPT, suggesting that claims of human-like language skills are premature and
misleading.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15694">COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1">Lin Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuanzhao Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1">Yu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruifeng Xu</a></p>
<p>The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Fitting (COPF), in which we estimate a series of optimal policies using
the Monte Carlo method, and then continually fit the policy sequence with the
function regularization. COPF involves a single learning phase and doesn't
necessitate complex reinforcement learning. Importantly, it shares the
capability with RLHF to learn from unlabeled data, making it flexible for
continual preference learning. Our experimental results show that COPF
outperforms strong Continuous learning (CL) baselines when it comes to
consistently aligning with human preferences on different tasks and domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16218">Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Song Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yaochen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haochen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zaiyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jundong Li</a></p>
<p>Large language models (LLMs) have recently transformed both the academic and
industrial landscapes due to their remarkable capacity to understand, analyze,
and generate texts based on their vast knowledge and reasoning ability.
Nevertheless, one major drawback of LLMs is their substantial computational
cost for pre-training due to their unprecedented amounts of parameters. The
disadvantage is exacerbated when new knowledge frequently needs to be
introduced into the pre-trained model. Therefore, it is imperative to develop
effective and efficient techniques to update pre-trained LLMs. Traditional
methods encode new knowledge in pre-trained LLMs through direct fine-tuning.
However, naively re-training LLMs can be computationally intensive and risks
degenerating valuable pre-trained knowledge irrelevant to the update in the
model. Recently, Knowledge-based Model Editing (KME) has attracted increasing
attention, which aims to precisely modify the LLMs to incorporate specific
knowledge, without negatively influencing other irrelevant knowledge. In this
survey, we aim to provide a comprehensive and in-depth overview of recent
advances in the field of KME. We first introduce a general formulation of KME
to encompass different KME strategies. Afterward, we provide an innovative
taxonomy of KME techniques based on how the new knowledge is introduced into
pre-trained LLMs, and investigate existing KME strategies while analyzing key
insights, advantages, and limitations of methods from each category. Moreover,
representative metrics, datasets, and applications of KME are introduced
accordingly. Finally, we provide an in-depth analysis regarding the
practicality and remaining challenges of KME and suggest promising research
directions for further advancement in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16350">Unraveling Feature Extraction Mechanisms in Neural Networks. (arXiv:2310.16350v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiaobing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaxi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wei Lu</a></p>
<p>The underlying mechanism of neural networks in capturing precise knowledge
has been the subject of consistent research efforts. In this work, we propose a
theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such
mechanisms. Specifically, considering the infinite network width, we
hypothesize the learning dynamics of target models may intuitively unravel the
features they acquire from training data, deepening our insights into their
internal mechanisms. We apply our approach to several fundamental models and
reveal how these models leverage statistical features during gradient descent
and how they are integrated into final decisions. We also discovered that the
choice of activation function can affect feature extraction. For instance, the
use of the \textit{ReLU} activation function could potentially introduce a bias
in features, providing a plausible explanation for its replacement with
alternative functions in recent pre-trained language models. Additionally, we
find that while self-attention and CNN models may exhibit limitations in
learning n-grams, multiplication-based models seem to excel in this area. We
verify these theoretical findings through experiments and find that they can be
applied to analyze language modeling tasks, which can be regarded as a special
variant of classification. Our contributions offer insights into the roles and
capacities of fundamental components within large language models, thereby
aiding the broader understanding of these complex systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16436">DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. (arXiv:2310.16436v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Ge Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiajin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong-Yu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sibei Yang</a></p>
<p>A long-standing goal of AI systems is to perform complex multimodal reasoning
like humans. Recently, large language models (LLMs) have made remarkable
strides in such multi-step reasoning on the language modality solely by
leveraging the chain of thought (CoT) to mimic human thinking. However, the
transfer of these advancements to multimodal contexts introduces heightened
challenges, including but not limited to the impractical need for
labor-intensive annotation and the limitations in terms of flexibility,
generalizability, and explainability. To evoke CoT reasoning in multimodality,
this work first conducts an in-depth analysis of these challenges posed by
multimodality and presents two key insights: "keeping critical thinking" and
"letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this
study proposes a novel DDCoT prompting that maintains a critical attitude
through negative-space prompting and incorporates multimodality into reasoning
by first dividing the reasoning responsibility of LLMs into reasoning and
recognition and then integrating the visual recognition capability of visual
models into the joint reasoning process. The rationales generated by DDCoT not
only improve the reasoning abilities of both large and small language models in
zero-shot prompting and fine-tuning learning, significantly outperforming
state-of-the-art methods but also exhibit impressive generalizability and
explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16713">SkyMath: Technical Report. (arXiv:2310.16713v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haihua Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wenjun Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenxia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lunan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jianfei Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Tianwen Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Biye Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Bo Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guoliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xuejie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xilin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rui Hu</a></p>
<p>Large language models (LLMs) have shown great potential to solve varieties of
natural language processing (NLP) tasks, including mathematical reasoning. In
this work, we present SkyMath, a large language model for mathematics with 13
billion parameters. By applying self-compare fine-tuning, we have enhanced
mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,
SkyMath outperforms all known open-source models of similar size and has
established a new SOTA performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16776">DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Devleena Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1">Vivek Khetan</a></p>
<p>Recent advances have led to the availability of many pre-trained language
models (PLMs); however, a question that remains is how much data is truly
needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,
a data-efficient fine-tuning framework that leverages unsupervised core-set
selection to minimize the amount of data needed to fine-tune PLMs for
downstream tasks. We demonstrate the efficacy of our DEFT framework in the
context of text-editing LMs, and compare to the state-of-the art text-editing
model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT
models are just as accurate as CoEDIT while being finetuned on ~70% less data.
</p>
</p>
</div>

    </div>
    </body>
    