<!DOCTYPE html>
<html>
<head>
<title>2023-12-08-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.02984">Diff-GO: Diffusion Goal-Oriented Communications to Achieve Ultra-High Spectrum Efficiency. (arXiv:2312.02984v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wijesinghe_A/0/1/0/all/0/1">Achintha Wijesinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wanninayaka_S/0/1/0/all/0/1">Suchinthaka Wanninayaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zhi Ding</a></p>
<p>The latest advances in artificial intelligence (AI) present many
unprecedented opportunities to achieve much improved bandwidth saving in
communications. Unlike conventional communication systems focusing on packet
transport, rich datasets and AI makes it possible to efficiently transfer only
the information most critical to the goals of message recipients. One of the
most exciting advances in generative AI known as diffusion model presents a
unique opportunity for designing ultra-fast communication systems well beyond
language-based messages. This work presents an ultra-efficient communication
design by utilizing generative AI-based on diffusion models as a specific
example of the general goal-oriented communication framework. To better control
the regenerated message at the receiver output, our diffusion system design
includes a local regeneration module with finite dimensional noise latent. The
critical significance of noise latent control and sharing residing on our
Diff-GO is the ability to introduce the concept of "local generative feedback"
(Local-GF), which enables the transmitter to monitor the quality and gauge the
quality or accuracy of the message recovery at the semantic system receiver. To
this end, we propose a new low-dimensional noise space for the training of
diffusion models, which significantly reduces the communication overhead and
achieves satisfactory message recovery performance. Our experimental results
demonstrate that the proposed noise space and the diffusion-based generative
model achieve ultra-high spectrum efficiency and accurate recovery of
transmitted image signals. By trading off computation for bandwidth efficiency
(C4BE), this new framework provides an important avenue to achieve exceptional
computation-bandwidth tradeoff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02997">Simulation-Based Inference of Surface Accumulation and Basal Melt Rates of an Antarctic Ice Shelf from Isochronal Layers. (arXiv:2312.02997v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Moss_G/0/1/0/all/0/1">Guy Moss</a>, <a href="http://arxiv.org/find/physics/1/au:+Visnjevic_V/0/1/0/all/0/1">Vjeran Vi&#x161;njevi&#x107;</a>, <a href="http://arxiv.org/find/physics/1/au:+Eisen_O/0/1/0/all/0/1">Olaf Eisen</a>, <a href="http://arxiv.org/find/physics/1/au:+Oraschewski_F/0/1/0/all/0/1">Falk M. Oraschewski</a>, <a href="http://arxiv.org/find/physics/1/au:+Schroder_C/0/1/0/all/0/1">Cornelius Schr&#xf6;der</a>, <a href="http://arxiv.org/find/physics/1/au:+Macke_J/0/1/0/all/0/1">Jakob H. Macke</a>, <a href="http://arxiv.org/find/physics/1/au:+Drews_R/0/1/0/all/0/1">Reinhard Drews</a></p>
<p>The ice shelves buttressing the Antarctic ice sheet determine the rate of
ice-discharge into the surrounding oceans. The geometry of ice shelves, and
hence their buttressing strength, is determined by ice flow as well as by the
local surface accumulation and basal melt rates, governed by atmospheric and
oceanic conditions. Contemporary methods resolve one of these rates, but
typically not both. Moreover, there is little information of how they changed
in time. We present a new method to simultaneously infer the surface
accumulation and basal melt rates averaged over decadal and centennial
timescales. We infer the spatial dependence of these rates along flow line
transects using internal stratigraphy observed by radars, using a kinematic
forward model of internal stratigraphy. We solve the inverse problem using
simulation-based inference (SBI). SBI performs Bayesian inference by training
neural networks on simulations of the forward model to approximate the
posterior distribution, allowing us to also quantify uncertainties over the
inferred parameters. We demonstrate the validity of our method on a synthetic
example, and apply it to Ekstr\"om Ice Shelf, Antarctica, for which newly
acquired radar measurements are available. We obtain posterior distributions of
surface accumulation and basal melt averaging over 42, 84, 146, and 188 years
before 2022. Our results suggest stable atmospheric and oceanographic
conditions over this period in this catchment of Antarctica. Use of observed
internal stratigraphy can separate the effects of surface accumulation and
basal melt, allowing them to be interpreted in a historical context of the last
centuries and beyond.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03001">Computer Vision for Increased Operative Efficiency via Identification of Instruments in the Neurosurgical Operating Room: A Proof-of-Concept Study. (arXiv:2312.03001v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zachem_T/0/1/0/all/0/1">Tanner J. Zachem</a> (1,2), <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Sully F. Chen</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Venkatraman_V/0/1/0/all/0/1">Vishal Venkatraman</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Sykes_D/0/1/0/all/0/1">David AW Sykes</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Prakash_R/0/1/0/all/0/1">Ravi Prakash</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Spellicy_S/0/1/0/all/0/1">Samantha Spellicy</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Suarez_A/0/1/0/all/0/1">Alexander D Suarez</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Ross_W/0/1/0/all/0/1">Weston Ross</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Codd_P/0/1/0/all/0/1">Patrick J. Codd</a> (1,2) ((1) Department of Neurosurgery, Duke University School of Medicine, Durham, NC, USA, (2) Department of Mechanical Engineering and Materials Science, Duke University, Durham, NC, USA)</p>
<p>Objectives Computer vision (CV) is a field of artificial intelligence that
enables machines to interpret and understand images and videos. CV has the
potential to be of assistance in the operating room (OR) to track surgical
instruments. We built a CV algorithm for identifying surgical instruments in
the neurosurgical operating room as a potential solution for surgical
instrument tracking and management to decrease surgical waste and opening of
unnecessary tools. Methods We collected 1660 images of 27 commonly used
neurosurgical instruments. Images were labeled using the VGG Image Annotator
and split into 80% training and 20% testing sets in order to train a U-Net
Convolutional Neural Network using 5-fold cross validation. Results Our U-Net
achieved a tool identification accuracy of 80-100% when distinguishing 25
classes of instruments, with 19/25 classes having accuracy over 90%. The model
performance was not adequate for sub classifying Adson, Gerald, and Debakey
forceps, which had accuracies of 60-80%. Conclusions We demonstrated the
viability of using machine learning to accurately identify surgical
instruments. Instrument identification could help optimize surgical tray
packing, decrease tool usage and waste, decrease incidence of instrument
misplacement events, and assist in timing of routine instrument maintenance.
More training data will be needed to increase accuracy across all surgical
instruments that would appear in a neurosurgical operating room. Such
technology has the potential to be used as a method to be used for proving what
tools are truly needed in each type of operation allowing surgeons across the
world to do more with less.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03002">The mechanistic basis of data dependence and abrupt learning in an in-context classification task. (arXiv:2312.03002v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reddy_G/0/1/0/all/0/1">Gautam Reddy</a></p>
<p>Transformer models exhibit in-context learning: the ability to accurately
predict the response to a novel query based on illustrative examples in the
input sequence. In-context learning contrasts with traditional in-weights
learning of query-output relationships. What aspects of the training data
distribution and architecture favor in-context vs in-weights learning? Recent
work has shown that specific distributional properties inherent in language,
such as burstiness, large dictionaries and skewed rank-frequency distributions,
control the trade-off or simultaneous appearance of these two forms of
learning. We first show that these results are recapitulated in a minimal
attention-only network trained on a simplified dataset. In-context learning
(ICL) is driven by the abrupt emergence of an induction head, which
subsequently competes with in-weights learning. By identifying progress
measures that precede in-context learning and targeted experiments, we
construct a two-parameter model of an induction head which emulates the full
data distributional dependencies displayed by the attention-based network. A
phenomenological model of induction head formation traces its abrupt emergence
to the sequential learning of three nested logits enabled by an intrinsic
curriculum. We propose that the sharp transitions in attention-based networks
arise due to a specific chain of multi-layer operations necessary to achieve
ICL, which is implemented by nested nonlinearities sequentially learned during
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03004">Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning. (arXiv:2312.03004v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinchuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1">Bei Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1">Chong Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1">Ling Tian</a></p>
<p>Temporal Knowledge Graph (TKG) reasoning that forecasts future events based
on historical snapshots distributed over timestamps is denoted as extrapolation
and has gained significant attention. Owing to its extreme versatility and
variation in spatial and temporal correlations, TKG reasoning presents a
challenging task, demanding efficient capture of concurrent structures and
evolutional interactions among facts. While existing methods have made strides
in this direction, they still fall short of harnessing the diverse forms of
intrinsic expressive semantics of TKGs, which encompass entity correlations
across multiple timestamps and periodicity of temporal information. This
limitation constrains their ability to thoroughly reflect historical
dependencies and future trends. In response to these drawbacks, this paper
proposes an innovative reasoning approach that focuses on Learning Multi-graph
Structure (LMS). Concretely, it comprises three distinct modules concentrating
on multiple aspects of graph structure knowledge within TKGs, including
concurrent and evolutional patterns along timestamps, query-specific
correlations across timestamps, and semantic dependencies of timestamps, which
capture TKG features from various perspectives. Besides, LMS incorporates an
adaptive gate for merging entity representations both along and across
timestamps effectively. Moreover, it integrates timestamp semantics into graph
attention calculations and time-aware decoders, in order to impose temporal
constraints on events and narrow down prediction scopes with historical
statistics. Extensive experimental results on five event-based benchmark
datasets demonstrate that LMS outperforms state-of-the-art extrapolation
models, indicating the superiority of modeling a multi-graph perspective for
TKG reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03005">Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature Representations. (arXiv:2312.03005v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jae Young Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wonjun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jaehyun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yongkwi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1">Young Seog Yoon</a></p>
<p>Anomaly detection is a critical and challenging task that aims to identify
data points deviating from normal patterns and distributions within a dataset.
Various methods have been proposed using a one-class-one-model approach, but
these techniques often face practical problems such as memory inefficiency and
the requirement of sufficient data for training. In particular, few-shot
anomaly detection presents significant challenges in industrial applications,
where limited samples are available before mass production. In this paper, we
propose a few-shot anomaly detection method that integrates adversarial
training loss to obtain more robust and generalized feature representations. We
utilize the adversarial loss previously employed in domain adaptation to align
feature distributions between source and target domains, to enhance feature
robustness and generalization in few-shot anomaly detection tasks. We
hypothesize that adversarial loss is effective when applied to features that
should have similar characteristics, such as those from the same layer in a
Siamese network's parallel branches or input-output pairs of
reconstruction-based methods. Experimental results demonstrate that the
proposed method generally achieves better performance when utilizing the
adversarial loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03006">Cone Ranking for Multi-Criteria Decision Making. (arXiv:2312.03006v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hamel_A/0/1/0/all/0/1">Andreas H Hamel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostner_D/0/1/0/all/0/1">Daniel Kostner</a></p>
<p>Recently introduced cone distribution functions from statistics are turned
into multi-criteria decision making (MCDM) tools. It is demonstrated that this
procedure can be considered as an upgrade of the weighted sum scalarization
insofar as it absorbs a whole collection of weighted sum scalarizations at once
instead of fixing a particular one in advance. Moreover, situations are
characterized in which different types of rank reversal occur, and it is
explained why this might even be useful for analyzing the ranking procedure. A
few examples will be discussed and a potential application in machine learning
is outlined.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03008">Deep Reinforcement Learning for Community Battery Scheduling under Uncertainties of Load, PV Generation, and Energy Prices. (arXiv:2312.03008v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jiarong Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>In response to the growing uptake of distributed energy resources (DERs),
community batteries have emerged as a promising solution to support renewable
energy integration, reduce peak load, and enhance grid reliability. This paper
presents a deep reinforcement learning (RL) strategy, centered around the soft
actor-critic (SAC) algorithm, to schedule a community battery system in the
presence of uncertainties, such as solar photovoltaic (PV) generation, local
demand, and real-time energy prices. We position the community battery to play
a versatile role, in integrating local PV energy, reducing peak load, and
exploiting energy price fluctuations for arbitrage, thereby minimizing the
system cost. To improve exploration and convergence during RL training, we
utilize the noisy network technique. This paper conducts a comparative study of
different RL algorithms, including proximal policy optimization (PPO) and deep
deterministic policy gradient (DDPG) algorithms, to evaluate their
effectiveness in the community battery scheduling problem. The results
demonstrate the potential of RL in addressing community battery scheduling
challenges and show that the SAC algorithm achieves the best performance
compared to RL and optimization benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03009">I-PHYRE: Interactive Physical Reasoning. (arXiv:2312.03009v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiqian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kewen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yixin Zhu</a></p>
<p>Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03012">A Waddington landscape for prototype learning in generalized Hopfield networks. (arXiv:2312.03012v1 [cond-mat.dis-nn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Boukacem_N/0/1/0/all/0/1">Nacer Eddine Boukacem</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Leary_A/0/1/0/all/0/1">Allen Leary</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Theriault_R/0/1/0/all/0/1">Robin Th&#xe9;riault</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gottlieb_F/0/1/0/all/0/1">Felix Gottlieb</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Mani_M/0/1/0/all/0/1">Madhav Mani</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Francois_P/0/1/0/all/0/1">Paul Fran&#xe7;ois</a></p>
<p>Networks in machine learning offer examples of complex high-dimensional
dynamical systems reminiscent of biological systems. Here, we study the
learning dynamics of Generalized Hopfield networks, which permit a
visualization of internal memories. These networks have been shown to proceed
through a 'feature-to-prototype' transition, as the strength of network
nonlinearity is increased, wherein the learned, or terminal, states of internal
memories transition from mixed to pure states. Focusing on the prototype
learning dynamics of the internal memories we observe a strong resemblance to
the canalized, or low-dimensional, dynamics of cells as they differentiate
within a Waddingtonian landscape. Dynamically, we demonstrate that learning in
a Generalized Hopfield Network proceeds through sequential 'splits' in memory
space. Furthermore, order of splitting is interpretable and reproducible. The
dynamics between the splits are canalized in the Waddington sense -- robust to
variations in detailed aspects of the system. In attempting to make the analogy
a rigorous equivalence, we study smaller subsystems that exhibit similar
properties to the full system. We combine analytical calculations with
numerical simulations to study the dynamical emergence of the
feature-to-prototype transition, and the behaviour of splits in the landscape,
saddles points, visited during learning. We exhibit regimes where saddles
appear and disappear through saddle-node bifurcations, qualitatively changing
the distribution of learned memories as the strength of the nonlinearity is
varied -- allowing us to systematically investigate the mechanisms that
underlie the emergence of Waddingtonian dynamics. Memories can thus
differentiate in a predictive and controlled way, revealing new bridges between
experimental biology, dynamical systems theory, and machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03013">Breast Ultrasound Report Generation using LangChain. (arXiv:2312.03013v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1">Jaeyoung Huh</a>, <a href="http://arxiv.org/find/eess/1/au:+Park_H/0/1/0/all/0/1">Hyun Jeong Park</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Breast ultrasound (BUS) is a critical diagnostic tool in the field of breast
imaging, aiding in the early detection and characterization of breast
abnormalities. Interpreting breast ultrasound images commonly involves creating
comprehensive medical reports, containing vital information to promptly assess
the patient's condition. However, the ultrasound imaging system necessitates
capturing multiple images of various parts to compile a single report,
presenting a time-consuming challenge. To address this problem, we propose the
integration of multiple image analysis tools through a LangChain using Large
Language Models (LLM), into the breast reporting process. Through a combination
of designated tools and text generation through LangChain, our method can
accurately extract relevant features from ultrasound images, interpret them in
a clinical context, and produce comprehensive and standardized reports. This
approach not only reduces the burden on radiologists and healthcare
professionals but also enhances the consistency and quality of reports. The
extensive experiments shows that each tools involved in the proposed method can
offer qualitatively and quantitatively significant results. Furthermore,
clinical evaluation on the generated reports demonstrates that the proposed
method can make report in clinically meaningful way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03014">Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey. (arXiv:2312.03014v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shengchao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1">Guodong Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dikai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengqi Zhang</a></p>
<p>As artificial intelligence (AI) continues to rapidly evolve, the realm of
Earth and atmospheric sciences is increasingly adopting data-driven models,
powered by progressive developments in deep learning (DL). Specifically, DL
techniques are extensively utilized to decode the chaotic and nonlinear aspects
of Earth systems, and to address climate challenges via understanding weather
and climate data. Cutting-edge performance on specific tasks within narrower
spatio-temporal scales has been achieved recently through DL. The rise of large
models, specifically large language models (LLMs), has enabled fine-tuning
processes that yield remarkable outcomes across various downstream tasks,
thereby propelling the advancement of general AI. However, we are still
navigating the initial stages of crafting general AI for weather and climate.
In this survey, we offer an exhaustive, timely overview of state-of-the-art AI
methodologies specifically engineered for weather and climate data, with a
special focus on time series and text data. Our primary coverage encompasses
four critical aspects: types of weather and climate data, principal model
architectures, model scopes and applications, and datasets for weather and
climate. Furthermore, in relation to the creation and application of foundation
models for weather and climate data understanding, we delve into the field's
prevailing challenges, offer crucial insights, and propose detailed avenues for
future research. This comprehensive approach equips practitioners with the
requisite knowledge to make substantial progress in this domain. Our survey
encapsulates the most recent breakthroughs in research on large, data-driven
models for weather and climate data understanding, emphasizing robust
foundations, current advancements, practical applications, crucial resources,
and prospective research opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03015">PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation. (arXiv:2312.03015v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuchen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiayuan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yunhao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a></p>
<p>Open-world 3D part segmentation is pivotal in diverse applications such as
robotics and AR/VR. Traditional supervised methods often grapple with limited
3D data availability and struggle to generalize to unseen object categories.
PartSLIP, a recent advancement, has made significant strides in zero- and
few-shot 3D part segmentation. This is achieved by harnessing the capabilities
of the 2D open-vocabulary detection module, GLIP, and introducing a heuristic
method for converting and lifting multi-view 2D bounding box predictions into
3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced
version designed to overcome the limitations of its predecessor. Our approach
incorporates two major improvements. First, we utilize a pre-trained 2D
segmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more
precise and accurate annotations than the 2D bounding boxes used in PartSLIP.
Second, PartSLIP++ replaces the heuristic 3D conversion process with an
innovative modified Expectation-Maximization algorithm. This algorithm
conceptualizes 3D instance segmentation as unobserved latent variables, and
then iteratively refines them through an alternating process of 2D-3D matching
and optimization with gradient descent. Through extensive evaluations, we show
that PartSLIP++ demonstrates better performance over PartSLIP in both low-shot
3D semantic and instance-based object part segmentation tasks. Code released at
https://github.com/zyc00/PartSLIP2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03016">Protein Language Model-Powered 3D Ligand Binding Site Prediction from Protein Sequence. (arXiv:2312.03016v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xie_L/0/1/0/all/0/1">Lei Xie</a></p>
<p>Prediction of ligand binding sites of proteins is a fundamental and important
task for understanding the function of proteins and screening potential drugs.
Most existing methods require experimentally determined protein holo-structures
as input. However, such structures can be unavailable on novel or less-studied
proteins. To tackle this limitation, we propose LaMPSite, which only takes
protein sequences and ligand molecular graphs as input for ligand binding site
predictions. The protein sequences are used to retrieve residue-level
embeddings and contact maps from the pre-trained ESM-2 protein language model.
The ligand molecular graphs are fed into a graph neural network to compute
atom-level embeddings. Then we compute and update the protein-ligand
interaction embedding based on the protein residue-level embeddings and ligand
atom-level embeddings, and the geometric constraints in the inferred protein
contact map and ligand distance map. A final pooling on protein-ligand
interaction embedding would indicate which residues belong to the binding
sites. Without any 3D coordinate information of proteins, our proposed model
achieves competitive performance compared to baseline methods that require 3D
protein structures when predicting binding sites. Given that less than 50% of
proteins have reliable structure information in the current stage, LaMPSite
will provide new opportunities for drug discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03017">AI-driven emergence of frequency information non-uniform distribution via THz metasurface spectrum prediction. (arXiv:2312.03017v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xiaohua Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yuqi Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Die Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiankun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_B/0/1/0/all/0/1">Bingxuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jianquan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1">Deyi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Liang Wu</a></p>
<p>Recently, artificial intelligence has been extensively deployed across
various scientific disciplines, optimizing and guiding the progression of
experiments through the integration of abundant datasets, whilst continuously
probing the vast theoretical space encapsulated within the data. Particularly,
deep learning models, due to their end-to-end adaptive learning capabilities,
are capable of autonomously learning intrinsic data features, thereby
transcending the limitations of traditional experience to a certain extent.
Here, we unveil previously unreported information characteristics pertaining to
different frequencies emerged during our work on predicting the terahertz
spectral modulation effects of metasurfaces based on AI-prediction. Moreover,
we have substantiated that our proposed methodology of simply adding
supplementary multi-frequency inputs to the existing dataset during the target
spectral prediction process can significantly enhance the predictive accuracy
of the network. This approach effectively optimizes the utilization of existing
datasets and paves the way for interdisciplinary research and applications in
artificial intelligence, chemistry, composite material design, biomedicine, and
other fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03020">Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment. (arXiv:2312.03020v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Surya_A/0/1/0/all/0/1">Aaditya Surya</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1">Aditya Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Kabore_J/0/1/0/all/0/1">Jarnell Kabore</a>, <a href="http://arxiv.org/find/eess/1/au:+Sasikumar_S/0/1/0/all/0/1">Subash Sasikumar</a></p>
<p>This research introduces a sophisticated transfer learning model based on
Google's MobileNetV2 for breast cancer tumor classification into normal,
benign, and malignant categories, utilizing a dataset of 1576 ultrasound images
(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of
0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and
MCC of 0.74. It examines image intensity distributions and misclassification
errors, offering improvements for future applications. Addressing dataset
imbalances, the study ensures a generalizable model. This work, using a dataset
from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,
emphasizes MobileNetV2's potential in medical imaging, aiming to improve
diagnostic precision in oncology. Additionally, the paper explores
Streamlit-based deployment for real-time tumor classification, demonstrating
MobileNetV2's applicability in medical imaging and setting a benchmark for
future research in oncology diagnostics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03022">Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction. (arXiv:2312.03022v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Hongbin Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1">Honghao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Aijia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wei Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1">Weiqiang Jia</a></p>
<p>Knowledge graph construction (KGC) is a multifaceted undertaking involving
the extraction of entities, relations, and events. Traditionally, large
language models (LLMs) have been viewed as solitary task-solving agents in this
complex landscape. However, this paper challenges this paradigm by introducing
a novel framework, CooperKGC. Departing from the conventional approach,
CooperKGC establishes a collaborative processing network, assembling a KGC
collaboration team capable of concurrently addressing entity, relation, and
event extraction tasks. Our experiments unequivocally demonstrate that
fostering collaboration and information interaction among diverse agents within
CooperKGC yields superior results compared to individual cognitive processes
operating in isolation. Importantly, our findings reveal that the collaboration
facilitated by CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03025">Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction. (arXiv:2312.03025v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1">Zilin Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a></p>
<p>The task of multimodal relation extraction has attracted significant research
attention, but progress is constrained by the scarcity of available training
data. One natural thought is to extend existing datasets with cross-modal
generative models. In this paper, we consider a novel problem setting, where
only unimodal data, either text or image, are available during training. We aim
to train a multimodal classifier from synthetic data that perform well on real
multimodal test data. However, training with synthetic data suffers from two
obstacles: lack of data diversity and label information loss. To alleviate the
issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta
GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to
promote diversity in the generated data and exploits a teacher network to
select valuable training samples with high mutual information with the
ground-truth labels. Comparing our method to direct training on synthetic data,
we observed a significant improvement of 24.06% F1 with synthetic text and
26.42% F1 with synthetic images. Notably, our best model trained on completely
synthetic images outperforms prior state-of-the-art models trained on real
multimodal data by a margin of 3.76% in F1. Our codebase will be made available
upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03037">Analysis and mining of low-carbon and energy-saving tourism data characteristics based on machine learning algorithm. (arXiv:2312.03037v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wierzbinski_L/0/1/0/all/0/1">Lukasz Wierzbinski</a></p>
<p>In order to study the formation mechanism of residents' low-carbon awareness
and provide an important basis for traffic managers to guide urban residents to
choose low-carbon travel mode, this paper proposes a low-carbon energy-saving
travel data feature analysis and mining based on machine learning algorithm.
This paper uses data mining technology to analyze the data of low-carbon travel
questionnaire, and regards the 15-dimensional problem under the framework of
planned behavior theory as the internal cause variable that characterizes
residents' low-carbon travel willingness. The author uses K-means clustering
algorithm to classify the intensity of residents' low-carbon travel
willingness, and applies the results as the explanatory variables to the random
forest model to explore the mechanism of residents' social attribute
characteristics, travel characteristics, etc. on their low-carbon travel
willingness. The experimental results show that based on the Silhouette index
test and t-SNE dimensionality reduction, residents' low-carbon travel
willingness can be divided into three categories: strong, neutral, and not
strong; Based on the importance index, the four most significant factors are
the occupation, residence, family composition and commuting time of residents.
Conclusion: This method provides policy recommendations for the development and
management of urban traffic low-carbon from multiple perspectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03038">Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit. (arXiv:2312.03038v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fanfei Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lele Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxin Wang</a></p>
<p>Transformer requires a fixed number of layers and heads which makes them
inflexible to the complexity of individual samples and expensive in training
and inference. To address this, we propose a sample-based Dynamic Hierarchical
Transformer (DHT) model whose layers and heads can be dynamically configured
with single data samples via solving contextual bandit problems. To determine
the number of layers and heads, we use the Uniform Confidence Bound while we
deploy combinatorial Thompson Sampling in order to select specific head
combinations given their number. Different from previous work that focuses on
compressing trained networks for inference only, DHT is not only advantageous
for adaptively optimizing the underlying network architecture during training
but also has a flexible network for efficient inference. To the best of our
knowledge, this is the first comprehensive data-driven dynamic transformer
without any additional auxiliary neural networks that implement the dynamic
system. According to the experiment results, we achieve up to 74% computational
savings for both training and inference with a minimal loss of accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03041">Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil. (arXiv:2312.03041v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Youwai_S/0/1/0/all/0/1">Sompote Youwai</a>, <a href="http://arxiv.org/find/cs/1/au:+Thongnoo_C/0/1/0/all/0/1">Chissanupong Thongnoo</a></p>
<p>This paper presents a novel deep learning model based on the transformer
architecture to predict the load-deformation behavior of large bored piles in
Bangkok subsoil. The model encodes the soil profile and pile features as
tokenization input, and generates the load-deformation curve as output. The
model also incorporates the previous sequential data of load-deformation curve
into the decoder to improve the prediction accuracy. The model also
incorporates the previous sequential data of load-deformation curve into the
decoder. The model shows a satisfactory accuracy and generalization ability for
the load-deformation curve prediction, with a mean absolute error of 5.72% for
the test data. The model could also be used for parametric analysis and design
optimization of piles under different soil and pile conditions, pile cross
section, pile length and type of pile.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03044">REST: Enhancing Group Robustness in DNNs through Reweighted Sparse Training. (arXiv:2312.03044v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiaxu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Lu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1">Mykola Pechenizkiy</a></p>
<p>The deep neural network (DNN) has been proven effective in various domains.
However, they often struggle to perform well on certain minority groups during
inference, despite showing strong performance on the majority of data groups.
This is because over-parameterized models learned \textit{bias attributes} from
a large number of \textit{bias-aligned} training samples. These bias attributes
are strongly spuriously correlated with the target variable, causing the models
to be biased towards spurious correlations (i.e., \textit{bias-conflicting}).
To tackle this issue, we propose a novel \textbf{re}weighted \textbf{s}parse
\textbf{t}raining framework, dubbed as \textit{\textbf{REST}}, which aims to
enhance the performance of biased data while improving computation and memory
efficiency. Our proposed REST framework has been experimentally validated on
three datasets, demonstrating its effectiveness in exploring unbiased
subnetworks. We found that REST reduces the reliance on spuriously correlated
features, leading to better performance across a wider range of data groups
with fewer training and inference resources. We highlight that the
\textit{REST} framework represents a promising approach for improving the
performance of DNNs on biased data, while simultaneously improving computation
and memory efficiency. By reducing the reliance on spurious correlations, REST
has the potential to enhance the robustness of DNNs and improve their
generalization capabilities. Code is released at
\url{https://github.com/zhao1402072392/REST}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03051">Generating Interpretable Networks using Hypernetworks. (arXiv:2312.03051v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_I/0/1/0/all/0/1">Isaac Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1">Max Tegmark</a></p>
<p>An essential goal in mechanistic interpretability to decode a network, i.e.,
to convert a neural network's raw weights to an interpretable algorithm. Given
the difficulty of the decoding problem, progress has been made to understand
the easier encoding problem, i.e., to convert an interpretable algorithm into
network weights. Previous works focus on encoding existing algorithms into
networks, which are interpretable by definition. However, focusing on encoding
limits the possibility of discovering new algorithms that humans have never
stumbled upon, but that are nevertheless interpretable. In this work, we
explore the possibility of using hypernetworks to generate interpretable
networks whose underlying algorithms are not yet known. The hypernetwork is
carefully designed such that it can control network complexity, leading to a
diverse family of interpretable algorithms ranked by their complexity. All of
them are interpretable in hindsight, although some of them are less intuitive
to humans, hence providing new insights regarding how to "think" like a neural
network. For the task of computing L1 norms, hypernetworks find three
algorithms: (a) the double-sided algorithm, (b) the convexity algorithm, (c)
the pudding algorithm, although only the first algorithm was expected by the
authors before experiments. We automatically classify these algorithms and
analyze how these algorithmic phases develop during training, as well as how
they are affected by complexity control. Furthermore, we show that a trained
hypernetwork can correctly construct models for input dimensions not seen in
training, demonstrating systematic generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03057">Advantage of Quantum Machine Learning from General Computational Advantages. (arXiv:2312.03057v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Yamasaki_H/0/1/0/all/0/1">Hayata Yamasaki</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Isogai_N/0/1/0/all/0/1">Natsuto Isogai</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Murao_M/0/1/0/all/0/1">Mio Murao</a></p>
<p>An overarching milestone of quantum machine learning (QML) is to demonstrate
the advantage of QML over all possible classical learning methods in
accelerating a common type of learning task as represented by supervised
learning with classical data. However, the provable advantages of QML in
supervised learning have been known so far only for the learning tasks designed
for using the advantage of specific quantum algorithms, i.e., Shor's
algorithms. Here we explicitly construct an unprecedentedly broader family of
supervised learning tasks with classical data to offer the provable advantage
of QML based on general quantum computational advantages, progressing beyond
Shor's algorithms. Our learning task is feasibly achievable by executing a
general class of functions that can be computed efficiently in polynomial time
for a large fraction of inputs by arbitrary quantum algorithms but not by any
classical algorithm. We prove the hardness of achieving this learning task for
any possible polynomial-time classical learning method. We also clarify
protocols for preparing the classical data to demonstrate this learning task in
experiments. These results open routes to exploit a variety of quantum
advantages in computing functions for the experimental demonstration of the
advantage of QML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03096">Incidental Polysemanticity. (arXiv:2312.03096v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lecomte_V/0/1/0/all/0/1">Victor Lecomte</a>, <a href="http://arxiv.org/find/cs/1/au:+Thaman_K/0/1/0/all/0/1">Kushal Thaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chow_T/0/1/0/all/0/1">Trevor Chow</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1">Rylan Schaeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1">Sanmi Koyejo</a></p>
<p>Polysemantic neurons (neurons that activate for a set of unrelated features)
have been seen as a significant obstacle towards interpretability of
task-optimized deep networks, with implications for AI safety. The classic
origin story of polysemanticity is that the data contains more "features" than
neurons, such that learning to perform a task forces the network to co-allocate
multiple unrelated features to the same neuron, endangering our ability to
understand the network's internal processing. In this work, we present a second
and non-mutually exclusive origin story of polysemanticity. We show that
polysemanticity can arise incidentally, even when there are ample neurons to
represent all features in the data, using a combination of theory and
experiments. This second type of polysemanticity occurs because random
initialization can, by chance alone, initially assign multiple features to the
same neuron, and the training dynamics then strengthen such overlap. Due to its
origin, we term this \textit{incidental polysemanticity}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03102">Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI. (arXiv:2312.03102v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Young_S/0/1/0/all/0/1">Sean I. Young</a>, <a href="http://arxiv.org/find/eess/1/au:+Balbastre_Y/0/1/0/all/0/1">Ya&#xeb;l Balbastre</a>, <a href="http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1">Bruce Fischl</a>, <a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1">Polina Golland</a>, <a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1">Juan Eugenio Iglesias</a></p>
<p>In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR)
refers to computational reconstruction of an unknown 3D magnetic resonance
volume from stacks of 2D slices corrupted by motion. While promising, current
SVR methods require multiple slice stacks for accurate 3D reconstruction,
leading to long scans and limiting their use in time-sensitive applications
such as fetal fMRI. Here, we propose a SVR method that overcomes the
shortcomings of previous work and produces state-of-the-art reconstructions in
the presence of extreme inter-slice motion. Inspired by the recent success of
single-view depth estimation methods, we formulate SVR as a single-stack motion
estimation task and train a fully convolutional network to predict a motion
stack for a given slice stack, producing a 3D reconstruction as a byproduct of
the predicted motion. Extensive experiments on the SVR of adult and fetal
brains demonstrate that our fully convolutional method is twice as accurate as
previous SVR methods. Our code is available at github.com/seannz/svr.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03120">The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning. (arXiv:2312.03120v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Subasi_O/0/1/0/all/0/1">Omer Subasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bel_O/0/1/0/all/0/1">Oceane Bel</a>, <a href="http://arxiv.org/find/cs/1/au:+Manzano_J/0/1/0/all/0/1">Joseph Manzano</a>, <a href="http://arxiv.org/find/cs/1/au:+Barker_K/0/1/0/all/0/1">Kevin Barker</a></p>
<p>With the advance of the powerful heterogeneous, parallel and distributed
computing systems and ever increasing immense amount of data, machine learning
has become an indispensable part of cutting-edge technology, scientific
research and consumer products. In this study, we present a review of modern
machine and deep learning. We provide a high-level overview for the latest
advanced machine learning algorithms, applications, and frameworks. Our
discussion encompasses parallel distributed learning, deep learning as well as
federated learning. As a result, our work serves as an introductory text to the
vast field of modern machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03126">Learning Curricula in Open-Ended Worlds. (arXiv:2312.03126v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minqi Jiang</a></p>
<p>Deep reinforcement learning (RL) provides powerful methods for training
optimal sequential decision-making agents. As collecting real-world
interactions can entail additional costs and safety risks, the common paradigm
of sim2real conducts training in a simulator, followed by real-world
deployment. Unfortunately, RL agents easily overfit to the choice of simulated
training environments, and worse still, learning ends when the agent masters
the specific set of simulated environments. In contrast, the real world is
highly open-ended, featuring endlessly evolving environments and challenges,
making such RL approaches unsuitable. Simply randomizing over simulated
environments is insufficient, as it requires making arbitrary distributional
assumptions and can be combinatorially less likely to sample specific
environment instances that are useful for learning. An ideal learning process
should automatically adapt the training environment to maximize the learning
potential of the agent over an open-ended task space that matches or surpasses
the complexity of the real world. This thesis develops a class of methods
called Unsupervised Environment Design (UED), which aim to produce such
open-ended processes. Given an environment design space, UED automatically
generates an infinite sequence or curriculum of training environments at the
frontier of the learning agent's capabilities. Through extensive empirical
studies and theoretical arguments founded on minimax-regret decision theory and
game theory, the findings in this thesis show that UED autocurricula can
produce RL agents exhibiting significantly improved robustness and
generalization to previously unseen environment instances. Such autocurricula
are promising paths toward open-ended learning systems that achieve more
general intelligence by continually generating and mastering additional
challenges of their own design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03134">A Hardware Evaluation Framework for Large Language Model Inference. (arXiv:2312.03134v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_A/0/1/0/all/0/1">August Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhakar_R/0/1/0/all/0/1">Rohan Prabhakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Wentzlaff_D/0/1/0/all/0/1">David Wentzlaff</a></p>
<p>The past year has witnessed the increasing popularity of Large Language
Models (LLMs). Their unprecedented scale and associated high hardware cost have
impeded their broader adoption, calling for efficient hardware designs. With
the large hardware needed to simply run LLM inference, evaluating different
hardware designs becomes a new bottleneck.
</p>
<p>This work introduces LLMCompass, a hardware evaluation framework for LLM
inference workloads. LLMCompass is fast, accurate, versatile, and able to
describe and evaluate different hardware designs. LLMCompass includes a mapper
to automatically find performance-optimal mapping and scheduling. It also
incorporates an area-based cost model to help architects reason about their
design choices. Compared to real-world hardware, LLMCompass' estimated latency
achieves an average 10.4% error rate across various operators with various
input sizes and an average 4.1% error rate for LLM inference. With LLMCompass,
simulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done
within 16 minutes on commodity hardware, including 26,400 rounds of the
mapper's parameter search.
</p>
<p>With the aid of LLMCompass, this work draws architectural implications and
explores new cost-effective hardware designs. By reducing the compute
capability or replacing High Bandwidth Memory (HBM) with traditional DRAM,
these new designs can achieve as much as 3.41x improvement in performance/cost
compared to an NVIDIA A100, making them promising choices for democratizing
LLMs.
</p>
<p>LLMCompass is planned to be fully open-source.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03140">FlexModel: A Framework for Interpretability of Distributed Large Language Models. (arXiv:2312.03140v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Matthew Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1">Muhammad Adil Asif</a>, <a href="http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1">John Willes</a>, <a href="http://arxiv.org/find/cs/1/au:+Emerson_D/0/1/0/all/0/1">David Emerson</a></p>
<p>With the growth of large language models, now incorporating billions of
parameters, the hardware prerequisites for their training and deployment have
seen a corresponding increase. Although existing tools facilitate model
parallelization and distributed training, deeper model interactions, crucial
for interpretability and responsible AI techniques, still demand thorough
knowledge of distributed computing. This often hinders contributions from
researchers with machine learning expertise but limited distributed computing
background. Addressing this challenge, we present FlexModel, a software package
providing a streamlined interface for engaging with models distributed across
multi-GPU and multi-node configurations. The library is compatible with
existing model distribution libraries and encapsulates PyTorch models. It
exposes user-registerable HookFunctions to facilitate straightforward
interaction with distributed model internals, bridging the gap between
distributed and single-device model paradigms. Primarily, FlexModel enhances
accessibility by democratizing model interactions and promotes more inclusive
research in the domain of large-scale neural networks. The package is found at
https://github.com/VectorInstitute/flex_model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03147">Neural parameter calibration and uncertainty quantification for epidemic forecasting. (arXiv:2312.03147v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gaskin_T/0/1/0/all/0/1">Thomas Gaskin</a>, <a href="http://arxiv.org/find/cs/1/au:+Conrad_T/0/1/0/all/0/1">Tim Conrad</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavliotis_G/0/1/0/all/0/1">Grigorios A. Pavliotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutte_C/0/1/0/all/0/1">Christof Sch&#xfc;tte</a></p>
<p>The recent COVID-19 pandemic has thrown the importance of accurately
forecasting contagion dynamics and learning infection parameters into sharp
focus. At the same time, effective policy-making requires knowledge of the
uncertainty on such predictions, in order, for instance, to be able to ready
hospitals and intensive care units for a worst-case scenario without needlessly
wasting resources. In this work, we apply a novel and powerful computational
method to the problem of learning probability densities on contagion parameters
and providing uncertainty quantification for pandemic projections. Using a
neural network, we calibrate an ODE model to data of the spread of COVID-19 in
Berlin in 2020, achieving both a significantly more accurate calibration and
prediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The
uncertainties on our predictions provide meaningful confidence intervals e.g.
on infection figures and hospitalisation rates, while training and running the
neural scheme takes minutes where MCMC takes hours. We show convergence of our
method to the true posterior on a simplified SIR model of epidemics, and also
demonstrate our method's learning capabilities on a reduced dataset, where a
complex model is learned from a small number of compartments for which data is
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03151">Multitask Learning Can Improve Worst-Group Outcomes. (arXiv:2312.03151v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1">Atharva Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1">Lucio Dery</a>, <a href="http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1">Amrith Setlur</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1">Aditi Raghunathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1">Ameet Talwalkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a></p>
<p>In order to create machine learning systems that serve a variety of users
well, it is vital to not only achieve high average performance but also ensure
equitable outcomes across diverse groups. However, most machine learning
methods are designed to improve a model's average performance on a chosen end
task without consideration for their impact on worst group error. Multitask
learning (MTL) is one such widely used technique. In this paper, we seek not
only to understand the impact of MTL on worst-group accuracy but also to
explore its potential as a tool to address the challenge of group-wise
fairness. We primarily consider the common setting of fine-tuning a pre-trained
model, where, following recent work (Gururangan et al., 2020; Dery et al.,
2023), we multitask the end task with the pre-training objective constructed
from the end task data itself. In settings with few or no group annotations, we
find that multitasking often, but not always, achieves better worst-group
accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative
distributionally robust optimization (DRO) method. Leveraging insights from
synthetic data experiments, we propose to modify standard MTL by regularizing
the joint multitask representation space. We run a large number of fine-tuning
experiments across computer vision and natural language and find that our
regularized MTL approach consistently outperforms JTT on both worst and average
group outcomes. Our official code can be found here:
https://github.com/atharvajk98/MTL-group-robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03160">HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces. (arXiv:2312.03160v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1">Haithem Turki</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1">Vasu Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulo_S/0/1/0/all/0/1">Samuel Rota Bul&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Porzi_L/0/1/0/all/0/1">Lorenzo Porzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kontschieder_P/0/1/0/all/0/1">Peter Kontschieder</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1">Michael Zollh&#xf6;fer</a>, <a href="http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1">Christian Richardt</a></p>
<p>Neural radiance fields provide state-of-the-art view synthesis quality but
tend to be slow to render. One reason is that they make use of volume
rendering, thus requiring many samples (and model queries) per ray at render
time. Although this representation is flexible and easy to optimize, most
real-world objects can be modeled more efficiently with surfaces instead of
volumes, requiring far fewer samples per ray. This observation has spurred
considerable progress in surface representations such as signed distance
functions, but these may struggle to model semi-opaque and thin structures. We
propose a method, HybridNeRF, that leverages the strengths of both
representations by rendering most objects as surfaces while modeling the
(typically) small fraction of challenging regions volumetrically. We evaluate
HybridNeRF against the challenging Eyeful Tower dataset along with other
commonly used view synthesis datasets. When comparing to state-of-the-art
baselines, including recent rasterization-based approaches, we improve error
rates by 15-30% while achieving real-time framerates (at least 36 FPS) for
virtual-reality resolutions (2Kx2K).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03166">Deep Learning for Fast Inference of Mechanistic Models&#x27; Parameters. (arXiv:2312.03166v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borisyak_M/0/1/0/all/0/1">Maxim Borisyak</a>, <a href="http://arxiv.org/find/cs/1/au:+Born_S/0/1/0/all/0/1">Stefan Born</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubauer_P/0/1/0/all/0/1">Peter Neubauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_Bournazou_M/0/1/0/all/0/1">Mariano Nicolas Cruz-Bournazou</a></p>
<p>Inferring parameters of macro-kinetic growth models, typically represented by
Ordinary Differential Equations (ODE), from the experimental data is a crucial
step in bioprocess engineering. Conventionally, estimates of the parameters are
obtained by fitting the mechanistic model to observations. Fitting, however,
requires a significant computational power. Specifically, during the
development of new bioprocesses that use previously unknown organisms or
strains, efficient, robust, and computationally cheap methods for parameter
estimation are of great value. In this work, we propose using Deep Neural
Networks (NN) for directly predicting parameters of mechanistic models given
observations. The approach requires spending computational resources for
training a NN, nonetheless, once trained, such a network can provide parameter
estimates orders of magnitude faster than conventional methods. We consider a
training procedure that combines Neural Networks and mechanistic models. We
demonstrate the performance of the proposed algorithms on data sampled from
several mechanistic models used in bioengineering describing a typical
industrial batch process and compare the proposed method, a typical
gradient-based fitting procedure, and the combination of the two. We find that,
while Neural Network estimates are slightly improved by further fitting, these
estimates are measurably better than the fitting procedure alone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03167">Adaptive spectral graph wavelets for collaborative filtering. (arXiv:2312.03167v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alshareet_O/0/1/0/all/0/1">Osama Alshareet</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1">A. Ben Hamza</a></p>
<p>Collaborative filtering is a popular approach in recommender systems, whose
objective is to provide personalized item suggestions to potential users based
on their purchase or browsing history. However, personalized recommendations
require considerable amount of behavioral data on users, which is usually
unavailable for new users, giving rise to the cold-start problem. To help
alleviate this challenging problem, we introduce a spectral graph wavelet
collaborative filtering framework for implicit feedback data, where users,
items and their interactions are represented as a bipartite graph.
Specifically, we first propose an adaptive transfer function by leveraging a
power transform with the goal of stabilizing the variance of graph frequencies
in the spectral domain. Then, we design a deep recommendation model for
efficient learning of low-dimensional embeddings of users and items using
spectral graph wavelets in an end-to-end fashion. In addition to capturing the
graph's local and global structures, our approach yields localization of graph
signals in both spatial and spectral domains, and hence not only learns
discriminative representations of users and items, but also promotes the
recommendation quality. The effectiveness of our proposed model is demonstrated
through extensive experiments on real-world benchmark datasets, achieving
better recommendation performance compared with strong baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03176">Active Learning for Abrupt Shifts Change-point Detection via Derivative-Aware Gaussian Processes. (arXiv:2312.03176v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1">Rong Pan</a></p>
<p>Change-point detection (CPD) is crucial for identifying abrupt shifts in
data, which influence decision-making and efficient resource allocation across
various domains. To address the challenges posed by the costly and
time-intensive data acquisition in CPD, we introduce the Derivative-Aware
Change Detection (DACD) method. It leverages the derivative process of a
Gaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point
locations effectively. DACD balances the exploitation and exploration of
derivative processes through multiple data acquisition functions (AFs). By
utilizing GP derivative mean and variance as criteria, DACD sequentially
selects the next sampling data point, thus enhancing algorithmic efficiency and
ensuring reliable and accurate results. We investigate the effectiveness of
DACD method in diverse scenarios and show it outperforms other active learning
change-point detection approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03177">Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning. (arXiv:2312.03177v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pathmanathan_P/0/1/0/all/0/1">Pankayaraj Pathmanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1">Natalia D&#xed;az-Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1">Javier Del Ser</a></p>
<p>In this work, we investigate the means of using curiosity on replay buffers
to improve offline multi-task continual reinforcement learning when tasks,
which are defined by the non-stationarity in the environment, are non labeled
and not evenly exposed to the learner in time. In particular, we investigate
the use of curiosity both as a tool for task boundary detection and as a
priority metric when it comes to retaining old transition tuples, which we
respectively use to propose two different buffers. Firstly, we propose a Hybrid
Reservoir Buffer with Task Separation (HRBTS), where curiosity is used to
detect task boundaries that are not known due to the task agnostic nature of
the problem. Secondly, by using curiosity as a priority metric when it comes to
retaining old transition tuples, a Hybrid Curious Buffer (HCB) is proposed. We
ultimately show that these buffers, in conjunction with regular reinforcement
learning algorithms, can be used to alleviate the catastrophic forgetting issue
suffered by the state of the art on replay buffers when the agent's exposure to
tasks is not equal along time. We evaluate catastrophic forgetting and the
efficiency of our proposed buffers against the latest works such as the Hybrid
Reservoir Buffer (HRB) and the Multi-Time Scale Replay Buffer (MTR) in three
different continual reinforcement learning settings. Experiments were done on
classical control tasks and Metaworld environment. Experiments show that our
proposed replay buffers display better immunity to catastrophic forgetting
compared to existing works in most of the settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03179">CaloQVAE : Simulating high-energy particle-calorimeter interactions using hybrid quantum-classical generative models. (arXiv:2312.03179v1 [hep-ex])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ex/1/au:+Hoque_S/0/1/0/all/0/1">Sehmimul Hoque</a> (1, 2), <a href="http://arxiv.org/find/hep-ex/1/au:+Jia_H/0/1/0/all/0/1">Hao Jia</a> (3), <a href="http://arxiv.org/find/hep-ex/1/au:+Abhishek_A/0/1/0/all/0/1">Abhishek Abhishek</a> (4), <a href="http://arxiv.org/find/hep-ex/1/au:+Fadaie_M/0/1/0/all/0/1">Mojde Fadaie</a> (1), <a href="http://arxiv.org/find/hep-ex/1/au:+Toledo_Marin_J/0/1/0/all/0/1">J. Quetzalcoatl Toledo-Mar&#xed;n</a> (4), <a href="http://arxiv.org/find/hep-ex/1/au:+Vale_T/0/1/0/all/0/1">Tiago Vale</a> (5, 4), <a href="http://arxiv.org/find/hep-ex/1/au:+Melko_R/0/1/0/all/0/1">Roger G. Melko</a> (1, 6), <a href="http://arxiv.org/find/hep-ex/1/au:+Swiatlowski_M/0/1/0/all/0/1">Maximilian Swiatlowski</a> (4), <a href="http://arxiv.org/find/hep-ex/1/au:+Fedorko_W/0/1/0/all/0/1">Wojciech T. Fedorko</a> (4) ((1) Perimeter Institute for Theoretical Physics, (2) Faculty of Mathematics, University of Waterloo, (3) Department of Physics and Astronomy, University of British Columbia, (4) TRIUMF, (5) Department of Physics, Simon Fraser University, (6) Department of Physics and Astronomy, University of Waterloo)</p>
<p>The Large Hadron Collider's high luminosity era presents major computational
challenges in the analysis of collision events. Large amounts of Monte Carlo
(MC) simulation will be required to constrain the statistical uncertainties of
the simulated datasets below these of the experimental data. Modelling of
high-energy particles propagating through the calorimeter section of the
detector is the most computationally intensive MC simulation task. We introduce
a technique combining recent advancements in generative models and quantum
annealing for fast and efficient simulation of high-energy particle-calorimeter
interactions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03186">Data-Driven Traffic Reconstruction and Kernel Methods for Identifying Stop-and-Go Congestion. (arXiv:2312.03186v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanchez_E/0/1/0/all/0/1">Edgar Ramirez Sanchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghavan_S/0/1/0/all/0/1">Shreyaa Raghavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Cathy Wu</a></p>
<p>Identifying stop-and-go events (SAGs) in traffic flow presents an important
avenue for advancing data-driven research for climate change mitigation and
sustainability, owing to their substantial impact on carbon emissions, travel
time, fuel consumption, and roadway safety. In fact, SAGs are estimated to
account for 33-50% of highway driving externalities. However, insufficient
attention has been paid to precisely quantifying where, when, and how much
these SAGs take place -necessary for downstream decision making, such as
intervention design and policy analysis. A key challenge is that the data
available to researchers and governments are typically sparse and aggregated to
a granularity that obscures SAGs. To overcome such data limitations, this study
thus explores the use of traffic reconstruction techniques for SAG
identification. In particular, we introduce a kernel-based method for
identifying spatio-temporal features in traffic and leverage bootstrapping to
quantify the uncertainty of the reconstruction process. Experimental results on
California highway data demonstrate the promise of the method for capturing
SAGs. This work contributes to a foundation for data-driven decision making to
advance sustainability of traffic systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03187">FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction. (arXiv:2312.03187v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shuangquan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Junhua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_V/0/1/0/all/0/1">Virginia R. de Sa</a></p>
<p>Researchers have proposed to use data of human preference feedback to
fine-tune text-to-image generative models. However, the scalability of human
feedback collection has been limited by its reliance on manual annotation.
Therefore, we develop and test a method to automatically annotate user
preferences from their spontaneous facial expression reaction to the generated
images. We collect a dataset of Facial Expression Reaction to Generated Images
(FERGI) and show that the activations of multiple facial action units (AUs) are
highly correlated with user evaluations of the generated images. Specifically,
AU4 (brow lowerer) is most consistently reflective of negative evaluations of
the generated image. This can be useful in two ways. Firstly, we can
automatically annotate user preferences between image pairs with substantial
difference in AU4 responses to them with an accuracy significantly
outperforming state-of-the-art scoring models. Secondly, directly integrating
the AU4 responses with the scoring models improves their consistency with human
preferences. Additionally, the AU4 response best reflects the user's evaluation
of the image fidelity, making it complementary to the state-of-the-art scoring
models, which are generally better at reflecting image-text alignment. Finally,
this method of automatic annotation with facial expression analysis can be
potentially generalized to other generation tasks. The code is available at
https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at
the same link for research purposes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03194">Corporate Bankruptcy Prediction with Domain-Adapted BERT. (arXiv:2312.03194v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1">Alex Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sangwon Yoon</a></p>
<p>This study performs BERT-based analysis, which is a representative
contextualized language model, on corporate disclosure data to predict
impending bankruptcies. Prior literature on bankruptcy prediction mainly
focuses on developing more sophisticated prediction methodologies with
financial variables. However, in our study, we focus on improving the quality
of input dataset. Specifically, we employ BERT model to perform sentiment
analysis on MD&amp;A disclosures. We show that BERT outperforms dictionary-based
predictions and Word2Vec-based predictions in terms of adjusted R-square in
logistic regression, k-nearest neighbor (kNN-5), and linear kernel support
vector machine (SVM). Further, instead of pre-training the BERT model from
scratch, we apply self-learning with confidence-based filtering to corporate
disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate
that the domain adaptation procedure brings a significant improvement in
prediction accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03196">Domain Invariant Representation Learning and Sleep Dynamics Modeling for Automatic Sleep Staging. (arXiv:2312.03196v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seungyeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Thai-Hoang Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Ping Zhang</a></p>
<p>Sleep staging has become a critical task in diagnosing and treating sleep
disorders to prevent sleep related diseases. With rapidly growing large scale
public sleep databases and advances in machine learning, significant progress
has been made toward automatic sleep staging. However, previous studies face
some critical problems in sleep studies; the heterogeneity of subjects'
physiological signals, the inability to extract meaningful information from
unlabeled sleep signal data to improve predictive performances, the difficulty
in modeling correlations between sleep stages, and the lack of an effective
mechanism to quantify predictive uncertainty. In this study, we propose a
neural network based automatic sleep staging model, named DREAM, to learn
domain generalized representations from physiological signals and models sleep
dynamics. DREAM learns sleep related and subject invariant representations from
diverse subjects' sleep signal segments and models sleep dynamics by capturing
interactions between sequential signal segments and between sleep stages. In
the experiments, we demonstrate that DREAM outperforms the existing sleep
staging methods on three datasets. The case study demonstrates that our model
can learn the generalized decision function resulting in good prediction
performances for the new subjects, especially in case there are differences
between testing and training subjects. The usage of unlabeled data shows the
benefit of leveraging unlabeled EEG data. Further, uncertainty quantification
demonstrates that DREAM provides prediction uncertainty, making the model
reliable and helping sleep experts in real world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03212">Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements and Provable Convergence. (arXiv:2312.03212v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shengbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a></p>
<p>The partially observable constrained optimization problems (POCOPs) impede
data-driven optimization techniques since an infeasible solution of POCOPs can
provide little information about the objective as well as the constraints. We
endeavor to design an efficient and provable method for expensive POCOPs under
the framework of constrained Bayesian optimization. Our method consists of two
key components. Firstly, we present an improved design of the acquisition
functions that introduces balanced exploration during optimization. We
rigorously study the convergence properties of this design to demonstrate its
effectiveness. Secondly, we propose a Gaussian process embedding different
likelihoods as the surrogate model for a partially observable constraint. This
model leads to a more accurate representation of the feasible regions compared
to traditional classification-based models. Our proposed method is empirically
studied on both synthetic and real-world problems. The results demonstrate the
competitiveness of our method for solving POCOPs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03213">Bootstrap Your Own Variance. (arXiv:2312.03213v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turishcheva_P/0/1/0/all/0/1">Polina Turishcheva</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1">Jason Ramapuram</a>, <a href="http://arxiv.org/find/cs/1/au:+Williamson_S/0/1/0/all/0/1">Sinead Williamson</a>, <a href="http://arxiv.org/find/cs/1/au:+Busbridge_D/0/1/0/all/0/1">Dan Busbridge</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhekane_E/0/1/0/all/0/1">Eeshan Dhekane</a>, <a href="http://arxiv.org/find/cs/1/au:+Webb_R/0/1/0/all/0/1">Russ Webb</a></p>
<p>Understanding model uncertainty is important for many applications. We
propose Bootstrap Your Own Variance (BYOV), combining Bootstrap Your Own Latent
(BYOL), a negative-free Self-Supervised Learning (SSL) algorithm, with Bayes by
Backprop (BBB), a Bayesian method for estimating model posteriors. We find that
the learned predictive std of BYOV vs. a supervised BBB model is well captured
by a Gaussian distribution, providing preliminary evidence that the learned
parameter posterior is useful for label free uncertainty estimation. BYOV
improves upon the deterministic BYOL baseline (+2.83% test ECE, +1.03% test
Brier) and presents better calibration and reliability when tested with various
augmentations (eg: +2.4% test ECE, +1.2% test Brier for Salt &amp; Pepper noise).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03216">SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning. (arXiv:2312.03216v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_E/0/1/0/all/0/1">Eric H. Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lizarraga_A/0/1/0/all/0/1">Andrew Lizarraga</a></p>
<p>In this paper, we introduce a novel algorithm - the Skill-Driven Skill
Recombination Algorithm (SDSRA) - an innovative framework that significantly
enhances the efficiency of achieving maximum entropy in reinforcement learning
tasks. We find that SDSRA achieves faster convergence compared to the
traditional Soft Actor-Critic (SAC) algorithm and produces improved policies.
By integrating skill-based strategies within the robust Actor-Critic framework,
SDSRA demonstrates remarkable adaptability and performance across a wide array
of complex and diverse benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03218">Accelerated Gradient Algorithms with Adaptive Subspace Search for Instance-Faster Optimization. (arXiv:2312.03218v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanshi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hanzhen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_P/0/1/0/all/0/1">Pengyun Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Cong Fang</a></p>
<p>Gradient-based minimax optimal algorithms have greatly promoted the
development of continuous optimization and machine learning. One seminal work
due to Yurii Nesterov [Nes83a] established $\tilde{\mathcal{O}}(\sqrt{L/\mu})$
gradient complexity for minimizing an $L$-smooth $\mu$-strongly convex
objective. However, an ideal algorithm would adapt to the explicit complexity
of a particular objective function and incur faster rates for simpler problems,
triggering our reconsideration of two defeats of existing optimization modeling
and analysis. (i) The worst-case optimality is neither the instance optimality
nor such one in reality. (ii) Traditional $L$-smoothness condition may not be
the primary abstraction/characterization for modern practical problems.
</p>
<p>In this paper, we open up a new way to design and analyze gradient-based
algorithms with direct applications in machine learning, including linear
regression and beyond. We introduce two factors $(\alpha, \tau_{\alpha})$ to
refine the description of the degenerated condition of the optimization
problems based on the observation that the singular values of Hessian often
drop sharply. We design adaptive algorithms that solve simpler problems without
pre-known knowledge with reduced gradient or analogous oracle accesses. The
algorithms also improve the state-of-art complexities for several problems in
machine learning, thereby solving the open problem of how to design faster
algorithms in light of the known complexity lower bounds. Specially, with the
$\mathcal{O}(1)$-nuclear norm bounded, we achieve an optimal
$\tilde{\mathcal{O}}(\mu^{-1/3})$ (v.s. $\tilde{\mathcal{O}}(\mu^{-1/2})$)
gradient complexity for linear regression. We hope this work could invoke the
rethinking for understanding the difficulty of modern problems in optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03231">Deep Multimodal Fusion for Surgical Feedback Classification. (arXiv:2312.03231v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1">Rafal Kocielnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1">Elyssa Y. Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Timothy N. Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lydia Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">De-An Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiayun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_A/0/1/0/all/0/1">Andrew J. Hung</a></p>
<p>Quantification of real-time informal feedback delivered by an experienced
surgeon to a trainee during surgery is important for skill improvements in
surgical training. Such feedback in the live operating room is inherently
multimodal, consisting of verbal conversations (e.g., questions and answers) as
well as non-verbal elements (e.g., through visual cues like pointing to
anatomic elements). In this work, we leverage a clinically-validated
five-category classification of surgical feedback: "Anatomic", "Technical",
"Procedural", "Praise" and "Visual Aid". We then develop a multi-label machine
learning model to classify these five categories of surgical feedback from
inputs of text, audio, and video modalities. The ultimate goal of our work is
to help automate the annotation of real-time contextual surgical feedback at
scale. Our automated classification of surgical feedback achieves AUCs ranging
from 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show
that high-quality manual transcriptions of feedback audio from experts improve
AUCs to between 76.5 and 96.2, which demonstrates a clear path toward future
improvements. Empirically, we find that the Staged training strategy, with
first pre-training each modality separately and then training them jointly, is
more effective than training different modalities altogether. We also present
intuitive findings on the importance of modalities for different feedback
categories. This work offers an important first look at the feasibility of
automated classification of real-world live surgical feedback based on text,
audio, and video modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03236">Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets. (arXiv:2312.03236v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jiale Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1">Hiroaki Ito</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_Arias_A/0/1/0/all/0/1">&#xc1;ngel L&#xf3;pez Garc&#xed;a-Arias</a>, <a href="http://arxiv.org/find/cs/1/au:+Okoshi_Y/0/1/0/all/0/1">Yasuyuki Okoshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Otsuka_H/0/1/0/all/0/1">Hikari Otsuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawamura_K/0/1/0/all/0/1">Kazushi Kawamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Thiem Van Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Motomura_M/0/1/0/all/0/1">Masato Motomura</a></p>
<p>The Strong Lottery Ticket Hypothesis (SLTH) demonstrates the existence of
high-performing subnetworks within a randomly initialized model, discoverable
through pruning a convolutional neural network (CNN) without any weight
training. A recent study, called Untrained GNNs Tickets (UGT), expanded SLTH
from CNNs to shallow graph neural networks (GNNs). However, discrepancies
persist when comparing baseline models with learned dense weights.
Additionally, there remains an unexplored area in applying SLTH to deeper GNNs,
which, despite delivering improved accuracy with additional layers, suffer from
excessive memory requirements. To address these challenges, this work utilizes
Multicoated Supermasks (M-Sup), a scalar pruning mask method, and implements it
in GNNs by proposing a strategy for setting its pruning thresholds adaptively.
In the context of deep GNNs, this research uncovers the existence of untrained
recurrent networks, which exhibit performance on par with their trained
feed-forward counterparts. This paper also introduces the Multi-Stage Folding
and Unshared Masks methods to expand the search space in terms of both
architecture and parameters. Through the evaluation of various datasets,
including the Open Graph Benchmark (OGB), this work establishes a triple-win
scenario for SLTH-based GNNs: by achieving high sparsity, competitive
performance, and high memory efficiency with up to 98.7\% reduction, it
demonstrates suitability for energy-efficient graph processing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03243">Generalizable Neural Physics Solvers by Baldwinian Evolution. (arXiv:2312.03243v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1">Jian Cheng Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ooi_C/0/1/0/all/0/1">Chin Chun Ooi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhishek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_P/0/1/0/all/0/1">Pao-Hsiung Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Low_J/0/1/0/all/0/1">Joshua Shao Zheng Low</a>, <a href="http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1">My Ha Dao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1">Yew-Soon Ong</a></p>
<p>Physics-informed neural networks (PINNs) are at the forefront of scientific
machine learning, making possible the creation of machine intelligence that is
cognizant of physical laws and able to accurately simulate them. In this paper,
the potential of discovering PINNs that generalize over an entire family of
physics tasks is studied, for the first time, through a biological lens of the
Baldwin effect. Drawing inspiration from the neurodevelopment of precocial
species that have evolved to learn, predict and react quickly to their
environment, we envision PINNs that are pre-wired with connection strengths
inducing strong biases towards efficient learning of physics. To this end,
evolutionary selection pressure (guided by proficiency over a family of tasks)
is coupled with lifetime learning (to specialize on a smaller subset of those
tasks) to produce PINNs that demonstrate fast and physics-compliant prediction
capabilities across a range of empirically challenging problem instances. The
Baldwinian approach achieves an order of magnitude improvement in prediction
accuracy at a fraction of the computation cost compared to state-of-the-art
results with PINNs meta-learned by gradient descent. This paper marks a leap
forward in the meta-learning of PINNs as generalizable physics solvers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03248">Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning. (arXiv:2312.03248v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Cong Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a></p>
<p>Modular and composable transfer learning is an emerging direction in the
field of Parameter Efficient Fine-Tuning, as it enables neural networks to
better organize various aspects of knowledge, leading to improved cross-task
generalization. In this paper, we introduce a novel approach Customized
Polytropon C-Poly that combines task-common skills and task-specific skills,
while the skill parameters being highly parameterized using low-rank
techniques. Each task is associated with a customizable number of exclusive
specialized skills and also benefits from skills shared with peer tasks. A
skill assignment matrix is jointly learned. To evaluate our approach, we
conducted extensive experiments on the Super-NaturalInstructions and the
SuperGLUE benchmarks. Our findings demonstrate that C-Poly outperforms
fully-shared, task-specific, and skill-indistinguishable baselines,
significantly enhancing the sample efficiency in multi-task learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03253">Seller-side Outcome Fairness in Online Marketplaces. (arXiv:2312.03253v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zikun Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Maragheh_R/0/1/0/all/0/1">Reza Yousefi Maragheh</a>, <a href="http://arxiv.org/find/cs/1/au:+Morishetti_L/0/1/0/all/0/1">Lalitesh Morishetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Vashishtha_S/0/1/0/all/0/1">Shanu Vashishtha</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jason Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Nag_K/0/1/0/all/0/1">Kaushiki Nag</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sushant Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Achan_K/0/1/0/all/0/1">Kannan Achan</a></p>
<p>This paper aims to investigate and achieve seller-side fairness within online
marketplaces, where many sellers and their items are not sufficiently exposed
to customers in an e-commerce platform. This phenomenon raises concerns
regarding the potential loss of revenue associated with less exposed items as
well as less marketplace diversity. We introduce the notion of seller-side
outcome fairness and build an optimization model to balance collected
recommendation rewards and the fairness metric. We then propose a
gradient-based data-driven algorithm based on the duality and bandit theory.
Our numerical experiments on real e-commerce data sets show that our algorithm
can lift seller fairness measures while not hurting metrics like collected
Gross Merchandise Value (GMV) and total purchases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03256">CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models. (arXiv:2312.03256v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hailin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zirui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Boxuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yikai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1">Bin Cui</a></p>
<p>Recently, the growing memory demands of embedding tables in Deep Learning
Recommendation Models (DLRMs) pose great challenges for model training and
deployment. Existing embedding compression solutions cannot simultaneously meet
three key design requirements: memory efficiency, low latency, and adaptability
to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,
and Fast Embedding compression framework that addresses the above requirements.
The design philosophy of CAFE is to dynamically allocate more memory resources
to important features (called hot features), and allocate less memory to
unimportant ones. In CAFE, we propose a fast and lightweight sketch data
structure, named HotSketch, to capture feature importance and report hot
features in real time. For each reported hot feature, we assign it a unique
embedding. For the non-hot features, we allow multiple features to share one
embedding by using hash embedding technique. Guided by our design philosophy,
we further propose a multi-level hash embedding framework to optimize the
embedding tables of non-hot features. We theoretically analyze the accuracy of
HotSketch, and analyze the model convergence against deviation. Extensive
experiments show that CAFE significantly outperforms existing embedding
compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo
Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The
source codes of CAFE are available at GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03259">f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization. (arXiv:2312.03259v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baharlouei_S/0/1/0/all/0/1">Sina Baharlouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Shivam Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1">Meisam Razaviyayn</a></p>
<p>Training and deploying machine learning models that meet fairness criteria
for protected groups are fundamental in modern artificial intelligence. While
numerous constraints and regularization terms have been proposed in the
literature to promote fairness in machine learning tasks, most of these methods
are not amenable to stochastic optimization due to the complex and nonlinear
structure of constraints and regularizers. Here, the term "stochastic" refers
to the ability of the algorithm to work with small mini-batches of data.
Motivated by the limitation of existing literature, this paper presents a
unified stochastic optimization framework for fair empirical risk minimization
based on f-divergence measures (f-FERM). The proposed stochastic algorithm
enjoys theoretical convergence guarantees. In addition, our experiments
demonstrate the superiority of fairness-accuracy tradeoffs offered by f-FERM
for almost all batch sizes (ranging from full-batch to batch size of one).
Moreover, we show that our framework can be extended to the case where there is
a distribution shift from training to the test data. Our extension is based on
a distributionally robust optimization reformulation of f-FERM objective under
$L_p$ norms as uncertainty sets. Again, in this distributionally robust
setting, f-FERM not only enjoys theoretical convergence guarantees but also
outperforms other baselines in the literature in the tasks involving
distribution shifts. An efficient stochastic implementation of $f$-FERM is
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03262">Low-Cost High-Power Membership Inference by Boosting Relativity. (arXiv:2312.03262v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zarifzadeh_S/0/1/0/all/0/1">Sajjad Zarifzadeh</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_P/0/1/0/all/0/1">Philippe Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Shokri_R/0/1/0/all/0/1">Reza Shokri</a></p>
<p>We present a robust membership inference attack (RMIA) that amplifies the
distinction between population data and the training data on any target model,
by effectively leveraging both reference models and reference data in our
likelihood ratio test. Our algorithm exhibits superior test power
(true-positive rate) when compared to prior methods, even at extremely low
false-positive error rates (as low as 0). Also, under computation constraints,
where only a limited number of reference models (as few as 1) are available,
our method performs exceptionally well, unlike some prior attacks that approach
random guessing in such scenarios. Our method lays the groundwork for
cost-effective and practical yet powerful and robust privacy risk analysis of
machine learning algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03277">Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization. (arXiv:2312.03277v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jimmy Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozlov_I/0/1/0/all/0/1">Igor Kozlov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1">Gregory Dudek</a></p>
<p>The use of learning-based methods for optimizing cellular radio access
networks (RAN) has received increasing attention in recent years. This
coincides with a rapid increase in the number of cell sites worldwide, driven
largely by dramatic growth in cellular network traffic. Training and
maintaining learned models that work well across a large number of cell sites
has thus become a pertinent problem. This paper proposes a scalable framework
for constructing a reinforcement learning policy bank that can perform RAN
optimization across a large number of cell sites with varying traffic patterns.
Central to our framework is a novel application of anomaly detection techniques
to assess the compatibility between sites (tasks) and the policy bank. This
allows our framework to intelligently identify when a policy can be reused for
a task, and when a new policy needs to be trained and added to the policy bank.
Our results show that our approach to compatibility assessment leads to an
efficient use of computational resources, by allowing us to construct a
performant policy bank without exhaustively training on all tasks, which makes
it applicable under real-world constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03288">STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition. (arXiv:2312.03288v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_N/0/1/0/all/0/1">Nguyen Huu Bao Long</a></p>
<p>Graph convolutional networks (GCNs) have been widely used and achieved
remarkable results in skeleton-based action recognition. We think the key to
skeleton-based action recognition is a skeleton hanging in frames, so we focus
on how the Graph Convolutional Convolution networks learn different topologies
and effectively aggregate joint features in the global temporal and local
temporal. In this work, we propose three Channel-wise Tolopogy Graph
Convolution based on Channel-wise Topology Refinement Graph Convolution
(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture
the upper-lower body part and hand-foot relationship skeleton features. After
that, to capture features of human skeletons changing in frames we design the
Temporal Attention Transformers to extract skeletons effectively. The Temporal
Attention Transformers can learn the temporal features of human skeleton
sequences. Finally, we fuse the temporal features output scale with MLP and
classification. We develop a powerful graph convolutional network named Spatial
Temporal Effective Body-part Cross Attention Transformer which notably
high-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models
are available at https://github.com/maclong01/STEP-CATFormer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03291">OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution. (arXiv:2312.03291v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weitang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Ying Wai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianle Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yi-Zhuang You</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a></p>
<p>We propose a novel model-centric evaluation framework, OmniInput, to evaluate
the quality of an AI/ML model's predictions on all possible inputs (including
human-unrecognizable ones), which is crucial for AI safety and reliability.
Unlike traditional data-centric evaluation based on pre-defined test sets, the
test set in OmniInput is self-constructed by the model itself and the model
quality is evaluated by investigating its output distribution. We employ an
efficient sampler to obtain representative inputs and the output distribution
of the trained model, which, after selective annotation, can be used to
estimate the model's precision and recall at different output values and a
comprehensive precision-recall curve. Our experiments demonstrate that
OmniInput enables a more fine-grained comparison between models, especially
when their performance is almost the same on pre-defined datasets, leading to
new findings and insights for how to train more robust, generalizable models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03292">Enhancing Molecular Property Prediction via Mixture of Collaborative Experts. (arXiv:2312.03292v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1">Xu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Shuang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Songqiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hailiang Huang</a></p>
<p>Molecular Property Prediction (MPP) task involves predicting biochemical
properties based on molecular features, such as molecular graph structures,
contributing to the discovery of lead compounds in drug development. To address
data scarcity and imbalance in MPP, some studies have adopted Graph Neural
Networks (GNN) as an encoder to extract commonalities from molecular graphs.
However, these approaches often use a separate predictor for each task,
neglecting the shared characteristics among predictors corresponding to
different tasks. In response to this limitation, we introduce the GNN-MoCE
architecture. It employs the Mixture of Collaborative Experts (MoCE) as
predictors, exploiting task commonalities while confronting the homogeneity
issue in the expert pool and the decision dominance dilemma within the expert
group. To enhance expert diversity for collaboration among all experts, the
Expert-Specific Projection method is proposed to assign a unique projection
perspective to each expert. To balance decision-making influence for
collaboration within the expert group, the Expert-Specific Loss is presented to
integrate individual expert loss into the weighted decision loss of the group
for more equitable training. Benefiting from the enhancements of MoCE in expert
creation, dynamic expert group formation, and experts' collaboration, our model
demonstrates superior performance over traditional methods on 24 MPP datasets,
especially in tasks with limited data or high imbalance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03296">Cooperative Probabilistic Trajectory Forecasting under Occlusion. (arXiv:2312.03296v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Anshul Nayak</a>, <a href="http://arxiv.org/find/cs/1/au:+Eskandarian_A/0/1/0/all/0/1">Azim Eskandarian</a></p>
<p>Perception and planning under occlusion is essential for safety-critical
tasks. Occlusion-aware planning often requires communicating the information of
the occluded object to the ego agent for safe navigation. However,
communicating rich sensor information under adverse conditions during
communication loss and limited bandwidth may not be always feasible. Further,
in GPS denied environments and indoor navigation, localizing and sharing of
occluded objects can be challenging. To overcome this, relative pose estimation
between connected agents sharing a common field of view can be a
computationally effective way of communicating information about surrounding
objects. In this paper, we design an end-to-end network that cooperatively
estimates the current states of occluded pedestrian in the reference frame of
ego agent and then predicts the trajectory with safety guarantees.
Experimentally, we show that the uncertainty-aware trajectory prediction of
occluded pedestrian by the ego agent is almost similar to the ground truth
trajectory assuming no occlusion. The current research holds promise for
uncertainty-aware navigation among multiple connected agents under occlusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03303">Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique. (arXiv:2312.03303v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagin_I/0/1/0/all/0/1">Ilya Tyagin</a>, <a href="http://arxiv.org/find/cs/1/au:+Safro_I/0/1/0/all/0/1">Ilya Safro</a></p>
<p>This paper presents a novel benchmarking framework Dyport for evaluating
biomedical hypothesis generation systems. Utilizing curated datasets, our
approach tests these systems under realistic conditions, enhancing the
relevance of our evaluations. We integrate knowledge from the curated databases
into a dynamic graph, accompanied by a method to quantify discovery importance.
This not only assesses hypothesis accuracy but also their potential impact in
biomedical research which significantly extends traditional link prediction
benchmarks. Applicability of our benchmarking process is demonstrated on
several link prediction systems applied on biomedical semantic knowledge
graphs. Being flexible, our benchmarking system is designed for broad
application in hypothesis generation quality verification, aiming to expand the
scope of scientific discovery within the biomedical research community.
Availability and implementation: Dyport framework is fully open-source. All
code and datasets are available at: https://github.com/IlyaTyagin/Dyport
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03307">Balanced Marginal and Joint Distributional Learning via Mixture Cramer-Wold Distance. (arXiv:2312.03307v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+An_S/0/1/0/all/0/1">Seunghwan An</a>, <a href="http://arxiv.org/find/stat/1/au:+Hong_S/0/1/0/all/0/1">Sungchul Hong</a>, <a href="http://arxiv.org/find/stat/1/au:+Jeon_J/0/1/0/all/0/1">Jong-June Jeon</a></p>
<p>In the process of training a generative model, it becomes essential to
measure the discrepancy between two high-dimensional probability distributions:
the generative distribution and the ground-truth distribution of the observed
dataset. Recently, there has been growing interest in an approach that involves
slicing high-dimensional distributions, with the Cramer-Wold distance emerging
as a promising method. However, we have identified that the Cramer-Wold
distance primarily focuses on joint distributional learning, whereas
understanding marginal distributional patterns is crucial for effective
synthetic data generation. In this paper, we introduce a novel measure of
dissimilarity, the mixture Cramer-Wold distance. This measure enables us to
capture both marginal and joint distributional information simultaneously, as
it incorporates a mixture measure with point masses on standard basis vectors.
Building upon the mixture Cramer-Wold distance, we propose a new generative
model called CWDAE (Cramer-Wold Distributional AutoEncoder), which shows
remarkable performance in generating synthetic data when applied to real
tabular datasets. Furthermore, our model offers the flexibility to adjust the
level of data privacy with ease.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03309">Benchmarking Continual Learning from Cognitive Perspectives. (arXiv:2312.03309v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoqian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1">Peipei Yang</a></p>
<p>Continual learning addresses the problem of continuously acquiring and
transferring knowledge without catastrophic forgetting of old concepts. While
humans achieve continual learning via diverse neurocognitive mechanisms, there
is a mismatch between cognitive properties and evaluation methods of continual
learning models. First, the measurement of continual learning models mostly
relies on evaluation metrics at a micro-level, which cannot characterize
cognitive capacities of the model. Second, the measurement is method-specific,
emphasizing model strengths in one aspect while obscuring potential weaknesses
in other respects. To address these issues, we propose to integrate model
cognitive capacities and evaluation metrics into a unified evaluation paradigm.
We first characterize model capacities via desiderata derived from cognitive
properties supporting human continual learning. The desiderata concern (1)
adaptability in varying lengths of task sequence; (2) sensitivity to dynamic
task variations; and (3) efficiency in memory usage and training time
consumption. Then we design evaluation protocols for each desideratum to assess
cognitive capacities of recent continual learning models. Experimental results
show that no method we consider has satisfied all the desiderata and is still
far away from realizing truly continual learning. Although some methods exhibit
some degree of adaptability and efficiency, no method is able to identify task
relationships when encountering dynamic task variations, or achieve a trade-off
in learning similarities and differences between tasks. Inspired by these
results, we discuss possible factors that influence model performance in these
desiderata and provide guidance for the improvement of continual learning
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03311">On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Abedsoltan_A/0/1/0/all/0/1">Amirhesam Abedsoltan</a>, <a href="http://arxiv.org/find/stat/1/au:+Belkin_M/0/1/0/all/0/1">Mikhail Belkin</a>, <a href="http://arxiv.org/find/stat/1/au:+Pandit_P/0/1/0/all/0/1">Parthe Pandit</a>, <a href="http://arxiv.org/find/stat/1/au:+Rademacher_L/0/1/0/all/0/1">Luis Rademacher</a></p>
<p>Kernel methods are a popular class of nonlinear predictive models in machine
learning. Scalable algorithms for learning kernel models need to be iterative
in nature, but convergence can be slow due to poor conditioning. Spectral
preconditioning is an important tool to speed-up the convergence of such
iterative algorithms for training kernel models. However computing and storing
a spectral preconditioner can be expensive which can lead to large
computational and storage overheads, precluding the application of kernel
methods to problems with large datasets. A Nystrom approximation of the
spectral preconditioner is often cheaper to compute and store, and has
demonstrated success in practical applications. In this paper we analyze the
trade-offs of using such an approximated preconditioner. Specifically, we show
that a sample of logarithmic size (as a function of the size of the dataset)
enables the Nystrom-based approximated preconditioner to accelerate gradient
descent nearly as well as the exact preconditioner, while also reducing the
computational and storage overheads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03318">Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift. (arXiv:2312.03318v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Saurabh Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1">Amrith Setlur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary Chase Lipton</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1">Sivaraman Balakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1">Virginia Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1">Aditi Raghunathan</a></p>
<p>Self-training and contrastive learning have emerged as leading techniques for
incorporating unlabeled data, both under distribution shift (unsupervised
domain adaptation) and when it is absent (semi-supervised learning). However,
despite the popularity and compatibility of these techniques, their efficacy in
combination remains unexplored. In this paper, we undertake a systematic
empirical investigation of this combination, finding that (i) in domain
adaptation settings, self-training and contrastive learning offer significant
complementary gains; and (ii) in semi-supervised learning settings,
surprisingly, the benefits are not synergistic. Across eight distribution shift
datasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains
3--8% higher accuracy than either approach independently. We then theoretically
analyze these techniques in a simplified model of distribution shift,
demonstrating scenarios under which the features produced by contrastive
learning can yield a good initialization for self-training to further amplify
gains and achieve optimal performance, even when either method alone would
fail.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03325">GCFA:Geodesic Curve Feature Augmentation via Shape Space Theory. (arXiv:2312.03325v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuexing Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1">Guanxin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bing Wang</a></p>
<p>Deep learning has yielded remarkable outcomes in various domains. However,
the challenge of requiring large-scale labeled samples still persists in deep
learning. Thus, data augmentation has been introduced as a critical strategy to
train deep learning models. However, data augmentation suffers from information
loss and poor performance in small sample environments. To overcome these
drawbacks, we propose a feature augmentation method based on shape space
theory, i.e., Geodesic curve feature augmentation, called GCFA in brevity.
First, we extract features from the image with the neural network model. Then,
the multiple image features are projected into a pre-shape space as features.
In the pre-shape space, a Geodesic curve is built to fit the features. Finally,
the many generated features on the Geodesic curve are used to train the various
machine learning models. The GCFA module can be seamlessly integrated with most
machine learning methods. And the proposed method is simple, effective and
insensitive for the small sample datasets. Several examples demonstrate that
the GCFA method can greatly improve the performance of the data preprocessing
model in a small sample environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03328">Deep Learning for Koopman-based Dynamic Movement Primitives. (arXiv:2312.03328v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tyler Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Henshaw_C/0/1/0/all/0/1">Carl Glen Henshaw</a></p>
<p>The challenge of teaching robots to perform dexterous manipulation, dynamic
locomotion, or whole--body manipulation from a small number of demonstrations
is an important research field that has attracted interest from across the
robotics community. In this work, we propose a novel approach by joining the
theories of Koopman Operators and Dynamic Movement Primitives to Learning from
Demonstration. Our approach, named \gls{admd}, projects nonlinear dynamical
systems into linear latent spaces such that a solution reproduces the desired
complex motion. Use of an autoencoder in our approach enables generalizability
and scalability, while the constraint to a linear system attains
interpretability. Our results are comparable to the Extended Dynamic Mode
Decomposition on the LASA Handwriting dataset but with training on only a small
fractions of the letters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03330">Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities. (arXiv:2312.03330v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Snoswell_A/0/1/0/all/0/1">Aaron J. Snoswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Nelson_L/0/1/0/all/0/1">Lucinda Nelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Hao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1">Flora D. Salim</a>, <a href="http://arxiv.org/find/cs/1/au:+Suzor_N/0/1/0/all/0/1">Nicolas Suzor</a>, <a href="http://arxiv.org/find/cs/1/au:+Burgess_J/0/1/0/all/0/1">Jean Burgess</a></p>
<p>Generic `toxicity' classifiers continue to be used for evaluating the
potential for harm in natural language generation, despite mounting evidence of
their shortcomings. We consider the challenge of measuring misogyny in natural
language generation, and argue that generic `toxicity' classifiers are
inadequate for this task. We use data from two well-characterised `Incel'
communities on Reddit that differ primarily in their degrees of misogyny to
construct a pair of training corpora which we use to fine-tune two language
models. We show that an open source `toxicity' classifier is unable to
distinguish meaningfully between generations from these models. We contrast
this with a misogyny-specific lexicon recently proposed by feminist
subject-matter experts, demonstrating that, despite the limitations of simple
lexicon-based approaches, this shows promise as a benchmark to evaluate
language models for misogyny, and that it is sensitive enough to reveal the
known differences in these Reddit communities. Our preliminary findings
highlight the limitations of a generic approach to evaluating harms, and
further emphasise the need for careful benchmark design and selection in
natural language evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03344">Interpretable Mechanistic Representations for Meal-level Glycemic Control in the Wild. (arXiv:2312.03344v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Alexander Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1">Emily B. Fox</a></p>
<p>Diabetes encompasses a complex landscape of glycemic control that varies
widely among individuals. However, current methods do not faithfully capture
this variability at the meal level. On the one hand, expert-crafted features
lack the flexibility of data-driven methods; on the other hand, learned
representations tend to be uninterpretable which hampers clinical adoption. In
this paper, we propose a hybrid variational autoencoder to learn interpretable
representations of CGM and meal data. Our method grounds the latent space to
the inputs of a mechanistic differential equation, producing embeddings that
reflect physiological quantities, such as insulin sensitivity, glucose
effectiveness, and basal glucose levels. Moreover, we introduce a novel method
to infer the glucose appearance rate, making the mechanistic model robust to
unreliable meal logs. On a dataset of CGM and self-reported meals from
individuals with type-2 diabetes and pre-diabetes, our unsupervised
representation discovers a separation between individuals proportional to their
disease severity. Our embeddings produce clusters that are up to 4x better than
naive, expert, black-box, and pure mechanistic features. Our method provides a
nuanced, yet interpretable, embedding space to compare glycemic control within
and across individuals, directly learnable from in-the-wild data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03351">On the variants of SVM methods applied to GPR data to classify tack coat characteristics in French pavements: two experimental case studies. (arXiv:2312.03351v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Andreoli_G/0/1/0/all/0/1">Gr&#xe9;gory Andreoli</a> (MAST-EMGCU), <a href="http://arxiv.org/find/stat/1/au:+Ihamouten_A/0/1/0/all/0/1">Amine Ihamouten</a> (MAST-LAMES), <a href="http://arxiv.org/find/stat/1/au:+Nguyen_M/0/1/0/all/0/1">Mai Lan Nguyen</a> (MAST-LAMES), <a href="http://arxiv.org/find/stat/1/au:+Fargier_Y/0/1/0/all/0/1">Yannick Fargier</a> (GERS-RRO), <a href="http://arxiv.org/find/stat/1/au:+Fauchard_C/0/1/0/all/0/1">Cyrille Fauchard</a> (ENDSUM), <a href="http://arxiv.org/find/stat/1/au:+Simonin_J/0/1/0/all/0/1">Jean-Michel Simonin</a> (MAST-LAMES), <a href="http://arxiv.org/find/stat/1/au:+Buliuk_V/0/1/0/all/0/1">Viktoriia Buliuk</a> (GERS-GeoEND), <a href="http://arxiv.org/find/stat/1/au:+Souriou_D/0/1/0/all/0/1">David Souriou</a> (FI-NDT), <a href="http://arxiv.org/find/stat/1/au:+Derobert_X/0/1/0/all/0/1">Xavier D&#xe9;robert</a> (GERS-GeoEND)</p>
<p>Among the commonly used non-destructive techniques, the Ground Penetrating
Radar (GPR) is one of the most widely adopted today for assessing pavement
conditions in France. However, conventional radar systems and their forward
processing methods have shown their limitations for the physical and
geometrical characterization of very thin layers such as tack coats. However,
the use of Machine Learning methods applied to GPR with an inverse approach
showed that it was numerically possible to identify the tack coat
characteristics despite masking effects due to low timefrequency resolution
noted in the raw B-scans. Thus, we propose in this paper to apply the inverse
approach based on Machine Learning, already validated in previous works on
numerical data, on two experimental cases with different pavement structures.
The first case corresponds to a validation on known pavement structures on the
Gustave Eiffel University (Nantes, France) with its pavement fatigue carousel
and the second case focuses on a new real road in Vend{\'e}e department
(France). In both case studies, the performances of SVM/SVR methods showed the
efficiency of supervised learning methods to classify and estimate the emulsion
proportioning in the tack coats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03360">Teaching Specific Scientific Knowledge into Large Language Models through Additional Training. (arXiv:2312.03360v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hatakeyama_Sato_K/0/1/0/all/0/1">Kan Hatakeyama-Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Igarashi_Y/0/1/0/all/0/1">Yasuhiko Igarashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Katakami_S/0/1/0/all/0/1">Shun Katakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabae_Y/0/1/0/all/0/1">Yuta Nabae</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayakawa_T/0/1/0/all/0/1">Teruaki Hayakawa</a></p>
<p>Through additional training, we explore embedding specialized scientific
knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that
effective knowledge integration requires reading texts from multiple
perspectives, especially in instructional formats. We utilize text augmentation
to tackle the scarcity of specialized texts, including style conversions and
translations. Hyperparameter optimization proves crucial, with different size
models (7b, 13b, and 70b) reasonably undergoing additional training. Validating
our methods, we construct a dataset of 65,000 scientific papers. Although we
have succeeded in partially embedding knowledge, the study highlights the
complexities and limitations of incorporating specialized information into
LLMs, suggesting areas for further improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03386">An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network. (arXiv:2312.03386v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taeyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongseok Yang</a></p>
<p>The recent theoretical analysis of deep neural networks in their
infinite-width limits has deepened our understanding of initialisation, feature
learning, and training of those networks, and brought new practical techniques
for finding appropriate hyperparameters, learning network weights, and
performing inference. In this paper, we broaden this line of research by
showing that this infinite-width analysis can be extended to the Jacobian of a
deep neural network. We show that a multilayer perceptron (MLP) and its
Jacobian at initialisation jointly converge to a Gaussian process (GP) as the
widths of the MLP's hidden layers go to infinity and characterise this GP. We
also prove that in the infinite-width limit, the evolution of the MLP under the
so-called robust training (i.e., training with a regulariser on the Jacobian)
is described by a linear first-order ordinary differential equation that is
determined by a variant of the Neural Tangent Kernel. We experimentally show
the relevance of our theoretical claims to wide finite networks, and
empirically analyse the properties of kernel regression solution to obtain an
insight into Jacobian regularisation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03395">Diffused Task-Agnostic Milestone Planner. (arXiv:2312.03395v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1">Mineui Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Minjae Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Songhwai Oh</a></p>
<p>Addressing decision-making problems using sequence modeling to predict future
trajectories shows promising results in recent years. In this paper, we take a
step further to leverage the sequence predictive method in wider areas such as
long-term planning, vision-based control, and multi-task decision-making. To
this end, we propose a method to utilize a diffusion-based generative sequence
model to plan a series of milestones in a latent space and to have an agent to
follow the milestones to accomplish a given task. The proposed method can learn
control-relevant, low-dimensional latent representations of milestones, which
makes it possible to efficiently perform long-term planning and vision-based
control. Furthermore, our approach exploits generation flexibility of the
diffusion model, which makes it possible to plan diverse trajectories for
multi-task decision-making. We demonstrate the proposed method across offline
reinforcement learning (RL) benchmarks and an visual manipulation environment.
The results show that our approach outperforms offline RL methods in solving
long-horizon, sparse-reward tasks and multi-task problems, while also achieving
the state-of-the-art performance on the most challenging vision-based
manipulation benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03397">Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning. (arXiv:2312.03397v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sangwoong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1">Dohyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1">Himchan Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Noh_Y/0/1/0/all/0/1">Yung-Kyun Noh</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1">Frank C. Park</a></p>
<p>We present Generalized Contrastive Divergence (GCD), a novel objective
function for training an energy-based model (EBM) and a sampler simultaneously.
GCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm
for training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution
with a trainable sampler, such as a diffusion model. In GCD, the joint training
of EBM and a diffusion model is formulated as a minimax problem, which reaches
an equilibrium when both models converge to the data distribution. The minimax
learning with GCD bears interesting equivalence to inverse reinforcement
learning, where the energy corresponds to a negative reward, the diffusion
model is a policy, and the real data is expert demonstrations. We present
preliminary yet promising results showing that joint training is beneficial for
both EBM and a diffusion model. GCD enables EBM training without MCMC while
improving the sample quality of a diffusion model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03404">An AI for Scientific Discovery Route between Amorphous Networks and Mechanical Behavior. (arXiv:2312.03404v1 [cond-mat.soft])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Zhu_C/0/1/0/all/0/1">Changliang Zhu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fang_C/0/1/0/all/0/1">Chenchao Fang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Jin_Z/0/1/0/all/0/1">Zhipeng Jin</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Li_B/0/1/0/all/0/1">Baowen Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Shen_X/0/1/0/all/0/1">Xiangying Shen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Xu_L/0/1/0/all/0/1">Lei Xu</a></p>
<p>"AI for science" is widely recognized as a future trend in the development of
scientific research. Currently, although machine learning algorithms have
played a crucial role in scientific research with numerous successful cases,
relatively few instances exist where AI assists researchers in uncovering the
underlying physical mechanisms behind a certain phenomenon and subsequently
using that mechanism to improve machine learning algorithms' efficiency. This
article uses the investigation into the relationship between extreme Poisson's
ratio values and the structure of amorphous networks as a case study to
illustrate how machine learning methods can assist in revealing underlying
physical mechanisms. Upon recognizing that the Poisson's ratio relies on the
low-frequency vibrational modes of dynamical matrix, we can then employ a
convolutional neural network, trained on the dynamical matrix instead of
traditional image recognition, to predict the Poisson's ratio of amorphous
networks with a much higher efficiency. Through this example, we aim to
showcase the role that artificial intelligence can play in revealing
fundamental physical mechanisms, which subsequently improves the machine
learning algorithms significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03406">SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting. (arXiv:2312.03406v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yanjun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Liang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Rong Jin</a></p>
<p>Spatiotemporal forecasting tasks, such as weather forecasting and traffic
prediction, offer significant societal benefits. These tasks can be effectively
approached as image forecasting problems using computer vision models. Vector
quantization (VQ) is a well-known method for discrete representation that
improves the latent space, leading to enhanced generalization and transfer
learning capabilities. One of the main challenges in using VQ for
spatiotemporal forecasting is how to balance between keeping enough details and
removing noises from the original patterns for better generalization. We
address this challenge by developing sparse vector quantization, or {\bf SVQ}
for short, that leverages sparse regression to make better trade-off between
the two objectives. The main innovation of this work is to approximate sparse
regression by a two-layer MLP and a randomly fixed or learnable matrix,
dramatically improving its computational efficiency. Through experiments
conducted on diverse datasets in multiple fields including weather forecasting,
traffic flow prediction, and video forecasting, we unequivocally demonstrate
that our proposed method consistently enhances the performance of base models
and achieves state-of-the-art results across all benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03413">Approximating Solutions to the Knapsack Problem using the Lagrangian Dual Framework. (arXiv:2312.03413v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keegan_M/0/1/0/all/0/1">Mitchell Keegan</a>, <a href="http://arxiv.org/find/cs/1/au:+Abolghasemi_M/0/1/0/all/0/1">Mahdi Abolghasemi</a></p>
<p>The Knapsack Problem is a classic problem in combinatorial optimisation.
Solving these problems may be computationally expensive. Recent years have seen
a growing interest in the use of deep learning methods to approximate the
solutions to such problems. A core problem is how to enforce or encourage
constraint satisfaction in predicted solutions. A promising approach for
predicting solutions to constrained optimisation problems is the Lagrangian
Dual Framework which builds on the method of Lagrangian Relaxation. In this
paper we develop neural network models to approximate Knapsack Problem
solutions using the Lagrangian Dual Framework while improving constraint
satisfaction. We explore the problems of output interpretation and model
selection within this context. Experimental results show strong constraint
satisfaction with a minor reduction of optimality as compared to a baseline
neural network which does not explicitly model the constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03414">Compressed Context Memory For Online Language Model Interaction. (arXiv:2312.03414v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jang-Hyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1">Junyoung Yeom</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Sangdoo Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hyun Oh Song</a></p>
<p>This paper presents a novel context compression method for Transformer
language models in online scenarios such as ChatGPT, where the context
continually expands. As the context lengthens, the attention process requires
more memory and computational resources, which in turn reduces the throughput
of the language model. To this end, we propose a compressed context memory
system that continually compresses the growing context into a compact memory
space. The compression process simply involves integrating a lightweight
conditional LoRA into the language model's forward pass during inference. Based
on the compressed context memory, the language model can perform inference with
reduced memory and attention operations. Through evaluations on conversation,
personalization, and multi-task learning, we demonstrate that our approach
achieves the performance level of a full context model with $5\times$ smaller
context memory space. Codes are available at
https://github.com/snu-mllab/context-memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03415">Run LoRA Run: Faster and Lighter LoRA Implementations. (arXiv:2312.03415v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cherniuk_D/0/1/0/all/0/1">Daria Cherniuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikhalev_A/0/1/0/all/0/1">Aleksandr Mikhalev</a>, <a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1">Ivan Oseledets</a></p>
<p>LoRA is a technique that reduces the number of trainable parameters in a
neural network by introducing low-rank adapters to linear layers. This
technique is used both for fine-tuning (LoRA, QLoRA) and full train (ReLoRA).
This paper presents the RunLoRA framework for efficient implementations of LoRA
that significantly improves the speed of neural network training and
fine-tuning using low-rank adapters. The proposed implementation optimizes the
computation of LoRA operations based on dimensions of corresponding linear
layer, layer input dimensions and lora rank by choosing best forward and
backward computation graph based on FLOPs and time estimations, resulting in
faster training without sacrificing accuracy. The experimental results show up
to 17% speedup on Llama family of models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03455">Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data. (arXiv:2312.03455v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Namgyal_T/0/1/0/all/0/1">Tashi Namgyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1">Alexander Hepburn</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1">Raul Santos-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1">Valero Laparra</a>, <a href="http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1">Jesus Malo</a></p>
<p>Perceptual metrics are traditionally used to evaluate the quality of natural
signals, such as images and audio. They are designed to mimic the perceptual
behaviour of human observers and usually reflect structures found in natural
signals. This motivates their use as loss functions for training generative
models such that models will learn to capture the structure held in the metric.
We take this idea to the extreme in the audio domain by training a compressive
autoencoder to reconstruct uniform noise, in lieu of natural data. We show that
training with perceptual losses improves the reconstruction of spectrograms and
re-synthesized audio at test time over models trained with a standard Euclidean
loss. This demonstrates better generalisation to unseen natural signals when
using perceptual metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03464">Subnetwork-to-go: Elastic Neural Network with Dynamic Training and Customizable Inference. (arXiv:2312.03464v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yi Luo</a></p>
<p>Deploying neural networks to different devices or platforms is in general
challenging, especially when the model size is large or model complexity is
high. Although there exist ways for model pruning or distillation, it is
typically required to perform a full round of model training or finetuning
procedure in order to obtain a smaller model that satisfies the model size or
complexity constraints. Motivated by recent works on dynamic neural networks,
we propose a simple way to train a large network and flexibly extract a
subnetwork from it given a model size or complexity constraint during
inference. We introduce a new way to allow a large model to be trained with
dynamic depth and width during the training phase, and after the large model is
trained we can select a subnetwork from it with arbitrary depth and width
during the inference phase with a relatively better performance compared to
training the subnetwork independently from scratch. Experiment results on a
music source separation model show that our proposed method can effectively
improve the separation performance across different subnetwork sizes and
complexities with a single large model, and training the large model takes
significantly shorter time than training all the different subnetworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03466">Search Strategies for Self-driving Laboratories with Pending Experiments. (arXiv:2312.03466v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeitler_J/0/1/0/all/0/1">Jakob Zeitler</a>, <a href="http://arxiv.org/find/cs/1/au:+Rupnow_C/0/1/0/all/0/1">Connor Rupnow</a></p>
<p>Self-driving laboratories (SDLs) consist of multiple stations that perform
material synthesis and characterisation tasks. To minimize station downtime and
maximize experimental throughput, it is practical to run experiments in
asynchronous parallel, in which multiple experiments are being performed at
once in different stages. Asynchronous parallelization of experiments, however,
introduces delayed feedback (i.e. "pending experiments"), which is known to
reduce Bayesian optimiser performance. Here, we build a simulator for a
multi-stage SDL and compare optimisation strategies for dealing with delayed
feedback and asynchronous parallelized operation. Using data from a real SDL,
we build a ground truth Bayesian optimisation simulator from 177 previously run
experiments for maximizing the conductivity of functional coatings. We then
compare search strategies such as expected improvement, noisy expected
improvement, 4-mode exploration and random sampling. We evaluate their
performance in terms of amount of delay and problem dimensionality. Our
simulation results showcase the trade-off between the asynchronous parallel
operation and delayed feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03475">Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion. (arXiv:2312.03475v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weitao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiujiu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuecang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengchao Liu</a></p>
<p>Recently, artificial intelligence for drug discovery has raised increasing
interest in both machine learning and chemistry domains. The fundamental
building block for drug discovery is molecule geometry and thus, the molecule's
geometrical representation is the main bottleneck to better utilize machine
learning techniques for drug discovery. In this work, we propose a pretraining
method for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn
both the 2D bond (topology) and 3D conformation (geometry) information, and a
diffusion process model is applied to mimic the augmented trajectories of such
two modalities, based on which, MoleculeJAE will learn the inherent chemical
structure in a self-supervised manner. Thus, the pretrained geometrical
representation in MoleculeJAE is expected to benefit downstream
geometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by
reaching state-of-the-art performance on 15 out of 20 tasks by comparing it
with 12 competitive baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03477">From Detection to Action Recognition: An Edge-Based Pipeline for Robot Human Perception. (arXiv:2312.03477v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Toupas_P/0/1/0/all/0/1">Petros Toupas</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsamis_G/0/1/0/all/0/1">Georgios Tsamis</a>, <a href="http://arxiv.org/find/cs/1/au:+Giakoumis_D/0/1/0/all/0/1">Dimitrios Giakoumis</a>, <a href="http://arxiv.org/find/cs/1/au:+Votis_K/0/1/0/all/0/1">Konstantinos Votis</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzovaras_D/0/1/0/all/0/1">Dimitrios Tzovaras</a></p>
<p>Mobile service robots are proving to be increasingly effective in a range of
applications, such as healthcare, monitoring Activities of Daily Living (ADL),
and facilitating Ambient Assisted Living (AAL). These robots heavily rely on
Human Action Recognition (HAR) to interpret human actions and intentions.
However, for HAR to function effectively on service robots, it requires prior
knowledge of human presence (human detection) and identification of individuals
to monitor (human tracking). In this work, we propose an end-to-end pipeline
that encompasses the entire process, starting from human detection and
tracking, leading to action recognition. The pipeline is designed to operate in
near real-time while ensuring all stages of processing are performed on the
edge, reducing the need for centralised computation. To identify the most
suitable models for our mobile robot, we conducted a series of experiments
comparing state-of-the-art solutions based on both their detection performance
and efficiency. To evaluate the effectiveness of our proposed pipeline, we
proposed a dataset comprising daily household activities. By presenting our
findings and analysing the results, we demonstrate the efficacy of our approach
in enabling mobile robots to understand and respond to human behaviour in
real-world scenarios relying mainly on the data from their RGB cameras.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03483">Exploring Answer Information Methods for Question Generation with Transformers. (arXiv:2312.03483v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chafekar_T/0/1/0/all/0/1">Talha Chafekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1">Aafiya Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1">Grishma Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1">Deepak Sharma</a></p>
<p>There has been a lot of work in question generation where different methods
to provide target answers as input, have been employed. This experimentation
has been mostly carried out for RNN based models. We use three different
methods and their combinations for incorporating answer information and explore
their effect on several automatic evaluation metrics. The methods that are used
are answer prompting, using a custom product method using answer embeddings and
encoder outputs, choosing sentences from the input paragraph that have answer
related information, and using a separate cross-attention attention block in
the decoder which attends to the answer. We observe that answer prompting
without any additional modes obtains the best scores across rouge, meteor
scores. Additionally, we use a custom metric to calculate how many of the
generated questions have the same answer, as the answer which is used to
generate them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03485">Precision of Individual Shapley Value Explanations. (arXiv:2312.03485v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Olsen_L/0/1/0/all/0/1">Lars Henry Berge Olsen</a></p>
<p>Shapley values are extensively used in explainable artificial intelligence
(XAI) as a framework to explain predictions made by complex machine learning
(ML) models. In this work, we focus on conditional Shapley values for
predictive models fitted to tabular data and explain the prediction
$f(\boldsymbol{x}^{*})$ for a single observation $\boldsymbol{x}^{*}$ at the
time. Numerous Shapley value estimation methods have been proposed and
empirically compared on an average basis in the XAI literature. However, less
focus has been devoted to analyzing the precision of the Shapley value
explanations on an individual basis. We extend our work in Olsen et al. (2023)
by demonstrating and discussing that the explanations are systematically less
precise for observations on the outer region of the training data distribution
for all used estimation methods. This is expected from a statistical point of
view, but to the best of our knowledge, it has not been systematically
addressed in the Shapley value literature. This is crucial knowledge for
Shapley values practitioners, who should be more careful in applying these
observations' corresponding Shapley value explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03491">Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis. (arXiv:2312.03491v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zehua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1">Guande He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kaiwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>In text-to-speech (TTS) synthesis, diffusion models have achieved promising
generation quality. However, because of the pre-defined data-to-noise diffusion
process, their prior distribution is restricted to a noisy representation,
which provides little information of the generation target. In this work, we
present a novel TTS system, Bridge-TTS, making the first attempt to substitute
the noisy Gaussian prior in established diffusion-based TTS methods with a
clean and deterministic one, which provides strong structural information of
the target. Specifically, we leverage the latent representation obtained from
text input as our prior, and build a fully tractable Schrodinger bridge between
it and the ground-truth mel-spectrogram, leading to a data-to-data process.
Moreover, the tractability and flexibility of our formulation allow us to
empirically study the design spaces such as noise schedules, as well as to
develop stochastic and deterministic samplers. Experimental results on the
LJ-Speech dataset illustrate the effectiveness of our method in terms of both
synthesis quality and sampling efficiency, significantly outperforming our
diffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast
TTS models in few-step scenarios. Project page: https://bridge-tts.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03492">Learning From Scenarios for Stochastic Repairable Scheduling. (arXiv:2312.03492v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Houten_K/0/1/0/all/0/1">Kim van den Houten</a>, <a href="http://arxiv.org/find/cs/1/au:+Tax_D/0/1/0/all/0/1">David M.J. Tax</a>, <a href="http://arxiv.org/find/cs/1/au:+Freydell_E/0/1/0/all/0/1">Esteban Freydell</a>, <a href="http://arxiv.org/find/cs/1/au:+Weerdt_M/0/1/0/all/0/1">Mathijs de Weerdt</a></p>
<p>When optimizing problems with uncertain parameter values in a linear
objective, decision-focused learning enables end-to-end learning of these
values. We are interested in a stochastic scheduling problem, in which
processing times are uncertain, which brings uncertain values in the
constraints, and thus repair of an initial schedule may be needed. Historical
realizations of the stochastic processing times are available. We show how
existing decision-focused learning techniques based on stochastic smoothing can
be adapted to this scheduling problem. We include an extensive experimental
evaluation to investigate in which situations decision-focused learning
outperforms the state of the art for such situations: scenario-based stochastic
optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03497">Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research. (arXiv:2312.03497v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Takagi_S/0/1/0/all/0/1">Shiro Takagi</a></p>
<p>This paper engages in a speculative exploration of the concept of an
artificial agent capable of conducting research. Initially, it examines how the
act of research can be conceptually characterized, aiming to provide a starting
point for discussions about what it means to create such agents. The focus then
shifts to the core components of research: question formulation, hypothesis
generation, and hypothesis verification. This discussion includes a
consideration of the potential and challenges associated with enabling machines
to autonomously perform these tasks. Subsequently, this paper briefly considers
the overlapping themes and interconnections that underlie them. Finally, the
paper presents preliminary thoughts on prototyping as an initial step towards
uncovering the challenges involved in developing these research-capable agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03510">Towards Sobolev Training. (arXiv:2312.03510v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kichler_N/0/1/0/all/0/1">Neil Kichler</a>, <a href="http://arxiv.org/find/cs/1/au:+Afghan_S/0/1/0/all/0/1">Sher Afghan</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_U/0/1/0/all/0/1">Uwe Naumann</a></p>
<p>The increasing use of stochastic models for describing complex phenomena
warrants surrogate models that capture the reference model characteristics at a
fraction of the computational cost, foregoing potentially expensive Monte Carlo
simulation. The predominant approach of fitting a large neural network and then
pruning it to a reduced size has commonly neglected shortcomings. The produced
surrogate models often will not capture the sensitivities and uncertainties
inherent in the original model. In particular, (higher-order) derivative
information of such surrogates could differ drastically. Given a large enough
network, we expect this derivative information to match. However, the pruned
model will almost certainly not share this behavior.
</p>
<p>In this paper, we propose to find surrogate models by using sensitivity
information throughout the learning and pruning process. We build on work using
Interval Adjoint Significance Analysis for pruning and combine it with the
recent advancements in Sobolev Training to accurately model the original
sensitivity information in the pruned neural network based surrogate model. We
experimentally underpin the method on an example of pricing a multidimensional
Basket option modelled through a stochastic differential equation with Brownian
motion. The proposed method is, however, not limited to the domain of
quantitative finance, which was chosen as a case study for intuitive
interpretations of the sensitivities. It serves as a foundation for building
further surrogate modelling techniques considering sensitivity information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03511">Kandinsky 3.0 Technical Report. (arXiv:2312.03511v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1">Vladimir Arkhipkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Filatov_A/0/1/0/all/0/1">Andrei Filatov</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilev_V/0/1/0/all/0/1">Viacheslav Vasilev</a>, <a href="http://arxiv.org/find/cs/1/au:+Maltseva_A/0/1/0/all/0/1">Anastasia Maltseva</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizov_S/0/1/0/all/0/1">Said Azizov</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlov_I/0/1/0/all/0/1">Igor Pavlov</a>, <a href="http://arxiv.org/find/cs/1/au:+Agafonova_J/0/1/0/all/0/1">Julia Agafonova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a></p>
<p>We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0
leverages a two times larger U-Net backbone, a ten times larger text encoder
and removes diffusion mapping. We describe the architecture of the model, the
data collection procedure, the training technique, and the production system of
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. By our side-by-side
comparisons, Kandinsky becomes better in text understanding and works better on
specific domains. Project page: https://ai-forever.github.io/Kandinsky-3
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03516">Clustering by Contour coreset and variational quantum eigensolver. (arXiv:2312.03516v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Yung_C/0/1/0/all/0/1">Canaan Yung</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Usman_M/0/1/0/all/0/1">Muhammad Usman</a></p>
<p>Recent work has proposed solving the k-means clustering problem on quantum
computers via the Quantum Approximate Optimization Algorithm (QAOA) and coreset
techniques. Although the current method demonstrates the possibility of quantum
k-means clustering, it does not ensure high accuracy and consistency across a
wide range of datasets. The existing coreset techniques are designed for
classical algorithms and there has been no quantum-tailored coreset technique
which is designed to boost the accuracy of quantum algorithms. In this work, we
propose solving the k-means clustering problem with the variational quantum
eigensolver (VQE) and a customised coreset method, the Contour coreset, which
has been formulated with specific focus on quantum algorithms. Extensive
simulations with synthetic and real-life data demonstrated that our VQE+Contour
Coreset approach outperforms existing QAOA+Coreset k-means clustering
approaches with higher accuracy and lower standard deviation. Our work has
shown that quantum tailored coreset techniques has the potential to
significantly boost the performance of quantum algorithms when compared to
using generic off-the-shelf coreset techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03526">On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm. (arXiv:2312.03526v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Peng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Daiwei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tao Lin</a></p>
<p>Contemporary machine learning requires training large neural networks on
massive datasets and thus faces the challenges of high computational demands.
Dataset distillation, as a recent emerging strategy, aims to compress
real-world datasets for efficient training. However, this line of research
currently struggle with large-scale and high-resolution datasets, hindering its
practicality and feasibility. To this end, we re-examine the existing dataset
distillation methods and identify three properties required for large-scale
real-world applications, namely, realism, diversity, and efficiency. As a
remedy, we propose RDED, a novel computationally-efficient yet effective data
distillation paradigm, to enable both diversity and realism of the distilled
data. Extensive empirical results over various neural architectures and
datasets demonstrate the advancement of RDED: we can distill the full
ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,
achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU
(while the SOTA only achieves 21% but requires 6 hours).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03556">Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention. (arXiv:2312.03556v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianjin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Motamed_S/0/1/0/all/0/1">Saman Motamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaddamanu_P/0/1/0/all/0/1">Praneetha Vaddamanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chen Henry Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Haene_C/0/1/0/all/0/1">Christian Haene</a>, <a href="http://arxiv.org/find/cs/1/au:+Bazin_J/0/1/0/all/0/1">Jean-Charles Bazin</a>, <a href="http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1">Fernando de la Torre</a></p>
<p>Face inpainting is important in various applications, such as photo
restoration, image editing, and virtual reality. Despite the significant
advances in face generative models, ensuring that a person's unique facial
identity is maintained during the inpainting process is still an elusive goal.
Current state-of-the-art techniques, exemplified by MyStyle, necessitate
resource-intensive fine-tuning and a substantial number of images for each new
identity. Furthermore, existing methods often fall short in accommodating
user-specified semantic attributes, such as beard or expression. To improve
inpainting results, and reduce the computational complexity during inference,
this paper proposes the use of Parallel Visual Attention (PVA) in conjunction
with diffusion models. Specifically, we insert parallel attention matrices to
each cross-attention module in the denoising network, which attends to features
extracted from reference images by an identity encoder. We train the added
attention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for
identity-preserving face inpainting. Experiments demonstrate that PVA attains
unparalleled identity resemblance in both face inpainting and face inpainting
with language guidance tasks, in comparison to various benchmarks, including
MyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA
ensures good identity preservation while offering effective
language-controllability. Additionally, in contrast to Custom Diffusion, PVA
requires just 40 fine-tuning steps for each new identity, which translates to a
significant speed increase of over 20 times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03561">Blueprinting the Future: Automatic Item Categorization using Hierarchical Zero-Shot and Few-Shot Classifiers. (arXiv:2312.03561v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1">Ting Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Stelter_K/0/1/0/all/0/1">Keith Stelter</a>, <a href="http://arxiv.org/find/stat/1/au:+Floyd_J/0/1/0/all/0/1">Jenn Floyd</a>, <a href="http://arxiv.org/find/stat/1/au:+ONeill_T/0/1/0/all/0/1">Thomas O&#x27;Neill</a>, <a href="http://arxiv.org/find/stat/1/au:+Hendrix_N/0/1/0/all/0/1">Nathaniel Hendrix</a>, <a href="http://arxiv.org/find/stat/1/au:+Bazemore_A/0/1/0/all/0/1">Andrew Bazemore</a>, <a href="http://arxiv.org/find/stat/1/au:+Rode_K/0/1/0/all/0/1">Kevin Rode</a>, <a href="http://arxiv.org/find/stat/1/au:+Newton_W/0/1/0/all/0/1">Warren Newton</a></p>
<p>In testing industry, precise item categorization is pivotal to align exam
questions with the designated content domains outlined in the assessment
blueprint. Traditional methods either entail manual classification, which is
laborious and error-prone, or utilize machine learning requiring extensive
training data, often leading to model underfit or overfit issues. This study
unveils a novel approach employing the zero-shot and few-shot Generative
Pretrained Transformer (GPT) classifier for hierarchical item categorization,
minimizing the necessity for training data, and instead, leveraging human-like
language descriptions to define categories. Through a structured python
dictionary, the hierarchical nature of examination blueprints is navigated
seamlessly, allowing for a tiered classification of items across multiple
levels. An initial simulation with artificial data demonstrates the efficacy of
this method, achieving an average accuracy of 92.91% measured by the F1 score.
This method was further applied to real exam items from the 2022 In-Training
Examination (ITE) conducted by the American Board of Family Medicine (ABFM),
reclassifying 200 items according to a newly formulated blueprint swiftly in 15
minutes, a task that traditionally could span several days among editors and
physicians. This innovative approach not only drastically cuts down
classification time but also ensures a consistent, principle-driven
categorization, minimizing human biases and discrepancies. The ability to
refine classifications by adjusting definitions adds to its robustness and
sustainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03580">Invariance &amp; Causal Representation Learning: Prospects and Limitations. (arXiv:2312.03580v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bing_S/0/1/0/all/0/1">Simon Bing</a>, <a href="http://arxiv.org/find/stat/1/au:+Wahl_J/0/1/0/all/0/1">Jonas Wahl</a>, <a href="http://arxiv.org/find/stat/1/au:+Ninad_U/0/1/0/all/0/1">Urmi Ninad</a>, <a href="http://arxiv.org/find/stat/1/au:+Runge_J/0/1/0/all/0/1">Jakob Runge</a></p>
<p>In causal models, a given mechanism is assumed to be invariant to changes of
other mechanisms. While this principle has been utilized for inference in
settings where the causal variables are observed, theoretical insights when the
variables of interest are latent are largely missing. We assay the connection
between invariance and causal representation learning by establishing
impossibility results which show that invariance alone is insufficient to
identify latent causal variables. Together with practical considerations, we
use these theoretical findings to highlight the need for additional constraints
in order to identify representations by exploiting invariance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03596">MMM: Generative Masked Motion Model. (arXiv:2312.03596v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pinyoanuntapong_E/0/1/0/all/0/1">Ekkasit Pinyoanuntapong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Minwoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a></p>
<p>Recent advances in text-to-motion generation using diffusion and
autoregressive models have shown promising results. However, these models often
suffer from a trade-off between real-time performance, high fidelity, and
motion editability. To address this gap, we introduce MMM, a novel yet simple
motion generation paradigm based on Masked Motion Model. MMM consists of two
key components: (1) a motion tokenizer that transforms 3D human motion into a
sequence of discrete tokens in latent space, and (2) a conditional masked
motion transformer that learns to predict randomly masked motion tokens,
conditioned on the pre-computed text tokens. By attending to motion and text
tokens in all directions, MMM explicitly captures inherent dependency among
motion tokens and semantic mapping between motion and text tokens. During
inference, this allows parallel and iterative decoding of multiple motion
tokens that are highly consistent with fine-grained text descriptions,
therefore simultaneously achieving high-fidelity and high-speed motion
generation. In addition, MMM has innate motion editability. By simply placing
mask tokens in the place that needs editing, MMM automatically fills the gaps
while guaranteeing smooth transitions between editing and non-editing parts.
Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM
surpasses current leading methods in generating high-quality motion (evidenced
by superior FID scores of 0.08 and 0.429), while offering advanced editing
features such as body-part modification, motion in-betweening, and the
synthesis of long motion sequences. In addition, MMM is two orders of magnitude
faster on a single mid-range GPU than editable motion diffusion models. Our
project page is available at \url{https://exitudio.github.io/MMM-page}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03606">DiffusionSat: A Generative Foundation Model for Satellite Imagery. (arXiv:2312.03606v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1">Samar Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Patrick Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Linqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1">Robin Rombach</a>, <a href="http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1">Marshall Burke</a>, <a href="http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1">David Lobell</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a></p>
<p>Diffusion models have achieved state-of-the-art results on many modalities
including images, speech, and video. However, existing models are not tailored
to support remote sensing data, which is widely used in important applications
including environmental monitoring and crop-yield prediction. Satellite images
are significantly different from natural images -- they can be multi-spectral,
irregularly sampled across time -- and existing diffusion models trained on
images from the Web do not support them. Furthermore, remote sensing data is
inherently spatio-temporal, requiring conditional generation tasks not
supported by traditional methods based on captions or images. In this paper, we
present DiffusionSat, to date the largest generative foundation model trained
on a collection of publicly available large, high-resolution remote sensing
datasets. As text-based captions are sparsely available for satellite images,
we incorporate the associated metadata such as geolocation as conditioning
information. Our method produces realistic samples and can be used to solve
multiple generative tasks including temporal generation, superresolution given
multi-spectral inputs and in-painting. Our method outperforms previous
state-of-the-art methods for satellite image generation and is the first
large-scale $\textit{generative}$ foundation model for satellite imagery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03611">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions. (arXiv:2312.03611v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yunhan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yukun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuan-Chen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Song-Hai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1">Tong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a></p>
<p>Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03612">Physical Symbolic Optimization. (arXiv:2312.03612v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tenachi_W/0/1/0/all/0/1">Wassim Tenachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibata_R/0/1/0/all/0/1">Rodrigo Ibata</a>, <a href="http://arxiv.org/find/cs/1/au:+Diakogiannis_F/0/1/0/all/0/1">Foivos I. Diakogiannis</a></p>
<p>We present a framework for constraining the automatic sequential generation
of equations to obey the rules of dimensional analysis by construction.
Combining this approach with reinforcement learning, we built $\Phi$-SO, a
Physical Symbolic Optimization method for recovering analytical functions from
physical data leveraging units constraints. Our symbolic regression algorithm
achieves state-of-the-art results in contexts in which variables and constants
have known physical units, outperforming all other methods on SRBench's Feynman
benchmark in the presence of noise (exceeding 0.1%) and showing resilience even
in the presence of significant (10%) levels of noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03619">Evaluation of Active Feature Acquisition Methods for Static Feature Settings. (arXiv:2312.03619v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kleist_H/0/1/0/all/0/1">Henrik von Kleist</a>, <a href="http://arxiv.org/find/stat/1/au:+Zamanian_A/0/1/0/all/0/1">Alireza Zamanian</a>, <a href="http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1">Ilya Shpitser</a>, <a href="http://arxiv.org/find/stat/1/au:+Ahmidi_N/0/1/0/all/0/1">Narges Ahmidi</a></p>
<p>Active feature acquisition (AFA) agents, crucial in domains like healthcare
where acquiring features is often costly or harmful, determine the optimal set
of features for a subsequent classification task. As deploying an AFA agent
introduces a shift in missingness distribution, it's vital to assess its
expected performance at deployment using retrospective data. In a companion
paper, we introduce a semi-offline reinforcement learning (RL) framework for
active feature acquisition performance evaluation (AFAPE) where features are
assumed to be time-dependent. Here, we study and extend the AFAPE problem to
cover static feature settings, where features are time-invariant, and hence
provide more flexibility to the AFA agents in deciding the order of the
acquisitions. In this static feature setting, we derive and adapt new inverse
probability weighting (IPW), direct method (DM), and double reinforcement
learning (DRL) estimators within the semi-offline RL framework. These
estimators can be applied when the missingness in the retrospective dataset
follows a missing-at-random (MAR) pattern. They also can be applied to
missing-not-at-random (MNAR) patterns in conjunction with appropriate existing
missing data techniques. We illustrate the improved data efficiency offered by
the semi-offline RL estimators in synthetic and real-world data experiments
under synthetic MAR and MNAR missingness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03632">Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models. (arXiv:2312.03632v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1">Dominik Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Churchill_A/0/1/0/all/0/1">Alexander Churchill</a>, <a href="http://arxiv.org/find/cs/1/au:+Sigtia_S/0/1/0/all/0/1">Siddharth Sigtia</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgiou_P/0/1/0/all/0/1">Panayiotis Georgiou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirsamadi_M/0/1/0/all/0/1">Matt Mirsamadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Aarshee Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Marchi_E/0/1/0/all/0/1">Erik Marchi</a></p>
<p>Interactions with virtual assistants typically start with a trigger phrase
followed by a command. In this work, we explore the possibility of making these
interactions more natural by eliminating the need for a trigger phrase. Our
goal is to determine whether a user addressed the virtual assistant based on
signals obtained from the streaming audio recorded by the device microphone. We
address this task by combining 1-best hypotheses and decoder signals from an
automatic speech recognition system with acoustic representations from an audio
encoder as input features to a large language model (LLM). In particular, we
are interested in data and resource efficient systems that require only a small
amount of training data and can operate in scenarios with only a single frozen
LLM available on a device. For this reason, our model is trained on 80k or less
examples of multimodal data using a combination of low-rank adaptation and
prefix tuning. We compare the proposed system to unimodal baselines and show
that the multimodal approach achieves lower equal-error-rates (EERs), while
using only a fraction of the training data. We also show that low-dimensional
specialized audio representations lead to lower EERs than high-dimensional
general audio representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03633">Not All Large Language Models (LLMs) Succumb to the &quot;Reversal Curse&quot;: A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models. (arXiv:2312.03633v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingye Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Da Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a></p>
<p>The "Reversal Curse" refers to the scenario where auto-regressive decoder
large language models (LLMs), such as ChatGPT, trained on "A is B" fail to
learn "B is A", demonstrating a basic failure of logical deduction. This raises
a red flag in the use of GPT models for certain general tasks such as
constructing knowledge graphs, considering their adherence to this symmetric
principle. In our study, we examined a bidirectional LLM, BERT, and found that
it is immune to the reversal curse. Driven by ongoing efforts to construct
biomedical knowledge graphs with LLMs, we also embarked on evaluating more
complex but essential deductive reasoning capabilities. This process included
first training encoder and decoder language models to master the intersection
($\cap$) and union ($\cup$) operations on two sets and then moving on to assess
their capability to infer different combinations of union ($\cup$) and
intersection ($\cap$) operations on three newly created sets. The findings
showed that while both encoder and decoder language models, trained for tasks
involving two sets (union/intersection), were proficient in such scenarios,
they encountered difficulties when dealing with operations that included three
sets (various combinations of union and intersection). Our research highlights
the distinct characteristics of encoder and decoder models in simple and
complex logical reasoning. In practice, the choice between BERT and GPT should
be guided by the specific requirements and nature of the task at hand,
leveraging their respective strengths in bidirectional context comprehension
and sequence prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03641">MotionCtrl: A Unified and Flexible Motion Controller for Video Generation. (arXiv:2312.03641v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhouxia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Ziyang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianshui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menghan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a></p>
<p>Motions in a video primarily consist of camera motion, induced by camera
movement, and object motion, resulting from object movement. Accurate control
of both camera and object motion is essential for video generation. However,
existing works either mainly focus on one type of motion or do not clearly
distinguish between the two, limiting their control capabilities and diversity.
Therefore, this paper presents MotionCtrl, a unified and flexible motion
controller for video generation designed to effectively and independently
control camera and object motion. The architecture and training strategy of
MotionCtrl are carefully devised, taking into account the inherent properties
of camera motion, object motion, and imperfect training data. Compared to
previous methods, MotionCtrl offers three main advantages: 1) It effectively
and independently controls camera motion and object motion, enabling more
fine-grained motion control and facilitating flexible and diverse combinations
of both types of motion. 2) Its motion conditions are determined by camera
poses and trajectories, which are appearance-free and minimally impact the
appearance or shape of objects in generated videos. 3) It is a relatively
generalizable model that can adapt to a wide array of camera poses and
trajectories once trained. Extensive qualitative and quantitative experiments
have been conducted to demonstrate the superiority of MotionCtrl over existing
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03642">Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap with Extremely Limited Data. (arXiv:2312.03642v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Olson_M/0/1/0/all/0/1">Matthew L. Olson</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shusen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1">Jayaraman J. Thiagarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kustowski_B/0/1/0/all/0/1">Bogdan Kustowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1">Weng-Keen Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1">Rushil Anirudh</a></p>
<p>Recent advances in machine learning, specifically transformer architecture,
have led to significant advancements in commercial domains. These powerful
models have demonstrated superior capability to learn complex relationships and
often generalize better to new data and problems. This paper presents a novel
transformer-powered approach for enhancing prediction accuracy in multi-modal
output scenarios, where sparse experimental data is supplemented with
simulation data. The proposed approach integrates transformer-based
architecture with a novel graph-based hyper-parameter optimization technique.
The resulting system not only effectively reduces simulation bias, but also
achieves superior prediction accuracy compared to the prior method. We
demonstrate the efficacy of our approach on inertial confinement fusion
experiments, where only 10 shots of real-world data are available, as well as
synthetic versions of these experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03644">MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment. (arXiv:2312.03644v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yali Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yudi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Biwei Huang</a></p>
<p>Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios
where online interaction is impractical or risky. While independent learning in
MARL offers flexibility and scalability, accurately assigning credit to
individual agents in offline settings poses challenges due to partial
observability and emergent behavior. Directly transferring the online credit
assignment method to offline settings results in suboptimal outcomes due to the
absence of real-time feedback and intricate agent interactions. Our approach,
MACCA, characterizing the generative process as a Dynamic Bayesian Network,
captures relationships between environmental variables, states, actions, and
rewards. Estimating this model on offline data, MACCA can learn each agent's
contribution by analyzing the causal relationship of their individual rewards,
ensuring accurate and interpretable credit assignment. Additionally, the
modularity of our approach allows it to seamlessly integrate with various
offline MARL methods. Theoretically, we proved that under the setting of the
offline dataset, the underlying causal structure and the function for
generating the individual rewards of agents are identifiable, which laid the
foundation for the correctness of our modeling. Experimentally, we tested MACCA
in two environments, including discrete and continuous action settings. The
results show that MACCA outperforms SOTA methods and improves performance upon
their backbones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03647">Editable Stain Transformation Of Histological Images Using Unpaired GANs. (arXiv:2312.03647v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sloboda_T/0/1/0/all/0/1">Tibor Sloboda</a>, <a href="http://arxiv.org/find/eess/1/au:+Hudec_L/0/1/0/all/0/1">Luk&#xe1;&#x161; Hudec</a>, <a href="http://arxiv.org/find/eess/1/au:+Benesova_W/0/1/0/all/0/1">Wanda Bene&#x161;ov&#xe1;</a></p>
<p>Double staining in histopathology, particularly for metaplastic breast
cancer, typically employs H&amp;E and P63 dyes. However, P63's tissue damage and
high cost necessitate alternative methods. This study introduces xAI-CycleGAN,
an advanced architecture combining Mask CycleGAN with explainability features
and structure-preserving capabilities for transforming H&amp;E stained breast
tissue images into P63-like images. The architecture allows for output editing,
enhancing resemblance to actual images and enabling further model refinement.
We showcase xAI-CycleGAN's efficacy in maintaining structural integrity and
generating high-quality images. Additionally, a histopathologist survey
indicates the generated images' realism is often comparable to actual images,
validating our model's high-quality output.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03651">MICRACLE: Inverse Reinforcement and Curriculum Learning Model for Human-inspired Mobile Robot Navigation. (arXiv:2312.03651v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gunukula_N/0/1/0/all/0/1">Nihal Gunukula</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1">Kshitij Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1">Aniket Bera</a></p>
<p>In emergency scenarios, mobile robots must navigate like humans, interpreting
stimuli to locate potential victims rapidly without interfering with first
responders. Existing socially-aware navigation algorithms face computational
and adaptability challenges. To overcome these, we propose a solution, MIRACLE
-- an inverse reinforcement and curriculum learning model, that employs
gamified learning to gather stimuli-driven human navigational data. This data
is then used to train a Deep Inverse Maximum Entropy Reinforcement Learning
model, reducing reliance on demonstrator abilities. Testing reveals a low loss
of 2.7717 within a 400-sized environment, signifying human-like response
replication. Current databases lack comprehensive stimuli-driven data,
necessitating our approach. By doing so, we enable robots to navigate emergency
situations with human-like perception, enhancing their life-saving
capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03654">Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies. (arXiv:2312.03654v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grbcic_L/0/1/0/all/0/1">Luka Grbcic</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1">Juliane M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1">Wibe Albert de Jong</a></p>
<p>This paper introduces a methodology designed to augment the inverse design
optimization process in scenarios constrained by limited compute, through the
strategic synergy of multi-fidelity evaluations, machine learning models, and
optimization algorithms. The proposed methodology is analyzed on two distinct
engineering inverse design problems: airfoil inverse design and the scalar
field reconstruction problem. It leverages a machine learning model trained
with low-fidelity simulation data, in each optimization cycle, thereby
proficiently predicting a target variable and discerning whether a
high-fidelity simulation is necessitated, which notably conserves computational
resources. Additionally, the machine learning model is strategically deployed
prior to optimization to reduce the search space, thereby further accelerating
convergence toward the optimal solution. The methodology has been employed to
enhance two optimization algorithms, namely Differential Evolution and Particle
Swarm Optimization. Comparative analyses illustrate performance improvements
across both algorithms. Notably, this method is adeptly adaptable across any
inverse design application, facilitating a harmonious synergy between a
representative low-fidelity machine learning model, and high-fidelity
simulation, and can be seamlessly applied across any variety of
population-based optimization algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03656">Interpretability Illusions in the Generalization of Simplified Models. (arXiv:2312.03656v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Friedman_D/0/1/0/all/0/1">Dan Friedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1">Andrew Lampinen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1">Lucas Dixon</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1">Asma Ghandeharioun</a></p>
<p>A common method to study deep learning systems is to use simplified model
representations -- for example, using singular value decomposition to visualize
the model's hidden states in a lower dimensional space. This approach assumes
that the results of these simplified are faithful to the original model. Here,
we illustrate an important caveat to this assumption: even if the simplified
representations can accurately approximate the full model on the training set,
they may fail to accurately capture the model's behavior out of distribution --
the understanding developed from simplified representations may be an illusion.
We illustrate this by training Transformer models on controlled datasets with
systematic generalization splits. First, we train models on the Dyck
balanced-parenthesis languages. We simplify these models using tools like
dimensionality reduction and clustering, and then explicitly test how these
simplified proxies match the behavior of the original model on various
out-of-distribution test sets. We find that the simplified proxies are
generally less faithful out of distribution. In cases where the original model
generalizes to novel structures or deeper depths, the simplified versions may
fail, or generalize better. This finding holds even if the simplified
representations do not directly depend on the training distribution. Next, we
study a more naturalistic task: predicting the next character in a dataset of
computer code. We find similar generalization gaps between the original model
and simplified proxies, and conduct further analysis to investigate which
aspects of the code completion task are associated with the largest gaps.
Together, our results raise questions about the extent to which mechanistic
interpretations derived using tools like SVD can reliably predict what a model
will do in novel situations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03666">Towards small and accurate convolutional neural networks for acoustic biodiversity monitoring. (arXiv:2312.03666v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaugg_S/0/1/0/all/0/1">Serge Zaugg</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mike van der Schaar</a>, <a href="http://arxiv.org/find/cs/1/au:+Erbs_F/0/1/0/all/0/1">Florence Erbs</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1">Antonio Sanchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Castell_J/0/1/0/all/0/1">Joan V. Castell</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramallo_E/0/1/0/all/0/1">Emiliano Ramallo</a>, <a href="http://arxiv.org/find/cs/1/au:+Andre_M/0/1/0/all/0/1">Michel Andr&#xe9;</a></p>
<p>Automated classification of animal sounds is a prerequisite for large-scale
monitoring of biodiversity. Convolutional Neural Networks (CNNs) are among the
most promising algorithms but they are slow, often achieve poor classification
in the field and typically require large training data sets. Our objective was
to design CNNs that are fast at inference time and achieve good classification
performance while learning from moderate-sized data. Recordings from a
rainforest ecosystem were used. Start and end-point of sounds from 20 bird
species were manually annotated. Spectrograms from 10 second segments were used
as CNN input. We designed simple CNNs with a frequency unwrapping layer
(SIMP-FU models) such that any output unit was connected to all spectrogram
frequencies but only to a sub-region of time, the Receptive Field (RF). Our
models allowed experimentation with different RF durations. Models either used
the time-indexed labels that encode start and end-point of sounds or simpler
segment-level labels. Models learning from time-indexed labels performed
considerably better than their segment-level counterparts. Best classification
performances was achieved for models with intermediate RF duration of 1.5
seconds. The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes on
the test set. On compact low-cost hardware the best SIMP-FU models evaluated up
to seven times faster than real-time data acquisition. RF duration was a major
driver of classification performance. The optimum of 1.5 s was in the same
range as the duration of the sounds. Our models achieved good classification
performance while learning from moderate-sized training data. This is explained
by the usage of time-indexed labels during training and adequately sized RF.
Results confirm the feasibility of deploying small CNNs with good
classification performance on compact low-cost devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03668">An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition. (arXiv:2312.03668v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1">Yukiya Hono</a>, <a href="http://arxiv.org/find/eess/1/au:+Mitsuda_K/0/1/0/all/0/1">Koh Mitsuda</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1">Tianyu Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Mitsui_K/0/1/0/all/0/1">Kentaro Mitsui</a>, <a href="http://arxiv.org/find/eess/1/au:+Wakatsuki_T/0/1/0/all/0/1">Toshiaki Wakatsuki</a>, <a href="http://arxiv.org/find/eess/1/au:+Sawada_K/0/1/0/all/0/1">Kei Sawada</a></p>
<p>Advances in machine learning have made it possible to perform various text
and speech processing tasks, including automatic speech recognition (ASR), in
an end-to-end (E2E) manner. Since typical E2E approaches require large amounts
of training data and resources, leveraging pre-trained foundation models
instead of training from scratch is gaining attention. Although there have been
attempts to use pre-trained speech and language models in ASR, most of them are
limited to using either. This paper explores the potential of integrating a
pre-trained speech representation model with a large language model (LLM) for
E2E ASR. The proposed model enables E2E ASR by generating text tokens in an
autoregressive manner via speech representations as speech prompts, taking
advantage of the vast knowledge provided by the LLM. Furthermore, the proposed
model can incorporate remarkable developments for LLM utilization, such as
inference optimization and parameter-efficient domain adaptation. Experimental
results show that the proposed model achieves performance comparable to modern
E2E ASR models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03671">Direct Exoplanet Detection Using Deep Convolutional Image Reconstruction (ConStruct): A New Algorithm for Post-Processing High-Contrast Images. (arXiv:2312.03671v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Wolf_T/0/1/0/all/0/1">Trevor N. Wolf</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Jones_B/0/1/0/all/0/1">Brandon A. Jones</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Bowler_B/0/1/0/all/0/1">Brendan P. Bowler</a></p>
<p>We present a novel machine-learning approach for detecting faint point
sources in high-contrast adaptive optics imaging datasets. The most widely used
algorithms for primary subtraction aim to decouple bright stellar speckle noise
from planetary signatures by subtracting an approximation of the temporally
evolving stellar noise from each frame in an imaging sequence. Our approach
aims to improve the stellar noise approximation and increase the planet
detection sensitivity by leveraging deep learning in a novel direct imaging
post-processing algorithm. We show that a convolutional autoencoder neural
network, trained on an extensive reference library of real imaging sequences,
accurately reconstructs the stellar speckle noise at the location of a
potential planet signal. This tool is used in a post-processing algorithm we
call Direct Exoplanet Detection with Convolutional Image Reconstruction, or
ConStruct. The reliability and sensitivity of ConStruct are assessed using real
Keck/NIRC2 angular differential imaging datasets. Of the 30 unique point
sources we examine, ConStruct yields a higher S/N than traditional PCA-based
processing for 67$\%$ of the cases and improves the relative contrast by up to
a factor of 2.6. This work demonstrates the value and potential of deep
learning to take advantage of a diverse reference library of point spread
function realizations to improve direct imaging post-processing. ConStruct and
its future improvements may be particularly useful as tools for post-processing
high-contrast images from the James Webb Space Telescope and extreme adaptive
optics instruments, both for the current generation and those being designed
for the upcoming 30 meter-class telescopes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03673">On the Role of the Action Space in Robot Manipulation Learning and Sim-to-Real Transfer. (arXiv:2312.03673v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aljalbout_E/0/1/0/all/0/1">Elie Aljalbout</a>, <a href="http://arxiv.org/find/cs/1/au:+Frank_F/0/1/0/all/0/1">Felix Frank</a>, <a href="http://arxiv.org/find/cs/1/au:+Karl_M/0/1/0/all/0/1">Maximilian Karl</a>, <a href="http://arxiv.org/find/cs/1/au:+Smagt_P/0/1/0/all/0/1">Patrick van der Smagt</a></p>
<p>We study the choice of action space in robot manipulation learning and
sim-to-real transfer. We define metrics that assess the performance, and
examine the emerging properties in the different action spaces. We train over
250 reinforcement learning~(RL) agents in simulated reaching and pushing tasks,
using 13 different control spaces. The choice of action spaces spans popular
choices in the literature as well as novel combinations of common design
characteristics. We evaluate the training performance in simulation and the
transfer to a real-world environment. We identify good and bad characteristics
of robotic action spaces and make recommendations for future designs. Our
findings have important implications for the design of RL algorithms for robot
manipulation tasks, and highlight the need for careful consideration of action
spaces when training and transferring RL agents for real-world robotics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03675">GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models. (arXiv:2312.03675v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziqi Li</a></p>
<p>This paper introduces GeoShapley, a game theory approach to measuring spatial
effects in machine learning models. GeoShapley extends the Nobel Prize-winning
Shapley value framework in game theory by conceptualizing location as a player
in a model prediction game, which enables the quantification of the importance
of location and the synergies between location and other features in a model.
GeoShapley is a model-agnostic approach and can be applied to statistical or
black-box machine learning models in various structures. The interpretation of
GeoShapley is directly linked with spatially varying coefficient models for
explaining spatial effects and additive models for explaining non-spatial
effects. Using simulated data, GeoShapley values are validated against known
data-generating processes and are used for cross-comparison of seven
statistical and machine learning models. An empirical example of house price
modeling is used to illustrate GeoShapley's utility and interpretation with
real world data. The method is available as an open-source Python package named
geoshapley.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03682">What Planning Problems Can A Relational Neural Network Solve?. (arXiv:2312.03682v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jiayuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1">Tom&#xe1;s Lozano-P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua B. Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1">Leslie Pack Kaelbling</a></p>
<p>Goal-conditioned policies are generally understood to be "feed-forward"
circuits, in the form of neural networks that map from the current state and
the goal specification to the next action to take. However, under what
circumstances such a policy can be learned and how efficient the policy will be
are not well understood. In this paper, we present a circuit complexity
analysis for relational neural networks (such as graph neural networks and
transformers) representing policies for planning problems, by drawing
connections with serialized goal regression search (S-GRS). We show that there
are three general classes of planning problems, in terms of the growth of
circuit width and depth as a function of the number of objects and planning
horizon, providing constructive proofs. We also illustrate the utility of this
analysis for designing neural networks for policy learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03690">Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling. (arXiv:2312.03690v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Zheng_Y/0/1/0/all/0/1">Yiwen Zheng</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Thakolkaran_P/0/1/0/all/0/1">Prakash Thakolkaran</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Smith_J/0/1/0/all/0/1">Jake A. Smith</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zheng_S/0/1/0/all/0/1">Shuxin Zheng</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Nguyen_B/0/1/0/all/0/1">Bichlien H. Nguyen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kumar_S/0/1/0/all/0/1">Siddhant Kumar</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Vashisth_A/0/1/0/all/0/1">Aniruddh Vashisth</a></p>
<p>Vitrimer is a new class of sustainable polymers with the ability of
self-healing through rearrangement of dynamic covalent adaptive networks.
However, a limited choice of constituent molecules restricts their property
space, prohibiting full realization of their potential applications. Through a
combination of molecular dynamics (MD) simulations and machine learning (ML),
particularly a novel graph variational autoencoder (VAE) model, we establish a
method for generating novel vitrimers and guide their inverse design based on
desired glass transition temperature (Tg). We build the first vitrimer dataset
of one million and calculate Tg on 8,424 of them by high-throughput MD
simulations calibrated by a Gaussian process model. The proposed VAE employs
dual graph encoders and a latent dimension overlapping scheme which allows for
individual representation of multi-component vitrimers. By constructing a
continuous latent space containing necessary information of vitrimers, we
demonstrate high accuracy and efficiency of our framework in discovering novel
vitrimers with desirable Tg beyond the training regime. The proposed vitrimers
with reasonable synthesizability cover a wide range of Tg and broaden the
potential widespread usage of vitrimeric materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03691">On the Role of Edge Dependency in Graph Generative Models. (arXiv:2312.03691v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chanpuriya_S/0/1/0/all/0/1">Sudhanshu Chanpuriya</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Cameron Musco</a>, <a href="http://arxiv.org/find/cs/1/au:+Sotiropoulos_K/0/1/0/all/0/1">Konstantinos Sotiropoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsourakakis_C/0/1/0/all/0/1">Charalampos Tsourakakis</a></p>
<p>In this work, we introduce a novel evaluation framework for generative models
of graphs, emphasizing the importance of model-generated graph overlap
(Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We
delineate a hierarchy of graph generative models categorized into three levels
of complexity: edge independent, node independent, and fully dependent models.
This hierarchy encapsulates a wide range of prevalent methods. We derive
theoretical bounds on the number of triangles and other short-length cycles
producible by each level of the hierarchy, contingent on the model overlap. We
provide instances demonstrating the asymptotic optimality of our bounds.
Furthermore, we introduce new generative models for each of the three
hierarchical levels, leveraging dense subgraph discovery (Gionis &amp; Tsourakakis,
2015). Our evaluation, conducted on real-world datasets, focuses on assessing
the output quality and overlap of our proposed models in comparison to other
popular models. Our results indicate that our simple, interpretable models
provide competitive baselines to popular generative models. Through this
investigation, we aim to propel the advancement of graph generative models by
offering a structured framework and robust evaluation metrics, thereby
facilitating the development of models capable of generating accurate and
edge-diverse graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03692">Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication. (arXiv:2312.03692v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1">Ali Naseh</a>, <a href="http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1">Jaechul Roh</a>, <a href="http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1">Amir Houmansadr</a></p>
<p>Diffusion-based models, such as the Stable Diffusion model, have
revolutionized text-to-image synthesis with their ability to produce
high-quality, high-resolution images. These advancements have prompted
significant progress in image generation and editing tasks. However, these
models also raise concerns due to their tendency to memorize and potentially
replicate exact training samples, posing privacy risks and enabling adversarial
attacks. Duplication in training datasets is recognized as a major factor
contributing to memorization, and various forms of memorization have been
studied so far. This paper focuses on two distinct and underexplored types of
duplication that lead to replication during inference in diffusion-based
models, particularly in the Stable Diffusion model. We delve into these
lesser-studied duplication phenomena and their implications through two case
studies, aiming to contribute to the safer and more responsible use of
generative models in various applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03700">OneLLM: One Framework to Align All Modalities with Language. (arXiv:2312.03700v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiaming Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1">Kaixiong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1">Peng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiangyu Yue</a></p>
<p>Multimodal large language models (MLLMs) have gained significant attention
due to their strong multimodal understanding capability. However, existing
works rely heavily on modality-specific encoders, which usually differ in
architecture and are limited to common modalities. In this paper, we present
OneLLM, an MLLM that aligns eight modalities to language using a unified
framework. We achieve this through a unified multimodal encoder and a
progressive multimodal alignment pipeline. In detail, we first train an image
projection module to connect a vision encoder with LLM. Then, we build a
universal projection module (UPM) by mixing multiple image projection modules
and dynamic routing. Finally, we progressively align more modalities to LLM
with the UPM. To fully leverage the potential of OneLLM in following
instructions, we also curated a comprehensive multimodal instruction dataset,
including 2M items from image, audio, video, point cloud, depth/normal map, IMU
and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,
encompassing tasks such as multimodal captioning, question answering and
reasoning, where it delivers excellent performance. Code, data, model and
online demo are available at https://github.com/csuhan/OneLLM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1901.10902">InfoBot: Transfer and Exploration via the Information Bottleneck. (arXiv:1901.10902v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>, <a href="http://arxiv.org/find/stat/1/au:+Islam_R/0/1/0/all/0/1">Riashat Islam</a>, <a href="http://arxiv.org/find/stat/1/au:+Strouse_D/0/1/0/all/0/1">Daniel Strouse</a>, <a href="http://arxiv.org/find/stat/1/au:+Ahmed_Z/0/1/0/all/0/1">Zafarali Ahmed</a>, <a href="http://arxiv.org/find/stat/1/au:+Botvinick_M/0/1/0/all/0/1">Matthew Botvinick</a>, <a href="http://arxiv.org/find/stat/1/au:+Larochelle_H/0/1/0/all/0/1">Hugo Larochelle</a>, <a href="http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/stat/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>A central challenge in reinforcement learning is discovering effective
policies for tasks where rewards are sparsely distributed. We postulate that in
the absence of useful reward signals, an effective exploration strategy should
seek out {\it decision states}. These states lie at critical junctions in the
state space from where the agent can transition to new, potentially unexplored
regions. We propose to learn about decision states from prior experience. By
training a goal-conditioned policy with an information bottleneck, we can
identify decision states by examining where the model actually leverages the
goal state. We find that this simple mechanism effectively identifies decision
states, even in partially observed settings. In effect, the model learns the
sensory cues that correlate with potential subgoals. In new environments, this
model can then identify novel subgoals for further exploration, guiding the
agent through a sequence of potential decision states and through new regions
of the state space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2006.03487">Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature. (arXiv:2006.03487v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhen Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1">Ryan Sze-Yin Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cochrane_T/0/1/0/all/0/1">Thomas Cochrane</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_P/0/1/0/all/0/1">Peter Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1">Terry Lyons</a></p>
<p>In this paper, we propose a dimensionless anomaly detection method for
multivariate streams. Our method is independent of the unit of measurement for
the different stream channels, therefore dimensionless. We first propose the
variance norm, a generalisation of Mahalanobis distance to handle
infinite-dimensional feature space and singular empirical covariance matrix
rigorously. We then combine the variance norm with the path signature, an
infinite collection of iterated integrals that provide global features of
streams, to propose SigMahaKNN, a method for anomaly detection on
(multivariate) streams. We show that SigMahaKNN is invariant to stream
reparametrisation, stream concatenation and has a graded discrimination power
depending on the truncation level of the path signature. We implement
SigMahaKNN as an open-source software, and perform extensive numerical
experiments, showing significantly improved anomaly detection on streams
compared to isolation forest and local outlier factors in applications ranging
from language analysis, hand-writing analysis, ship movement paths analysis and
univariate time-series analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2006.16144">Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs. (arXiv:2006.16144v3 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1">Siddhartha Mishra</a>, <a href="http://arxiv.org/find/math/1/au:+Molinaro_R/0/1/0/all/0/1">Roberto Molinaro</a></p>
<p>Physics informed neural networks (PINNs) have recently been widely used for
robust and accurate approximation of PDEs. We provide rigorous upper bounds on
the generalization error of PINNs approximating solutions of the forward
problem for PDEs. An abstract formalism is introduced and stability properties
of the underlying PDE are leveraged to derive an estimate for the
generalization error in terms of the training error and number of training
samples. This abstract framework is illustrated with several examples of
nonlinear PDEs. Numerical experiments, validating the proposed theory, are also
presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.01138">Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating a class of inverse problems for PDEs. (arXiv:2007.01138v3 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1">Siddhartha Mishra</a>, <a href="http://arxiv.org/find/math/1/au:+Molinaro_R/0/1/0/all/0/1">Roberto Molinaro</a></p>
<p>Physics informed neural networks (PINNs) have recently been very successfully
applied for efficiently approximating inverse problems for PDEs. We focus on a
particular class of inverse problems, the so-called data assimilation or unique
continuation problems, and prove rigorous estimates on the generalization error
of PINNs approximating them. An abstract framework is presented and conditional
stability estimates for the underlying inverse problem are employed to derive
the estimate on the PINN generalization error, providing rigorous justification
for the use of PINNs in this context. The abstract framework is illustrated
with examples of four prototypical linear PDEs. Numerical experiments,
validating the proposed theory, are also presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.13291">Physics Informed Neural Networks for Simulating Radiative Transfer. (arXiv:2009.13291v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Siddhartha Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Molinaro_R/0/1/0/all/0/1">Roberto Molinaro</a></p>
<p>We propose a novel machine learning algorithm for simulating radiative
transfer. Our algorithm is based on physics informed neural networks (PINNs),
which are trained by minimizing the residual of the underlying radiative
tranfer equations. We present extensive experiments and theoretical error
estimates to demonstrate that PINNs provide a very easy to implement, fast,
robust and accurate method for simulating radiative transfer. We also present a
PINN based algorithm for simulating inverse problems for radiative transfer
efficiently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.12909">Algorithm as Experiment: Machine Learning, Market Design, and Policy Eligibility Rules. (arXiv:2104.12909v6 [econ.EM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Narita_Y/0/1/0/all/0/1">Yusuke Narita</a>, <a href="http://arxiv.org/find/econ/1/au:+Yata_K/0/1/0/all/0/1">Kohei Yata</a></p>
<p>Algorithms make a growing portion of policy and business decisions. We
develop a treatment-effect estimator using algorithmic decisions as instruments
for a class of stochastic and deterministic algorithms. Our estimator is
consistent and asymptotically normal for well-defined causal effects. A special
case of our setup is multidimensional regression discontinuity designs with
complex boundaries. We apply our estimator to evaluate the Coronavirus Aid,
Relief, and Economic Security Act, which allocated many billions of dollars
worth of relief funding to hospitals via an algorithmic rule. The funding is
shown to have little effect on COVID-19-related hospital activities. Naive
estimates exhibit selection bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.12065">Provably Accelerated Decentralized Gradient Method Over Unbalanced Directed Graphs. (arXiv:2107.12065v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Song_Z/0/1/0/all/0/1">Zhuoqing Song</a>, <a href="http://arxiv.org/find/math/1/au:+Shi_L/0/1/0/all/0/1">Lei Shi</a>, <a href="http://arxiv.org/find/math/1/au:+Pu_S/0/1/0/all/0/1">Shi Pu</a>, <a href="http://arxiv.org/find/math/1/au:+Yan_M/0/1/0/all/0/1">Ming Yan</a></p>
<p>We consider the decentralized optimization problem, where a network of $n$
agents aims to collaboratively minimize the average of their individual smooth
and convex objective functions through peer-to-peer communication in a directed
graph. To tackle this problem, we propose two accelerated gradient tracking
methods, namely APD and APD-SC, for non-strongly convex and strongly convex
objective functions, respectively. We show that APD and APD-SC converge at the
rates $O\left(\frac{1}{k^2}\right)$ and $O\left(\left(1 -
C\sqrt{\frac{\mu}{L}}\right)^k\right)$, respectively, up to constant factors
depending only on the mixing matrix. APD and APD-SC are the first decentralized
methods over unbalanced directed graphs that achieve the same provable
acceleration as centralized methods. Numerical experiments demonstrate the
effectiveness of both methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.09971">Learning Robust Output Control Barrier Functions from Safe Expert Demonstrations. (arXiv:2111.09971v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lindemann_L/0/1/0/all/0/1">Lars Lindemann</a>, <a href="http://arxiv.org/find/eess/1/au:+Robey_A/0/1/0/all/0/1">Alexander Robey</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_L/0/1/0/all/0/1">Lejun Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1">Satyajeet Das</a>, <a href="http://arxiv.org/find/eess/1/au:+Tu_S/0/1/0/all/0/1">Stephen Tu</a>, <a href="http://arxiv.org/find/eess/1/au:+Matni_N/0/1/0/all/0/1">Nikolai Matni</a></p>
<p>This paper addresses learning safe output feedback control laws from partial
observations of expert demonstrations. We assume that a model of the system
dynamics and a state estimator are available along with corresponding error
bounds, e.g., estimated from data in practice. We first propose robust output
control barrier functions (ROCBFs) as a means to guarantee safety, as defined
through controlled forward invariance of a safe set. We then formulate an
optimization problem to learn ROCBFs from expert demonstrations that exhibit
safe system behavior, e.g., data collected from a human operator or an expert
controller. When the parametrization of the ROCBF is linear, then we show that,
under mild assumptions, the optimization problem is convex. Along with the
optimization problem, we provide verifiable conditions in terms of the density
of the data, smoothness of the system model and state estimator, and the size
of the error bounds that guarantee validity of the obtained ROCBF. Towards
obtaining a practical control algorithm, we propose an algorithmic
implementation of our theoretical framework that accounts for assumptions made
in our framework in practice. We empirically validate our algorithm in the
autonomous driving simulator CARLA and demonstrate how to learn safe control
laws from RGB camera images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.12909">Optimal Variable Clustering for High-Dimensional Matrix Valued Data. (arXiv:2112.12909v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1">Inbeom Lee</a>, <a href="http://arxiv.org/find/stat/1/au:+Deng_S/0/1/0/all/0/1">Siyi Deng</a>, <a href="http://arxiv.org/find/stat/1/au:+Ning_Y/0/1/0/all/0/1">Yang Ning</a></p>
<p>Matrix valued data has become increasingly prevalent in many applications.
Most of the existing clustering methods for this type of data are tailored to
the mean model and do not account for the dependence structure of the features,
which can be very informative, especially in high-dimensional settings or when
mean information is not available. To extract the information from the
dependence structure for clustering, we propose a new latent variable model for
the features arranged in matrix form, with some unknown membership matrices
representing the clusters for the rows and columns. Under this model, we
further propose a class of hierarchical clustering algorithms using the
difference of a weighted covariance matrix as the dissimilarity measure.
Theoretically, we show that under mild conditions, our algorithm attains
clustering consistency in the high-dimensional setting. While this consistency
result holds for our algorithm with a broad class of weighted covariance
matrices, the conditions for this result depend on the choice of the weight. To
investigate how the weight affects the theoretical performance of our
algorithm, we establish the minimax lower bound for clustering under our latent
variable model in terms of some cluster separation metric. Given these results,
we identify the optimal weight in the sense that using this weight guarantees
our algorithm to be minimax rate-optimal. The practical implementation of our
algorithm with the optimal weight is also discussed. Simulation studies show
that our algorithm performs better than existing methods in terms of the
adjusted Rand index (ARI). The method is applied to a genomic dataset and
yields meaningful interpretations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.04093">Systematic Literature Review: Quantum Machine Learning and its applications. (arXiv:2201.04093v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Garcia_D/0/1/0/all/0/1">David Peral Garc&#xed;a</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Cruz_Benito_J/0/1/0/all/0/1">Juan Cruz-Benito</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Garcia_Penalvo_F/0/1/0/all/0/1">Francisco Jos&#xe9; Garc&#xed;a-Pe&#xf1;alvo</a></p>
<p>Quantum computing is the process of performing calculations using quantum
mechanics. This field studies the quantum behavior of certain subatomic
particles for subsequent use in performing calculations, as well as for
large-scale information processing. These capabilities can give quantum
computers an advantage in terms of computational time and cost over classical
computers. Nowadays, there are scientific challenges that are impossible to
perform by classical computation due to computational complexity or the time
the calculation would take, and quantum computation is one of the possible
answers. However, current quantum devices have not yet the necessary qubits and
are not fault-tolerant enough to achieve these goals. Nonetheless, there are
other fields like machine learning or chemistry where quantum computation could
be useful with current quantum devices. This manuscript aims to present a
Systematic Literature Review of the papers published between 2017 and 2023 to
identify, analyze and classify the different algorithms used in quantum machine
learning and their applications. Consequently, this study identified 94
articles that used quantum machine learning techniques and algorithms. The main
types of found algorithms are quantum implementations of classical machine
learning algorithms, such as support vector machines or the k-nearest neighbor
model, and classical deep learning algorithms, like quantum neural networks.
Many articles try to solve problems currently answered by classical machine
learning but using quantum devices and algorithms. Even though results are
promising, quantum machine learning is far from achieving its full potential.
An improvement in the quantum hardware is required since the existing quantum
computers lack enough quality, speed, and scale to allow quantum computing to
achieve its full potential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10629">Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a></p>
<p>In data-rich domains such as vision, language, and speech, deep learning
prevails to deliver high-performance task-specific models and can even learn
general task-agnostic representations for efficient finetuning to downstream
tasks. However, deep learning in resource-limited domains still faces multiple
challenges including (i) limited data, (ii) constrained model development cost,
and (iii) lack of adequate pre-trained models for effective finetuning. This
paper provides an overview of model reprogramming to bridge this gap. Model
reprogramming enables resource-efficient cross-domain machine learning by
repurposing and reusing a well-developed pre-trained model from a source domain
to solve tasks in a target domain without model finetuning, where the source
and target domains can be vastly different. In many applications, model
reprogramming outperforms transfer learning and training from scratch. This
paper elucidates the methodology of model reprogramming, summarizes existing
use cases, provides a theoretical explanation of the success of model
reprogramming, and concludes with a discussion on open-ended research questions
and opportunities. A list of model reprogramming studies is actively maintained
and updated at https://github.com/IBM/model-reprogramming.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.01850">T-Cal: An optimal test for the calibration of predictive models. (arXiv:2203.01850v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lee_D/0/1/0/all/0/1">Donghwan Lee</a>, <a href="http://arxiv.org/find/stat/1/au:+Huang_X/0/1/0/all/0/1">Xinmeng Huang</a>, <a href="http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1">Hamed Hassani</a>, <a href="http://arxiv.org/find/stat/1/au:+Dobriban_E/0/1/0/all/0/1">Edgar Dobriban</a></p>
<p>The prediction accuracy of machine learning methods is steadily increasing,
but the calibration of their uncertainty predictions poses a significant
challenge. Numerous works focus on obtaining well-calibrated predictive models,
but less is known about reliably assessing model calibration. This limits our
ability to know when algorithms for improving calibration have a real effect,
and when their improvements are merely artifacts due to random noise in finite
datasets. In this work, we consider detecting mis-calibration of predictive
models using a finite validation dataset as a hypothesis testing problem. The
null hypothesis is that the predictive model is calibrated, while the
alternative hypothesis is that the deviation from calibration is sufficiently
large.
</p>
<p>We find that detecting mis-calibration is only possible when the conditional
probabilities of the classes are sufficiently smooth functions of the
predictions. When the conditional class probabilities are H\"older continuous,
we propose T-Cal, a minimax optimal test for calibration based on a debiased
plug-in estimator of the $\ell_2$-Expected Calibration Error (ECE). We further
propose Adaptive T-Cal, a version that is adaptive to unknown smoothness. We
verify our theoretical findings with a broad range of experiments, including
with several popular deep neural net architectures and several standard
post-hoc calibration methods. T-Cal is a practical general-purpose tool, which
-- combined with classical tests for discrete-valued predictors -- can be used
to test the calibration of virtually any probabilistic classification method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.08620">Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing. (arXiv:2204.08620v4 [stat.AP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1">Zhi Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Bhandaram_U/0/1/0/all/0/1">Uma Bhandaram</a>, <a href="http://arxiv.org/find/stat/1/au:+Garg_N/0/1/0/all/0/1">Nikhil Garg</a></p>
<p>Modern city governance relies heavily on crowdsourcing to identify problems
such as downed trees and power lines. A major concern is that residents do not
report problems at the same rates, with heterogeneous reporting delays directly
translating to downstream disparities in how quickly incidents can be
addressed. Here we develop a method to identify reporting delays without using
external ground-truth data. Our insight is that the rates at which duplicate
reports are made about the same incident can be leveraged to disambiguate
whether an incident has occurred by investigating its reporting rate once it
has occurred. We apply our method to over 100,000 resident reports made in New
York City and to over 900,000 reports made in Chicago, finding that there are
substantial spatial and socioeconomic disparities in how quickly incidents are
reported. We further validate our methods using external data and demonstrate
how estimating reporting delays leads to practical insights and interventions
for a more equitable, efficient government service.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04979">Convolutional layers are equivariant to discrete shifts but not continuous translations. (arXiv:2206.04979v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McGreivy_N/0/1/0/all/0/1">Nick McGreivy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hakim_A/0/1/0/all/0/1">Ammar Hakim</a></p>
<p>The purpose of this short and simple note is to clarify a common
misconception about convolutional neural networks (CNNs). CNNs are made up of
convolutional layers which are shift equivariant due to weight sharing.
However, convolutional layers are not translation equivariant, even when
boundary effects are ignored and when pooling and subsampling are absent. This
is because shift equivariance is a discrete symmetry while translation
equivariance is a continuous symmetry. This fact is well known among
researchers in equivariant machine learning, but is usually overlooked among
non-experts. To minimize confusion, we suggest using the term `shift
equivariance' to refer to discrete shifts in pixels and `translation
equivariance' to refer to continuous translations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.08204">Inherent Inconsistencies of Feature Importance. (arXiv:2206.08204v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Harel_N/0/1/0/all/0/1">Nimrod Harel</a>, <a href="http://arxiv.org/find/cs/1/au:+Obolski_U/0/1/0/all/0/1">Uri Obolski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilad_Bachrach_R/0/1/0/all/0/1">Ran Gilad-Bachrach</a></p>
<p>The rapid advancement and widespread adoption of machine learning-driven
technologies have underscored the practical and ethical need for creating
interpretable artificial intelligence systems. Feature importance, a method
that assigns scores to the contribution of individual features on prediction
outcomes, seeks to bridge this gap as a tool for enhancing human comprehension
of these systems. Feature importance serves as an explanation of predictions in
diverse contexts, whether by providing a global interpretation of a phenomenon
across the entire dataset or by offering a localized explanation for the
outcome of a specific data point. Furthermore, feature importance is being used
both for explaining models and for identifying plausible causal relations in
the data, independently from the model. However, it is worth noting that these
various contexts have traditionally been explored in isolation, with limited
theoretical foundations.
</p>
<p>This paper presents an axiomatic framework designed to establish coherent
relationships among the different contexts of feature importance scores.
Notably, our work unveils a surprising conclusion: when we combine the proposed
properties with those previously outlined in the literature, we demonstrate the
existence of an inconsistency. This inconsistency highlights that certain
essential properties of feature importance scores cannot coexist harmoniously
within a single framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.08225">All the World&#x27;s a (Hyper)Graph: A Data Drama. (arXiv:2206.08225v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1">Corinna Coupette</a>, <a href="http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1">Jilles Vreeken</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1">Bastian Rieck</a></p>
<p>We introduce Hyperbard, a dataset of diverse relational data representations
derived from Shakespeare's plays. Our representations range from simple graphs
capturing character co-occurrence in single scenes to hypergraphs encoding
complex communication settings and character contributions as hyperedges with
edge-specific node weights. By making multiple intuitive representations
readily available for experimentation, we facilitate rigorous representation
robustness checks in graph learning, graph mining, and network analysis,
highlighting the advantages and drawbacks of specific representations.
Leveraging the data released in Hyperbard, we demonstrate that many solutions
to popular graph mining problems are highly dependent on the representation
choice, thus calling current graph curation practices into question. As an
homage to our data source, and asserting that science can also be art, we
present all our points in the form of a play.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.10706">TraSE: Towards Tackling Authorial Style from a Cognitive Science Perspective. (arXiv:2206.10706v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilson_R/0/1/0/all/0/1">Ronald Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhandarkar_A/0/1/0/all/0/1">Avanti Bhandarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodard_D/0/1/0/all/0/1">Damon Woodard</a></p>
<p>Stylistic analysis of text is a key task in research areas ranging from
authorship attribution to forensic analysis and personality profiling. The
existing approaches for stylistic analysis are plagued by issues like topic
influence, lack of discriminability for large number of authors and the
requirement for large amounts of diverse data. In this paper, the source of
these issues are identified along with the necessity for a cognitive
perspective on authorial style in addressing them. A novel feature
representation, called Trajectory-based Style Estimation (TraSE), is introduced
to support this purpose. Authorship attribution experiments with over 27,000
authors and 1.4 million samples in a cross-domain scenario resulted in 90%
attribution accuracy suggesting that the feature representation is immune to
such negative influences and an excellent candidate for stylistic analysis.
Finally, a qualitative analysis is performed on TraSE using physical human
characteristics, like age, to validate its claim on capturing cognitive traits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.03932">Memory-free Online Change-point Detection: A Novel Neural Network Approach. (arXiv:2207.03932v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1">Zahra Atashgahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1">Decebal Constantin Mocanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Veldhuis_R/0/1/0/all/0/1">Raymond Veldhuis</a>, <a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1">Mykola Pechenizkiy</a></p>
<p>Change-point detection (CPD), which detects abrupt changes in the data
distribution, is recognized as one of the most significant tasks in time series
analysis. Despite the extensive literature on offline CPD, unsupervised online
CPD still suffers from major challenges, including scalability, hyperparameter
tuning, and learning constraints. To mitigate some of these challenges, in this
paper, we propose a novel deep learning approach for unsupervised online CPD
from multi-dimensional time series, named Adaptive LSTM-Autoencoder
Change-Point Detection (ALACPD). ALACPD exploits an LSTM-autoencoder-based
neural network to perform unsupervised online CPD. It continuously adapts to
the incoming samples without keeping the previously received input, thus being
memory-free. We perform an extensive evaluation on several real-world time
series CPD benchmarks. We show that ALACPD, on average, ranks first among
state-of-the-art CPD algorithms in terms of quality of the time series
segmentation, and it is on par with the best performer in terms of the accuracy
of the estimated change-points. The implementation of ALACPD is available
online on Github\footnote{\url{https://github.com/zahraatashgahi/ALACPD}}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.12835">Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Barp_A/0/1/0/all/0/1">Alessandro Barp</a>, <a href="http://arxiv.org/find/stat/1/au:+Simon_Gabriel_C/0/1/0/all/0/1">Carl-Johann Simon-Gabriel</a>, <a href="http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1">Mark Girolami</a>, <a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1">Lester Mackey</a></p>
<p>Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD)
have grown central to a wide range of applications, including hypothesis
testing, sampler selection, distribution approximation, and variational
inference. In each setting, these kernel-based discrepancy measures are
required to (i) separate a target P from other probability measures or even
(ii) control weak convergence to P. In this article we derive new sufficient
and necessary conditions to ensure (i) and (ii). For MMDs on separable metric
spaces, we characterize those kernels that separate Bochner embeddable measures
and introduce simple conditions for separating all measures with unbounded
kernels and for controlling convergence with bounded kernels. We use these
results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD
separation and convergence control and to develop the first KSDs known to
exactly metrize weak convergence to P. Along the way, we highlight the
implications of our results for hypothesis testing, measuring and improving
sample quality, and sampling with Stein variational gradient descent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13179">A simple probabilistic neural network for machine understanding. (arXiv:2210.13179v5 [cond-mat.dis-nn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Xie_R/0/1/0/all/0/1">Rongrong Xie</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Marsili_M/0/1/0/all/0/1">Matteo Marsili</a></p>
<p>We discuss probabilistic neural networks with a fixed internal representation
as models for machine understanding. Here understanding is intended as mapping
data to an already existing representation which encodes an {\em a priori}
organisation of the feature space. We derive the internal representation by
requiring that it satisfies the principles of maximal relevance and of maximal
ignorance about how different features are combined. We show that, when hidden
units are binary variables, these two principles identify a unique model -- the
Hierarchical Feature Model (HFM) -- which is fully solvable and provides a
natural interpretation in terms of features. We argue that learning machines
with this architecture enjoy a number of interesting properties, like the
continuity of the representation with respect to changes in parameters and
data, the possibility to control the level of compression and the ability to
support functions that go beyond generalisation. We explore the behaviour of
the model with extensive numerical experiments and argue that models where the
internal representation is fixed reproduce a learning modality which is
qualitatively different from that of traditional models such as Restricted
Boltzmann Machines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.16380">Incorporating Crowdsourced Annotator Distributions into Ensemble Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri. (arXiv:2210.16380v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+West_G/0/1/0/all/0/1">Graham West</a>, <a href="http://arxiv.org/find/cs/1/au:+Swindall_M/0/1/0/all/0/1">Matthew I. Swindall</a>, <a href="http://arxiv.org/find/cs/1/au:+Keener_B/0/1/0/all/0/1">Ben Keener</a>, <a href="http://arxiv.org/find/cs/1/au:+Player_T/0/1/0/all/0/1">Timothy Player</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Alex C. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Brusuelas_J/0/1/0/all/0/1">James H. Brusuelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallin_J/0/1/0/all/0/1">John F. Wallin</a></p>
<p>Performing classification on noisy, crowdsourced image datasets can prove
challenging even for the best neural networks. Two issues which complicate the
problem on such datasets are class imbalance and ground-truth uncertainty in
labeling. The AL-ALL and AL-PUB datasets - consisting of tightly cropped,
individual characters from images of ancient Greek papyri - are strongly
affected by both issues. The application of ensemble modeling to such datasets
can help identify images where the ground-truth is questionable and quantify
the trustworthiness of those samples. As such, we apply stacked generalization
consisting of nearly identical ResNets with different loss functions: one
utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence
(KLD). Both networks use labels drawn from a crowd-sourced consensus. This
consensus is derived from a Normalized Distribution of Annotations (NDA) based
on all annotations for a given character in the dataset. For the second
network, the KLD is calculated with respect to the NDA. For our ensemble model,
we apply a k-nearest neighbors model to the outputs of the CXE and KLD
networks. Individually, the ResNet models have approximately 93% accuracy,
while the ensemble model achieves an accuracy of &gt; 95%, increasing the
classification trustworthiness. We also perform an analysis of the Shannon
entropy of the various models' output distributions to measure classification
uncertainty. Our results suggest that entropy is useful for predicting model
misclassifications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04223">Vicious Classifiers: Data Reconstruction Attack at Inference Time. (arXiv:2212.04223v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1">Mohammad Malekzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1">Deniz Gunduz</a></p>
<p>Privacy-preserving inference in edge computing paradigms encourages the users
of machine-learning services to locally run a model on their private input, for
a target task, and only share the model's outputs with the server. We study how
a vicious server can reconstruct the input data by observing only the model's
outputs, while keeping the target accuracy very close to that of a honest
server: by jointly training a target model (to run at users' side) and an
attack model for data reconstruction (to secretly use at server's side). We
present a new measure to assess the reconstruction risk in edge inference. Our
evaluations on six benchmark datasets demonstrate that the model's input can be
approximately reconstructed from the outputs of a single target inference. We
propose a potential defense mechanism that helps to distinguish vicious versus
honest classifiers at inference time. We discuss open challenges and directions
for future studies and release our code as a benchmark for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.07624">Neuroevolution of Physics-Informed Neural Nets: Benchmark Problems and Comparative Results. (arXiv:2212.07624v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yong_N/0/1/0/all/0/1">Nicholas Sung Wei Yong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1">Jian Cheng Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_P/0/1/0/all/0/1">Pao-Hsiung Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhishek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ooi_C/0/1/0/all/0/1">Chinchun Ooi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1">Yew-Soon Ong</a></p>
<p>The potential of learned models for fundamental scientific research and
discovery is drawing increasing attention worldwide. Physics-informed neural
networks (PINNs), where the loss function directly embeds governing equations
of scientific phenomena, is one of the key techniques at the forefront of
recent advances. PINNs are typically trained using stochastic gradient descent
methods, akin to their deep learning counterparts. However, analysis in this
paper shows that PINNs' unique loss formulations lead to a high degree of
complexity and ruggedness that may not be conducive for gradient descent.
Unlike in standard deep learning, PINN training requires globally optimum
parameter values that satisfy physical laws as closely as possible. Spurious
local optimum, indicative of erroneous physics, must be avoided. Hence,
neuroevolution algorithms, with their superior global search capacity, may be a
better choice for PINNs relative to gradient descent methods. Here, we propose
a set of five benchmark problems, with open-source codes, spanning diverse
physical phenomena for novel neuroevolution algorithm development. Using this,
we compare two neuroevolution algorithms against the commonly used stochastic
gradient descent, and our baseline results support the claim that
neuroevolution can surpass gradient descent, ensuring better physics compliance
in the predicted outputs. %Furthermore, implementing neuroevolution with JAX
leads to orders of magnitude speedup relative to standard implementations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09710">Continual Learning for Instruction Following from Realtime Feedback. (arXiv:2212.09710v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1">Alane Suhr</a>, <a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1">Yoav Artzi</a></p>
<p>We propose and deploy an approach to continually train an
instruction-following agent from feedback provided by users during
collaborative interactions. During interaction, human users instruct an agent
using natural language, and provide realtime binary feedback as they observe
the agent following their instructions. We design a contextual bandit learning
approach, converting user feedback to immediate reward. We evaluate through
thousands of human-agent interactions, demonstrating 15.4% absolute improvement
in instruction execution accuracy over time. We also show our approach is
robust to several design variations, and that the feedback signal is roughly
equivalent to the learning signal of supervised demonstration data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00845">CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training. (arXiv:2302.00845v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1">A. Feder Cooper</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wentao Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_K/0/1/0/all/0/1">Khiem Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_T/0/1/0/all/0/1">Tiancheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_C/0/1/0/all/0/1">Charlie F. Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yucheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1">Christopher De Sa</a></p>
<p>Recent research on online Gradient Balancing (GraB) has revealed that there
exist permutation-based example orderings for SGD that are guaranteed to
outperform random reshuffling (RR). Whereas RR arbitrarily permutes training
examples, GraB leverages stale gradients from prior epochs to order examples --
achieving a provably faster convergence rate than RR. However, GraB is limited
by design: while it demonstrates an impressive ability to scale-up training on
centralized data, it does not naturally extend to modern distributed ML
workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which
uses insights from prior work on kernel thinning to translate the benefits of
provably faster permutation-based example ordering to distributed settings.
With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate
over centralized GraB and outperforms distributed RR on a variety of benchmark
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02560">Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US. (arXiv:2302.02560v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tec_M/0/1/0/all/0/1">Mauricio Tec</a>, <a href="http://arxiv.org/find/cs/1/au:+Mudele_O/0/1/0/all/0/1">Oladimeji Mudele</a>, <a href="http://arxiv.org/find/cs/1/au:+Josey_K/0/1/0/all/0/1">Kevin Josey</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominici_F/0/1/0/all/0/1">Francesca Dominici</a></p>
<p>In policy research, one of the most critical analytic tasks is to estimate
the causal effect of a policy-relevant shift to the distribution of a
continuous exposure/treatment on an outcome of interest. We call this problem
shift-response function (SRF) estimation. Existing neural network methods
involving robust causal-effect estimators lack theoretical guarantees and
practical implementations for SRF estimation. Motivated by a key
policy-relevant question in public health, we develop a neural network method
and its theoretical underpinnings to estimate SRFs with robustness and
efficiency guarantees. We then apply our method to data consisting of 68
million individuals and 27 million deaths across the U.S. to estimate the
causal effect from revising the US National Ambient Air Quality Standards
(NAAQS) for PM 2.5 from 12 $\mu g/m^3$ to 9 $\mu g/m^3$. This change has been
recently proposed by the US Environmental Protection Agency (EPA). Our goal is
to estimate, for the first time, the reduction in deaths that would result from
this anticipated revision using causal methods for SRFs. Our proposed method,
called {T}argeted {R}egularization for {E}xposure {S}hifts with Neural
{Net}works (TRESNET), contributes to the neural network literature for causal
inference in two ways: first, it proposes a targeted regularization loss with
theoretical properties that ensure double robustness and achieves asymptotic
efficiency specific for SRF estimation; second, it enables loss functions from
the exponential family of distributions to accommodate non-continuous outcome
distributions (such as hospitalization or mortality counts). We complement our
application with benchmark experiments that demonstrate TRESNET's broad
applicability and competitiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07320">Model-tuning Via Prompts Makes NLP Models Adversarially Robust. (arXiv:2303.07320v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1">Mrigank Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1">Pratyush Maini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a>, <a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1">Danish Pruthi</a></p>
<p>In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token's hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP-FT). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations. In
this work, we demonstrate surprising gains in adversarial robustness enjoyed by
Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream
tasks. Rather than appending an MLP head to make output prediction, MVP appends
a prompt template to the input, and makes prediction via text
infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3
different models, MVP improves performance against adversarial substitutions by
an average of 8% over standard methods and even outperforms adversarial
training-based state-of-art defenses by 3.5%. By combining MVP with adversarial
training, we achieve further improvements in adversarial robustness while
maintaining performance on unperturbed examples. Finally, we conduct ablations
to investigate the mechanism underlying these gains. Notably, we find that the
main causes of vulnerability of MLP-FT can be attributed to the misalignment
between pre-training and fine-tuning tasks, and the randomly initialized MLP
parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09128">Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arakelyan_S/0/1/0/all/0/1">Shushan Arakelyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1">Rocktim Jyoti Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a></p>
<p>We systematically study how three large language models with code
capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data.
We consider two fundamental applications - code summarization, and code
generation. We split data into domains following its natural boundaries - by an
organization, by a project, and by a module within the software project. We
establish that samples from each new domain present all the models with a
significant challenge of distribution shift. We study how established methods
adapt models to better generalize to new domains. Our experiments show that
while multitask learning alone is a reasonable baseline, combining it with
few-shot finetuning on examples retrieved from training data can achieve very
strong performance. Moreover, this solution can outperform direct finetuning
for very low-data scenarios. Finally, we consider variations of this approach
to create a more broadly applicable method to adapt to multiple domains at
once. We find that for code generation, a model adapted to multiple domains
simultaneously performs on par with those adapted to a single domain
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04934">Model Sparsity Can Simplify Machine Unlearning. (arXiv:2304.04934v11 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinghan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiancheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1">Parikshit Ram</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuguang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gaowen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pranay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a></p>
<p>In response to recent data regulation requirements, machine unlearning (MU)
has emerged as a critical process to remove the influence of specific examples
from a given model. Although exact unlearning can be achieved through complete
model retraining using the remaining dataset, the associated computational
costs have driven the development of efficient, approximate unlearning
techniques. Moving beyond data-centric MU approaches, our study introduces a
novel model-based perspective: model sparsification via weight pruning, which
is capable of reducing the gap between exact unlearning and approximate
unlearning. We show in both theory and practice that model sparsity can boost
the multi-criteria unlearning performance of an approximate unlearner, closing
the approximation gap, while continuing to be efficient. This leads to a new MU
paradigm, termed prune first, then unlearn, which infuses a sparse model prior
into the unlearning process. Building on this insight, we also develop a
sparsity-aware unlearning method that utilizes sparsity regularization to
enhance the training process of approximate unlearning. Extensive experiments
show that our proposals consistently benefit MU in various unlearning
scenarios. A notable highlight is the 77% unlearning efficacy gain of
fine-tuning (one of the simplest unlearning methods) when using sparsity-aware
unlearning. Furthermore, we demonstrate the practical impact of our proposed MU
methods in addressing other machine learning challenges, such as defending
against backdoor attacks and enhancing transfer learning. Codes are available
at https://github.com/OPTML-Group/Unlearn-Sparse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14176">Exploring the flavor structure of quarks and leptons with reinforcement learning. (arXiv:2304.14176v2 [hep-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Nishimura_S/0/1/0/all/0/1">Satsuki Nishimura</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Miyao_C/0/1/0/all/0/1">Coh Miyao</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Otsuka_H/0/1/0/all/0/1">Hajime Otsuka</a></p>
<p>We propose a method to explore the flavor structure of quarks and leptons
with reinforcement learning. As a concrete model, we utilize a basic
value-based algorithm for models with $U(1)$ flavor symmetry. By training
neural networks on the $U(1)$ charges of quarks and leptons, the agent finds 21
models to be consistent with experimentally measured masses and mixing angles
of quarks and leptons. In particular, an intrinsic value of normal ordering
tends to be larger than that of inverted ordering, and the normal ordering is
well fitted with the current experimental data in contrast to the inverted
ordering. A specific value of effective mass for the neutrinoless double beta
decay and a sizable leptonic CP violation induced by an angular component of
flavon field are predicted by autonomous behavior of the agent. Our finding
results indicate that the reinforcement learning can be a new method for
understanding the flavor structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01090">Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems. (arXiv:2305.01090v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1">Kevin Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jesus_C/0/1/0/all/0/1">Carlos E. P&#xe9;rez De Jes&#xfa;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_A/0/1/0/all/0/1">Andrew J. Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1">Michael D. Graham</a></p>
<p>While many phenomena in physics and engineering are formally
high-dimensional, their long-time dynamics often live on a lower-dimensional
manifold. The present work introduces an autoencoder framework that combines
implicit regularization with internal linear layers and $L_2$ regularization
(weight decay) to automatically estimate the underlying dimensionality of a
data set, produce an orthogonal manifold coordinate system, and provide the
mapping functions between the ambient space and manifold space, allowing for
out-of-sample projections. We validate our framework's ability to estimate the
manifold dimension for a series of datasets from dynamical systems of varying
complexities and compare to other state-of-the-art estimators. We analyze the
training dynamics of the network to glean insight into the mechanism of
low-rank learning and find that collectively each of the implicit regularizing
layers compound the low-rank representation and even self-correct during
training. Analysis of gradient descent dynamics for this architecture in the
linear case reveals the role of the internal linear layers in leading to faster
decay of a "collective weight variable" incorporating all layers, and the role
of weight decay in breaking degeneracies and thus driving convergence along
directions in which no decay would occur in its absence. We show that this
framework can be naturally extended for applications of state-space modeling
and forecasting by generating a data-driven dynamic model of a spatiotemporally
chaotic partial differential equation using only the manifold coordinates.
Finally, we demonstrate that our framework is robust to hyperparameter choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11042">A unified framework for information-theoretic generalization bounds. (arXiv:2305.11042v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1">Yifeng Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Raginsky_M/0/1/0/all/0/1">Maxim Raginsky</a></p>
<p>This paper presents a general methodology for deriving information-theoretic
generalization bounds for learning algorithms. The main technical tool is a
probabilistic decorrelation lemma based on a change of measure and a relaxation
of Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelation
lemma in combination with other techniques, such as symmetrization, couplings,
and chaining in the space of probability measures, we obtain new upper bounds
on the generalization error, both in expectation and in high probability, and
recover as special cases many of the existing generalization bounds, including
the ones based on mutual information, conditional mutual information,
stochastic chaining, and PAC-Bayes inequalities. In addition, the
Fernique-Talagrand upper bound on the expected supremum of a subgaussian
process emerges as a special case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14164">Improved Convergence of Score-Based Diffusion Models via Prediction-Correction. (arXiv:2305.14164v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pedrotti_F/0/1/0/all/0/1">Francesco Pedrotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Maas_J/0/1/0/all/0/1">Jan Maas</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondelli_M/0/1/0/all/0/1">Marco Mondelli</a></p>
<p>Score-based generative models (SGMs) are powerful tools to sample from
complex data distributions. Their underlying idea is to (i) run a forward
process for time $T_1$ by adding noise to the data, (ii) estimate its score
function, and (iii) use such estimate to run a reverse process. As the reverse
process is initialized with the stationary distribution of the forward one, the
existing analysis paradigm requires $T_1\to\infty$. This is however
problematic: from a theoretical viewpoint, for a given precision of the score
approximation, the convergence guarantee fails as $T_1$ diverges; from a
practical viewpoint, a large $T_1$ increases computational costs and leads to
error propagation. This paper addresses the issue by considering a version of
the popular predictor-corrector scheme: after running the forward process, we
first estimate the final distribution via an inexact Langevin dynamics and then
revert the process. Our key technical contribution is to provide convergence
guarantees which require to run the forward process only for a fixed finite
time $T_1$. Our bounds exhibit a mild logarithmic dependence on the input
dimension and the subgaussian norm of the target distribution, have minimal
assumptions on the data, and require only to control the $L^2$ loss on the
score approximation, which is the quantity minimized in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14387">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1">Yann Dubois</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuechen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1">Rohan Taori</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1">Ishaan Gulrajani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1">Jimmy Ba</a>, <a href="http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1">Carlos Guestrin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1">Tatsunori B. Hashimoto</a></p>
<p>Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their ability to follow user instructions well. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following process
faces three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 45x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15136">ReSync: Riemannian Subgradient-based Robust Rotation Synchronization. (arXiv:2305.15136v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Liu_H/0/1/0/all/0/1">Huikang Liu</a>, <a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1">Xiao Li</a>, <a href="http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1">Anthony Man-Cho So</a></p>
<p>This work presents ReSync, a Riemannian subgradient-based algorithm for
solving the robust rotation synchronization problem, which arises in various
engineering applications. ReSync solves a least-unsquared minimization
formulation over the rotation group, which is nonsmooth and nonconvex, and aims
at recovering the underlying rotations directly. We provide strong theoretical
guarantees for ReSync under the random corruption setting. Specifically, we
first show that the initialization procedure of ReSync yields a proper initial
point that lies in a local region around the ground-truth rotations. We next
establish the weak sharpness property of the aforementioned formulation and
then utilize this property to derive the local linear convergence of ReSync to
the ground-truth rotations. By combining these guarantees, we conclude that
ReSync converges linearly to the ground-truth rotations under appropriate
conditions. Experiment results demonstrate the effectiveness of ReSync.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15613">Learning Deep O($n$)-Equivariant Hyperspheres. (arXiv:2305.15613v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1">Pavlo Melnyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1">Michael Felsberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1">M&#xe5;rten Wadenb&#xe4;ck</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_A/0/1/0/all/0/1">Andreas Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1">Cuong Le</a></p>
<p>This paper presents an approach to learning (deep) $n$D features equivariant
under orthogonal transformations, utilizing hyperspheres and regular
$n$-simplexes. Our main contributions are theoretical and tackle major
challenges in geometric deep learning such as equivariance and invariance under
geometric transformations. Namely, we enrich the recently developed theory of
steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on
neurons with spherical decision surfaces -- by extending said neurons to $n$D,
which we call deep equivariant hyperspheres, and enabling their multi-layer
construction. Using synthetic and real-world data in $n$D, we experimentally
verify our theoretical contributions and find that our approach is superior to
the competing methods for benchmark datasets in all but one case, additionally
demonstrating a better speed/performance trade-off in all but one other case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16498">Coherent Soft Imitation Learning. (arXiv:2305.16498v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1">Joe Watson</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Sandy H. Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1">Nicolas Heess</a></p>
<p>Imitation learning methods seek to learn from an expert either through
behavioral cloning (BC) of the policy or inverse reinforcement learning (IRL)
of the reward. Such methods enable agents to learn complex tasks from humans
that are difficult to capture with hand-designed reward functions. Choosing BC
or IRL for imitation depends on the quality and state-action coverage of the
demonstrations, as well as additional access to the Markov decision process.
Hybrid strategies that combine BC and IRL are not common, as initial policy
optimization against inaccurate rewards diminishes the benefit of pretraining
the policy with BC. This work derives an imitation method that captures the
strengths of both BC and IRL. In the entropy-regularized ('soft') reinforcement
learning setting, we show that the behaviour-cloned policy can be used as both
a shaped reward and a critic hypothesis space by inverting the regularized
policy update. This coherency facilitates fine-tuning cloned policies using the
reward estimate and additional interactions with the environment. This approach
conveniently achieves imitation learning through initial behaviour cloning,
followed by refinement via RL with online or offline data sources. The
simplicity of the approach enables graceful scaling to high-dimensional and
vision-based tasks, with stable learning and minimal hyperparameter tuning, in
contrast to adversarial approaches. For the open-source implementation and
simulation results, see https://joemwatson.github.io/csil/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17209">Functional Flow Matching. (arXiv:2305.17209v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kerrigan_G/0/1/0/all/0/1">Gavin Kerrigan</a>, <a href="http://arxiv.org/find/cs/1/au:+Migliorini_G/0/1/0/all/0/1">Giosue Migliorini</a>, <a href="http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1">Padhraic Smyth</a></p>
<p>We propose Functional Flow Matching (FFM), a function-space generative model
that generalizes the recently-introduced Flow Matching model to operate in
infinite-dimensional spaces. Our approach works by first defining a path of
probability measures that interpolates between a fixed Gaussian measure and the
data distribution, followed by learning a vector field on the underlying space
of functions that generates this path of measures. Our method does not rely on
likelihoods or simulations, making it well-suited to the function space
setting. We provide both a theoretical framework for building such models and
an empirical evaluation of our techniques. We demonstrate through experiments
on several real-world benchmarks that our proposed FFM method outperforms
several recently proposed function-space generative models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17390">SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks. (arXiv:2305.17390v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bill Yuchen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yicheng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Karina Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shiyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1">Chandra Bhagavatula</a>, <a href="http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1">Prithviraj Ammanabrolu</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a></p>
<p>We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent's action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18411">Feature-Learning Networks Are Consistent Across Widths At Realistic Scales. (arXiv:2305.18411v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vyas_N/0/1/0/all/0/1">Nikhil Vyas</a>, <a href="http://arxiv.org/find/cs/1/au:+Atanasov_A/0/1/0/all/0/1">Alexander Atanasov</a>, <a href="http://arxiv.org/find/cs/1/au:+Bordelon_B/0/1/0/all/0/1">Blake Bordelon</a>, <a href="http://arxiv.org/find/cs/1/au:+Morwani_D/0/1/0/all/0/1">Depen Morwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sainathan_S/0/1/0/all/0/1">Sabarish Sainathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a></p>
<p>We study the effect of width on the dynamics of feature-learning neural
networks across a variety of architectures and datasets. Early in training,
wide neural networks trained on online data have not only identical loss curves
but also agree in their point-wise test predictions throughout training. For
simple tasks such as CIFAR-5m this holds throughout training for networks of
realistic widths. We also show that structural properties of the models,
including internal representations, preactivation distributions, edge of
stability phenomena, and large learning rate effects are consistent across
large widths. This motivates the hypothesis that phenomena seen in realistic
models can be captured by infinite-width, feature-learning limits. For harder
tasks (such as ImageNet and language modeling), and later training times,
finite-width deviations grow systematically. Two distinct effects cause these
deviations across widths. First, the network output has
initialization-dependent variance scaling inversely with width, which can be
removed by ensembling networks. We observe, however, that ensembles of narrower
networks perform worse than a single wide network. We call this the bias of
narrower width. We conclude with a spectral perspective on the origin of this
finite-width bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01546">Publicly available datasets of breast histopathology H&amp;E whole-slide images: A scoping review. (arXiv:2306.01546v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tafavvoghi_M/0/1/0/all/0/1">Masoud Tafavvoghi</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Bongo_L/0/1/0/all/0/1">Lars Ailo Bongo</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Shvetsov_N/0/1/0/all/0/1">Nikita Shvetsov</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Busund_L/0/1/0/all/0/1">Lill-Tove Rasmussen Busund</a> (3), <a href="http://arxiv.org/find/eess/1/au:+Mollersen_K/0/1/0/all/0/1">Kajsa M&#xf8;llersen</a> (1) ((1) Department of Community Medicine, UiT The Arctic University of Norway, Troms&#xf8;, Norway, (2) Department of Computer Science, UiT The Arctic University of Norway, Troms&#xf8;, Norway, (3) Department of Medical Biology, UiT The Arctic University of Norway, Troms&#xf8;, Norway)</p>
<p>Advancements in digital pathology and computing resources have made a
significant impact in the field of computational pathology for breast cancer
diagnosis and treatment. However, access to high-quality labeled
histopathological images of breast cancer is a big challenge that limits the
development of accurate and robust deep learning models. In this scoping
review, we identified the publicly available datasets of breast H&amp;E stained
whole-slide images (WSI) that can be used to develop deep learning algorithms.
We systematically searched nine scientific literature databases and nine
research data repositories and found 17 publicly available datasets containing
10385 H&amp;E WSIs of breast cancer. Moreover, we reported image metadata and
characteristics for each dataset to assist researchers in selecting proper
datasets for specific tasks in breast cancer computational pathology. In
addition, we compiled two lists of breast H&amp;E patches and private datasets as
supplementary resources for researchers. Notably, only 28% of the included
articles utilized multiple datasets, and only 14% used an external validation
set, suggesting that the performance of other developed models may be
susceptible to overestimation. The TCGA-BRCA was used in 52% of the selected
studies. This dataset has a considerable selection bias that can impact the
robustness and generalizability of the trained algorithms. There is also a lack
of consistent metadata reporting of breast WSI datasets that can be an issue in
developing accurate deep learning models, indicating the necessity of
establishing explicit guidelines for documenting breast WSI dataset
characteristics and metadata.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08274">A Simple and Scalable Graph Neural Network for Large Directed Graphs. (arXiv:2306.08274v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maekawa_S/0/1/0/all/0/1">Seiji Maekawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1">Yuya Sasaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Onizuka_M/0/1/0/all/0/1">Makoto Onizuka</a></p>
<p>Node classification is one of the hottest tasks in graph analysis. Though
existing studies have explored various node representations in directed and
undirected graphs, they have overlooked the distinctions of their capabilities
to capture the information of graphs. To tackle the limitation, we investigate
various combinations of node representations (aggregated features vs. adjacency
lists) and edge direction awareness within an input graph (directed vs.
undirected). We address the first empirical study to benchmark the performance
of various GNNs that use either combination of node representations and edge
direction awareness. Our experiments demonstrate that no single combination
stably achieves state-of-the-art results across datasets, which indicates that
we need to select appropriate combinations depending on the dataset
characteristics. In response, we propose a simple yet holistic classification
method A2DUG which leverages all combinations of node representations in
directed and undirected graphs. We demonstrate that A2DUG stably performs well
on various datasets and improves the accuracy up to 11.29 compared with the
state-of-the-art methods. To spur the development of new methods, we publicly
release our complete codebase under the MIT license.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11475">Delegated Classification. (arXiv:2306.11475v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saig_E/0/1/0/all/0/1">Eden Saig</a>, <a href="http://arxiv.org/find/cs/1/au:+Talgam_Cohen_I/0/1/0/all/0/1">Inbal Talgam-Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_N/0/1/0/all/0/1">Nir Rosenfeld</a></p>
<p>When machine learning is outsourced to a rational agent, conflicts of
interest might arise and severely impact predictive performance. In this work,
we propose a theoretical framework for incentive-aware delegation of machine
learning tasks. We model delegation as a principal-agent game, in which
accurate learning can be incentivized by the principal using performance-based
contracts. Adapting the economic theory of contract design to this setting, we
define budget-optimal contracts and prove they take a simple threshold form
under reasonable assumptions. In the binary-action case, the optimality of such
contracts is shown to be equivalent to the classic Neyman-Pearson lemma,
establishing a formal connection between contract design and statistical
hypothesis testing. Empirically, we demonstrate that budget-optimal contracts
can be constructed using small-scale data, leveraging recent advances in the
study of learning curves and scaling laws. Performance and economic outcomes
are evaluated using synthetic and real-world classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13255">Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models. (arXiv:2306.13255v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">David X. Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahai_A/0/1/0/all/0/1">Anant Sahai</a></p>
<p>We study the asymptotic generalization of an overparameterized linear model
for multiclass classification under the Gaussian covariates bi-level model
introduced in Subramanian et al.~'22, where the number of data points,
features, and classes all grow together. We fully resolve the conjecture posed
in Subramanian et al.~'22, matching the predicted regimes for generalization.
Furthermore, our new lower bounds are akin to an information-theoretic strong
converse: they establish that the misclassification rate goes to 0 or 1
asymptotically. One surprising consequence of our tight results is that the
min-norm interpolating classifier can be asymptotically suboptimal relative to
noninterpolating classifiers in the regime where the min-norm interpolating
regressor is known to be optimal.
</p>
<p>The key to our tight analysis is a new variant of the Hanson-Wright
inequality which is broadly useful for multiclass problems with sparse labels.
As an application, we show that the same type of analysis can be used to
analyze the related multilabel classification problem under the same bi-level
ensemble.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13824">Adaptive Privacy Composition for Accuracy-first Mechanisms. (arXiv:2306.13824v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rogers_R/0/1/0/all/0/1">Ryan Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Samorodnitsky_G/0/1/0/all/0/1">Gennady Samorodnitsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a></p>
<p>In many practical applications of differential privacy, practitioners seek to
provide the best privacy guarantees subject to a target level of accuracy. A
recent line of work by Ligett et al. '17 and Whitehouse et al. '22 has
developed such accuracy-first mechanisms by leveraging the idea of noise
reduction that adds correlated noise to the sufficient statistic in a private
computation and produces a sequence of increasingly accurate answers. A major
advantage of noise reduction mechanisms is that the analysts only pay the
privacy cost of the least noisy or most accurate answer released. Despite this
appealing property in isolation, there has not been a systematic study on how
to use them in conjunction with other differentially private mechanisms. A
fundamental challenge is that the privacy guarantee for noise reduction
mechanisms is (necessarily) formulated as ex-post privacy that bounds the
privacy loss as a function of the released outcome. Furthermore, there has yet
to be any study on how ex-post private mechanisms compose, which allows us to
track the accumulated privacy over several mechanisms. We develop privacy
filters [Rogers et al. '16, Feldman and Zrnic '21, and Whitehouse et al. '22']
that allow an analyst to adaptively switch between differentially private and
ex-post private mechanisms subject to an overall differential privacy
guarantee.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14468">A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1">Nuoya Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a></p>
<p>We take the first step in studying general sequential decision-making under
two adaptivity constraints: rare policy switch and batch learning. First, we
provide a general class called the Eluder Condition class, which includes a
wide range of reinforcement learning classes. Then, for the rare policy switch
constraint, we provide a generic algorithm to achieve a
$\widetilde{\mathcal{O}}(\log K) $ switching cost with a
$\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch
learning constraint, we provide an algorithm that provides a
$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$
This paper is the first work considering rare policy switch and batch learning
under general function classes, which covers nearly all the models studied in
the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020),
linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong
et al. 2021; Gao et al. 2021), generalized linear function approximation (Qiao
et al. 2023), and also some new classes such as the low $D_\Delta$-type Bellman
eluder dimension problem, linear mixture MDP, kernelized nonlinear regulator
and undercomplete partially observed Markov decision process (POMDP).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16111">Time Regularization in Optimal Time Variable Learning. (arXiv:2306.16111v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herberg_E/0/1/0/all/0/1">Evelyn Herberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzog_R/0/1/0/all/0/1">Roland Herzog</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohne_F/0/1/0/all/0/1">Frederik K&#xf6;hne</a></p>
<p>Recently, optimal time variable learning in deep neural networks (DNNs) was
introduced in <a href="/abs/2204.08528">arXiv:2204.08528</a>. In this manuscript we extend the concept by
introducing a regularization term that directly relates to the time horizon in
discrete dynamical systems. Furthermore, we propose an adaptive pruning
approach for Residual Neural Networks (ResNets), which reduces network
complexity without compromising expressiveness, while simultaneously decreasing
training time. The results are illustrated by applying the proposed concepts to
classification tasks on the well known MNIST and Fashion MNIST data sets. Our
PyTorch code is available on
https://github.com/frederikkoehne/time_variable_learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03761">DyEdgeGAT: Dynamic Edge via Graph Attention for Early Fault Detection in IIoT Systems. (arXiv:2307.03761v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1">Mengjie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a></p>
<p>In the industrial Internet of Things, condition monitoring sensor signals
from complex systems often exhibit strong nonlinear and stochastic
spatial-temporal dynamics under varying operating conditions. Such complex
dynamics make fault detection particularly challenging. Although previously
proposed methods effectively model these dynamics, they often neglect the
dynamic evolution of relationships between sensor signals. Undetected shifts in
these relationships can potentially result in significant system failures.
Another limitation is their inability to effectively distinguish between novel
operating conditions and actual faults. To address this gap, we propose
DyEdgeGAT (Dynamic Edge via Graph Attention), a novel approach capable of
detecting various faults, especially those characterized by relationship
changes at early stages, while distinguishing faults from novel operating
conditions. DyEdgeGAT is a graph-based framework that provides a novel graph
inference scheme for multivariate time series that dynamically constructs edges
to represent and track the evolution of relationships between time series.
Additionally, it addresses a commonly overlooked aspect: the cause-and-effect
relationships within the system, such as between control inputs and
measurements. By incorporating system-independent variables as contexts of
operating conditions into node dynamics extraction, DyEdgeGAT enhances its
robustness against novel operating conditions. We rigorously evaluate
DyEdgeGAT's performance using both a synthetic dataset, designed to simulate
varying levels of fault severity and a real-world industrial-scale benchmark
containing a variety of fault types with different detection complexities. Our
findings demonstrate that DyEdgeGAT is highly effective in fault detection,
showing particular strength in early fault detection while maintaining
robustness under novel operating conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04749">Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback. (arXiv:2307.04749v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1">Jaskirat Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a></p>
<p>The field of text-conditioned image generation has made unparalleled progress
with the recent advent of latent diffusion models. While remarkable, as the
complexity of given text input increases, the state-of-the-art diffusion models
may still fail in generating images which accurately convey the semantics of
the given prompt. Furthermore, it has been observed that such misalignments are
often left undetected by pretrained multi-modal models such as CLIP. To address
these problems, in this paper we explore a simple yet effective decompositional
approach towards both evaluation and improvement of text-to-image alignment. In
particular, we first introduce a Decompositional-Alignment-Score which given a
complex prompt decomposes it into a set of disjoint assertions. The alignment
of each assertion with generated images is then measured using a VQA model.
Finally, alignment scores for different assertions are combined aposteriori to
give the final text-to-image alignment score. Experimental analysis reveals
that the proposed alignment metric shows significantly higher correlation with
human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also
find that the assertion level alignment scores provide a useful feedback which
can then be used in a simple iterative procedure to gradually increase the
expression of different assertions in the final image outputs. Human user
studies indicate that the proposed approach surpasses previous state-of-the-art
by 8.7% in overall text-to-image alignment accuracy. Project page for our paper
is available at https://1jsingh.github.io/divide-evaluate-and-refine
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09477">Towards Ordinal Data Science. (arXiv:2307.09477v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1">Gerd Stumme</a>, <a href="http://arxiv.org/find/cs/1/au:+Durrschnabel_D/0/1/0/all/0/1">Dominik D&#xfc;rrschnabel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1">Tom Hanika</a></p>
<p>Order is one of the main instruments to measure the relationship between
objects in (empirical) data. However, compared to methods that use numerical
properties of objects, the amount of ordinal methods developed is rather small.
One reason for this is the limited availability of computational resources in
the last century that would have been required for ordinal computations.
Another reason -- particularly important for this line of research -- is that
order-based methods are often seen as too mathematically rigorous for applying
them to real-world data. In this paper, we will therefore discuss different
means for measuring and 'calculating' with ordinal structures -- a specific
class of directed graphs -- and show how to infer knowledge from them. Our aim
is to establish Ordinal Data Science as a fundamentally new research agenda.
Besides cross-fertilization with other cornerstone machine learning and
knowledge representation methods, a broad range of disciplines will benefit
from this endeavor, including, psychology, sociology, economics, web science,
knowledge engineering, scientometrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16273">zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training. (arXiv:2307.16273v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haochen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1">Tonghe Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jason Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyang Zhang</a></p>
<p>The recent advancements in deep learning have brought about significant
changes in various aspects of people's lives. Meanwhile, these rapid
developments have raised concerns about the legitimacy of the training process
of deep neural networks. To protect the intellectual properties of AI
developers, directly examining the training process by accessing the model
parameters and training data is often prohibited for verifiers.
</p>
<p>In response to this challenge, we present zero-knowledge deep learning
(zkDL), an efficient zero-knowledge proof for deep learning training. To
address the long-standing challenge of verifiable computations of
non-linearities in deep learning training, we introduce zkReLU, a specialized
proof for the ReLU activation and its backpropagation. zkReLU turns the
disadvantage of non-arithmetic relations into an advantage, leading to the
creation of FAC4DNN, our specialized arithmetic circuit design for modelling
neural networks. This design aggregates the proofs over different layers and
training steps, without being constrained by their sequential order in the
training process.
</p>
<p>With our new CUDA implementation that achieves full compatibility with the
tensor structures and the aggregated proof design, zkDL enables the generation
of complete and sound proofs in less than a second per batch update for an
8-layer neural network with 10M parameters and a batch size of 64, while
provably ensuring the privacy of data and model parameters. To our best
knowledge, we are not aware of any existing work on zero-knowledge proof of
deep learning training that is scalable to million-size networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13490">TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. (arXiv:2308.13490v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Phothilimthana_P/0/1/0/all/0/1">Phitchaya Mangpo Phothilimthana</a>, <a href="http://arxiv.org/find/cs/1/au:+Abu_El_Haija_S/0/1/0/all/0/1">Sami Abu-El-Haija</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1">Kaidi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fatemi_B/0/1/0/all/0/1">Bahare Fatemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Burrows_M/0/1/0/all/0/1">Mike Burrows</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendis_C/0/1/0/all/0/1">Charith Mendis</a>, <a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1">Bryan Perozzi</a></p>
<p>Precise hardware performance models play a crucial role in code
optimizations. They can assist compilers in making heuristic decisions or aid
autotuners in identifying the optimal configuration for a given program. For
example, the autotuner for XLA, a machine learning compiler, discovered 10-20%
speedup on state-of-the-art models serving substantial production traffic at
Google. Although there exist a few datasets for program performance prediction,
they target small sub-programs such as basic blocks or kernels. This paper
introduces TpuGraphs, a performance prediction dataset on full tensor programs,
represented as computational graphs, running on Tensor Processing Units (TPUs).
Each graph in the dataset represents the main computation of a machine learning
workload, e.g., a training epoch or an inference step. Each data sample
contains a computational graph, a compilation configuration, and the execution
time of the graph when compiled with the configuration. The graphs in the
dataset are collected from open-source machine learning programs, featuring
popular model architectures, e.g., ResNet, EfficientNet, Mask R-CNN, and
Transformer. TpuGraphs provides 25x more graphs than the largest graph property
prediction dataset (with comparable graph sizes), and 770x larger graphs on
average compared to existing performance prediction datasets on machine
learning programs. This graph-level prediction task on large graphs introduces
new challenges in learning, ranging from scalability, training efficiency, to
model quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07992">An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone. (arXiv:2309.07992v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haq_I/0/1/0/all/0/1">Ijaz Ul Haq</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byung Suk Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizzo_D/0/1/0/all/0/1">Donna M. Rizzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Perdrial_J/0/1/0/all/0/1">Julia N Perdrial</a></p>
<p>This paper presents an automated machine learning framework designed to
assist hydrologists in detecting anomalies in time series data generated by
sensors in a research watershed in the northeastern United States critical
zone. The framework specifically focuses on identifying peak-pattern anomalies,
which may arise from sensor malfunctions or natural phenomena. However, the use
of classification methods for anomaly detection poses challenges, such as the
requirement for labeled data as ground truth and the selection of the most
suitable deep learning model for the given task and dataset. To address these
challenges, our framework generates labeled datasets by injecting synthetic
peak patterns into synthetically generated time series data and incorporates an
automated hyperparameter optimization mechanism. This mechanism generates an
optimized model instance with the best architectural and training parameters
from a pool of five selected models, namely Temporal Convolutional Network
(TCN), InceptionTime, MiniRocket, Residual Networks (ResNet), and Long
Short-Term Memory (LSTM). The selection is based on the user's preferences
regarding anomaly detection accuracy and computational cost. The framework
employs Time-series Generative Adversarial Networks (TimeGAN) as the synthetic
dataset generator. The generated model instances are evaluated using a
combination of accuracy and computational cost metrics, including training time
and memory, during the anomaly detection process. Performance evaluation of the
framework was conducted using a dataset from a watershed, demonstrating
consistent selection of the most fitting model instance that satisfies the
user's preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10954">In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Milios_A/0/1/0/all/0/1">Aristides Milios</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1">Dzmitry Bahdanau</a></p>
<p>In-context learning (ICL) using large language models for tasks with many
labels is challenging due to the limited context window, which makes it
difficult to fit a sufficient number of examples in the prompt. In this paper,
we use a pre-trained dense retrieval model to bypass this limitation, giving
the model only a partial view of the full label space for each inference call.
Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art
performance in few-shot settings for three common intent classification
datasets, with no finetuning. We also surpass fine-tuned performance on
fine-grained sentiment classification in certain cases. We analyze the
performance across number of in-context examples and different model scales,
showing that larger models are necessary to effectively and consistently make
use of larger context lengths for ICL. By running several ablations, we analyze
the model's use of: a) the similarity of the in-context examples to the current
input, b) the semantic content of the class names, and c) the correct
correspondence between examples and labels. We demonstrate that all three are
needed to varying degrees depending on the domain, contrary to certain recent
works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14209">Continual Driving Policy Optimization with Closed-Loop Individualized Curricula. (arXiv:2309.14209v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1">Haoyi Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yizhou Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xingjian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jianming Hu</a></p>
<p>The safety of autonomous vehicles (AV) has been a long-standing top concern,
stemming from the absence of rare and safety-critical scenarios in the
long-tail naturalistic driving distribution. To tackle this challenge, a surge
of research in scenario-based autonomous driving has emerged, with a focus on
generating high-risk driving scenarios and applying them to conduct
safety-critical testing of AV models. However, limited work has been explored
on the reuse of these extensive scenarios to iteratively improve AV models.
Moreover, it remains intractable and challenging to filter through gigantic
scenario libraries collected from other AV models with distinct behaviors,
attempting to extract transferable information for current AV improvement.
Therefore, we develop a continual driving policy optimization framework
featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into
a set of standardized sub-modules for flexible implementation choices: AV
Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a
collision prediction task, where it estimates the chance of AV failures in
these scenarios at each iteration. Subsequently, by re-sampling from historical
scenarios based on these failure probabilities, CLIC tailors individualized
curricula for downstream training, aligning them with the evaluated capability
of AV. Accordingly, CLIC not only maximizes the utilization of the vast
pre-collected scenario library for closed-loop driving policy optimization but
also facilitates AV improvement by individualizing its training with more
challenging cases out of those poorly organized scenarios. Experimental results
clearly indicate that CLIC surpasses other curriculum-based training
strategies, showing substantial improvement in managing risky scenarios, while
still maintaining proficiency in handling simpler cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xianzheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhigang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuelong Li</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03696">Function-Space Optimality of Neural Architectures With Multivariate Nonlinearities. (arXiv:2310.03696v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Parhi_R/0/1/0/all/0/1">Rahul Parhi</a>, <a href="http://arxiv.org/find/stat/1/au:+Unser_M/0/1/0/all/0/1">Michael Unser</a></p>
<p>We investigate the function-space optimality (specifically, the Banach-space
optimality) of a large class of shallow neural architectures with multivariate
nonlinearities/activation functions. To that end, we construct a new family of
Banach spaces defined via a regularization operator, the $k$-plane transform,
and a sparsity-promoting norm. We prove a representer theorem that states that
the solution sets to learning problems posed over these Banach spaces are
completely characterized by neural architectures with multivariate
nonlinearities. These optimal architectures have skip connections and are
tightly connected to orthogonal weight normalization and multi-index models,
both of which have received recent interest in the neural network community.
Our framework is compatible with a number of classical nonlinearities including
the rectified linear unit (ReLU) activation function, the norm activation
function, and the radial basis functions found in the theory of
thin-plate/polyharmonic splines. We also show that the underlying spaces are
special instances of reproducing kernel Banach spaces and variation spaces. Our
results shed light on the regularity of functions learned by neural networks
trained on data, particularly with multivariate nonlinearities, and provide new
theoretical motivation for several architectural choices found in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05736">LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. (arXiv:2310.05736v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Huiqiang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qianhui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chin-Yew Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuqing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Lili Qiu</a></p>
<p>Large language models (LLMs) have been applied in various applications due to
their astonishing capabilities. With advancements in technologies such as
chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed
to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of
tokens. To accelerate model inference and reduce cost, this paper presents
LLMLingua, a coarse-to-fine prompt compression method that involves a budget
controller to maintain semantic integrity under high compression ratios, a
token-level iterative compression algorithm to better model the interdependence
between compressed contents, and an instruction tuning based method for
distribution alignment between language models. We conduct experiments and
analysis over four datasets from different scenarios, i.e., GSM8K, BBH,
ShareGPT, and Arxiv-March23; showing that the proposed approach yields
state-of-the-art performance and allows for up to 20x compression with little
performance loss. Our code is available at https://aka.ms/LLMLingua.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08577">Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models. (arXiv:2310.08577v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Udandarao_V/0/1/0/all/0/1">Vishaal Udandarao</a>, <a href="http://arxiv.org/find/cs/1/au:+Burg_M/0/1/0/all/0/1">Max F. Burg</a>, <a href="http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1">Samuel Albanie</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a></p>
<p>Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of Visual Data-Type Identification, a basic perceptual skill
with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual data-types, spanning four broad categories. An extensive zero-shot
evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced
performance landscape. While VLMs are reasonably good at identifying certain
stylistic \textit{data-types}, such as cartoons and sketches, they struggle
with simpler data-types arising from basic manipulations like image rotations
or additive noise. Our findings reveal that (i) model scaling alone yields
marginal gains for contrastively-trained models like CLIP, and (ii) there is a
pronounced drop in performance for the largest auto-regressively trained VLMs
like OpenFlamingo. This finding points to a blind spot in current frontier
VLMs: they excel in recognizing semantic content but fail to acquire an
understanding of visual data-types through scaling. By analyzing the
pre-training distributions of these models and incorporating data-type
information into the captions during fine-tuning, we achieve a significant
enhancement in performance. By exploring this previously uncharted task, we aim
to set the stage for further advancing VLMs to equip them with visual data-type
understanding. Code and datasets are released at
https://github.com/bethgelab/DataTypeIdentification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11891">A Hyperparameter Study for Quantum Kernel Methods. (arXiv:2310.11891v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Egginger_S/0/1/0/all/0/1">Sebastian Egginger</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sakhnenko_A/0/1/0/all/0/1">Alona Sakhnenko</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Lorenz_J/0/1/0/all/0/1">Jeanette Miriam Lorenz</a></p>
<p>Quantum kernel methods are a promising method in quantum machine learning
thanks to the guarantees connected to them. Their accessibility for analytic
considerations also opens up the possibility of prescreening datasets based on
their potential for a quantum advantage. To do so, earlier works developed the
geometric difference, which can be understood as a closeness measure between
two kernel-based machine learning approaches, most importantly between a
quantum kernel and classical kernel. This metric links the quantum and
classical model complexities. Therefore, it raises the question of whether the
geometric difference, based on its relation to model complexity, can be a
useful tool in evaluations other than for the potential for quantum advantage.
In this work, we investigate the effects of hyperparameter choice on the model
performance and the generalization gap between classical and quantum kernels.
The importance of hyperparameter optimization is well known also for classical
machine learning. Especially for the quantum Hamiltonian evolution feature map,
the scaling of the input data has been shown to be crucial. However, there are
additional parameters left to be optimized, like the best number of qubits to
trace out before computing a projected quantum kernel. We investigate the
influence of these hyperparameters and compare the classically reliable method
of cross validation with the method of choosing based on the geometric
difference. Based on the thorough investigation of the hyperparameters across
11 datasets we identified commodities that can be exploited when examining a
new dataset. In addition, our findings contribute to better understanding of
the applicability of the geometric difference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15890">Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aketi_S/0/1/0/all/0/1">Sai Aparna Aketi</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>The current state-of-the-art decentralized learning algorithms mostly assume
the data distribution to be Independent and Identically Distributed (IID).
However, in practical scenarios, the distributed datasets can have
significantly heterogeneous data distributions across the agents. In this work,
we present a novel approach for decentralized learning on heterogeneous data,
where data-free knowledge distillation through contrastive loss on
cross-features is utilized to improve performance. Cross-features for a pair of
neighboring agents are the features (i.e., last hidden layer activations)
obtained from the data of an agent with respect to the model parameters of the
other agent. We demonstrate the effectiveness of the proposed technique through
an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10,
CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and
network topologies. Our experiments show that the proposed method achieves
superior performance (0.2-4% improvement in test accuracy) compared to other
existing techniques for decentralized learning on heterogeneous data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19786">From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1">Yuval Dagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1">Constantinos Daskalakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1">Maxwell Fishelson</a>, <a href="http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1">Noah Golowich</a></p>
<p>We provide a novel reduction from swap-regret minimization to external-regret
minimization, which improves upon the classical reductions of Blum-Mansour
[BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the
space of actions. We show that, whenever there exists a no-external-regret
algorithm for some hypothesis class, there must also exist a no-swap-regret
algorithm for that same class. For the problem of learning with expert advice,
our result implies that it is possible to guarantee that the swap regret is
bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$
per iteration complexity, where $N$ is the number of experts, while the
classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$
rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes
with an associated lower bound, which -- in contrast to that in [BM07] -- holds
for oblivious and $\ell_1$-constrained adversaries and learners that can employ
distributions over experts, showing that the number of rounds must be
$\tilde\Omega(N/\epsilon^2)$ or exponential in $1/\epsilon$.
</p>
<p>Our reduction implies that, if no-regret learning is possible in some game,
then this game must have approximate correlated equilibria, of arbitrarily good
approximation. This strengthens the folklore implication of no-regret learning
that approximate coarse correlated equilibria exist. Importantly, it provides a
sufficient condition for the existence of correlated equilibrium which vastly
extends the requirement that the action set is finite, thus answering a
question left open by [DG22; Ass+23]. Moreover, it answers several outstanding
questions about equilibrium computation and learning in games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00775">Harnessing machine learning for accurate treatment of overlapping opacity species in general circulation models. (arXiv:2311.00775v3 [astro-ph.EP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Schneider_A/0/1/0/all/0/1">Aaron David Schneider</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Molliere_P/0/1/0/all/0/1">Paul Molli&#xe8;re</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Louppe_G/0/1/0/all/0/1">Gilles Louppe</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Carone_L/0/1/0/all/0/1">Ludmila Carone</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Jorgensen_U/0/1/0/all/0/1">Uffe Gr&#xe5;e J&#xf8;rgensen</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Decin_L/0/1/0/all/0/1">Leen Decin</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Helling_C/0/1/0/all/0/1">Christiane Helling</a></p>
<p>To understand high precision observations of exoplanets and brown dwarfs, we
need detailed and complex general circulation models (GCMs) that incorporate
hydrodynamics, chemistry, and radiation. For this study, we specifically
examined the coupling between chemistry and radiation in GCMs and compared
different methods for the mixing of opacities of different chemical species in
the correlated-k assumption, when equilibrium chemistry cannot be assumed. We
propose a fast machine learning method based on DeepSets (DS), which
effectively combines individual correlated-k opacities (k-tables). We evaluated
the DS method alongside other published methods such as adaptive equivalent
extinction (AEE) and random overlap with rebinning and resorting (RORR). We
integrated these mixing methods into our GCM (expeRT/MITgcm) and assessed their
accuracy and performance for the example of the hot Jupiter HD~209458 b. Our
findings indicate that the DS method is both accurate and efficient for GCM
usage, whereas RORR is too slow. Additionally, we observed that the accuracy of
AEE depends on its specific implementation and may introduce numerical issues
in achieving radiative transfer solution convergence. We then applied the DS
mixing method in a simplified chemical disequilibrium situation, where we
modeled the rainout of TiO and VO, and confirmed that the rainout of TiO and VO
would hinder the formation of a stratosphere. To further expedite the
development of consistent disequilibrium chemistry calculations in GCMs, we
provide documentation and code for coupling the DS mixing method with
correlated-k radiative transfer solvers. The DS method has been extensively
tested to be accurate enough for GCMs; however, other methods might be needed
for accelerating atmospheric retrievals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04064">KPI Extraction from Maintenance Work Orders -- A Comparison of Expert Labeling, Text Classification and AI-Assisted Tagging for Computing Failure Rates of Wind Turbines. (arXiv:2311.04064v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lutz_M/0/1/0/all/0/1">Marc-Alexander Lutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Schafermeier_B/0/1/0/all/0/1">Bastian Sch&#xe4;fermeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Sexton_R/0/1/0/all/0/1">Rachael Sexton</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharp_M/0/1/0/all/0/1">Michael Sharp</a>, <a href="http://arxiv.org/find/cs/1/au:+Dima_A/0/1/0/all/0/1">Alden Dima</a>, <a href="http://arxiv.org/find/cs/1/au:+Faulstich_S/0/1/0/all/0/1">Stefan Faulstich</a>, <a href="http://arxiv.org/find/cs/1/au:+Aluri_J/0/1/0/all/0/1">Jagan Mohini Aluri</a></p>
<p>Maintenance work orders are commonly used to document information about wind
turbine operation and maintenance. This includes details about proactive and
reactive wind turbine downtimes, such as preventative and corrective
maintenance. However, the information contained in maintenance work orders is
often unstructured and difficult to analyze, presenting challenges for
decision-makers wishing to use it for optimizing operation and maintenance. To
address this issue, this work compares three different approaches to calculate
reliability by performance indicators from maintenance work orders. The first
approach involves manual labeling of the maintenance work orders by domain
experts, using the schema defined in an industrial guideline to assign the
label accordingly. The second approach involves the development of a model that
automatically labels the maintenance work orders using text classification
methods. Through this method, we are able to achieve macro average and weighted
average F1-Scores of 0.75 and 0.85 respectively. The third technique uses an
AI-assisted tagging tool to tag and structure the raw maintenance information,
together with a novel rule-based approach for extracting relevant maintenance
work orders for failure rate calculation. In our experiments the AI-assisted
tool leads to a 88% drop in tagging time in comparison to the other two
approaches, while expert labeling and text classification are more accurate in
KPI extraction. Overall, our findings make extracting maintenance information
from maintenance work orders more efficient, enable the assessment of
reliability key performance indicators and therefore support the optimization
of wind turbine operation and maintenance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04256">Foundational propositions of hesitant fuzzy sets and parameter reductions of hesitant fuzzy information systems. (arXiv:2311.04256v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shizhan Lu</a></p>
<p>Hesitant fuzzy sets are widely used in the instances of uncertainty and
hesitation. The inclusion relationship is an important and foundational
definition for sets. Hesitant fuzzy set, as a kind of set, needs explicit
definition of inclusion relationship. Base on the hesitant fuzzy membership
degree of discrete form, several kinds of inclusion relationships for hesitant
fuzzy sets are proposed. And then some foundational propositions of hesitant
fuzzy sets and the families of hesitant fuzzy sets are presented. Finally, some
foundational propositions of hesitant fuzzy information systems with respect to
parameter reductions are put forward, and an example and an algorithm are given
to illustrate the processes of parameter reductions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09058">Constrained Parameter Regularization. (arXiv:2311.09058v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franke_J/0/1/0/all/0/1">J&#xf6;rg K.H. Franke</a>, <a href="http://arxiv.org/find/cs/1/au:+Hefenbrock_M/0/1/0/all/0/1">Michael Hefenbrock</a>, <a href="http://arxiv.org/find/cs/1/au:+Koehler_G/0/1/0/all/0/1">Gregor Koehler</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a></p>
<p>Regularization is a critical component in deep learning training, with weight
decay being a commonly used approach. It applies a constant penalty coefficient
uniformly across all parameters. This may be unnecessarily restrictive for some
parameters, while insufficiently restricting others. To dynamically adjust
penalty coefficients for different parameter groups, we present constrained
parameter regularization (CPR) as an alternative to traditional weight decay.
Instead of applying a single constant penalty to all parameters, we enforce an
upper bound on a statistical measure (e.g., the L$_2$-norm) of parameter
groups. Consequently, learning becomes a constraint optimization problem, which
we address by an adaptation of the augmented Lagrangian method. CPR only
requires two hyperparameters and incurs no measurable runtime overhead.
Additionally, we propose a simple but efficient mechanism to adapt the upper
bounds during the optimization. We provide empirical evidence of CPR's efficacy
in experiments on the "grokking" phenomenon, computer vision, and language
modeling tasks. Our results demonstrate that CPR counteracts the effects of
grokking and consistently matches or outperforms traditional weight decay.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09428">Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models. (arXiv:2311.09428v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yueqing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1">Ali Payani</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a></p>
<p>This work investigates the potential of undermining both fairness and
detection performance in abusive language detection. In a dynamic and complex
digital world, it is crucial to investigate the vulnerabilities of these
detection models to adversarial fairness attacks to improve their fairness
robustness. We propose a simple yet effective framework FABLE that leverages
backdoor attacks as they allow targeted control over the fairness and detection
performance. FABLE explores three types of trigger designs (i.e., rare,
artificial, and natural triggers) and novel sampling strategies. Specifically,
the adversary can inject triggers into samples in the minority group with the
favored outcome (i.e., "non-abusive") and flip their labels to the unfavored
outcome, i.e., "abusive". Experiments on benchmark datasets demonstrate the
effectiveness of FABLE attacking fairness and utility in abusive language
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13750">Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder. (arXiv:2311.13750v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a></p>
<p>This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. We hope
this study can inspire exploration of more general multi-modal representation
learning for autonomous agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14948">Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1">Sahil Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1">Avi Schwarzschild</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1">Soumye Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav Mohanty Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1">Chirag Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1">Jeff Bilmes</a></p>
<p>Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach. In this work, we demonstrate that the efficacy of
CleanCLIP in mitigating backdoors is highly dependent on the particular
objective used during model pre-training. We observe that stronger pre-training
objectives correlate with harder to remove backdoors behaviors. We show this by
training multimodal models on two large datasets consisting of 3 million (CC3M)
and 6 million (CC6M) datapoints, under various pre-training objectives,
followed by poison removal using CleanCLIP. We find that CleanCLIP is
ineffective when stronger pre-training objectives are used, even with extensive
hyperparameter tuning. Our findings underscore critical considerations for ML
practitioners who pre-train models using large-scale web-curated data and are
concerned about potential backdoor threats. Notably, our results suggest that
simpler pre-training objectives are more amenable to effective backdoor
removal. This insight is pivotal for practitioners seeking to balance the
trade-offs between using stronger pre-training objectives and security against
backdoor attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16173">Conditions for Length Generalization in Learning Reasoning Skills. (arXiv:2311.16173v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Changnan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bing Liu</a></p>
<p>Reasoning is a fundamental capability of AI agents. Recently, large language
models (LLMs) have shown remarkable abilities to perform reasoning tasks.
However, numerous evaluations of the reasoning capabilities of LLMs have also
showed some limitations. An outstanding limitation is length generalization,
meaning that when trained on reasoning problems of smaller lengths or sizes,
the resulting models struggle with problems of larger sizes or lengths. This
potentially indicates some theoretical limitations of generalization in
learning reasoning skills. These evaluations and their observations motivated
us to perform a theoretical study of the length generalization problem. This
work focuses on reasoning tasks that can be formulated as Markov dynamic
processes (MDPs) and/or directed acyclic graphs (DAGs). It identifies and
proves conditions that decide whether the length generalization problem can be
solved or not for a reasoning task in a particular representation. Experiments
are also conducted to verify the theoretical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17030">Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching. (arXiv:2311.17030v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Makelov_A/0/1/0/all/0/1">Aleksandar Makelov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_G/0/1/0/all/0/1">Georg Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1">Neel Nanda</a></p>
<p>Mechanistic interpretability aims to understand model behaviors in terms of
specific, interpretable features, often hypothesized to manifest as
low-dimensional subspaces of activations. Specifically, recent studies have
explored subspace interventions (such as activation patching) as a way to
simultaneously manipulate model behavior and attribute the features behind it
to given subspaces.
</p>
<p>In this work, we demonstrate that these two aims diverge, potentially leading
to an illusory sense of interpretability. Counterintuitively, even if a
subspace intervention makes the model's output behave as if the value of a
feature was changed, this effect may be achieved by activating a dormant
parallel pathway leveraging another subspace that is causally disconnected from
model outputs. We demonstrate this phenomenon in a distilled mathematical
example, in two real-world domains (the indirect object identification task and
factual recall), and present evidence for its prevalence in practice. In the
context of factual recall, we further show a link to rank-1 fact editing,
providing a mechanistic explanation for previous work observing an
inconsistency between fact editing performance and fact localization.
</p>
<p>However, this does not imply that activation patching of subspaces is
intrinsically unfit for interpretability. To contextualize our findings, we
also show what a success case looks like in a task (indirect object
identification) where prior manual circuit analysis informs an understanding of
the location of a feature. We explore the additional evidence needed to argue
that a patched subspace is faithful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18260">Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. (arXiv:2311.18260v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tanno_R/0/1/0/all/0/1">Ryutaro Tanno</a>, <a href="http://arxiv.org/find/eess/1/au:+Barrett_D/0/1/0/all/0/1">David G.T. Barrett</a>, <a href="http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1">Andrew Sellergren</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghaisas_S/0/1/0/all/0/1">Sumedh Ghaisas</a>, <a href="http://arxiv.org/find/eess/1/au:+Dathathri_S/0/1/0/all/0/1">Sumanth Dathathri</a>, <a href="http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1">Abigail See</a>, <a href="http://arxiv.org/find/eess/1/au:+Welbl_J/0/1/0/all/0/1">Johannes Welbl</a>, <a href="http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/eess/1/au:+Azizi_S/0/1/0/all/0/1">Shekoofeh Azizi</a>, <a href="http://arxiv.org/find/eess/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/eess/1/au:+Schaekermann_M/0/1/0/all/0/1">Mike Schaekermann</a>, <a href="http://arxiv.org/find/eess/1/au:+May_R/0/1/0/all/0/1">Rhys May</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1">Roy Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1">SiWai Man</a>, <a href="http://arxiv.org/find/eess/1/au:+Ahmed_Z/0/1/0/all/0/1">Zahra Ahmed</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahdavi_S/0/1/0/all/0/1">Sara Mahdavi</a>, <a href="http://arxiv.org/find/eess/1/au:+Belgrave_D/0/1/0/all/0/1">Danielle Belgrave</a>, <a href="http://arxiv.org/find/eess/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a>, <a href="http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1">Shravya Shetty</a>, <a href="http://arxiv.org/find/eess/1/au:+Kohli_P/0/1/0/all/0/1">Pushmeet Kohli</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1">Po-Sen Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/eess/1/au:+Ktena_I/0/1/0/all/0/1">Ira Ktena</a></p>
<p>Radiology reports are an instrumental part of modern medicine, informing key
clinical decisions such as diagnosis and treatment. The worldwide shortage of
radiologists, however, restricts access to expert care and imposes heavy
workloads, contributing to avoidable errors and delays in report delivery.
While recent progress in automated report generation with vision-language
models offer clear potential in ameliorating the situation, the path to
real-world adoption has been stymied by the challenge of evaluating the
clinical quality of AI-generated reports. In this study, we build a
state-of-the-art report generation system for chest radiographs,
\textit{Flamingo-CXR}, by fine-tuning a well-known vision-language foundation
model on radiology data. To evaluate the quality of the AI-generated reports, a
group of 16 certified radiologists provide detailed evaluations of AI-generated
and human written reports for chest X-rays from an intensive care setting in
the United States and an inpatient setting in India. At least one radiologist
(out of two per case) preferred the AI report to the ground truth report in
over 60$\%$ of cases for both datasets. Amongst the subset of AI-generated
reports that contain errors, the most frequently cited reasons were related to
the location and finding, whereas for human written reports, most mistakes were
related to severity and finding. This disparity suggested potential
complementarity between our AI system and human experts, prompting us to
develop an assistive scenario in which \textit{Flamingo-CXR} generates a
first-draft report, which is subsequently revised by a clinician. This is the
first demonstration of clinician-AI collaboration for report writing, and the
resultant reports are assessed to be equivalent or preferred by at least one
radiologist to reports written by experts alone in 80$\%$ of in-patient cases
and 60$\%$ of intensive care cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00710">SpaCE: The Spatial Confounding Environment. (arXiv:2312.00710v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tec_M/0/1/0/all/0/1">Mauricio Tec</a>, <a href="http://arxiv.org/find/cs/1/au:+Trisovic_A/0/1/0/all/0/1">Ana Trisovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Audirac_M/0/1/0/all/0/1">Michelle Audirac</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodward_S/0/1/0/all/0/1">Sophie Woodward</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jie Kate Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoshnevis_N/0/1/0/all/0/1">Naeem Khoshnevis</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominici_F/0/1/0/all/0/1">Francesca Dominici</a></p>
<p>Spatial confounding poses a significant challenge in scientific studies
involving spatial data, where unobserved spatial variables can influence both
treatment and outcome, possibly leading to spurious associations. To address
this problem, we introduce SpaCE: The Spatial Confounding Environment, the
first toolkit to provide realistic benchmark datasets and tools for
systematically evaluating causal inference methods designed to alleviate
spatial confounding. Each dataset includes training data, true counterfactuals,
a spatial graph with coordinates, and smoothness and confounding scores
characterizing the effect of a missing spatial confounder. It also includes
realistic semi-synthetic outcomes and counterfactuals, generated using
state-of-the-art machine learning ensembles, following best practices for
causal inference benchmarks. The datasets cover real treatment and covariates
from diverse domains, including climate, health and social sciences. SpaCE
facilitates an automated end-to-end pipeline, simplifying data loading,
experimental setup, and evaluating machine learning and causal inference
models. The SpaCE project provides several dozens of datasets of diverse sizes
and spatial complexity. It is publicly available as a Python package,
encouraging community feedback and contributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00886">Nash Learning from Human Feedback. (arXiv:2312.00886v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a>, <a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>, <a href="http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1">Daniele Calandriello</a>, <a href="http://arxiv.org/find/stat/1/au:+Azar_M/0/1/0/all/0/1">Mohammad Gheshlaghi Azar</a>, <a href="http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1">Mark Rowland</a>, <a href="http://arxiv.org/find/stat/1/au:+Guo_Z/0/1/0/all/0/1">Zhaohan Daniel Guo</a>, <a href="http://arxiv.org/find/stat/1/au:+Tang_Y/0/1/0/all/0/1">Yunhao Tang</a>, <a href="http://arxiv.org/find/stat/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>, <a href="http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1">Thomas Mesnard</a>, <a href="http://arxiv.org/find/stat/1/au:+Michi_A/0/1/0/all/0/1">Andrea Michi</a>, <a href="http://arxiv.org/find/stat/1/au:+Selvi_M/0/1/0/all/0/1">Marco Selvi</a>, <a href="http://arxiv.org/find/stat/1/au:+Girgin_S/0/1/0/all/0/1">Sertan Girgin</a>, <a href="http://arxiv.org/find/stat/1/au:+Momchev_N/0/1/0/all/0/1">Nikola Momchev</a>, <a href="http://arxiv.org/find/stat/1/au:+Bachem_O/0/1/0/all/0/1">Olivier Bachem</a>, <a href="http://arxiv.org/find/stat/1/au:+Mankowitz_D/0/1/0/all/0/1">Daniel J. Mankowitz</a>, <a href="http://arxiv.org/find/stat/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>, <a href="http://arxiv.org/find/stat/1/au:+Piot_B/0/1/0/all/0/1">Bilal Piot</a></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
</p>
<p>In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
</p>
<p>In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01210">When accurate prediction models yield harmful self-fulfilling prophecies. (arXiv:2312.01210v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Amsterdam_W/0/1/0/all/0/1">Wouter A.C. van Amsterdam</a>, <a href="http://arxiv.org/find/stat/1/au:+Geloven_N/0/1/0/all/0/1">Nan van Geloven</a>, <a href="http://arxiv.org/find/stat/1/au:+Krijthe_J/0/1/0/all/0/1">Jesse H. Krijthe</a>, <a href="http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1">Rajesh Ranganath</a>, <a href="http://arxiv.org/find/stat/1/au:+Cina_G/0/1/0/all/0/1">Giovanni Cin&#xe1;</a></p>
<p>Prediction models are popular in medical research and practice. By predicting
an outcome of interest for specific patients, these models may help inform
difficult treatment decisions, and are often hailed as the poster children for
personalized, data-driven healthcare.
</p>
<p>We show however, that using prediction models for decision making can lead to
harmful decisions, even when the predictions exhibit good discrimination after
deployment. These models are harmful self-fulfilling prophecies: their
deployment harms a group of patients but the worse outcome of these patients
does not invalidate the predictive power of the model. Our main result is a
formal characterization of a set of such prediction models. Next we show that
models that are well calibrated before and after deployment are useless for
decision making as they made no change in the data distribution. These results
point to the need to revise standard practices for validation, deployment and
evaluation of prediction models that are used in medical decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01454">D-Bot: Database Diagnosis System using Large Language Models. (arXiv:2312.01454v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xuanhe Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guoliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhaoyan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weize Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jianming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiesi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1">Ruohang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1">Guoyang Zeng</a></p>
<p>Database administrators (DBAs) play an important role in managing,
maintaining and optimizing database systems. However, it is hard and tedious
for DBAs to manage a large number of databases and give timely response
(waiting for hours is intolerable in many online cases). In addition, existing
empirical methods only support limited diagnosis scenarios, which are also
labor-intensive to update the diagnosis rules for database version updates.
Recently large language models (LLMs) have shown great potential in various
fields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can
automatically acquire knowledge from diagnosis documents, and generate
reasonable and well-founded diagnosis report (i.e., identifying the root causes
and solutions) within acceptable time (e.g., under 10 minutes compared to hours
by a DBA). The techniques in D-Bot include (i) offline knowledge extraction
from documents, (ii) automatic prompt generation (e.g., knowledge matching,
tool retrieval), (iii) root cause analysis using tree search algorithm, and
(iv) collaborative mechanism for complex anomalies with multiple root causes.
We verify D-Bot on real benchmarks (including 539 anomalies of six typical
applications), and the results show that D-Bot can effectively analyze the root
causes of unseen anomalies and significantly outperforms traditional methods
and vanilla models like GPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02079">Deep Set Neural Networks for forecasting asynchronous bioprocess timeseries. (arXiv:2312.02079v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borisyak_M/0/1/0/all/0/1">Maxim Borisyak</a>, <a href="http://arxiv.org/find/cs/1/au:+Born_S/0/1/0/all/0/1">Stefan Born</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubauer_P/0/1/0/all/0/1">Peter Neubauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_Bournazou_M/0/1/0/all/0/1">Mariano Nicolas Cruz-Bournazou</a></p>
<p>Cultivation experiments often produce sparse and irregular time series.
Classical approaches based on mechanistic models, like Maximum Likelihood
fitting or Monte-Carlo Markov chain sampling, can easily account for sparsity
and time-grid irregularities, but most statistical and Machine Learning tools
are not designed for handling sparse data out-of-the-box. Among popular
approaches there are various schemes for filling missing values (imputation)
and interpolation into a regular grid (alignment). However, such methods
transfer the biases of the interpolation or imputation models to the target
model. We show that Deep Set Neural Networks equipped with triplet encoding of
the input data can successfully handle bio-process data without any need for
imputation or alignment procedures. The method is agnostic to the particular
nature of the time series and can be adapted for any task, for example, online
monitoring, predictive control, design of experiments, etc. In this work, we
focus on forecasting. We argue that such an approach is especially suitable for
typical cultivation processes, demonstrate the performance of the method on
several forecasting tasks using data generated from macrokinetic growth models
under realistic conditions, and compare the method to a conventional fitting
procedure and methods based on imputation and alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02125">TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques. (arXiv:2312.02125v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panahandeh_A/0/1/0/all/0/1">Amir Panahandeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Asemi_H/0/1/0/all/0/1">Hanie Asemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nourani_E/0/1/0/all/0/1">Esmaeil Nourani</a></p>
<p>Recent advances in language models (LMs), have demonstrated significant
efficacy in tasks related to the arts and humanities. While LMs have exhibited
exceptional performance across a wide range of natural language processing
tasks, there are notable challenges associated with their utilization on small
datasets and their ability to replicate more creative human capacities. In this
study, we aim to address these challenges by training a Persian classical
poetry generation model using a transformer architecture on a specialized
dataset with no pretraining. Additionally, we propose a novel decoding method
to enhance coherence and meaningfulness in the generated poetry, effectively
managing the tradeoff between diversity and quality. Furthermore, the results
of our training approach and the proposed decoding method are evaluated through
comprehensive set of automatic and human evaluations and showed its superior
capability to generate coherent and meaningful poetry in compare to other
decoding methods and an existing Persian large language model (LLM).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02168">The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch. (arXiv:2312.02168v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tim Z. Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zenn_J/0/1/0/all/0/1">Johannes Zenn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1">Robert Bamler</a></p>
<p>The Street View House Numbers (SVHN) dataset is a popular benchmark dataset
in deep learning. Originally designed for digit classification tasks, the SVHN
dataset has been widely used as a benchmark for various other tasks including
generative modeling. However, with this work, we aim to warn the community
about an issue of the SVHN dataset as a benchmark for generative modeling
tasks: we discover that the official split into training set and test set of
the SVHN dataset are not drawn from the same distribution. We empirically show
that this distribution mismatch has little impact on the classification task
(which may explain why this issue has not been detected before), but it
severely affects the evaluation of probabilistic generative models, such as
Variational Autoencoders and diffusion models. As a workaround, we propose to
mix and re-split the official training and test set when SVHN is used for tasks
other than classification. We publish a new split and the indices we used to
create it at https://jzenn.github.io/svhn-remix/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02248">Towards early diagnosis of Alzheimer&#x27;s disease: Advances in immune-related blood biomarkers and computational modeling approaches. (arXiv:2312.02248v2 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Krix_S/0/1/0/all/0/1">Sophia Krix</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wilczynski_E/0/1/0/all/0/1">Ella Wilczynski</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Falgas_N/0/1/0/all/0/1">Neus Falg&#xe0;s</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sanchez_Valle_R/0/1/0/all/0/1">Raquel S&#xe1;nchez-Valle</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yoles_E/0/1/0/all/0/1">Eti Yoles</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nevo_U/0/1/0/all/0/1">Uri Nevo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Baruch_K/0/1/0/all/0/1">Kuti Baruch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Frohlich_H/0/1/0/all/0/1">Holger Fr&#xf6;hlich</a></p>
<p>Alzheimer's disease has an increasing prevalence in the population
world-wide, yet current diagnostic methods based on recommended biomarkers are
only available in specialized clinics. Due to these circumstances, Alzheimer's
disease is usually diagnosed late, which contrasts with the currently available
treatment options that are only effective for patients at an early stage.
Blood-based biomarkers could fill in the gap of easily accessible and low-cost
methods for early diagnosis of the disease. In particular, immune-based
blood-biomarkers might be a promising option, given the recently discovered
cross-talk of immune cells of the central nervous system with those in the
peripheral immune system. With the help of machine learning algorithms and
mechanistic modeling approaches, such as agent-based modeling, an in-depth
analysis of the simulation of cell dynamics is possible as well as of
high-dimensional omics resources indicative of pathway signaling changes. Here,
we give a background on advances in research on brain-immune system cross-talk
in Alzheimer's disease and review recent machine learning and mechanistic
modeling approaches which leverage modern omics technologies for blood-based
immune system-related biomarker discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02429">PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models. (arXiv:2312.02429v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1">Wei-Cheng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jyun-Yu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Darabsah_M/0/1/0/all/0/1">Mutasem Al-Darabsah</a>, <a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1">Choon Hui Teo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hsiang-Fu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishwanathan_S/0/1/0/all/0/1">S.V.N. Vishwanathan</a></p>
<p>Embedding-based Retrieval Models (ERMs) have emerged as a promising framework
for large-scale text retrieval problems due to powerful large language models.
Nevertheless, fine-tuning ERMs to reach state-of-the-art results can be
expensive due to the extreme scale of data as well as the complexity of
multi-stages pipelines (e.g., pre-training, fine-tuning, distillation). In this
work, we propose the PEFA framework, namely ParamEter-Free Adapters, for fast
tuning of ERMs without any backward pass in the optimization. At index building
stage, PEFA equips the ERM with a non-parametric k-nearest neighbor (kNN)
component. At inference stage, PEFA performs a convex combination of two
scoring functions, one from the ERM and the other from the kNN. Based on the
neighborhood definition, PEFA framework induces two realizations, namely
PEFA-XL (i.e., extra large) using double ANN indices and PEFA-XS (i.e., extra
small) using a single ANN index. Empirically, PEFA achieves significant
improvement on two retrieval applications. For document retrieval, regarding
Recall@100 metric, PEFA improves not only pre-trained ERMs on Trivia-QA by an
average of 13.2%, but also fine-tuned ERMs on NQ-320K by an average of 5.5%,
respectively. For product search, PEFA improves the Recall@100 of the
fine-tuned ERMs by an average of 5.3% and 14.5%, for PEFA-XS and PEFA-XL,
respectively. Our code is available at
https://github.com/amzn/pecos/tree/mainline/examples/pefa-wsdm24.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02850">A Kernel-Based Neural Network Test for High-dimensional Sequencing Data Analysis. (arXiv:2312.02850v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hou_T/0/1/0/all/0/1">Tingting Hou</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_C/0/1/0/all/0/1">Chang Jiang</a>, <a href="http://arxiv.org/find/stat/1/au:+Lu_Q/0/1/0/all/0/1">Qing Lu</a></p>
<p>The recent development of artificial intelligence (AI) technology, especially
the advance of deep neural network (DNN) technology, has revolutionized many
fields. While DNN plays a central role in modern AI technology, it has been
rarely used in sequencing data analysis due to challenges brought by
high-dimensional sequencing data (e.g., overfitting). Moreover, due to the
complexity of neural networks and their unknown limiting distributions,
building association tests on neural networks for genetic association analysis
remains a great challenge. To address these challenges and fill the important
gap of using AI in high-dimensional sequencing data analysis, we introduce a
new kernel-based neural network (KNN) test for complex association analysis of
sequencing data. The test is built on our previously developed KNN framework,
which uses random effects to model the overall effects of high-dimensional
genetic data and adopts kernel-based neural network structures to model complex
genotype-phenotype relationships. Based on KNN, a Wald-type test is then
introduced to evaluate the joint association of high-dimensional genetic data
with a disease phenotype of interest, considering non-linear and non-additive
effects (e.g., interaction effects). Through simulations, we demonstrated that
our proposed method attained higher power compared to the sequence kernel
association test (SKAT), especially in the presence of non-linear and
interaction effects. Finally, we apply the methods to the whole genome
sequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative
(ADNI) study, investigating new genes associated with the hippocampal volume
change over time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02858">Towards Causal Representations of Climate Model Data. (arXiv:2312.02858v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boussard_J/0/1/0/all/0/1">Julien Boussard</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagda_C/0/1/0/all/0/1">Chandni Nagda</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaltenborn_J/0/1/0/all/0/1">Julia Kaltenborn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_C/0/1/0/all/0/1">Charlotte Emilie Elektra Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1">Philippe Brouillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurwicz_Y/0/1/0/all/0/1">Yaniv Gurwicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowack_P/0/1/0/all/0/1">Peer Nowack</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1">David Rolnick</a></p>
<p>Climate models, such as Earth system models (ESMs), are crucial for
simulating future climate change based on projected Shared Socioeconomic
Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated
and invaluable, machine learning-based emulators trained on existing simulation
data can project additional climate scenarios much faster and are
computationally efficient. However, they often lack generalizability and
interpretability. This work delves into the potential of causal representation
learning, specifically the \emph{Causal Discovery with Single-parent Decoding}
(CDSD) method, which could render climate model emulation efficient
\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,
focusing on emissions, temperature, and precipitation. Our findings shed light
on the challenges, limitations, and promise of using CDSD as a stepping stone
towards more interpretable and robust climate model emulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08289">$\textbf{A}^2\textbf{CiD}^2$: Accelerating Asynchronous Communication in Decentralized Deep Learning. (arXiv:2306.08289v2 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nabli_A/0/1/0/all/0/1">Adel Nabli</a> (MLIA, Mila), <a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1">Eugene Belilovsky</a> (Mila), <a href="http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1">Edouard Oyallon</a> (MLIA)</p>
<p>Distributed training of Deep Learning models has been critical to many recent
successes in the field. Current standard methods primarily rely on synchronous
centralized algorithms which induce major communication bottlenecks and
synchronization locks at scale. Decentralized asynchronous algorithms are
emerging as a potential alternative but their practical applicability still
lags. In order to mitigate the increase in communication cost that naturally
comes with scaling the number of workers, we introduce a principled
asynchronous, randomized, gossip-based optimization algorithm which works
thanks to a continuous local momentum named $\textbf{A}^2\textbf{CiD}^2$. Our
method allows each worker to continuously process mini-batches without
stopping, and run a peer-to-peer averaging routine in parallel, reducing idle
time. In addition to inducing a significant communication acceleration at no
cost other than adding a local momentum variable, minimal adaptation is
required to incorporate $\textbf{A}^2\textbf{CiD}^2$ to standard asynchronous
approaches. Our theoretical analysis proves accelerated rates compared to
previous asynchronous decentralized baselines and we empirically show that
using our $\textbf{A}^2\textbf{CiD}^2$ momentum significantly decrease
communication costs in poorly connected networks. In particular, we show
consistent improvement on the ImageNet dataset using up to 64 asynchronous
workers (A100 GPUs) and various communication network topologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11230">Zipformer: A faster and better encoder for automatic speech recognition. (arXiv:2310.11230v2 [eess.AS] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1">Zengwei Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1">Liyong Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1">Xiaoyu Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kang_W/0/1/0/all/0/1">Wei Kang</a>, <a href="http://arxiv.org/find/eess/1/au:+Kuang_F/0/1/0/all/0/1">Fangjun Kuang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1">Zengrui Jin</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1">Long Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1">Daniel Povey</a></p>
<p>The Conformer has become the most popular encoder model for automatic speech
recognition (ASR). It adds convolution modules to a transformer to learn both
local and global dependencies. In this work we describe a faster, more
memory-efficient, and better-performing transformer, called Zipformer. Modeling
changes include: 1) a U-Net-like encoder structure where middle stacks operate
at lower frame rates; 2) reorganized block structure with more modules, within
which we re-use attention weights for efficiency; 3) a modified form of
LayerNorm called BiasNorm allows us to retain some length information; 4) new
activation functions SwooshR and SwooshL work better than Swish. We also
propose a new optimizer, called ScaledAdam, which scales the update by each
tensor's current scale to keep the relative change about the same, and also
explictly learns the parameter scale. It achieves faster convergence and better
performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and
WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer
over other state-of-the-art ASR models. Our code is publicly available at
https://github.com/k2-fsa/icefall.
</p>
</p>
</div>

    </div>
    </body>
    