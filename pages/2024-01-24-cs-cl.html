<!DOCTYPE html>
<html>
<head>
<title>2024-01-24-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.10893">Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1">Deepak Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishaan_A/0/1/0/all/0/1">Anjali Ishaan</a></p>
<p>Knowledge graph embedding transforms knowledge graphs into a continuous,
low-dimensional space, facilitating inference and completion tasks. This field
is mainly divided into translational distance models and semantic matching
models. A key challenge in translational distance models is their inability to
effectively differentiate between 'head' and 'tail' entities in graphs. To
address this, the novel location-sensitive embedding (LSE) method has been
developed. LSE innovatively modifies the head entity using relation-specific
mappings, conceptualizing relations as linear transformations rather than mere
translations. The theoretical foundations of LSE, including its
representational capabilities and its connections to existing models, have been
thoroughly examined. A more streamlined variant, LSEd, employs a diagonal
matrix for transformations to enhance practical efficiency. In tests conducted
on four large-scale datasets for link prediction, LSEd either outperforms or is
competitive with leading contemporary models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10900">Towards building a monitoring platform for a challenge-oriented smart specialisation with RIS3-MCAT. (arXiv:2401.10900v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fuster_E/0/1/0/all/0/1">Enric Fuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_T/0/1/0/all/0/1">Tatiana Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Carretero_H/0/1/0/all/0/1">Hermes Carretero</a>, <a href="http://arxiv.org/find/cs/1/au:+Duran_Silva_N/0/1/0/all/0/1">Nicolau Duran-Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Guixe_R/0/1/0/all/0/1">Roger Guix&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Pujol_J/0/1/0/all/0/1">Josep Pujol</a>, <a href="http://arxiv.org/find/cs/1/au:+Rondelli_B/0/1/0/all/0/1">Bernardo Rondelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Rull_G/0/1/0/all/0/1">Guillem Rull</a>, <a href="http://arxiv.org/find/cs/1/au:+Cortijo_M/0/1/0/all/0/1">Marta Cortijo</a>, <a href="http://arxiv.org/find/cs/1/au:+Romagosa_M/0/1/0/all/0/1">Montserrat Romagosa</a></p>
<p>In the new research and innovation (R&amp;I) paradigm, aimed at a transformation
towards more sustainable, inclusive and fair pathways to address societal and
environmental challenges, and at generating new patterns of specialisation and
new trajectories for socioeconomic development, it is essential to provide
monitoring systems and tools to map and understand the contribution of R&amp;I
policies and projects. To address this transformation, we present the RIS3-MCAT
platform, the result of a line of work aimed at exploring the potential of open
data, semantic analysis, and data visualisation, for monitoring
challenge-oriented smart specialisation in Catalonia. RIS3-MCAT is an
interactive platform that facilitates access to R&amp;I project data in formats
that allow for sophisticated analyses of a large volume of texts, enabling the
detailed study of thematic specialisations and challenges beyond classical
classification systems. Its conceptualisation, development framework and use
are presented in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10940">RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1">Majid Ramezani</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammad_Shahi_H/0/1/0/all/0/1">Hamed Mohammad-Shahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Daliry_M/0/1/0/all/0/1">Mahshid Daliry</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1">Soroor Rahmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Asghari_A/0/1/0/all/0/1">Amir-Hosein Asghari</a></p>
<p>In the era of information proliferation, discerning the credibility of news
content poses an ever-growing challenge. This paper introduces RELIANCE, a
pioneering ensemble learning system designed for robust information and fake
news credibility evaluation. Comprising five diverse base models, including
Support Vector Machine (SVM), naive Bayes, logistic regression, random forest,
and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs
an innovative approach to integrate their strengths, harnessing the collective
intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the
superiority of RELIANCE over individual models, indicating its efficacy in
distinguishing between credible and non-credible information sources. RELIANCE,
also surpasses baseline models in information and news credibility assessment,
establishing itself as an effective solution for evaluating the reliability of
information sources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10995">The Radiation Oncology NLP Database. (arXiv:2401.10995v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1">Jason Holmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Wenxiong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenbin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Hongying Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peilong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1">Muhammad Ali Elahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hongmin Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Quanzheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiajian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>We present the Radiation Oncology NLP Database (ROND), the first dedicated
Natural Language Processing (NLP) dataset for radiation oncology, an important
medical specialty that has received limited attention from the NLP community in
the past. With the advent of Artificial General Intelligence (AGI), there is an
increasing need for specialized datasets and benchmarks to facilitate research
and development. ROND is specifically designed to address this gap in the
domain of radiation oncology, a field that offers many opportunities for NLP
exploration. It encompasses various NLP tasks including Logic Reasoning, Text
Classification, Named Entity Recognition (NER), Question Answering (QA), Text
Summarization, and Patient-Clinician Conversations, each with a distinct focus
on radiation oncology concepts and application cases. In addition, we have
developed an instruction-tuning dataset consisting of over 20k instruction
pairs (based on ROND) and trained a large language model, CancerChat. This
serves to demonstrate the potential of instruction-tuning large language models
within a highly-specialized medical domain. The evaluation results in this
study could serve as baseline results for future research. ROND aims to
stimulate advancements in radiation oncology and clinical NLP by offering a
platform for testing and improving algorithms and models in a domain-specific
context. The ROND dataset is a joint effort of multiple U.S. health
institutions. The data is available at
https://github.com/zl-liu/Radiation-Oncology-NLP-Database.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11021">Analysis and Detection of Multilingual Hate Speech Using Transformer Based Deep Learning. (arXiv:2401.11021v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arijit Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Nandy_S/0/1/0/all/0/1">Somashree Nandy</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1">Rupam Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Srijan Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1">Diganta Saha</a></p>
<p>Hate speech is harmful content that directly attacks or promotes hatred
against members of groups or individuals based on actual or perceived aspects
of identity, such as racism, religion, or sexual orientation. This can affect
social life on social media platforms as hateful content shared through social
media can harm both individuals and communities. As the prevalence of hate
speech increases online, the demand for automated detection as an NLP task is
increasing. In this work, the proposed method is using transformer-based model
to detect hate speech in social media, like twitter, Facebook, WhatsApp,
Instagram, etc. The proposed model is independent of languages and has been
tested on Italian, English, German, Bengali. The Gold standard datasets were
collected from renowned researcher Zeerak Talat, Sara Tonelli, Melanie Siegel,
and Rezaul Karim. The success rate of the proposed model for hate speech
detection is higher than the existing baseline and state-of-the-art models with
accuracy in Bengali dataset is 89%, in English: 91%, in German dataset 91% and
in Italian dataset it is 77%. The proposed algorithm shows substantial
improvement to the benchmark method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11033">FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models&#x27; Training?. (arXiv:2401.11033v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1">Shaina Raza</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghuge_S/0/1/0/all/0/1">Shardul Ghuge</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Chen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandya_D/0/1/0/all/0/1">Deval Pandya</a></p>
<p>Advancements in Large Language Models (LLMs) highlight the need for ethical
practices and data integrity. We introduce a framework that embeds FAIR
(Findable, Accessible, Interoperable, Reusable) data principles into LLM
training. This approach marks a shift towards practices compliant with FAIR
standards. Our framework presents guidelines for integrating FAIR data
principles into LLM training. This initiative includes a checklist for
researchers and developers. We also demonstrate its practical application
through a case study focused on bias identification and mitigation in our
FAIR-compliant dataset. This work is a significant contribution to AI ethics
and data science, advocating for balanced and ethical training methods in LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11048">PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge. (arXiv:2401.11048v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chih-Hsuan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1">Alexis Allot</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1">Po-Ting Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1">Robert Leaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Shubo Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Ling Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhizheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a></p>
<p>PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a
biomedical literature resource using state-of-the-art AI techniques to offer
semantic and relation searches for key concepts like proteins, genetic
variants, diseases, and chemicals. It currently provides over one billion
entity and relation annotations across approximately 36 million PubMed
abstracts and 6 million full-text articles from the PMC open access subset,
updated weekly. PubTator 3.0's online interface and API utilize these
precomputed entity relations and synonyms to provide advanced search
capabilities and enable large-scale analyses, streamlining many complex
information needs. We showcase the retrieval quality of PubTator 3.0 using a
series of entity pair queries, demonstrating that PubTator 3.0 retrieves a
greater number of articles than either PubMed or Google Scholar, with higher
precision in the top 20 results. We further show that integrating ChatGPT
(GPT-4) with PubTator APIs dramatically improves the factuality and
verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive
set of features and tools that allow researchers to navigate the ever-expanding
wealth of biomedical literature, expediting research and unlocking valuable
insights for scientific discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11052">Mining experimental data from Materials Science literature with Large Language Models. (arXiv:2401.11052v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Foppiano_L/0/1/0/all/0/1">Luca Foppiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambard_G/0/1/0/all/0/1">Guillaume Lambard</a>, <a href="http://arxiv.org/find/cs/1/au:+Amagasa_T/0/1/0/all/0/1">Toshiyuki Amagasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1">Masashi Ishii</a></p>
<p>This study is dedicated to evaluating the capabilities of advanced large
language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in the
extraction of structured information from scientific documents within the field
of materials science. We introduce a novel methodology for the comparative
analysis of intricate material expressions, emphasising the standardisation of
chemical formulas to tackle the complexities inherent in materials science
information assessment. To this end, we primarily focus on two critical tasks
of information extraction: (i) a named entity recognition (NER) of studied
materials and physical properties and (ii) a relation extraction (RE) between
these entities. The performance of LLMs in executing these tasks is benchmarked
against traditional models based on the BERT architecture and rule-based
approaches. For NER, LLMs fail to outperform the baseline with zero-shot
prompting and exhibit only limited improvement with few-shot prompting.
However, for RE, a GPT-3.5-Turbo fine-tuned with the appropriate strategy
outperforms all models, including the baseline. Without any fine-tuning, GPT-4
and GPT-4-Turbo display remarkable reasoning and relationship extraction
capabilities after being provided with merely a couple of examples, surpassing
the baseline. Overall, the results suggest that although LLMs demonstrate
relevant reasoning skills in connecting concepts, for tasks requiring
extracting complex domain-specific entities like materials, specialised models
are currently a better choice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11107">Exploiting Duality in Open Information Extraction with Predicate Prompt. (arXiv:2401.11107v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Deqing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huimin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zongyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Rui Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1">Yunsen Xian</a></p>
<p>Open information extraction (OpenIE) aims to extract the schema-free triplets
in the form of (\emph{subject}, \emph{predicate}, \emph{object}) from a given
sentence. Compared with general information extraction (IE), OpenIE poses more
challenges for the IE models, {especially when multiple complicated triplets
exist in a sentence. To extract these complicated triplets more effectively, in
this paper we propose a novel generative OpenIE model, namely \emph{DualOIE},
which achieves a dual task at the same time as extracting some triplets from
the sentence, i.e., converting the triplets into the sentence.} Such dual task
encourages the model to correctly recognize the structure of the given sentence
and thus is helpful to extract all potential triplets from the sentence.
Specifically, DualOIE extracts the triplets in two steps: 1) first extracting a
sequence of all potential predicates, 2) then using the predicate sequence as a
prompt to induce the generation of triplets. Our experiments on two benchmarks
and our dataset constructed from Meituan demonstrate that DualOIE achieves the
best performance among the state-of-the-art baselines. Furthermore, the online
A/B test on Meituan platform shows that 0.93\% improvement of QV-CTR and 0.56\%
improvement of UV-CTR have been obtained when the triplets extracted by DualOIE
were leveraged in Meituan's search system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11120">Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1">David Oniani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xizhi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Visweswaran_S/0/1/0/all/0/1">Shyam Visweswaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1">Sumit Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Kooragayalu_S/0/1/0/all/0/1">Shravan Kooragayalu</a>, <a href="http://arxiv.org/find/cs/1/au:+Polanska_K/0/1/0/all/0/1">Katelyn Polanska</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanshan Wang</a></p>
<p>Background Large Language Models (LLMs), enhanced with Clinical Practice
Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS).
However, methods for incorporating CPGs into LLMs are not well studied. Methods
We develop three distinct methods for incorporating CPGs into LLMs: Binary
Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and
Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of
the proposed methods, we create a set of synthetic patient descriptions and
conduct both automatic and human evaluation of the responses generated by four
LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was
used as the baseline method. We focus on CDS for COVID-19 outpatient treatment
as the case study. Results All four LLMs exhibit improved performance when
enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP
and PAGC in automatic evaluation. All of the proposed methods demonstrated high
performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate
superior performance, as compared to plain LLMs with ZSP, in providing accurate
recommendations for COVID-19 outpatient treatment, which also highlights the
potential for broader applications beyond the case study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11143">Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ioannides_G/0/1/0/all/0/1">Georgios Ioannides</a>, <a href="http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1">Aman Chadha</a>, <a href="http://arxiv.org/find/cs/1/au:+Elkins_A/0/1/0/all/0/1">Aaron Elkins</a></p>
<p>We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM's
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11185">How the Advent of Ubiquitous Large Language Models both Stymie and Turbocharge Dynamic Adversarial Question Generation. (arXiv:2401.11185v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1">Yoo Yeon Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1">Ishani Mondal</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1">Jordan Boyd-Graber</a></p>
<p>Dynamic adversarial question generation, where humans write examples to stump
a model, aims to create examples that are realistic and informative. However,
the advent of large language models (LLMs) has been a double-edged sword for
human authors: more people are interested in seeing and pushing the limits of
these models, but because the models are so much stronger an opponent, they are
harder to defeat. To understand how these models impact adversarial question
writing process, we enrich the writing guidance with LLMs and retrieval models
for the authors to reason why their questions are not adversarial. While
authors could create interesting, challenging adversarial questions, they
sometimes resort to tricks that result in poor questions that are ambiguous,
subjective, or confusing not just to a computer but also to humans. To address
these issues, we propose new metrics and incentives for eliciting good,
challenging questions and present a new dataset of adversarially authored
questions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11206">InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance. (arXiv:2401.11206v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chenkun Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinghao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Ke Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Botian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>With the rapid development of large language models (LLMs), they are not only
used as general-purpose AI assistants but are also customized through further
fine-tuning to meet the requirements of different applications. A pivotal
factor in the success of current LLMs is the alignment process. Current
alignment methods, such as supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), focus on training-time alignment and are
often complex and cumbersome to implement. Therefore, we develop
\textbf{InferAligner}, a novel inference-time alignment method that utilizes
cross-model guidance for harmlessness alignment. InferAligner utilizes safety
steering vectors extracted from safety-aligned model to modify the activations
of the target model when responding to harmful inputs, thereby guiding the
target model to provide harmless responses. Experimental results show that our
method can be very effectively applied to domain-specific models in finance,
medicine, and mathematics, as well as to multimodal large language models
(MLLMs) such as LLaVA. It significantly diminishes the Attack Success Rate
(ASR) of both harmful instructions and jailbreak attacks, while maintaining
almost unchanged performance in downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11207">Unfair TOS: An Automated Approach using Customized BERT. (arXiv:2401.11207v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akash_B/0/1/0/all/0/1">Bathini Sai Akash</a>, <a href="http://arxiv.org/find/cs/1/au:+Kupireddy_A/0/1/0/all/0/1">Akshara Kupireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Murthy_L/0/1/0/all/0/1">Lalita Bhanu Murthy</a></p>
<p>Terms of Service (ToS) form an integral part of any agreement as it defines
the legal relationship between a service provider and an end-user. Not only do
they establish and delineate reciprocal rights and responsibilities, but they
also provide users with information on essential aspects of contracts that
pertain to the use of digital spaces. These aspects include a wide range of
topics, including limitation of liability, data protection, etc. Users tend to
accept the ToS without going through it before using any application or
service. Such ignorance puts them in a potentially weaker situation in case any
action is required. Existing methodologies for the detection or classification
of unfair clauses are however obsolete and show modest performance. In this
research paper, we present SOTA(State of The Art) results on unfair clause
detection from ToS documents based on unprecedented Fine-tuning BERT in
integration with SVC(Support Vector Classifier). The study shows proficient
performance with a macro F1-score of 0.922 at unfair clause detection, and
superior performance is also shown in the classification of unfair clauses by
each tag. Further, a comparative analysis is performed by answering research
questions on the Transformer models utilized. In order to further research and
experimentation the code and results are made available on
https://github.com/batking24/Unfair-TOS-An-Automated-Approach-based-on-Fine-tuning-BERT-in-conjunction-with-ML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11218">End-to-End Argument Mining over Varying Rhetorical Structures. (arXiv:2401.11218v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chistova_E/0/1/0/all/0/1">Elena Chistova</a></p>
<p>Rhetorical Structure Theory implies no single discourse interpretation of a
text, and the limitations of RST parsers further exacerbate inconsistent
parsing of similar structures. Therefore, it is important to take into account
that the same argumentative structure can be found in semantically similar
texts with varying rhetorical structures. In this work, the differences between
paraphrases within the same argument scheme are evaluated from a rhetorical
perspective. The study proposes a deep dependency parsing model to assess the
connection between rhetorical and argument structures. The model utilizes
rhetorical relations; RST structures of paraphrases serve as training data
augmentations. The method allows for end-to-end argumentation analysis using a
rhetorical tree instead of a word sequence. It is evaluated on the bilingual
Microtexts corpus, and the first results on fully-fledged argument parsing for
the Russian version of the corpus are reported. The results suggest that
argument mining can benefit from multiple variants of discourse structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11246">Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine. (arXiv:2401.11246v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1">Bongsu Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jundong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1">Tae-Rim Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Chang-Eop Kim</a></p>
<p>We propose a natural language prompt-based retrieval augmented generation
(Prompt-RAG), a novel approach to enhance the performance of generative large
language models (LLMs) in niche domains. Conventional RAG methods mostly
require vector embeddings, yet the suitability of generic LLM-based embedding
representations for specialized domains remains uncertain. To explore and
exemplify this point, we compared vector embeddings from Korean Medicine (KM)
and Conventional Medicine (CM) documents, finding that KM document embeddings
correlated more with token overlaps and less with human-assessed document
relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from
conventional RAG models, operates without the need for embedding vectors. Its
performance was assessed through a Question-Answering (QA) chatbot application,
where responses were evaluated for relevance, readability, and informativeness.
The results showed that Prompt-RAG outperformed existing models, including
ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and
informativeness. Despite challenges like content structuring and response
latency, the advancements in LLMs are expected to encourage the use of
Prompt-RAG, making it a promising tool for other domains in need of RAG
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11248">Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval. (arXiv:2401.11248v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1">Guangyuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zijia Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Songlin Hu</a></p>
<p>Masked auto-encoder pre-training has emerged as a prevalent technique for
initializing and enhancing dense retrieval systems. It generally utilizes
additional Transformer decoder blocks to provide sustainable supervision
signals and compress contextual information into dense representations.
However, the underlying reasons for the effectiveness of such a pre-training
technique remain unclear. The usage of additional Transformer-based decoders
also incurs significant computational costs. In this study, we aim to shed
light on this issue by revealing that masked auto-encoder (MAE) pre-training
with enhanced decoding significantly improves the term coverage of input tokens
in dense representations, compared to vanilla BERT checkpoints. Building upon
this observation, we propose a modification to the traditional MAE by replacing
the decoder of a masked auto-encoder with a completely simplified Bag-of-Word
prediction task. This modification enables the efficient compression of lexical
signals into dense representations through unsupervised pre-training.
Remarkably, our proposed method achieves state-of-the-art retrieval performance
on several large-scale retrieval benchmarks without requiring any additional
parameters, which provides a 67% training speed-up compared to standard masked
auto-encoder pre-training with enhanced decoding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11268">Word-Level ASR Quality Estimation for Efficient Corpus Sampling and Post-Editing through Analyzing Attentions of a Reference-Free Metric. (arXiv:2401.11268v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javadi_G/0/1/0/all/0/1">Golara Javadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuksel_K/0/1/0/all/0/1">Kamer Ali Yuksel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferreira_T/0/1/0/all/0/1">Thiago Castro Ferreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Badrashiny_M/0/1/0/all/0/1">Mohamed Al-Badrashiny</a></p>
<p>In the realm of automatic speech recognition (ASR), the quest for models that
not only perform with high accuracy but also offer transparency in their
decision-making processes is crucial. The potential of quality estimation (QE)
metrics is introduced and evaluated as a novel tool to enhance explainable
artificial intelligence (XAI) in ASR systems. Through experiments and analyses,
the capabilities of the NoRefER (No Reference Error Rate) metric are explored
in identifying word-level errors to aid post-editors in refining ASR
hypotheses. The investigation also extends to the utility of NoRefER in the
corpus-building process, demonstrating its effectiveness in augmenting datasets
with insightful annotations. The diagnostic aspects of NoRefER are examined,
revealing its ability to provide valuable insights into model behaviors and
decision patterns. This has proven beneficial for prioritizing hypotheses in
post-editing workflows and fine-tuning ASR models. The findings suggest that
NoRefER is not merely a tool for error detection but also a comprehensive
framework for enhancing ASR systems' transparency, efficiency, and
effectiveness. To ensure the reproducibility of the results, all source codes
of this study are made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.01636">Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinyang Liu</a></p>
<p>With the fast development of Deep Learning techniques, Named Entity
Recognition (NER) is becoming more and more important in the information
extraction task. The greatest difficulty that the NER task faces is to keep the
detectability even when types of NE and documents are unfamiliar. Realizing
that the specificity information may contain potential meanings of a word and
generate semantic-related features for word embedding, we develop a
distribution-aware word embedding and implement three different methods to make
use of the distribution information in a NER framework. And the result shows
that the performance of NER will be improved if the word specificity is
incorporated into existing NER methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.14647">Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks. (arXiv:2203.14647v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruiz_Dolz_R/0/1/0/all/0/1">Ramon Ruiz-Dolz</a>, <a href="http://arxiv.org/find/cs/1/au:+Heras_S/0/1/0/all/0/1">Stella Heras</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_Fornes_A/0/1/0/all/0/1">Ana Garc&#xed;a-Fornes</a></p>
<p>The lack of annotated data on professional argumentation and complete
argumentative debates has led to the oversimplification and the inability of
approaching more complex natural language processing tasks. Such is the case of
the automatic debate evaluation. In this paper, we propose an original hybrid
method to automatically evaluate argumentative debates. For that purpose, we
combine concepts from argumentation theory such as argumentation frameworks and
semantics, with Transformer-based architectures and neural graph networks.
Furthermore, we obtain promising results that lay the basis on an unexplored
new instance of the automatic analysis of natural language arguments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.14358">Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications. (arXiv:2206.14358v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yining Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shixu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Plasek_J/0/1/0/all/0/1">Joseph M. Plasek</a>, <a href="http://arxiv.org/find/cs/1/au:+Bates_D/0/1/0/all/0/1">David W. Bates</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Li Zhou</a></p>
<p>Understanding public discourse on emergency use of unproven therapeutics is
crucial for monitoring safe use and combating misinformation. We developed a
natural language processing-based pipeline to comprehend public perceptions of
and stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter
over time. This retrospective study included 609,189 US-based tweets from
January 29, 2020, to November 30, 2021, about four drugs that garnered
significant public attention during the COVID-19 pandemic: (1)
Hydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2)
Molnupiravir and Remdesivir, FDA-approved treatments for eligible patients.
Time-trend analysis was employed to understand popularity trends and related
events. Content and demographic analyses were conducted to explore potential
rationales behind people's stances on each drug. Time-trend analysis indicated
that Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir
and Remdesivir, particularly during COVID-19 surges. Hydroxychloroquine and
Ivermectin discussions were highly politicized, related to conspiracy theories,
hearsay, and celebrity influences. The distribution of stances between the two
major US political parties was significantly different (P &lt; .001); Republicans
were more likely to support Hydroxychloroquine (55%) and Ivermectin (30%) than
Democrats. People with healthcare backgrounds tended to oppose
Hydroxychloroquine (7%) more than the general population, while the general
population was more likely to support Ivermectin (14%). Our study found that
social media users have varying perceptions and stances on off-label versus
FDA-authorized drug use at different stages of COVID-19. This indicates that
health systems, regulatory agencies, and policymakers should design tailored
strategies to monitor and reduce misinformation to promote safe drug use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06419">AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations. (arXiv:2302.06419v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lian_J/0/1/0/all/0/1">Jiachen Lian</a>, <a href="http://arxiv.org/find/eess/1/au:+Baevski_A/0/1/0/all/0/1">Alexei Baevski</a>, <a href="http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1">Wei-Ning Hsu</a>, <a href="http://arxiv.org/find/eess/1/au:+Auli_M/0/1/0/all/0/1">Michael Auli</a></p>
<p>Self-supervision has shown great potential for audio-visual speech
recognition by vastly reducing the amount of labeled data required to build
good systems. However, existing methods are either not entirely end-to-end or
do not train joint representations of both modalities. In this paper, we
introduce AV-data2vec which addresses these challenges and builds audio-visual
representations based on predicting contextualized representations which has
been successful in the uni-modal case. The model uses a shared transformer
encoder for both audio and video and can combine both modalities to improve
speech recognition. Results on LRS3 show that AV-data2vec consistently
outperforms existing methods under all settings with the same amount of data
and model size.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08577">For Generated Text, Is NLI-Neutral Text the Best Text?. (arXiv:2302.08577v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mersinias_M/0/1/0/all/0/1">Michail Mersinias</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1">Kyle Mahowald</a></p>
<p>We explore incorporating natural language inference (NLI) into the text
generative pipeline by using a pre-trained NLI model to assess whether a
generated sentence entails, contradicts, or is neutral to the prompt and
preceding text. First, we show that the NLI task is predictive of generation
errors made by GPT-3. We use these results to develop an NLI-informed
generation procedure for GPT-J. Then, we evaluate these generations by
obtaining human annotations on error types and overall quality. We find that an
NLI strategy of maximizing entailment improves text generation when the nucleus
sampling randomness parameter value is high, while one which maximizes
contradiction is in fact productive when the parameter value is low. Overall,
though, we demonstrate that an NLI strategy of maximizing the neutral class
provides the highest quality of generated text (significantly better than the
vanilla generations), regardless of parameter value.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12584">VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining. (arXiv:2302.12584v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruiz_Dolz_R/0/1/0/all/0/1">Ramon Ruiz-Dolz</a>, <a href="http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1">Javier Iranzo-S&#xe1;nchez</a></p>
<p>In this paper, we describe VivesDebate-Speech, a corpus of spoken
argumentation created to leverage audio features for argument mining tasks. The
creation of this corpus represents an important contribution to the
intersection of speech processing and argument mining communities, and one of
the most complete publicly available resources in this topic. Moreover, we have
performed a set of first-of-their-kind experiments which show an improvement
when integrating audio features into the argument mining pipeline. The provided
results can be used as a baseline for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03047">ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1">Dong An</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hanqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenguan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Keji He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a></p>
<p>Vision-language navigation is a task that requires an agent to follow
instructions to navigate in environments. It becomes increasingly crucial in
the field of embodied AI, with potential applications in autonomous navigation,
search and rescue, and human-robot interaction. In this paper, we propose to
address a more practical yet challenging counterpart setting - vision-language
navigation in continuous environments (VLN-CE). To develop a robust VLN-CE
agent, we propose a new navigation framework, ETPNav, which focuses on two
critical skills: 1) the capability to abstract environments and generate
long-range navigation plans, and 2) the ability of obstacle-avoiding control in
continuous environments. ETPNav performs online topological mapping of
environments by self-organizing predicted waypoints along a traversed path,
without prior environmental experience. It privileges the agent to break down
the navigation procedure into high-level planning and low-level control.
Concurrently, ETPNav utilizes a transformer-based cross-modal planner to
generate navigation plans based on topological maps and instructions. The plan
is then performed through an obstacle-avoiding controller that leverages a
trial-and-error heuristic to prevent navigation from getting stuck in
obstacles. Experimental results demonstrate the effectiveness of the proposed
method. ETPNav yields more than 10% and 20% improvements over prior
state-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is
available at https://github.com/MarSaKi/ETPNav.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14317">ICE-Score: Instructing Large Language Models to Evaluate Code. (arXiv:2304.14317v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1">Terry Yue Zhuo</a></p>
<p>Recent advancements in the field of natural language generation have
facilitated the use of large language models to assess the quality of generated
text. Although these models have shown promising results in tasks such as
machine translation and summarization, their applicability in code intelligence
tasks remains limited without human involvement. The complexity of programming
concepts required for such tasks makes it difficult to develop evaluation
metrics that align with human judgment. Token-matching-based metrics, such as
BLEU, have demonstrated weak correlations with human practitioners in code
intelligence tasks. Moreover, utilizing human-written test suites to evaluate
functional correctness can be challenging in domains with low resources. To
overcome these obstacles, we propose \texttt{ICE-Score}, a new evaluation
metric via instructing large language models (LLMs) for code assessments. Our
metric addresses the limitations of existing approaches by achieving superior
correlations with functional correctness and human preferences, without the
need for test oracles or references. We evaluate the efficacy of our metric on
two different aspects (\textit{human preference} and \textit{execution
success}) and four programming languages. Our results demonstrate that our
metric surpasses state-of-the-art metrics for code generation, delivering high
levels of accuracy and consistency across various programming languages and
tasks. We also make our evaluation metric and datasets available to the
public\footnote{\url{https://github.com/terryyz/ice-score}}, encouraging
further research in evaluating code intelligence tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05352">A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture. (arXiv:2305.05352v6 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1">Jon Whittle</a></p>
<p>The recent release of large language model (LLM) based chatbots, such as
ChatGPT, has attracted huge interest in foundation models. It is widely
believed that foundation models will serve as the fundamental building blocks
for future AI systems. As foundation models are in their early stages, the
design of foundation model based systems has not yet been systematically
explored. There is limited understanding about the impact of introducing
foundation models in software architecture. Therefore, in this paper, we
propose a taxonomy of foundation model based systems, which classifies and
compares the characteristics of foundation models and design options of
foundation model based systems. Our taxonomy comprises three categories: the
pretraining and adaptation of foundation models, the architecture design of
foundation model based systems, and responsible-AI-by-design. This taxonomy can
serve as concrete guidance for making major architectural design decisions when
designing foundation model based systems and highlights trade-offs arising from
design decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14189">Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1">Christof Monz</a></p>
<p>Using a vocabulary that is shared across languages is common practice in
Multilingual Neural Machine Translation (MNMT). In addition to its simple
design, shared tokens play an important role in positive knowledge transfer,
assuming that shared tokens refer to similar meanings across languages.
However, when word overlap is small, especially due to different writing
systems, transfer is inhibited. In this paper, we define word-level information
transfer pathways via word equivalence classes and rely on graph networks to
fuse word embeddings across languages. Our experiments demonstrate the
advantages of our approach: 1) embeddings of words with similar meanings are
better aligned across languages, 2) our method achieves consistent BLEU
improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less
than 1.0\% additional trainable parameters are required with a limited increase
in computational costs, while inference time remains identical to the baseline.
We release the codebase to the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14578">Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification Using Graph Neural Networks?. (arXiv:2305.14578v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bugueno_M/0/1/0/all/0/1">Margarita Bugue&#xf1;o</a>, <a href="http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1">Gerard de Melo</a></p>
<p>Given the success of Graph Neural Networks (GNNs) for structure-aware machine
learning, many studies have explored their use for text classification, but
mostly in specific domains with limited data characteristics. Moreover, some
strategies prior to GNNs relied on graph mining and classical machine learning,
making it difficult to assess their effectiveness in modern settings. This work
extensively investigates graph representation methods for text classification,
identifying practical implications and open challenges. We compare different
graph construction schemes using a variety of GNN architectures and setups
across five datasets, encompassing short and long documents as well as
unbalanced scenarios in diverse domains. Two Transformer-based large language
models are also included to complement the study. The results show that i)
although the effectiveness of graphs depends on the textual input features and
domain, simple graph constructions perform better the longer the documents are,
ii) graph representations are especially beneficial for longer documents,
outperforming Transformer-based models, iii) graph methods are particularly
efficient at solving the task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16326">Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jingcheng Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Keloth_V/0/1/0/all/0/1">Vipina Kuttichi Keloth</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xueqing Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1">Kalpana Raja</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hua Xu</a></p>
<p>Biomedical literature is growing rapidly, making it challenging to curate and
extract knowledge manually. Biomedical natural language processing (BioNLP)
techniques that can automatically extract information from biomedical
literature help alleviate this burden. Recently, large Language Models (LLMs),
such as GPT-3 and GPT-4, have gained significant attention for their impressive
performance. However, their effectiveness in BioNLP tasks and impact on method
development and downstream users remain understudied. This pilot study (1)
establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and
one-shot settings in eight BioNLP datasets across four applications: named
entity recognition, relation extraction, multi-label document classification,
and semantic similarity and reasoning, (2) examines the errors produced by the
LLMs and categorized the errors into three types: missingness, inconsistencies,
and unwanted artificial content, and (3) provides suggestions for using LLMs in
BioNLP applications. We make the datasets, baselines, and results publicly
available to the community via
https://github.com/qingyu-qc/gpt_bionlp_benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00824">Zero and Few-shot Semantic Parsing with Ambiguous Inputs. (arXiv:2306.00824v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1">Elias Stengel-Eskin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rawlins_K/0/1/0/all/0/1">Kyle Rawlins</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a></p>
<p>Despite the frequent challenges posed by ambiguity when representing meaning
via natural language, it is often ignored or deliberately removed in tasks
mapping language to formally-designed representations, which generally assume a
one-to-one mapping between linguistic and formal representations. We attempt to
address this shortcoming by introducing AmP, a framework, dataset, and
challenge for translating ambiguous natural language to formal representations
like logic and code. We define templates and generate data for five
well-documented linguistic ambiguities. Using AmP, we investigate how several
few-shot text-to-code systems handle ambiguity, introducing three new metrics.
We find that large pre-trained models perform poorly at capturing the
distribution of possible meanings without deliberate instruction. However,
models are able to capture the distribution well when ambiguity is attested in
their inputs. These results motivate a call for including ambiguity explicitly
in datasets and promote considering the distribution of possible outputs when
evaluating systems. Data and code: https://github.com/esteng/ambiguous_parsing
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16001">Streamlining Social Media Information Extraction for Public Health Research with Deep Learning. (arXiv:2306.16001v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yining Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shixu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yujie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Foer_D/0/1/0/all/0/1">Dinah Foer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Peilin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Li Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Objective: Social media-based public health research is crucial for epidemic
surveillance, but most studies identify relevant corpora with keyword matching.
This study develops a system to streamline the process of curating colloquial
medical dictionaries. We demonstrate the pipeline by curating a UMLS-colloquial
symptom dictionary from COVID-19-related tweets as proof of concept. Methods:
COVID-19-related tweets from February 1, 2020, to April 30, 2022 were used. The
pipeline includes three modules: a named entity recognition module to detect
symptoms in tweets; an entity normalization module to aggregate detected
entities; and a mapping module that iteratively maps entities to Unified
Medical Language System concepts. A random 500 entity sample were drawn from
the final dictionary for accuracy validation. Additionally, we conducted a
symptom frequency distribution analysis to compare our dictionary to a
pre-defined lexicon from previous research. Results: We identified 498,480
unique symptom entity expressions from the tweets. Pre-processing reduces the
number to 18,226. The final dictionary contains 38,175 unique expressions of
symptoms that can be mapped to 966 UMLS concepts (accuracy = 95%). Symptom
distribution analysis found that our dictionary detects more symptoms and is
effective at identifying psychiatric disorders like anxiety and depression,
often missed by pre-defined lexicons. Conclusion: This study advances public
health research by implementing a novel, systematic pipeline for curating
symptom lexicons from social media data. The final lexicon's high accuracy,
validated by medical professionals, underscores the potential of this
methodology to reliably interpret and categorize vast amounts of unstructured
social media data into actionable medical insights across diverse linguistic
and regional landscapes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04408">TIM: Teaching Large Language Models to Translate with Comparison. (arXiv:2307.04408v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jiali Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yongjing Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Open-sourced large language models (LLMs) have demonstrated remarkable
efficacy in various tasks with instruction tuning. However, these models can
sometimes struggle with tasks that require more specialized knowledge such as
translation. One possible reason for such deficiency is that instruction tuning
aims to generate fluent and coherent text that continues from a given
instruction without being constrained by any task-specific requirements.
Moreover, it can be more challenging for tuning smaller LLMs with lower-quality
training data. To address this issue, we propose a novel framework using
examples in comparison to teach LLMs to learn translation. Our approach
involves presenting the model with examples of correct and incorrect
translations and using a preference loss to guide the model's learning. We
evaluate our method on WMT2022 test sets and show that it outperforms existing
methods. Our findings offer a new perspective on fine-tuning LLMs for
translation tasks and provide a promising solution for generating high-quality
translations. Please refer to Github for more details:
https://github.com/lemon0830/TIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01538">ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning. (arXiv:2309.01538v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Linhao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1">Jiaxin Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1">Bo Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan-Fang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1">Gholamreza Haffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a></p>
<p>Logical rules are essential for uncovering the logical connections between
relations, which could improve reasoning performance and provide interpretable
results on knowledge graphs (KGs). Although there have been many efforts to
mine meaningful logical rules over KGs, existing methods suffer from
computationally intensive searches over the rule space and a lack of
scalability for large-scale KGs. Besides, they often ignore the semantics of
relations which is crucial for uncovering logical connections. Recently, large
language models (LLMs) have shown impressive performance in the field of
natural language processing and various applications, owing to their emergent
ability and generalizability. In this paper, we propose a novel framework,
ChatRule, unleashing the power of large language models for mining logical
rules over knowledge graphs. Specifically, the framework is initiated with an
LLM-based rule generator, leveraging both the semantic and structural
information of KGs to prompt LLMs to generate logical rules. To refine the
generated rules, a rule ranking module estimates the rule quality by
incorporating facts from existing KGs. Last, the ranked rules can be used to
conduct reasoning over KGs. ChatRule is evaluated on four large-scale KGs,
w.r.t. different rule quality metrics and downstream tasks, showing the
effectiveness and scalability of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05173">DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhengxiang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1">Aldo Lipani</a></p>
<p>Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer's quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving substantial memory and time costs compared to vanilla
PT and its variants, without changing trainable parameter sizes. Through
extensive experiments on 23 natural language processing (NLP) and
vision-language (VL) tasks, we demonstrate that DePT outperforms
state-of-the-art PEFT approaches, including the full fine-tuning baseline, in
some scenarios. Additionally, we empirically show that DEPT grows more
efficient as the model size increases. Our further study reveals that DePT
integrates seamlessly with parameter-efficient transfer learning in the
few-shot learning setting and highlights its adaptability to various model
architectures and sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12244">ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seo_W/0/1/0/all/0/1">Woosuk Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chanmo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Young-Ho Kim</a></p>
<p>Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the findings, we discuss opportunities for
leveraging LLMs to design child-friendly chatbots to support children in
sharing emotions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12247">Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Beizhe Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1">Qiang Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Juan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuhui Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Danding Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1">Peng Qi</a></p>
<p>Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations. Recent advances in large language models (LLMs) have
shown remarkable performance in various tasks, but whether and how LLMs could
help with fake news detection remains underexplored. In this paper, we
investigate the potential of LLMs in fake news detection. First, we conduct an
empirical study and find that a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis
attributes such a gap to the LLM's inability to select and integrate rationales
properly to conclude. Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection but can be a good advisor
for SLMs by providing multi-perspective instructive rationales. To instantiate
this proposal, we design an adaptive rationale guidance network for fake news
detection (ARG), in which SLMs selectively acquire insights on news analysis
from the LLMs' rationales. We further derive a rationale-free version of ARG by
distillation, namely ARG-D, which services cost-sensitive scenarios without
querying LLMs. Experiments on two real-world datasets demonstrate that ARG and
ARG-D outperform three types of baseline methods, including SLM-based,
LLM-based, and combinations of small and large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01361">GenSim: Generating Robotic Simulation Tasks via Large Language Models. (arXiv:2310.01361v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lirui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1">Yiyang Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zhecheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1">Mohit Shridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1">Chen Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yuzhe Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a></p>
<p>Collecting large amounts of real-world interaction data to train general
robotic policies is often prohibitively expensive, thus motivating the use of
simulation data. However, existing methods for data generation have generally
focused on scene-level diversity (e.g., object instances and poses) rather than
task-level diversity, due to the human effort required to come up with and
verify novel tasks. This has made it challenging for policies trained on
simulation data to demonstrate significant task-level generalization. In this
paper, we propose to automatically generate rich simulation environments and
expert demonstrations by exploiting a large language models' (LLM) grounding
and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed
generation, wherein a target task is given to the LLM and the LLM proposes a
task curriculum to solve the target task, and exploratory generation, wherein
the LLM bootstraps from previous tasks and iteratively proposes novel tasks
that would be helpful in solving more complex tasks. We use GPT4 to expand the
existing benchmark by ten times to over 100 tasks, on which we conduct
supervised finetuning and evaluate several LLMs including finetuned GPTs and
Code Llama on code generation for robotic simulation tasks. Furthermore, we
observe that LLMs-generated simulation programs can enhance task-level
generalization significantly when used for multitask policy training. We
further find that with minimal sim-to-real adaptation, the multitask policies
pretrained on GPT4-generated simulation tasks exhibit stronger transfer to
unseen long-horizon tasks in the real world and outperform baselines by 25%.
See the project website (https://liruiw.github.io/gensim) for code, demos, and
videos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01386">Who is ChatGPT? Benchmarking LLMs&#x27; Psychological Portrayal Using PsychoBench. (arXiv:2310.01386v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jen-tse Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1">Eric John Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1">Man Ho Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shujie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Youliang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1">Wenxiang Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1">Michael R. Lyu</a></p>
<p>Large Language Models (LLMs) have recently showcased their remarkable
capacities, not only in natural language processing tasks but also across
diverse domains such as clinical medicine, legal consultation, and education.
LLMs become more than mere applications, evolving into assistants capable of
addressing diverse user requests. This narrows the distinction between human
beings and artificial intelligence agents, raising intriguing questions
regarding the potential manifestation of personalities, temperaments, and
emotions within LLMs. In this paper, we propose a framework, PsychoBench, for
evaluating diverse psychological aspects of LLMs. Comprising thirteen scales
commonly used in clinical psychology, PsychoBench further classifies these
scales into four distinct categories: personality traits, interpersonal
relationships, motivational tests, and emotional abilities. Our study examines
five popular models, namely text-davinci-003, gpt-3.5-turbo, gpt-4, LLaMA-2-7b,
and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the
safety alignment protocols and test the intrinsic natures of LLMs. We have made
PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02118">TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus. (arXiv:2310.02118v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferreira_R/0/1/0/all/0/1">Rafael Ferreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Tavares_D/0/1/0/all/0/1">Diogo Tavares</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1">Diogo Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Valerio_R/0/1/0/all/0/1">Rodrigo Val&#xe9;rio</a>, <a href="http://arxiv.org/find/cs/1/au:+Bordalo_J/0/1/0/all/0/1">Jo&#xe3;o Bordalo</a>, <a href="http://arxiv.org/find/cs/1/au:+Simoes_I/0/1/0/all/0/1">In&#xea;s Sim&#xf5;es</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_V/0/1/0/all/0/1">Vasco Ramos</a>, <a href="http://arxiv.org/find/cs/1/au:+Semedo_D/0/1/0/all/0/1">David Semedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Magalhaes_J/0/1/0/all/0/1">Jo&#xe3;o Magalh&#xe3;es</a></p>
<p>In this report, we describe the vision, challenges, and scientific
contributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot
Challenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal,
knowledgeable, and engaging assistant that can guide users towards the
successful completion of complex manual tasks. To achieve this, we focus our
efforts on three main research questions: (1) Humanly-Shaped Conversations, by
providing information in a knowledgeable way; (2) Multimodal Stimulus, making
use of various modalities including voice, images, and videos; and (3)
Zero-shot Conversational Flows, to improve the robustness of the interaction to
unseen scenarios. TWIZ is an assistant capable of supporting a wide range of
tasks, with several innovative features such as creative cooking, video
navigation through voice, and the robust TWIZ-LLM, a Large Language Model
trained for dialoguing about complex manual tasks. Given ratings and feedback
provided by users, we observed that TWIZ bot is an effective and robust system,
capable of guiding users through tasks while providing several multimodal
stimuli.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02255">MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. (arXiv:2310.02255v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Pan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1">Hritik Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tony Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiacheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1">Michel Galley</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04691">EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Siyu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kenny Q. Zhu</a></p>
<p>Neural language models are probabilistic models of human text. They are
predominantly trained using maximum likelihood estimation (MLE), which is
equivalent to minimizing the forward cross-entropy between the empirical data
distribution and the model distribution. However, various degeneration
phenomena are still widely observed when decoding from the distributions
learned by such models. We establish that the forward cross-entropy is
suboptimal as a distance metric for aligning human and model distribution due
to its (1) recall-prioritization (2) negative diversity ignorance and (3)
train-test mismatch. In this paper, we propose Earth Mover Distance
Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on
the inherent properties of earth mover distance to address the aforementioned
challenges. Due to the high complexity of direct computation, we further
introduce a feasible upper bound for EMO to ease end-to-end training. Upon
extensive evaluation of language models trained using EMO and MLE. We find that
EMO demonstrates a consistently better language modeling performance than MLE
across domains. Moreover, EMO demonstrates noteworthy enhancements in
downstream performance with minimal fine-tuning on merely 25,000 sentences.
This highlights the tremendous potential of EMO as a lightweight calibration
method for enhancing large-scale pre-trained language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10873">IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models. (arXiv:2310.10873v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaokun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1">Xiaobo Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling-Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiale Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingyun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a></p>
<p>In-context learning is a promising paradigm that utilizes in-context examples
as prompts for the predictions of large language models. These prompts are
crucial for achieving strong performance. However, since the prompts need to be
sampled from a large volume of annotated examples, finding the right prompt may
result in high annotation costs. To address this challenge, this paper
introduces an influence-driven selective annotation method that aims to
minimize annotation costs while improving the quality of in-context examples.
The essence of our method is to select a pivotal subset from a large-scale
unlabeled data pool to annotate for the subsequent sampling of prompts.
Specifically, a directed graph is first constructed to represent unlabeled
data. Afterward, the influence of candidate unlabeled subsets is quantified
with a diffusion process. A simple yet effective greedy algorithm for unlabeled
data selection is lastly introduced. It iteratively selects the data if it
provides a maximum marginal gain with respect to quantified influence. Compared
with previous efforts on selective annotations, our influence-driven method
works in an end-to-end manner, avoids an intractable explicit balance between
data diversity and representativeness, and enjoys theoretical support.
Experiments confirm the superiority of the proposed method on various
benchmarks, achieving better performance under lower time consumption during
subset selection. The project page is available at
https://skzhang1.github.io/IDEAL/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15823">Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+ElBakry_A/0/1/0/all/0/1">Ahmed ElBakry</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabr_M/0/1/0/all/0/1">Mohamed Gabr</a>, <a href="http://arxiv.org/find/cs/1/au:+ElNokrashy_M/0/1/0/all/0/1">Muhammad ElNokrashy</a>, <a href="http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1">Badr AlKhamissi</a></p>
<p>A Reverse Dictionary is a tool enabling users to discover a word based on its
provided definition, meaning, or description. Such a technique proves valuable
in various scenarios, aiding language learners who possess a description of a
word without its identity, and benefiting writers seeking precise terminology.
These scenarios often encapsulate what is referred to as the
"Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning
solution for the Arabic Reverse Dictionary shared task. This task focuses on
deriving a vector representation of an Arabic word from its accompanying
description. The shared task encompasses two distinct subtasks: the first
involves an Arabic definition as input, while the second employs an English
definition. For the first subtask, our approach relies on an ensemble of
finetuned Arabic BERT-based models, predicting the word embedding for a given
definition. The final representation is obtained through averaging the output
embeddings from each model within the ensemble. In contrast, the most effective
solution for the second subtask involves translating the English test
definitions into Arabic and applying them to the finetuned models originally
trained for the first subtask. This straightforward method achieves the highest
score across both subtasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07989">Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code. (arXiv:2311.07989v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziyin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingchang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Cong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zi Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a></p>
<p>In this work we systematically review the recent advancements in code
processing with language models, covering 50+ models, 30+ evaluation tasks,
170+ datasets, and 700+ related works. We break down code processing models
into general language models represented by the GPT family and specialized
models that are specifically pretrained on code, often with tailored
objectives. We discuss the relations and differences between these models, and
highlight the historical transition of code modeling from statistical models
and RNNs to pretrained Transformers and LLMs, which is exactly the same course
that had been taken by NLP. We also discuss code-specific features such as AST,
CFG, and unit tests, along with their application in training code language
models, and identify key challenges and potential future directions in this
domain. We keep the survey open and updated on GitHub at
https://github.com/codefuse-ai/Awesome-Code-LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14212">Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kern_C/0/1/0/all/0/1">Christoph Kern</a>, <a href="http://arxiv.org/find/stat/1/au:+Eckman_S/0/1/0/all/0/1">Stephanie Eckman</a>, <a href="http://arxiv.org/find/stat/1/au:+Beck_J/0/1/0/all/0/1">Jacob Beck</a>, <a href="http://arxiv.org/find/stat/1/au:+Chew_R/0/1/0/all/0/1">Rob Chew</a>, <a href="http://arxiv.org/find/stat/1/au:+Ma_B/0/1/0/all/0/1">Bolei Ma</a>, <a href="http://arxiv.org/find/stat/1/au:+Kreuter_F/0/1/0/all/0/1">Frauke Kreuter</a></p>
<p>When training data are collected from human annotators, the design of the
annotation instrument, the instructions given to annotators, the
characteristics of the annotators, and their interactions can impact training
data. This study demonstrates that design choices made when creating an
annotation instrument also impact the models trained on the resulting
annotations. We introduce the term annotation sensitivity to refer to the
impact of annotation data collection methods on the annotations themselves and
on downstream model performance and predictions. We collect annotations of hate
speech and offensive language in five experimental conditions of an annotation
instrument, randomly assigning annotators to conditions. We then fine-tune BERT
models on each of the five resulting datasets and evaluate model performance on
a holdout portion of each condition. We find considerable differences between
the conditions for 1) the share of hate speech/offensive language annotations,
2) model performance, 3) model predictions, and 4) model learning curves. Our
results emphasize the crucial role played by the annotation instrument which
has received little attention in the machine learning literature. We call for
additional research into how and why the instrument impacts the annotations to
inform the development of best practices in instrument design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02317">GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs. (arXiv:2312.02317v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1">Luca Rossetto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1">Michael Cochez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1">Abraham Bernstein</a></p>
<p>Most current methods for multi-hop question answering (QA) over knowledge
graphs (KGs) only provide final conclusive answers without explanations, such
as a set of KG entities that is difficult for normal users to review and
comprehend. This issue severely limits the application of KG-based QA in
real-world scenarios. However, it is non-trivial to solve due to two
challenges: First, annotations of reasoning chains of multi-hop questions,
which could serve as supervision for explanation generation, are usually
lacking. Second, it is difficult to maintain high efficiency when explicit KG
triples need to be retrieved to generate explanations. In this paper, we
propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to
solve this issue. GNN2R can provide both final answers and reasoning subgraphs
as a rationale behind final answers efficiently with only weak supervision that
is available through question-final answer pairs. We extensively evaluated
GNN2R with detailed analyses in experiments. The results demonstrate that, in
terms of effectiveness, efficiency, and quality of generated explanations,
GNN2R outperforms existing state-of-the-art methods that are applicable to this
task. Our code and pre-trained models are available at
https://github.com/ruijie-wang-uzh/GNN2R.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03122">Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations. (arXiv:2312.03122v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shahriar_T/0/1/0/all/0/1">Tasmia Shahriar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_K/0/1/0/all/0/1">Kelly Ramos</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1">Noboru Matsuda</a></p>
<p>Human educators possess an intrinsic ability to anticipate and seek
educational explanations from students, which drives them to pose
thought-provoking questions when students cannot articulate these explanations
independently. We aim to imbue Intelligent Tutoring Systems with this ability
using few-shot learning capability of Large Language Models. Our work proposes
a novel prompting technique, Assertion Enhanced Few-Shot Learning, to
facilitate the generation of accurate, detailed oriented educational
explanations. Our central hypothesis is that, in educational domain, few-shot
demonstrations are necessary but not a sufficient condition for quality
explanation generation. We conducted a study involving 12 in-service teachers,
comparing our approach to Traditional Few-Shot Learning. The results show that
Assertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and
yields higher-quality explanations, as evaluated by teachers. We also conduct a
qualitative ablation study to factor the impact of assertions to provide
educator-friendly prompting guidelines for generating explanations in their
domain of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07930">Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Baihe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hanlin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jiantao Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1">Michael I. Jordan</a></p>
<p>We study statistical watermarking by formulating it as a hypothesis testing
problem, a general framework which subsumes all previous statistical
watermarking methods. Key to our formulation is a coupling of the output tokens
and the rejection region, realized by pseudo-random generators in practice,
that allows non-trivial trade-off between the Type I error and Type II error.
We characterize the Uniformly Most Powerful (UMP) watermark in the general
hypothesis testing setting and the minimax Type II error in the model-agnostic
setting. In the common scenario where the output is a sequence of $n$ tokens,
we establish nearly matching upper and lower bounds on the number of i.i.d.
tokens required to guarantee small Type I and Type II errors. Our rate of
$\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$
highlights potentials for improvement from the rate of $h^{-2}$ in the previous
works. Moreover, we formulate the robust watermarking problem where users are
allowed to perform a class of perturbations on the generated texts, and
characterize the optimal type II error of robust UMP tests via a linear
programming problem. To the best of our knowledge, this is the first systematic
statistical treatment on the watermarking problem with near-optimal rates in
the i.i.d. setting, which might be of interest for future works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11532">Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1">YoungJoon Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jongwon Choi</a></p>
<p>This paper introduces a novel approach for topic modeling utilizing latent
codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely
encapsulating the rich information of the pre-trained embeddings such as the
pre-trained language model. From the novel interpretation of the latent
codebooks and embeddings as conceptual bag-of-words, we propose a new
generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates
the original documents related to the respective latent codebook. The TVQ-VAE
can visualize the topics with various generative distributions including the
traditional BoW distribution and the autoregressive image generation. Our
experimental results on document analysis and image generation demonstrate that
TVQ-VAE effectively captures the topic context which reveals the underlying
structures of the dataset and supports flexible forms of document generation.
Official implementation of the proposed TVQ-VAE is available at
https://github.com/clovaai/TVQ-VAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15407">A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators. (arXiv:2312.15407v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1">Luis Fernando D&#x27;Haro</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Malu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a></p>
<p>Automatic evaluation is an integral aspect of dialogue system research. The
traditional reference-based NLG metrics are generally found to be unsuitable
for dialogue assessment. Consequently, recent studies have suggested various
unique, reference-free neural metrics that better align with human evaluations.
Notably among them, large language models (LLMs), particularly the
instruction-tuned variants like ChatGPT, are shown to be promising substitutes
for human judges. Yet, existing works on utilizing LLMs for automatic dialogue
evaluation are limited in their scope in terms of the number of meta-evaluation
datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains
inconclusive how effective these LLMs are. To this end, we conduct a
comprehensive study on the application of LLMs for automatic dialogue
evaluation. Specifically, we analyze the multi-dimensional evaluation
capability of 30 recently emerged LLMs at both turn and dialogue levels, using
a comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the
robustness of the LLMs in handling various adversarial perturbations at both
turn and dialogue levels. Finally, we explore how model-level and
dimension-level ensembles impact the evaluation performance. All resources are
available at https://github.com/e0397123/comp-analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17080">MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation. (arXiv:2312.17080v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhongshen Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pengguang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haiyun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jiaya Jia</a></p>
<p>In this work, we introduce a novel evaluation paradigm for Large Language
Models, one that challenges them to engage in meta-reasoning. This approach
addresses critical shortcomings in existing math problem-solving benchmarks,
traditionally used to evaluate the cognitive capabilities of agents. Our
paradigm shifts the focus from result-oriented assessments, which often
overlook the reasoning process, to a more holistic evaluation that effectively
differentiates the cognitive capabilities among models. For example, in our
benchmark, GPT-4 demonstrates a performance five times better than GPT3-5. The
significance of this new paradigm lies in its ability to reveal potential
cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to
uncover due to their saturation and lack of effective differentiation among
varying reasoning abilities. Our comprehensive analysis includes several
state-of-the-art math models from both open-source and closed-source
communities, uncovering fundamental deficiencies in their training and
evaluation approaches. This paper not only advocates for a paradigm shift in
the assessment of LLMs but also contributes to the ongoing discourse on the
trajectory towards Artificial General Intelligence (AGI). By promoting the
adoption of meta-reasoning evaluation methods similar to ours, we aim to
facilitate a more accurate assessment of the true cognitive abilities of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04620">Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shimin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Agents based on Large Language Models (LLMs) are increasingly permeating
various domains of human production and life, highlighting the importance of
aligning them with human values. The current alignment of AI systems primarily
focuses on passively aligning LLMs through human intervention. However, agents
possess characteristics like receiving environmental feedback and
self-evolution, rendering the LLM alignment methods inadequate. In response, we
propose an evolutionary framework for agent evolution and alignment, named
EvolutionaryAgent, which transforms agent alignment into a process of evolution
and selection under the principle of survival of the fittest. In an environment
where social norms continuously evolve, agents better adapted to the current
social norms will have a higher probability of survival and proliferation,
while those inadequately aligned dwindle over time. Experimental results
assessing the agents from multiple perspectives in aligning with social norms
demonstrate that EvolutionaryAgent can align progressively better with the
evolving social norms while maintaining its proficiency in general tasks.
Effectiveness tests conducted on various open and closed-source LLMs as the
foundation for agents also prove the applicability of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04925">The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Mingyu Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qinkai Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_D/0/1/0/all/0/1">Dong Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haiyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1">Yanda Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1">Mengnan Du</a></p>
<p>Chain of Thought (CoT) is significant in improving the reasoning abilities of
large language models (LLMs). However, the correlation between the
effectiveness of CoT and the length of reasoning steps in prompts remains
largely unknown. To shed light on this, we have conducted several empirical
experiments to explore the relations. Specifically, we design experiments that
expand and compress the rationale reasoning steps within CoT demonstrations,
while keeping all other factors constant. We have the following key findings.
First, the results indicate that lengthening the reasoning steps in prompts,
even without adding new information into the prompt, considerably enhances
LLMs' reasoning abilities across multiple datasets. Alternatively, shortening
the reasoning steps, even while preserving the key information, significantly
diminishes the reasoning abilities of models. This finding highlights the
importance of the number of steps in CoT prompts and provides practical
guidance to make better use of LLMs' potential in complex problem-solving
scenarios. Second, we also investigated the relationship between the
performance of CoT and the rationales used in demonstrations. Surprisingly, the
result shows that even incorrect rationales can yield favorable outcomes if
they maintain the requisite length of inference. Third, we observed that the
advantages of increasing reasoning steps are task-dependent: simpler tasks
require fewer steps, whereas complex tasks gain significantly from longer
inference sequences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05949">Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1">Meihuizi Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jinming Wen</a></p>
<p>In-context learning, a paradigm bridging the gap between pre-training and
fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in
few-shot settings. Unlike traditional fine-tuning methods, in-context learning
adapts pre-trained models to unseen tasks without updating any parameters.
Despite being widely applied, in-context learning is vulnerable to malicious
attacks. In this work, we raise security concerns regarding this paradigm. Our
studies demonstrate that an attacker can manipulate the behavior of large
language models by poisoning the demonstration context, without the need for
fine-tuning the model. Specifically, we have designed a new backdoor attack
method, named ICLAttack, to target large language models based on in-context
learning. Our method encompasses two types of attacks: poisoning demonstration
examples and poisoning prompts, which can make models behave in accordance with
predefined intentions. ICLAttack does not require additional fine-tuning to
implant a backdoor, thus preserving the model's generality. Furthermore, the
poisoned examples are correctly labeled, enhancing the natural stealth of our
attack method. Extensive experimental results across several language models,
ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of
our attack method, exemplified by a high average attack success rate of 95.0%
across the three datasets on OPT models. Our findings highlight the
vulnerabilities of language models, and we hope this work will raise awareness
of the possible security threats associated with in-context learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06766">Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements. (arXiv:2401.06766v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Voronov_A/0/1/0/all/0/1">Anton Voronov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lena Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1">Max Ryabinin</a></p>
<p>Large language models demonstrate a remarkable capability for learning to
solve new tasks from a few examples. The prompt template, or the way the input
examples are formatted to obtain the prompt, is an important yet often
overlooked aspect of in-context learning. In this work, we conduct a
comprehensive study of the template format's influence on the in-context
learning performance. We evaluate the impact of the prompt template across
models (from 770M to 70B parameters) and 4 standard classification datasets. We
show that a poor choice of the template can reduce the performance of the
strongest models and inference methods to a random guess level. More
importantly, the best templates do not transfer between different setups and
even between models of the same family. Our findings show that the currently
prevalent approach to evaluation, which ignores template selection, may give
misleading results due to different templates in different works. As a first
step towards mitigating this issue, we propose Template Ensembles that
aggregate model predictions across several templates. This simple test-time
augmentation boosts average performance while being robust to the choice of
random set of templates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.07510">Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a></p>
<p>ChatGPT explores a strategic blueprint of question answering (QA) in
delivering medical diagnosis, treatment recommendations, and other healthcare
support. This is achieved through the increasing incorporation of medical
domain data via natural language processing (NLP) and multimodal paradigms. By
transitioning the distribution of text, images, videos, and other modalities
from the general domain to the medical domain, these techniques have expedited
the progress of medical domain question answering (MDQA). They bridge the gap
between human natural language and sophisticated medical domain knowledge or
expert manual annotations, handling large-scale, diverse, unbalanced, or even
unlabeled data analysis scenarios in medical contexts. Central to our focus is
the utilizing of language models and multimodal paradigms for medical question
answering, aiming to guide the research community in selecting appropriate
mechanisms for their specific medical research requirements. Specialized tasks
such as unimodal-related question answering, reading comprehension, reasoning,
diagnosis, relation extraction, probability modeling, and others, as well as
multimodal-related tasks like vision question answering, image caption,
cross-modal retrieval, report summarization, and generation, are discussed in
detail. Each section delves into the intricate specifics of the respective
method under consideration. This paper highlights the structures and
advancements of medical domain explorations against general domain methods,
emphasizing their applications across different tasks and datasets. It also
outlines current challenges and opportunities for future medical domain
research, paving the way for continued innovation and application in this
rapidly evolving field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09003">Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoxiong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1">Andrew Chi-Chih Yao</a></p>
<p>Despite recent progress in improving the mathematical reasoning ability of
large language models(LLMs), solving competition-level math problems without
the use of external tools remains challenging for open-source LLMs. In this
work, we introduce the MMIQC dataset, a mixture of processed web data and
synthetic question-response pairs, to equip base models with better
mathematical reasoning skills. In different model sizes, the models fine-tuned
on MMIQC consistently outperform their counterparts by a clear margin on MATH
test set. Notably, DeepSeek-67B-MMIQC achieves a 41.0% accuracy, 4.2% higher
than the previous open-source SOTA. Our experiments also show that a large part
of the improvement can be attributed to our novel augmentation method
IQC(Iterative Question Composing), where we iteratively ask an LLM to compose
new questions from the given seed problems and do rejection sampling from
another LLM. MMIQC has now been released on
https://huggingface.co/datasets/Vivacem/MMIQC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09074">Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1">Emanuele La Malfa</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1">Christoph Weinhuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Torre_O/0/1/0/all/0/1">Orazio Torre</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Fangru Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1">Anthony Cohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1">Nigel Shadbolt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1">Michael Wooldridge</a></p>
<p>We investigate the extent to which Large Language Models (LLMs) can simulate
the execution of computer code and algorithms. We begin by looking at straight
line programs, and show that current LLMs demonstrate poor performance even
with such simple programs -- performance rapidly degrades with the length of
code. We then investigate the ability of LLMs to simulate programs that contain
critical paths and redundant instructions. We also go beyond straight line
program simulation with sorting algorithms and nested loops, and we show the
computational complexity of a routine directly affects the ability of an LLM to
simulate its execution. We observe that LLMs execute instructions sequentially
and with a low error margin only for short programs or standard procedures.
LLMs' code simulation is in tension with their pattern recognition and
memorisation capabilities: on tasks where memorisation is detrimental, we
propose a novel prompting method to simulate code execution line by line.
Empirically, our new Chain of Simulation (CoSm) method improves on the standard
Chain of Thought prompting approach by avoiding the pitfalls of memorisation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09333">Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora. (arXiv:2401.09333v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gordillo_D/0/1/0/all/0/1">Diana Davila Gordillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Timoneda_J/0/1/0/all/0/1">Joan Timoneda</a>, <a href="http://arxiv.org/find/cs/1/au:+Vera_S/0/1/0/all/0/1">Sebastian Vallejo Vera</a></p>
<p>Current methods to identify and classify racist language in text rely on
small-n qualitative approaches or large-n approaches focusing exclusively on
overt forms of racist discourse. This article provides a step-by-step
generalizable guideline to identify and classify different forms of racist
discourse in large corpora. In our approach, we start by conceptualizing racism
and its different manifestations. We then contextualize these racist
manifestations to the time and place of interest, which allows researchers to
identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a
cross-lingual model for supervised text classification with a cutting-edge
contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our
pretrained model, outperform other state-of-the-art approaches in classifying
racism in large corpora. We illustrate our approach using a corpus of tweets
relating to the Ecuadorian ind\'igena community between 2018 and 2021.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09798">All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Takemoto_K/0/1/0/all/0/1">Kazuhiro Takemoto</a></p>
<p>Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where
safeguards are bypassed to produce ethically harmful prompts. This study
proposes a simple black-box method to effectively generate jailbreak prompts,
overcoming the high complexity and computational costs associated with existing
methods. The proposed technique iteratively rewrites harmful prompts into
non-harmful expressions using the target LLM itself, based on the hypothesis
that LLMs can directly sample expressions that bypass safeguards. Demonstrated
through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this
method achieved an attack success rate of over 80% within an average of 5
iterations and remained effective despite model updates. The generated
jailbreak prompts were naturally-worded and concise; moreover, they were
difficult-to-defend. These results indicate that creating effective jailbreak
prompts is simpler than previously considered, suggesting that black-box
jailbreak attacks pose a more serious threat.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10015">Towards Hierarchical Spoken Language Dysfluency Modeling. (arXiv:2401.10015v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1">Jiachen Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Anumanchipalli_G/0/1/0/all/0/1">Gopala Anumanchipalli</a></p>
<p>Speech disfluency modeling is the bottleneck for both speech therapy and
language learning. However, there is no effective AI solution to systematically
tackle this problem. We solidify the concept of disfluent speech and disfluent
speech modeling. We then present Hierarchical Unconstrained Disfluency Modeling
(H-UDM) approach, the hierarchical extension of UDM that addresses both
disfluency transcription and detection to eliminate the need for extensive
manual annotation. Our experimental findings serve as clear evidence of the
effectiveness and reliability of the methods we have introduced, encompassing
both transcription and detection tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10189">Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qingyun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongxiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Huimin Zhao</a></p>
<p>Fine-grained few-shot entity extraction in the chemical domain faces two
unique challenges. First, compared with entity extraction tasks in the general
domain, sentences from chemical papers usually contain more entities. Moreover,
entity extraction models usually have difficulty extracting entities of
long-tailed types. In this paper, we propose Chem-FINESE, a novel
sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to
address these two challenges. Our Chem-FINESE has two components: a seq2seq
entity extractor to extract named entities from the input sentence and a
seq2seq self-validation module to reconstruct the original input sentence from
extracted entities. Inspired by the fact that a good entity extraction system
needs to extract entities faithfully, our new self-validation module leverages
entity extraction results to reconstruct the original input sentence. Besides,
we design a new contrastive loss to reduce excessive copying during the
extraction process. Finally, we release ChemNER+, a new fine-grained chemical
entity extraction dataset that is annotated by domain experts with the ChemNER
schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets
show that our newly proposed framework has contributed up to 8.26% and 6.84%
absolute F1-score gains respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10337">Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tu Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Srndic_N/0/1/0/all/0/1">Nedim Srndic</a>, <a href="http://arxiv.org/find/cs/1/au:+Neth_A/0/1/0/all/0/1">Alexander Neth</a></p>
<p>Tactics, Techniques and Procedures (TTPs) represent sophisticated attack
patterns in the cybersecurity domain, described encyclopedically in textual
knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP
mapping, is an important and challenging task. Conventional learning approaches
often target the problem in the classical multi-class or multilabel
classification setting. This setting hinders the learning ability of the model
due to a large number of classes (i.e., TTPs), the inevitable skewness of the
label distribution and the complex hierarchical structure of the label space.
We formulate the problem in a different learning paradigm, where the assignment
of a text to a TTP label is decided by the direct semantic similarity between
the two, thus reducing the complexity of competing solely over the large
labeling space. To that end, we propose a neural matching architecture with an
effective sampling-based learn-to-compare mechanism, facilitating the learning
process of the matching model despite constrained resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10491">Knowledge Fusion of Large Language Models. (arXiv:2401.10491v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1">Fanqi Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xinting Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1">Deng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1">Xiaojun Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1">Wei Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shuming Shi</a></p>
<p>While training large language models (LLMs) from scratch can generate models
with distinct functionalities and strengths, it comes at significant costs and
may result in redundant capabilities. Alternatively, a cost-effective and
compelling approach is to merge existing pre-trained LLMs into a more potent
model. However, due to the varying architectures of these LLMs, directly
blending their weights is impractical. In this paper, we introduce the notion
of knowledge fusion for LLMs, aimed at combining the capabilities of existing
LLMs and transferring them into a single LLM. By leveraging the generative
distributions of source LLMs, we externalize their collective knowledge and
unique strengths, thereby potentially elevating the capabilities of the target
model beyond those of any individual source LLM. We validate our approach using
three popular LLMs with different architectures--Llama-2, MPT, and
OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the
fusion of LLMs can improve the performance of the target model across a range
of capabilities such as reasoning, commonsense, and code generation. Our code,
model weights, and data are public at
\url{https://github.com/fanqiwan/FuseLLM}.
</p>
</p>
</div>

    </div>
    </body>
    