<!DOCTYPE html>
<html>
<head>
<title>2023-07-11-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.03758">Federated Learning over a Wireless Network: Distributed User Selection through Random Access. (arXiv:2307.03758v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shiyao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Ce Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Songtao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1">Tao Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1">Lingjuan Lyu</a></p>
<p>User selection has become crucial for decreasing the communication costs of
federated learning (FL) over wireless networks. However, centralized user
selection causes additional system complexity. This study proposes a network
intrinsic approach of distributed user selection that leverages the radio
resource competition mechanism in random access. Taking the carrier sensing
multiple access (CSMA) mechanism as an example of random access, we manipulate
the contention window (CW) size to prioritize certain users for obtaining radio
resources in each round of training. Training data bias is used as a target
scenario for FL with user selection. Prioritization is based on the distance
between the newly trained local model and the global model of the previous
round. To avoid excessive contribution by certain users, a counting mechanism
is used to ensure fairness. Simulations with various datasets demonstrate that
this method can rapidly achieve convergence similar to that of the centralized
user selection approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03759">A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1">Huan Yee Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambon_D/0/1/0/all/0/1">Daniele Zambon</a>, <a href="http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1">Cesare Alippi</a>, <a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1">Geoffrey I. Webb</a>, <a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1">Irwin King</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a></p>
<p>Time series are the primary data type used to record dynamic system
measurements and generated in great volume by both physical sensors and online
processes (virtual sensors). Time series analytics is therefore crucial to
unlocking the wealth of information implicit in available data. With the recent
advancements in graph neural networks (GNNs), there has been a surge in
GNN-based approaches for time series analysis. Approaches can explicitly model
inter-temporal and inter-variable relationships, which traditional and other
deep neural network-based methods struggle to do. In this survey, we provide a
comprehensive review of graph neural networks for time series analysis
(GNN4TS), encompassing four fundamental dimensions: Forecasting,
classification, anomaly detection, and imputation. Our aim is to guide
designers and practitioners to understand, build applications, and advance
research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy
of GNN4TS. Then, we present and discuss representative research works and,
finally, discuss mainstream applications of GNN4TS. A comprehensive discussion
of potential future research directions completes the survey. This survey, for
the first time, brings together a vast array of knowledge on GNN-based time
series research, highlighting both the foundations, practical applications, and
opportunities of graph neural networks for time series analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03761">Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1">Mengjie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a></p>
<p>In the era of digital transformation, systems monitored by the Industrial
Internet of Things (IIoTs) generate large amounts of Multivariate Time Series
(MTS) data through heterogeneous sensor networks. While this data facilitates
condition monitoring and anomaly detection, the increasing complexity and
interdependencies within the sensor network pose significant challenges for
anomaly detection. Despite progress in this field, much of the focus has been
on point anomalies and contextual anomalies, with lesser attention paid to
collective anomalies. A less addressed but common variant of collective
anomalies is when the abnormal collective behavior is caused by shifts in
interrelationships within the system. This can be due to abnormal environmental
conditions like overheating, improper operational settings resulting from
cyber-physical attacks, or system-level faults. To address these challenges,
this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a
graph-based anomaly detection framework that leverages the attention mechanism
to construct a continuous graph representation of multivariate time series by
inferring dynamic edges between time series. DyGATAD incorporates an operating
condition-aware reconstruction combined with a topology-based anomaly score,
thereby enhancing the detection ability of relationship shifts. We evaluate the
performance of DyGATAD using both a synthetic dataset with controlled varying
fault severity levels and an industrial-scale multiphase flow facility
benchmark featuring various fault types with different detection difficulties.
Our proposed approach demonstrated superior performance in collective anomaly
detection for sensor networks, showing particular strength in early-stage fault
detection, even in the case of faults with minimal severity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03762">Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models. (arXiv:2307.03762v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yuxi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a></p>
<p>In this perspective paper, we first comprehensively review existing
evaluations of Large Language Models (LLMs) using both standardized tests and
ability-oriented benchmarks. We pinpoint several problems with current
evaluation methods that tend to overstate the capabilities of LLMs. We then
articulate what artificial general intelligence should encompass beyond the
capabilities of LLMs. We propose four characteristics of generally intelligent
agents: 1) they can perform unlimited tasks; 2) they can generate new tasks
within a context; 3) they operate based on a value system that underpins task
generation; and 4) they have a world model reflecting reality, which shapes
their interaction with the world. Building on this viewpoint, we highlight the
missing pieces in artificial general intelligence, that is, the unity of
knowing and acting. We argue that active engagement with objects in the real
world delivers more robust signals for forming conceptual representations.
Additionally, knowledge acquisition isn't solely reliant on passive input but
requires repeated trials and errors. We conclude by outlining promising future
research directions in the field of artificial general intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03764">For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran&#x27;s Gender Struggles. (arXiv:2307.03764v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khorramrouz_A/0/1/0/all/0/1">Adel Khorramrouz</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1">Sujan Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1">Ashiqur R. KhudaBukhsh</a></p>
<p>In this paper, we present a computational analysis of the Persian language
Twitter discourse with the aim to estimate the shift in stance toward gender
equality following the death of Mahsa Amini in police custody. We present an
ensemble active learning pipeline to train a stance classifier. Our novelty
lies in the involvement of Iranian women in an active role as annotators in
building this AI system. Our annotators not only provide labels, but they also
suggest valuable keywords for more meaningful corpus creation as well as
provide short example documents for a guided sampling step. Our analyses
indicate that Mahsa Amini's death triggered polarized Persian language
discourse where both fractions of negative and positive tweets toward gender
equality increased. The increase in positive tweets was slightly greater than
the increase in negative tweets. We also observe that with respect to account
creation time, between the state-aligned Twitter accounts and pro-protest
Twitter accounts, pro-protest accounts are more similar to baseline Persian
Twitter activity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03798">CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1">Matthias Freiberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Kun_P/0/1/0/all/0/1">Peter Kun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lovlie_A/0/1/0/all/0/1">Anders Sundnes L&#xf8;vlie</a>, <a href="http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1">Sebastian Risi</a></p>
<p>Models leveraging both visual and textual data such as Contrastive
Language-Image Pre-training (CLIP), are increasingly gaining importance. In
this work, we show that despite their versatility, such models are vulnerable
to what we refer to as fooling master images. Fooling master images are capable
of maximizing the confidence score of a CLIP model for a significant number of
widely varying prompts, while being unrecognizable for humans. We demonstrate
how fooling master images can be mined by searching the latent space of
generative models by means of an evolution strategy or stochastic gradient
descent. We investigate the properties of the mined fooling master images, and
find that images trained on a small number of image captions potentially
generalize to a much larger number of semantically related captions. Further,
we evaluate two possible mitigation strategies and find that vulnerability to
fooling master examples is closely related to a modality gap in contrastive
pre-trained multi-modal networks. From the perspective of vulnerability to
off-manifold attacks, we therefore argue for the mitigation of modality gaps in
CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints
are available at https://github.com/matfrei/CLIPMasterPrints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03810">URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirchhof_M/0/1/0/all/0/1">Michael Kirchhof</a>, <a href="http://arxiv.org/find/cs/1/au:+Mucsanyi_B/0/1/0/all/0/1">B&#xe1;lint Mucs&#xe1;nyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seong Joon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1">Enkelejda Kasneci</a></p>
<p>Representation learning has significantly driven the field to develop
pretrained models that can act as a valuable starting point when transferring
to new datasets. With the rising demand for reliable machine learning and
uncertainty quantification, there is a need for pretrained models that not only
provide embeddings but also transferable uncertainty estimates. To guide the
development of such models, we propose the Uncertainty-aware Representation
Learning (URL) benchmark. Besides the transferability of the representations,
it also measures the zero-shot transferability of the uncertainty estimate
using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers
that are pretrained on ImageNet and transferred to eight downstream datasets.
We find that approaches that focus on the uncertainty of the representation
itself or estimate the prediction risk directly outperform those that are based
on the probabilities of upstream classes. Yet, achieving transferable
uncertainty quantification remains an open challenge. Our findings indicate
that it is not necessarily in conflict with traditional representation learning
goals. Code is provided under https://github.com/mkirchhof/url .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03817">Exploring and Characterizing Large Language Models For Embedded System Development and Debugging. (arXiv:2307.03817v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Englhardt_Z/0/1/0/all/0/1">Zachary Englhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Richard Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nissanka_D/0/1/0/all/0/1">Dilini Nissanka</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayanswamy_G/0/1/0/all/0/1">Girish Narayanswamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Breda_J/0/1/0/all/0/1">Joseph Breda</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Shwetak Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1">Vikram Iyer</a></p>
<p>Large language models (LLMs) have shown remarkable abilities to generate
code, however their ability to develop software for embedded systems, which
requires cross-domain knowledge of hardware and software has not been studied.
In this paper we systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2)
to assess their performance for embedded system development, study how human
programmers interact with these tools, and develop an AI-based software
engineering workflow for building embedded systems.
</p>
<p>We develop an an end-to-end hardware-in-the-loop evaluation platform for
verifying LLM generated programs using sensor actuator pairs. We compare all
three models with N=450 experiments and find surprisingly that GPT-4 especially
shows an exceptional level of cross-domain understanding and reasoning, in some
cases generating fully correct programs from a single prompt. In N=50 trials,
GPT-4 produces functional I2C interfaces 66% of the time. GPT-4 also produces
register-level drivers, code for LoRa communication, and context-specific power
optimizations for an nRF52 program resulting in over 740x current reduction to
12.2 uA. We also characterize the models' limitations to develop a
generalizable workflow for using LLMs in embedded system development. We
evaluate the workflow with 15 users including novice and expert programmers. We
find that our workflow improves productivity for all users and increases the
success rate for building a LoRa environmental sensor from 25% to 100%,
including for users with zero hardware or C/C++ experience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03826">How does AI chat change search behaviors?. (arXiv:2307.03826v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Capra_R/0/1/0/all/0/1">Robert Capra</a>, <a href="http://arxiv.org/find/cs/1/au:+Arguello_J/0/1/0/all/0/1">Jaime Arguello</a></p>
<p>Generative AI tools such as chatGPT are poised to change the way people
engage with online information. Recently, Microsoft announced their "new Bing"
search system which incorporates chat and generative AI technology from OpenAI.
Google has announced plans to deploy search interfaces that incorporate similar
types of technology. These new technologies will transform how people can
search for information. The research presented here is an early investigation
into how people make use of a generative AI chat system (referred to simply as
chat from here on) as part of a search process, and how the incorporation of
chat systems with existing search tools may effect users search behaviors and
strategies.
</p>
<p>We report on an exploratory user study with 10 participants who used a
combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing
Web Search v5 API. Participants completed three search tasks. In this pre-print
paper of preliminary results, we report on ways that users integrated AI chat
into their search process, things they liked and disliked about the chat
system, their trust in the chat responses, and their mental models of how the
chat system generated responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03827">Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI. (arXiv:2307.03827v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ghazvanchahi_A/0/1/0/all/0/1">Abdollah Ghazvanchahi</a>, <a href="http://arxiv.org/find/eess/1/au:+Maralani_P/0/1/0/all/0/1">Pejman Jahbedar Maralani</a>, <a href="http://arxiv.org/find/eess/1/au:+Moody_A/0/1/0/all/0/1">Alan R. Moody</a>, <a href="http://arxiv.org/find/eess/1/au:+Khademi_A/0/1/0/all/0/1">April Khademi</a></p>
<p>Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI
suffer a reduction in performance when applied on data from a scanner or centre
that is out-of-distribution (OOD) from the training data. This is critical for
translation and widescale adoption, since current models cannot be readily
applied to data from new institutions. In this work, we evaluate several
intensity standardization methods for MRI as a preprocessing step for WML
segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI.
We evaluate a method specifically developed for FLAIR MRI called IAMLAB along
with other popular normalization techniques such as White-strip, Nyul and
Z-score. We proposed an Ensemble model that combines predictions from each of
these models. A skip-connection UNet (SC UNet) was trained on the standardized
images, as well as the original data and segmentation performance was evaluated
over several dimensions. The training (in-distribution) data consists of a
single study, of 60 volumes, and the test (OOD) data is 128 unseen volumes from
three clinical cohorts. Results show IAMLAB and Ensemble provide higher WML
segmentation performance compared to models from original data or other
normalization methods. IAMLAB &amp; Ensemble have the highest dice similarity
coefficient (DSC) on the in-distribution data (0.78 &amp; 0.80) and on clinical OOD
data. DSC was significantly higher for IAMLAB compared to the original data
(p&lt;0.05) for all lesion categories (LL&gt;25mL: 0.77 vs. 0.71; 10mL&lt;= LL&lt;25mL:
0.66 vs. 0.61; LL&lt;10mL: 0.53 vs. 0.52). The IAMLAB and Ensemble normalization
methods are mitigating MRI domain shift and are optimal for DL-based WML
segmentation in unseen FLAIR data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03833">Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation. (arXiv:2307.03833v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhongyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhuoran Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1">Wenhao Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng-Yen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jenq-Neng Hwang</a></p>
<p>Learning-based methods have dominated the 3D human pose estimation (HPE)
tasks with significantly better performance in most benchmarks than traditional
optimization-based methods. Nonetheless, 3D HPE in the wild is still the
biggest challenge of learning-based models, whether with 2D-3D lifting,
image-to-3D, or diffusion-based methods, since the trained networks implicitly
learn camera intrinsic parameters and domain-based 3D human pose distributions
and estimate poses by statistical average. On the other hand, the
optimization-based methods estimate results case-by-case, which can predict
more diverse and sophisticated human poses in the wild. By combining the
advantages of optimization-based and learning-based methods, we propose the
Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the
problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO
achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm
without training with any 2D-3D or image-3D pairs. Moreover, our
single-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE
$42.6$mm on cross-dataset evaluation, which even outperforms learning-based
methods trained on 3DPW.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03838">RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaomeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1">Tsung-Yi Ho</a></p>
<p>Recent advances in large language models (LLMs) and the intensifying
popularity of ChatGPT-like applications have blurred the boundary of
high-quality text generation between humans and machines. However, in addition
to the anticipated revolutionary changes to our technology and society, the
difficulty of distinguishing LLM-generated texts (AI-text) from human-generated
texts poses new challenges of misuse and fairness, such as fake content
generation, plagiarism, and false accusation of innocent writers. While
existing works show that current AI-text detectors are not robust to LLM-based
paraphrasing, this paper aims to bridge this gap by proposing a new framework
called RADAR, which jointly trains a Robust AI-text Detector via Adversarial
leaRning. RADAR is based on adversarial training of a paraphraser and a
detector. The paraphraser's goal is to generate realistic contents to evade
AI-text detection. RADAR uses the feedback from the detector to update the
paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly
2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,
experimental results show that RADAR significantly outperforms existing AI-text
detection methods, especially when paraphrasing is in place. We also identify
the strong transferability of RADAR from instruction-tuned LLMs to other LLMs,
and evaluate the improved capability of RADAR via GPT-3.5.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03848">Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1">Idan Attias</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1">Steve Hanneke</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1">Alkis Kalavasis</a>, <a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1">Amin Karbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Velegkas_G/0/1/0/all/0/1">Grigoris Velegkas</a></p>
<p>In this work, we aim to characterize the statistical complexity of realizable
regression both in the PAC learning setting and the online learning setting.
</p>
<p>Previous work had established the sufficiency of finiteness of the fat
shattering dimension for PAC learnability and the necessity of finiteness of
the scaled Natarajan dimension, but little progress had been made towards a
more complete characterization since the work of Simon 1997 (SICOMP '97). To
this end, we first introduce a minimax instance optimal learner for realizable
regression and propose a novel dimension that both qualitatively and
quantitatively characterizes which classes of real-valued predictors are
learnable. We then identify a combinatorial dimension related to the Graph
dimension that characterizes ERM learnability in the realizable setting.
Finally, we establish a necessary condition for learnability based on a
combinatorial dimension related to the DS dimension, and conjecture that it may
also be sufficient in this context.
</p>
<p>Additionally, in the context of online learning we provide a dimension that
characterizes the minimax instance optimal cumulative loss up to a constant
factor and design an optimal online learner for realizable regression, thus
resolving an open question raised by Daskalakis and Golowich in STOC '22.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03853">Teach Me How to Learn: A Perspective Review towards User-centered Neuro-symbolic Learning for Robotic Surgical Systems. (arXiv:2307.03853v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gomaa_A/0/1/0/all/0/1">Amr Gomaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdy_B/0/1/0/all/0/1">Bilal Mahdy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleer_N/0/1/0/all/0/1">Niko Kleer</a>, <a href="http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1">Michael Feld</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirchner_F/0/1/0/all/0/1">Frank Kirchner</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruger_A/0/1/0/all/0/1">Antonio Kr&#xfc;ger</a></p>
<p>Recent advances in machine learning models allowed robots to identify objects
on a perceptual nonsymbolic level (e.g., through sensor fusion and natural
language understanding). However, these primarily black-box learning models
still lack interpretation and transferability and require high data and
computational demand. An alternative solution is to teach a robot on both
perceptual nonsymbolic and conceptual symbolic levels through hybrid
neurosymbolic learning approaches with expert feedback (i.e., human-in-the-loop
learning). This work proposes a concept for this user-centered hybrid learning
paradigm that focuses on robotic surgical situations. While most recent
research focused on hybrid learning for non-robotic and some generic robotic
domains, little work focuses on surgical robotics. We survey this related
research while focusing on human-in-the-loop surgical robotic systems. This
evaluation highlights the most prominent solutions for autonomous surgical
robots and the challenges surgeons face when interacting with these systems.
Finally, we envision possible ways to address these challenges using online
apprenticeship learning based on implicit and explicit feedback from expert
surgeons.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03860">Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization. (arXiv:2307.03860v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ogunfowora_O/0/1/0/all/0/1">Oluwaseyi Ogunfowora</a>, <a href="http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1">Homayoun Najjaran</a></p>
<p>Systems and machines undergo various failure modes that result in machine
health degradation, so maintenance actions are required to restore them back to
a state where they can perform their expected functions. Since maintenance
tasks are inevitable, maintenance planning is essential to ensure the smooth
operations of the production system and other industries at large. Maintenance
planning is a decision-making problem that aims at developing optimum
maintenance policies and plans that help reduces maintenance costs, extend
asset life, maximize their availability, and ultimately ensure workplace
safety. Reinforcement learning is a data-driven decision-making algorithm that
has been increasingly applied to develop dynamic maintenance plans while
leveraging the continuous information from condition monitoring of the system
and machine states. By leveraging the condition monitoring data of systems and
machines with reinforcement learning, smart maintenance planners can be
developed, which is a precursor to achieving a smart factory. This paper
presents a literature review on the applications of reinforcement and deep
reinforcement learning for maintenance planning and optimization problems. To
capture the common ideas without losing touch with the uniqueness of each
publication, taxonomies used to categorize the systems were developed, and
reviewed publications were highlighted, classified, and summarized based on
these taxonomies. Adopted methodologies, findings, and well-defined
interpretations of the reviewed studies were summarized in graphical and
tabular representations to maximize the utility of the work for both
researchers and practitioners. This work also highlights the research gaps, key
insights from the literature, and areas for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03867">Personalized Resource Allocation in Wireless Networks: An AI-Enabled and Big Data-Driven Multi-Objective Optimization. (arXiv:2307.03867v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alkurd_R/0/1/0/all/0/1">Rawan Alkurd</a>, <a href="http://arxiv.org/find/cs/1/au:+Abualhaol_I/0/1/0/all/0/1">Ibrahim Abualhaol</a>, <a href="http://arxiv.org/find/cs/1/au:+Yanikomeroglu_H/0/1/0/all/0/1">Halim Yanikomeroglu</a></p>
<p>The design and optimization of wireless networks have mostly been based on
strong mathematical and theoretical modeling. Nonetheless, as novel
applications emerge in the era of 5G and beyond, unprecedented levels of
complexity will be encountered in the design and optimization of the network.
As a result, the use of Artificial Intelligence (AI) is envisioned for wireless
network design and optimization due to the flexibility and adaptability it
offers in solving extremely complex problems in real-time. One of the main
future applications of AI is enabling user-level personalization for numerous
use cases. AI will revolutionize the way we interact with computers in which
computers will be able to sense commands and emotions from humans in a
non-intrusive manner, making the entire process transparent to users. By
leveraging this capability, and accelerated by the advances in computing
technologies, wireless networks can be redesigned to enable the personalization
of network services to the user level in real-time. While current wireless
networks are being optimized to achieve a predefined set of quality
requirements, the personalization technology advocated in this article is
supported by an intelligent big data-driven layer designed to micro-manage the
scarce network resources. This layer provides the intelligence required to
decide the necessary service quality that achieves the target satisfaction
level for each user. Due to its dynamic and flexible design, personalized
networks are expected to achieve unprecedented improvements in optimizing two
contradicting objectives in wireless networks: saving resources and improving
user satisfaction levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03872">Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment. (arXiv:2307.03872v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dy_A/0/1/0/all/0/1">Amanda Dy</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1">Ngoc-Nhu Jennifer Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Mirjahanmardi_S/0/1/0/all/0/1">Seyed Hossein Mirjahanmardi</a>, <a href="http://arxiv.org/find/eess/1/au:+Dawe_M/0/1/0/all/0/1">Melanie Dawe</a>, <a href="http://arxiv.org/find/eess/1/au:+Fyles_A/0/1/0/all/0/1">Anthony Fyles</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1">Wei Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1">Fei-Fei Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Androutsos_D/0/1/0/all/0/1">Dimitrios Androutsos</a>, <a href="http://arxiv.org/find/eess/1/au:+Done_S/0/1/0/all/0/1">Susan Done</a>, <a href="http://arxiv.org/find/eess/1/au:+Khademi_A/0/1/0/all/0/1">April Khademi</a></p>
<p>Deep learning systems have been proposed to improve the objectivity and
efficiency of Ki- 67 PI scoring. The challenge is that while very accurate,
deep learning techniques suffer from reduced performance when applied to
out-of-domain data. This is a critical challenge for clinical translation, as
models are typically trained using data available to the vendor, which is not
from the target domain. To address this challenge, this study proposes a domain
adaptation pipeline that employs an unsupervised framework to generate silver
standard (pseudo) labels in the target domain, which is used to augment the
gold standard (GS) source domain data. Five training regimes were tested on two
validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained
on target silver standard (SS) labels, (2) GS Only: trained on source GS
labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS:
trained on source GS labels and fine-tuned on target SS labels, and our
proposed method (5) SS+GS: trained on source SS labels and fine-tuned on source
GS labels. The SS+GS method yielded significantly (p &lt; 0.05) higher PI accuracy
(95.9%) and more consistent results compared to the GS Only model on target
data. Analysis of t-SNE plots showed features learned by the SS+GS models are
more aligned for source and target data, resulting in improved generalization.
The proposed pipeline provides an efficient method for learning the target
distribution without manual annotations, which are time-consuming and costly to
generate for medical images. This framework can be applied to any target site
as a per-laboratory calibration method, for widescale deployment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03875">Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Beibin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mellou_K/0/1/0/all/0/1">Konstantina Mellou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathuri_J/0/1/0/all/0/1">Jeevan Pathuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Menache_I/0/1/0/all/0/1">Ishai Menache</a></p>
<p>Supply chain operations traditionally involve a variety of complex decision
making problems. Over the last few decades, supply chains greatly benefited
from advances in computation, which allowed the transition from manual
processing to automation and cost-effective optimization. Nonetheless, business
operators still need to spend substantial efforts in \emph{explaining} and
interpreting the optimization outcomes to stakeholders. Motivated by the recent
advances in Large Language Models (LLMs), we study how this disruptive
technology can help bridge the gap between supply chain automation and human
comprehension and trust thereof. We design \name{} -- a framework that accepts
as input queries in plain text, and outputs insights about the underlying
optimization outcomes. Our framework does not forgo the state-of-the-art
combinatorial optimization technology, but rather leverages it to
quantitatively answer what-if scenarios (e.g., how would the cost change if we
used supplier B instead of supplier A for a given demand?). Importantly, our
design does not require sending proprietary data over to LLMs, which can be a
privacy concern in some circumstances. We demonstrate the effectiveness of our
framework on a real server placement scenario within Microsoft's cloud supply
chain. Along the way, we develop a general evaluation benchmark, which can be
used to evaluate the accuracy of the LLM output in other scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03877">Designing Mixed-Initiative Video Games. (arXiv:2307.03877v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Daijin Yang</a></p>
<p>The development of Artificial Intelligence (AI) enables humans to co-create
content with machines. The unexpectedness of AI-generated content can bring
inspiration and entertainment to users. However, the co-creation interactions
are always designed for content creators and have poor accessibility. To
explore gamification of mixed-initiative co-creation and make human-AI
interactions accessible and fun for players, I prototyped Snake Story, a
mixed-initiative game where players can select AI-generated texts to write a
story of a snake by playing a "Snake" like game. A controlled experiment was
conducted to investigate the dynamics of player-AI interactions with and
without the game component in the designed interface. As a result of a study
with 11 players (n=11), I found that players utilized different strategies when
playing with the two versions, game mechanics significantly affected the output
stories, players' creative process, as well as role perceptions, and players
with different backgrounds showed different preferences for the two versions.
Based on these results, I further discussed considerations for mixed-initiative
game design. This work aims to inspire the design of engaging co-creation
experiences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03887">Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Netzorg_R/0/1/0/all/0/1">Robin Netzorg</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaxun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bin Yu</a></p>
<p>In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model's output to specific
features of the data. One such of these methods is the prototypical part
network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
this method often learns to classify from spurious or inconsistent parts of the
image. Hoping to remedy this, we take inspiration from the recent developments
in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns to
identify non-spurious prototypes. In place of a full RL update, we propose the
reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet),
which adds an additional three steps to the ProtoPNet training loop. The first
two steps are reward-based reweighting and reselection, which align prototypes
with human feedback. The final step is retraining to realign the model's
features with the updated prototypes. We find that R3-ProtoPNet improves the
overall consistency and meaningfulness of the prototypes, but lower the test
predictive accuracy when used independently. When multiple R3-ProtoPNets are
incorporated into an ensemble, we find an increase in test predictive
performance while maintaining interpretability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03906">ScriptWorld: Text Based Environment For Learning Procedural Knowledge. (arXiv:2307.03906v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Abhinav Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1">Areeb Ahmad</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_U/0/1/0/all/0/1">Umang Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1">Ashutosh Modi</a></p>
<p>Text-based games provide a framework for developing natural language
understanding and commonsense knowledge about the world in reinforcement
learning based agents. Existing text-based environments often rely on fictional
situations and characters to create a gaming framework and are far from
real-world scenarios. In this paper, we introduce ScriptWorld: a text-based
environment for teaching agents about real-world daily chores and hence
imparting commonsense knowledge. To the best of our knowledge, it is the first
interactive text-based gaming framework that consists of daily real-world human
activities designed using scripts dataset. We provide gaming environments for
10 daily activities and perform a detailed analysis of the proposed
environment. We develop RL-based baseline models/agents to play the games in
Scriptworld. To understand the role of language models in such environments, we
leverage features obtained from pre-trained language models in the RL agents.
Our experiments show that prior knowledge obtained from a pre-trained language
model helps to solve real-world text-based gaming environments. We release the
environment via Github: https://github.com/Exploration-Lab/ScriptWorld
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03913">Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zaifeng Gao</a></p>
<p>Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
</p>
<p>Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03928">Bounding data reconstruction attacks with the hypothesis testing interpretation of differential privacy. (arXiv:2307.03928v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1">Georgios Kaissis</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayes_J/0/1/0/all/0/1">Jamie Hayes</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1">Alexander Ziller</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a></p>
<p>We explore Reconstruction Robustness (ReRo), which was recently proposed as
an upper bound on the success of data reconstruction attacks against machine
learning models. Previous research has demonstrated that differential privacy
(DP) mechanisms also provide ReRo, but so far, only asymptotic Monte Carlo
estimates of a tight ReRo bound have been shown. Directly computable ReRo
bounds for general DP mechanisms are thus desirable. In this work, we establish
a connection between hypothesis testing DP and ReRo and derive closed-form,
analytic or numerical ReRo bounds for the Laplace and Gaussian mechanisms and
their subsampled variants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03936">Towards Efficient In-memory Computing Hardware for Quantized Neural Networks: State-of-the-art, Open Challenges and Perspectives. (arXiv:2307.03936v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krestinskaya_O/0/1/0/all/0/1">Olga Krestinskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Salama_K/0/1/0/all/0/1">Khaled Nabil Salama</a></p>
<p>The amount of data processed in the cloud, the development of
Internet-of-Things (IoT) applications, and growing data privacy concerns force
the transition from cloud-based to edge-based processing. Limited energy and
computational resources on edge push the transition from traditional von
Neumann architectures to In-memory Computing (IMC), especially for machine
learning and neural network applications. Network compression techniques are
applied to implement a neural network on limited hardware resources.
Quantization is one of the most efficient network compression techniques
allowing to reduce the memory footprint, latency, and energy consumption. This
paper provides a comprehensive review of IMC-based Quantized Neural Networks
(QNN) and links software-based quantization approaches to IMC hardware
implementation. Moreover, open challenges, QNN design requirements,
recommendations, and perspectives along with an IMC-based QNN hardware roadmap
are provided.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03937">Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks. (arXiv:2307.03937v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shixuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Changjun Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1">Kewei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunfei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1">Peng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yizhou Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhong Liu</a></p>
<p>Heterogeneous Information Networks (HINs) are information networks with
multiple types of nodes and edges. The concept of meta-path, i.e., a sequence
of entity types and relation types connecting two entities, is proposed to
provide the meta-level explainable semantics for various HIN tasks.
Traditionally, meta-paths are primarily used for schema-simple HINs, e.g.,
bibliographic networks with only a few entity types, where meta-paths are often
enumerated with domain knowledge. However, the adoption of meta-paths for
schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and
relation types, has been limited due to the computational complexity associated
with meta-path enumeration. Additionally, effectively assessing meta-paths
requires enumerating relevant path instances, which adds further complexity to
the meta-path learning process. To address these challenges, we propose
SchemaWalk, an inductive meta-path learning framework for schema-complex HINs.
We represent meta-paths with schema-level representations to support the
learning of the scores of meta-paths for varying relations, mitigating the need
of exhaustive path instance enumeration for each relation. Further, we design a
reinforcement-learning based path-finding agent, which directly navigates the
network schema (i.e., schema graph) to learn policies for establishing
meta-paths with high coverage and confidence for multiple relations. Extensive
experiments on real data sets demonstrate the effectiveness of our proposed
paradigm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03941">Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Finckenberg_Broman_P/0/1/0/all/0/1">Pamela Finckenberg-Broman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thong Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shidong Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1">Mark Staples</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a></p>
<p>The Right to be Forgotten (RTBF) was first established as the result of the
ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and
was later included as the Right to Erasure under the General Data Protection
Regulation (GDPR) of European Union to allow individuals the right to request
personal data be deleted by organizations. Specifically for search engines,
individuals can send requests to organizations to exclude their information
from the query results. With the recent development of Large Language Models
(LLMs) and their use in chatbots, LLM-enabled software systems have become
popular. But they are not excluded from the RTBF. Compared with the indexing
approach used by search engines, LLMs store, and process information in a
completely different way. This poses new challenges for compliance with the
RTBF. In this paper, we explore these challenges and provide our insights on
how to implement technical solutions for the RTBF, including the use of machine
unlearning, model editing, and prompting engineering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03966">Multi-Intent Detection in User Provided Annotations for Programming by Examples Systems. (arXiv:2307.03966v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1">Nischal Ashok Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1">Nitin Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Guttula_S/0/1/0/all/0/1">Shanmukha Guttula</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1">Hima Patel</a></p>
<p>In mapping enterprise applications, data mapping remains a fundamental part
of integration development, but its time consuming. An increasing number of
applications lack naming standards, and nested field structures further add
complexity for the integration developers. Once the mapping is done, data
transformation is the next challenge for the users since each application
expects data to be in a certain format. Also, while building integration flow,
developers need to understand the format of the source and target data field
and come up with transformation program that can change data from source to
target format. The problem of automatic generation of a transformation program
through program synthesis paradigm from some specifications has been studied
since the early days of Artificial Intelligence (AI). Programming by Example
(PBE) is one such kind of technique that targets automatic inferencing of a
computer program to accomplish a format or string conversion task from
user-provided input and output samples. To learn the correct intent, a diverse
set of samples from the user is required. However, there is a possibility that
the user fails to provide a diverse set of samples. This can lead to multiple
intents or ambiguity in the input and output samples. Hence, PBE systems can
get confused in generating the correct intent program. In this paper, we
propose a deep neural network based ambiguity prediction model, which analyzes
the input-output strings and maps them to a different set of properties
responsible for multiple intent. Users can analyze these properties and
accordingly can provide new samples or modify existing samples which can help
in building a better PBE system for mapping enterprise applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03973">Autonomy 2.0: The Quest for Economies of Scale. (arXiv:2307.03973v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shuang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shaoshan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuhao Zhu</a></p>
<p>With the advancement of robotics and AI technologies in the past decade, we
have now entered the age of autonomous machines. In this new age of information
technology, autonomous machines, such as service robots, autonomous drones,
delivery robots, and autonomous vehicles, rather than humans, will provide
services. In this article, through examining the technical challenges and
economic impact of the digital economy, we argue that scalability is both
highly necessary from a technical perspective and significantly advantageous
from an economic perspective, thus is the key for the autonomy industry to
achieve its full potential. Nonetheless, the current development paradigm,
dubbed Autonomy 1.0, scales with the number of engineers, instead of with the
amount of data or compute resources, hence preventing the autonomy industry to
fully benefit from the economies of scale, especially the exponentially
cheapening compute cost and the explosion of available data. We further analyze
the key scalability blockers and explain how a new development paradigm, dubbed
Autonomy 2.0, can address these problems to greatly boost the autonomy
industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03988">PCG-based Static Underground Garage Scenario Generation. (arXiv:2307.03988v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenjin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kai Li</a></p>
<p>Autonomous driving technology has five levels, from L0 to L5. Currently, only
the L2 level (partial automation) can be achieved, and there is a long way to
go before reaching the final level of L5 (full automation). The key to crossing
these levels lies in training the autonomous driving model. However, relying
solely on real-world road data to train the model is far from enough and
consumes a great deal of resources. Although there are already examples of
training autonomous driving models through simulators that simulate real-world
scenarios, these scenarios require complete manual construction. Directly
converting 3D scenes from road network formats will lack a large amount of
detail and cannot be used as training sets. Underground parking garage static
scenario simulation is regarded as a procedural content generation (PCG)
problem. This paper will use the Sarsa algorithm to solve procedural content
generation on underground garage structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04005">Proceedings Ninetheenth conference on Theoretical Aspects of Rationality and Knowledge. (arXiv:2307.04005v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verbrugge_R/0/1/0/all/0/1">Rineke Verbrugge</a> (University of Groningen)</p>
<p>The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a
conference that aims to bring together researchers from a wide variety of
fields, including computer science, artificial intelligence, game theory,
decision theory, philosophy, logic, linguistics, and cognitive science. Its
goal is to further our understanding of interdisciplinary issues involving
reasoning about rationality and knowledge.
</p>
<p>Previous conferences have been held biennially around the world since 1986,
on the initiative of Joe Halpern (Cornell University). Topics of interest
include, but are not limited to, semantic models for knowledge, belief,
awareness and uncertainty, bounded rationality and resource-bounded reasoning,
commonsense epistemic reasoning, epistemic logic, epistemic game theory,
knowledge and action, applications of reasoning about knowledge and other
mental states, belief revision, computational social choice, algorithmic game
theory, and foundations of multi-agent systems. Information about TARK,
including conference proceedings, is available at the website
<a href="http://www.tark.org/">this http URL</a>
</p>
<p>These proceedings contain the papers that have been accepted for presentation
at the Nineteenth Conference on Theoretical Aspects of Rationality and
Knowledge (TARK 2023), held between June 28 and June 30, 2023, at the
University of Oxford, United Kingdom. The conference website can be found at
https://sites.google.com/view/tark-2023
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04019">GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments. (arXiv:2307.04019v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohamed_I/0/1/0/all/0/1">Ihab S. Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1">Mahmoud Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lantao Liu</a></p>
<p>Robotic navigation in unknown, cluttered environments with limited sensing
capabilities poses significant challenges in robotics. Local trajectory
optimization methods, such as Model Predictive Path Intergal (MPPI), are a
promising solution to this challenge. However, global guidance is required to
ensure effective navigation, especially when encountering challenging
environmental conditions or navigating beyond the planning horizon. This study
presents the GP-MPPI, an online learning-based control strategy that integrates
MPPI with a local perception model based on Sparse Gaussian Process (SGP). The
key idea is to leverage the learning capability of SGP to construct a variance
(uncertainty) surface, which enables the robot to learn about the navigable
space surrounding it, identify a set of suggested subgoals, and ultimately
recommend the optimal subgoal that minimizes a predefined cost function to the
local MPPI planner. Afterward, MPPI computes the optimal control sequence that
satisfies the robot and collision avoidance constraints. Such an approach
eliminates the necessity of a global map of the environment or an offline
training process. We validate the efficiency and robustness of our proposed
control strategy through both simulated and real-world experiments of 2D
autonomous navigation tasks in complex unknown environments, demonstrating its
superiority in guiding the robot safely towards its desired goal while avoiding
obstacles and escaping entrapment in local minima. The GPU implementation of
GP-MPPI, including the supplementary video, is available at
https://github.com/IhabMohamed/GP-MPPI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04028">Measuring the Success of Diffusion Models at Imitating Human Artists. (arXiv:2307.04028v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1">Stephen Casper</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zifan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mogulothu_S/0/1/0/all/0/1">Shreya Mogulothu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marinov_Z/0/1/0/all/0/1">Zachary Marinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_C/0/1/0/all/0/1">Chinmay Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Yew_R/0/1/0/all/0/1">Rui-Jie Yew</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1">Zheng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1">Dylan Hadfield-Menell</a></p>
<p>Modern diffusion models have set the state-of-the-art in AI image generation.
Their success is due, in part, to training on Internet-scale data which often
includes copyrighted work. This prompts questions about the extent to which
these models learn from, imitate, or copy the work of human artists. This work
suggests that tying copyright liability to the capabilities of the model may be
useful given the evolving ecosystem of generative models. Specifically, much of
the legal analysis of copyright and generative systems focuses on the use of
protected data for training. As a result, the connections between data,
training, and the system are often obscured. In our approach, we consider
simple image classification techniques to measure a model's ability to imitate
specific artists. Specifically, we use Contrastive Language-Image Pretrained
(CLIP) encoders to classify images in a zero-shot fashion. Our process first
prompts a model to imitate a specific artist. Then, we test whether CLIP can be
used to reclassify the artist (or the artist's work) from the imitation. If
these tests match the imitation back to the original artist, this suggests the
model can imitate that artist's expression. Our approach is simple and
quantitative. Furthermore, it uses standard techniques and does not require
additional training. We demonstrate our approach with an audit of Stable
Diffusion's capacity to imitate 70 professional digital artists with
copyrighted work online. When Stable Diffusion is prompted to imitate an artist
from this set, we find that the artist can be identified from the imitation
with an average accuracy of 81.0%. Finally, we also show that a sample of the
artist's work can be matched to these imitation images with a high degree of
statistical reliability. Overall, these results suggest that Stable Diffusion
is broadly successful at imitating individual human artists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04029">On &quot;Indifference&quot; and Backward Induction in Games with Perfect Information. (arXiv:2307.04029v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Megiddo_N/0/1/0/all/0/1">Nimrod Megiddo</a></p>
<p>Indifference of a player with respect to two distinct outcomes of a game
cannot be handled by small perturbations, because the actual choice may have
significant impact on other players, and cause them to act in a way that has
significant impact of the indifferent player. It is argued that ties among
rational choices can be resolved by refinements of the concept of rationality
based on the utilities of other players. One such refinement is the concept of
Tit-for-Tat.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04033">Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ambekar_S/0/1/0/all/0/1">Sameer Ambekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zehao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiayi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1">Xiantong Zhen</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees G. M. Snoek</a></p>
<p>This paper strives for domain generalization, where models are trained
exclusively on source domains before being deployed at unseen target domains.
We follow the strict separation of source training and target testing but
exploit the value of the unlabeled target data itself during inference. We make
three contributions. First, we propose probabilistic pseudo-labeling of target
samples to generalize the source-trained model to the target domain at test
time. We formulate the generalization at test time as a variational inference
problem by modeling pseudo labels as distributions to consider the uncertainty
during generalization and alleviate the misleading signal of inaccurate pseudo
labels. Second, we learn variational neighbor labels that incorporate the
information of neighboring target samples to generate more robust pseudo
labels. Third, to learn the ability to incorporate more representative target
information and generate more precise and robust variational neighbor labels,
we introduce a meta-generalization stage during training to simulate the
generalization procedure. Experiments on six widely-used datasets demonstrate
the benefits, abilities, and effectiveness of our proposal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04036">Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations. (arXiv:2307.04036v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tong Steven Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yuyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Khaladkar_S/0/1/0/all/0/1">Shubham Khaladkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Young-Ho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Sungsoo Ray Hong</a></p>
<p>The local explanation provides heatmaps on images to explain how
Convolutional Neural Networks (CNNs) derive their output. Due to its visual
straightforwardness, the method has been one of the most popular explainable AI
(XAI) methods for diagnosing CNNs. Through our formative study (S1), however,
we captured ML engineers' ambivalent perspective about the local explanation as
a valuable and indispensable envision in building CNNs versus the process that
exhausts them due to the heuristic nature of detecting vulnerability. Moreover,
steering the CNNs based on the vulnerability learned from the diagnosis seemed
highly challenging. To mitigate the gap, we designed DeepFuse, the first
interactive design that realizes the direct feedback loop between a user and
CNNs in diagnosing and revising CNN's vulnerability using local explanations.
DeepFuse helps CNN engineers to systemically search "unreasonable" local
explanations and annotate the new boundaries for those identified as
unreasonable in a labor-efficient manner. Next, it steers the model based on
the given annotation such that the model doesn't introduce similar mistakes. We
conducted a two-day study (S2) with 12 experienced CNN engineers. Using
DeepFuse, participants made a more accurate and "reasonable" model than the
current state-of-the-art. Also, participants found the way DeepFuse guides
case-based reasoning can practically improve their current practice. We provide
implications for design that explain how future HCI-driven design can move our
practice forward to make XAI-driven insights more actionable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04050">Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks. (arXiv:2307.04050v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ojha_R/0/1/0/all/0/1">Ritesh Ojha</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenbo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khir_R/0/1/0/all/0/1">Reem Khir</a>, <a href="http://arxiv.org/find/cs/1/au:+Erera_A/0/1/0/all/0/1">Alan Erera</a>, <a href="http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1">Pascal Van Hentenryck</a></p>
<p>The load planning problem is a critical challenge in service network design
for parcel carriers: it decides how many trailers (or loads) to assign for
dispatch over time between pairs of terminals. Another key challenge is to
determine a flow plan, which specifies how parcel volumes are assigned to
planned loads. This paper considers the Dynamic Load Planning Problem (DLPP)
that considers both flow and load planning challenges jointly to adjust loads
and flows as the demand forecast changes over time before the day of
operations. The paper aims at developing a decision-support tool to inform
planners making these decisions at terminals across the network. The paper
formulates the DLPP as a MIP and shows that it admits a large number of
symmetries in a network where each commodity can be routed through primary and
alternate paths. As a result, an optimization solver may return fundamentally
different solutions to closely related problems, confusing planners and
reducing trust in optimization. To remedy this limitation, the paper proposes a
Goal-Directed Optimization that eliminates those symmetries by generating
optimal solutions staying close to a reference plan. The paper also proposes an
optimization proxy to address the computational challenges of the optimization
models. The proxy combines a machine learning model and a feasibility
restoration model and finds solutions that satisfy real-time constraints
imposed by planners-in-the-loop. An extensive computational study on industrial
instances shows that the optimization proxy is around 10 times faster than the
commercial solver in obtaining the same quality solutions and orders of
magnitude faster for generating solutions that are consistent with each other.
The proposed approach also demonstrates the benefits of the DLPP for load
consolidation, and the significant savings obtained from combining machine
learning and optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04055">Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Liu_P/0/1/0/all/0/1">Pangpang Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1">Will Wei Sun</a></p>
<p>Personalized pricing, which involves tailoring prices based on individual
characteristics, is commonly used by firms to implement a consumer-specific
pricing policy. In this process, buyers can also strategically manipulate their
feature data to obtain a lower price, incurring certain manipulation costs.
Such strategic behavior can hinder firms from maximizing their profits. In this
paper, we study the contextual dynamic pricing problem with strategic buyers.
The seller does not observe the buyer's true feature, but a manipulated feature
according to buyers' strategic behavior. In addition, the seller does not
observe the buyers' valuation of the product, but only a binary response
indicating whether a sale happens or not. Recognizing these challenges, we
propose a strategic dynamic pricing policy that incorporates the buyers'
strategic behavior into the online learning to maximize the seller's cumulative
revenue. We first prove that existing non-strategic pricing policies that
neglect the buyers' strategic behavior result in a linear $\Omega(T)$ regret
with $T$ the total time horizon, indicating that these policies are not better
than a random pricing policy. We then establish that our proposed policy
achieves a sublinear regret upper bound of $O(\sqrt{T})$. Importantly, our
policy is not a mere amalgamation of existing dynamic pricing policies and
strategic behavior handling algorithms. Our policy can also accommodate the
scenario when the marginal cost of manipulation is unknown in advance. To
account for it, we simultaneously estimate the valuation parameter and the cost
parameter in the online pricing policy, which is shown to also achieve an
$O(\sqrt{T})$ regret bound. Extensive experiments support our theoretical
developments and demonstrate the superior performance of our policy compared to
other pricing policies that are unaware of the strategic behaviors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04075">Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Liangrui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dazhen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1">Yutao Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhichao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_P/0/1/0/all/0/1">Pengfei Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liwen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Shaoliang Peng</a></p>
<p>Due to the high heterogeneity and clinical characteristics of cancer, there
are significant differences in multi-omics data and clinical features among
subtypes of different cancers. Therefore, the identification and discovery of
cancer subtypes are crucial for the diagnosis, treatment, and prognosis of
cancer. In this study, we proposed a generalization framework based on
attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze
cancer multi-omics data for the identification and characterization of cancer
subtypes. AMUCL framework includes a unsupervised multi-head attention
mechanism, which deeply extracts multi-omics data features. Importantly, a
decoupled contrastive learning model (DMACL) based on a multi-head attention
mechanism is proposed to learn multi-omics data features and clusters and
identify new cancer subtypes. This unsupervised contrastive learning method
clusters subtypes by calculating the similarity between samples in the feature
space and sample space of multi-omics data. Compared to 11 other deep learning
models, the DMACL model achieved a C-index of 0.002, a Silhouette score of
0.801, and a Davies Bouldin Score of 0.38 on a single-cell multi-omics dataset.
On a cancer multi-omics dataset, the DMACL model obtained a C-index of 0.016, a
Silhouette score of 0.688, and a Davies Bouldin Score of 0.46, and obtained the
most reliable cancer subtype clustering results for each type of cancer.
Finally, we used the DMACL model in the AMUCL framework to reveal six cancer
subtypes of AML. By analyzing the GO functional enrichment, subtype-specific
biological functions, and GSEA of AML, we further enhanced the interpretability
of cancer subtype analysis based on the generalizable AMUCL framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04090">DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1">Allen Roush</a></p>
<p>Recent work within the Argument Mining community has shown the applicability
of Natural Language Processing systems for solving problems found within
competitive debate. One of the most important tasks within competitive debate
is for debaters to create high quality debate cases. We show that effective
debate cases can be constructed using constrained shortest path traversals on
Argumentative Semantic Knowledge Graphs. We study this potential in the context
of a type of American Competitive Debate, called Policy Debate, which already
has a large scale dataset targeting it called DebateSum. We significantly
improve upon DebateSum by introducing 53180 new examples, as well as further
useful metadata for every example, to the dataset. We leverage the txtai
semantic search and knowledge graph toolchain to produce and contribute 9
semantic knowledge graphs built on this dataset. We create a unique method for
evaluating which knowledge graphs are better in the context of producing policy
debate cases. A demo which automatically generates debate cases, along with all
other code and the Knowledge Graphs, are open-sourced and made available to the
public here: https://github.com/Hellisotherpeople/DebateKG
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04098">A User Study on Explainable Online Reinforcement Learning for Adaptive Systems. (arXiv:2307.04098v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Metzger_A/0/1/0/all/0/1">Andreas Metzger</a>, <a href="http://arxiv.org/find/cs/1/au:+Laufer_J/0/1/0/all/0/1">Jan Laufer</a>, <a href="http://arxiv.org/find/cs/1/au:+Feit_F/0/1/0/all/0/1">Felix Feit</a>, <a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1">Klaus Pohl</a></p>
<p>Online reinforcement learning (RL) is increasingly used for realizing
adaptive systems in the presence of design time uncertainty. Online RL
facilitates learning from actual operational data and thereby leverages
feedback only available at runtime. However, Online RL requires the definition
of an effective and correct reward function, which quantifies the feedback to
the RL algorithm and thereby guides learning. With Deep RL gaining interest,
the learned knowledge is no longer explicitly represented, but is represented
as a neural network. For a human, it becomes practically impossible to relate
the parametrization of the neural network to concrete RL decisions. Deep RL
thus essentially appears as a black box, which severely limits the debugging of
adaptive systems. We previously introduced the explainable RL technique
XRL-DINE, which provides visual insights into why certain decisions were made
at important time points. Here, we introduce an empirical user study involving
54 software engineers from academia and industry to assess (1) the performance
of software engineers when performing different tasks using XRL-DINE and (2)
the perceived usefulness and ease of use of XRL-DINE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04114">FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zihao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1">Yunkai Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1">Dong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huishuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weiran Huang</a></p>
<p>Few-shot learning aims to train models that can be generalized to novel
classes with only a few samples. Recently, a line of works are proposed to
enhance few-shot learning with accessible semantic information from class
names. However, these works focus on improving existing modules such as visual
prototypes and feature extractors of the standard few-shot learning framework.
This limits the full potential use of semantic information. In this paper, we
propose a novel few-shot learning framework that uses pre-trained language
models based on contrastive learning. To address the challenge of alignment
between visual features and textual embeddings obtained from text-based
pre-trained language model, we carefully design the textual branch of our
framework and introduce a metric module to generalize the cosine similarity.
For better transferability, we let the metric module adapt to different
few-shot tasks and adopt MAML to train the model via bi-level optimization.
Moreover, we conduct extensive experiments on multiple benchmarks to
demonstrate the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04131">Carbon-Efficient Neural Architecture Search. (arXiv:2307.04131v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tian Guo</a></p>
<p>This work presents a novel approach to neural architecture search (NAS) that
aims to reduce energy costs and increase carbon efficiency during the model
design process. The proposed framework, called carbon-efficient NAS (CE-NAS),
consists of NAS evaluation algorithms with different energy requirements, a
multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS
dynamically balances energy-efficient sampling and energy-consuming evaluation
tasks based on current carbon emissions. Using a recent NAS benchmark dataset
and two carbon traces, our trace-driven simulations demonstrate that CE-NAS
achieves better carbon and search efficiency than the three baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04132">Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seshadri_A/0/1/0/all/0/1">Amrit Diggavi Seshadri</a>, <a href="http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1">Alessandra Russo</a></p>
<p>In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip's corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04137">A Novel Explainable Artificial Intelligence Model in Image Classification problem. (arXiv:2307.04137v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Quoc Hung Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Truong Thanh Hung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Vo Thanh Khang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1">Xuan Phong Nguyen</a></p>
<p>In recent years, artificial intelligence is increasingly being applied widely
in many different fields and has a profound and direct impact on human life.
Following this is the need to understand the principles of the model making
predictions. Since most of the current high-precision models are black boxes,
neither the AI scientist nor the end-user deeply understands what's going on
inside these models. Therefore, many algorithms are studied for the purpose of
explaining AI models, especially those in the problem of image classification
in the field of computer vision such as LIME, CAM, GradCAM. However, these
algorithms still have limitations such as LIME's long execution time and CAM's
confusing interpretation of concreteness and clarity. Therefore, in this paper,
we propose a new method called Segmentation - Class Activation Mapping (SeCAM)
that combines the advantages of these algorithms above, while at the same time
overcoming their disadvantages. We tested this algorithm with various models,
including ResNet50, Inception-v3, VGG16 from ImageNet Large Scale Visual
Recognition Challenge (ILSVRC) data set. Outstanding results when the algorithm
has met all the requirements for a specific explanation in a remarkably concise
time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04147">A Survey and Approach to Chart Classification. (arXiv:2307.04147v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhote_A/0/1/0/all/0/1">Anurag Dhote</a>, <a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1">Mohammed Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1">David S Doermann</a></p>
<p>Charts represent an essential source of visual information in documents and
facilitate a deep understanding and interpretation of information typically
conveyed numerically. In the scientific literature, there are many charts, each
with its stylistic differences. Recently the document understanding community
has begun to address the problem of automatic chart understanding, which begins
with chart classification. In this paper, we present a survey of the current
state-of-the-art techniques for chart classification and discuss the available
datasets and their supported chart types. We broadly classify these
contributions as traditional approaches based on ML, CNN, and Transformers.
Furthermore, we carry out an extensive comparative performance analysis of
CNN-based and transformer-based approaches on the recently published CHARTINFO
UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The
data set includes 15 different chart categories, including 22,923 training
images and 13,260 test images. We have implemented a vision-based transformer
model that produces state-of-the-art results in chart classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04149">Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Ayush Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhambhu_Y/0/1/0/all/0/1">Yash Bhambhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckchash_H/0/1/0/all/0/1">Himanshu Buckchash</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1">Deepak K. Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasad_D/0/1/0/all/0/1">Dilip K. Prasad</a></p>
<p>Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04192">SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1">Min-Yen Kan</a>, <a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1">Soujanya Poria</a></p>
<p>Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04195">Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work. (arXiv:2307.04195v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Somin Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Menassa_C/0/1/0/all/0/1">Carol C. Menassa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamat_V/0/1/0/all/0/1">Vineet R. Kamat</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1">Joyce Y. Chai</a></p>
<p>The introduction of robots is widely considered to have significant potential
of alleviating the issues of worker shortage and stagnant productivity that
afflict the construction industry. However, it is challenging to use fully
automated robots in complex and unstructured construction sites. Human-Robot
Collaboration (HRC) has shown promise of combining human workers' flexibility
and robot assistants' physical abilities to jointly address the uncertainties
inherent in construction work. When introducing HRC in construction, it is
critical to recognize the importance of teamwork and supervision in field
construction and establish a natural and intuitive communication system for the
human workers and robotic assistants. Natural language-based interaction can
enable intuitive and familiar communication with robots for human workers who
are non-experts in robot programming. However, limited research has been
conducted on this topic in construction. This paper proposes a framework to
allow human workers to interact with construction robots based on natural
language instructions. The proposed method consists of three stages: Natural
Language Understanding (NLU), Information Mapping (IM), and Robot Control (RC).
Natural language instructions are input to a language model to predict a tag
for each word in the NLU module. The IM module uses the result of the NLU
module and building component information to generate the final instructional
output essential for a robot to acknowledge and perform the construction task.
A case study for drywall installation is conducted to evaluate the proposed
approach. The obtained results highlight the potential of using natural
language-based interaction to replicate the communication that occurs between
human workers within the context of human-robot teams.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04208">On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise. (arXiv:2307.04208v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arthur_L/0/1/0/all/0/1">Lauren Arthur</a>, <a href="http://arxiv.org/find/cs/1/au:+Costello_J/0/1/0/all/0/1">Jason Costello</a>, <a href="http://arxiv.org/find/cs/1/au:+Hardy_J/0/1/0/all/0/1">Jonathan Hardy</a>, <a href="http://arxiv.org/find/cs/1/au:+OBrien_W/0/1/0/all/0/1">Will O&#x27;Brien</a>, <a href="http://arxiv.org/find/cs/1/au:+Rea_J/0/1/0/all/0/1">James Rea</a>, <a href="http://arxiv.org/find/cs/1/au:+Rees_G/0/1/0/all/0/1">Gareth Rees</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganev_G/0/1/0/all/0/1">Georgi Ganev</a></p>
<p>Generative AI technologies are gaining unprecedented popularity, causing a
mix of excitement and apprehension through their remarkable capabilities. In
this paper, we study the challenges associated with deploying synthetic data, a
subfield of Generative AI. Our focus centers on enterprise deployment, with an
emphasis on privacy concerns caused by the vast amount of personal and highly
sensitive data. We identify 40+ challenges and systematize them into five main
groups -- i) generation, ii) infrastructure &amp; architecture, iii) governance,
iv) compliance &amp; regulation, and v) adoption. Additionally, we discuss a
strategic and systematic approach that enterprises can employ to effectively
address the challenges and achieve their goals by establishing trust in the
implemented solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04216">Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data. (arXiv:2307.04216v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hieu Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_H/0/1/0/all/0/1">Hernan Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jian Tao</a></p>
<p>Lossy compression has become an important technique to reduce data size in
many domains. This type of compression is especially valuable for large-scale
scientific data, whose size ranges up to several petabytes. Although
Autoencoder-based models have been successfully leveraged to compress images
and videos, such neural networks have not widely gained attention in the
scientific data domain. Our work presents a neural network that not only
significantly compresses large-scale scientific data but also maintains high
reconstruction quality. The proposed model is tested with scientific benchmark
data available publicly and applied to a large-scale high-resolution climate
modeling data set. Our model achieves a compression ratio of 140 on several
benchmark data sets without compromising the reconstruction quality. Simulation
data from the High-Resolution Community Earth System Model (CESM) Version 1.3
over 500 years are also being compressed with a compression ratio of 200 while
the reconstruction error is negligible for scientific analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04217">LakeBench: Benchmarks for Data Discovery over Data Lakes. (arXiv:2307.04217v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srinivas_K/0/1/0/all/0/1">Kavitha Srinivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolby_J/0/1/0/all/0/1">Julian Dolby</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1">Ibrahim Abdelaziz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanzadeh_O/0/1/0/all/0/1">Oktie Hassanzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kokel_H/0/1/0/all/0/1">Harsha Kokel</a>, <a href="http://arxiv.org/find/cs/1/au:+Khatiwada_A/0/1/0/all/0/1">Aamod Khatiwada</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedapati_T/0/1/0/all/0/1">Tejaswini Pedapati</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1">Subhajit Chaudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Samulowitz_H/0/1/0/all/0/1">Horst Samulowitz</a></p>
<p>Within enterprises, there is a growing need to intelligently navigate data
lakes, specifically focusing on data discovery. Of particular importance to
enterprises is the ability to find related tables in data repositories. These
tables can be unionable, joinable, or subsets of each other. There is a dearth
of benchmarks for these tasks in the public domain, with related work targeting
private datasets. In LakeBench, we develop multiple benchmarks for these tasks
by using the tables that are drawn from a diverse set of data sources such as
government data from CKAN, Socrata, and the European Central Bank. We compare
the performance of 4 publicly available tabular foundational models on these
tasks. None of the existing models had been trained on the data discovery tasks
that we developed for this benchmark; not surprisingly, their performance shows
significant room for improvement. The results suggest that the establishment of
such benchmarks may be useful to the community to build tabular models usable
for data discovery in data lakes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04223">Real-time Human Detection in Fire Scenarios using Infrared and Thermal Imaging Fusion. (arXiv:2307.04223v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1">Truong-Dong Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_N/0/1/0/all/0/1">Nghe-Nhan Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1">My-Ha Le</a></p>
<p>Fire is considered one of the most serious threats to human lives which
results in a high probability of fatalities. Those severe consequences stem
from the heavy smoke emitted from a fire that mostly restricts the visibility
of escaping victims and rescuing squad. In such hazardous circumstances, the
use of a vision-based human detection system is able to improve the ability to
save more lives. To this end, a thermal and infrared imaging fusion strategy
based on multiple cameras for human detection in low-visibility scenarios
caused by smoke is proposed in this paper. By processing with multiple cameras,
vital information can be gathered to generate more useful features for human
detection. Firstly, the cameras are calibrated using a Light Heating
Chessboard. Afterward, the features extracted from the input images are merged
prior to being passed through a lightweight deep neural network to perform the
human detection task. The experiments conducted on an NVIDIA Jetson Nano
computer demonstrated that the proposed method can process with reasonable
speed and can achieve favorable performance with a mAP@0.5 of 95%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04245">A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing. (arXiv:2307.04245v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rakshit_A/0/1/0/all/0/1">Aishik Rakshit</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1">Samyak Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Anirban Dasgupta</a></p>
<p>Optical Character Recognition (OCR) technology finds applications in
digitizing books and unstructured documents, along with applications in other
domains such as mobility statistics, law enforcement, traffic, security
systems, etc. The state-of-the-art methods work well with the OCR with printed
text on license plates, shop names, etc. However, applications such as printed
textbooks and handwritten texts have limited accuracy with existing techniques.
The reason may be attributed to similar-looking characters and variations in
handwritten characters. Since these issues are challenging to address with OCR
technologies exclusively, we propose a post-processing approach using Natural
Language Processing (NLP) tools. This work presents an end-to-end pipeline that
first performs OCR on the handwritten or printed text and then improves its
accuracy using NLP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04251">ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1">Salman Mohamadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1">Ghulam Mujtaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1">Gianfranco Doretto</a>, <a href="http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1">Donald A. Adjeroh</a></p>
<p>ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04287">Generalizing Graph ODE for Learning Complex System Dynamics across Environments. (arXiv:2307.04287v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zijie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yizhou Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a></p>
<p>Learning multi-agent system dynamics has been extensively studied for various
real-world applications, such as molecular dynamics in biology. Most of the
existing models are built to learn single system dynamics from observed
historical data and predict the future trajectory. In practice, however, we
might observe multiple systems that are generated across different
environments, which differ in latent exogenous factors such as temperature and
gravity. One simple solution is to learn multiple environment-specific models,
but it fails to exploit the potential commonalities among the dynamics across
environments and offers poor prediction results where per-environment data is
sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary
Differential Equations), a machine learning framework for learning continuous
multi-agent system dynamics across environments. Our model learns system
dynamics using neural ordinary differential equations (ODE) parameterized by
Graph Neural Networks (GNNs) to capture the continuous interaction among
agents. We achieve the model generalization by assuming the dynamics across
different environments are governed by common physics laws that can be captured
via learning a shared ODE function. The distinct latent exogenous factors
learned for each environment are incorporated into the ODE function to account
for their differences. To improve model performance, we additionally design two
regularization losses to (1) enforce the orthogonality between the learned
initial states and exogenous factors via mutual information minimization; and
(2) reduce the temporal variance of learned exogenous factors within the same
system via contrastive learning. Experiments over various physical simulations
show that our model can accurately predict system dynamics, especially in the
long range, and can generalize well to new systems with few observations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04292">A Demand-Driven Perspective on Generative Audio AI. (arXiv:2307.04292v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Oh_S/0/1/0/all/0/1">Sangshin Oh</a>, <a href="http://arxiv.org/find/eess/1/au:+Kang_M/0/1/0/all/0/1">Minsung Kang</a>, <a href="http://arxiv.org/find/eess/1/au:+Moon_H/0/1/0/all/0/1">Hyeongi Moon</a>, <a href="http://arxiv.org/find/eess/1/au:+Choi_K/0/1/0/all/0/1">Keunwoo Choi</a>, <a href="http://arxiv.org/find/eess/1/au:+Chon_B/0/1/0/all/0/1">Ben Sangbae Chon</a></p>
<p>To achieve successful deployment of AI research, it is crucial to understand
the demands of the industry. In this paper, we present the results of a survey
conducted with professional audio engineers, in order to determine research
priorities and define various research tasks. We also summarize the current
challenges in audio quality and controllability based on the survey. Our
analysis emphasizes that the availability of datasets is currently the main
bottleneck for achieving high-quality audio generation. Finally, we suggest
potential solutions for some revealed issues with empirical evidence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04303">Learning to Generate Equitable Text in Dialogue from Biased Training Data. (arXiv:2307.04303v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1">Anthony Sicilia</a>, <a href="http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1">Malihe Alikhani</a></p>
<p>The ingrained principles of fairness in a dialogue system's decision-making
process and generated responses are crucial for user engagement, satisfaction,
and task achievement. Absence of equitable and inclusive principles can hinder
the formation of common ground, which in turn negatively impacts the overall
performance of the system. For example, misusing pronouns in a user interaction
may cause ambiguity about the intended subject. Yet, there is no comprehensive
study of equitable text generation in dialogue. Aptly, in this work, we use
theories of computational learning to study this problem. We provide formal
definitions of equity in text generation, and further, prove formal connections
between learning human-likeness and learning equity: algorithms for improving
equity ultimately reduce to algorithms for improving human-likeness (on
augmented data). With this insight, we also formulate reasonable conditions
under which text generation algorithms can learn to generate equitable text
without any modifications to the biased training data on which they learn. To
exemplify our theory in practice, we look at a group of algorithms for the
GuessWhat?! visual dialogue game and, using this example, test our theory
empirically. Our theory accurately predicts relative-performance of multiple
algorithms in generating equitable text as measured by both human and automated
evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.03774">Learning to extrapolate using continued fractions: Predicting the critical temperature of superconductor materials. (arXiv:2012.03774v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moscato_P/0/1/0/all/0/1">Pablo Moscato</a>, <a href="http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1">Mohammad Nazmul Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kevin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sloan_J/0/1/0/all/0/1">Julia Sloan</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1">Jon C. de Oliveira</a></p>
<p>In the field of Artificial Intelligence (AI) and Machine Learning (ML), the
approximation of unknown target functions $y=f(\mathbf{x})$ using limited
instances $S={(\mathbf{x^{(i)}},y^{(i)})}$, where $\mathbf{x^{(i)}} \in D$ and
$D$ represents the domain of interest, is a common objective. We refer to $S$
as the training set and aim to identify a low-complexity mathematical model
that can effectively approximate this target function for new instances
$\mathbf{x}$. Consequently, the model's generalization ability is evaluated on
a separate set $T=\{\mathbf{x^{(j)}}\} \subset D$, where $T \neq S$, frequently
with $T \cap S = \emptyset$, to assess its performance beyond the training set.
However, certain applications require accurate approximation not only within
the original domain $D$ but also in an extended domain $D'$ that encompasses
$D$. This becomes particularly relevant in scenarios involving the design of
new structures, where minimizing errors in approximations is crucial. For
example, when developing new materials through data-driven approaches, the
AI/ML system can provide valuable insights to guide the design process by
serving as a surrogate function. Consequently, the learned model can be
employed to facilitate the design of new laboratory experiments. In this paper,
we propose a method for multivariate regression based on iterative fitting of a
continued fraction, incorporating additive spline models. We compare the
performance of our method with established techniques, including AdaBoost,
Kernel Ridge, Linear Regression, Lasso Lars, Linear Support Vector Regression,
Multi-Layer Perceptrons, Random Forests, Stochastic Gradient Descent, and
XGBoost. To evaluate these methods, we focus on an important problem in the
field: predicting the critical temperature of superconductors based on
physical-chemical characteristics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.12556">A Survey on Visual Transformer. (arXiv:2012.12556v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hanting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinghao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenhua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yehui Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1">An Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chunjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yixing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhaohui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yiman Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Transformer, first applied to the field of natural language processing, is a
type of deep neural network mainly based on the self-attention mechanism.
Thanks to its strong representation capabilities, researchers are looking at
ways to apply transformer to computer vision tasks. In a variety of visual
benchmarks, transformer-based models perform similar to or better than other
types of networks such as convolutional and recurrent neural networks. Given
its high performance and less need for vision-specific inductive bias,
transformer is receiving more and more attention from the computer vision
community. In this paper, we review these vision transformer models by
categorizing them in different tasks and analyzing their advantages and
disadvantages. The main categories we explore include the backbone network,
high/mid-level vision, low-level vision, and video processing. We also include
efficient transformer methods for pushing transformer into real device-based
applications. Furthermore, we also take a brief look at the self-attention
mechanism in computer vision, as it is the base component in transformer.
Toward the end of this paper, we discuss the challenges and provide several
further research directions for vision transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.14452">A unified logical framework for explanations in classifier systems. (arXiv:2105.14452v8 [cs.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinghan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lorini_E/0/1/0/all/0/1">Emiliano Lorini</a></p>
<p>Recent years have witnessed a renewed interest in Boolean function in
explaining binary classifiers in the field of explainable AI (XAI). The
standard approach of Boolean function is propositional logic. We present a
modal language of a ceteris paribus nature which supports reasoning about
binary input classifiers and their properties. We study a family of classifier
models, axiomatize it as two proof systems regarding the cardinality of the
language and show completeness of our axiomatics. Moreover, we prove that
satisfiability checking problem for our modal language is NEXPTIME-complete in
the infinite-variable case, while it becomes polynomial in the finite-variable
case. We furthermore identify an interesting NP fragment of our language in the
infinite-variable case. We leverage the language to formalize counterfactual
conditional as well as a variety of notions of explanation including abductive,
contrastive and counterfactual explanations, and biases. Finally, we present
two extensions of our language: a dynamic extension by the notion of assignment
enabling classifier change and an epistemic extension in which the classifier's
uncertainty about the actual input can be represented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.12468">SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhihong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zuyue Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lingxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1">Chenjia Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a></p>
<p>Offline reinforcement learning (RL) harnesses the power of massive datasets
for resolving sequential decision problems. Most existing papers only discuss
defending against out-of-distribution (OOD) actions while we investigate a
broader issue, the spurious correlations between epistemic uncertainty and
decision-making, an essential factor that causes suboptimality. In this paper,
we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically
effective and theoretically provable algorithm. We empirically show that SCORE
achieves the SoTA performance with 3.1x acceleration on various tasks in a
standard benchmark (D4RL). The proposed algorithm introduces an annealing
behavior cloning regularizer to help produce a high-quality estimation of
uncertainty which is critical for eliminating spurious correlations from
suboptimality. Theoretically, we justify the rationality of the proposed method
and prove its convergence to the optimal policy with a sublinear rate under
mild assumptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.09266">GFlowNet Foundations. (arXiv:2111.09266v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahlou_S/0/1/0/all/0/1">Salem Lahlou</a>, <a href="http://arxiv.org/find/cs/1/au:+Deleu_T/0/1/0/all/0/1">Tristan Deleu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1">Edward J. Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1">Mo Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1">Emmanuel Bengio</a></p>
<p>Generative Flow Networks (GFlowNets) have been introduced as a method to
sample a diverse set of candidates in an active learning context, with a
training objective that makes them approximately sample in proportion to a
given reward function. In this paper, we show a number of additional
theoretical properties of GFlowNets. They can be used to estimate joint
probability distributions and the corresponding marginal distributions where
some variables are unspecified and, of particular interest, can represent
distributions over composite objects like sets and graphs. GFlowNets amortize
the work typically done by computationally expensive MCMC methods in a single
but trained generative pass. They could also be used to estimate partition
functions and free energies, conditional probabilities of supersets
(supergraphs) given a subset (subgraph), as well as marginal distributions over
all supersets (supergraphs) of a given set (graph). We introduce variations
enabling the estimation of entropy and mutual information, sampling from a
Pareto frontier, connections to reward-maximizing policies, and extensions to
stochastic environments, continuous actions and modular energy functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.11089">Monocular Road Planar Parallax Estimation. (arXiv:2111.11089v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haobo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Teng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_W/0/1/0/all/0/1">Wei Sui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jiafeng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lefei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qian Zhang</a></p>
<p>Estimating the 3D structure of the drivable surface and surrounding
environment is a crucial task for assisted and autonomous driving. It is
commonly solved either by using 3D sensors such as LiDAR or directly predicting
the depth of points via deep learning. However, the former is expensive, and
the latter lacks the use of geometry information for the scene. In this paper,
instead of following existing methodologies, we propose Road Planar Parallax
Attention Network (RPANet), a new deep neural network for 3D sensing from
monocular image sequences based on planar parallax, which takes full advantage
of the omnipresent road plane geometry in driving scenes. RPANet takes a pair
of images aligned by the homography of the road plane as input and outputs a
$\gamma$ map (the ratio of height to depth) for 3D reconstruction. The $\gamma$
map has the potential to construct a two-dimensional transformation between two
consecutive frames. It implies planar parallax and can be combined with the
road plane serving as a reference to estimate the 3D structure by warping the
consecutive frames. Furthermore, we introduce a novel cross-attention module to
make the network better perceive the displacements caused by planar parallax.
To verify the effectiveness of our method, we sample data from the Waymo Open
Dataset and construct annotations related to planar parallax. Comprehensive
experiments are conducted on the sampled dataset to demonstrate the 3D
reconstruction accuracy of our approach in challenging scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08581">Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2112.08581v5 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Weijie Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1">Benjamin Doerr</a></p>
<p>The non-dominated sorting genetic algorithm II (NSGA-II) is the most
intensively used multi-objective evolutionary algorithm (MOEA) in real-world
applications. However, in contrast to several simple MOEAs analyzed also via
mathematical means, no such study exists for the NSGA-II so far. In this work,
we show that mathematical runtime analyses are feasible also for the NSGA-II.
As particular results, we prove that with a population size four times larger
than the size of the Pareto front, the NSGA-II with two classic mutation
operators and four different ways to select the parents satisfies the same
asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic
OneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the population
size is only equal to the size of the Pareto front, then the NSGA-II cannot
efficiently compute the full Pareto front: for an exponential number of
iterations, the population will always miss a constant fraction of the Pareto
front. Our experiments confirm the above findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.07286">Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youfang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Sheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1">Kai Lv</a></p>
<p>Safety exploration can be regarded as a constrained Markov decision problem
where the expected long-term cost is constrained. Previous off-policy
algorithms convert the constrained optimization problem into the corresponding
unconstrained dual problem by introducing the Lagrangian relaxation technique.
However, the cost function of the above algorithms provides inaccurate
estimations and causes the instability of the Lagrange multiplier learning. In
this paper, we present a novel off-policy reinforcement learning algorithm
called Conservative Distributional Maximum a Posteriori Policy Optimization
(CDMPO). At first, to accurately judge whether the current situation satisfies
the constraints, CDMPO adapts distributional reinforcement learning method to
estimate the Q-function and C-function. Then, CDMPO uses a conservative value
function loss to reduce the number of violations of constraints during the
exploration process. In addition, we utilize Weighted Average Proportional
Integral Derivative (WAPID) to update the Lagrange multiplier stably. Empirical
results show that the proposed method has fewer violations of constraints in
the early exploration process. The final test results also illustrate that our
method has better risk control.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.11931">Fast Interpretable Greedy-Tree Sums. (arXiv:2201.11931v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yan Shuo Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1">Chandan Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasseri_K/0/1/0/all/0/1">Keyan Nasseri</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Abhineet Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1">James Duncan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ronen_O/0/1/0/all/0/1">Omer Ronen</a>, <a href="http://arxiv.org/find/cs/1/au:+Epland_M/0/1/0/all/0/1">Matthew Epland</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_A/0/1/0/all/0/1">Aaron Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bin Yu</a></p>
<p>Modern machine learning has achieved impressive prediction performance, but
often sacrifices interpretability, a critical consideration in high-stakes
domains such as medicine. In such settings, practitioners often use highly
interpretable decision tree models, but these suffer from inductive bias
against additive structure. To overcome this bias, we propose Fast
Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to
simultaneously grow a flexible number of trees in summation. By combining
logical rules with addition, FIGS is able to adapt to additive structure while
remaining highly interpretable. Extensive experiments on real-world datasets
show that FIGS achieves state-of-the-art prediction performance. To demonstrate
the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical
decision instruments (CDIs), which are tools for guiding clinical
decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS
that accounts for the heterogeneity in medical data. G-FIGS derives CDIs that
reflect domain knowledge and enjoy improved specificity (by up to 20% over
CART) without sacrificing sensitivity or interpretability. To provide further
insight into FIGS, we prove that FIGS learns components of additive models, a
property we refer to as disentanglement. Further, we show (under oracle
conditions) that unconstrained tree-sum models leverage disentanglement to
generalize more efficiently than single decision tree models when fitted to
additive regression functions. Finally, to avoid overfitting with an
unconstrained number of splits, we develop Bagging-FIGS, an ensemble version of
FIGS that borrows the variance reduction techniques of random forests.
Bagging-FIGS enjoys competitive performance with random forests and XGBoost on
real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.12893">Cryptocurrency Valuation: An Explainable AI Approach. (arXiv:2201.12893v8 [econ.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Liu_Y/0/1/0/all/0/1">Yulin Liu</a>, <a href="http://arxiv.org/find/econ/1/au:+Zhang_L/0/1/0/all/0/1">Luyao Zhang</a></p>
<p>Currently, there are no convincing proxies for the fundamentals of
cryptocurrency assets. We propose a new market-to-fundamental ratio, the
price-to-utility (PU) ratio, utilizing unique blockchain accounting methods. We
then proxy various existing fundamental-to-market ratios by Bitcoin historical
data and find they have little predictive power for short-term bitcoin returns.
However, PU ratio effectively predicts long-term bitcoin returns than
alternative methods. Furthermore, we verify the explainability of PU ratio
using machine learning. Finally, we present an automated trading strategy
advised by the PU ratio that outperforms the conventional buy-and-hold and
market-timing strategies. Our research contributes to explainable AI in finance
from three facets: First, our market-to-fundamental ratio is based on classic
monetary theory and the unique UTXO model of Bitcoin accounting rather than ad
hoc; Second, the empirical evidence testifies the buy-low and sell-high
implications of the ratio; Finally, we distribute the trading algorithms as
open-source software via Python Package Index for future research, which is
exceptional in finance research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04530">DORA: Exploring Outlier Representations in Deep Neural Networks. (arXiv:2206.04530v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a>, <a href="http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1">Mayukh Deb</a>, <a href="http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1">Dennis Grinwald</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a></p>
<p>Deep Neural Networks (DNNs) excel at learning complex abstractions within
their internal representations. However, the concepts they learn remain opaque,
a problem that becomes particularly acute when models unintentionally learn
spurious correlations. In this work, we present DORA (Data-agnOstic
Representation Analysis), the first data-agnostic framework for analyzing the
representational space of DNNs. Central to our framework is the proposed
Extreme-Activation (EA) distance measure, which assesses similarities between
representations by analyzing their activation patterns on data points that
cause the highest level of activation. As spurious correlations often manifest
in features of data that are anomalous to the desired task, such as watermarks
or artifacts, we demonstrate that internal representations capable of detecting
such artifactual concepts can be found by analyzing relationships within neural
representations. We validate the EA metric quantitatively, demonstrating its
effectiveness both in controlled scenarios and real-world applications.
Finally, we provide practical examples from popular Computer Vision models to
illustrate that representations identified as outliers using the EA metric
often correspond to undesired and spurious concepts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.12252">Indecision Trees: Learning Argument-Based Reasoning under Quantified Uncertainty. (arXiv:2206.12252v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kent_J/0/1/0/all/0/1">Jonathan S. Kent</a>, <a href="http://arxiv.org/find/cs/1/au:+Menager_D/0/1/0/all/0/1">David H. Menager</a></p>
<p>Using Machine Learning systems in the real world can often be problematic,
with inexplicable black-box models, the assumed certainty of imperfect
measurements, or providing a single classification instead of a probability
distribution.
</p>
<p>This paper introduces Indecision Trees, a modification to Decision Trees
which learn under uncertainty, can perform inference under uncertainty, provide
a robust distribution over the possible labels, and can be disassembled into a
set of logical arguments for use in other reasoning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.06154">On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1">Luca Bortolussi</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1">Ginevra Carbone</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurenti_L/0/1/0/all/0/1">Luca Laurenti</a>, <a href="http://arxiv.org/find/cs/1/au:+Patane_A/0/1/0/all/0/1">Andrea Patane</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanguinetti_G/0/1/0/all/0/1">Guido Sanguinetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1">Matthew Wicker</a></p>
<p>Vulnerability to adversarial attacks is one of the principal hurdles to the
adoption of deep learning in safety-critical applications. Despite significant
efforts, both practical and theoretical, training deep learning models robust
to adversarial attacks is still an open problem. In this paper, we analyse the
geometry of adversarial attacks in the large-data, overparameterized limit for
Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to
gradient-based attacks arises as a result of degeneracy in the data
distribution, i.e., when the data lies on a lower-dimensional submanifold of
the ambient space. As a direct consequence, we demonstrate that in this limit
BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we
prove that the expected gradient of the loss with respect to the BNN posterior
distribution is vanishing, even when each neural network sampled from the
posterior is vulnerable to gradient-based attacks. Experimental results on the
MNIST, Fashion MNIST, and half moons datasets, representing the finite data
regime, with BNNs trained with Hamiltonian Monte Carlo and Variational
Inference, support this line of arguments, showing that BNNs can display both
high accuracy on clean data and robustness to both gradient-based and
gradient-free based adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.01307">Multilingual Coreference Resolution in Multiparty Dialogue. (arXiv:2208.01307v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Boyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1">Patrick Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1">Mahsa Yarmohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a></p>
<p>Existing multiparty dialogue datasets for entity coreference resolution are
nascent, and many challenges are still unaddressed. We create a large-scale
dataset, Multilingual Multiparty Coref (MMC), for this task based on TV
transcripts. Due to the availability of gold-quality subtitles in multiple
languages, we propose reusing the annotations to create silver coreference
resolution data in other languages (Chinese and Farsi) via annotation
projection. On the gold (English) data, off-the-shelf models perform relatively
poorly on MMC, suggesting that MMC has broader coverage of multiparty
coreference than prior datasets. On the silver data, we find success both using
it for data augmentation and training from scratch, which effectively simulates
the zero-shot cross-lingual setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10264">Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aher_G/0/1/0/all/0/1">Gati Aher</a>, <a href="http://arxiv.org/find/cs/1/au:+Arriaga_R/0/1/0/all/0/1">Rosa I. Arriaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a></p>
<p>We introduce a new type of test, called a Turing Experiment (TE), for
evaluating to what extent a given language model, such as GPT models, can
simulate different aspects of human behavior. A TE can also reveal consistent
distortions in a language model's simulation of a specific human behavior.
Unlike the Turing Test, which involves simulating a single arbitrary
individual, a TE requires simulating a representative sample of participants in
human subject research. We carry out TEs that attempt to replicate
well-established findings from prior studies. We design a methodology for
simulating TEs and illustrate its use to compare how well different language
models are able to reproduce classic economic, psycholinguistic, and social
psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock
Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings
were replicated using recent models, while the last TE reveals a
"hyper-accuracy distortion" present in some language models (including ChatGPT
and GPT-4), which could affect downstream applications in education and the
arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10967">The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1">Ashwin De Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1">Rahul Ramesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1">Carey E. Priebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1">Pratik Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1">Joshua T. Vogelstein</a></p>
<p>We expect the generalization error to improve with more samples from a
similar task, and to deteriorate with more samples from an out-of-distribution
(OOD) task. In this work, we show a counter-intuitive phenomenon: the
generalization error of a task can be a non-monotonic function of the number of
OOD samples. As the number of OOD samples increases, the generalization error
on the target task improves before deteriorating beyond a threshold. In other
words, there is value in training on small amounts of OOD data. We use Fisher's
Linear Discriminant on synthetic datasets and deep networks on computer vision
benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate
and analyze this phenomenon. In the idealistic setting where we know which
samples are OOD, we show that these non-monotonic trends can be exploited using
an appropriately weighted objective of the target and OOD empirical risk. While
its practical utility is limited, this does suggest that if we can detect OOD
samples, then there may be ways to benefit from them. When we do not know which
samples are OOD, we show how a number of go-to strategies such as
data-augmentation, hyper-parameter optimization, and pre-training are not
enough to ensure that the target generalization error does not deteriorate with
the number of OOD samples in the dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.12306">Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qingyun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Manling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1">Hou Pong Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lifu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1">Julia Hockenmaier</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1">Girish Chowdhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a></p>
<p>Goal-oriented generative script learning aims to generate subsequent steps to
reach a particular goal, which is an essential task to assist robots or humans
in performing stereotypical activities. An important aspect of this process is
the ability to capture historical states visually, which provides detailed
information that is not covered by text and will guide subsequent steps.
Therefore, we propose a new task, Multimedia Generative Script Learning, to
generate subsequent steps by tracking historical states in both text and vision
modalities, as well as presenting the first benchmark containing 5,652 tasks
and 79,089 multimedia steps. This task is challenging in three aspects: the
multimedia challenge of capturing the visual states in images, the induction
challenge of performing unseen tasks, and the diversity challenge of covering
different information in individual steps. We propose to encode visual state
changes through a selective multimedia encoder to address the multimedia
challenge, transfer knowledge from previously observed tasks using a
retrieval-augmented decoder to overcome the induction challenge, and further
present distinct information at each step by optimizing a diversity-oriented
contrastive learning objective. We define metrics to evaluate both generation
and inductive quality. Experiment results demonstrate that our approach
significantly outperforms strong baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.02370">Continual Learning, Fast and Slow. (arXiv:2209.02370v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1">Quang Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1">Steven C. H. Hoi</a></p>
<p>According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.04547">Defend Data Poisoning Attacks on Voice Authentication. (arXiv:2209.04547v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Baird_C/0/1/0/all/0/1">Cameron Baird</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dan Lin</a></p>
<p>With the advances in deep learning, speaker verification has achieved very
high accuracy and is gaining popularity as a type of biometric authentication
option in many scenes of our daily life, especially the growing market of web
services. Compared to traditional passwords, "vocal passwords" are much more
convenient as they relieve people from memorizing different passwords. However,
new machine learning attacks are putting these voice authentication systems at
risk. Without a strong security guarantee, attackers could access legitimate
users' web accounts by fooling the deep neural network (DNN) based voice
recognition models. In this paper, we demonstrate an easy-to-implement data
poisoning attack to the voice authentication system, which can hardly be
captured by existing defense mechanisms. Thus, we propose a more robust defense
method, called Guardian, which is a convolutional neural network-based
discriminator. The Guardian discriminator integrates a series of novel
techniques including bias reduction, input augmentation, and ensemble learning.
Our approach is able to distinguish about 95% of attacked accounts from normal
accounts, which is much more effective than existing approaches with only 60%
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.16575">Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms. (arXiv:2210.16575v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dagdanov_R/0/1/0/all/0/1">Resul Dagdanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Durmus_H/0/1/0/all/0/1">Halil Durmus</a>, <a href="http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1">Nazim Kemal Ure</a></p>
<p>In this work, we propose a self-improving artificial intelligence system to
enhance the safety performance of reinforcement learning (RL)-based autonomous
driving (AD) agents using black-box verification methods. RL algorithms have
become popular in AD applications in recent years. However, the performance of
existing RL algorithms heavily depends on the diversity of training scenarios.
A lack of safety-critical scenarios during the training phase could result in
poor generalization performance in real-world driving applications. We propose
a novel framework in which the weaknesses of the training set are explored
through black-box verification methods. After discovering AD failure scenarios,
the RL agent's training is re-initiated via transfer learning to improve the
performance of previously unsafe scenarios. Simulation results demonstrate that
our approach efficiently discovers safety failures of action decisions in
RL-based adaptive cruise control (ACC) applications and significantly reduces
the number of vehicle collisions through iterative applications of our method.
The source code is publicly available at
https://github.com/data-and-decision-lab/self-improving-RL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.15657">Is Conditional Generative Modeling all you need for Decision-Making?. (arXiv:2211.15657v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ajay_A/0/1/0/all/0/1">Anurag Ajay</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yilun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Abhi Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1">Tommi Jaakkola</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01735">Neural Fourier Filter Bank. (arXiv:2212.01735v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhijie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yuhe Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kwang Moo Yi</a></p>
<p>We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02098">A Machine with Short-Term, Episodic, and Semantic Memory Systems. (arXiv:2212.02098v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taewoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1">Michael Cochez</a>, <a href="http://arxiv.org/find/cs/1/au:+Francois_Lavet_V/0/1/0/all/0/1">Vincent Fran&#xe7;ois-Lavet</a>, <a href="http://arxiv.org/find/cs/1/au:+Neerincx_M/0/1/0/all/0/1">Mark Neerincx</a>, <a href="http://arxiv.org/find/cs/1/au:+Vossen_P/0/1/0/all/0/1">Piek Vossen</a></p>
<p>Inspired by the cognitive science theory of the explicit human memory
systems, we have modeled an agent with short-term, episodic, and semantic
memory systems, each of which is modeled with a knowledge graph. To evaluate
this system and analyze the behavior of this agent, we designed and released
our own reinforcement learning agent environment, "the Room", where an agent
has to learn how to encode, store, and retrieve memories to maximize its return
by answering questions. We show that our deep Q-learning based agent
successfully learns whether a short-term memory should be forgotten, or rather
be stored in the episodic or semantic memory systems. Our experiments indicate
that an agent with human-like memory systems can outperform an agent without
this memory structure in the environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03712">Enhanced Multi-Objective A* with Partial Expansion. (arXiv:2212.03712v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kothare_V/0/1/0/all/0/1">Valmiki Kothare</a>, <a href="http://arxiv.org/find/math/1/au:+Ren_Z/0/1/0/all/0/1">Zhongqiang Ren</a>, <a href="http://arxiv.org/find/math/1/au:+Rathinam_S/0/1/0/all/0/1">Sivakumar Rathinam</a>, <a href="http://arxiv.org/find/math/1/au:+Choset_H/0/1/0/all/0/1">Howie Choset</a></p>
<p>The Multi-Objective Shortest Path Problem (MO-SPP), typically posed on a
graph, determines a set of paths from a start vertex to a destination vertex
while optimizing multiple objectives. In general, there does not exist a single
solution path that can simultaneously optimize all the objectives and the
problem thus seeks to find a set of so-called Pareto-optimal solutions. To
address this problem, several Multi-Objective A* (MOA*) algorithms were
recently developed to quickly compute solutions with quality guarantees.
However, these MOA* algorithms often suffer from high memory usage, especially
when the branching factor (i.e. the number of neighbors of any vertex) of the
graph is large. This work thus aims at reducing the high memory consumption of
MOA* with little increase in the runtime. By generalizing and unifying several
single- and multi-objective search algorithms, we develop the Runtime and
Memory Efficient MOA* (RME-MOA*) approach, which can balance between runtime
and memory efficiency by tuning two user-defined hyper-parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.06951">AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v5 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yihang Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zesen Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Luyao Zhang</a></p>
<p>Blockchain has empowered computer systems to be more secure using a
distributed network. However, the current blockchain design suffers from
fairness issues in transaction ordering. Miners are able to reorder
transactions to generate profits, the so-called miner extractable value (MEV).
Existing research recognizes MEV as a severe security issue and proposes
potential solutions, including prominent Flashbots. However, previous studies
have mostly analyzed blockchain data, which might not capture the impacts of
MEV in a much broader AI society. Thus, in this research, we applied natural
language processing (NLP) methods to comprehensively analyze topics in tweets
on MEV. We collected more than 20000 tweets with #MEV and #Flashbots hashtags
and analyzed their topics. Our results show that the tweets discussed profound
topics of ethical concern, including security, equity, emotional sentiments,
and the desire for solutions to MEV. We also identify the co-movements of MEV
activities on blockchain and social media platforms. Our study contributes to
the literature at the interface of blockchain security, MEV solutions, and AI
ethics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.14106">Provable Robust Saliency-based Explanations. (arXiv:2212.14106v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chenghua Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1">Guixiang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Ming Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Sihong Xie</a></p>
<p>Robust explanations of machine learning models are critical to establishing
human trust in the models. The top-$k$ intersection is widely used to evaluate
the robustness of explanations. However, most existing attacking and defense
strategies are based on $\ell_p$ norms, thus creating a mismatch between the
evaluation and optimization objectives. To this end, we define explanation
thickness for measuring top-$k$ salient features ranking stability, and design
the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize
the thickness and stabilize the top salient features efficiently.
Theoretically, we prove a connection between R2ET and adversarial training;
using a novel multi-objective optimization formulation and a generalization
error bound, we further prove that the surrogate objective can improve both the
numerical and statistical stability of the explanations. Experiments with a
wide spectrum of network architectures and data modalities demonstrate that
R2ET attains higher explanation robustness under stealthy attacks while
retaining model accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01828">On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kessler_S/0/1/0/all/0/1">Samuel Kessler</a>, <a href="http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1">Adam Cobb</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1">Tim G. J. Rudner</a>, <a href="http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1">Stefan Zohren</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1">Stephen J. Roberts</a></p>
<p>Sequential Bayesian inference can be used for continual learning to prevent
catastrophic forgetting of past tasks and provide an informative prior when
learning new tasks. We revisit sequential Bayesian inference and test whether
having access to the true posterior is guaranteed to prevent catastrophic
forgetting in Bayesian neural networks. To do this we perform sequential
Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as
a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo
samples. We find that this approach fails to prevent catastrophic forgetting
demonstrating the difficulty in performing sequential Bayesian inference in
neural networks. From there we study simple analytical examples of sequential
Bayesian inference and CL and highlight the issue of model misspecification
which can lead to sub-optimal continual learning performance despite exact
inference. Furthermore, we discuss how task data imbalances can cause
forgetting. From these limitations, we argue that we need probabilistic models
of the continual learning generative process rather than relying on sequential
Bayesian inference over Bayesian neural network weights. In this vein, we also
propose a simple baseline called Prototypical Bayesian Continual Learning,
which is competitive with state-of-the-art Bayesian continual learning methods
on class incremental continual learning vision benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.07733">Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1">Aaron Defazio</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1">Konstantin Mishchenko</a></p>
<p>D-Adaptation is an approach to automatically setting the learning rate which
asymptotically achieves the optimal rate of convergence for minimizing convex
Lipschitz functions, with no back-tracking or line searches, and no additional
function value or gradient evaluations per step. Our approach is the first
hyper-parameter free method for this class without additional multiplicative
log factors in the convergence rate. We present extensive experiments for SGD
and Adam variants of our method, where the method automatically matches
hand-tuned learning rates across more than a dozen diverse machine learning
problems, including large-scale vision and language problems.
</p>
<p>An open-source implementation is available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13359">IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1">Guoyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinbao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiayi Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaochu Jin</a></p>
<p>Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently many advanced algorithms have been
published, but their performance deviates greatly. We realize that the lack of
actual IM settings most probably hinders the development and usage of these
methods in real-world applications. As far as we know, IAD methods are not
evaluated systematically. As a result, this makes it difficult for researchers
to analyze them because they are designed for different or special cases. To
solve this problem, we first propose a uniform IM setting to assess how well
these algorithms perform, which includes several aspects, i.e., various levels
of supervision (unsupervised vs. semi-supervised), few-shot learning, continual
learning, noisy labels, memory usage, and inference speed. Moreover, we
skillfully build a comprehensive image anomaly detection benchmark (IM-IAD)
that includes 16 algorithms on 7 mainstream datasets with uniform settings. Our
extensive experiments (17,017 in total) provide in-depth insights for IAD
algorithm redesign or selection under the IM setting. Next, the proposed
benchmark IM-IAD gives challenges as well as directions for the future. To
foster reproducibility and accessibility, the source code of IM-IAD is uploaded
on the website, https://github.com/M-3LAB/IM-IAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01018">Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Longa_A/0/1/0/all/0/1">Antonio Longa</a>, <a href="http://arxiv.org/find/cs/1/au:+Lachi_V/0/1/0/all/0/1">Veronica Lachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Santin_G/0/1/0/all/0/1">Gabriele Santin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1">Monica Bianchini</a>, <a href="http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1">Bruno Lepri</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Lio</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1">Franco Scarselli</a>, <a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1">Andrea Passerini</a></p>
<p>Graph Neural Networks (GNNs) have become the leading paradigm for learning on
(static) graph-structured data. However, many real-world systems are dynamic in
nature, since the graph and node/edge attributes change over time. In recent
years, GNN-based models for temporal graphs have emerged as a promising area of
research to extend the capabilities of GNNs. In this work, we provide the first
comprehensive overview of the current state-of-the-art of temporal GNN,
introducing a rigorous formalization of learning settings and tasks and a novel
taxonomy categorizing existing approaches in terms of how the temporal aspect
is represented and processed. We conclude the survey with a discussion of the
most relevant open challenges for the field, from both research and application
perspectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13485">FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning. (arXiv:2302.13485v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xixu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Federated learning (FL) has emerged as a new paradigm for privacy-preserving
computation in recent years. Unfortunately, FL faces two critical challenges
that hinder its actual performance: data distribution heterogeneity and high
resource costs brought by large foundation models. Specifically, the non-IID
data in different clients make existing FL algorithms hard to converge while
the high resource costs, including computational and communication costs that
increase the deployment difficulty in real-world scenarios. In this paper, we
propose an effective yet simple method, named FedCLIP, to achieve fast
generalization and personalization for CLIP in federated learning. Concretely,
we design an attention-based adapter for the large model, CLIP, and the rest
operations merely depend on adapters. Lightweight adapters can make the most
use of pretrained model information and ensure models be adaptive for clients
in specific tasks. Simultaneously, small-scale operations can mitigate the
computational burden and communication burden caused by large models. Extensive
experiments are conducted on three datasets with distribution shifts.
Qualitative and quantitative results demonstrate that FedCLIP significantly
outperforms other baselines (9% overall improvements on PACS) and effectively
reduces computational and communication costs (283x faster than FedAVG). Our
code will be available at: https://github.com/microsoft/PersonalizedFL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00501">OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System. (arXiv:2303.00501v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1">Chao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shuai Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenfang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaxing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xuyang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shanshan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qiong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yibo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1">Fengxiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1">Bohua Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_R/0/1/0/all/0/1">Rongcheng Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Heliang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dongkai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Daqing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shijin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yukang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanpu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shixiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yibing Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Automated machine learning (AutoML) seeks to build ML models with minimal
human effort. While considerable research has been conducted in the area of
AutoML in general, aiming to take humans out of the loop when building
artificial intelligence (AI) applications, scant literature has focused on how
AutoML works well in open-environment scenarios such as the process of training
and updating large models, industrial supply chains or the industrial
metaverse, where people often face open-loop problems during the search
process: they must continuously collect data, update data and models, satisfy
the requirements of the development and deployment environment, support massive
devices, modify evaluation metrics, etc. Addressing the open-environment issue
with pure data-driven approaches requires considerable data, computing
resources, and effort from dedicated data engineers, making current AutoML
systems and platforms inefficient and computationally intractable.
Human-computer interaction is a practical and feasible way to tackle the
problem of open-environment AI. In this paper, we introduce OmniForce, a
human-centered AutoML (HAML) system that yields both human-assisted ML and
ML-assisted human techniques, to put an AutoML system into practice and build
adaptive AI in open-environment scenarios. Specifically, we present OmniForce
in terms of ML version management; pipeline-driven development and deployment
collaborations; a flexible search strategy framework; and widely provisioned
and crowdsourced application algorithms, including large models. Furthermore,
the (large) models constructed by OmniForce can be automatically turned into
remote services in a few minutes; this process is dubbed model as a service
(MaaS). Experimental results obtained in multiple search spaces and real-world
use cases demonstrate the efficacy and efficiency of OmniForce.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02401">Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Toan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1">Minh Nhat Vu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vuong_A/0/1/0/all/0/1">An Vuong</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dzung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1">Thieu Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a></p>
<p>Affordance detection is a challenging problem with a wide variety of robotic
applications. Traditional affordance detection methods are limited to a
predefined set of affordance labels, hence potentially restricting the
adaptability of intelligent robots in complex and dynamic environments. In this
paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,
which is capable of detecting an unbounded number of affordances in 3D point
clouds. By simultaneously learning the affordance text and the point feature,
OpenAD successfully exploits the semantic relationships between affordances.
Therefore, our proposed method enables zero-shot detection and can be able to
detect previously unseen affordances without a single annotation example.
Intensive experimental results show that OpenAD works effectively on a wide
range of affordance detection setups and outperforms other baselines by a large
margin. Additionally, we demonstrate the practicality of the proposed OpenAD in
real-world robotic applications with a fast inference speed (~100ms). Our
project is available at https://openad2023.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04865">Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games. (arXiv:2303.04865v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhaoyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zaiwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yiheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1">Adam Wierman</a></p>
<p>We introduce a class of networked Markov potential games in which agents are
associated with nodes in a network. Each agent has its own local potential
function, and the reward of each agent depends only on the states and actions
of the agents within a neighborhood. In this context, we propose a localized
actor-critic algorithm. The algorithm is scalable since each agent uses only
local information and does not need access to the global state. Further, the
algorithm overcomes the curse of dimensionality through the use of function
approximation. Our main results provide finite-sample guarantees up to a
localization error and a function approximation error. Specifically, we achieve
an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by
the averaged Nash regret. This is the first finite-sample bound for multi-agent
competitive games that does not depend on the number of agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07308">NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects. (arXiv:2303.07308v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jiahui Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yilun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1">Kurran Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua B. Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1">John J. Leonard</a></p>
<p>We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and
illustrate how it supports object SLAM for consistent spatial understanding
with long-term scene changes. NeuSE is a set of latent object embeddings
created from partial object observations. It serves as a compact point cloud
surrogate for complete object models, encoding full shape information while
transforming SE(3)-equivariantly in tandem with the object in the physical
world. With NeuSE, relative frame transforms can be directly derived from
inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape
and pose characterization, can operate independently or in conjunction with
typical SLAM systems. It directly infers SE(3) camera pose constraints that are
compatible with general SLAM pose graph optimization, while also maintaining a
lightweight object-centric map that adapts to real-world changes. Our approach
is evaluated on synthetic and real-world sequences featuring changed objects
and shows improved localization accuracy and change-aware mapping capability,
when working either standalone or jointly with a common SLAM pipeline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14773">BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning. (arXiv:2303.14773v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1">Changdae Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1">Hyeji Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hee-young Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1">YongTaek Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_G/0/1/0/all/0/1">Geunyoung Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1">Jiyoung Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1">Hosik Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kyungwoo Song</a></p>
<p>With the surge of large-scale pre-trained models (PTMs), fine-tuning these
models to numerous downstream tasks becomes a crucial problem. Consequently,
parameter efficient transfer learning (PETL) of large models has grasped huge
attention. While recent PETL methods showcase impressive performance, they rely
on optimistic assumptions: 1) the entire parameter set of a PTM is available,
and 2) a sufficiently large memory capacity for the fine-tuning is equipped.
However, in most real-world applications, PTMs are served as a black-box API or
proprietary software without explicit parameter accessibility. Besides, it is
hard to meet a large memory requirement for modern PTMs. In this work, we
propose black-box visual prompting (BlackVIP), which efficiently adapts the
PTMs without knowledge about model architectures and parameters. BlackVIP has
two components; 1) Coordinator and 2) simultaneous perturbation stochastic
approximation with gradient correction (SPSA-GC). The Coordinator designs
input-dependent image-shaped visual prompts, which improves few-shot adaptation
and robustness on distribution/location shift. SPSA-GC efficiently estimates
the gradient of a target model to update Coordinator. Extensive experiments on
16 datasets demonstrate that BlackVIP enables robust adaptation to diverse
domains without accessing PTMs' parameters, with minimal memory requirements.
Code: \url{https://github.com/changdaeoh/BlackVIP}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17607">Machine learning for discovering laws of nature. (arXiv:2303.17607v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1">Lizhi Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_K/0/1/0/all/0/1">Kevin Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_H/0/1/0/all/0/1">Houwen Xin</a></p>
<p>Based on Darwin's natural selection, we developed "machine scientists" to
discover the laws of nature by learning from raw data. "Machine scientists"
construct physical theories by applying a logic tree (state Decision Tree) and
a value tree (observation Function Tree); the logical tree determines the state
of the entity, and the value tree determines the absolute value between the two
observations of the entity. A logic Tree and a value tree together can
reconstruct an entity's trajectory and make predictions about its future
outcomes. Our proposed algorithmic model has an emphasis on machine learning -
where "machine scientists" builds up its experience by being rewarded or
punished for each decision they make - eventually leading to rediscovering
Newton's equation (classical physics) and the Born's rule (quantum mechanics).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00008">On the Creativity of Large Language Models. (arXiv:2304.00008v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1">Giorgio Franceschelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1">Mirco Musolesi</a></p>
<p>Large Language Models (LLMs) are revolutionizing several areas of Artificial
Intelligence. One of the most remarkable applications is creative writing,
e.g., poetry or storytelling: the generated outputs are often of astonishing
quality. However, a natural question arises: can LLMs be really considered
creative? In this article we firstly analyze the development of LLMs under the
lens of creativity theories, investigating the key open questions and
challenges. In particular, we focus our discussion around the dimensions of
value, novelty and surprise as proposed by Margaret Boden in her work. Then, we
consider different classic perspectives, namely product, process, press and
person. We discuss a set of ``easy'' and ``hard'' problems in machine
creativity, presenting them in relation to LLMs. Finally, we examine the
societal impact of these technologies with a particular focus on the creative
industries, analyzing the opportunities offered by them, the challenges arising
by them and the potential associated risks, from both legal and ethical points
of view.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00215">Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Quan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhendong Mao</a></p>
<p>Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element's contribution to the prediction
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14251">Variational Bayes Made Easy. (arXiv:2304.14251v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Mohammad Emtiyaz Khan</a></p>
<p>Variational Bayes is a popular method for approximate inference but its
derivation can be cumbersome. To simplify the process, we give a 3-step recipe
to identify the posterior form by explicitly looking for linearity with respect
to expectations of well-known distributions. We can then directly write the
update by simply ``reading-off'' the terms in front of those expectations. The
recipe makes the derivation easier, faster, shorter, and more general.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06569">How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v4 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10292">Linear Query Approximation Algorithms for Non-monotone Submodular Maximization under Knapsack Constraint. (arXiv:2305.10292v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1">Canh V. Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Tan D. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1">Dung T.K. Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Thai_M/0/1/0/all/0/1">My T. Thai</a></p>
<p>This work, for the first time, introduces two constant factor approximation
algorithms with linear query complexity for non-monotone submodular
maximization over a ground set of size $n$ subject to a knapsack constraint,
$\mathsf{DLA}$ and $\mathsf{RLA}$. $\mathsf{DLA}$ is a deterministic algorithm
that provides an approximation factor of $6+\epsilon$ while $\mathsf{RLA}$ is a
randomized algorithm with an approximation factor of $4+\epsilon$. Both run in
$O(n \log(1/\epsilon)/\epsilon)$ query complexity. The key idea to obtain a
constant approximation ratio with linear query lies in: (1) dividing the ground
set into two appropriate subsets to find the near-optimal solution over these
subsets with linear queries, and (2) combining a threshold greedy with
properties of two disjoint sets or a random selection process to improve
solution quality. In addition to the theoretical analysis, we have evaluated
our proposed solutions with three applications: Revenue Maximization, Image
Summarization, and Maximum Weighted Cut, showing that our algorithms not only
return comparative results to state-of-the-art algorithms but also require
significantly fewer queries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16044">Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v4 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1">Gehua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Huajin Tang</a></p>
<p>-- A theoretical framework that subsumes conventional deterministic spiking
neural networks and surrogate gradients, facilitating more efficient and
effective employment of various neuromorphic hardware developments in
real-world applications.
</p>
<p>-- Scalable spiking neural models that incorporate noisy neuronal dynamics
for implicit regularization, improved robustness, and computational accounts of
biological neural computation, revealing that unreliable neural substrates
yield reliable computation and learning.
</p>
<p>Networks of spiking neurons underpin the extraordinary information-processing
capabilities of the brain and have emerged as pillar models in neuromorphic
intelligence. Despite extensive research on spiking neural networks (SNNs),
most are established on deterministic models. Integrating noise into SNNs leads
to biophysically more realistic neural dynamics and may benefit model
performance. This work presents the noisy spiking neural network (NSNN) and the
noise-driven learning rule (NDL) by introducing a spiking neuron model
incorporating noisy neuronal dynamics. Our approach shows how noise may serve
as a resource for computation and learning and theoretically provides a
framework for general SNNs. We show that our method exhibits competitive
performance and improved robustness against challenging perturbations than
deterministic SNNs and better reproduces probabilistic neural computation in
neural coding. This study offers a powerful and easy-to-use tool for machine
learning, neuromorphic intelligence practitioners, and computational
neuroscience researchers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18451">Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Namkyeong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kanghoon Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_G/0/1/0/all/0/1">Gyoung S. Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sein Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1">Chanyoung Park</a></p>
<p>Recently, molecular relational learning, whose goal is to predict the
interaction behavior between molecular pairs, got a surge of interest in
molecular sciences due to its wide range of applications. In this work, we
propose CMRL that is robust to the distributional shift in molecular relational
learning by detecting the core substructure that is causally related to
chemical reactions. To do so, we first assume a causal relationship based on
the domain knowledge of molecular sciences and construct a structural causal
model (SCM) that reveals the relationship between variables. Based on the SCM,
we introduce a novel conditional intervention framework whose intervention is
conditioned on the paired molecule. With the conditional intervention
framework, our model successfully learns from the causal substructure and
alleviates the confounding effect of shortcut substructures that are spuriously
correlated to chemical reactions. Extensive experiments on various tasks with
real-world and synthetic datasets demonstrate the superiority of CMRL over
state-of-the-art baseline models. Our code is available at
https://github.com/Namkyeong/CMRL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18703">Large Language Models, Natural Language Processing, Domain Specialization. (arXiv:2305.18703v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1">Chen Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xujiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiaying Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chengyuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Can Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1">Tanmoy Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1">Hejie Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuchao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianjiao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Panalkar_A/0/1/0/all/0/1">Amit Panalkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yanchi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhengzhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1">Chris White</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jian Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Carl Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Liang Zhao</a></p>
<p>Large language models (LLMs) have significantly advanced the field of natural
language processing (NLP), providing a highly useful, task-agnostic foundation
for a wide range of applications. However, directly applying LLMs to solve
sophisticated problems in specific domains meets many hurdles, caused by the
heterogeneity of domain data, the sophistication of domain knowledge, the
uniqueness of domain objectives, and the diversity of the constraints (e.g.,
various social norms, cultural conformity, religious beliefs, and ethical
standards in the domain applications). Domain specification techniques are key
to make large language models disruptive in many applications. Specifically, to
solve these hurdles, there has been a notable increase in research and
practices conducted in recent years on the domain specialization of LLMs. This
emerging field of study, with its substantial potential for impact,
necessitates a comprehensive and systematic review to better summarize and
guide ongoing work in this area. In this article, we present a comprehensive
survey on domain specification techniques for large language models, an
emerging direction critical for large language model applications. First, we
propose a systematic taxonomy that categorizes the LLM domain-specialization
techniques based on the accessibility to LLMs and summarizes the framework for
all the subcategories as well as their relations and differences to each other.
Second, we present an extensive taxonomy of critical application domains that
can benefit dramatically from specialized LLMs, discussing their practical
significance and open challenges. Last, we offer our insights into the current
research status and future trends in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01076">Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1">Samridhi Choudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1">Siegfried Kunzmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a></p>
<p>Fine-tuned transformer models have shown superior performances in many
natural language tasks. However, the large model size prohibits deploying
high-performance transformer models on resource-constrained devices. This paper
proposes a quantization-aware tensor-compressed training approach to reduce the
model size, arithmetic operations, and ultimately runtime latency of
transformer-based models. We compress the embedding and linear layers of
transformers into small low-rank tensor cores, which significantly reduces
model parameters. A quantization-aware training with learnable scale factors is
used to further obtain low-precision representations of the tensor-compressed
models. The developed approach can be used for both end-to-end training and
distillation-based training. To improve the convergence, a layer-by-layer
distillation is applied to distill a quantized and tensor-compressed student
model from a pre-trained transformer. The performance is demonstrated in two
natural language understanding tasks, showing up to $63\times$ compression
ratio, little accuracy loss and remarkable inference and training speedup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01505">Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dou Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1">Yinan Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lingwei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Songlin Hu</a></p>
<p>Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations in a supervised manner. SACL applies
contrast-aware adversarial training to generate worst-case samples and uses
joint class-spread contrastive learning to extract structured representations.
It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training (CAT) strategy to learn more diverse features from context and enhance
the model's context robustness. Under the framework with CAT, we develop a
sequence-based SACL-LSTM to learn label-consistent and context-robust features
for ERC. Experiments on three datasets show that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of SACL and CAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01657">DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1">Guanqun Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Lei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yanan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Meng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuqiang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaodong He</a></p>
<p>Empathy is a crucial factor in open-domain conversations, which naturally
shows one's caring and understanding to others. Though several methods have
been proposed to generate empathetic responses, existing works often lead to
monotonous empathy that refers to generic and safe expressions. In this paper,
we propose to use explicit control to guide the empathy expression and design a
framework DiffusEmp based on conditional diffusion language model to unify the
utilization of dialogue context and attribute-oriented control signals.
Specifically, communication mechanism, intent, and semantic frame are imported
as multi-grained signals that control the empathy realization from coarse to
fine levels. We then design a specific masking strategy to reflect the
relationship between multi-grained signals and response tokens, and integrate
it into the diffusion model to influence the generative process. Experimental
results on a benchmark dataset EmpatheticDialogue show that our framework
outperforms competitive baselines in terms of controllability, informativeness,
and diversity without the loss of context-relatedness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01772">Re-imagining health and well-being in low resource African settings using an augmented AI system and a 3D digital twin. (arXiv:2306.01772v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moodley_D/0/1/0/all/0/1">Deshendran Moodley</a>, <a href="http://arxiv.org/find/cs/1/au:+Seebregts_C/0/1/0/all/0/1">Christopher Seebregts</a></p>
<p>This paper discusses and explores the potential and relevance of recent
developments in artificial intelligence (AI) and digital twins for health and
well-being in low-resource African countries. We use the case of public health
emergency response to disease outbreaks and epidemic control. There is
potential to take advantage of the increasing availability of data and
digitization to develop advanced AI methods for analysis and prediction. Using
an AI systems perspective, we review emerging trends in AI systems and digital
twins and propose an initial augmented AI system architecture to illustrate how
an AI system can work with a 3D digital twin to address public health goals. We
highlight scientific knowledge discovery, continual learning, pragmatic
interoperability, and interactive explanation and decision-making as essential
research challenges for AI systems and digital twins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02312">(Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility. (arXiv:2306.02312v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1">Kacper Sokol</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia E. Vogt</a></p>
<p>Ante-hoc interpretability has become the holy grail of explainable artificial
intelligence for high-stakes domains such as healthcare; however, this notion
is elusive, lacks a widely-accepted definition and depends on the operational
context. It can refer to predictive models whose structure adheres to
domain-specific constraints, or ones that are inherently transparent. The
latter conceptualisation assumes observers who judge this quality, whereas the
former presupposes them to have technical and domain expertise (thus alienating
other groups of explainees). Additionally, the distinction between ante-hoc
interpretability and the less desirable post-hoc explainability, which refers
to methods that construct a separate explanatory model, is vague given that
transparent predictive models may still require (post-)processing to yield
suitable explanatory insights. Ante-hoc interpretability is thus an overloaded
concept that comprises a range of implicit properties, which we unpack in this
paper to better understand what is needed for its safe adoption across
high-stakes domains. To this end, we outline modelling and explaining
desiderata that allow us to navigate its distinct realisations in view of the
envisaged application and audience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04139">A Comprehensive Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1">Heejoon Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">To Eun Kim</a></p>
<p>In recent years, generative diffusion models have achieved a rapid paradigm
shift in deep generative models by showing groundbreaking performance across
various applications. Meanwhile, structured data, encompassing tabular and time
series data, has been received comparatively limited attention from the deep
learning research community, despite its omnipresence and extensive
applications. Thus, there is still a lack of literature and its reviews on
structured data modelling via diffusion models, compared to other data
modalities such as visual and textual data. To address this gap, we present a
comprehensive review of recently proposed diffusion models in the field of
structured data. First, this survey provides a concise overview of the
score-based diffusion model theory, subsequently proceeding to the technical
descriptions of the majority of pioneering works that used structured data in
both data-driven general tasks and domain-specific applications. Thereafter, we
analyse and discuss the limitations and challenges shown in existing works and
suggest potential research directions. We hope this review serves as a catalyst
for the research community, promoting developments in generative diffusion
models for structured data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08861">Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization. (arXiv:2306.08861v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_M/0/1/0/all/0/1">Makito Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Chen-Chieh Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1">Keito Inoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Yojima_S/0/1/0/all/0/1">Sentaro Yojima</a>, <a href="http://arxiv.org/find/cs/1/au:+Takahashi_M/0/1/0/all/0/1">Masafumi Takahashi</a></p>
<p>In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset both to the
public and the market. We conduct a comprehensive study on motion style
transfer in the experiment using the state-of-the-art method, and the results
show the proposed dataset's validity for the motion style transfer task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09417">Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mehta_S/0/1/0/all/0/1">Shivam Mehta</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Siyang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Alexanderson_S/0/1/0/all/0/1">Simon Alexanderson</a>, <a href="http://arxiv.org/find/eess/1/au:+Beskow_J/0/1/0/all/0/1">Jonas Beskow</a>, <a href="http://arxiv.org/find/eess/1/au:+Szekely_E/0/1/0/all/0/1">&#xc9;va Sz&#xe9;kely</a>, <a href="http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1">Gustav Eje Henter</a></p>
<p>With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. For
synthesised examples please see https://shivammehta25.github.io/Diff-TTSG
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15079">From $O(\sqrt{n})$ to $O(\log(n))$ in Quadratic Programming. (arXiv:2306.15079v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wu_L/0/1/0/all/0/1">Liang Wu</a></p>
<p>A "dark cloud" hangs over numerical optimization theory for decades, namely,
whether an optimization algorithm $O(\log(n))$ iteration complexity exists.
"Yes", this paper answers, with a new optimization algorithm and strict theory
proof. It starts with box-constrained quadratic programming (Box-QP), and many
practical optimization problems fall into Box-QP. General smooth quadratic
programming (QP), nonsmooth Lasso, and support vector machine (or regression)
can be reformulated as Box-QP via duality theory. It is the first time to
present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which
behaves like a "direct" method: the required number of iterations is
deterministic with exact value
$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$.
This significant breakthrough enables us to transition from the $O(\sqrt{n})$
to the $O(\log(n))$ optimization algorithm, whose amazing scalability is
particularly relevant in today's era of big data and artificial intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15666">Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weber_Wulff_D/0/1/0/all/0/1">Debora Weber-Wulff</a> (University of Applied Sciences HTW Berlin, Germany), <a href="http://arxiv.org/find/cs/1/au:+Anohina_Naumeca_A/0/1/0/all/0/1">Alla Anohina-Naumeca</a> (Riga Technical University, Latvia), <a href="http://arxiv.org/find/cs/1/au:+Bjelobaba_S/0/1/0/all/0/1">Sonja Bjelobaba</a> (Uppsala University, Sweden), <a href="http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1">Tom&#xe1;&#x161; Folt&#xfd;nek</a> (Masaryk University, Czechia), <a href="http://arxiv.org/find/cs/1/au:+Guerrero_Dib_J/0/1/0/all/0/1">Jean Guerrero-Dib</a> (Universidad de Monterrey, Mexico), <a href="http://arxiv.org/find/cs/1/au:+Popoola_O/0/1/0/all/0/1">Olumide Popoola</a> (Queen Mary University of London, UK), <a href="http://arxiv.org/find/cs/1/au:+Sigut_P/0/1/0/all/0/1">Petr &#x160;igut</a> (Masaryk University, Czechia), <a href="http://arxiv.org/find/cs/1/au:+Waddington_L/0/1/0/all/0/1">Lorna Waddington</a> (University of Leeds, UK)</p>
<p>Recent advances in generative pre-trained transformer large language models
have emphasised the potential risks of unfair use of artificial intelligence
(AI) generated content in an academic environment and intensified efforts in
searching for solutions to detect such content. The paper examines the general
functionality of detection tools for artificial intelligence generated text and
evaluates them based on accuracy and error type analysis. Specifically, the
study seeks to answer research questions about whether existing detection tools
can reliably differentiate between human-written text and ChatGPT-generated
text, and whether machine translation and content obfuscation techniques affect
the detection of AI-generated text. The research covers 12 publicly available
tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely
used in the academic setting. The researchers conclude that the available
detection tools are neither accurate nor reliable and have a main bias towards
classifying the output as human-written rather than detecting AI-generated
text. Furthermore, content obfuscation techniques significantly worsen the
performance of tools. The study makes several significant contributions. First,
it summarises up-to-date similar scientific and non-scientific efforts in the
field. Second, it presents the result of one of the most comprehensive tests
conducted so far, based on a rigorous research methodology, an original
document set, and a broad coverage of tools. Third, it discusses the
implications and drawbacks of using detection tools for AI-generated text in
academic settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17170">An Overview on Generative AI at Scale with Edge-Cloud Computing. (arXiv:2306.17170v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yun-Cheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jintang Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chengwei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1">C.-C. Jay Kuo</a></p>
<p>As a specific category of artificial intelligence (AI), generative artificial
intelligence (GenAI) generates new content that resembles what is created by
humans. The rapid development of GenAI systems has created a huge amount of new
data on the Internet, posing new challenges to current computing and
communication frameworks. Currently, GenAI services rely on the traditional
cloud computing framework due to the need for large computation resources.
However, such services will encounter high latency because of data transmission
and a high volume of requests. On the other hand, edge-cloud computing can
provide adequate computation power and low latency at the same time through the
collaboration between edges and the cloud. Thus, it is attractive to build
GenAI systems at scale by leveraging the edge-cloud computing paradigm. In this
overview paper, we review recent developments in GenAI and edge-cloud
computing, respectively. Then, we use two exemplary GenAI applications to
discuss technical challenges in scaling up their solutions using edge-cloud
collaborative systems. Finally, we list design considerations for training and
deploying GenAI systems at scale and point out future research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01473">Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmadi_R/0/1/0/all/0/1">Raha Ahmadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajabi_M/0/1/0/all/0/1">Mohammad Javad Rajabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalooie_M/0/1/0/all/0/1">Mohammad Khalooie</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1">Mohammad Sabokrou</a></p>
<p>Deep learning models have demonstrated remarkable capabilities in learning
complex patterns and concepts from training data. However, recent findings
indicate that these models tend to rely heavily on simple and easily
discernible features present in the background of images rather than the main
concepts or objects they are intended to classify. This phenomenon poses a
challenge to image classifiers as the crucial elements of interest in images
may be overshadowed. In this paper, we propose a novel approach to address this
issue and improve the learning of main concepts by image classifiers. Our
central idea revolves around concurrently guiding the model's attention toward
the foreground during the classification task. By emphasizing the foreground,
which encapsulates the primary objects of interest, we aim to shift the focus
of the model away from the dominant influence of the background. To accomplish
this, we introduce a mechanism that encourages the model to allocate sufficient
attention to the foreground. We investigate various strategies, including
modifying the loss function or incorporating additional architectural
components, to enable the classifier to effectively capture the primary concept
within an image. Additionally, we explore the impact of different foreground
attention mechanisms on model performance and provide insights into their
effectiveness. Through extensive experimentation on benchmark datasets, we
demonstrate the efficacy of our proposed approach in improving the
classification accuracy of image classifiers. Our findings highlight the
importance of foreground attention in enhancing model understanding and
representation of the main concepts within images. The results of this study
contribute to advancing the field of image classification and provide valuable
insights for developing more robust and accurate deep-learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01689">Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Assos_A/0/1/0/all/0/1">Angelos Assos</a>, <a href="http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1">Idan Attias</a>, <a href="http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1">Yuval Dagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1">Constantinos Daskalakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1">Maxwell Fishelson</a></p>
<p>While ERM suffices to attain near-optimal generalization error in the
stochastic learning setting, this is not known to be the case in the online
learning setting, where algorithms for general concept classes rely on
computationally inefficient oracles such as the Standard Optimal Algorithm
(SOA). In this work, we propose an algorithm for online binary classification
setting that relies solely on ERM oracle calls, and show that it has finite
regret in the realizable setting and sublinearly growing regret in the agnostic
setting. We bound the regret in terms of the Littlestone and threshold
dimensions of the underlying concept class.
</p>
<p>We obtain similar results for nonparametric games, where the ERM oracle can
be interpreted as a best response oracle, finding the best response of a player
to a given history of play of the other players. In this setting, we provide
learning algorithms that only rely on best response oracles and converge to
approximate-minimax equilibria in two-player zero-sum games and approximate
coarse correlated equilibria in multi-player general-sum games, as long as the
game has a bounded fat-threshold dimension. Our algorithms apply to both
binary-valued and real-valued games and can be viewed as providing
justification for the wide use of double oracle and multiple oracle algorithms
in the practice of solving large games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03039">Art Authentication with Vision Transformers. (arXiv:2307.03039v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1">Ludovica Schaerf</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1">Carina Popovici</a>, <a href="http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1">Eric Postma</a></p>
<p>In recent years, Transformers, initially developed for language, have been
successfully applied to visual tasks. Vision Transformers have been shown to
push the state-of-the-art in a wide range of tasks, including image
classification, object detection, and semantic segmentation. While ample
research has shown promising results in art attribution and art authentication
tasks using Convolutional Neural Networks, this paper examines if the
superiority of Vision Transformers extends to art authentication, improving,
thus, the reliability of computer-based authentication of artworks. Using a
carefully compiled dataset of authentic paintings by Vincent van Gogh and two
contrast datasets, we compare the art authentication performances of Swin
Transformers with those of EfficientNet. Using a standard contrast set
containing imitations and proxies (works by painters with styles closely
related to van Gogh), we find that EfficientNet achieves the best performance
overall. With a contrast set that only consists of imitations, we find the Swin
Transformer to be superior to EfficientNet by achieving an authentication
accuracy of over 85%. These results lead us to conclude that Vision
Transformers represent a strong and promising contender in art authentication,
particularly in enhancing the computer-based ability to detect artistic
imitations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03393">Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhikai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Haitao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Wei Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hongzhi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaochi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wenqi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a></p>
<p>Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03419">QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geerkens_S/0/1/0/all/0/1">Simon Geerkens</a>, <a href="http://arxiv.org/find/cs/1/au:+Sieberichs_C/0/1/0/all/0/1">Christian Sieberichs</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1">Alexander Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Waschulzik_T/0/1/0/all/0/1">Thomas Waschulzik</a></p>
<p>The importance of high data quality is increasing with the growing impact and
distribution of ML systems and big data. Also the planned AI Act from the
European commission defines challenging legal requirements for data quality
especially for the market introduction of safety relevant ML systems. In this
paper we introduce a novel approach that supports the data quality assurance
process of multiple data quality aspects. This approach enables the
verification of quantitative data quality requirements. The concept and
benefits are introduced and explained on small example data sets. How the
method is applied is demonstrated on the well known MNIST data set based an
handwritten digits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00457">GenRec: Large Language Model for Generative Recommendation. (arXiv:2307.00457v2 [cs.IR] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jianchao Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zelong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Juntao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>In recent years, large language models (LLM) have emerged as powerful tools
for diverse natural language processing tasks. However, their potential for
recommender systems under the generative recommendation paradigm remains
relatively unexplored. This paper presents an innovative approach to
recommendation systems using large language models (LLMs) based on text data.
In this paper, we present a novel LLM for generative recommendation (GenRec)
that utilized the expressive power of LLM to directly generate the target item
to recommend, rather than calculating ranking score for each candidate item one
by one as in traditional discriminative recommendation. GenRec uses LLM's
understanding ability to interpret context, learn user preferences, and
generate relevant recommendation. Our proposed approach leverages the vast
knowledge encoded in large language models to accomplish recommendation tasks.
We first we formulate specialized prompts to enhance the ability of LLM to
comprehend recommendation tasks. Subsequently, we use these prompts to
fine-tune the LLaMA backbone LLM on a dataset of user-item interactions,
represented by textual data, to capture user preferences and item
characteristics. Our research underscores the potential of LLM-based generative
recommendation in revolutionizing the domain of recommendation systems and
offers a foundational framework for future explorations in this field. We
conduct extensive experiments on benchmark datasets, and the experiments shows
that our GenRec has significant better results on large dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01226">vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weijie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaoyu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1">Srinivasan H. Sengamedu</a>, <a href="http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1">Francis Iannacci</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jinjin Zhao</a></p>
<p>Recently, Neural Topic Models (NTM), inspired by variational autoencoders,
have attracted a lot of research interest; however, these methods have limited
applications in the real world due to the challenge of incorporating human
knowledge. This work presents a semi-supervised neural topic modeling method,
vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and
optimal transport. When a few keywords per topic are provided, vONTSS in the
semi-supervised setting generates potential topics and optimizes topic-keyword
quality and topic classification. Experiments show that vONTSS outperforms
existing semi-supervised topic modeling methods in classification accuracy and
diversity. vONTSS also supports unsupervised topic modeling. Quantitative and
qualitative experiments show that vONTSS in the unsupervised setting
outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered
and coherent topics on benchmark datasets. It is also much faster than the
state-of-the-art weakly supervised text classification method while achieving
similar classification performance. We further prove the equivalence of optimal
transport loss and cross-entropy loss at the global minimum.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02889">Learning to Solve Tasks with Exploring Prior Behaviours. (arXiv:2307.02889v1 [cs.RO] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Ruiqi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1">Tianhong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chongjie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Celiktutan_O/0/1/0/all/0/1">Oya Celiktutan</a></p>
<p>Demonstrations are widely used in Deep Reinforcement Learning (DRL) for
facilitating solving tasks with sparse rewards. However, the tasks in
real-world scenarios can often have varied initial conditions from the
demonstration, which would require additional prior behaviours. For example,
consider we are given the demonstration for the task of \emph{picking up an
object from an open drawer}, but the drawer is closed in the training. Without
acquiring the prior behaviours of opening the drawer, the robot is unlikely to
solve the task. To address this, in this paper we propose an Intrinsic Rewards
Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with
the ability to explore and acquire the required prior behaviours and then
connect to the task-specific behaviours in the demonstration to solve
sparse-reward tasks without requiring additional demonstration of the prior
behaviours. The performance of our method outperforms other baselines on three
navigation tasks and one robotic manipulation task with sparse rewards. Codes
are available at https://github.com/Ricky-Zhu/IRDEC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03056">Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1">Kevin Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1">Lucas Torroba Hennigen</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1">Niklas Stoehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1">Alexander Warstadt</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>Many popular feature-attribution methods for interpreting deep neural
networks rely on computing the gradients of a model's output with respect to
its inputs. While these methods can indicate which input features may be
important for the model's prediction, they reveal little about the inner
workings of the model itself. In this paper, we observe that the gradient
computation of a model is a special case of a more general formulation using
semirings. This observation allows us to generalize the backpropagation
algorithm to efficiently compute other interpretable statistics about the
gradient graph of a neural network, such as the highest-weighted path and
entropy. We implement this generalized algorithm, evaluate it on synthetic
datasets to better understand the statistics it computes, and apply it to study
BERT's behavior on the subject-verb number agreement task (SVA). With this
method, we (a) validate that the amount of gradient flow through a component of
a model reflects its importance to a prediction and (b) for SVA, identify which
pathways of the self-attention mechanism are most important.
</p>
</p>
</div>

    </div>
    </body>
    