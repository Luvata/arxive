<!DOCTYPE html>
<html>
<head>
<title>2024-01-31-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.15105">Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery. (arXiv:2401.15105v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sui_J/0/1/0/all/0/1">Jialu Sui</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1">Yiyang Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1">Wenhan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaokang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Pun_M/0/1/0/all/0/1">Man-On Pun</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiaying Liu</a></p>
<p>The presence of cloud layers severely compromises the quality and
effectiveness of optical remote sensing (RS) images. However, existing
deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties
in accurately reconstructing the original visual authenticity and detailed
semantic content of the images. To tackle this challenge, this work proposes to
encompass enhancements at the data and methodology fronts. On the data side, an
ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial
resolution is established. This benchmark incorporates rich detailed textures
and diverse cloud coverage, serving as a robust foundation for designing and
assessing CR models. From the methodology perspective, a novel diffusion-based
framework for CR called Diffusion Enhancement (DE) is proposed to perform
progressive texture detail recovery, which mitigates the training difficulty
with improved inference accuracy. Additionally, a Weight Allocation (WA)
network is developed to dynamically adjust the weights for feature fusion,
thereby further improving performance, particularly in the context of
ultra-resolution image generation. Furthermore, a coarse-to-fine training
strategy is applied to effectively expedite training convergence while reducing
the computational complexity required to handle ultra-resolution images.
Extensive experiments on the newly established CUHK-CR and existing datasets
such as RICE confirm that the proposed DE framework outperforms existing
DL-based methods in terms of both perceptual quality and signal fidelity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15111">Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning. (arXiv:2401.15111v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1">Mingquan Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Tianhao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_Z/0/1/0/all/0/1">Zhaoyi Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Holste_G/0/1/0/all/0/1">Gregory Holste</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_Y/0/1/0/all/0/1">Ying Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shih_G/0/1/0/all/0/1">George Shih</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1">Yifan Peng</a></p>
<p>Purpose: Limited studies exploring concrete methods or approaches to tackle
and enhance model fairness in the radiology domain. Our proposed AI model
utilizes supervised contrastive learning to minimize bias in CXR diagnosis.
</p>
<p>Materials and Methods: In this retrospective study, we evaluated our proposed
method on two datasets: the Medical Imaging and Data Resource Center (MIDRC)
dataset with 77,887 CXR images from 27,796 patients collected as of April 20,
2023 for COVID-19 diagnosis, and the NIH Chest X-ray (NIH-CXR) dataset with
112,120 CXR images from 30,805 patients collected between 1992 and 2015. In the
NIH-CXR dataset, thoracic abnormalities include atelectasis, cardiomegaly,
effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation,
edema, emphysema, fibrosis, pleural thickening, or hernia. Our proposed method
utilizes supervised contrastive learning with carefully selected positive and
negative samples to generate fair image embeddings, which are fine-tuned for
subsequent tasks to reduce bias in chest X-ray (CXR) diagnosis. We evaluated
the methods using the marginal AUC difference ($\delta$ mAUC).
</p>
<p>Results: The proposed model showed a significant decrease in bias across all
subgroups when compared to the baseline models, as evidenced by a paired T-test
(p&lt;0.0001). The $\delta$ mAUC obtained by our method were 0.0116 (95\% CI,
0.0110-0.0123), 0.2102 (95% CI, 0.2087-0.2118), and 0.1000 (95\% CI,
0.0988-0.1011) for sex, race, and age on MIDRC, and 0.0090 (95\% CI,
0.0082-0.0097) for sex and 0.0512 (95% CI, 0.0512-0.0532) for age on NIH-CXR,
respectively.
</p>
<p>Conclusion: Employing supervised contrastive learning can mitigate bias in
CXR diagnosis, addressing concerns of fairness and reliability in deep
learning-based diagnostic methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15113">Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maslov_K/0/1/0/all/0/1">Konstantin A. Maslov</a>, <a href="http://arxiv.org/find/cs/1/au:+Persello_C/0/1/0/all/0/1">Claudio Persello</a>, <a href="http://arxiv.org/find/cs/1/au:+Schellenberger_T/0/1/0/all/0/1">Thomas Schellenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1">Alfred Stein</a></p>
<p>Accurate global glacier mapping is critical for understanding climate change
impacts. It is challenged by glacier diversity, difficult-to-classify debris
and big data processing. Here we propose Glacier-VisionTransformer-U-Net
(GlaViTU), a convolutional-transformer deep learning model, and five strategies
for multitemporal global-scale glacier mapping using open satellite imagery.
Assessing the spatial, temporal and cross-sensor generalisation shows that our
best strategy achieves intersection over union &gt;0.85 on previously unobserved
images in most cases, which drops to &gt;0.75 for debris-rich areas such as
High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice.
Additionally, adding synthetic aperture radar data, namely, backscatter and
interferometric coherence, increases the accuracy in all regions where
available. The calibrated confidence for glacier extents is reported making the
predictions more reliable and interpretable. We also release a benchmark
dataset that covers 9% of glaciers worldwide. Our results support efforts
towards automated multitemporal and global glacier mapping.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15118">GeoDecoder: Empowering Multimodal Map Understanding. (arXiv:2401.15118v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1">Feng Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1">Mian Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zixian Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a></p>
<p>This paper presents GeoDecoder, a dedicated multimodal model designed for
processing geospatial information in maps. Built on the BeitGPT architecture,
GeoDecoder incorporates specialized expert modules for image and text
processing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying
base map, which inherently encompasses essential details about road and
building shapes, relative positions, and other attributes. Through the
utilization of rendering techniques, the model seamlessly integrates external
data and features such as symbol markers, drive trajectories, heatmaps, and
user-defined markers, eliminating the need for extra feature engineering. The
text module of GeoDecoder accepts various context texts and question prompts,
generating text outputs in the style of GPT. Furthermore, the GPT-based model
allows for the training and execution of multiple tasks within the same model
in an end-to-end manner. To enhance map cognition and enable GeoDecoder to
acquire knowledge about the distribution of geographic entities in Beijing, we
devised eight fundamental geospatial tasks and conducted pretraining of the
model using large-scale text-image samples. Subsequently, rapid fine-tuning was
performed on three downstream tasks, resulting in significant performance
improvements. The GeoDecoder model demonstrates a comprehensive understanding
of map elements and their associated operations, enabling efficient and
high-quality application of diverse geospatial tasks in different business
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15120">Context-driven self-supervised visual learning: Harnessing the environment as a data source. (arXiv:2401.15120v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lizhen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">James Z. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1">Wonseuk Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wyble_B/0/1/0/all/0/1">Brad Wyble</a></p>
<p>Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15164">AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations. (arXiv:2401.15164v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Devulapally_N/0/1/0/all/0/1">Naresh Kumar Devulapally</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1">Sidharth Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_S/0/1/0/all/0/1">Sreyasee Das Bhattacharjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Junsong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yu-Ping Chang</a></p>
<p>Analyzing individual emotions during group conversation is crucial in
developing intelligent agents capable of natural human-machine interaction.
While reliable emotion recognition techniques depend on different modalities
(text, audio, video), the inherent heterogeneity between these modalities and
the dynamic cross-modal interactions influenced by an individual's unique
behavioral patterns make the task of emotion recognition very challenging. This
difficulty is compounded in group settings, where the emotion and its temporal
evolution are not only influenced by the individual but also by external
contexts like audience reaction and context of the ongoing conversation. To
meet this challenge, we propose a Multimodal Attention Network that captures
cross-modal interactions at various levels of spatial abstraction by jointly
learning its interactive bunch of mode-specific Peripheral and Central
networks. The proposed MAN injects cross-modal attention via its Peripheral
key-value pairs within each layer of a mode-specific Central query network. The
resulting cross-attended mode-specific descriptors are then combined using an
Adaptive Fusion technique that enables the model to integrate the
discriminative and complementary mode-specific data patterns within an
instance-specific multimodal descriptor. Given a dialogue represented by a
sequence of utterances, the proposed AMuSE model condenses both spatial and
temporal features into two dense descriptors: speaker-level and
utterance-level. This helps not only in delivering better classification
performance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy)
in large-scale public datasets but also helps the users in understanding the
reasoning behind each emotion prediction made by the model via its Multimodal
Explainability Visualization module.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15175">Kitchen Food Waste Image Segmentation and Classification for Compost Nutrients Estimation. (arXiv:2401.15175v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_R/0/1/0/all/0/1">Raiyan Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1">Mohsena Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yueyang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Huayi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1">George Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanghui Wang</a></p>
<p>The escalating global concern over extensive food wastage necessitates
innovative solutions to foster a net-zero lifestyle and reduce emissions. The
LILA home composter presents a convenient means of recycling kitchen scraps and
daily food waste into nutrient-rich, high-quality compost. To capture the
nutritional information of the produced compost, we have created and annotated
a large high-resolution image dataset of kitchen food waste with segmentation
masks of 19 nutrition-rich categories. Leveraging this dataset, we benchmarked
four state-of-the-art semantic segmentation models on food waste segmentation,
contributing to the assessment of compost quality of Nitrogen, Phosphorus, or
Potassium. The experiments demonstrate promising results of using segmentation
models to discern food waste produced in our daily lives. Based on the
experiments, SegFormer, utilizing MIT-B5 backbone, yields the best performance
with a mean Intersection over Union (mIoU) of 67.09. Class-based results are
also provided to facilitate further analysis of different food waste classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15204">LYT-Net: Lightweight YUV Transformer-based Network for Low-Light Image Enhancement. (arXiv:2401.15204v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brateanu_A/0/1/0/all/0/1">A. Brateanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Balmez_R/0/1/0/all/0/1">R. Balmez</a>, <a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1">A. Avram</a>, <a href="http://arxiv.org/find/cs/1/au:+Orhei_C/0/1/0/all/0/1">C. C. Orhei</a></p>
<p>In recent years, deep learning-based solutions have proven successful in the
domains of image enhancement. This paper introduces LYT-Net, or Lightweight YUV
Transformer-based Network, as a novel approach for low-light image enhancement.
The proposed architecture, distinct from conventional Retinex-based models,
leverages the YUV color space's natural separation of luminance (Y) and
chrominance (U and V) to simplify the intricate task of disentangling light and
color information in images. By utilizing the strengths of transformers, known
for their capability to capture long-range dependencies, LYT-Net ensures a
comprehensive contextual understanding of the image while maintaining reduced
model complexity. By employing a novel hybrid loss function, our proposed
method achieves state-of-the-art results on low-light image enhancement
datasets, all while being considerably more compact than its counterparts. The
source code and pre-trained models are available at
https://github.com/albrateanu/LYT-Net
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15223">Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis. (arXiv:2401.15223v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingshi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Grujicic_D/0/1/0/all/0/1">Dusan Grujicic</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeger_S/0/1/0/all/0/1">Steven De Saeger</a>, <a href="http://arxiv.org/find/cs/1/au:+Heremans_S/0/1/0/all/0/1">Stien Heremans</a>, <a href="http://arxiv.org/find/cs/1/au:+Somers_B/0/1/0/all/0/1">Ben Somers</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew B. Blaschko</a></p>
<p>In recent years, machine learning has become crucial in remote sensing
analysis, particularly in the domain of Land-use/Land-cover (LULC). The synergy
of machine learning and satellite imagery analysis has demonstrated significant
productivity in this field, as evidenced by several studies. A notable
challenge within this area is the semantic segmentation mapping of land usage
over extensive territories, where the accessibility of accurate land-use data
and the reliability of ground truth land-use labels pose significant
difficulties. For example, providing a detailed and accurate pixel-wise labeled
dataset of the Flanders region, a first-level administrative division of
Belgium, can be particularly insightful. Yet there is a notable lack of
regulated, formalized datasets and workflows for such studies in many regions
globally. This paper introduces a comprehensive approach to addressing these
gaps. We present a densely labeled ground truth map of Flanders paired with
Sentinel-2 satellite imagery. Our methodology includes a formalized dataset
division and sampling method, utilizing the topographic map layout
'Kaartbladversnijdingen,' and a detailed semantic segmentation model training
pipeline. Preliminary benchmarking results are also provided to demonstrate the
efficacy of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15235">CascadedGaze: Efficiency in Global Context Extraction for Image Restoration. (arXiv:2401.15235v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ghasemabadi_A/0/1/0/all/0/1">Amirhosein Ghasemabadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Salameh_M/0/1/0/all/0/1">Mohammad Salameh</a>, <a href="http://arxiv.org/find/eess/1/au:+Janjua_M/0/1/0/all/0/1">Muhammad Kamran Janjua</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1">Chunhua Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_F/0/1/0/all/0/1">Fengyu Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Niu_D/0/1/0/all/0/1">Di Niu</a></p>
<p>Image restoration tasks traditionally rely on convolutional neural networks.
However, given the local nature of the convolutional operator, they struggle to
capture global information. The promise of attention mechanisms in Transformers
is to circumvent this problem, but it comes at the cost of intensive
computational overhead. Many recent studies in image restoration have focused
on solving the challenge of balancing performance and computational cost via
Transformer variants. In this paper, we present CascadedGaze Network (CGNet),
an encoder-decoder architecture that employs Global Context Extractor (GCE), a
novel and efficient way to capture global information for image restoration.
The GCE module leverages small kernels across convolutional layers to learn
global dependencies, without requiring self-attention. Extensive experimental
results show that our approach outperforms a range of state-of-the-art methods
on denoising benchmark datasets including both real image denoising and
synthetic image denoising, as well as on image deblurring task, while being
more computationally efficient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15236">Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones. (arXiv:2401.15236v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Motetti_B/0/1/0/all/0/1">Beatrice Alessandra Motetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Crupi_L/0/1/0/all/0/1">Luca Crupi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elshaigi_M/0/1/0/all/0/1">Mustafa Omer Mohammed Elamin Elshaigi</a>, <a href="http://arxiv.org/find/cs/1/au:+Risso_M/0/1/0/all/0/1">Matteo Risso</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1">Daniele Jahier Pagliari</a>, <a href="http://arxiv.org/find/cs/1/au:+Palossi_D/0/1/0/all/0/1">Daniele Palossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1">Alessio Burrello</a></p>
<p>Sub-10cm diameter nano-drones are gaining momentum thanks to their
applicability in scenarios prevented to bigger flying drones, such as in narrow
environments and close to humans. However, their tiny form factor also brings
their major drawback: ultra-constrained memory and processors for the onboard
execution of their perception pipelines. Therefore, lightweight deep
learning-based approaches are becoming increasingly popular, stressing how
computational efficiency and energy-saving are paramount as they can make the
difference between a fully working closed-loop system and a failing one. In
this work, to maximize the exploitation of the ultra-limited resources aboard
nano-drones, we present a novel adaptive deep learning-based mechanism for the
efficient execution of a vision-based human pose estimation task. We leverage
two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different
regression performance vs. computational costs trade-offs. By combining these
CNNs with three novel adaptation strategies based on the output's temporal
consistency and on auxiliary tasks to swap the CNN being executed proactively,
we present six different systems. On a real-world dataset and the actual
nano-drone hardware, our best-performing system, compared to executing only the
bigger and most accurate SoA model, shows 28% latency reduction while keeping
the same mean absolute error (MAE), 3% MAE reduction while being iso-latency,
and the absolute peak performance, i.e., 6% better than SoA model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15261">Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes. (arXiv:2401.15261v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Diandian Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1">Deng-Ping Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tongyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1">Christos Sakaridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>The estimation of implicit cross-frame correspondences and the high
computational cost have long been major challenges in video semantic
segmentation (VSS) for driving scenes. Prior works utilize keyframes, feature
propagation, or cross-frame attention to address these issues. By contrast, we
are the first to harness vanishing point (VP) priors for more effective
segmentation. Intuitively, objects near VPs (i.e., away from the vehicle) are
less discernible. Moreover, they tend to move radially away from the VP over
time in the usual case of a forward-facing camera, a straight road, and linear
forward motion of the vehicle. Our novel, efficient network for VSS, named
VPSeg, incorporates two modules that utilize exactly this pair of static and
dynamic VP priors: sparse-to-dense feature mining (DenseVP) and VP-guided
motion fusion (MotionVP). MotionVP employs VP-guided motion estimation to
establish explicit correspondences across frames and help attend to the most
relevant features from neighboring frames, while DenseVP enhances weak dynamic
features in distant regions around VPs. These modules operate within a
context-detail framework, which separates contextual features from
high-resolution local features at different input resolutions to reduce
computational costs. Contextual and local features are integrated through
contextualized motion attention (CMA) for the final prediction. Extensive
experiments on two popular driving segmentation benchmarks, Cityscapes and
ACDC, demonstrate that VPSeg outperforms previous SOTA methods, with only
modest computational overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15266">SAM-based instance segmentation models for the automation of masonry crack detection. (arXiv:2401.15266v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zehao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Lovell_L/0/1/0/all/0/1">Lucy Lovell</a>, <a href="http://arxiv.org/find/cs/1/au:+Faramarzi_A/0/1/0/all/0/1">Asaad Faramarzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ninic_J/0/1/0/all/0/1">Jelena Ninic</a></p>
<p>Automating visual inspection for capturing defects based on civil structures
appearance is crucial due to its currently labour-intensive and time-consuming
nature. An important aspect of automated inspection is image acquisition, which
is rapid and cost-effective considering the pervasive developments in both
software and hardware computing in recent years. Previous studies largely
focused on concrete and asphalt, with less attention to masonry cracks. The
latter also lacks publicly available datasets. In this paper, we first present
a corresponding data set for instance segmentation with 1,300 annotated images
(640 pixels x 640 pixels), named as MCrack1300, covering bricks, broken bricks,
and cracks. We then test several leading algorithms for benchmarking, including
the latest large-scale model, the prompt-based Segment Anything Model (SAM). We
fine-tune the encoder using Low-Rank Adaptation (LoRA) and proposed two novel
methods for automation of SAM execution. The first method involves abandoning
the prompt encoder and connecting the SAM encoder to other decoders, while the
second method introduces a learnable self-generating prompter. In order to
ensure the seamless integration of the two proposed methods with SAM encoder
section, we redesign the feature extractor. Both proposed methods exceed
state-of-the-art performance, surpassing the best benchmark by approximately 3%
for all classes and around 6% for cracks specifically. Based on successful
detection, we propose a method based on a monocular camera and the Hough Line
Transform to automatically transform images into orthographic projection maps.
By incorporating known real sizes of brick units, we accurately estimate crack
dimensions, with the results differing by less than 10% from those obtained by
laser scanning. Overall, we address important research gaps in automated
masonry crack detection and size estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15275">Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks. (arXiv:2401.15275v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuliang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a></p>
<p>Transformer neural networks are increasingly replacing prior architectures in
a wide range of applications in different data modalities. The increasing size
and computational demands of fine-tuning large pre-trained transformer neural
networks pose significant challenges for the widespread adoption of these
models for applications that demand on-edge computing. To tackle this
challenge, continual learning (CL) emerges as a solution by facilitating the
transfer of knowledge across tasks that arrive sequentially for an autonomously
learning agent. However, current CL methods mainly focus on learning tasks that
are exclusively vision-based or language-based. We propose a transformer-based
CL framework focusing on learning tasks that involve both vision and language,
known as Vision-and-Language (VaL) tasks. Due to the success of transformers in
other modalities, our architecture has the potential to be used in multimodal
learning settings. In our framework, we benefit from introducing extra
parameters to a base transformer to specialize the network for each task. As a
result, we enable dynamic model expansion to learn several tasks in a sequence.
We also use knowledge distillation to benefit from relevant past experiences to
learn the current task more efficiently. Our proposed method, Task Attentive
Multimodal Continual Learning (TAM-CL), allows for the exchange of information
between tasks while mitigating the problem of catastrophic forgetting. Notably,
our approach is scalable, incurring minimal memory and time overhead. TAM-CL
achieves state-of-the-art (SOTA) performance on challenging multimodal tasks
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15282">GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis. (arXiv:2401.15282v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jing Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Moyun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1">Kuo Feng Hung</a></p>
<p>Detecting glass regions is a challenging task due to the ambiguity of their
transparency and reflection properties. These transparent glasses share the
visual appearance of both transmitted arbitrary background scenes and reflected
objects, thus having no fixed patterns.Recent visual foundation models, which
are trained on vast amounts of data, have manifested stunning performance in
terms of image perception and image generation. To segment glass surfaces with
higher accuracy, we make full use of two visual foundation models: Segment
Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass
surface segmentor named GEM, which only consists of a SAM backbone, a simple
feature pyramid, a discerning query selection module, and a mask decoder. The
discerning query selection can adaptively identify glass surface features,
assigning them as initialized queries in the mask decoder. We also propose a
Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed
S-GSD via diffusion model with four different scales, which contain 1x, 5x,
10x, and 20x of the original real data size. This dataset is a feasible source
for transfer learning. The scale of synthetic data has positive impacts on
transfer learning, while the improvement will gradually saturate as the amount
of data increases. Extensive experiments demonstrate that GEM achieves a new
state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets
are available at: https://github.com/isbrycee/GEM-Glass-Segmentor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15287">Applications of Tao General Difference in Discrete Domain. (arXiv:2401.15287v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1">Linmi Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruiyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Donglai Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1">Wu Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Feilong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jingmao Cui</a></p>
<p>Numerical difference computation is one of the cores and indispensable in the
modern digital era. Tao general difference (TGD) is a novel theory and approach
to difference computation for discrete sequences and arrays in multidimensional
space. Built on the solid theoretical foundation of the general difference in a
finite interval, the TGD operators demonstrate exceptional signal processing
capabilities in real-world applications. A novel smoothness property of a
sequence is defined on the first- and second TGD. This property is used to
denoise one-dimensional signals, where the noise is the non-smooth points in
the sequence. Meanwhile, the center of the gradient in a finite interval can be
accurately location via TGD calculation. This solves a traditional challenge in
computer vision, which is the precise localization of image edges with noise
robustness. Furthermore, the power of TGD operators extends to spatio-temporal
edge detection in three-dimensional arrays, enabling the identification of
kinetic edges in video data. These diverse applications highlight the
properties of TGD in discrete domain and the significant promise of TGD for the
computation across signal processing, image analysis, and video analytic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15288">STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics. (arXiv:2401.15288v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vakhniuk_V/0/1/0/all/0/1">Volodymyr Vakhniuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1">Ayush Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Ragini Gupta</a></p>
<p>We propose an efficient cross-cameras surveillance system called,STAC, that
leverages spatio-temporal associations between multiple cameras to provide
real-time analytics and inference under constrained network environments. STAC
is built using the proposed omni-scale feature learning people reidentification
(reid) algorithm that allows accurate detection, tracking and re-identification
of people across cameras using the spatio-temporal characteristics of video
frames. We integrate STAC with frame filtering and state-of-the-art compression
for streaming technique (that is, ffmpeg libx264 codec) to remove redundant
information from cross-camera frames. This helps in optimizing the cost of
video transmission as well as compute/processing, while maintaining high
accuracy for real-time query inference. The introduction of AICity Challenge
2023 Data [1] by NVIDIA has allowed exploration of systems utilizing
multi-camera people tracking algorithms. We evaluate the performance of STAC
using this dataset to measure the accuracy metrics and inference rate for reid.
Additionally, we quantify the reduction in video streams achieved through frame
filtering and compression using FFmpeg compared to the raw camera streams. For
completeness, we make available our repository to reproduce the results,
available at https://github.com/VolodymyrVakhniuk/CS444_Final_Project.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15293">SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ataiefard_F/0/1/0/all/0/1">Foozhan Ataiefard</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1">Walid Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1">Habib Hajimolahoseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Asani_S/0/1/0/all/0/1">Saina Asani</a>, <a href="http://arxiv.org/find/cs/1/au:+Javadi_F/0/1/0/all/0/1">Farnoosh Javadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1">Mohammad Hassanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Awad_O/0/1/0/all/0/1">Omar Mohamed Awad</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1">Austin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Vision transformers are known to be more computationally and data-intensive
than CNN models. These transformer models such as ViT, require all the input
image tokens to learn the relationship among them. However, many of these
tokens are not informative and may contain irrelevant information such as
unrelated background or unimportant scenery. These tokens are overlooked by the
multi-head self-attention (MHSA), resulting in many redundant and unnecessary
computations in MHSA and the feed-forward network (FFN). In this work, we
propose a method to optimize the amount of unnecessary interactions between
unimportant tokens by separating and sending them through a different low-cost
computational path. Our method does not add any parameters to the ViT model and
aims to find the best trade-off between training throughput and achieving a 0%
loss in the Top-1 accuracy of the final model. Our experimental results on
training ViT-small from scratch show that SkipViT is capable of effectively
dropping 55% of the tokens while gaining more than 13% training throughput and
maintaining classification accuracy at the level of the baseline model on
Huawei Ascend910A.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15296">A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions. (arXiv:2401.15296v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1">Haocong Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Chunyan Miao</a></p>
<p>Person re-identification via 3D skeletons is an important emerging research
area that triggers great interest in the pattern recognition community. With
distinctive advantages for many application scenarios, a great diversity of 3D
skeleton based person re-identification (SRID) methods have been proposed in
recent years, effectively addressing prominent problems in skeleton modeling
and feature learning. Despite recent advances, to the best of our knowledge,
little effort has been made to comprehensively summarize these studies and
their challenges. In this paper, we attempt to fill this gap by providing a
systematic survey on current SRID approaches, model designs, challenges, and
future directions. Specifically, we first formulate the SRID problem, and
propose a taxonomy of SRID research with a summary of benchmark datasets,
commonly-used model architectures, and an analytical review of different
methods' characteristics. Then, we elaborate on the design principles of SRID
models from multiple aspects to offer key insights for model improvement.
Finally, we identify critical challenges confronting current studies and
discuss several promising directions for future research of SRID.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15307">ParaTransCNN: Parallelized TransCNN Encoder for Medical Image Segmentation. (arXiv:2401.15307v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1">Hongkun Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jing Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Duan_Y/0/1/0/all/0/1">Yuping Duan</a></p>
<p>The convolutional neural network-based methods have become more and more
popular for medical image segmentation due to their outstanding performance.
However, they struggle with capturing long-range dependencies, which are
essential for accurately modeling global contextual correlations. Thanks to the
ability to model long-range dependencies by expanding the receptive field, the
transformer-based methods have gained prominence. Inspired by this, we propose
an advanced 2D feature extraction method by combining the convolutional neural
network and Transformer architectures. More specifically, we introduce a
parallelized encoder structure, where one branch uses ResNet to extract local
information from images, while the other branch uses Transformer to extract
global information. Furthermore, we integrate pyramid structures into the
Transformer to extract global information at varying resolutions, especially in
intensive prediction tasks. To efficiently utilize the different information in
the parallelized encoder at the decoder stage, we use a channel attention
module to merge the features of the encoder and propagate them through skip
connections and bottlenecks. Intensive numerical experiments are performed on
both aortic vessel tree, cardiac, and multi-organ datasets. By comparing with
state-of-the-art medical image segmentation methods, our method is shown with
better segmentation accuracy, especially on small organs. The code is publicly
available on https://github.com/HongkunSun/ParaTransCNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15313">Multi-Robot Relative Pose Estimation in SE(2) with Observability Analysis: A Comparison of Extended Kalman Filtering and Robust Pose Graph Optimization. (arXiv:2401.15313v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1">Kihoon Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sim_H/0/1/0/all/0/1">Hyunjae Sim</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1">Seungwon Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yonghee Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jae Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kwang-Ki K. Kim</a></p>
<p>In this paper, we consider multi-robot localization problems with focus on
cooperative localization and observability analysis of relative pose
estimation. For cooperative localization, there is extra information available
to each robot via communication network and message passing. If odometry data
of a target robot can be transmitted to the ego-robot then the observability of
their relative pose estimation can be achieved by range-only or bearing-only
measurements provided both of their linear velocities are non-zero. If odometry
data of a target robot is not directly transmitted but estimated by the
ego-robot then there must be both range and bearing measurements to guarantee
the observability of relative pose estimation. For ROS/Gazebo simulations, we
consider four different sensing and communication structures in which extended
Kalman filtering (EKF) and pose graph optimization (PGO) estimation with
different robust loss functions (filtering and smoothing with different batch
sizes of sliding window) are compared in terms of estimation accuracy. For
hardware experiments, two Turtlebot3 equipped with UWB modules are used for
real-world inter-robot relative pose estimation, in which both EKF and PGO are
applied and compared.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15318">Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yintong Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Ying Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1">Zeshun Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1">Tianjia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hongzhi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a></p>
<p>We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15319">You Only Look Bottom-Up for Monocular 3D Object Detection. (arXiv:2401.15319v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1">Kaixin Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dingyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Dingkang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongcheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dikubab_W/0/1/0/all/0/1">Wondimu Dikubab</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jianwei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Monocular 3D Object Detection is an essential task for autonomous driving.
Meanwhile, accurate 3D object detection from pure images is very challenging
due to the loss of depth information. Most existing image-based methods infer
objects' location in 3D space based on their 2D sizes on the image plane, which
usually ignores the intrinsic position clues from images, leading to
unsatisfactory performances. Motivated by the fact that humans could leverage
the bottom-up positional clues to locate objects in 3D space from a single
image, in this paper, we explore the position modeling from the image feature
column and propose a new method named You Only Look Bottum-Up (YOLOBU).
Specifically, our YOLOBU leverages Column-based Cross Attention to determine
how much a pixel contributes to pixels above it. Next, the Row-based Reverse
Cumulative Sum (RRCS) is introduced to build the connections of pixels in the
bottom-up direction. Our YOLOBU fully explores the position clues for monocular
3D detection via building the relationship of pixels from the bottom-up way.
Extensive experiments on the KITTI dataset demonstrate the effectiveness and
superiority of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15335">L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Ping Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingchuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingfu Zhang</a></p>
<p>In the rapidly evolving field of machine learning, adversarial attacks
present a significant challenge to model robustness and security.
Decision-based attacks, which only require feedback on the decision of a model
rather than detailed probabilities or scores, are particularly insidious and
difficult to defend against. This work introduces L-AutoDA (Large Language
Model-based Automated Decision-based Adversarial Attacks), a novel approach
leveraging the generative capabilities of Large Language Models (LLMs) to
automate the design of these attacks. By iteratively interacting with LLMs in
an evolutionary framework, L-AutoDA automatically designs competitive attack
algorithms efficiently without much human effort. We demonstrate the efficacy
of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline
methods in both success rate and computational efficiency. Our findings
underscore the potential of language models as tools for adversarial attack
generation and highlight new avenues for the development of robust AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15348">AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model. (arXiv:2401.15348v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Beijia Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yuefan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shuai_Q/0/1/0/all/0/1">Qing Shuai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaowei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Youyi Zheng</a></p>
<p>Recent communities have seen significant progress in building photo-realistic
animatable avatars from sparse multi-view videos. However, current workflows
struggle to render realistic garment dynamics for loose-fitting characters as
they predominantly rely on naked body models for human modeling while leaving
the garment part un-modeled. This is mainly due to that the deformations
yielded by loose garments are highly non-rigid, and capturing such deformations
often requires dense views as supervision. In this paper, we introduce
AniDress, a novel method for generating animatable human avatars in loose
clothes using very sparse multi-view videos (4-8 in our setting). To allow the
capturing and appearance learning of loose garments in such a situation, we
employ a virtual bone-based garment rigging model obtained from physics-based
simulation data. Such a model allows us to capture and render complex garment
dynamics through a set of low-dimensional bone transformations. Technically, we
develop a novel method for estimating temporal coherent garment dynamics from a
sparse multi-view video. To build a realistic rendering for unseen garment
status using coarse estimations, a pose-driven deformable neural radiance field
conditioned on both body and garment motions is introduced, providing explicit
control of both parts. At test time, the new garment poses can be captured from
unseen situations, derived from a physics-based or neural network-based
simulator to drive unseen garment dynamics. To evaluate our approach, we create
a multi-view dataset that captures loose-dressed performers with diverse
motions. Experiments show that our method is able to render natural garment
dynamics that deviate highly from the body and generalize well to both unseen
views and poses, surpassing the performance of existing methods. The code and
data will be publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15354">DeepGI: An Automated Approach for Gastrointestinal Tract Segmentation in MRI Scans. (arXiv:2401.15354v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Ye Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1">Yulu Gong</a>, <a href="http://arxiv.org/find/eess/1/au:+Cui_D/0/1/0/all/0/1">Dongji Cui</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xinrui Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1">Xinyu Shen</a></p>
<p>Gastrointestinal (GI) tract cancers pose a global health challenge, demanding
precise radiotherapy planning for optimal treatment outcomes. This paper
introduces a cutting-edge approach to automate the segmentation of GI tract
regions in magnetic resonance imaging (MRI) scans. Leveraging advanced deep
learning architectures, the proposed model integrates Inception-V4 for initial
classification, UNet++ with a VGG19 encoder for 2.5D data, and Edge UNet for
grayscale data segmentation. Meticulous data preprocessing, including
innovative 2.5D processing, is employed to enhance adaptability, robustness,
and accuracy.
</p>
<p>This work addresses the manual and time-consuming segmentation process in
current radiotherapy planning, presenting a unified model that captures
intricate anatomical details. The integration of diverse architectures, each
specializing in unique aspects of the segmentation task, signifies a novel and
comprehensive solution. This model emerges as an efficient and accurate tool
for clinicians, marking a significant advancement in the field of GI tract
image segmentation for radiotherapy planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15362">Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval. (arXiv:2401.15362v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1">Ayush Dubey</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1">Shiv Ram Dubey</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Satish Kumar Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1">Wei-Ta Chu</a></p>
<p>Unsupervised image retrieval aims to learn the important visual
characteristics without any given level to retrieve the similar images for a
given query image. The Convolutional Neural Network (CNN)-based approaches have
been extensively exploited with self-supervised contrastive learning for image
hashing. However, the existing approaches suffer due to lack of effective
utilization of global features by CNNs and biased-ness created by false
negative pairs in the contrastive learning. In this paper, we propose a
TransClippedCLR model by encoding the global context of an image using
Transformer having local context through patch based processing, by generating
the hash codes through product quantization and by avoiding the potential false
negative pairs through clipped contrastive learning. The proposed model is
tested with superior performance for unsupervised image retrieval on benchmark
datasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent
state-of-the-art deep models. The results using the proposed clipped
contrastive learning are greatly improved on all datasets as compared to same
backbone network with vanilla contrastive learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15365">An open dataset for oracle bone script recognition and decipherment. (arXiv:2401.15365v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaile Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1">Jinpeng Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Haisu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1">Zhebin Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lianwen Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Oracle Bone Script (OBS), one of the earliest known forms of ancient Chinese
writing, holds invaluable insights into the humanities and geography of the
Shang Dynasty, dating back 3,000 years. The immense historical and cultural
significance of these writings cannot be overstated. However, the passage of
time has obscured much of their meaning, presenting a significant challenge in
deciphering these ancient texts. With the advent of Artificial Intelligence
(AI), employing AI to assist in interpreting OBS has become a feasible option.
Yet, progress in this area has been hindered by a lack of high-quality
datasets. To address this issue, this paper details the creation of the
HUST-OBS dataset. This dataset encompasses 77,064 images of 1,588 individual
deciphered scripts and 62,989 images of 9,411 undeciphered characters, with a
total of 140,053 images, compiled from diverse sources. Additionally, all
images and labels have been reviewed and corrected by experts in oracle bone
studies. The hope is that this dataset could inspire and assist future research
in deciphering those unknown OBS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15366">Face to Cartoon Incremental Super-Resolution using Knowledge Distillation. (arXiv:2401.15366v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Devkatte_T/0/1/0/all/0/1">Trinetra Devkatte</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1">Shiv Ram Dubey</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Satish Kumar Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadid_A/0/1/0/all/0/1">Abdenour Hadid</a></p>
<p>Facial super-resolution/hallucination is an important area of research that
seeks to enhance low-resolution facial images for a variety of applications.
While Generative Adversarial Networks (GANs) have shown promise in this area,
their ability to adapt to new, unseen data remains a challenge. This paper
addresses this problem by proposing an incremental super-resolution using GANs
with knowledge distillation (ISR-KD) for face to cartoon. Previous research in
this area has not investigated incremental learning, which is critical for
real-world applications where new data is continually being generated. The
proposed ISR-KD aims to develop a novel unified framework for facial
super-resolution that can handle different settings, including different types
of faces such as cartoon face and various levels of detail. To achieve this, a
GAN-based super-resolution network was pre-trained on the CelebA dataset and
then incrementally trained on the iCartoonFace dataset, using knowledge
distillation to retain performance on the CelebA test set while improving the
performance on iCartoonFace test set. Our experiments demonstrate the
effectiveness of knowledge distillation in incrementally adding capability to
the model for cartoon face super-resolution while retaining the learned
knowledge for facial hallucination tasks in GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15380">Open-RadVLAD: Fast and Robust Radar Place Recognition. (arXiv:2401.15380v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1">Matthew Gadd</a>, <a href="http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1">Paul Newman</a></p>
<p>Radar place recognition often involves encoding a live scan as a vector and
matching this vector to a database in order to recognise that the vehicle is in
a location that it has visited before. Radar is inherently robust to lighting
or weather conditions, but place recognition with this sensor is still affected
by: (1) viewpoint variation, i.e. translation and rotation, (2) sensor
artefacts or "noises". For 360-degree scanning radar, rotation is readily dealt
with by in some way aggregating across azimuths. Also, we argue in this work
that it is more critical to deal with the richness of representation and sensor
noises than it is to deal with translational invariance - particularly in urban
driving where vehicles predominantly follow the same lane when repeating a
route. In our method, for computational efficiency, we use only the polar
representation. For partial translation invariance and robustness to signal
noise, we use only a one-dimensional Fourier Transform along radial returns. We
also achieve rotational invariance and a very discriminative descriptor space
by building a vector of locally aggregated descriptors. Our method is more
comprehensively tested than all prior radar place recognition work - over an
exhaustive combination of all 870 pairs of trajectories from 30 Oxford Radar
RobotCar Dataset sequences (each approximately 10 km). Code and detailed
results are provided at github.com/mttgdd/open-radvlad, as an open
implementation and benchmark for future work in this area. We achieve a median
of 91.52% in Recall@1, outstripping the 69.55% for the only other open
implementation, RaPlace, and at a fraction of its computational cost (relying
on fewer integral transforms e.g. Radon, Fourier, and inverse Fourier).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15414">An Implicit Physical Face Model Driven by Expression and Style. (arXiv:2401.15414v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lingchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zoss_G/0/1/0/all/0/1">Gaspard Zoss</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1">Prashanth Chandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Gotardo_P/0/1/0/all/0/1">Paulo Gotardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1">Markus Gross</a>, <a href="http://arxiv.org/find/cs/1/au:+Solenthaler_B/0/1/0/all/0/1">Barbara Solenthaler</a>, <a href="http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1">Eftychios Sifakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradley_D/0/1/0/all/0/1">Derek Bradley</a></p>
<p>3D facial animation is often produced by manipulating facial deformation
models (or rigs), that are traditionally parameterized by expression controls.
A key component that is usually overlooked is expression 'style', as in, how a
particular expression is performed. Although it is common to define a semantic
basis of expressions that characters can perform, most characters perform each
expression in their own style. To date, style is usually entangled with the
expression, and it is not possible to transfer the style of one character to
another when considering facial animation. We present a new face model, based
on a data-driven implicit neural physics model, that can be driven by both
expression and style separately. At the core, we present a framework for
learning implicit physics-based actuations for multiple subjects
simultaneously, trained on a few arbitrary performance capture sequences from a
small set of identities. Once trained, our method allows generalized
physics-based facial animation for any of the trained identities, extending to
unseen performances. Furthermore, it grants control over the animation style,
enabling style transfer from one character to another or blending styles of
different characters. Lastly, as a physics-based model, it is capable of
synthesizing physical effects, such as collision handling, setting our method
apart from conventional approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15422">A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yue Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chenlu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuan Wu</a></p>
<p>Large models, encompassing large language and diffusion models, have shown
exceptional promise in approximating human-level intelligence, garnering
significant interest from both academic and industrial spheres. However, the
training of these large models necessitates vast quantities of high-quality
data, and with continuous updates to these models, the existing reservoir of
high-quality data may soon be depleted. This challenge has catalyzed a surge in
research focused on data augmentation methods. Leveraging large models, these
data augmentation techniques have outperformed traditional approaches. This
paper offers an exhaustive review of large model-driven data augmentation
methods, adopting a comprehensive perspective. We begin by establishing a
classification of relevant studies into three main categories: image
augmentation, text augmentation, and paired data augmentation. Following this,
we delve into various data post-processing techniques pertinent to large
model-based data augmentation. Our discussion then expands to encompass the
array of applications for these data augmentation methods within natural
language processing, computer vision, and audio signal processing. We proceed
to evaluate the successes and limitations of large model-based data
augmentation across different scenarios. Concluding our review, we highlight
prospective challenges and avenues for future exploration in the field of data
augmentation. Our objective is to furnish researchers with critical insights,
ultimately contributing to the advancement of more sophisticated large models.
We consistently maintain the related open-source materials at:
https://github.com/MLGroup-JLU/LLM-data-aug-survey.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15434">Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI. (arXiv:2401.15434v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jingyun Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1">Yading Yuan</a></p>
<p>Federated Learning (FL) enables collaborative model training among medical
centers without sharing private data. However, traditional FL risks on server
failures and suboptimal performance on local data due to the nature of
centralized model aggregation. To address these issues, we present Gossip
Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for
direct peer-to-peer communication. In addition, GML encourages each site to
optimize its local model through mutual learning to account for data variations
among different sites. For the task of tumor segmentation using 146 cases from
four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed
local models and achieved similar performance as FedAvg with only 25%
communication overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15448">A Systematic Review of Available Datasets in Additive Manufacturing. (arXiv:2401.15448v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mileo_A/0/1/0/all/0/1">Alessandra Mileo</a>, <a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1">Alan F. Smeaton</a></p>
<p>In-situ monitoring incorporating data from visual and other sensor
technologies, allows the collection of extensive datasets during the Additive
Manufacturing (AM) process. These datasets have potential for determining the
quality of the manufactured output and the detection of defects through the use
of Machine Learning during the manufacturing process. Open and annotated
datasets derived from AM processes are necessary for the machine learning
community to address this opportunity, which creates difficulties in the
application of computer vision-related machine learning in AM. This systematic
review investigates the availability of open image-based datasets originating
from AM processes that align with a number of pre-defined selection criteria.
The review identifies existing gaps among the current image-based datasets in
the domain of AM, and points to the need for greater availability of open
datasets in order to allow quality assessment and defect detection during
additive manufacturing, to develop.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15455">New Foggy Object Detecting Model. (arXiv:2401.15455v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banavathu_R/0/1/0/all/0/1">Rahul Banavathu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sree_M/0/1/0/all/0/1">Modem Veda Sree</a>, <a href="http://arxiv.org/find/cs/1/au:+Sri_B/0/1/0/all/0/1">Bollina Kavya Sri</a>, <a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1">Suddhasil De</a></p>
<p>Object detection in reduced visibility has become a prominent research area.
The existing techniques are not accurate enough in recognizing objects under
such circumstances. This paper introduces a new foggy object detection method
through a two-staged architecture of region identification from input images
and detecting objects in such regions. The paper confirms notable improvements
of the proposed method's accuracy and detection time over existing techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15458">A New Method for Vehicle Logo Recognition Based on Swin Transformer. (arXiv:2401.15458v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Doudou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1">Jianli Xiao</a></p>
<p>Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big
data analysis to monitor real-time traffic conditions, aiming to improve
traffic efficiency and safety. Accurate vehicle recognition is crucial in this
process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables
effective management and monitoring by distinguishing vehicles on the road.
Convolutional Neural Networks (CNNs) have made impressive strides in VLR
research. However, achieving higher performance demands significant time and
computational resources for training. Recently, the rise of Transformer models
has brought new opportunities to VLR. Swin Transformer, with its efficient
computation and global feature modeling capabilities, outperforms CNNs under
challenging conditions. In this paper, we implement real-time VLR using Swin
Transformer and fine-tune it for optimal performance. Extensive experiments
conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD)
demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%,
respectively. Additionally, the use of a transfer learning strategy enables our
method to be on par with state-of-the-art VLR methods. These findings affirm
the superiority of our approach over existing methods. Future research can
explore and optimize the application of the Swin Transformer in other vehicle
vision recognition tasks to drive advancements in ITS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2002.05907">A Survey on 3D Skeleton-Based Action Recognition Using Learning Method. (arXiv:2002.05907v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1">Bin Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Runwei Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a></p>
<p>3D skeleton-based action recognition (3D SAR) has gained significant
attention within the computer vision community, owing to the inherent
advantages offered by skeleton data. As a result, a plethora of impressive
works, including those based on conventional handcrafted features and learned
feature extraction methods, have been conducted over the years. However, prior
surveys on action recognition have primarily focused on video or RGB
data-dominated approaches, with limited coverage of reviews related to skeleton
data. Furthermore, despite the extensive application of deep learning methods
in this field, there has been a notable absence of research that provides an
introductory or comprehensive review from the perspective of deep learning
architectures. To address these limitations, this survey first underscores the
importance of action recognition and emphasizes the significance of 3D skeleton
data as a valuable modality. Subsequently, we provide a comprehensive
introduction to mainstream action recognition techniques based on four
fundamental deep architectures, i.e., Recurrent Neural Networks (RNNs),
Convolutional Neural Networks (CNNs), Graph Convolutional Network (GCN), and
Transformers. All methods with the corresponding architectures are then
presented in a data-driven manner with detailed discussion. Finally, we offer
insights into the current largest 3D skeleton dataset, NTU-RGB+D, and its new
edition, NTU-RGB+D 120, along with an overview of several top-performing
algorithms on these datasets. To the best of our knowledge, this research
represents the first comprehensive discussion of deep learning-based action
recognition using 3D skeleton data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2004.13324">Learning Feature Descriptors using Camera Pose Supervision. (arXiv:2004.13324v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaowei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1">Bharath Hariharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1">Noah Snavely</a></p>
<p>Recent research on learned visual descriptors has shown promising
improvements in correspondence estimation, a key component of many 3D vision
tasks. However, existing descriptor learning frameworks typically require
ground-truth correspondences between feature points for training, which are
challenging to acquire at scale. In this paper we propose a novel
weakly-supervised framework that can learn feature descriptors solely from
relative camera poses between images. To do so, we devise both a new loss
function that exploits the epipolar constraint given by camera poses, and a new
model architecture that makes the whole pipeline differentiable and efficient.
Because we no longer need pixel-level ground-truth correspondences, our
framework opens up the possibility of training on much larger and more diverse
datasets for better and unbiased descriptors. We call the resulting descriptors
CAmera Pose Supervised, or CAPS, descriptors. Though trained with weak
supervision, CAPS descriptors outperform even prior fully-supervised
descriptors and achieve state-of-the-art performance on a variety of geometric
tasks. Project Page: https://qianqianwang68.github.io/CAPS/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.06746">Periocular Embedding Learning with Consistent Knowledge Distillation from Face. (arXiv:2012.06746v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1">Yoon Gyo Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jaewoo Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Low_C/0/1/0/all/0/1">Cheng Yaw Low</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1">Jacky Chen Long Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiong_L/0/1/0/all/0/1">Leslie Ching Ow Tiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Teoh_A/0/1/0/all/0/1">Andrew Beng Jin Teoh</a></p>
<p>Periocular biometric, the peripheral area of the ocular, is a collaborative
alternative to the face, especially when the face is occluded or masked.
However, in practice, sole periocular biometric capture the least salient
facial features, thereby lacking discriminative information, particularly in
wild environments. To address these problems, we transfer discriminatory
information from the face to support the training of a periocular network by
using knowledge distillation. Specifically, we leverage face images for
periocular embedding learning, but periocular alone is utilized for identity
identification or verification. To enhance periocular embeddings by face
effectively, we proposeConsistent Knowledge Distillation (CKD) that imposes
consistency between face and periocular networks across prediction and feature
layers. We find that imposing consistency at the prediction layer enables (1)
extraction of global discriminative relationship information from face images
and (2) effective transfer of the information from the face network to the
periocular network. Particularly, consistency regularizes the prediction units
to extract and store profound inter-class relationship information of face
images. (3) The feature layer consistency, on the other hand, makes the
periocular features robust against identity-irrelevant attributes. Overall, CKD
empowers the sole periocular network to produce robust discriminative
embeddings for periocular recognition in the wild. We theoretically and
empirically validate the core principles of the distillation mechanism in CKD,
discovering that CKD is equivalent to label smoothing with a novel
sparsity-oriented regularizer that helps the network prediction to capture the
global discriminative relationship. Extensive experiments reveal that CKD
achieves state-of-the-art results on standard periocular recognition benchmark
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.14331">Methods to integrate multinormals and compute classification measures. (arXiv:2012.14331v11 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1">Abhranil Das</a>, <a href="http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1">Wilson S Geisler</a></p>
<p>Univariate and multivariate normal probability distributions are widely used
when modeling decisions under uncertainty. Computing the performance of such
models requires integrating these distributions over specific domains, which
can vary widely across models. Besides some special cases, there exist no
general analytical expressions, standard numerical methods or software for
these integrals. Here we present mathematical results and open-source software
that provide (i) the probability in any domain of a normal in any dimensions
with any parameters, (ii) the probability density, cumulative distribution, and
inverse cumulative distribution of any function of a normal vector, (iii) the
classification errors among any number of normal distributions, the
Bayes-optimal discriminability index and relation to the operating
characteristic, (iv) dimension reduction and visualizations for such problems,
and (v) tests for how reliably these methods may be used on given data. We
demonstrate these tools with vision research applications of detecting
occluding objects in natural scenes, and detecting camouflage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.04565">Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations. (arXiv:2103.04565v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haimin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Min Xu</a></p>
<p>Deep neural networks have been successfully applied in various machine
learning tasks. However, studies show that neural networks are susceptible to
adversarial attacks. This exposes a potential threat to neural network-based
intelligent systems. We observe that the probability of the correct result
outputted by the neural network increases by applying small first-order
perturbations generated for non-predicted class labels to adversarial examples.
Based on this observation, we propose a method for counteracting adversarial
perturbations to improve adversarial robustness. In the proposed method, we
randomly select a number of class labels and generate small first-order
perturbations for these selected labels. The generated perturbations are added
together and then clamped onto a specified space. The obtained perturbation is
finally added to the adversarial example to counteract the adversarial
perturbation contained in the example. The proposed method is applied at
inference time and does not require retraining or finetuning the model. We
experimentally validate the proposed method on CIFAR-10 and CIFAR-100. The
results demonstrate that our method effectively improves the defense
performance of several transformation-based defense methods, especially against
strong adversarial examples generated using more iterations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.12435">Understanding Adversarial Robustness from Feature Maps of Convolutional Layers. (arXiv:2202.12435v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Cong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a></p>
<p>The adversarial robustness of a neural network mainly relies on two factors:
model capacity and anti-perturbation ability. In this paper, we study the
anti-perturbation ability of the network from the feature maps of convolutional
layers. Our theoretical analysis discovers that larger convolutional feature
maps before average pooling can contribute to better resistance to
perturbations, but the conclusion is not true for max pooling. It brings new
inspiration to the design of robust neural networks and urges us to apply these
findings to improve existing architectures. The proposed modifications are very
simple and only require upsampling the inputs or slightly modifying the stride
configurations of downsampling operators. We verify our approaches on several
benchmark neural network architectures, including AlexNet, VGG, RestNet18, and
PreActResNet18. Non-trivial improvements in terms of both natural accuracy and
adversarial robustness can be achieved under various attack and defense
mechanisms. The code is available at \url{https://github.com/MTandHJ/rcm}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.01327">Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mantripragada_K/0/1/0/all/0/1">Kiran Mantripragada</a>, <a href="http://arxiv.org/find/eess/1/au:+Qureshi_F/0/1/0/all/0/1">Faisal Z. Qureshi</a></p>
<p>We present a method for hyperspectral pixel {\it unmixing}. The proposed
method assumes that (1) {\it abundances} can be encoded as Dirichlet
distributions and (2) spectra of {\it endmembers} can be represented as
multivariate Normal distributions. The method solves the problem of abundance
estimation and endmember extraction within a variational autoencoder setting
where a Dirichlet bottleneck layer models the abundances, and the decoder
performs endmember extraction. The proposed method can also leverage transfer
learning paradigm, where the model is only trained on synthetic data containing
pixels that are linear combinations of one or more endmembers of interest. In
this case, we retrieve endmembers (spectra) from the United States Geological
Survey Spectral Library. The model thus trained can be subsequently used to
perform pixel unmixing on "real data" that contains a subset of the endmembers
used to generated the synthetic data. The model achieves state-of-the-art
results on several benchmarks: Cuprite, Urban Hydice and Samson. We also
present new synthetic dataset, OnTech-HSI-Syn-21, that can be used to study
hyperspectral pixel unmixing methods. We showcase the transfer learning
capabilities of the proposed model on Cuprite and OnTech-HSI-Syn-21 datasets.
In summary, the proposed method can be applied for pixel unmixing a variety of
domains, including agriculture, forestry, mineralogy, analysis of materials,
healthcare, etc. Additionally, the proposed method eschews the need for
labelled data for training by leveraging the transfer learning paradigm, where
the model is trained on synthetic data generated using the endmembers present
in the "real" data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.12476">Stable Optimization for Large Vision Model Based Deep Image Prior in Cone-Beam CT Reconstruction. (arXiv:2203.12476v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1">Minghui Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yangdi Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yingying Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1">Guangwei Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1">Qingqing Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1">Hongxiang Lin</a></p>
<p>Large Vision Model (LVM) has recently demonstrated great potential for
medical imaging tasks, potentially enabling image enhancement for sparse-view
Cone-Beam Computed Tomography (CBCT), despite requiring a substantial amount of
data for training. Meanwhile, Deep Image Prior (DIP) effectively guides an
untrained neural network to generate high-quality CBCT images without any
training data. However, the original DIP method relies on a well-defined
forward model and a large-capacity backbone network, which is notoriously
difficult to converge. In this paper, we propose a stable optimization method
for the forward-model-free, LVM-based DIP model for sparse-view CBCT. Our
approach consists of two main characteristics: (1) multi-scale perceptual loss
(MSPL) which measures the similarity of perceptual features between the
reference and output images at multiple resolutions without the need for any
forward model, and (2) a reweighting mechanism that stabilizes the iteration
trajectory of MSPL. One shot optimization is used to simultaneously and stably
reweight MSPL and optimize LVM. We evaluate our approach on two publicly
available datasets: SPARE and Walnut. The results show significant improvements
in both image quality metrics and visualization that demonstrates reduced
streak artifacts. The source code is available upon request.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11131">Heterogeneous Semantic Transfer for Multi-label Recognition with Partial Labels. (arXiv:2205.11131v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianshui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1">Tao Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingbo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yukai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhijing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>Multi-label image recognition with partial labels (MLR-PL), in which some
labels are known while others are unknown for each image, may greatly reduce
the cost of annotation and thus facilitate large-scale MLR. We find that strong
semantic correlations exist within each image and across different images, and
these correlations can help transfer the knowledge possessed by the known
labels to retrieve the unknown labels and thus improve the performance of the
MLR-PL task (see Figure 1). In this work, we propose a novel heterogeneous
semantic transfer (HST) framework that consists of two complementary transfer
modules that explore both within-image and cross-image semantic correlations to
transfer the knowledge possessed by known labels to generate pseudo labels for
the unknown labels. Specifically, an intra-image semantic transfer (IST) module
learns an image-specific label co-occurrence matrix for each image and maps the
known labels to complement the unknown labels based on these matrices.
Additionally, a cross-image transfer (CST) module learns category-specific
feature-prototype similarities and then helps complement the unknown labels
that have high degrees of similarity with the corresponding prototypes.
Finally, both the known and generated pseudo labels are used to train MLR
models. Extensive experiments conducted on the Microsoft COCO, Visual Genome,
and Pascal VOC 2007 datasets show that the proposed HST framework achieves
superior performance to that of current state-of-the-art algorithms.
Specifically, it obtains mean average precision (mAP) improvements of 1.4%,
3.3%, and 0.4% on the three datasets over the results of the best-performing
previously developed algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.13092">Dual-Perspective Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels. (arXiv:2205.13092v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1">Tao Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianshui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hefeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yukai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhijing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Liang Lin</a></p>
<p>Despite achieving impressive progress, current multi-label image recognition
(MLR) algorithms heavily depend on large-scale datasets with complete labels,
making collecting large-scale datasets extremely time-consuming and
labor-intensive. Training the multi-label image recognition models with partial
labels (MLR-PL) is an alternative way, in which merely some labels are known
while others are unknown for each image. However, current MLP-PL algorithms
rely on pre-trained image similarity models or iteratively updating the image
classification models to generate pseudo labels for the unknown labels. Thus,
they depend on a certain amount of annotations and inevitably suffer from
obvious performance drops, especially when the known label proportion is low.
To address this dilemma, we propose a dual-perspective semantic-aware
representation blending (DSRB) that blends multi-granularity category-specific
semantic representation across different images, from instance and prototype
perspective respectively, to transfer information of known labels to complement
unknown labels. Specifically, an instance-perspective representation blending
(IPRB) module is designed to blend the representations of the known labels in
an image with the representations of the corresponding unknown labels in
another image to complement these unknown labels. Meanwhile, a
prototype-perspective representation blending (PPRB) module is introduced to
learn more stable representation prototypes for each category and blends the
representation of unknown labels with the prototypes of corresponding labels,
in a location-sensitive manner, to complement these unknown labels. Extensive
experiments on the MS-COCO, Visual Genome, and Pascal VOC 2007 datasets show
that the proposed DSRB consistently outperforms current state-of-the-art
algorithms on all known label proportion settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15401">VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis. (arXiv:2205.15401v3 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Angtian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1">Adam Kortylewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a></p>
<p>The Gaussian reconstruction kernels have been proposed by Westover (1990) and
studied by the computer graphics community back in the 90s, which gives an
alternative representation of object 3D geometry from meshes and point clouds.
On the other hand, current state-of-the-art (SoTA) differentiable renderers,
Liu et al. (2019), use rasterization to collect triangles or points on each
image pixel and blend them based on the viewing distance. In this paper, we
propose VoGE, which utilizes the volumetric Gaussian reconstruction kernels as
geometric primitives. The VoGE rendering pipeline uses ray tracing to capture
the nearest primitives and blends them as mixtures based on their volume
density distributions along the rays. To efficiently render via VoGE, we
propose an approximate closeform solution for the volume density aggregation
and a coarse-to-fine rendering strategy. Finally, we provide a CUDA
implementation of VoGE, which enables real-time level rendering with a
competitive rendering speed in comparison to PyTorch3D. Quantitative and
qualitative experiment results show VoGE outperforms SoTA counterparts when
applied to various vision tasks, e.g., object pose estimation, shape/texture
fitting, and occlusion reasoning. The VoGE library and demos are available at:
https://github.com/Angtian/VoGE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02286">AugLoss: A Robust Augmentation-based Fine Tuning Methodology. (arXiv:2206.02286v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Otstot_K/0/1/0/all/0/1">Kyle Otstot</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1">Andrew Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cava_J/0/1/0/all/0/1">John Kevin Cava</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1">Lalitha Sankar</a></p>
<p>Deep Learning (DL) models achieve great successes in many domains. However,
DL models increasingly face safety and robustness concerns, including noisy
labeling in the training stage and feature distribution shifts in the testing
stage. Previous works made significant progress in addressing these problems,
but the focus has largely been on developing solutions for only one problem at
a time. For example, recent work has argued for the use of tunable robust loss
functions to mitigate label noise, and data augmentation (e.g., AugMix) to
combat distribution shifts. As a step towards addressing both problems
simultaneously, we introduce AugLoss, a simple but effective methodology that
achieves robustness against both train-time noisy labeling and test-time
feature distribution shifts by unifying data augmentation and robust loss
functions. We conduct comprehensive experiments in varied settings of
real-world dataset corruption to showcase the gains achieved by AugLoss
compared to previous state-of-the-art methods. Lastly, we hope this work will
open new directions for designing more robust and reliable DL models under
real-world corruptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.13532">Contrastive Masked Autoencoders are Stronger Vision Learners. (arXiv:2207.13532v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhicheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaojie Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chengze Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1">Qibin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Dongmei Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xiaohui Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a></p>
<p>Masked image modeling (MIM) has achieved promising results on various vision
tasks. However, the limited discriminability of learned representation
manifests there is still plenty to go for making a stronger vision learner.
Towards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new
self-supervised pre-training method for learning more comprehensive and capable
vision representations. By elaboratively unifying contrastive learning (CL) and
masked image model (MIM) through novel designs, CMAE leverages their respective
advantages and learns representations with both strong instance
discriminability and local perceptibility. Specifically, CMAE consists of two
branches where the online branch is an asymmetric encoder-decoder and the
momentum branch is a momentum updated encoder. During training, the online
encoder reconstructs original images from latent representations of masked
images to learn holistic features. The momentum encoder, fed with the full
images, enhances the feature discriminability via contrastive learning with its
online counterpart. To make CL compatible with MIM, CMAE introduces two new
components, i.e. pixel shifting for generating plausible positive views and
feature decoder for complementing features of contrastive pairs. Thanks to
these novel designs, CMAE effectively improves the representation quality and
transfer performance over its MIM counterpart. CMAE achieves the
state-of-the-art performance on highly competitive benchmarks of image
classification, semantic segmentation and object detection. Notably, CMAE-Base
achieves $85.3\%$ top-1 accuracy on ImageNet and $52.5\%$ mIoU on ADE20k,
surpassing previous best results by $0.7\%$ and $1.8\%$ respectively. The
source code is publicly accessible at
\url{https://github.com/ZhichengHuang/CMAE}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04522">HORIZON: High-Resolution Semantically Controlled Panorama Synthesis. (arXiv:2210.04522v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Kun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1">Lei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenfei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Ming Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuai Ma</a></p>
<p>Panorama synthesis endeavors to craft captivating 360-degree visual
landscapes, immersing users in the heart of virtual worlds. Nevertheless,
contemporary panoramic synthesis techniques grapple with the challenge of
semantically guiding the content generation process. Although recent
breakthroughs in visual synthesis have unlocked the potential for semantic
control in 2D flat images, a direct application of these methods to panorama
synthesis yields distorted content. In this study, we unveil an innovative
framework for generating high-resolution panoramas, adeptly addressing the
issues of spherical distortion and edge discontinuity through sophisticated
spherical modeling. Our pioneering approach empowers users with semantic
control, harnessing both image and text inputs, while concurrently streamlining
the generation of high-resolution panoramas using parallel decoding. We
rigorously evaluate our methodology on a diverse array of indoor and outdoor
datasets, establishing its superiority over recent related work, in terms of
both quantitative and qualitative performance metrics. Our research elevates
the controllability, efficiency, and fidelity of panorama synthesis to new
levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14164">No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naderi_H/0/1/0/all/0/1">Hanieh Naderi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinesh_C/0/1/0/all/0/1">Chinthaka Dinesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1">Ivan V. Bajic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1">Shohreh Kasaei</a></p>
<p>Adversarial attacks pose serious challenges for deep neural network
(DNN)-based analysis of various input signals. In the case of 3D point clouds,
methods have been developed to identify points that play a key role in network
decision, and these become crucial in generating existing adversarial attacks.
For example, a saliency map approach is a popular method for identifying
adversarial drop points, whose removal would significantly impact the network
decision. Generally, methods for identifying adversarial points rely on the
access to the DNN model itself to determine which points are critically
important for the model's decision. This paper aims to provide a novel
viewpoint on this problem, where adversarial points can be predicted without
access to the target DNN model, which is referred to as a ``no-box'' attack. To
this end, we define 14 point cloud features and use multiple linear regression
to examine whether these features can be used for adversarial point prediction,
and which combination of features is best suited for this purpose. Experiments
show that a suitable combination of features is able to predict adversarial
points of four different networks -- PointNet, PointNet++, DGCNN, and PointConv
-- significantly better than a random guess and comparable to white-box
attacks. Additionally, we show that no-box attack is transferable to unseen
models. The results also provide further insight into DNNs for point cloud
classification, by showing which features play key roles in their
decision-making process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13359">IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1">Guoyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinbao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiayi Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaochu Jin</a></p>
<p>Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently, many advanced algorithms have been
reported, but their performance deviates considerably with various IM settings.
We realize that the lack of a uniform IM benchmark is hindering the development
and usage of IAD methods in real-world applications. In addition, it is
difficult for researchers to analyze IAD algorithms without a uniform
benchmark. To solve this problem, we propose a uniform IM benchmark, for the
first time, to assess how well these algorithms perform, which includes various
levels of supervision (unsupervised versus fully supervised), learning
paradigms (few-shot, continual and noisy label), and efficiency (memory usage
and inference speed). Then, we construct a comprehensive image anomaly
detection benchmark (IM-IAD), which includes 19 algorithms on seven major
datasets with a uniform setting. Extensive experiments (17,017 total) on IM-IAD
provide in-depth insights into IAD algorithm redesign or selection. Moreover,
the proposed IM-IAD benchmark challenges existing algorithms and suggests
future research directions. To foster reproducibility and accessibility, the
source code of IM-IAD is uploaded on the website,
https://github.com/M-3LAB/IM-IAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09026">Towards Commonsense Knowledge based Fuzzy Systems for Supporting Size-Related Fine-Grained Object Detection. (arXiv:2303.09026v7 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianhua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a></p>
<p>Deep learning has become the dominating approach for object detection. To
achieve accurate fine-grained detection, one needs to employ a large enough
model and a vast amount of data annotations. In this paper, we propose a
commonsense knowledge inference module (CKIM) which leverages commonsense
knowledge to assist a lightweight deep neural network base coarse-grained
object detector to achieve accurate fine-grained detection. Specifically, we
focus on a scenario where a single image contains objects of similar categories
but varying sizes, and we establish a size-related commonsense knowledge
inference module (CKIM) that maps the coarse-grained labels produced by the DL
detector to size-related fine-grained labels. Considering that rule-based
systems are one of the popular methods of knowledge representation and
reasoning, our experiments explored two types of rule-based CKIMs, implemented
using crisp-rule and fuzzy-rule approaches, respectively. Experimental results
demonstrate that compared with baseline methods, our approach achieves accurate
fine-grained detection with a reduced amount of annotated data and smaller
model size. Our code is available at: https://github.com/ZJLAB-AMMI/CKIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14703">Exploring the Interplay Between Colorectal Cancer Subtypes Genomic Variants and Cellular Morphology: A Deep-Learning Approach. (arXiv:2303.14703v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hezi_H/0/1/0/all/0/1">Hadar Hezi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shats_D/0/1/0/all/0/1">Daniel Shats</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevich_D/0/1/0/all/0/1">Daniel Gurevich</a>, <a href="http://arxiv.org/find/cs/1/au:+Maruvka_Y/0/1/0/all/0/1">Yosef E. Maruvka</a>, <a href="http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1">Moti Freiman</a></p>
<p>Molecular subtypes of colorectal cancer (CRC) significantly influence
treatment decisions. While convolutional neural networks (CNNs) have recently
been introduced for automated CRC subtype identification using H&amp;E stained
histopathological images, the correlation between CRC subtype genomic variants
and their corresponding cellular morphology expressed by their imaging
phenotypes is yet to be fully explored. The goal of this study was to determine
such correlations by incorporating genomic variants in CNN models for CRC
subtype classification from H&amp;E images. We utilized the publicly available
TCGA-CRC-DX dataset, which comprises whole slide images from 360 CRC-diagnosed
patients (260 for training and 100 for testing). This dataset also provides
information on CRC subtype classifications and genomic variations. We trained
CNN models for CRC subtype classification that account for potential
correlation between genomic variations within CRC subtypes and their
corresponding cellular morphology patterns. We assessed the interplay between
CRC subtypes' genomic variations and cellular morphology patterns by evaluating
the CRC subtype classification accuracy of the different models in a stratified
5-fold cross-validation experimental setup using the area under the ROC curve
(AUROC) and average precision (AP) as the performance metrics. Combining the
CNN models account for variations in CIMP and SNP further improved
classification accuracy (AUROC: 0.847$\pm$0.01 vs. 0.787$\pm$0.03, p$=$0.01,
AP: 0.68$\pm$0.02 vs. 0.64$\pm$0.05).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14897">Seer: Language Instructed Video Prediction with Latent Diffusion Models. (arXiv:2303.14897v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1">Xianfan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1">Chuan Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Weirui Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaming Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a></p>
<p>Imagining the future trajectory is the key for robots to make sound planning
and successfully reach their goals. Therefore, text-conditioned video
prediction (TVP) is an essential task to facilitate general robot policy
learning. To tackle this task and empower robots with the ability to foresee
the future, we propose a sample and computation-efficient model, named
\textbf{Seer}, by inflating the pretrained text-to-image (T2I) stable diffusion
models along the temporal axis. We enhance the U-Net and language conditioning
model by incorporating computation-efficient spatial-temporal attention.
Furthermore, we introduce a novel Frame Sequential Text Decomposer module that
dissects a sentence's global instruction into temporally aligned
sub-instructions, ensuring precise integration into each frame of generation.
Our framework allows us to effectively leverage the extensive prior knowledge
embedded in pretrained T2I models across the frames. With the
adaptable-designed architecture, Seer makes it possible to generate
high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a
few layers on a small amount of data. The experimental results on Something
Something V2 (SSv2), Bridgedata and EpicKitchens-100 datasets demonstrate our
superior video prediction performance with around 480-GPU hours versus CogVideo
with over 12,480-GPU hours: achieving the 31% FVD improvement compared to the
current SOTA model on SSv2 and 83.7% average preference in the human
evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15198">Image Deblurring by Exploring In-depth Properties of Transformer. (arXiv:2303.15198v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Pengwei Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Junjun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jiayi Ma</a></p>
<p>Image deblurring continues to achieve impressive performance with the
development of generative models. Nonetheless, there still remains a
displeasing problem if one wants to improve perceptual quality and quantitative
scores of recovered image at the same time. In this study, drawing inspiration
from the research of transformer properties, we introduce the pretrained
transformers to address this problem. In particular, we leverage deep features
extracted from a pretrained vision transformer (ViT) to encourage recovered
images to be sharp without sacrificing the performance measured by the
quantitative metrics. The pretrained transformer can capture the global
topological relations (i.e., self-similarity) of image, and we observe that the
captured topological relations about the sharp image will change when blur
occurs. By comparing the transformer features between recovered image and
target one, the pretrained transformer provides high-resolution blur-sensitive
semantic information, which is critical in measuring the sharpness of the
deblurred image. On the basis of the advantages, we present two types of novel
perceptual losses to guide image deblurring. One regards the features as
vectors and computes the discrepancy between representations extracted from
recovered image and target one in Euclidean space. The other type considers the
features extracted from an image as a distribution and compares the
distribution discrepancy between recovered image and target one. We demonstrate
the effectiveness of transformer properties in improving the perceptual quality
while not sacrificing the quantitative scores (PSNR) over the most competitive
models, such as Uformer, Restormer, and NAFNet, on defocus deblurring and
motion deblurring tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03696">MOPA: Modular Object Navigation with PointGoal Agents. (arXiv:2304.03696v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raychaudhuri_S/0/1/0/all/0/1">Sonia Raychaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Campari_T/0/1/0/all/0/1">Tommaso Campari</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_U/0/1/0/all/0/1">Unnat Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1">Manolis Savva</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a></p>
<p>We propose a simple but effective modular approach MOPA (Modular ObjectNav
with PointGoal agents) to systematically investigate the inherent modularity of
the object navigation task in Embodied AI. MOPA consists of four modules: (a)
an object detection module trained to identify objects from RGB images, (b) a
map building module to build a semantic map of the observed objects, (c) an
exploration module enabling the agent to explore the environment, and (d) a
navigation module to move to identified target objects. We show that we can
effectively reuse a pretrained PointGoal agent as the navigation model instead
of learning to navigate from scratch, thus saving time and compute. We also
compare various exploration strategies for MOPA and find that a simple uniform
strategy significantly outperforms more advanced exploration methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05482">Computational Pathology: A Survey Review and The Way Forward. (arXiv:2304.05482v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hosseini_M/0/1/0/all/0/1">Mahdi S. Hosseini</a>, <a href="http://arxiv.org/find/eess/1/au:+Bejnordi_B/0/1/0/all/0/1">Babak Ehteshami Bejnordi</a>, <a href="http://arxiv.org/find/eess/1/au:+Trinh_V/0/1/0/all/0/1">Vincent Quoc-Huy Trinh</a>, <a href="http://arxiv.org/find/eess/1/au:+Hasan_D/0/1/0/all/0/1">Danial Hasan</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xingwen Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_T/0/1/0/all/0/1">Taehyo Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Haochen Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1">Theodore Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chinniah_K/0/1/0/all/0/1">Kajanan Chinniah</a>, <a href="http://arxiv.org/find/eess/1/au:+Maghsoudlou_S/0/1/0/all/0/1">Sina Maghsoudlou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1">Ryan Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1">Stephen Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1">Jiadai Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chan_L/0/1/0/all/0/1">Lyndon Chan</a>, <a href="http://arxiv.org/find/eess/1/au:+Khaki_S/0/1/0/all/0/1">Samir Khaki</a>, <a href="http://arxiv.org/find/eess/1/au:+Buin_A/0/1/0/all/0/1">Andrei Buin</a>, <a href="http://arxiv.org/find/eess/1/au:+Chaji_F/0/1/0/all/0/1">Fatemeh Chaji</a>, <a href="http://arxiv.org/find/eess/1/au:+Salehi_A/0/1/0/all/0/1">Ala Salehi</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_B/0/1/0/all/0/1">Bich Ngoc Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1">Dimitris Samaras</a>, <a href="http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1">Konstantinos N. Plataniotis</a></p>
<p>Computational Pathology CPath is an interdisciplinary science that augments
developments of computational approaches to analyze and model medical
histopathology images. The main objective for CPath is to develop
infrastructure and workflows of digital diagnostics as an assistive CAD system
for clinical pathology, facilitating transformational changes in the diagnosis
and treatment of cancer that are mainly address by CPath tools. With
evergrowing developments in deep learning and computer vision algorithms, and
the ease of the data flow from digital pathology, currently CPath is witnessing
a paradigm shift. Despite the sheer volume of engineering and scientific works
being introduced for cancer image analysis, there is still a considerable gap
of adopting and integrating these algorithms in clinical practice. This raises
a significant question regarding the direction and trends that are undertaken
in CPath. In this article we provide a comprehensive review of more than 800
papers to address the challenges faced in problem design all-the-way to the
application and implementation viewpoints. We have catalogued each paper into a
model-card by examining the key works and challenges faced to layout the
current landscape in CPath. We hope this helps the community to locate relevant
works and facilitate understanding of the field's future directions. In a
nutshell, we oversee the CPath developments in cycle of stages which are
required to be cohesively linked together to address the challenges associated
with such multidisciplinary science. We overview this cycle from different
perspectives of data-centric, model-centric, and application-centric problems.
We finally sketch remaining challenges and provide directions for future
technical developments and clinical integration of CPath
(https://github.com/AtlasAnalyticsLab/CPath_Survey).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11609">PiClick: Picking the desired mask in click-based interactive segmentation. (arXiv:2304.11609v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1">Cilin Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haochen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaolong Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Guoliang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1">Efstratios Gavves</a></p>
<p>Click-based interactive segmentation aims to generate target masks via human
clicking, which facilitates efficient pixel-level annotation and image editing.
In such a task, target ambiguity remains a problem hindering the accuracy and
efficiency of segmentation. That is, in scenes with rich context, one click may
correspond to multiple potential targets, while most previous interactive
segmentors only generate a single mask and fail to deal with target ambiguity.
In this paper, we propose a novel interactive segmentation network named
PiClick, to yield all potentially reasonable masks and suggest the most
plausible one for the user. Specifically, PiClick utilizes a Transformer-based
architecture to generate all potential target masks by mutually interactive
mask queries. Moreover, a Target Reasoning module is designed in PiClick to
automatically suggest the user-desired mask from all candidates, relieving
target ambiguity and extra-human efforts. Extensive experiments on 9
interactive segmentation datasets demonstrate PiClick performs favorably
against previous state-of-the-arts considering the segmentation results.
Moreover, we show that PiClick effectively reduces human efforts in annotating
and picking the desired masks. To ease the usage and inspire future research,
we release the source code of PiClick together with a plug-and-play annotation
tool at https://github.com/cilinyan/PiClick.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12639">Change detection needs change information: improving deep 3D point cloud change detection. (arXiv:2304.12639v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gelis_I/0/1/0/all/0/1">Iris de G&#xe9;lis</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Corpetti_T/0/1/0/all/0/1">Thomas Corpetti</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1">S&#xe9;bastien Lef&#xe8;vre</a> (2) ((1) Magellium, (2) Institut de Recherche en Informatique et Syst&#xe8;mes Al&#xe9;atoires IRISA - UMR 6074 - Universit&#xe9; Bretagne Sud, (3) Littoral - Environnement - T&#xe9;l&#xe9;d&#xe9;tection - G&#xe9;omatique LETG - UMR 6554 - Universit&#xe9; Rennes 2)</p>
<p>Change detection is an important task that rapidly identifies modified areas,
particularly when multi-temporal data are concerned. In landscapes with a
complex geometry (e.g., urban environment), vertical information is a very
useful source of knowledge that highlights changes and classifies them into
different categories. In this study, we focus on change segmentation using raw
three-dimensional (3D) point clouds (PCs) directly to avoid any information
loss due to the rasterization processes. While deep learning has recently
proven its effectiveness for this particular task by encoding the information
through Siamese networks, we investigate herein the idea of also using change
information in the early steps of deep networks. To do this, we first propose
to provide a Siamese KPConv state-of-the-art (SoTA) network with hand-crafted
features, especially a change-related one, which improves the mean of the
Intersection over Union (IoU) over the classes of change by 4.70%. Considering
that a major improvement is obtained due to the change-related feature, we then
propose three new architectures to address 3D PC change segmentation:
OneConvFusion, Triplet KPConv, and Encoder Fusion SiamKPConv. All these
networks consider the change information in the early steps and outperform the
SoTA methods. In particular, Encoder Fusion SiamKPConv overtakes the SoTA
approaches by more than 5% of the mean of the IoU over the classes of change,
emphasizing the value of having the network focus on change information for the
change detection task. The code is available at
https://github.com/IdeGelis/torch-points3d-SiamKPConvVariants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01611">AutoColor: Learned Light Power Control for Multi-Color Holograms. (arXiv:2305.01611v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1">Yicheng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kavakli_K/0/1/0/all/0/1">Koray Kavakl&#x131;</a>, <a href="http://arxiv.org/find/cs/1/au:+Urey_H/0/1/0/all/0/1">Hakan Urey</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Aksit_K/0/1/0/all/0/1">Kaan Ak&#x15f;it</a></p>
<p>Multi-color holograms rely on simultaneous illumination from multiple light
sources. These multi-color holograms could utilize light sources better than
conventional single-color holograms and can improve the dynamic range of
holographic displays. In this letter, we introduce AutoColor , the first
learned method for estimating the optimal light source powers required for
illuminating multi-color holograms. For this purpose, we establish the first
multi-color hologram dataset using synthetic images and their depth
information. We generate these synthetic images using a trending pipeline
combining generative, large language, and monocular depth estimation models.
Finally, we train our learned model using our dataset and experimentally
demonstrate that AutoColor significantly decreases the number of steps required
to optimize multi-color holograms from &gt; 1000 to 70 iteration steps without
compromising image quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02961">FUSegNet: A Deep Convolutional Neural Network for Foot Ulcer Segmentation. (arXiv:2305.02961v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1">Mrinal Kanti Dhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Taiyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1">Yash Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1">Sandeep Gopalakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zeyun Yu</a></p>
<p>This paper presents FUSegNet, a new model for foot ulcer segmentation in
diabetes patients, which uses the pre-trained EfficientNet-b7 as a backbone to
address the issue of limited training samples. A modified spatial and channel
squeeze-and-excitation (scSE) module called parallel scSE or P-scSE is proposed
that combines additive and max-out scSE. A new arrangement is introduced for
the module by fusing it in the middle of each decoder stage. As the top decoder
stage carries a limited number of feature maps, max-out scSE is bypassed there
to form a shorted P-scSE. A set of augmentations, comprising geometric,
morphological, and intensity-based augmentations, is applied before feeding the
data into the network. The proposed model is first evaluated on a publicly
available chronic wound dataset where it achieves a data-based dice score of
92.70%, which is the highest score among the reported approaches. The model
outperforms other scSE-based UNet models in terms of Pratt's figure of merits
(PFOM) scores in most categories, which evaluates the accuracy of edge
localization. The model is then tested in the MICCAI 2021 FUSeg challenge,
where a variation of FUSegNet called x-FUSegNet is submitted. The x-FUSegNet
model, which takes the average of outputs obtained by FUSegNet using 5-fold
cross-validation, achieves a dice score of 89.23%, placing it at the top of the
FUSeg Challenge leaderboard. The source code for the model is available on
https://github.com/mrinal054/FUSegNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.05499">Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model. (arXiv:2305.05499v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Riya_F/0/1/0/all/0/1">Farhin Farhad Riya</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoque_S/0/1/0/all/0/1">Shahinul Hoque</a>, <a href="http://arxiv.org/find/cs/1/au:+Onim_M/0/1/0/all/0/1">Md Saif Hassan Onim</a>, <a href="http://arxiv.org/find/cs/1/au:+Michaud_E/0/1/0/all/0/1">Edward Michaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1">Edmon Begoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jinyuan Stella Sun</a></p>
<p>The widespread adoption of Image Processing has propelled Object Recognition
(OR) models into essential roles across various applications, demonstrating the
power of AI and enabling crucial services. Among the applications, traffic sign
recognition stands out as a popular research topic, given its critical
significance in the development of autonomous vehicles. Despite their
significance, real-world challenges, such as alterations to traffic signs, can
negatively impact the performance of OR models. This study investigates the
influence of altered traffic signs on the accuracy and effectiveness of object
recognition, employing a publicly available dataset to introduce alterations in
shape, color, content, visibility, angles and background. Focusing on the
YOLOv7 (You Only Look Once) model, the study demonstrates a notable decline in
detection and classification accuracy when confronted with traffic signs in
unusual conditions including the altered traffic signs. Notably, the
alterations explored in this study are benign examples and do not involve
algorithms used for generating adversarial machine learning samples. This study
highlights the significance of enhancing the robustness of object detection
models in real-life scenarios and the need for further investigation in this
area to improve their accuracy and reliability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11467">Learning Sequence Descriptor based on Spatio-Temporal Attention for Visual Place Recognition. (arXiv:2305.11467v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junqiao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fenglin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yingfeng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_G/0/1/0/all/0/1">Gengxuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1">Wenjie Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1">Chen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1">Tiantian Feng</a></p>
<p>Visual Place Recognition (VPR) aims to retrieve frames from a geotagged
database that are located at the same place as the query frame. To improve the
robustness of VPR in perceptually aliasing scenarios, sequence-based VPR
methods are proposed. These methods are either based on matching between frame
sequences or extracting sequence descriptors for direct retrieval. However, the
former is usually based on the assumption of constant velocity, which is
difficult to hold in practice, and is computationally expensive and subject to
sequence length. Although the latter overcomes these problems, existing
sequence descriptors are constructed by aggregating features of multiple frames
only, without interaction on temporal information, and thus cannot obtain
descriptors with spatio-temporal discrimination.In this paper, we propose a
sequence descriptor that effectively incorporates spatio-temporal information.
Specifically, spatial attention within the same frame is utilized to learn
spatial feature patterns, while attention in corresponding local regions of
different frames is utilized to learn the persistence or change of features
over time. We use a sliding window to control the temporal range of attention
and use relative positional encoding to construct sequential relationships
between different features. This allows our descriptors to capture the
intrinsic dynamics in a sequence of frames.Comprehensive experiments on
challenging benchmark datasets show that the proposed approach outperforms
recent state-of-the-art methods.The code is available at
https://github.com/tiev-tongji/Spatio-Temporal-SeqVPR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16304">Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zheyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weixuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1">Damien Teney</a>, <a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1">Stephen Gould</a></p>
<p>Composed image retrieval aims to find an image that best matches a given
multi-modal user query consisting of a reference image and text pair. Existing
methods commonly pre-compute image embeddings over the entire corpus and
compare these to a reference image embedding modified by the query text at test
time. Such a pipeline is very efficient at test time since fast vector
distances can be used to evaluate candidates, but modifying the reference image
embedding guided only by a short textual description can be difficult,
especially independent of potential candidates. An alternative approach is to
allow interactions between the query and every possible candidate, i.e.,
reference-text-candidate triplets, and pick the best from the entire set.
Though this approach is more discriminative, for large-scale datasets the
computational cost is prohibitive since pre-computation of candidate embeddings
is no longer possible. We propose to combine the merits of both schemes using a
two-stage model. Our first stage adopts the conventional vector distancing
metric and performs a fast pruning among candidates. Meanwhile, our second
stage employs a dual-encoder architecture, which effectively attends to the
input triplet of reference-text-candidate and re-ranks the candidates. Both
stages utilize a vision-and-language pre-trained network, which has proven
beneficial for various downstream tasks. Our method consistently outperforms
state-of-the-art approaches on standard benchmarks for the task. Our
implementation is available at
https://github.com/Cuberick-Orion/Candidate-Reranking-CIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17891">The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification. (arXiv:2305.17891v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Linhao Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiaoyuan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1">Kexue Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Manning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhijian Song</a></p>
<p>This paper introduces the novel concept of few-shot weakly supervised
learning for pathology Whole Slide Image (WSI) classification, denoted as FSWC.
A solution is proposed based on prompt learning and the utilization of a large
language model, GPT-4. Since a WSI is too large and needs to be divided into
patches for processing, WSI classification is commonly approached as a Multiple
Instance Learning (MIL) problem. In this context, each WSI is considered a bag,
and the obtained patches are treated as instances. The objective of FSWC is to
classify both bags and instances with only a limited number of labeled bags.
Unlike conventional few-shot learning problems, FSWC poses additional
challenges due to its weak bag labels within the MIL framework. Drawing
inspiration from the recent achievements of vision-language models (V-L models)
in downstream few-shot classification tasks, we propose a two-level prompt
learning MIL framework tailored for pathology, incorporating language prior
knowledge. Specifically, we leverage CLIP to extract instance features for each
patch, and introduce a prompt-guided pooling strategy to aggregate these
instance features into a bag feature. Subsequently, we employ a small number of
labeled bags to facilitate few-shot prompt learning based on the bag features.
Our approach incorporates the utilization of GPT-4 in a question-and-answer
mode to obtain language prior knowledge at both the instance and bag levels,
which are then integrated into the instance and bag level language prompts.
Additionally, a learnable component of the language prompts is trained using
the available few-shot labeled data. We conduct extensive experiments on three
real WSI datasets encompassing breast cancer, lung cancer, and cervical cancer,
demonstrating the notable performance of the proposed method in bag and
instance classification. All codes will be available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01875">DiffECG: A Versatile Probabilistic Diffusion Model for ECG Signals Synthesis. (arXiv:2306.01875v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neifar_N/0/1/0/all/0/1">Nour Neifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Hamadou_A/0/1/0/all/0/1">Achraf Ben-Hamadou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mdhaffar_A/0/1/0/all/0/1">Afef Mdhaffar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jmaiel_M/0/1/0/all/0/1">Mohamed Jmaiel</a></p>
<p>Within cardiovascular disease detection using deep learning applied to ECG
signals, the complexities of handling physiological signals have sparked
growing interest in leveraging deep generative models for effective data
augmentation. In this paper, we introduce a novel versatile approach based on
denoising diffusion probabilistic models for ECG synthesis, addressing three
scenarios: (i) heartbeat generation, (ii) partial signal imputation, and (iii)
full heartbeat forecasting. Our approach presents the first generalized
conditional approach for ECG synthesis, and our experimental results
demonstrate its effectiveness for various ECG-related tasks. Moreover, we show
that our approach outperforms other state-of-the-art ECG generative models and
can enhance the performance of state-of-the-art classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02245">SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model. (arXiv:2306.02245v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dingyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Dingkang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongcheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zhikang Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xiaoqing Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>With the development of large language models, many remarkable linguistic
systems like ChatGPT have thrived and achieved astonishing success on many
tasks, showing the incredible power of foundation models. In the spirit of
unleashing the capability of foundation models on vision tasks, the Segment
Anything Model (SAM), a vision foundation model for image segmentation, has
been proposed recently and presents strong zero-shot ability on many downstream
2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be
explored, especially 3D object detection. With this inspiration, we explore
adapting the zero-shot ability of SAM to 3D object detection in this paper. We
propose a SAM-powered BEV processing pipeline to detect objects and get
promising results on the large-scale Waymo open dataset. As an early attempt,
our method takes a step toward 3D object detection with vision foundation
models and presents the opportunity to unleash their power on 3D vision tasks.
The code is released at https://github.com/DYZhang09/SAM3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12681">One at a Time: Progressive Multi-step Volumetric Probability Learning for Reliable 3D Scene Perception. (arXiv:2306.12681v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yasheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jingxin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1">Wenjun Zeng</a></p>
<p>Numerous studies have investigated the pivotal role of reliable 3D volume
representation in scene perception tasks, such as multi-view stereo (MVS) and
semantic scene completion (SSC). They typically construct 3D probability
volumes directly with geometric correspondence, attempting to fully address the
scene perception tasks in a single forward pass. However, such a single-step
solution makes it hard to learn accurate and convincing volumetric probability,
especially in challenging regions like unexpected occlusions and complicated
light reflections. Therefore, this paper proposes to decompose the complicated
3D volume representation learning into a sequence of generative steps to
facilitate fine and reliable scene perception. Considering the recent advances
achieved by strong generative diffusion models, we introduce a multi-step
learning framework, dubbed as VPD, dedicated to progressively refining the
Volumetric Probability in a Diffusion process. Extensive experiments are
conducted on scene perception tasks including multi-view stereo (MVS) and
semantic scene completion (SSC), to validate the efficacy of our method in
learning reliable volumetric representations. Notably, for the SSC task, our
work stands out as the first to surpass LiDAR-based methods on the
SemanticKITTI dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16048">Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity. (arXiv:2306.16048v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenlin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_T/0/1/0/all/0/1">Tiffany Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Abhay Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanbei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Manchen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1">Paolo Favaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1">Joseph Tighe</a>, <a href="http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1">Davide Modolo</a></p>
<p>This paper introduces innovative benchmarks to evaluate Vision-Language
Models (VLMs) in real-world zero-shot recognition tasks, focusing on the
granularity and specificity of prompting text. We propose a unique evaluation
protocol using adapted ImageNet and MS-COCO datasets to assess models'
consistency in recognizing concepts at varying granularity levels and their
sensitivity to the specificity of language inputs. Our extensive evaluation
reveals that state-of-the-art VLMs, including contrastive models like CLIP,
struggle with granularity and are sensitive to text specificity, impacting
their effectiveness in open-world settings. This comprehensive study, a first
in evaluating VLMs from these perspectives, provides valuable insights and
tools for the community, highlighting the limitations and paving the way for
enhanced models with better generalization in zero-shot recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07950">Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1">Sahil Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1">Martin Swany</a></p>
<p>In distributed training, deep neural networks (DNNs) are launched over
multiple workers concurrently and aggregate their local updates on each step in
bulk-synchronous parallel (BSP) training. However, BSP does not linearly
scale-out due to high communication cost of aggregation. To mitigate this
overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous
Parallel (SSP) either reduce synchronization frequency or eliminate it
altogether, usually at the cost of lower final accuracy. In this paper, we
present \texttt{SelSync}, a practical, low-overhead method for DNN training
that dynamically chooses to incur or avoid communication at each step either by
calling the aggregation op or applying local updates based on their
significance. We propose various optimizations as part of \texttt{SelSync} to
improve convergence in the context of \textit{semi-synchronous} training. Our
system converges to the same or better accuracy than BSP while reducing
training time by up to 14$\times$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12017">DISCO: Distribution-Aware Calibration for Object Detection with Noisy Bounding Boxes. (arXiv:2308.12017v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Donghao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jialin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinpeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiancheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1">Qiang Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1">Bin-Bin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng-Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guangyong Chen</a></p>
<p>Large-scale well-annotated datasets are of great importance for training an
effective object detector. However, obtaining accurate bounding box annotations
is laborious and demanding. Unfortunately, the resultant noisy bounding boxes
could cause corrupt supervision signals and thus diminish detection
performance. Motivated by the observation that the real ground-truth is usually
situated in the aggregation region of the proposals assigned to a noisy
ground-truth, we propose DIStribution-aware CalibratiOn (DISCO) to model the
spatial distribution of proposals for calibrating supervision signals. In
DISCO, spatial distribution modeling is performed to statistically extract the
potential locations of objects. Based on the modeled distribution, three
distribution-aware techniques, i.e., distribution-aware proposal augmentation
(DA-Aug), distribution-aware box refinement (DA-Ref), and distribution-aware
confidence estimation (DA-Est), are developed to improve classification,
localization, and interpretability, respectively. Extensive experiments on
large-scale noisy image datasets (i.e., Pascal VOC and MS-COCO) demonstrate
that DISCO can achieve state-of-the-art detection performance, especially at
high noise levels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12608">HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation. (arXiv:2308.12608v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaohao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1">Zhiwu Qing</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Changxin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1">Nong Sang</a></p>
<p>Point-supervised Temporal Action Localization (PSTAL) is an emerging research
direction for label-efficient learning. However, current methods mainly focus
on optimizing the network either at the snippet-level or the instance-level,
neglecting the inherent reliability of point annotations at both levels. In
this paper, we propose a Hierarchical Reliability Propagation (HR-Pro)
framework, which consists of two reliability-aware stages: Snippet-level
Discrimination Learning and Instance-level Completeness Learning, both stages
explore the efficient propagation of high-confidence cues in point annotations.
For snippet-level learning, we introduce an online-updated memory to store
reliable snippet prototypes for each class. We then employ a Reliability-aware
Attention Block to capture both intra-video and inter-video dependencies of
snippets, resulting in more discriminative and robust snippet representation.
For instance-level learning, we propose a point-based proposal generation
approach as a means of connecting snippets and instances, which produces
high-confidence proposals for further optimization at the instance level.
Through multi-level reliability-aware learning, we obtain more reliable
confidence scores and more accurate temporal boundaries of predicted proposals.
Our HR-Pro achieves state-of-the-art performance on multiple challenging
benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably,
our HR-Pro largely surpasses all previous point-supervised methods, and even
outperforms several competitive fully supervised methods. Code will be
available at https://github.com/pipixin321/HR-Pro.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03160">ResFields: Residual Neural Fields for Spatiotemporal Signals. (arXiv:2309.03160v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1">Marko Mihajlovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Prokudin_S/0/1/0/all/0/1">Sergey Prokudin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siyu Tang</a></p>
<p>Neural fields, a category of neural networks trained to represent
high-frequency signals, have gained significant attention in recent years due
to their impressive performance in modeling complex 3D data, such as signed
distance (SDFs) or radiance fields (NeRFs), via a single multi-layer perceptron
(MLP). However, despite the power and simplicity of representing signals with
an MLP, these methods still face challenges when modeling large and complex
temporal signals due to the limited capacity of MLPs. In this paper, we propose
an effective approach to address this limitation by incorporating temporal
residual layers into neural fields, dubbed ResFields. It is a novel class of
networks specifically designed to effectively represent complex temporal
signals. We conduct a comprehensive analysis of the properties of ResFields and
propose a matrix factorization technique to reduce the number of trainable
parameters and enhance generalization capabilities. Importantly, our
formulation seamlessly integrates with existing MLP-based neural fields and
consistently improves results across various challenging tasks: 2D video
approximation, dynamic shape modeling via temporal SDFs, and dynamic NeRF
reconstruction. Lastly, we demonstrate the practical utility of ResFields by
showcasing its effectiveness in capturing dynamic 3D scenes from sparse RGBD
cameras of a lightweight capture system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06054">Breaking through the learning plateaus of in-context learning in Transformer. (arXiv:2309.06054v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jingwen Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a></p>
<p>In-context learning, i.e., learning from context examples, is an impressive
ability of Transformer. Training Transformers to possess this in-context
learning skill is computationally intensive due to the occurrence of learning
plateaus, which are periods within the training process where there is minimal
or no enhancement in the model's in-context learning capability. To study the
mechanism behind the learning plateaus, we conceptually seperate a component
within the model's internal representation that is exclusively affected by the
model's weights. We call this the "weights component", and the remainder is
identified as the "context component". By conducting meticulous and controlled
experiments on synthetic tasks, we note that the persistence of learning
plateaus correlates with compromised functionality of the weights component.
Recognizing the impaired performance of the weights component as a fundamental
behavior drives learning plateaus, we have developed three strategies to
expedite the learning of Transformers. The effectiveness of these strategies is
further confirmed in natural language processing tasks. In conclusion, our
research demonstrates the feasibility of cultivating a powerful in-context
learning ability within AI systems in an eco-friendly manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12953">Inter-vendor harmonization of Computed Tomography (CT) reconstruction kernels using unpaired image translation. (arXiv:2309.12953v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Krishnan_A/0/1/0/all/0/1">Aravind R. Krishnan</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1">Kaiwen Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Thomas Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1">Chenyu Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Remedios_L/0/1/0/all/0/1">Lucas W. Remedios</a>, <a href="http://arxiv.org/find/eess/1/au:+Kanakaraj_P/0/1/0/all/0/1">Praitayini Kanakaraj</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1">Ho Hin Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Bao_S/0/1/0/all/0/1">Shunxing Bao</a>, <a href="http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1">Kim L. Sandler</a>, <a href="http://arxiv.org/find/eess/1/au:+Maldonado_F/0/1/0/all/0/1">Fabien Maldonado</a>, <a href="http://arxiv.org/find/eess/1/au:+Isgum_I/0/1/0/all/0/1">Ivana Isgum</a>, <a href="http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1">Bennett A. Landman</a></p>
<p>The reconstruction kernel in computed tomography (CT) generation determines
the texture of the image. Consistency in reconstruction kernels is important as
the underlying CT texture can impact measurements during quantitative image
analysis. Harmonization (i.e., kernel conversion) minimizes differences in
measurements due to inconsistent reconstruction kernels. Existing methods
investigate harmonization of CT scans in single or multiple manufacturers.
However, these methods require paired scans of hard and soft reconstruction
kernels that are spatially and anatomically aligned. Additionally, a large
number of models need to be trained across different kernel pairs within
manufacturers. In this study, we adopt an unpaired image translation approach
to investigate harmonization between and across reconstruction kernels from
different manufacturers by constructing a multipath cycle generative
adversarial network (GAN). We use hard and soft reconstruction kernels from the
Siemens and GE vendors from the National Lung Screening Trial dataset. We use
50 scans from each reconstruction kernel and train a multipath cycle GAN. To
evaluate the effect of harmonization on the reconstruction kernels, we
harmonize 50 scans each from Siemens hard kernel, GE soft kernel and GE hard
kernel to a reference Siemens soft kernel (B30f) and evaluate percent
emphysema. We fit a linear model by considering the age, smoking status, sex
and vendor and perform an analysis of variance (ANOVA) on the emphysema scores.
Our approach minimizes differences in emphysema measurement and highlights the
impact of age, sex, smoking status and vendor on emphysema quantification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14630">Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns. (arXiv:2309.14630v2 [econ.EM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Gunsilius_F/0/1/0/all/0/1">Florian Gunsilius</a>, <a href="http://arxiv.org/find/econ/1/au:+Dijcke_D/0/1/0/all/0/1">David Van Dijcke</a></p>
<p>Discontinuities in regression functions can reveal important insights. In
many contexts, like geographic settings, such discontinuities are multivariate
and unknown a priori. We propose a non-parametric regression method that
estimates the location and size of discontinuities by segmenting the regression
surface. This estimator is based on a convex relaxation of the Mumford-Shah
functional, for which we establish identification and convergence. We use it to
show that an internet shutdown in India resulted in a reduction of economic
activity by 25--35%, greatly surpassing previous estimates and shedding new
light on the true cost of such shutdowns for digital economies globally.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01040">Unsupervised motion segmentation in one go: Smooth long-term model over a video. (arXiv:2310.01040v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meunier_E/0/1/0/all/0/1">Etienne Meunier</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouthemy_P/0/1/0/all/0/1">Patrick Bouthemy</a></p>
<p>Human beings have the ability to continuously analyze a video and immediately
extract the main motion components. Motion segmentation methods based on deep
learning often proceed frame by frame. We want to go beyond this paradigm, and
perform the motion segmentation in series of flow fields of any length, up to
the complete video sequence. It will be a prominent added value for downstream
computer vision tasks, and could provide a pretext criterion for unsupervised
video representation learning. In this perspective, we propose a novel
long-term spatio-temporal model operating in a totally unsupervised way. It
takes as input the volume of consecutive optical flow (OF) fields, and delivers
a volume of segments of coherent motion over the video. More specifically, we
have designed a transformer-based network, where we leverage a mathematically
well-founded framework, the Evidence Lower Bound (ELBO), to infer the loss
function. The loss function combines a flow reconstruction term involving
spatio-temporal parametric motion models combining, in a novel way, polynomial
(quadratic) motion models for the $(x,y)$-spatial dimensions and B-splines for
the time dimension of the video sequence, and a regularization term enforcing
temporal consistency on the masks. We report experiments on four VOS benchmarks
with convincing quantitative results. We also highlight through visual results
the key contributions on temporal consistency brought by our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03337">Denoising Diffusion Step-aware Models. (arXiv:2310.03337v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yukang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Luozhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingcong Chen</a></p>
<p>Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for
data generation across various domains. However, a significant bottleneck is
the necessity for whole-network computation during every step of the generative
process, leading to high computational overheads. This paper presents a novel
framework, Denoising Diffusion Step-aware Models (DDSM), to address this
challenge. Unlike conventional approaches, DDSM employs a spectrum of neural
networks whose sizes are adapted according to the importance of each generative
step, as determined through evolutionary search. This step-wise network
variation effectively circumvents redundant computational efforts, particularly
in less critical steps, thereby enhancing the efficiency of the diffusion
model. Furthermore, the step-aware design can be seamlessly integrated with
other efficiency-geared diffusion models such as DDIMs and latent diffusion,
thus broadening the scope of computational savings. Empirical evaluations
demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%
for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all
without compromising the generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05969">Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Muharram_A/0/1/0/all/0/1">Arief Purnama Muharram</a>, <a href="http://arxiv.org/find/eess/1/au:+Haryono_H/0/1/0/all/0/1">Hollyana Puteri Haryono</a>, <a href="http://arxiv.org/find/eess/1/au:+Juma_A/0/1/0/all/0/1">Abassi Haji Juma</a>, <a href="http://arxiv.org/find/eess/1/au:+Puspasari_I/0/1/0/all/0/1">Ira Puspasari</a>, <a href="http://arxiv.org/find/eess/1/au:+Utama_N/0/1/0/all/0/1">Nugraha Priya Utama</a></p>
<p>Reading and interpreting chest X-ray images is one of the most radiologist's
routines. However, it still can be challenging, even for the most experienced
ones. Therefore, we proposed a multi-model deep learning-based automated chest
X-ray report generator system designed to assist radiologists in their work.
The basic idea of the proposed system is by utilizing multi
binary-classification models for detecting multi abnormalities, with each model
responsible for detecting one abnormality, in a single image. In this study, we
limited the radiology abnormalities detection to only cardiomegaly, lung
effusion, and consolidation. The system generates a radiology report by
performing the following three steps: image pre-processing, utilizing deep
learning models to detect abnormalities, and producing a report. The aim of the
image pre-processing step is to standardize the input by scaling it to 128x128
pixels and slicing it into three segments, which covers the upper, lower, and
middle parts of the lung. After pre-processing, each corresponding model
classifies the image, resulting in a 0 (zero) for no abnormality detected and a
1 (one) for the presence of an abnormality. The prediction outputs of each
model are then concatenated to form a 'result code'. The 'result code' is used
to construct a report by selecting the appropriate pre-determined sentence for
each detected abnormality in the report generation step. The proposed system is
expected to reduce the workload of radiologists and increase the accuracy of
chest X-ray diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00689">Collaboration in Immersive Environments: Challenges and Solutions. (arXiv:2311.00689v3 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doroudian_S/0/1/0/all/0/1">Shahin Doroudian</a></p>
<p>Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02340">MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching. (arXiv:2311.02340v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1">Miaojie Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Junda Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1">Hao Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Longliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1">Gangwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qingyong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a></p>
<p>Stereo matching is a fundamental task in scene comprehension. In recent
years, the method based on iterative optimization has shown promise in stereo
matching. However, the current iteration framework employs a single-peak
lookup, which struggles to handle the multi-peak problem effectively.
Additionally, the fixed search range used during the iteration process limits
the final convergence effects. To address these issues, we present a novel
iterative optimization architecture called MC-Stereo. This architecture
mitigates the multi-peak distribution problem in matching through the
multi-peak lookup strategy, and integrates the coarse-to-fine concept into the
iterative framework via the cascade search range. Furthermore, given that
feature representation learning is crucial for successful learn-based stereo
matching, we introduce a pre-trained network to serve as the feature extractor,
enhancing the front end of the stereo matching pipeline. Based on these
improvements, MC-Stereo ranks first among all publicly available methods on the
KITTI-2012 and KITTI-2015 benchmarks, and also achieves state-of-the-art
performance on ETH3D. Code is available at
https://github.com/MiaoJieF/MC-Stereo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12076">Towards Few-shot Out-of-Distribution Detection. (arXiv:2311.12076v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiuqing Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yongbin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Heng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1">Jun Cen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yifan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sook Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1">Park Dong Sun</a></p>
<p>Out-of-distribution (OOD) detection is critical for ensuring the reliability
of open-world intelligent systems. Despite the notable advancements in existing
OOD detection methodologies, our study identifies a significant performance
drop under the scarcity of training samples. In this context, we introduce a
novel few-shot OOD detection benchmark, carefully constructed to address this
gap. Our empirical analysis reveals the superiority of ParameterEfficient
Fine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter
tuning, over conventional techniques, including fully fine-tuning and linear
probing tuning in the few-shot OOD detection task. Recognizing some crucial
information from the pre-trained model, which is pivotal for OOD detection, may
be lost during the fine-tuning process, we propose a method termed
DomainSpecific and General Knowledge Fusion (DSGF). This approach is designed
to be compatible with diverse fine-tuning frameworks. Our experiments show that
the integration of DSGF significantly enhances the few-shot OOD detection
capabilities across various methods and fine-tuning methodologies, including
fully fine-tuning, visual adapter tuning, and visual prompt tuning. The code
will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12265">Virtual Home Staging: Inverse Rendering and Editing an Indoor Panorama under Natural Illumination. (arXiv:2311.12265v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1">Guanzhou Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawyer_A/0/1/0/all/0/1">Azadeh O. Sawyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1">Srinivasa G. Narasimhan</a></p>
<p>We propose a novel inverse rendering method that enables the transformation
of existing indoor panoramas with new indoor furniture layouts under natural
illumination. To achieve this, we captured indoor HDR panoramas along with
real-time outdoor hemispherical HDR photographs. Indoor and outdoor HDR images
were linearly calibrated with measured absolute luminance values for accurate
scene relighting. Our method consists of three key components: (1) panoramic
furniture detection and removal, (2) automatic floor layout design, and (3)
global rendering with scene geometry, new furniture objects, and a real-time
outdoor photograph. We demonstrate the effectiveness of our workflow in
rendering indoor scenes under different outdoor illumination conditions.
Additionally, we contribute a new calibrated HDR (Cali-HDR) dataset that
consists of 137 calibrated indoor panoramas and their associated outdoor
photographs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02493">Flexible Communication for Optimal Distributed Learning over Unpredictable Networks. (arXiv:2312.02493v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1">Sahil Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1">Martin Swany</a></p>
<p>Gradient compression alleviates expensive communication in distributed deep
learning by sending fewer values and its corresponding indices, typically via
Allgather (AG). Training with high compression ratio (CR) achieves high
accuracy like DenseSGD, but has lower parallel scaling due to high
communication cost (i.e., parallel efficiency). Using lower CRs improves
parallel efficiency by lowering synchronization cost, but degrades model
accuracy as well (statistical efficiency). Further, speedup attained with
different models and CRs also varies with network latency, effective bandwidth
and collective op used for aggregation. In many cases, collectives like
Allreduce (AR) have lower cost than AG to exchange the same amount of data. In
this paper, we propose an AR-compatible Topk compressor that is
bandwidth-optimal and thus performs better than AG in certain network
configurations. We develop a flexible communication strategy that switches
between AG and AR based on which collective is optimal in the current settings,
and model the pareto-relationship between parallel and statistical efficiency
as a multi-objective optimization (MOO) problem to dynamically adjust CR and
accelerate training while still converging to high accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05928">AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer. (arXiv:2312.05928v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1">Joonwoo Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sooyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yuewei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1">Shinjae Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1">Jiook Cha</a></p>
<p>Neural style transfer (NST) has evolved significantly in recent years. Yet,
despite its rapid progress and advancement, existing NST methods either
struggle to transfer aesthetic information from a style effectively or suffer
from high computational costs and inefficiencies in feature disentanglement due
to using pre-trained models. This work proposes a lightweight but effective
model, AesFA -- Aesthetic Feature-Aware NST. The primary idea is to decompose
the image via its frequencies to better disentangle aesthetic styles from the
reference image while training the entire model in an end-to-end manner to
exclude pre-trained models at inference completely. To improve the network's
ability to extract more distinct representations and further enhance the
stylization quality, this work introduces a new aesthetic feature: contrastive
loss. Extensive experiments and ablations show the approach not only
outperforms recent NST methods in terms of stylization quality, but it also
achieves faster inference. Codes are available at
https://github.com/Sooyyoungg/AesFA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09486">Unraveling Batch Normalization for Realistic Test-Time Adaptation. (arXiv:2312.09486v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zixian Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jingwei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiufeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaizhu Huang</a></p>
<p>While recent test-time adaptations exhibit efficacy by adjusting batch
normalization to narrow domain disparities, their effectiveness diminishes with
realistic mini-batches due to inaccurate target estimation. As previous
attempts merely introduce source statistics to mitigate this issue, the
fundamental problem of inaccurate target estimation still persists, leaving the
intrinsic test-time domain shifts unresolved. This paper delves into the
problem of mini-batch degradation. By unraveling batch normalization, we
discover that the inexact target statistics largely stem from the substantially
reduced class diversity in batch. Drawing upon this insight, we introduce a
straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge
the class diversity gap between training and testing batches. Importantly, our
TEMA adaptively extends the scope of typical methods beyond the current batch
to incorporate a diverse set of class information, which in turn boosts an
accurate target estimation. Built upon this foundation, we further design a
novel layer-wise rectification strategy to consistently promote test-time
performance. Our proposed method enjoys a unique advantage as it requires
neither training nor tuning parameters, offering a truly hassle-free solution.
It significantly enhances model robustness against shifted domains and
maintains resilience in diverse real-world scenarios with various batch sizes,
achieving state-of-the-art performance on several major benchmarks. Code is
available at \url{https://github.com/kiwi12138/RealisticTTA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13250">The role of data embedding in equivariant quantum convolutional neural networks. (arXiv:2312.13250v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Das_S/0/1/0/all/0/1">Sreetama Das</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Martina_S/0/1/0/all/0/1">Stefano Martina</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Caruso_F/0/1/0/all/0/1">Filippo Caruso</a></p>
<p>Geometric deep learning refers to the scenario in which the symmetries of a
dataset are used to constrain the parameter space of a neural network and thus,
improve their trainability and generalization. Recently this idea has been
incorporated into the field of quantum machine learning, which has given rise
to equivariant quantum neural networks (EQNNs). In this work, we investigate
the role of classical-to-quantum embedding on the performance of equivariant
quantum convolutional neural networks (EQCNNs) for the classification of
images. We discuss the connection between the data embedding method and the
resulting representation of a symmetry group and analyze how changing
representation affects the expressibility of an EQCNN. We numerically compare
the classification accuracy of EQCNNs with three different basis-permuted
amplitude embeddings to the one obtained from a non-equivariant quantum
convolutional neural network (QCNN). Our results show a clear dependence of
classification accuracy on the underlying embedding, especially for initial
training iterations. The improvement in classification accuracy of EQCNN over
non-equivariant QCNN may be present or absent depending on the particular
embedding and dataset used. It is expected that the results of this work can be
useful to the community for a better understanding of the importance of data
embedding choice in the context of geometric quantum machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.13778">Progressive Evolution from Single-Point to Polygon for Scene Text. (arXiv:2312.13778v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1">Linger Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Mingxin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xudong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lianwen Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>The advancement of text shape representations towards compactness has
enhanced text detection and spotting performance, but at a high annotation
cost. Current models use single-point annotations to reduce costs, yet they
lack sufficient localization information for downstream applications. To
overcome this limitation, we introduce Point2Polygon, which can efficiently
transform single-points into compact polygons. Our method uses a coarse-to-fine
process, starting with creating and selecting anchor points based on
recognition confidence, then vertically and horizontally refining the polygon
using recognition information to optimize its shape. We demonstrate the
accuracy of the generated polygons through extensive experiments: 1) By
creating polygons from ground truth points, we achieved an accuracy of 82.0% on
ICDAR 2015; 2) In training detectors with polygons generated by our method, we
attained 86% of the accuracy relative to training with ground truth (GT); 3)
Additionally, the proposed Point2Polygon can be seamlessly integrated to
empower single-point spotters to generate polygons. This integration led to an
impressive 82.5% accuracy for the generated polygons. It is worth mentioning
that our method relies solely on synthetic recognition information, eliminating
the need for any manual annotation beyond single points.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15915">ChartBench: A Benchmark for Complex Visual Reasoning in Charts. (arXiv:2312.15915v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhengzhuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Sinan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yiyan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chengjin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jian Guo</a></p>
<p>Multimodal Large Language Models (MLLMs) demonstrate impressive image
understanding and generating capabilities. However, existing benchmarks employ
limited charts that deviate from real-world scenarios, posing challenges in
accurately assessing the chart comprehension of MLLMs. To overcome this
constraint, we propose ChartBench, an exhaustive chart benchmark specifically
designed to evaluate MLLMs' chart comprehension and data reliability through
complex visual reasoning. ChartBench encompasses a wide spectrum, including 42
categories, 2.1K charts, and 16.8K question-answer pairs. Diverging from
previous benchmarks, ChartBench avoids employing data point annotation charts
or metadata prompts directly. Instead, it compels MLLMs to derive values akin
to human understanding by leveraging inherent chart elements such as color,
legends, or coordinate systems. Additionally, we propose an enhanced evaluation
metric, Acc+, which facilitates the evaluation of MLLMs without needing
labor-intensive manual efforts or costly evaluations based on GPT. Our
extensive experimental evaluation involves 12 widely-used open-sourced and 2
proprietary MLLMs, revealing the limitations of MLLMs in interpreting charts
and providing valuable insights to encourage closer scrutiny of this aspect.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01577">Test-Time Personalization with Meta Prompt for Gaze Estimation. (arXiv:2401.01577v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Julia Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1">Mohammad Hassanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1">Konstantinos Plataniotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yuanhao Yu</a></p>
<p>Despite the recent remarkable achievement in gaze estimation, efficient and
accurate personalization of gaze estimation without labels is a practical
problem but rarely touched on in the literature. To achieve efficient
personalization, we take inspiration from the recent advances in Natural
Language Processing (NLP) by updating a negligible number of parameters,
"prompts", at the test time. Specifically, the prompt is additionally attached
without perturbing original network and can contain less than 1% of a
ResNet-18's parameters. Our experiments show high efficiency of the prompt
tuning approach. The proposed one can be 10 times faster in terms of adaptation
speed than the methods compared. However, it is non-trivial to update the
prompt for personalized gaze estimation without labels. At the test time, it is
essential to ensure that the minimizing of particular unsupervised loss leads
to the goals of minimizing gaze estimation error. To address this difficulty,
we propose to meta-learn the prompt to ensure that its updates align with the
goal. Our experiments show that the meta-learned prompt can be effectively
adapted even with a simple symmetry loss. In addition, we experiment on four
cross-dataset validations to show the remarkable advantages of the proposed
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05602">Nucleus subtype classification using inter-modality learning. (arXiv:2401.05602v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Remedios_L/0/1/0/all/0/1">Lucas W. Remedios</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1">Shunxing Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Remedios_S/0/1/0/all/0/1">Samuel W. Remedios</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Ho Hin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1">Leon Y. Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Thomas Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1">Ruining Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Can Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_K/0/1/0/all/0/1">Ken S. Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Roland_J/0/1/0/all/0/1">Joseph T. Roland</a>, <a href="http://arxiv.org/find/cs/1/au:+Washington_M/0/1/0/all/0/1">Mary K. Washington</a>, <a href="http://arxiv.org/find/cs/1/au:+Coburn_L/0/1/0/all/0/1">Lori A. Coburn</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_K/0/1/0/all/0/1">Keith T. Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1">Yuankai Huo</a>, <a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1">Bennett A. Landman</a></p>
<p>Understanding the way cells communicate, co-locate, and interrelate is
essential to understanding human physiology. Hematoxylin and eosin (H&amp;E)
staining is ubiquitously available both for clinical studies and research. The
Colon Nucleus Identification and Classification (CoNIC) Challenge has recently
innovated on robust artificial intelligence labeling of six cell types on H&amp;E
stains of the colon. However, this is a very small fraction of the number of
potential cell classification types. Specifically, the CoNIC Challenge is
unable to classify epithelial subtypes (progenitor, endocrine, goblet),
lymphocyte subtypes (B, helper T, cytotoxic T), or connective subtypes
(fibroblasts, stromal). In this paper, we propose to use inter-modality
learning to label previously un-labelable cell types on virtual H&amp;E. We
leveraged multiplexed immunofluorescence (MxIF) histology imaging to identify
14 subclasses of cell types. We performed style transfer to synthesize virtual
H&amp;E from MxIF and transferred the higher density labels from MxIF to these
virtual H&amp;E images. We then evaluated the efficacy of learning in this
approach. We identified helper T and progenitor nuclei with positive predictive
values of $0.34 \pm 0.15$ (prevalence $0.03 \pm 0.01$) and $0.47 \pm 0.1$
(prevalence $0.07 \pm 0.02$) respectively on virtual H&amp;E. This approach
represents a promising step towards automating annotation in digital pathology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.09720">GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting. (arXiv:2401.09720v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengtian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1">Shengxiang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhifeng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Keyu Chen</a></p>
<p>In this work, we propose a novel clothed human reconstruction method called
GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural
radiance based models, 3D Gaussian Splatting has recently demonstrated great
performance in terms of training time and rendering quality. However, applying
the static 3D Gaussian Splatting model to the dynamic human reconstruction
problem is non-trivial due to complicated non-rigid deformations and rich cloth
details. To address these challenges, our method considers explicit pose-guided
deformation to associate dynamic Gaussians across the canonical space and the
observation space, introducing a physically-based prior with regularized
transformations helps mitigate ambiguity between the two spaces. During the
training process, we further propose a pose refinement strategy to update the
pose regression for compensating the inaccurate initial estimation and a
split-with-scale mechanism to enhance the density of regressed point clouds.
The experiments validate that our method can achieve state-of-the-art
photorealistic novel-view rendering results with high-quality details for
dynamic clothed human bodies, along with explicit geometry reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10711">Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Chenghang Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yixuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1">Weifeng Ge</a></p>
<p>Video Question Answering (VideoQA) aims to answer natural language questions
based on the information observed in videos. Despite the recent success of
Large Multimodal Models (LMMs) in image-language understanding and reasoning,
they deal with VideoQA insufficiently by simply taking uniformly sampled frames
as visual inputs, which ignores question-relevant visual clues. Moreover, there
are no human annotations for question-critical timestamps in existing VideoQA
datasets. In light of this, we propose a novel weakly supervised framework to
enforce the LMMs to reason out the answers with question-critical moments as
visual inputs. Specifically, we fuse the question and answer pairs as event
descriptions to find multiple keyframes as target moments, which will be
pseudo-labels. With these pseudo-labels as additionally weak supervision, we
devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG
learns multiple Gaussian functions to characterize the temporal structure of
the video, and sample question-critical frames as positive moments to be the
visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks
verify the effectiveness of our framework, and we achieve substantial
improvements compared to previous state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.11414">S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving. (arXiv:2401.11414v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuang-Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fisher Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qijun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Rui Fan</a></p>
<p>Semantic segmentation and stereo matching are two essential components of 3D
environmental perception systems for autonomous driving. Nevertheless,
conventional approaches often address these two problems independently,
employing separate models for each task. This approach poses practical
limitations in real-world scenarios, particularly when computational resources
are scarce or real-time performance is imperative. Hence, in this article, we
introduce S$^3$M-Net, a novel joint learning framework developed to perform
semantic segmentation and stereo matching simultaneously. Specifically,
S$^3$M-Net shares the features extracted from RGB images between both tasks,
resulting in an improved overall scene understanding capability. This feature
sharing process is realized using a feature fusion adaption (FFA) module, which
effectively transforms the shared features into semantic space and subsequently
fuses them with the encoded disparity features. The entire joint learning
framework is trained by minimizing a novel semantic consistency-guided (SCG)
loss, which places emphasis on the structural consistency in both tasks.
Extensive experimental results conducted on the vKITTI2 and KITTI datasets
demonstrate the effectiveness of our proposed joint learning framework and its
superior performance compared to other state-of-the-art single-task networks.
Our project webpage is accessible at mias.group/S3M-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12665">ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jianjian Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1">Peng Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yuhan Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1">Chongjun Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a></p>
<p>Recently, foundational models such as CLIP and SAM have shown promising
performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However,
either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible
key drawbacks: 1) CLIP primarily focuses on global feature alignment across
different inputs, leading to imprecise segmentation of local anomalous parts;
2) SAM tends to generate numerous redundant masks without proper prompt
constraints, resulting in complex post-processing requirements. In this work,
we innovatively propose a CLIP and SAM collaboration framework called ClipSAM
for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding
capability for anomaly localization and rough segmentation, which is further
used as the prompt constraints for SAM to refine the anomaly segmentation
results. In details, we introduce a crucial Unified Multi-scale Cross-modal
Interaction (UMCI) module for interacting language with visual features at
multiple scales of CLIP to reason anomaly positions. Then, we design a novel
Multi-level Mask Refinement (MMR) module, which utilizes the positional
information as multi-level prompts for SAM to acquire hierarchical levels of
masks and merges them. Extensive experiments validate the effectiveness of our
approach, achieving the optimal segmentation performance on the MVTec-AD and
VisA datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12729">Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1">Jibinraj Antony</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegiste_V/0/1/0/all/0/1">Vinit Hegiste</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazeri_A/0/1/0/all/0/1">Ali Nazeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Tavakoli_H/0/1/0/all/0/1">Hooman Tavakoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Walunj_S/0/1/0/all/0/1">Snehal Walunj</a>, <a href="http://arxiv.org/find/cs/1/au:+Plociennik_C/0/1/0/all/0/1">Christiane Plociennik</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruskowski_M/0/1/0/all/0/1">Martin Ruskowski</a></p>
<p>Object Detection (OD) has proven to be a significant computer vision method
in extracting localized class information and has multiple applications in the
industry. Although many of the state-of-the-art (SOTA) OD models perform well
on medium and large sized objects, they seem to under perform on small objects.
In most of the industrial use cases, it is difficult to collect and annotate
data for small objects, as it is time-consuming and prone to human errors.
Additionally, those datasets are likely to be unbalanced and often result in an
inefficient model convergence. To tackle this challenge, this study presents a
novel approach that injects additional data points to improve the performance
of the OD models. Using synthetic data generation, the difficulties in data
collection and annotations for small object data points can be minimized and to
create a dataset with balanced distribution. This paper discusses the effects
of a simple proportional class-balancing technique, to enable better anchor
matching of the OD models. A comparison was carried out on the performances of
the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and
synthetic datasets within an industrial use case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12900">PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting. (arXiv:2401.12900v3 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhongyuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1">Zhenyu Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1">Guoping Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kanglin Liu</a></p>
<p>Despite much progress, achieving real-time high-fidelity head avatar
animation is still difficult and existing methods have to trade-off between
speed and quality. 3DMM based methods often fail to model non-facial structures
such as eyeglasses and hairstyles, while neural implicit models suffer from
deformation inflexibility and rendering inefficiency. Although 3D Gaussian has
been demonstrated to possess promising capability for geometry representation
and radiance field reconstruction, applying 3D Gaussian in head avatar creation
remains a major challenge since it is difficult for 3D Gaussian to model the
head shape variations caused by changing poses and expressions. In this paper,
we introduce PSAvatar, a novel framework for animatable head avatar creation
that utilizes discrete geometric primitive to create a parametric morphable
shape model and employs 3D Gaussian for fine detail representation and high
fidelity rendering. The parametric morphable shape model is a Point-based
Morphable Shape Model (PMSM) which uses points instead of meshes for 3D
representation to achieve enhanced representation flexibility. The PMSM first
converts the FLAME mesh to points by sampling on the surfaces as well as off
the meshes to enable the reconstruction of not only surface-like structures but
also complex geometries such as eyeglasses and hairstyles. By aligning these
points with the head shape in an analysis-by-synthesis manner, the PMSM makes
it possible to utilize 3D Gaussian for fine detail representation and
appearance modeling, thus enabling the creation of high-fidelity avatars. We
show that PSAvatar can reconstruct high-fidelity head avatars of a variety of
subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a
resolution of 512 $\times$ 512 ).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12946">Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization. (arXiv:2401.12946v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zimeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhiyang Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Rui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Cheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xiaoxiao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1">Shiqing Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoming Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a></p>
<p>We introduce Coverage Axis++, a novel and efficient approach to 3D shape
skeletonization. The current state-of-the-art approaches for this task often
rely on the watertightness of the input or suffer from substantial
computational costs, thereby limiting their practicality. To address this
challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal
points, offering a high-accuracy approximation of the Medial Axis Transform
(MAT) while significantly mitigating computational intensity for various shape
representations. We introduce a simple yet effective strategy that considers
both shape coverage and uniformity to derive skeletal points. The selection
procedure enforces consistency with the shape structure while favoring the
dominant medial balls, which thus introduces a compact underlying shape
representation in terms of MAT. As a result, Coverage Axis++ allows for
skeletonization for various shape representations (e.g., water-tight meshes,
triangle soups, point clouds), specification of the number of skeletal points,
few hyperparameters, and highly efficient computation with improved
reconstruction accuracy. Extensive experiments across a wide range of 3D shapes
validate the efficiency and effectiveness of Coverage Axis++. The code will be
publicly available once the paper is published.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13329">Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval. (arXiv:2401.13329v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1">Dezhao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1">Shaogang Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiabo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hailin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Video Moment Retrieval (VMR) requires precise modelling of fine-grained
moment-text associations to capture intricate visual-language relationships.
Due to the lack of a diverse and generalisable VMR dataset to facilitate
learning scalable moment-text associations, existing methods resort to joint
training on both source and target domain videos for cross-domain applications.
Meanwhile, recent developments in vision-language multimodal models pre-trained
on large-scale image-text and/or video-text pairs are only based on coarse
associations (weakly labelled). They are inadequate to provide fine-grained
moment-text correlations required for cross-domain VMR. In this work, we solve
the problem of unseen cross-domain VMR, where certain visual and textual
concepts do not overlap across domains, by only utilising target domain
sentences (text prompts) without accessing their videos. To that end, we
explore generative video diffusion for fine-grained editing of source videos
controlled by the target sentences, enabling us to simulate target domain
videos. We address two problems in video editing for optimising unseen domain
VMR: (1) generation of high-quality simulation videos of different moments with
subtle distinctions, (2) selection of simulation videos that complement
existing source training videos without introducing harmful noise or
unnecessary repetitions. On the first problem, we formulate a two-stage video
diffusion generation controlled simultaneously by (1) the original video
structure of a source video, (2) subject specifics, and (3) a target sentence
prompt. This ensures fine-grained variations between video moments. On the
second problem, we introduce a hybrid selection mechanism that combines two
quantitative metrics for noise filtering and one qualitative metric for
leveraging VMR prediction on simulation video selection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13956">A New Image Quality Database for Multiple Industrial Processes. (arXiv:2401.13956v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xuanchao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yanlin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chengxu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1">Ke Gu</a></p>
<p>Recent years have witnessed a broader range of applications of image
processing technologies in multiple industrial processes, such as smoke
detection, security monitoring, and workpiece inspection. Different kinds of
distortion types and levels must be introduced into an image during the
processes of acquisition, compression, transmission, storage, and display,
which might heavily degrade the image quality and thus strongly reduce the
final display effect and clarity. To verify the reliability of existing image
quality assessment methods, we establish a new industrial process image
database (IPID), which contains 3000 distorted images generated by applying
different levels of distortion types to each of the 50 source images. We
conduct the subjective test on the aforementioned 3000 images to collect their
subjective quality ratings in a well-suited laboratory environment. Finally, we
perform comparison experiments on IPID database to investigate the performance
of some objective image quality assessment algorithms. The experimental results
show that the state-of-the-art image quality assessment methods have difficulty
in predicting the quality of images that contain multiple distortion types.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13998">WAL-Net: Weakly supervised auxiliary task learning network for carotid plaques classification. (arXiv:2401.13998v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gan_H/0/1/0/all/0/1">Haitao Gan</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_L/0/1/0/all/0/1">Lingchao Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1">Ran Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1">Weiyan Gan</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1">Furong Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xiaoyan Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zhi Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1">Zhongwei Huang</a></p>
<p>The classification of carotid artery ultrasound images is a crucial means for
diagnosing carotid plaques, holding significant clinical relevance for
predicting the risk of stroke. Recent research suggests that utilizing plaque
segmentation as an auxiliary task for classification can enhance performance by
leveraging the correlation between segmentation and classification tasks.
However, this approach relies on obtaining a substantial amount of
challenging-to-acquire segmentation annotations. This paper proposes a novel
weakly supervised auxiliary task learning network model (WAL-Net) to explore
the interdependence between carotid plaque classification and segmentation
tasks. The plaque classification task is primary task, while the plaque
segmentation task serves as an auxiliary task, providing valuable information
to enhance the performance of the primary task. Weakly supervised learning is
adopted in the auxiliary task to completely break away from the dependence on
segmentation annotations. Experiments and evaluations are conducted on a
dataset comprising 1270 carotid plaque ultrasound images from Wuhan University
Zhongnan Hospital. Results indicate that the proposed method achieved an
approximately 1.3% improvement in carotid plaque classification accuracy
compared to the baseline network. Specifically, the accuracy of mixed-echoic
plaques classification increased by approximately 3.3%, demonstrating the
effectiveness of our approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14132">Enabling Cross-Camera Collaboration for Video Analytics on Distributed Smart Cameras. (arXiv:2401.14132v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1">Chulhong Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1">Juheon Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Acer_U/0/1/0/all/0/1">Utku Gunay Acer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1">Fahim Kawsar</a></p>
<p>Overlapping cameras offer exciting opportunities to view a scene from
different angles, allowing for more advanced, comprehensive and robust
analysis. However, existing visual analytics systems for multi-camera streams
are mostly limited to (i) per-camera processing and aggregation and (ii)
workload-agnostic centralized processing architectures. In this paper, we
present Argus, a distributed video analytics system with cross-camera
collaboration on smart cameras. We identify multi-camera, multi-target tracking
as the primary task of multi-camera video analytics and develop a novel
technique that avoids redundant, processing-heavy identification tasks by
leveraging object-wise spatio-temporal association in the overlapping fields of
view across multiple cameras. We further develop a set of techniques to perform
these operations across distributed cameras without cloud support at low
latency by (i) dynamically ordering the camera and object inspection sequence
and (ii) flexibly distributing the workload across smart cameras, taking into
account network transmission and heterogeneous computational capacities.
Evaluation of three real-world overlapping camera datasets with two Nvidia
Jetson devices shows that Argus reduces the number of object identifications
and end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to
the state-of-the-art), while achieving comparable tracking quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14257">Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minglin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Weihao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yukun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1">Zhe Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yisheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zilong Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1">Liefeng Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yulan Guo</a></p>
<p>Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14403">Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Haoyu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendonca_R/0/1/0/all/0/1">Russell Mendonca</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1">Kenneth Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a></p>
<p>Deploying robots in open-ended unstructured environments such as homes has
been a long-standing research problem. However, robots are often studied only
in closed-off lab settings, and prior mobile manipulation work is restricted to
pick-move-place, which is arguably just the tip of the iceberg in this area. In
this paper, we introduce Open-World Mobile Manipulation System, a full-stack
approach to tackle realistic articulated object operation, e.g. real-world
doors, cabinets, drawers, and refrigerators in open-ended unstructured
environments. The robot utilizes an adaptive learning framework to initially
learns from a small set of data through behavior cloning, followed by learning
from online practice on novel objects that fall outside the training
distribution. We also develop a low-cost mobile manipulation hardware platform
capable of safe and autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we utilize 20 articulate
objects across 4 buildings in the CMU campus. With less than an hour of online
learning for each object, the system is able to increase success rate from 50%
of BC pre-training to 95% using online adaptation. Video results at
https://open-world-mobilemanip.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.15071">From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities. (arXiv:2401.15071v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chaochao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guodong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Hongxing Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hongzhi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jing Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jingyi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jinlan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kexin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kunchang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lijun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1">Lu Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Meiqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1">Qibing Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sirui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yali Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yan Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaru Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yinan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingchun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongting Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yujiong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_Y/0/1/0/all/0/1">Yurong Mou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zaibin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhelun Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhenfei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhipin Wang</a></p>
<p>Multi-modal Large Language Models (MLLMs) have shown impressive abilities in
generating reasonable responses with respect to multi-modal contents. However,
there is still a wide gap between the performance of recent MLLM-based
applications and the expectation of the broad public, even though the most
powerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper
strives to enhance understanding of the gap through the lens of a qualitative
study on the generalizability, trustworthiness, and causal reasoning
capabilities of recent proprietary and open-source MLLMs across four
modalities: ie, text, code, image, and video, ultimately aiming to improve the
transparency of MLLMs. We believe these properties are several representative
factors that define the reliability of MLLMs, in supporting various downstream
applications. To be specific, we evaluate the closed-source GPT-4 and Gemini
and 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed
cases, where the qualitative results are then summarized into 12 scores (ie, 4
modalities times 3 properties). In total, we uncover 14 empirical findings that
are useful to understand the capabilities and limitations of both proprietary
and open-source MLLMs, towards more reliable downstream multi-modal
applications.
</p>
</p>
</div>

    </div>
    </body>
    