<!DOCTYPE html>
<html>
<head>
<title>2023-08-26-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.12290">Integer Factorisation, Fermat &amp; Machine Learning on a Classical Computer. (arXiv:2308.12290v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blake_S/0/1/0/all/0/1">Sam Blake</a></p>
<p>In this paper we describe a deep learning--based probabilistic algorithm for
integer factorisation. We use Lawrence's extension of Fermat's factorisation
algorithm to reduce the integer factorisation problem to a binary
classification problem. To address the classification problem, based on the
ease of generating large pseudo--random primes, a corpus of training data, as
large as needed, is synthetically generated. We will introduce the algorithm,
summarise some experiments, analyse where these experiments fall short, and
finally put out a call to others to reproduce, verify and see if this approach
can be improved to a point where it becomes a practical, scalable factorisation
algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12299">Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization. (arXiv:2308.12299v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1">Xing-Yu Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Hao_S/0/1/0/all/0/1">Shaogang Hao</a></p>
<p>As the feature size of integrated circuits continues to decrease, optical
proximity correction (OPC) has emerged as a crucial resolution enhancement
technology for ensuring high printability in the lithography process. Recently,
level set-based inverse lithography technology (ILT) has drawn considerable
attention as a promising OPC solution, showcasing its powerful pattern
fidelity, especially in advanced process. However, massive computational time
consumption of ILT limits its applicability to mainly correcting partial layers
and hotspot regions. Deep learning (DL) methods have shown great potential in
accelerating ILT. However, lack of domain knowledge of inverse lithography
limits the ability of DL-based algorithms in process window (PW) enhancement
and etc. In this paper, we propose an inverse lithography physics-informed deep
neural level set (ILDLS) approach for mask optimization. This approach utilizes
level set based-ILT as a layer within the DL framework and iteratively conducts
mask prediction and correction to significantly enhance printability and PW in
comparison with results from pure DL and ILT. With this approach, computation
time is reduced by a few orders of magnitude versus ILT. By gearing up DL with
knowledge of inverse lithography physics, ILDLS provides a new and efficient
mask optimization solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12304">Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes. (arXiv:2308.12304v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Magner_A/0/1/0/all/0/1">Abram Magner</a>, <a href="http://arxiv.org/find/stat/1/au:+Padakandla_A/0/1/0/all/0/1">Arun Padakandla</a></p>
<p>We characterize learnability for quantum measurement classes by establishing
matching necessary and sufficient conditions for their PAC learnability, along
with corresponding sample complexity bounds, in the setting where the learner
is given access only to prepared quantum states. We first probe the results
from previous works on this setting. We show that the empirical risk defined in
previous works and matching the definition in the classical theory fails to
satisfy the uniform convergence property enjoyed in the classical setting for
some learnable classes. Moreover, we show that VC dimension generalization
upper bounds in previous work are frequently infinite, even for
finite-dimensional POVM classes. To surmount the failure of the standard ERM to
satisfy uniform convergence, we define a new learning rule -- denoised ERM. We
show this to be a universal learning rule for POVM and probabilistically
observed concept classes, and the condition for it to satisfy uniform
convergence is finite fat shattering dimension of the class. We give
quantitative sample complexity upper and lower bounds for learnability in terms
of finite fat-shattering dimension and a notion of approximate finite
partitionability into approximately jointly measurable subsets, which allow for
sample reuse. We then show that finite fat shattering dimension implies finite
coverability by approximately jointly measurable subsets, leading to our
matching conditions. We also show that every measurement class defined on a
finite-dimensional Hilbert space is PAC learnable. We illustrate our results on
several example POVM classes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12305">FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haokun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Krompass_D/0/1/0/all/0/1">Denis Krompass</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a></p>
<p>Recently, foundation models have exhibited remarkable advancements in
multi-modal learning. These models, equipped with millions (or billions) of
parameters, typically require a substantial amount of data for finetuning.
However, collecting and centralizing training data from diverse sectors becomes
challenging due to distinct privacy regulations. Federated Learning (FL)
emerges as a promising solution, enabling multiple clients to collaboratively
train neural networks without centralizing their local data. To alleviate
client computation burdens and communication overheads, previous works have
adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a
small fraction of the model parameters are optimized and communicated during
federated communications. Nevertheless, most previous works have focused on a
single modality and neglected one common phenomenon, i.e., the presence of data
heterogeneity across the clients. Therefore, in this work, we propose a
finetuning framework tailored to heterogeneous multi-modal FL, called Federated
Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a
Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the
client local updates and applying Mutual Knowledge Distillation (MKD) for an
efficient knowledge transfer. FedDAT is the first approach that enables an
efficient distributed finetuning of foundation models for a variety of
heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we
conduct extensive experiments on four multi-modality FL benchmarks with
different types of data heterogeneity, where FedDAT substantially outperforms
the existing centralized PEFT methods adapted for FL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12311">Fast Exact NPN Classification with Influence-aided Canonical Form. (arXiv:2308.12311v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yonghe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1">Liwei Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1">Guojie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shenggen Zheng</a></p>
<p>NPN classification has many applications in the synthesis and verification of
digital circuits. The canonical-form-based method is the most common approach,
designing a canonical form as representative for the NPN equivalence class
first and then computing the transformation function according to the canonical
form. Most works use variable symmetries and several signatures, mainly based
on the cofactor, to simplify the canonical form construction and computation.
This paper describes a novel canonical form and its computation algorithm by
introducing Boolean influence to NPN classification, which is a basic concept
in analysis of Boolean functions. We show that influence is
input-negation-independent, input-permutation-dependent, and has other
structural information than previous signatures for NPN classification.
Therefore, it is a significant ingredient in speeding up NPN classification.
Experimental results prove that influence plays an important role in reducing
the transformation enumeration in computing the canonical form. Compared with
the state-of-the-art algorithm implemented in ABC, our influence-aided
canonical form for exact NPN classification gains up to 5.5x speedup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12315">Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Ronghang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Dongliang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1">Daiqing Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a></p>
<p>As AI systems have obtained significant performance to be deployed widely in
our daily live and human society, people both enjoy the benefits brought by
these technologies and suffer many social issues induced by these systems. To
make AI systems good enough and trustworthy, plenty of researches have been
done to build guidelines for trustworthy AI systems. Machine learning is one of
the most important parts for AI systems and representation learning is the
fundamental technology in machine learning. How to make the representation
learning trustworthy in real-world application, e.g., cross domain scenarios,
is very valuable and necessary for both machine learning and AI system fields.
Inspired by the concepts in trustworthy AI, we proposed the first trustworthy
representation learning across domains framework which includes four concepts,
i.e, robustness, privacy, fairness, and explainability, to give a comprehensive
literature review on this research direction. Specifically, we first introduce
the details of the proposed trustworthy framework for representation learning
across domains. Second, we provide basic notions and comprehensively summarize
existing methods for the trustworthy framework from four concepts. Finally, we
conclude this survey with insights and discussions on future research
directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12316">Graph Neural Stochastic Differential Equations. (arXiv:2308.12316v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bergna_R/0/1/0/all/0/1">Richard Bergna</a>, <a href="http://arxiv.org/find/cs/1/au:+Opolka_F/0/1/0/all/0/1">Felix Opolka</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1">Jose Miguel Hernandez-Lobato</a></p>
<p>We present a novel model Graph Neural Stochastic Differential Equations
(Graph Neural SDEs). This technique enhances the Graph Neural Ordinary
Differential Equations (Graph Neural ODEs) by embedding randomness into data
representation using Brownian motion. This inclusion allows for the assessment
of prediction uncertainty, a crucial aspect frequently missed in current
models. In our framework, we spotlight the \textit{Latent Graph Neural SDE}
variant, demonstrating its effectiveness. Through empirical studies, we find
that Latent Graph Neural SDEs surpass conventional models like Graph
Convolutional Networks and Graph Neural ODEs, especially in confidence
prediction, making them superior in handling out-of-distribution detection
across both static and spatio-temporal contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12325">Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network. (arXiv:2308.12325v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Ho_J/0/1/0/all/0/1">John Ho</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yin_Z/0/1/0/all/0/1">Zhao-Heng Yin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1">Colin Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Overhauser_H/0/1/0/all/0/1">Henry Overhauser</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Swanson_K/0/1/0/all/0/1">Kyle Swanson</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ha_Y/0/1/0/all/0/1">Yang Ha</a></p>
<p>Predicting the solubility of given molecules is an important task in the
pharmaceutical industry, and consequently this is a well-studied topic. In this
research, we revisited this problem with the advantage of modern computing
resources. We applied two machine learning models, a linear regression model
and a graph convolutional neural network model, on multiple experimental
datasets. Both methods can make reasonable predictions while the GCNN model had
the best performance. However, the current GCNN model is a black box, while
feature importance analysis from the linear regression model offers more
insights into the underlying chemical influences. Using the linear regression
model, we show how each functional group affects the overall solubility.
Ultimately, knowing how chemical structure influences chemical properties is
crucial when designing new drugs. Future work should aim to combine the high
performance of GCNNs with the interpretability of linear regression, unlocking
new advances in next generation high throughput screening.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12351">Improving Generative Model-based Unfolding with Schr\&quot;{o}dinger Bridges. (arXiv:2308.12351v1 [hep-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Diefenbacher_S/0/1/0/all/0/1">Sascha Diefenbacher</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Liu_G/0/1/0/all/0/1">Guan-Horng Liu</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Mikuni_V/0/1/0/all/0/1">Vinicius Mikuni</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Nachman_B/0/1/0/all/0/1">Benjamin Nachman</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Nie_W/0/1/0/all/0/1">Weili Nie</a></p>
<p>Machine learning-based unfolding has enabled unbinned and high-dimensional
differential cross section measurements. Two main approaches have emerged in
this research area: one based on discriminative models and one based on
generative models. The main advantage of discriminative models is that they
learn a small correction to a starting simulation while generative models scale
better to regions of phase space with little data. We propose to use
Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding
approach that combines the strengths of both discriminative and generative
models. The key feature of SBUnfold is that its generative model maps one set
of events into another without having to go through a known probability density
as is the case for normalizing flows and standard diffusion models. We show
that SBUnfold achieves excellent performance compared to state of the art
methods on a synthetic Z+jets dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12354">Machine Learning Small Molecule Properties in Drug Discovery. (arXiv:2308.12354v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Schapin_N/0/1/0/all/0/1">Nikolai Schapin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Majewski_M/0/1/0/all/0/1">Maciej Majewski</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Varela_A/0/1/0/all/0/1">Alejandro Varela</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Arroniz_C/0/1/0/all/0/1">Carlos Arroniz</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fabritiis_G/0/1/0/all/0/1">Gianni De Fabritiis</a></p>
<p>Machine learning (ML) is a promising approach for predicting small molecule
properties in drug discovery. Here, we provide a comprehensive overview of
various ML methods introduced for this purpose in recent years. We review a
wide range of properties, including binding affinities, solubility, and ADMET
(Absorption, Distribution, Metabolism, Excretion, and Toxicity). We discuss
existing popular datasets and molecular descriptors and embeddings, such as
chemical fingerprints and graph-based neural networks. We highlight also
challenges of predicting and optimizing multiple properties during hit-to-lead
and lead optimization stages of drug discovery and explore briefly possible
multi-objective optimization techniques that can be used to balance diverse
properties while optimizing lead candidates. Finally, techniques to provide an
understanding of model predictions, especially for critical decision-making in
drug discovery are assessed. Overall, this review provides insights into the
landscape of ML models for small molecule property predictions in drug
discovery. So far, there are multiple diverse approaches, but their
performances are often comparable. Neural networks, while more flexible, do not
always outperform simpler models. This shows that the availability of
high-quality training data remains crucial for training accurate models and
there is a need for standardized benchmarks, additional performance metrics,
and best practices to enable richer comparisons between the different
techniques and models that can shed a better light on the differences between
the many techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12355">Renormalizing Diffusion Models. (arXiv:2308.12355v1 [hep-th])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-th/1/au:+Cotler_J/0/1/0/all/0/1">Jordan Cotler</a>, <a href="http://arxiv.org/find/hep-th/1/au:+Rezchikov_S/0/1/0/all/0/1">Semon Rezchikov</a></p>
<p>We explain how to use diffusion models to learn inverse renormalization group
flows of statistical and quantum field theories. Diffusion models are a class
of machine learning models which have been used to generate samples from
complex distributions, such as the distribution of natural images, by learning
the inverse process to a diffusion process which adds noise to the data until
the distribution of the data is pure noise. Nonperturbative renormalization
group schemes can naturally be written as diffusion processes in the space of
fields. We combine these observations in a concrete framework for building
ML-based models for studying field theories, in which the models learn the
inverse process to an explicitly-specified renormalization group scheme. We
detail how these models define a class of adaptive bridge (or parallel
tempering) samplers for lattice field theory. Because renormalization group
schemes have a physical meaning, we provide explicit prescriptions for how to
compare results derived from models associated to several different
renormalization group schemes of interest. We also explain how to use diffusion
models in a variational method to find ground states of quantum systems. We
apply some of our methods to numerically find RG flows of interacting
statistical field theories. From the perspective of machine learning, our work
provides an interpretation of multiscale diffusion models, and gives
physically-inspired suggestions for diffusion models which should have novel
properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12367">SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haochen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1">Shubham Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Patra_S/0/1/0/all/0/1">Sunandita Patra</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1">Sriram Gopalakrishnan</a></p>
<p>With the growing use of machine learning (ML) models in critical domains such
as finance and healthcare, the need to offer recourse for those adversely
affected by the decisions of ML models has become more important; individuals
ought to be provided with recommendations on actions to take for improving
their situation and thus receive a favorable decision. Prior work on sequential
algorithmic recourse -- which recommends a series of changes -- focuses on
action feasibility and uses the proximity of feature changes to determine
action costs. However, the uncertainties of feature changes and the risk of
higher than average costs in recourse have not been considered. It is
undesirable if a recourse could (with some probability) result in a worse
situation from which recovery requires an extremely high cost. It is essential
to incorporate risks when computing and evaluating recourse. We call the
recourse computed with such risk considerations as Safer Algorithmic Recourse
(SafeAR). The objective is to empower people to choose a recourse based on
their risk tolerance. In this work, we discuss and show how existing recourse
desiderata can fail to capture the risk of higher costs. We present a method to
compute recourse policies that consider variability in cost and connect
algorithmic recourse literature with risk-sensitive reinforcement learning. We
also adopt measures ``Value at Risk'' and ``Conditional Value at Risk'' from
the financial literature to summarize risk concisely. We apply our method to
two real-world datasets and compare policies with different levels of
risk-aversion using risk measures and recourse desiderata (sparsity and
proximity).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12371">Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation. (arXiv:2308.12371v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1">Rafael Henrique Vareto</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1">Manuel G&#xfc;nther</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1">William Robson Schwartz</a></p>
<p>Open-set face recognition refers to a scenario in which biometric systems
have incomplete knowledge of all existing subjects. Therefore, they are
expected to prevent face samples of unregistered subjects from being identified
as previously enrolled identities. This watchlist context adds an arduous
requirement that calls for the dismissal of irrelevant faces by focusing mainly
on subjects of interest. As a response, this work introduces a novel method
that associates an ensemble of compact neural networks with a margin-based cost
function that explores additional samples. Supplementary negative samples can
be obtained from external databases or synthetically built at the
representation level in training time with a new mix-up feature augmentation
approach. Deep neural networks pre-trained on large face datasets serve as the
preliminary feature extraction module. We carry out experiments on well-known
LFW and IJB-C datasets where results show that the approach is able to boost
closed and open-set identification rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12381">Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krstovski_K/0/1/0/all/0/1">Kriste Krstovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Ye Xu</a></p>
<p>A person's gender is a crucial piece of information when performing research
across a wide range of scientific disciplines, such as medicine, sociology,
political science, and economics, to name a few. However, in increasing
instances, especially given the proliferation of big data, gender information
is not readily available. In such cases researchers need to infer gender from
readily available information, primarily from persons' names. While inferring
gender from name may raise some ethical questions, the lack of viable
alternatives means that researchers have to resort to such approaches when the
goal justifies the means - in the majority of such studies the goal is to
examine patterns and determinants of gender disparities. The necessity of
name-to-gender inference has generated an ever-growing domain of algorithmic
approaches and software products. These approaches have been used throughout
the world in academia, industry, governmental and non-governmental
organizations. Nevertheless, the existing approaches have yet to be
systematically evaluated and compared, making it challenging to determine the
optimal approach for future research. In this work, we conducted a large scale
performance evaluation of existing approaches for name-to-gender inference.
Analysis are performed using a variety of large annotated datasets of names. We
further propose two new hybrid approaches that achieve better performance than
any single existing approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12388">FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_O/0/1/0/all/0/1">Ou Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qun Jin</a></p>
<p>In data imputation, effectively addressing missing values is pivotal,
especially in intricate datasets. This paper delves into the FIML Optimized
Self-attention (FOSA) framework, an innovative approach that amalgamates the
strengths of Full Information Maximum Likelihood (FIML) estimation with the
capabilities of self-attention neural networks. Our methodology commences with
an initial estimation of missing values via FIML, subsequently refining these
estimates by leveraging the self-attention mechanism. Our comprehensive
experiments on both simulated and real-world datasets underscore FOSA's
pronounced advantages over traditional FIML techniques, encapsulating facets of
accuracy, computational efficiency, and adaptability to diverse data
structures. Intriguingly, even in scenarios where the Structural Equation Model
(SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust
architecture of FOSA's self-attention component adeptly rectifies and optimizes
the imputation outcomes. Our empirical tests reveal that FOSA consistently
delivers commendable predictions, even in the face of up to 40% random
missingness, highlighting its robustness and potential for wide-scale
applications in data imputation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12393">Machine learning in parameter estimation of nonlinear systems. (arXiv:2308.12393v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1">Kaushal Kumar</a></p>
<p>Accurately estimating parameters in complex nonlinear systems is crucial
across scientific and engineering fields. We present a novel approach for
parameter estimation using a neural network with the Huber loss function. This
method taps into deep learning's abilities to uncover parameters governing
intricate behaviors in nonlinear equations. We validate our approach using
synthetic data and predefined functions that model system dynamics. By training
the neural network with noisy time series data, it fine-tunes the Huber loss
function to converge to accurate parameters. We apply our method to damped
oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz
systems under multiplicative noise. The trained neural network accurately
estimates parameters, evident from closely matching latent dynamics. Comparing
true and estimated trajectories visually reinforces our method's precision and
robustness. Our study underscores the Huber loss-guided neural network as a
versatile tool for parameter estimation, effectively uncovering complex
relationships in nonlinear systems. The method navigates noise and uncertainty
adeptly, showcasing its adaptability to real-world challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12420">Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hernandez_W/0/1/0/all/0/1">Walter Hernandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tylinski_K/0/1/0/all/0/1">Kamil Tylinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_A/0/1/0/all/0/1">Alastair Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Roche_N/0/1/0/all/0/1">Niall Roche</a>, <a href="http://arxiv.org/find/cs/1/au:+Vadgama_N/0/1/0/all/0/1">Nikhil Vadgama</a>, <a href="http://arxiv.org/find/cs/1/au:+Treiblmaier_H/0/1/0/all/0/1">Horst Treiblmaier</a>, <a href="http://arxiv.org/find/cs/1/au:+Shangguan_J/0/1/0/all/0/1">Jiangbo Shangguan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tasca_P/0/1/0/all/0/1">Paolo Tasca</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiahua Xu</a></p>
<p>Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating
comprehensive insights into their diverse components. However, a systematic
literature review that emphasizes the Environmental, Sustainability, and
Governance (ESG) components of DLT remains lacking. To bridge this gap, we
selected 107 seed papers to build a citation network of 63,083 references and
refined it to a corpus of 24,539 publications for analysis. Then, we labeled
the named entities in 46 papers according to twelve top-level categories
derived from an established technology taxonomy and enhanced the taxonomy by
pinpointing DLT's ESG elements. Leveraging transformer-based language models,
we fine-tuned a pre-trained language model for a Named Entity Recognition (NER)
task using our labeled dataset. We used our fine-tuned language model to
distill the corpus to 505 key papers, facilitating a literature review via
named entities and temporal graph analysis on DLT evolution in the context of
ESG. Our contributions are a methodology to conduct a machine learning-driven
systematic literature review in the DLT field, placing a special emphasis on
ESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed
of 54,808 named entities, designed for DLT and ESG-related explorations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12438">Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges. (arXiv:2308.12438v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yahmed_A/0/1/0/all/0/1">Ahmed Haj Yahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbassi_A/0/1/0/all/0/1">Altaf Allah Abbassi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1">Amin Nikanjam</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1">Foutse Khomh</a></p>
<p>Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in
reinforcement learning, has shown significant potential in achieving
human-level autonomy in a wide range of domains, including robotics, computer
vision, and computer games. This potential justifies the enthusiasm and growing
interest in DRL in both academia and industry. However, the community currently
focuses mostly on the development phase of DRL systems, with little attention
devoted to DRL deployment. In this paper, we propose an empirical study on
Stack Overflow (SO), the most popular Q&amp;A forum for developers, to uncover and
understand the challenges practitioners faced when deploying DRL systems.
Specifically, we categorized relevant SO posts by deployment platforms:
server/cloud, mobile/embedded system, browser, and game engine. After filtering
and manual analysis, we examined 357 SO posts about DRL deployment,
investigated the current state, and identified the challenges related to
deploying DRL systems. Then, we investigate the prevalence and difficulty of
these challenges. Results show that the general interest in DRL deployment is
growing, confirming the study's relevance and importance. Results also show
that DRL deployment is more difficult than other DRL issues. Additionally, we
built a taxonomy of 31 unique challenges in deploying DRL to different
platforms. On all platforms, RL environment-related challenges are the most
popular, and communication-related challenges are the most difficult among
practitioners. We hope our study inspires future research and helps the
community overcome the most common and difficult challenges practitioners face
when deploying DRL systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12439">BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tinghao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiangyu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Ping He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a></p>
<p>We present a novel defense, against backdoor attacks on Deep Neural Networks
(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)
into DNNs. Our defense falls within the category of post-development defenses
that operate independently of how the model was generated. The proposed defense
is built upon a novel reverse engineering approach that can directly extract
backdoor functionality of a given backdoored model to a backdoor expert model.
The approach is straightforward -- finetuning the backdoored model over a small
set of intentionally mislabeled clean samples, such that it unlearns the normal
functionality while still preserving the backdoor functionality, and thus
resulting in a model (dubbed a backdoor expert model) that can only recognize
backdoor inputs. Based on the extracted backdoor expert model, we show the
feasibility of devising highly accurate backdoor input detectors that filter
out the backdoor inputs during model inference. Further augmented by an
ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert
(Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA
backdoor attacks while minimally impacting clean utility. The effectiveness of
BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)
across various model architectures (ResNet, VGG, MobileNetV2 and Vision
Transformer).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12443">TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction. (arXiv:2308.12443v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1">Xueqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1">Luyao Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiongchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Qiong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1">Huidong Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yi-Hwa Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Palyo_R/0/1/0/all/0/1">Richard Palyo</a>, <a href="http://arxiv.org/find/eess/1/au:+Miller_E/0/1/0/all/0/1">Edward J. Miller</a>, <a href="http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1">Albert J. Sinusas</a>, <a href="http://arxiv.org/find/eess/1/au:+Spottiswoode_B/0/1/0/all/0/1">Bruce Spottiswoode</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dvornek_N/0/1/0/all/0/1">Nicha C. Dvornek</a></p>
<p>The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of
cross-frame distribution in dynamic cardiac positron emission tomography (PET)
raise significant challenges for inter-frame motion correction, particularly
for the early frames where conventional intensity-based image registration
techniques are not applicable. Alternatively, a promising approach utilizes
generative methods to handle the tracer distribution changes to assist existing
registration methods. To improve frame-wise registration and parametric
quantification, we propose a Temporally and Anatomically Informed Generative
Adversarial Network (TAI-GAN) to transform the early frames into the late
reference frame using an all-to-one mapping. Specifically, a feature-wise
linear modulation layer encodes channel-wise parameters generated from temporal
tracer kinetics information, and rough cardiac segmentations with local shifts
serve as the anatomical information. We validated our proposed method on a
clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted
early frames with high image quality, comparable to the real reference frames.
After TAI-GAN conversion, motion estimation accuracy and clinical myocardial
blood flow (MBF) quantification were improved compared to using the original
frames. Our code is published at https://github.com/gxq1998/TAI-GAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12445">An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems. (arXiv:2308.12445v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yahmed_A/0/1/0/all/0/1">Ahmed Haj Yahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchoucha_R/0/1/0/all/0/1">Rached Bouchoucha</a>, <a href="http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1">Houssem Ben Braiek</a>, <a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1">Foutse Khomh</a></p>
<p>Deep reinforcement learning (DRL) is increasingly applied in large-scale
productions like Netflix and Facebook. As with most data-driven systems, DRL
systems can exhibit undesirable behaviors due to environmental drifts, which
often occur in constantly-changing production settings. Continual Learning (CL)
is the inherent self-healing approach for adapting the DRL agent in response to
the environment's conditions shifts. However, successive shifts of considerable
magnitude may cause the production environment to drift from its original
state. Recent studies have shown that these environmental drifts tend to drive
CL into long, or even unsuccessful, healing cycles, which arise from
inefficiencies such as catastrophic forgetting, warm-starting failure, and slow
convergence. In this paper, we propose Dr. DRL, an effective self-healing
approach for DRL systems that integrates a novel mechanism of intentional
forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately
erases the DRL system's minor behaviors to systematically prioritize the
adaptation of the key problem-solving skills. Using well-established DRL
algorithms, Dr. DRL is compared with vanilla CL on various drifted
environments. Dr. DRL is able to reduce, on average, the healing time and
fine-tuning episodes by, respectively, 18.74% and 17.72%. Dr. DRL successfully
helps agents to adapt to 19.63% of drifted environments left unsolved by
vanilla CL while maintaining and even enhancing by up to 45% the obtained
rewards for drifted environments that are resolved by both approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12453">Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sagers_L/0/1/0/all/0/1">Luke W. Sagers</a>, <a href="http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1">James A. Diao</a>, <a href="http://arxiv.org/find/cs/1/au:+Melas_Kyriazi_L/0/1/0/all/0/1">Luke Melas-Kyriazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1">Matthew Groh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1">Pranav Rajpurkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Adamson_A/0/1/0/all/0/1">Adewole S. Adamson</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotemberg_V/0/1/0/all/0/1">Veronica Rotemberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1">Roxana Daneshjou</a>, <a href="http://arxiv.org/find/cs/1/au:+Manrai_A/0/1/0/all/0/1">Arjun K. Manrai</a></p>
<p>While hundreds of artificial intelligence (AI) algorithms are now approved or
cleared by the US Food and Drugs Administration (FDA), many studies have shown
inconsistent generalization or latent bias, particularly for underrepresented
populations. Some have proposed that generative AI could reduce the need for
real data, but its utility in model development remains unclear. Skin disease
serves as a useful case study in synthetic image generation due to the
diversity of disease appearance, particularly across the protected attribute of
skin tone. Here we show that latent diffusion models can scalably generate
images of skin disease and that augmenting model training with these data
improves performance in data-limited settings. These performance gains saturate
at synthetic-to-real image ratios above 10:1 and are substantially smaller than
the gains obtained from adding real images. As part of our analysis, we
generate and analyze a new dataset of 458,920 synthetic images produced using
several generation strategies. Our results suggest that synthetic data could
serve as a force-multiplier for model development, but the collection of
diverse real-world data remains the most important step to improve medical AI
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12454">PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wijesinghe_A/0/1/0/all/0/1">Achintha Wijesinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zhi Ding</a></p>
<p>Recent advances of generative learning models are accompanied by the growing
interest in federated learning (FL) based on generative adversarial network
(GAN) models. In the context of FL, GAN can capture the underlying client data
structure, and regenerate samples resembling the original data distribution
without compromising the private raw data. Although most existing GAN-based FL
works focus on training a global model, Personalized FL (PFL) sometimes can be
more effective in view of client data heterogeneity in terms of distinct data
sample distributions, feature spaces, and labels. To cope with client
heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation
strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in
different scenarios. More specially, we first learn the similarity among
clients and then develop an weighted collaborative data aggregation. The
empirical results through the rigorous experimentation on several well-known
datasets demonstrate the effectiveness of PFL-GAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12459">Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series. (arXiv:2308.12459v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ruiz_Moreno_E/0/1/0/all/0/1">Emilio Ruiz-Moreno</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_Ramos_L/0/1/0/all/0/1">Luis Miguel L&#xf3;pez-Ramos</a>, <a href="http://arxiv.org/find/eess/1/au:+Beferull_Lozano_B/0/1/0/all/0/1">Baltasar Beferull-Lozano</a></p>
<p>Digitalizing real-world analog signals typically involves sampling in time
and discretizing in amplitude. Subsequent signal reconstructions inevitably
incur an error that depends on the amplitude resolution and the temporal
density of the acquired samples. From an implementation viewpoint, consistent
signal reconstruction methods have proven a profitable error-rate decay as the
sampling rate increases. Despite that, these results are obtained under offline
settings. Therefore, a research gap exists regarding methods for consistent
signal reconstruction from data streams. This paper presents a method that
consistently reconstructs streamed multivariate time series of quantization
intervals under a zero-delay response requirement. On the other hand, previous
work has shown that the temporal dependencies within univariate time series can
be exploited to reduce the roughness of zero-delay signal reconstructions. This
work shows that the spatiotemporal dependencies within multivariate time series
can also be exploited to achieve improved results. Specifically, the
spatiotemporal dependencies of the multivariate time series are learned, with
the assistance of a recurrent neural network, to reduce the roughness of the
signal reconstruction on average while ensuring consistency. Our experiments
show that our proposed method achieves a favorable error-rate decay with the
sampling rate compared to a similar but non-consistent reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12481">Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices. (arXiv:2308.12481v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1">Hannah Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1">Allison Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Buer_C/0/1/0/all/0/1">Celine Buer</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1">Emily Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_K/0/1/0/all/0/1">Kayleen Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gong_L/0/1/0/all/0/1">Lauryn Gong</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1">Zhiqi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1">Jianbin Tang</a></p>
<p>This paper presents a cost-effective, low-power approach to unintentional
fall detection using knowledge distillation-based LSTM (Long Short-Term Memory)
models to significantly improve accuracy. With a primary focus on analyzing
time-series data collected from various sensors, the solution offers real-time
detection capabilities, ensuring prompt and reliable identification of falls.
The authors investigate fall detection models that are based on different
sensors, comparing their accuracy rates and performance. Furthermore, they
employ the technique of knowledge distillation to enhance the models'
precision, resulting in refined accurate configurations that consume lower
power. As a result, this proposed solution presents a compelling avenue for the
development of energy-efficient fall detection systems for future advancements
in this critical domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12492">Optimizing Neural Network Scale for ECG Classification. (arXiv:2308.12492v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byeong Tak Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1">Yong-Yeon Jo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1">Joon-Myoung Kwon</a></p>
<p>We study scaling convolutional neural networks (CNNs), specifically targeting
Residual neural networks (ResNet), for analyzing electrocardiograms (ECGs).
Although ECG signals are time-series data, CNN-based models have been shown to
outperform other neural networks with different architectures in ECG analysis.
However, most previous studies in ECG analysis have overlooked the importance
of network scaling optimization, which significantly improves performance. We
explored and demonstrated an efficient approach to scale ResNet by examining
the effects of crucial parameters, including layer depth, the number of
channels, and the convolution kernel size. Through extensive experiments, we
found that a shallower network, a larger number of channels, and smaller kernel
sizes result in better performance for ECG classifications. The optimal network
scale might differ depending on the target task, but our findings provide
insight into obtaining more efficient and accurate models with fewer computing
resources or less time. In practice, we demonstrate that a narrower search
space based on our findings leads to higher performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12497">False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations. (arXiv:2308.12497v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1">Mohammad Majid Akhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Masood_R/0/1/0/all/0/1">Rahat Masood</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikram_M/0/1/0/all/0/1">Muhammad Ikram</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanhere_S/0/1/0/all/0/1">Salil S. Kanhere</a></p>
<p>The rapid spread of false information and persistent manipulation attacks on
online social networks (OSNs), often for political, ideological, or financial
gain, has affected the openness of OSNs. While researchers from various
disciplines have investigated different manipulation-triggering elements of
OSNs (such as understanding information diffusion on OSNs or detecting
automated behavior of accounts), these works have not been consolidated to
present a comprehensive overview of the interconnections among these elements.
Notably, user psychology, the prevalence of bots, and their tactics in relation
to false information detection have been overlooked in previous research. To
address this research gap, this paper synthesizes insights from various
disciplines to provide a comprehensive analysis of the manipulation landscape.
By integrating the primary elements of social media manipulation (SMM),
including false information, bots, and malicious campaigns, we extensively
examine each SMM element. Through a systematic investigation of prior research,
we identify commonalities, highlight existing gaps, and extract valuable
insights in the field. Our findings underscore the urgent need for
interdisciplinary research to effectively combat social media manipulations,
and our systematization can guide future research efforts and assist OSN
providers in ensuring the safety and integrity of their platforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12510">Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1">Jiang-Tian Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xialei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1">Andrew D. Bagdanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a></p>
<p>Class Incremental Learning (CIL) aims to sequentially learn new classes while
avoiding catastrophic forgetting of previous knowledge. We propose to use
Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally
designed to learn useful representations through reconstructive unsupervised
learning, and they can be easily integrated with a supervised loss for
classification. Moreover, MAEs can reliably reconstruct original input images
from randomly selected patches, which we use to store exemplars from past tasks
more efficiently for CIL. We also propose a bilateral MAE framework to learn
from image-level and embedding-level fusion, which produces better-quality
reconstructed images and more stable representations. Our experiments confirm
that our approach performs better than the state-of-the-art on CIFAR-100,
ImageNet-Subset, and ImageNet-Full. The code is available at
https://github.com/scok30/MAE-CIL .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12517">Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1">Hyunsik Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jeonghyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jinhyeok Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1">Gwanghyeon Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1">Moonkyu Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Youm_D/0/1/0/all/0/1">Donghoon Youm</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1">Jemin Hwangbo</a></p>
<p>Several earlier studies have shown impressive control performance in complex
robotic systems by designing the controller using a neural network and training
it with model-free reinforcement learning. However, these outstanding
controllers with natural motion style and high task performance are developed
through extensive reward engineering, which is a highly laborious and
time-consuming process of designing numerous reward terms and determining
suitable reward coefficients. In this work, we propose a novel reinforcement
learning framework for training neural network controllers for complex robotic
systems consisting of both rewards and constraints. To let the engineers
appropriately reflect their intent to constraints and handle them with minimal
computation overhead, two constraint types and an efficient policy optimization
algorithm are suggested. The learning framework is applied to train locomotion
controllers for several legged robots with different morphology and physical
attributes to traverse challenging terrains. Extensive simulation and
real-world experiments demonstrate that performant controllers can be trained
with significantly less reward engineering, by tuning only a single reward
coefficient. Furthermore, a more straightforward and intuitive engineering
process can be utilized, thanks to the interpretability and generalizability of
constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12526">UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023. (arXiv:2308.12526v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yajun Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1">Chuanying Niu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhan_Y/0/1/0/all/0/1">Yibin Zhan</a>, <a href="http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1">Yanhua Long</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1">Dongxing Xu</a></p>
<p>This report describes the UNISOUND submission for Track1 and Track2 of
VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC 2023). We submit the same
system on Track 1 and Track 2, which is trained with only VoxCeleb2-dev.
Large-scale ResNet and RepVGG architectures are developed for the challenge. We
propose a consistency-aware score calibration method, which leverages the
stability of audio voiceprints in similarity score by a Consistency Measure
Factor (CMF). CMF brings a huge performance boost in this challenge. Our final
system is a fusion of six models and achieves the first place in Track 1 and
second place in Track 2 of VoxSRC 2023. The minDCF of our submission is 0.0855
and the EER is 1.5880%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12530">SieveNet: Selecting Point-Based Features for Mesh Networks. (arXiv:2308.12530v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shengchao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1">Yishun Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1">Rui Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bingbing Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhong Zheng</a></p>
<p>Meshes are widely used in 3D computer vision and graphics, but their
irregular topology poses challenges in applying them to existing neural network
architectures. Recent advances in mesh neural networks turn to remeshing and
push the boundary of pioneer methods that solely take the raw meshes as input.
Although the remeshing offers a regular topology that significantly facilitates
the design of mesh network architectures, features extracted from such remeshed
proxies may struggle to retain the underlying geometry faithfully, limiting the
subsequent neural network's capacity. To address this issue, we propose
SieveNet, a novel paradigm that takes into account both the regular topology
and the exact geometry. Specifically, this method utilizes structured mesh
topology from remeshing and accurate geometric information from
distortion-aware point sampling on the surface of the original mesh.
Furthermore, our method eliminates the need for hand-crafted feature
engineering and can leverage off-the-shelf network architectures such as the
vision transformer. Comprehensive experimental results on classification and
segmentation tasks well demonstrate the effectiveness and superiority of our
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12532">FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minchan Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangmook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12539">CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vipul Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1">Pranav Narayanan Venkit</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1">Hugo Lauren&#xe7;on</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1">Shomir Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1">Rebecca J. Passonneau</a></p>
<p>As language models (LMs) become increasingly powerful, it is important to
quantify and compare them for sociodemographic bias with potential for harm.
Prior bias measurement datasets are sensitive to perturbations in their
manually designed templates, therefore unreliable. To achieve reliability, we
introduce the Comprehensive Assessment of Language Model bias (CALM), a
benchmark dataset to quantify bias in LMs across three tasks. We integrate 16
existing datasets across different domains, such as Wikipedia and news
articles, to filter 224 templates from which we construct a dataset of 78,400
examples. We compare the diversity of CALM with prior datasets on metrics such
as average semantic similarity, and variation in template length, and test the
sensitivity to small perturbations. We show that our dataset is more diverse
and reliable than previous datasets, thus better capture the breadth of
linguistic variation required to reliably evaluate model bias. We evaluate 20
large language models including six prominent families of LMs such as Llama-2.
In two LM series, OPT and Bloom, we found that larger parameter models are more
biased than lower parameter models. We found the T0 series of models to be the
least biased. Furthermore, we noticed a tradeoff between gender and racial bias
with increasing model size in some model series. The code is available at
https://github.com/vipulgupta1011/CALM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12551">A Co-training Approach for Noisy Time Series Learning. (arXiv:2308.12551v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weiqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsung_F/0/1/0/all/0/1">Fugee Tsung</a></p>
<p>In this work, we focus on robust time series representation learning. Our
assumption is that real-world time series is noisy and complementary
information from different views of the same time series plays an important
role while analyzing noisy input. Based on this, we create two views for the
input time series through two different encoders. We conduct co-training based
contrastive learning iteratively to learn the encoders. Our experiments
demonstrate that this co-training approach leads to a significant improvement
in performance. Especially, by leveraging the complementary information from
different views, our proposed TS-CoT method can mitigate the impact of data
noise and corruption. Empirical evaluations on four time series benchmarks in
unsupervised and semi-supervised settings reveal that TS-CoT outperforms
existing methods. Furthermore, the representations learned by TS-CoT can
transfer well to downstream tasks through fine-tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12553">Don&#x27;t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy. (arXiv:2308.12553v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Puli_A/0/1/0/all/0/1">Aahlad Puli</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lily Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wald_Y/0/1/0/all/0/1">Yoav Wald</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1">Rajesh Ranganath</a></p>
<p>Common explanations for shortcut learning assume that the shortcut improves
prediction under the training distribution but not in the test distribution.
Thus, models trained via the typical gradient-based optimization of
cross-entropy, which we call default-ERM, utilize the shortcut. However, even
when the stable feature determines the label in the training distribution and
the shortcut does not provide any additional information, like in perception
tasks, default-ERM still exhibits shortcut learning. Why are such solutions
preferred when the loss for default-ERM can be driven to zero using the stable
feature alone? By studying a linear perception task, we show that default-ERM's
preference for maximizing the margin leads to models that depend more on the
shortcut than the stable feature, even without overparameterization. This
insight suggests that default-ERM's implicit inductive bias towards max-margin
is unsuitable for perception tasks. Instead, we develop an inductive bias
toward uniform margins and show that this bias guarantees dependence only on
the perfect stable feature in the linear perception task. We develop loss
functions that encourage uniform-margin solutions, called margin control
(MARG-CTRL). MARG-CTRL mitigates shortcut learning on a variety of vision and
language tasks, showing that better inductive biases can remove the need for
expensive two-stage shortcut-mitigating methods in perception tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12554">Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling. (arXiv:2308.12554v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Bu_F/0/1/0/all/0/1">Fanjin Bu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zhen Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_M/0/1/0/all/0/1">Meng Han</a></p>
<p>In order to coordinate energy interactions among various communities and
energy conversions among multi-energy subsystems within the multi-community
integrated energy system under uncertain conditions, and achieve overall
optimization and scheduling of the comprehensive energy system, this paper
proposes a comprehensive scheduling model that utilizes a multi-agent deep
reinforcement learning algorithm to learn load characteristics of different
communities and make decisions based on this knowledge. In this model, the
scheduling problem of the integrated energy system is transformed into a Markov
decision process and solved using a data-driven deep reinforcement learning
algorithm, which avoids the need for modeling complex energy coupling
relationships between multi-communities and multi-energy subsystems. The
simulation results show that the proposed method effectively captures the load
characteristics of different communities and utilizes their complementary
features to coordinate reasonable energy interactions among them. This leads to
a reduction in wind curtailment rate from 16.3% to 0% and lowers the overall
operating cost by 5445.6 Yuan, demonstrating significant economic and
environmental benefits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12562">Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions. (arXiv:2308.12562v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1">Aditya Chattopadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1">Benjamin David Haeffele</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1">Rene Vidal</a></p>
<p>Variational Information Pursuit (V-IP) is a framework for making
interpretable predictions by design by sequentially selecting a short chain of
task-relevant, user-defined and interpretable queries about the data that are
most informative for the task. While this allows for built-in interpretability
in predictive models, applying V-IP to any task requires data samples with
dense concept-labeling by domain experts, limiting the application of V-IP to
small-scale tasks where manual data annotation is feasible. In this work, we
extend the V-IP framework with Foundational Models (FMs) to address this
limitation. More specifically, we use a two-step process, by first leveraging
Large Language Models (LLMs) to generate a sufficiently large candidate set of
task-relevant interpretable concepts, then using Large Multimodal Models to
annotate each data sample by semantic similarity with each concept in the
generated concept set. While other interpretable-by-design frameworks such as
Concept Bottleneck Models (CBMs) require an additional step of removing
repetitive and non-discriminative concepts to have good interpretability and
test performance, we mathematically and empirically justify that, with a
sufficiently informative and task-relevant query (concept) set, the proposed
FM+V-IP method does not require any type of concept filtering. In addition, we
show that FM+V-IP with LLM generated concepts can achieve better test
performance than V-IP with human annotated concepts, demonstrating the
effectiveness of LLMs at generating efficient query sets. Finally, when
compared to other interpretable-by-design frameworks such as CBMs, FM+V-IP can
achieve competitive test performance using fewer number of concepts/queries in
both cases with filtered or unfiltered concept sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12563">Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1">Thi Kieu Khanh Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1">Narges Armanfard</a></p>
<p>Mainstream unsupervised anomaly detection algorithms often excel in academic
datasets, yet their real-world performance is restricted due to the controlled
experimental conditions involving clean training data. Addressing the challenge
of training with noise, a prevalent issue in practical anomaly detection, is
frequently overlooked. In a pioneering endeavor, this study delves into the
realm of label-level noise within sensory time-series anomaly detection (TSAD).
This paper presents a novel and practical end-to-end unsupervised TSAD when the
training data are contaminated with anomalies. The introduced approach, called
TSAD-C, is devoid of access to abnormality labels during the training phase.
TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities
(aka noise) present in the training data, a Variable Dependency Modeling module
to capture both long-term intra- and inter-variable dependencies within the
decontaminated data that can be considered as a surrogate of the pure normal
data, and an Anomaly Scoring module to detect anomalies. Our extensive
experiments conducted on three widely used physiological datasets conclusively
demonstrate that our approach surpasses existing methodologies, thus
establishing a new state-of-the-art performance in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12573">Conditional Kernel Imitation Learning for Continuous State Environments. (arXiv:2308.12573v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_R/0/1/0/all/0/1">Rishabh Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahlin_N/0/1/0/all/0/1">Nathan Dahlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rahul Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayyar_A/0/1/0/all/0/1">Ashutosh Nayyar</a></p>
<p>Imitation Learning (IL) is an important paradigm within the broader
reinforcement learning (RL) methodology. Unlike most of RL, it does not assume
availability of reward-feedback. Reward inference and shaping are known to be
difficult and error-prone methods particularly when the demonstration data
comes from human experts. Classical methods such as behavioral cloning and
inverse reinforcement learning are highly sensitive to estimation errors, a
problem that is particularly acute in continuous state space problems.
Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning
problems into distribution-matching problems which often require additional
online interaction data to be effective. In this paper, we consider the problem
of imitation learning in continuous state space environments based solely on
observed behavior, without access to transition dynamics information, reward
structure, or, most importantly, any additional interactions with the
environment. Our approach is based on the Markov balance equation and
introduces a novel conditional kernel density estimation-based imitation
learning framework. It involves estimating the environment's transition
dynamics using conditional kernel density estimators and seeks to satisfy the
probabilistic balance equations for the environment. We establish that our
estimators satisfy basic asymptotic consistency requirements. Through a series
of numerical experiments on continuous state benchmark environments, we show
consistently superior empirical performance over many state-of-the-art IL
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12575">Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction. (arXiv:2308.12575v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuxi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1">Shaowen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1">Flora D. Salim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1">Antonio Jimeno Yepes</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jun Shen</a></p>
<p>The Intensive Care Unit (ICU) is one of the most important parts of a
hospital, which admits critically ill patients and provides continuous
monitoring and treatment. Various patient outcome prediction methods have been
attempted to assist healthcare professionals in clinical decision-making.
Existing methods focus on measuring the similarity between patients using deep
neural networks to capture the hidden feature structures. However, the
higher-order relationships are ignored, such as patient characteristics (e.g.,
diagnosis codes) and their causal effects on downstream clinical predictions.
</p>
<p>In this paper, we propose a novel Hypergraph Convolutional Network that
allows the representation of non-pairwise relationships among diagnosis codes
in a hypergraph to capture the hidden feature structures so that fine-grained
patient similarity can be calculated for personalized mortality risk
prediction. Evaluation using a publicly available eICU Collaborative Research
Database indicates that our method achieves superior performance over the
state-of-the-art models on mortality risk prediction. Moreover, the results of
several case studies demonstrated the effectiveness of constructing graph
networks in providing good transparency and robustness in decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12581">A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Puning Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1">Zhiguo Wan</a></p>
<p>Federated learning systems are susceptible to adversarial attacks. To combat
this, we introduce a novel aggregator based on Huber loss minimization, and
provide a comprehensive theoretical analysis. Under independent and identically
distributed (i.i.d) assumption, our approach has several advantages compared to
existing methods. Firstly, it has optimal dependence on $\epsilon$, which
stands for the ratio of attacked clients. Secondly, our approach does not need
precise knowledge of $\epsilon$. Thirdly, it allows different clients to have
unequal data sizes. We then broaden our analysis to include non-i.i.d data,
such that clients have slightly different distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12584">LORD: Leveraging Open-Set Recognition with Unknown Data. (arXiv:2308.12584v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koch_T/0/1/0/all/0/1">Tobias Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1">Christian Riess</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_T/0/1/0/all/0/1">Thomas K&#xf6;hler</a></p>
<p>Handling entirely unknown data is a challenge for any deployed classifier.
Classification models are typically trained on a static pre-defined dataset and
are kept in the dark for the open unassigned feature space. As a result, they
struggle to deal with out-of-distribution data during inference. Addressing
this task on the class-level is termed open-set recognition (OSR). However,
most OSR methods are inherently limited, as they train closed-set classifiers
and only adapt the downstream predictions to OSR. This work presents LORD, a
framework to Leverage Open-set Recognition by exploiting unknown Data. LORD
explicitly models open space during classifier training and provides a
systematic evaluation for such approaches. We identify three model-agnostic
training strategies that exploit background data and applied them to
well-established classifiers. Due to LORD's extensive evaluation protocol, we
consistently demonstrate improved recognition of unknown data. The benchmarks
facilitate in-depth analysis across various requirement levels. To mitigate
dependency on extensive and costly background datasets, we explore mixup as an
off-the-shelf data generation technique. Our experiments highlight mixup's
effectiveness as a substitute for background datasets. Lightweight constraints
on mixup synthesis further improve OSR performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12585">Persistent learning signals and working memory without continuous attractors. (arXiv:2308.12585v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Park_I/0/1/0/all/0/1">Il Memming Park</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sagodi_A/0/1/0/all/0/1">&#xc1;bel S&#xe1;godi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Soko%5Cl_P/0/1/0/all/0/1">Piotr Aleksander Sok&#xf3;&#x142;</a></p>
<p>Neural dynamical systems with stable attractor structures, such as point
attractors and continuous attractors, are hypothesized to underlie meaningful
temporal behavior that requires working memory. However, working memory may not
support useful learning signals necessary to adapt to changes in the temporal
structure of the environment. We show that in addition to the continuous
attractors that are widely implicated, periodic and quasi-periodic attractors
can also support learning arbitrarily long temporal relationships. Unlike the
continuous attractors that suffer from the fine-tuning problem, the less
explored quasi-periodic attractors are uniquely qualified for learning to
produce temporally structured behavior. Our theory has broad implications for
the design of artificial learning systems and makes predictions about
observable signatures of biological neural dynamics that can support temporal
dependence learning and working memory. Based on our theory, we developed a new
initialization scheme for artificial recurrent neural networks that outperforms
standard methods for tasks that require learning temporal dynamics. Moreover,
we propose a robust recurrent memory mechanism for integrating and maintaining
head direction without a ring attractor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12599">Exploiting Time-Frequency Conformers for Music Audio Enhancement. (arXiv:2308.12599v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chae_Y/0/1/0/all/0/1">Yunkee Chae</a>, <a href="http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1">Junghyun Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sungho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyogu Lee</a></p>
<p>With the proliferation of video platforms on the internet, recording musical
performances by mobile devices has become commonplace. However, these
recordings often suffer from degradation such as noise and reverberation, which
negatively impact the listening experience. Consequently, the necessity for
music audio enhancement (referred to as music enhancement from this point
onward), involving the transformation of degraded audio recordings into
pristine high-quality music, has surged to augment the auditory experience. To
address this issue, we propose a music enhancement system based on the
Conformer architecture that has demonstrated outstanding performance in speech
enhancement tasks. Our approach explores the attention mechanisms of the
Conformer and examines their performance to discover the best approach for the
music enhancement task. Our experimental results show that our proposed model
achieves state-of-the-art performance on single-stem music enhancement.
Furthermore, our system can perform general music enhancement with multi-track
mixtures, which has not been examined in previous work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12606">A Greedy Approach for Offering to Telecom Subscribers. (arXiv:2308.12606v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bhunre_P/0/1/0/all/0/1">Piyush Kanti Bhunre</a>, <a href="http://arxiv.org/find/stat/1/au:+Sen_T/0/1/0/all/0/1">Tanmay Sen</a>, <a href="http://arxiv.org/find/stat/1/au:+Sarkar_A/0/1/0/all/0/1">Arijit Sarkar</a></p>
<p>Customer retention or churn prevention is a challenging task of a telecom
operator. One of the effective approaches is to offer some attractive incentive
or additional services or money to the subscribers for keeping them engaged and
make sure they stay in the operator's network for longer time. Often, operators
allocate certain amount of monetary budget to carry out the offer campaign. The
difficult part of this campaign is the selection of a set of customers from a
large subscriber-base and deciding the amount that should be offered to an
individual so that operator's objective is achieved. There may be multiple
objectives (e.g., maximizing revenue, minimizing number of churns) for
selection of subscriber and selection of an offer to the selected subscriber.
Apart from monetary benefit, offers may include additional data, SMS, hots-spot
tethering, and many more. This problem is known as offer optimization. In this
paper, we propose a novel combinatorial algorithm for solving offer
optimization under heterogeneous offers by maximizing expected revenue under
the scenario of subscriber churn, which is, in general, seen in telecom domain.
The proposed algorithm is efficient and accurate even for a very large
subscriber-base.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12612">Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection. (arXiv:2308.12612v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junjie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zhihao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shutao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yue Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huaan Li</a></p>
<p>The rapid growth of deep learning (DL) has spurred interest in enhancing
log-based anomaly detection. This approach aims to extract meaning from log
events (log message templates) and develop advanced DL models for anomaly
detection. However, these DL methods face challenges like heavy reliance on
training data, labels, and computational resources due to model complexity. In
contrast, traditional machine learning and data mining techniques are less
data-dependent and more efficient but less effective than DL. To make log-based
anomaly detection more practical, the goal is to enhance traditional techniques
to match DL's effectiveness. Previous research in a different domain (linking
questions on Stack Overflow) suggests that optimized traditional techniques can
rival state-of-the-art DL methods. Drawing inspiration from this concept, we
conducted an empirical study. We optimized the unsupervised PCA (Principal
Component Analysis), a traditional technique, by incorporating lightweight
semantic-based log representation. This addresses the issue of unseen log
events in training data, enhancing log representation. Our study compared seven
log-based anomaly detection methods, including four DL-based, two traditional,
and the optimized PCA technique, using public and industrial datasets. Results
indicate that the optimized unsupervised PCA technique achieves similar
effectiveness to advanced supervised/semi-supervised DL methods while being
more stable with limited training data and resource-efficient. This
demonstrates the adaptability and strength of traditional techniques through
small yet impactful adaptations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12625">Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs. (arXiv:2308.12625v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuqiong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yushun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1">Fuqiang Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhou Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1">Bing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1">Ailin Zhao</a></p>
<p>Logs are valuable information for oil and gas fields as they help to
determine the lithology of the formations surrounding the borehole and the
location and reserves of subsurface oil and gas reservoirs. However, important
logs are often missing in horizontal or old wells, which poses a challenge in
field applications. In this paper, we utilize data from the 2020 machine
learning competition of the SPWLA, which aims to predict the missing
compressional wave slowness and shear wave slowness logs using other logs in
the same borehole. We employ the NGBoost algorithm to construct an Ensemble
Learning model that can predicate the results as well as their uncertainty.
Furthermore, we combine the SHAP method to investigate the interpretability of
the machine learning model. We compare the performance of the NGBosst model
with four other commonly used Ensemble Learning methods, including Random
Forest, GBDT, XGBoost, LightGBM. The results show that the NGBoost model
performs well in the testing set and can provide a probability distribution for
the prediction results. In addition, the variance of the probability
distribution of the predicted log can be used to justify the quality of the
constructed log. Using the SHAP explainable machine learning model, we
calculate the importance of each input log to the predicted results as well as
the coupling relationship among input logs. Our findings reveal that the
NGBoost model tends to provide greater slowness prediction results when the
neutron porosity and gamma ray are large, which is consistent with the
cognition of petrophysical models. Furthermore, the machine learning model can
capture the influence of the changing borehole caliper on slowness, where the
influence of borehole caliper on slowness is complex and not easy to establish
a direct relationship. These findings are in line with the physical principle
of borehole acoustics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12634">Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cersovsky_J/0/1/0/all/0/1">Josef Cersovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1">Sadegh Mohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1">Dagmar Kainmueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoehne_J/0/1/0/all/0/1">Johannes Hoehne</a></p>
<p>The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12646">The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings. (arXiv:2308.12646v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kucherenko_T/0/1/0/all/0/1">Taras Kucherenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagy_R/0/1/0/all/0/1">Rajmund Nagy</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1">Youngwoo Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1">Jieyeon Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolov_T/0/1/0/all/0/1">Teodor Nikolov</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsakov_M/0/1/0/all/0/1">Mihail Tsakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1">Gustav Eje Henter</a></p>
<p>This paper reports on the GENEA Challenge 2023, in which participating teams
built speech-driven gesture-generation systems using the same speech and motion
dataset, followed by a joint evaluation. This year's challenge provided data on
both sides of a dyadic interaction, allowing teams to generate full-body motion
for an agent given its speech (text and audio) and the speech and motion of the
interlocutor. We evaluated 12 submissions and 2 baselines together with
held-out motion-capture data in several large-scale user studies. The studies
focused on three aspects: 1) the human-likeness of the motion, 2) the
appropriateness of the motion for the agent's own speech whilst controlling for
the human-likeness of the motion, and 3) the appropriateness of the motion for
the behaviour of the interlocutor in the interaction, using a setup that
controls for both the human-likeness of the motion and the agent's own speech.
We found a large span in human-likeness between challenge submissions, with a
few systems rated close to human mocap. Appropriateness seems far from being
solved, with most submissions performing in a narrow range slightly above
chance, far behind natural motion. The effect of the interlocutor is even more
subtle, with submitted systems at best performing barely above chance.
Interestingly, a dyadic system being highly appropriate for agent speech does
not necessarily imply high appropriateness for the interlocutor. Additional
material is available via the project website at
https://svito-zar.github.io/GENEAchallenge2023/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12649">APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT. (arXiv:2308.12649v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galler_H/0/1/0/all/0/1">Hadar Schreiber Galler</a>, <a href="http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1">Tom Zahavy</a>, <a href="http://arxiv.org/find/cs/1/au:+Desjardins_G/0/1/0/all/0/1">Guillaume Desjardins</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1">Alon Cohen</a></p>
<p>We study diverse skill discovery in reward-free environments, aiming to
discover all possible skills in simple grid-world environments where prior
methods have struggled to succeed. This problem is formulated as mutual
training of skills using an intrinsic reward and a discriminator trained to
predict a skill given its trajectory. Our initial solution replaces the
standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs)
discriminator and combines it with a novel intrinsic reward function and a
dropout regularization technique. The combined approach is named APART: Diverse
Skill Discovery using All Pairs with Ascending Reward and Dropout. We
demonstrate that APART discovers all the possible skills in grid worlds with
remarkably fewer samples than previous works. Motivated by the empirical
success of APART, we further investigate an even simpler algorithm that
achieves maximum skills by altering VIC, rescaling its intrinsic reward, and
tuning the temperature of its softmax discriminator. We believe our findings
shed light on the crucial factors underlying success of skill discovery
algorithms in reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12661">Don&#x27;t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1">Paul Gavrikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1">Janis Keuper</a></p>
<p>Assessing the robustness of deep neural networks against out-of-distribution
inputs is crucial, especially in safety-critical domains like autonomous
driving, but also in safety systems where malicious actors can digitally alter
inputs to circumvent safety guards. However, designing effective
out-of-distribution tests that encompass all possible scenarios while
preserving accurate label information is a challenging task. Existing
methodologies often entail a compromise between variety and constraint levels
for attacks and sometimes even both. In a first step towards a more holistic
robustness evaluation of image classification models, we introduce an attack
method based on image solarization that is conceptually straightforward yet
avoids jeopardizing the global structure of natural images independent of the
intensity. Through comprehensive evaluations of multiple ImageNet models, we
demonstrate the attack's capacity to degrade accuracy significantly, provided
it is not integrated into the training augmentations. Interestingly, even then,
no full immunity to accuracy deterioration is achieved. In other settings, the
attack can often be simplified into a black-box attack with model-independent
parameters. Defenses against other corruptions do not consistently extend to be
effective against our specific attack.
</p>
<p>Project website: https://github.com/paulgavrikov/adversarial_solarization
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12666">Geodesic Mode Connectivity. (arXiv:2308.12666v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Charlie Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1">Theodore Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sarah Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Laine_R/0/1/0/all/0/1">Rudolf Laine</a></p>
<p>Mode connectivity is a phenomenon where trained models are connected by a
path of low loss. We reframe this in the context of Information Geometry, where
neural networks are studied as spaces of parameterized distributions with
curved geometry. We hypothesize that shortest paths in these spaces, known as
geodesics, correspond to mode-connecting paths in the loss landscape. We
propose an algorithm to approximate geodesics and demonstrate that they achieve
mode connectivity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12670">Optimal data pooling for shared learning in maintenance operations. (arXiv:2308.12670v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Drent_C/0/1/0/all/0/1">Collin Drent</a>, <a href="http://arxiv.org/find/cs/1/au:+Drent_M/0/1/0/all/0/1">Melvin Drent</a>, <a href="http://arxiv.org/find/cs/1/au:+Houtum_G/0/1/0/all/0/1">Geert-Jan van Houtum</a></p>
<p>This paper addresses the benefits of pooling data for shared learning in
maintenance operations. We consider a set of systems subject to Poisson
degradation that are coupled through an a-priori unknown rate. Decision
problems involving these systems are high-dimensional Markov decision processes
(MDPs). We present a decomposition result that reduces such an MDP to
two-dimensional MDPs, enabling structural analyses and computations. We
leverage this decomposition to demonstrate that pooling data can lead to
significant cost reductions compared to not pooling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12673">Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition. (arXiv:2308.12673v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Daskalakis_D/0/1/0/all/0/1">Dimitrios Daskalakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gkalelis_N/0/1/0/all/0/1">Nikolaos Gkalelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mezaris_V/0/1/0/all/0/1">Vasileios Mezaris</a></p>
<p>In this paper, we introduce Masked Feature Modelling (MFM), a novel approach
for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM
utilizes a pretrained Visual Tokenizer to reconstruct masked features of
objects within a video, leveraging the MiniKinetics dataset. We then
incorporate the pre-trained GAT block into a state-of-the-art bottom-up
supervised video-event recognition architecture, ViGAT, to improve the model's
starting point and overall accuracy. Experimental evaluations on the YLI-MED
dataset demonstrate the effectiveness of MFM in improving event recognition
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12679">A Continual Learning Approach for Cross-Domain White Blood Cell Classification. (arXiv:2308.12679v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadafi_A/0/1/0/all/0/1">Ario Sadafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Salehi_R/0/1/0/all/0/1">Raheleh Salehi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gruber_A/0/1/0/all/0/1">Armin Gruber</a>, <a href="http://arxiv.org/find/cs/1/au:+Boushehri_S/0/1/0/all/0/1">Sayedali Shetab Boushehri</a>, <a href="http://arxiv.org/find/cs/1/au:+Giehr_P/0/1/0/all/0/1">Pascal Giehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1">Carsten Marr</a></p>
<p>Accurate classification of white blood cells in peripheral blood is essential
for diagnosing hematological diseases. Due to constantly evolving clinical
settings, data sources, and disease classifications, it is necessary to update
machine learning classification models regularly for practical real-world use.
Such models significantly benefit from sequentially learning from incoming data
streams without forgetting previously acquired knowledge. However, models can
suffer from catastrophic forgetting, causing a drop in performance on previous
tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual
learning approach for class incremental and domain incremental scenarios in
white blood cell classification. To choose representative samples from previous
tasks, we employ exemplar set selection based on the model's predictions. This
involves selecting the most confident samples and the most challenging samples
identified through uncertainty estimation of the model. We thoroughly evaluated
our proposed approach on three white blood cell classification datasets that
differ in color, resolution, and class composition, including scenarios where
new domains or new classes are introduced to the model with every task. We also
test a long class incremental experiment with both new domains and new classes.
Our results demonstrate that our approach outperforms established baselines in
continual learning, including existing iCaRL and EWC methods for classifying
white blood cells in cross-domain environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12680">Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints. (arXiv:2308.12680v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hanchi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1">Deheng Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a></p>
<p>We propose a novel master-slave architecture to solve the top-$K$
combinatorial multi-armed bandits problem with non-linear bandit feedback and
diversity constraints, which, to the best of our knowledge, is the first
combinatorial bandits setting considering diversity constraints under bandit
feedback. Specifically, to efficiently explore the combinatorial and
constrained action space, we introduce six slave models with distinguished
merits to generate diversified samples well balancing rewards and constraints
as well as efficiency. Moreover, we propose teacher learning based optimization
and the policy co-training technique to boost the performance of the multiple
slave models. The master model then collects the elite samples provided by the
slave models and selects the best sample estimated by a neural contextual
UCB-based network to make a decision with a trade-off between exploration and
exploitation. Thanks to the elaborate design of slave models, the co-training
mechanism among slave models, and the novel interactions between the master and
slave models, our approach significantly surpasses existing state-of-the-art
algorithms in both synthetic and real datasets for recommendation tasks. The
code is available at:
\url{https://github.com/huanghanchi/Master-slave-Algorithm-for-Top-K-Bandits}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12681">LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanci Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a></p>
<p>Federated learning (FL) is an emerging approach for training machine learning
models collaboratively while preserving data privacy. The need for privacy
protection makes it difficult for FL models to achieve global transparency and
explainability. To address this limitation, we incorporate logic-based
explanations into FL by proposing the Logical Reasoning-based eXplainable
Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local
logic rules based on their local data and send them, along with model updates,
to the FL server. The FL server connects the local logic rules through a proper
logical connector that is derived based on properties of client data, without
requiring access to the raw data. In addition, the server also aggregates the
local model updates with weight values determined by the quality of the
clients' local data as reflected by their uploaded logic rules. The results
show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and
5.41% in terms of classification accuracy, rule accuracy and rule fidelity,
respectively. The explicit rule evaluation and expression under LR-XFL enable
human experts to validate and correct the rules on the server side, hence
improving the global FL model's robustness to errors. It has the potential to
enhance the transparency of FL models for areas like healthcare and finance
where both data privacy and explainability are important.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12686">Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment. (arXiv:2308.12686v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Painblanc_F/0/1/0/all/0/1">Fran&#xe7;ois Painblanc</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapel_L/0/1/0/all/0/1">Laetitia Chapel</a>, <a href="http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1">Nicolas Courty</a>, <a href="http://arxiv.org/find/cs/1/au:+Friguet_C/0/1/0/all/0/1">Chlo&#xe9; Friguet</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1">Charlotte Pelletier</a>, <a href="http://arxiv.org/find/cs/1/au:+Tavenard_R/0/1/0/all/0/1">Romain Tavenard</a></p>
<p>While large volumes of unlabeled data are usually available, associated
labels are often scarce. The unsupervised domain adaptation problem aims at
exploiting labels from a source domain to classify data from a related, yet
different, target domain. When time series are at stake, new difficulties arise
as temporal shifts may appear in addition to the standard feature distribution
shift. In this paper, we introduce the Match-And-Deform (MAD) approach that
aims at finding correspondences between the source and target time series while
allowing temporal distortions. The associated optimization problem
simultaneously aligns the series thanks to an optimal transport loss and the
time stamps through dynamic time warping. When embedded into a deep neural
network, MAD helps learning new representations of time series that both align
the domains and maximize the discriminative power of the network. Empirical
studies on benchmark datasets and remote sensing data demonstrate that MAD
makes meaningful sample-to-sample pairing and time shift estimation, reaching
similar or better classification performance than state-of-the-art deep time
series domain adaptation strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12691">An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression. (arXiv:2308.12691v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lyu_B/0/1/0/all/0/1">Bohan Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianzhong Li</a></p>
<p>This paper introduces a new data analysis method for big data using a newly
defined regression model named multiple model linear regression(MMLR), which
separates input datasets into subsets and construct local linear regression
models of them. The proposed data analysis method is shown to be more efficient
and flexible than other regression based methods. This paper also proposes an
approximate algorithm to construct MMLR models based on
$(\epsilon,\delta)$-estimator, and gives mathematical proofs of the correctness
and efficiency of MMLR algorithm, of which the time complexity is linear with
respect to the size of input datasets. This paper also empirically implements
the method on both synthetic and real-world datasets, the algorithm shows to
have comparable performance to existing regression methods in many cases, while
it takes almost the shortest time to provide a high prediction accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12696">Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balabin_N/0/1/0/all/0/1">Nikita Balabin</a>, <a href="http://arxiv.org/find/cs/1/au:+Voronkova_D/0/1/0/all/0/1">Daria Voronkova</a>, <a href="http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1">Ilya Trofimov</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1">Evgeny Burnaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1">Serguei Barannikov</a></p>
<p>We propose TopDis (Topological Disentanglement), a method for learning
disentangled representations via adding multi-scale topological loss term.
Disentanglement is a crucial property of data representations substantial for
the explainability and robustness of deep learning models and a step towards
high-level cognition. The state-of-the-art method based on VAE minimizes the
total correlation of the joint distribution of latent variables. We take a
different perspective on disentanglement by analyzing topological properties of
data manifolds. In particular, we optimize the topological similarity for data
manifolds traversals. To the best of our knowledge, our paper is the first one
to propose a differentiable topological loss for disentanglement. Our
experiments have shown that the proposed topological loss improves
disentanglement scores such as MIG, FactorVAE score, SAP score and DCI
disentanglement score with respect to state-of-the-art results. Our method
works in an unsupervised manner, permitting to apply it for problems without
labeled factors of variation. Additionally, we show how to use the proposed
topological loss to find disentangled directions in a trained GAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12716">Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks. (arXiv:2308.12716v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Sahin_T/0/1/0/all/0/1">T. Sahin</a>, <a href="http://arxiv.org/find/math/1/au:+Danwitz_M/0/1/0/all/0/1">M. von Danwitz</a>, <a href="http://arxiv.org/find/math/1/au:+Popp_A/0/1/0/all/0/1">A. Popp</a></p>
<p>This paper explores the ability of physics-informed neural networks (PINNs)
to solve forward and inverse problems of contact mechanics for small
deformation elasticity. We deploy PINNs in a mixed-variable formulation
enhanced by output transformation to enforce Dirichlet and Neumann boundary
conditions as hard constraints. Inequality constraints of contact problems,
namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft
constraints by incorporating them into the loss function during network
training. To formulate the loss function contribution of KKT constraints,
existing approaches applied to elastoplasticity problems are investigated and
we explore a nonlinear complementarity problem (NCP) function, namely
Fischer-Burmeister, which possesses advantageous characteristics in terms of
optimization. Based on the Hertzian contact problem, we show that PINNs can
serve as pure partial differential equation (PDE) solver, as data-enhanced
forward model, as inverse solver for parameter identification, and as
fast-to-evaluate surrogate model. Furthermore, we demonstrate the importance of
choosing proper hyperparameters, e.g. loss weights, and a combination of Adam
and L-BFGS-B optimizers aiming for better results in terms of accuracy and
training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12726">Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game. (arXiv:2308.12726v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahimi_M/0/1/0/all/0/1">Masoud Rahimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1">Hadi Moradi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vahabie_A/0/1/0/all/0/1">Abdol-hossein Vahabie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kebriaei_H/0/1/0/all/0/1">Hamed Kebriaei</a></p>
<p>Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a
player's experience in video games. Recently, Reinforcement Learning (RL)
methods have been employed for DDA in non-competitive games; nevertheless, they
rely solely on discrete state-action space with a small search space. In this
paper, we propose a continuous RL-based DDA methodology for a visual working
memory (VWM) game to handle the complex search space for the difficulty of
memorization. The proposed RL-based DDA tailors game difficulty based on the
player's score and game difficulty in the last trial. We defined a continuous
metric for the difficulty of memorization. Then, we consider the task
difficulty and the vector of difficulty-score as the RL's action and state,
respectively. We evaluated the proposed method through a within-subject
experiment involving 52 subjects. The proposed approach was compared with two
rule-based difficulty adjustment methods in terms of player's score and game
experience measured by a questionnaire. The proposed RL-based approach resulted
in a significantly better game experience in terms of competence, tension, and
negative and positive affect. Players also achieved higher scores and win
rates. Furthermore, the proposed RL-based DDA led to a significantly less
decline in the score in a 20-trial session.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12729">Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection. (arXiv:2308.12729v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shijie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xin Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuejiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1">Binfeng Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuangyang Wang</a></p>
<p>Customer lifetime value (LTV) prediction is essential for mobile game
publishers trying to optimize the advertising investment for each user
acquisition based on the estimated worth. In mobile games, deploying
microtransactions is a simple yet effective monetization strategy, which
attracts a tiny group of game whales who splurge on in-game purchases. The
presence of such game whales may impede the practicality of existing LTV
prediction models, since game whales' purchase behaviours always exhibit varied
distribution from general users. Consequently, identifying game whales can open
up new opportunities to improve the accuracy of LTV prediction models. However,
little attention has been paid to applying game whale detection in LTV
prediction, and existing works are mainly specialized for the long-term LTV
prediction with the assumption that the high-quality user features are
available, which is not applicable in the UA stage. In this paper, we propose
ExpLTV, a novel multi-task framework to perform LTV prediction and game whale
detection in a unified way. In ExpLTV, we first innovatively design a deep
neural network-based game whale detector that can not only infer the intrinsic
order in accordance with monetary value, but also precisely identify high
spenders (i.e., game whales) and low spenders. Then, by treating the game whale
detector as a gating network to decide the different mixture patterns of LTV
experts assembling, we can thoroughly leverage the shared information and
scenario-specific information (i.e., game whales modelling and low spenders
modelling). Finally, instead of separately designing a purchase rate estimator
for two tasks, we design a shared estimator that can preserve the inner task
relationships. The superiority of ExpLTV is further validated via extensive
experiments on three industrial datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12734">Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion. (arXiv:2308.12734v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1">Jordan J. Bird</a>, <a href="http://arxiv.org/find/cs/1/au:+Lotfi_A/0/1/0/all/0/1">Ahmad Lotfi</a></p>
<p>There are growing implications surrounding generative AI in the speech domain
that enable voice cloning and real-time voice conversion from one individual to
another. This technology poses a significant ethical threat and could lead to
breaches of privacy and misrepresentation, thus there is an urgent need for
real-time detection of AI-generated speech for DeepFake Voice Conversion. To
address the above emerging issues, the DEEP-VOICE dataset is generated in this
study, comprised of real human speech from eight well-known figures and their
speech converted to one another using Retrieval-based Voice Conversion.
Presenting as a binary classification problem of whether the speech is real or
AI-generated, statistical analysis of temporal audio features through t-testing
reveals that there are significantly different distributions. Hyperparameter
optimisation is implemented for machine learning models to identify the source
of speech. Following the training of 208 individual machine learning models
over 10-fold cross validation, it is found that the Extreme Gradient Boosting
model can achieve an average classification accuracy of 99.3% and can classify
speech in real-time, at around 0.004 milliseconds given one second of speech.
All data generated for this study is released publicly for future research on
AI speech detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12740">Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ai_L/0/1/0/all/0/1">Lun Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Shi-Shun Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Wang-Zhou Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hallett_L/0/1/0/all/0/1">Liam Hallett</a>, <a href="http://arxiv.org/find/cs/1/au:+Muggleton_S/0/1/0/all/0/1">Stephen H. Muggleton</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldwin_G/0/1/0/all/0/1">Geoff S. Baldwin</a></p>
<p>An important application of Synthetic Biology is the engineering of the host
cell system to yield useful products. However, an increase in the scale of the
host system leads to huge design space and requires a large number of
validation trials with high experimental costs. A comprehensible machine
learning approach that efficiently explores the hypothesis space and guides
experimental design is urgently needed for the Design-Build-Test-Learn (DBTL)
cycle of the host cell system. We introduce a novel machine learning framework
ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive
logical reasoning and actively learns from training examples. In contrast to
numerical models, ILP-iML1515 is built on comprehensible logical
representations of a genome-scale metabolic model and can update the model by
learning new logical structures from auxotrophic mutant trials. The ILP-iML1515
framework 1) allows high-throughput simulations and 2) actively selects
experiments that reduce the experimental cost of learning gene functions in
comparison to randomly selected experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12751">Motion In-Betweening with Phase Manifolds. (arXiv:2308.12751v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Starke_P/0/1/0/all/0/1">Paul Starke</a>, <a href="http://arxiv.org/find/cs/1/au:+Starke_S/0/1/0/all/0/1">Sebastian Starke</a>, <a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1">Taku Komura</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinicke_F/0/1/0/all/0/1">Frank Steinicke</a></p>
<p>This paper introduces a novel data-driven motion in-betweening system to
reach target poses of characters by making use of phases variables learned by a
Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network
model, in which the phases cluster movements in both space and time with
different expert weights. Each generated set of weights then produces a
sequence of poses in an autoregressive manner between the current and target
state of the character. In addition, to satisfy poses which are manually
modified by the animators or where certain end effectors serve as constraints
to be reached by the animation, a learned bi-directional control scheme is
implemented to satisfy such constraints. The results demonstrate that using
phases for motion in-betweening tasks sharpen the interpolated movements, and
furthermore stabilizes the learning process. Moreover, using phases for motion
in-betweening tasks can also synthesize more challenging movements beyond
locomotion behaviors. Additionally, style control is enabled between given
target keyframes. Our proposed framework can compete with popular
state-of-the-art methods for motion in-betweening in terms of motion quality
and generalization, especially in the existence of long transition durations.
Our framework contributes to faster prototyping workflows for creating animated
character sequences, which is of enormous interest for the game and film
industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12761">IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation. (arXiv:2308.12761v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Aung_N/0/1/0/all/0/1">Nyothiri Aung</a>, <a href="http://arxiv.org/find/eess/1/au:+Kechadi_T/0/1/0/all/0/1">Tahar Kechadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1">Liming Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Dhelim_S/0/1/0/all/0/1">Sahraoui Dhelim</a></p>
<p>CNNs have been widely applied for medical image analysis. However, limited
memory capacity is one of the most common drawbacks of processing
high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized
first before processing, which can result in a loss of resolution, increase
class imbalance, and affect the performance of the segmentation algorithms. In
this paper, we propose an end-to-end deep learning approach called IP-UNet.
IP-UNet is a UNet-based model that performs multi-class segmentation on
Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming
3D volumes. IP-UNet uses limited memory capability for training without losing
the original 3D image resolution. We compare the performance of three models in
terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D
segmentation of the CT scan images using a conventional 2D UNet model. 2)
IP-UNet that operates on data obtained by merging the extracted Maximum
Intensity Projection (MIP), Closest Vessel Projection (CVP), and Average
Intensity Projection (AvgIP) representations of the source 3D volumes, then
applying the UNet model on the output IP images. 3) 3D-UNet model directly
reads the 3D volumes constructed from a series of CT scan images and outputs
the 3D volume of the predicted segmentation. We test the performance of these
methods on 3D volumetric images for automatic breast calcification detection.
Experimental results show that IP-Unet can achieve similar segmentation
accuracy with 3D-Unet but with much better performance. It reduces the training
time by 70\% and memory consumption by 92\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12767">On the Consistency of Average Embeddings for Item Recommendation. (arXiv:2308.12767v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bendada_W/0/1/0/all/0/1">Walid Bendada</a>, <a href="http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1">Guillaume Salha-Galvan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1">Romain Hennequin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouabca_T/0/1/0/all/0/1">Thomas Bouab&#xe7;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Cazenave_T/0/1/0/all/0/1">Tristan Cazenave</a></p>
<p>A prevalent practice in recommender systems consists of averaging item
embeddings to represent users or higher-level concepts in the same embedding
space. This paper investigates the relevance of such a practice. For this
purpose, we propose an expected precision score, designed to measure the
consistency of an average embedding relative to the items used for its
construction. We subsequently analyze the mathematical expression of this score
in a theoretical setting with specific assumptions, as well as its empirical
behavior on real-world data from music streaming services. Our results
emphasize that real-world averages are less consistent for recommendation,
which paves the way for future research to better align real-world embeddings
with assumptions from our theoretical setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12772">Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward. (arXiv:2308.12772v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1">Taisuke Kobayashi</a></p>
<p>Robot control using reinforcement learning has become popular, but its
learning process generally terminates halfway through an episode for safety and
time-saving reasons. This study addresses the problem of the most popular
exception handling that temporal-difference (TD) learning performs at such
termination. That is, by forcibly assuming zero value after termination,
unintentionally implicit underestimation or overestimation occurs, depending on
the reward design in the normal states. When the episode is terminated due to
task failure, the failure may be highly valued with the unintentional
overestimation, and the wrong policy may be acquired. Although this problem can
be avoided by paying attention to the reward design, it is essential in
practical use of TD learning to review the exception handling at termination.
This paper therefore proposes a method to intentionally underestimate the value
after termination to avoid learning failures due to the unintentional
overestimation. In addition, the degree of underestimation is adjusted
according to the degree of stationarity at termination, thereby preventing
excessive exploration due to the intentional underestimation. Simulations and
real robot experiments showed that the proposed method can stably obtain the
optimal policies for various tasks and reward designs.
https://youtu.be/AxXr8uFOe7M
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12785">Single-shot Bayesian approximation for neural networks. (arXiv:2308.12785v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brach_K/0/1/0/all/0/1">Kai Brach</a>, <a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1">Beate Sick</a>, <a href="http://arxiv.org/find/cs/1/au:+Durr_O/0/1/0/all/0/1">Oliver D&#xfc;rr</a></p>
<p>Deep neural networks (NNs) are known for their high-prediction performances.
However, NNs are prone to yield unreliable predictions when encountering
completely new situations without indicating their uncertainty. Bayesian
variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide
uncertainty measures and simultaneously increase the prediction performance.
The only disadvantage of BNNs is their higher computation time during test time
because they rely on a sampling approach. Here we present a single-shot MC
dropout approximation that preserves the advantages of BNNs while being as fast
as NNs. Our approach is based on moment propagation (MP) and allows to
analytically approximate the expected value and the variance of the MC dropout
signal for commonly used layers in NNs, i.e. convolution, max pooling, dense,
softmax, and dropout layers. The MP approach can convert an NN into a BNN
without re-training given the NN has been trained with standard dropout. We
evaluate our approach on different benchmark datasets and a simulated toy
example in a classification and regression setting. We demonstrate that our
single-shot MC dropout approximation resembles the point estimate and the
uncertainty estimate of the predictive distribution that is achieved with an MC
approach, while being fast enough for real-time deployments of BNNs. We show
that using part of the saved time to combine our MP approach with deep ensemble
techniques does further improve the uncertainty measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12794">Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods. (arXiv:2308.12794v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reijnen_R/0/1/0/all/0/1">Robbert Reijnen</a>, <a href="http://arxiv.org/find/cs/1/au:+Straaten_K/0/1/0/all/0/1">Kjell van Straaten</a>, <a href="http://arxiv.org/find/cs/1/au:+Bukhsh_Z/0/1/0/all/0/1">Zaharah Bukhsh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingqian Zhang</a></p>
<p>We introduce an open-source GitHub repository containing comprehensive
benchmarks for a wide range of machine scheduling problems, including Job Shop
Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling
(FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent
Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our
primary goal is to provide a centralized hub for researchers, practitioners,
and enthusiasts interested in tackling machine scheduling challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12800">ICU Mortality Prediction Using Long Short-Term Memory Networks. (arXiv:2308.12800v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mili_M/0/1/0/all/0/1">Manel Mili</a> (FSM, TIM), <a href="http://arxiv.org/find/cs/1/au:+Kerkeni_A/0/1/0/all/0/1">Asma Kerkeni</a> (ISIMM, TIM), <a href="http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1">Asma Ben Abdallah</a> (ISIMM, TIM), <a href="http://arxiv.org/find/cs/1/au:+Bedoui_M/0/1/0/all/0/1">Mohamed Hedi Bedoui</a> (TIM)</p>
<p>Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in
complex temporal data regarding patient physiology, which presents an upscale
context for clinical data analysis. In the other hand, identifying the
time-series patterns within these data may provide a high aptitude to predict
clinical events. Hence, we investigate, during this work, the implementation of
an automatic data-driven system, which analyzes large amounts of multivariate
temporal data derived from Electronic Health Records (EHRs), and extracts
high-level information so as to predict in-hospital mortality and Length of
Stay (LOS) early. Practically, we investigate the applicability of LSTM network
by reducing the time-frame to 6-hour so as to enhance clinical tasks. The
experimental results highlight the efficiency of LSTM model with rigorous
multivariate time-series measurements for building real-world prediction
engines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12820">Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1">Avni Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1">Bogdan Kulynych</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1">Tsui-Wei Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ustun_B/0/1/0/all/0/1">Berk Ustun</a></p>
<p>Machine learning models are often used to decide who will receive a loan, a
job interview, or a public benefit. Standard techniques to build these models
use features about people but overlook their actionability. In turn, models can
assign predictions that are fixed, meaning that consumers who are denied loans,
interviews, or benefits may be permanently locked out from access to credit,
employment, or assistance. In this work, we introduce a formal testing
procedure to flag models that assign fixed predictions that we call recourse
verification. We develop machinery to reliably determine if a given model can
provide recourse to its decision subjects from a set of user-specified
actionability constraints. We demonstrate how our tools can ensure recourse and
adversarial robustness in real-world datasets and use them to study the
infeasibility of recourse in real-world lending datasets. Our results highlight
how models can inadvertently assign fixed predictions that permanently bar
access, and we provide tools to design algorithms that account for
actionability when developing models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12828">Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph. (arXiv:2308.12828v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shalit_N/0/1/0/all/0/1">Nadav Shalit</a>, <a href="http://arxiv.org/find/cs/1/au:+Fire_M/0/1/0/all/0/1">Michael Fire</a>, <a href="http://arxiv.org/find/cs/1/au:+Kagan_D/0/1/0/all/0/1">Dima Kagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Elia_E/0/1/0/all/0/1">Eran Ben-Elia</a></p>
<p>Public transport routing plays a crucial role in transit network design,
ensuring a satisfactory level of service for passengers. However, current
routing solutions rely on traditional operational research heuristics, which
can be time-consuming to implement and lack the ability to provide quick
solutions. Here, we propose a novel deep learning-based methodology for a
decision support system that enables public transport (PT) planners to identify
short-term route improvements rapidly. By seamlessly adjusting specific
sections of routes between two stops during specific times of the day, our
method effectively reduces times and enhances PT services. Leveraging diverse
data sources such as GTFS and smart card data, we extract features and model
the transportation network as a directed graph. Using self-supervision, we
train a deep learning model for predicting lateness values for road segments.
</p>
<p>These lateness values are then utilized as edge weights in the transportation
graph, enabling efficient path searching. Through evaluating the method on Tel
Aviv, we are able to reduce times on more than 9\% of the routes. The improved
routes included both intraurban and suburban routes showcasing a fact
highlighting the model's versatility. The findings emphasize the potential of
our data-driven decision support system to enhance public transport and city
logistics, promoting greater efficiency and reliability in PT services.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12843">Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning. (arXiv:2308.12843v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alzorgan_H/0/1/0/all/0/1">Hazim Alzorgan</a>, <a href="http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1">Abolfazl Razi</a>, <a href="http://arxiv.org/find/cs/1/au:+Moshayedi_A/0/1/0/all/0/1">Ata Jahangir Moshayedi</a></p>
<p>In this paper, we investigate the operation of an aerial manipulator system,
namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with
two degrees of freedom to carry out actuation tasks on the fly. Our solution is
based on employing a Q-learning method to control the trajectory of the tip of
the arm, also called \textit{end-effector}. More specifically, we develop a
motion planning model based on Time To Collision (TTC), which enables a
quadrotor UAV to navigate around obstacles while ensuring the manipulator's
reachability. Additionally, we utilize a model-based Q-learning model to
independently track and control the desired trajectory of the manipulator's
end-effector, given an arbitrary baseline trajectory for the UAV platform. Such
a combination enables a variety of actuation tasks such as high-altitude
welding, structural monitoring and repair, battery replacement, gutter
cleaning, sky scrapper cleaning, and power line maintenance in hard-to-reach
and risky environments while retaining compatibility with flight control
firmware. Our RL-based control mechanism results in a robust control strategy
that can handle uncertainties in the motion of the UAV, offering promising
performance. Specifically, our method achieves 92\% accuracy in terms of
average displacement error (i.e. the mean distance between the target and
obtained trajectory points) using Q-learning with 15,000 episodes
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12844">Probabilistic load forecasting with Reservoir Computing. (arXiv:2308.12844v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guerra_M/0/1/0/all/0/1">Michele Guerra</a>, <a href="http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1">Simone Scardapane</a>, <a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1">Filippo Maria Bianchi</a></p>
<p>Some applications of deep learning require not only to provide accurate
results but also to quantify the amount of confidence in their prediction. The
management of an electric power grid is one of these cases: to avoid risky
scenarios, decision-makers need both precise and reliable forecasts of, for
example, power loads. For this reason, point forecasts are not enough hence it
is necessary to adopt methods that provide an uncertainty quantification.
</p>
<p>This work focuses on reservoir computing as the core time series forecasting
method, due to its computational efficiency and effectiveness in predicting
time series. While the RC literature mostly focused on point forecasting, this
work explores the compatibility of some popular uncertainty quantification
methods with the reservoir setting. Both Bayesian and deterministic approaches
to uncertainty assessment are evaluated and compared in terms of their
prediction accuracy, computational resource efficiency and reliability of the
estimated uncertainty, based on a set of carefully chosen performance metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12857">Fast Adversarial Training with Smooth Convergence. (arXiv:2308.12857v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1">Mengnan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lihe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1">Yuqiu Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Baocai Yin</a></p>
<p>Fast adversarial training (FAT) is beneficial for improving the adversarial
robustness of neural networks. However, previous FAT work has encountered a
significant issue known as catastrophic overfitting when dealing with large
perturbation budgets, \ie the adversarial robustness of models declines to near
zero during training.
</p>
<p>To address this, we analyze the training process of prior FAT work and
observe that catastrophic overfitting is accompanied by the appearance of loss
convergence outliers.
</p>
<p>Therefore, we argue a moderately smooth loss convergence process will be a
stable FAT process that solves catastrophic overfitting.
</p>
<p>To obtain a smooth loss convergence process, we propose a novel oscillatory
constraint (dubbed ConvergeSmooth) to limit the loss difference between
adjacent epochs. The convergence stride of ConvergeSmooth is introduced to
balance convergence and smoothing. Likewise, we design weight centralization
without introducing additional hyperparameters other than the loss balance
coefficient.
</p>
<p>Our proposed methods are attack-agnostic and thus can improve the training
stability of various FAT techniques.
</p>
<p>Extensive experiments on popular datasets show that the proposed methods
efficiently avoid catastrophic overfitting and outperform all previous FAT
methods. Code is available at \url{https://github.com/FAT-CS/ConvergeSmooth}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12859">Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture. (arXiv:2308.12859v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Juan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Borchers_D/0/1/0/all/0/1">David L. Borchers</a></p>
<p>Passive acoustic monitoring can be an effective way of monitoring wildlife
populations that are acoustically active but difficult to survey visually.
Digital recorders allow surveyors to gather large volumes of data at low cost,
but identifying target species vocalisations in these data is non-trivial.
Machine learning (ML) methods are often used to do the identification. They can
process large volumes of data quickly, but they do not detect all vocalisations
and they do generate some false positives (vocalisations that are not from the
target species). Existing wildlife abundance survey methods have been designed
specifically to deal with the first of these mistakes, but current methods of
dealing with false positives are not well-developed. They do not take account
of features of individual vocalisations, some of which are more likely to be
false positives than others. We propose three methods for acoustic spatial
capture-recapture inference that integrate individual-level measures of
confidence from ML vocalisation identification into the likelihood and hence
integrate ML uncertainty into inference. The methods include a mixture model in
which species identity is a latent variable. We test the methods by simulation
and find that in a scenario based on acoustic data from Hainan gibbons, in
which ignoring false positives results in 17% positive bias, our methods give
negligible bias and coverage probabilities that are close to the nominal 95%
level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12864">Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution. (arXiv:2308.12864v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_S/0/1/0/all/0/1">Sarah Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Poncet_P/0/1/0/all/0/1">Philippe Poncet</a></p>
<p>In this article, we present a novel data assimilation strategy in pore-scale
imaging and demonstrate that this makes it possible to robustly address
reactive inverse problems incorporating Uncertainty Quantification (UQ).
Pore-scale modeling of reactive flow offers a valuable opportunity to
investigate the evolution of macro-scale properties subject to dynamic
processes. Yet, they suffer from imaging limitations arising from the
associated X-ray microtomography (X-ray microCT) process, which induces
discrepancies in the properties estimates. Assessment of the kinetic parameters
also raises challenges, as reactive coefficients are critical parameters that
can cover a wide range of values. We account for these two issues and ensure
reliable calibration of pore-scale modeling, based on dynamical microCT images,
by integrating uncertainty quantification in the workflow.
</p>
<p>The present method is based on a multitasking formulation of reactive inverse
problems combining data-driven and physics-informed techniques in calcite
dissolution. This allows quantifying morphological uncertainties on the
porosity field and estimating reactive parameter ranges through prescribed PDE
models with a latent concentration field and dynamical microCT. The data
assimilation strategy relies on sequential reinforcement incorporating
successively additional PDE constraints. We guarantee robust and unbiased
uncertainty quantification by straightforward adaptive weighting of Bayesian
Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity
changes during geochemical transformations. We demonstrate successful Bayesian
Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT
images with meaningful posterior distribution on the reactive parameters and
dimensionless numbers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12871">IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency. (arXiv:2308.12871v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghafouri_S/0/1/0/all/0/1">Saeid Ghafouri</a>, <a href="http://arxiv.org/find/cs/1/au:+Razavi_K/0/1/0/all/0/1">Kamran Razavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Salmani_M/0/1/0/all/0/1">Mehran Salmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanaee_A/0/1/0/all/0/1">Alireza Sanaee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lorido_Botran_T/0/1/0/all/0/1">Tania Lorido-Botran</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Doyle_J/0/1/0/all/0/1">Joseph Doyle</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamshidi_P/0/1/0/all/0/1">Pooyan Jamshidi</a></p>
<p>Efficiently optimizing multi-model inference pipelines for fast, accurate,
and cost-effective inference is a crucial challenge in ML production systems,
given their tight end-to-end latency requirements. To simplify the exploration
of the vast and intricate trade-off space of accuracy and cost in inference
pipelines, providers frequently opt to consider one of them. However, the
challenge lies in reconciling accuracy and cost trade-offs. To address this
challenge and propose a solution to efficiently manage model variants in
inference pipelines, we present IPA, an online deep-learning Inference Pipeline
Adaptation system that efficiently leverages model variants for each deep
learning task. Model variants are different versions of pre-trained models for
the same deep learning task with variations in resource requirements, latency,
and accuracy. IPA dynamically configures batch size, replication, and model
variants to optimize accuracy, minimize costs, and meet user-defined latency
SLAs using Integer Programming. It supports multi-objective settings for
achieving different trade-offs between accuracy and cost objectives while
remaining adaptable to varying workloads and dynamic traffic patterns.
Extensive experiments on a Kubernetes implementation with five real-world
inference pipelines demonstrate that IPA improves normalized accuracy by up to
35% with a minimal cost increase of less than 5%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12874">Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanchis_Agudo_M/0/1/0/all/0/1">Marcial Sanchis-Agudo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duraisamy_K/0/1/0/all/0/1">Karthik Duraisamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinuesa_R/0/1/0/all/0/1">Ricardo Vinuesa</a></p>
<p>To improve the robustness of transformer neural networks used for
temporal-dynamics prediction of chaotic systems, we propose a novel attention
mechanism called easy attention. Due to the fact that self attention only makes
usage of the inner product of queries and keys, it is demonstrated that the
keys, queries and softmax are not necessary for obtaining the attention score
required to capture long-term dependencies in temporal sequences. Through
implementing singular-value decomposition (SVD) on the softmax attention score,
we further observe that the self attention compresses contribution from both
queries and keys in the spanned space of the attention score. Therefore, our
proposed easy-attention method directly treats the attention scores as
learnable parameters. This approach produces excellent results when
reconstructing and predicting the temporal dynamics of chaotic systems
exhibiting more robustness and less complexity than the self attention or the
widely-used long short-term memory (LSTM) network. Our results show great
potential for applications in more complex high-dimensional dynamical systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12882">LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition. (arXiv:2308.12882v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dibbo_S/0/1/0/all/0/1">Sayanton V. Dibbo</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1">Juston S. Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1">Garrett T. Kenyon</a>, <a href="http://arxiv.org/find/cs/1/au:+Teti_M/0/1/0/all/0/1">Michael A. Teti</a></p>
<p>Audio classification aims at recognizing audio signals, including speech
commands or sound events. However, current audio classifiers are susceptible to
perturbations and adversarial attacks. In addition, real-world audio
classification tasks often suffer from limited labeled data. To help bridge
these gaps, previous work developed neuro-inspired convolutional neural
networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)
in the first layer (i.e., LCANets) for computer vision. LCANets learn in a
combination of supervised and unsupervised learning, reducing dependency on
labeled samples. Motivated by the fact that auditory cortex is also sparse, we
extend LCANets to audio recognition tasks and introduce LCANets++, which are
CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that
LCANets++ are more robust than standard CNNs and LCANets against perturbations,
e.g., background noise, as well as black-box and white-box attacks, e.g.,
evasion and fast gradient sign (FGSM) attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12885">Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection. (arXiv:2308.12885v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Inel_O/0/1/0/all/0/1">Oana Inel</a>, <a href="http://arxiv.org/find/cs/1/au:+Draws_T/0/1/0/all/0/1">Tim Draws</a>, <a href="http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1">Lora Aroyo</a></p>
<p>The rapid entry of machine learning approaches in our daily activities and
high-stakes domains demands transparency and scrutiny of their fairness and
reliability. To help gauge machine learning models' robustness, research
typically focuses on the massive datasets used for their deployment, e.g.,
creating and maintaining documentation for understanding their origin, process
of development, and ethical considerations. However, data collection for AI is
still typically a one-off practice, and oftentimes datasets collected for a
certain purpose or application are reused for a different problem.
Additionally, dataset annotations may not be representative over time, contain
ambiguous or erroneous annotations, or be unable to generalize across issues or
domains. Recent research has shown these practices might lead to unfair,
biased, or inaccurate outcomes. We argue that data collection for AI should be
performed in a responsible manner where the quality of the data is thoroughly
scrutinized and measured through a systematic set of appropriate metrics. In
this paper, we propose a Responsible AI (RAI) methodology designed to guide the
data collection with a set of metrics for an iterative in-depth analysis of the
factors influencing the quality and reliability} of the generated data. We
propose a granular set of measurements to inform on the internal reliability of
a dataset and its external stability over time. We validate our approach across
nine existing datasets and annotation tasks and four content modalities. This
approach impacts the assessment of data robustness used for AI applied in the
real world, where diversity of users and content is eminent. Furthermore, it
deals with fairness and accountability aspects in data collection by providing
systematic and transparent quality analysis for data collections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12896">Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Landeghem_J/0/1/0/all/0/1">Jordy Van Landeghem</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Sanket Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew B. Blaschko</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a></p>
<p>This paper highlights the need to bring document classification benchmarking
closer to real-world applications, both in the nature of data tested ($X$:
multi-channel, multi-paged, multi-industry; $Y$: class distributions and label
set variety) and in classification tasks considered ($f$: multi-page document,
page stream, and document bundle classification, ...). We identify the lack of
public multi-page document classification datasets, formalize different
classification tasks arising in application scenarios, and motivate the value
of targeting efficient multi-page document representations. An experimental
study on proposed multi-page document classification datasets demonstrates that
current benchmarks have become irrelevant and need to be updated to evaluate
complete documents, as they naturally occur in practice. This reality check
also calls for more mature evaluation methodologies, covering calibration
evaluation, inference complexity (time-memory), and a range of realistic
distribution shifts (e.g., born-digital vs. scanning noise, shifting page
order). Our study ends on a hopeful note by recommending concrete avenues for
future improvements.}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12899">Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]. (arXiv:2308.12899v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jiawei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chengkai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyuan Wang</a></p>
<p>The field of urban spatial-temporal prediction is advancing rapidly with the
development of deep learning techniques and the availability of large-scale
datasets. However, challenges persist in accessing and utilizing diverse urban
spatial-temporal datasets from different sources and stored in different
formats, as well as determining effective model structures and components with
the proliferation of deep learning models. This work addresses these challenges
and provides three significant contributions. Firstly, we introduce "atomic
files", a unified storage format designed for urban spatial-temporal big data,
and validate its effectiveness on 40 diverse datasets, simplifying data
management. Secondly, we present a comprehensive overview of technological
advances in urban spatial-temporal prediction models, guiding the development
of robust models. Thirdly, we conduct extensive experiments using diverse
models and datasets, establishing a performance leaderboard and identifying
promising research directions. Overall, this work effectively manages urban
spatial-temporal data, guides future efforts, and facilitates the development
of accurate and efficient urban spatial-temporal prediction models. It can
potentially make long-term contributions to urban spatial-temporal data
management and prediction, ultimately leading to improved urban living
standards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12902">CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shakibania_H/0/1/0/all/0/1">Hossein Shakibania</a>, <a href="http://arxiv.org/find/cs/1/au:+Raoufi_S/0/1/0/all/0/1">Sina Raoufi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khotanlou_H/0/1/0/all/0/1">Hassan Khotanlou</a></p>
<p>Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN's potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12908">POLCA: Power Oversubscription in LLM Cloud Providers. (arXiv:2308.12908v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1">Pratyush Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Choukse_E/0/1/0/all/0/1">Esha Choukse</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaojie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Goiri_I/0/1/0/all/0/1">&#xcd;&#xf1;igo Goiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Warrier_B/0/1/0/all/0/1">Brijesh Warrier</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahalingam_N/0/1/0/all/0/1">Nithish Mahalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bianchini_R/0/1/0/all/0/1">Ricardo Bianchini</a></p>
<p>Recent innovation in large language models (LLMs), and their myriad use-cases
have rapidly driven up the compute capacity demand for datacenter GPUs. Several
cloud providers and other enterprises have made substantial plans of growth in
their datacenters to support these new workloads. One of the key bottleneck
resources in datacenters is power, and given the increasing model sizes of
LLMs, they are becoming increasingly power intensive. In this paper, we show
that there is a significant opportunity to oversubscribe power in LLM clusters.
Power oversubscription improves the power efficiency of these datacenters,
allowing more deployable servers per datacenter, and reduces the deployment
time, since building new datacenters is slow.
</p>
<p>We extensively characterize the power consumption patterns of a variety of
LLMs and their configurations. We identify the differences between the
inference and training power consumption patterns. Based on our analysis of
these LLMs, we claim that the average and peak power utilization in LLM
clusters for inference should not be very high. Our deductions align with the
data from production LLM clusters, revealing that inference workloads offer
substantial headroom for power oversubscription. However, the stringent set of
telemetry and controls that GPUs offer in a virtualized environment, makes it
challenging to have a reliable and robust power oversubscription mechanism.
</p>
<p>We propose POLCA, our framework for power oversubscription that is robust,
reliable, and readily deployable for GPU clusters. Using open-source models to
replicate the power patterns observed in production, we simulate POLCA and
demonstrate that we can deploy 30% more servers in the same GPU cluster for
inference, with minimal performance loss
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12918">Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks. (arXiv:2308.12918v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Harshith_J/0/1/0/all/0/1">John Harshith</a>, <a href="http://arxiv.org/find/cs/1/au:+Gill_M/0/1/0/all/0/1">Mantej Singh Gill</a>, <a href="http://arxiv.org/find/cs/1/au:+Jothimani_M/0/1/0/all/0/1">Madhan Jothimani</a></p>
<p>There have been recent adversarial attacks that are difficult to find. These
new adversarial attacks methods may pose challenges to current deep learning
cyber defense systems and could influence the future defense of cyberattacks.
The authors focus on this domain in this research paper. They explore the
consequences of vulnerabilities in AI systems. This includes discussing how
they might arise, differences between randomized and adversarial examples and
also potential ethical implications of vulnerabilities. Moreover, it is
important to train the AI systems appropriately when they are in testing phase
and getting them ready for broader use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12919">Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1">Lijun Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tieniu Tan</a></p>
<p>The emergence of vision-language models (VLMs), such as CLIP, has spurred a
significant research effort towards their application for downstream supervised
learning tasks. Although some previous studies have explored the unsupervised
fine-tuning of CLIP, they often rely on prior knowledge in the form of class
names associated with ground truth labels. In this paper, we delve into a
realistic unsupervised fine-tuning scenario by assuming that the unlabeled data
might contain out-of-distribution samples from unknown classes. Furthermore, we
emphasize the importance of simultaneously enhancing out-of-distribution
detection capabilities alongside the recognition of instances associated with
predefined class labels.
</p>
<p>To tackle this problem, we present a simple, efficient, and effective
fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages
sample-level confidence to approximately minimize the conditional entropy of
confident instances and maximize the marginal entropy of less confident
instances. Apart from optimizing the textual prompts, UEO also incorporates
optimization of channel-wise affine transformations within the visual branch of
CLIP. Through extensive experiments conducted across 15 domains and 4 different
types of prior knowledge, we demonstrate that UEO surpasses baseline methods in
terms of both generalization and out-of-distribution detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12921">An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control. (arXiv:2308.12921v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shojaeighadikolaei_A/0/1/0/all/0/1">Amin Shojaeighadikolaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1">Morteza Hashemi</a></p>
<p>The increasing trend in adopting electric vehicles (EVs) will significantly
impact the residential electricity demand, which results in an increased risk
of transformer overload in the distribution grid. To mitigate such risks, there
are urgent needs to develop effective EV charging controllers. Currently, the
majority of the EV charge controllers are based on a centralized approach for
managing individual EVs or a group of EVs. In this paper, we introduce a
decentralized Multi-agent Reinforcement Learning (MARL) charging framework that
prioritizes the preservation of privacy for EV owners. We employ the
Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient
(CTDE-DDPG) scheme, which provides valuable information to users during
training while maintaining privacy during execution. Our results demonstrate
that the CTDE framework improves the performance of the charging network by
reducing the network costs. Moreover, we show that the Peak-to-Average Ratio
(PAR) of the total demand is reduced, which, in turn, reduces the risk of
transformer overload during the peak hours.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12923">Diagnosing Infeasible Optimization Problems Using Large Language Models. (arXiv:2308.12923v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Constante_Flores_G/0/1/0/all/0/1">Gonzalo E. Constante-Flores</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Can Li</a></p>
<p>Decision-making problems can be represented as mathematical optimization
models, finding wide applications in fields such as economics, engineering and
manufacturing, transportation, and health care. Optimization models are
mathematical abstractions of the problem of making the best decision while
satisfying a set of requirements or constraints. One of the primary barriers to
deploying these models in practice is the challenge of helping practitioners
understand and interpret such models, particularly when they are infeasible,
meaning no decision satisfies all the constraints. Existing methods for
diagnosing infeasible optimization models often rely on expert systems,
necessitating significant background knowledge in optimization. In this paper,
we introduce OptiChat, a first-of-its-kind natural language-based system
equipped with a chatbot GUI for engaging in interactive conversations about
infeasible optimization models. OptiChat can provide natural language
descriptions of the optimization model itself, identify potential sources of
infeasibility, and offer suggestions to make the model feasible. The
implementation of OptiChat is built on GPT-4, which interfaces with an
optimization solver to identify the minimal subset of constraints that render
the entire optimization problem infeasible, also known as the Irreducible
Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought,
key-retrieve, and sentiment prompts to enhance OptiChat's reliability. Our
experiments demonstrate that OptiChat assists both expert and non-expert users
in improving their understanding of the optimization models, enabling them to
quickly identify the sources of infeasibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12925">Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1">Philipp Renz</a>, <a href="http://arxiv.org/find/cs/1/au:+Cutajar_K/0/1/0/all/0/1">Kurt Cutajar</a>, <a href="http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1">Niall Twomey</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1">Gavin K. C. Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hanting Xie</a></p>
<p>Low-count time series describe sparse or intermittent events, which are
prevalent in large-scale online platforms that capture and monitor diverse data
types. Several distinct challenges surface when modelling low-count time
series, particularly low signal-to-noise ratios (when anomaly signatures are
provably undetectable), and non-uniform performance (when average metrics are
not representative of local behaviour). The time series anomaly detection
community currently lacks explicit tooling and processes to model and reliably
detect anomalies in these settings. We address this gap by introducing a novel
generative procedure for creating benchmark datasets comprising of low-count
time series with anomalous segments. Via a mixture of theoretical and empirical
analysis, our work explains how widely-used algorithms struggle with the
distribution overlap between normal and anomalous segments. In order to
mitigate this shortcoming, we then leverage our findings to demonstrate how
anomaly score smoothing consistently improves performance. The practical
utility of our analysis and recommendation is validated on a real-world dataset
containing sales data for retail stores.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12939">Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries. (arXiv:2308.12939v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zhiwei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1">Paris Perdikaris</a></p>
<p>Recently deep learning surrogates and neural operators have shown promise in
solving partial differential equations (PDEs). However, they often require a
large amount of training data and are limited to bounded domains. In this work,
we present a novel physics-informed neural operator method to solve
parametrized boundary value problems without labeled data. By reformulating the
PDEs into boundary integral equations (BIEs), we can train the operator network
solely on the boundary of the domain. This approach reduces the number of
required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's
dimension, leading to a significant acceleration of the training process.
Additionally, our method can handle unbounded problems, which are unattainable
for existing physics-informed neural networks (PINNs) and neural operators. Our
numerical experiments show the effectiveness of parametrized complex geometries
and unbounded problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12949">Label Budget Allocation in Multi-Task Learning. (arXiv:2308.12949v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Ximeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1">Kate Saenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Mellina_C/0/1/0/all/0/1">Clayton Mellina</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1">Xiao Bian</a></p>
<p>The cost of labeling data often limits the performance of machine learning
systems. In multi-task learning, related tasks provide information to each
other and improve overall performance, but the label cost can vary among tasks.
How should the label budget (i.e. the amount of money spent on labeling) be
allocated among different tasks to achieve optimal multi-task performance? We
are the first to propose and formally define the label budget allocation
problem in multi-task learning and to empirically show that different budget
allocation strategies make a big difference to its performance. We propose a
Task-Adaptive Budget Allocation algorithm to robustly generate the optimal
budget allocation adaptive to different multi-task learning settings.
Specifically, we estimate and then maximize the extent of new information
obtained from the allocated budget as a proxy for multi-task learning
performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy
of our approach over other widely used heuristic labeling strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12952">BridgeData V2: A Dataset for Robot Learning at Scale. (arXiv:2308.12952v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Walke_H/0/1/0/all/0/1">Homer Walke</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_K/0/1/0/all/0/1">Kevin Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1">Abraham Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Moo Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1">Max Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chongyi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tony Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansen_Estruch_P/0/1/0/all/0/1">Philippe Hansen-Estruch</a>, <a href="http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1">Quan Vuong</a>, <a href="http://arxiv.org/find/cs/1/au:+He_A/0/1/0/all/0/1">Andre He</a>, <a href="http://arxiv.org/find/cs/1/au:+Myers_V/0/1/0/all/0/1">Vivek Myers</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1">Kuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>We introduce BridgeData V2, a large and diverse dataset of robotic
manipulation behaviors designed to facilitate research on scalable robot
learning. BridgeData V2 contains 60,096 trajectories collected across 24
environments on a publicly available low-cost robot. BridgeData V2 provides
extensive task and environment variability, leading to skills that can
generalize across environments, domains, and institutions, making the dataset a
useful resource for a broad range of researchers. Additionally, the dataset is
compatible with a wide variety of open-vocabulary, multi-task learning methods
conditioned on goal images or natural language instructions. In our
experiments, we train 6 state-of-the-art imitation learning and offline
reinforcement learning methods on our dataset, and find that they succeed on a
suite of tasks requiring varying amounts of generalization. We also demonstrate
that the performance of these methods improves with more data and higher
capacity models, and that training on a greater variety of skills leads to
improved generalization. By publicly sharing BridgeData V2 and our pre-trained
models, we aim to accelerate research in scalable robot learning methods.
Project page at https://rail-berkeley.github.io/bridgedata
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12956">DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1">Huafeng Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiawu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Ming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xuefeng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Min Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>Vision-Language Pre-training (VLP) shows remarkable progress with the
assistance of extremely heavy parameters, which challenges deployment in real
applications. Knowledge distillation is well recognized as the essential
procedure in model compression. However, existing knowledge distillation
techniques lack an in-depth investigation and analysis of VLP, and practical
guidelines for VLP-oriented distillation are still not yet explored. In this
paper, we present DLIP, a simple yet efficient Distilling Language-Image
Pre-training framework, through which we investigate how to distill a light VLP
model. Specifically, we dissect the model distillation from multiple
dimensions, such as the architecture characteristics of different modules and
the information transfer of different modalities. We conduct comprehensive
experiments and provide insights on distilling a light but performant VLP
model. Experimental results reveal that DLIP can achieve a state-of-the-art
accuracy/efficiency trade-off across diverse cross-modal tasks, e.g.,
image-text retrieval, image captioning and visual question answering. For
example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while
achieving comparable or better performance. Furthermore, DLIP succeeds in
retaining more than 95% of the performance with 22.4% parameters and 24.8%
FLOPs compared to the teacher model and accelerates inference speed by 2.7x.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12964">Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunji Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jiyoung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jin-Hwa Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1">Jung-Woo Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun-Yan Zhu</a></p>
<p>Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12967">NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes. (arXiv:2308.12967v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1">Muhammad Zubair Irshad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1">Sergey Zakharov</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Katherine Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1">Vitor Guizilini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1">Thomas Kollar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1">Adrien Gaidon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1">Zsolt Kira</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1">Rares Ambrus</a></p>
<p>Recent implicit neural representations have shown great results for novel
view synthesis. However, existing methods require expensive per-scene
optimization from many views hence limiting their application to real-world
unbounded urban settings where the objects of interest or backgrounds are
observed from very few views. To mitigate this challenge, we introduce a new
approach called NeO 360, Neural fields for sparse view synthesis of outdoor
scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes
from a single or a few posed RGB images. The essence of our approach is in
capturing the distribution of complex real-world outdoor 3D scenes and using a
hybrid image-conditional triplanar representation that can be queried from any
world point. Our representation combines the best of both voxel-based and
bird's-eye-view (BEV) representations and is more effective and expressive than
each. NeO 360's representation allows us to learn from a large collection of
unbounded 3D scenes while offering generalizability to new views and novel
scenes from as few as a single image during inference. We demonstrate our
approach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS
360, and show that NeO 360 outperforms state-of-the-art generalizable methods
for novel view synthesis while also offering editing and composition
capabilities. Project page:
https://zubair-irshad.github.io/projects/neo360.html
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12968">Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation. (arXiv:2308.12968v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuxin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Liming Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a></p>
<p>Automatic high-quality rendering of anime scenes from complex real-world
images is of significant practical value. The challenges of this task lie in
the complexity of the scenes, the unique features of anime style, and the lack
of high-quality datasets to bridge the domain gap. Despite promising attempts,
previous efforts are still incompetent in achieving satisfactory results with
consistent semantic preservation, evident stylization, and fine details. In
this study, we propose Scenimefy, a novel semi-supervised image-to-image
translation framework that addresses these challenges. Our approach guides the
learning with structure-consistent pseudo paired data, simplifying the pure
unsupervised setting. The pseudo data are derived uniquely from a
semantic-constrained StyleGAN leveraging rich model priors like CLIP. We
further apply segmentation-guided data selection to obtain high-quality pseudo
supervision. A patch-wise contrastive style loss is introduced to improve
stylization and fine details. Besides, we contribute a high-resolution anime
scene dataset to facilitate future research. Our extensive experiments
demonstrate the superiority of our method over state-of-the-art baselines in
terms of both perceptual quality and quantitative performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12970">NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory. (arXiv:2308.12970v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kairanda_N/0/1/0/all/0/1">Navami Kairanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1">Marc Habermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a>, <a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1">Vladislav Golyanik</a></p>
<p>Cloth simulation is an extensively studied problem, with a plethora of
solutions available in computer graphics literature. Existing cloth simulators
produce realistic cloth deformations that obey different types of boundary
conditions. Nevertheless, their operational principle remains limited in
several ways: They operate on explicit surface representations with a fixed
spatial resolution, perform a series of discretised updates (which bounds their
temporal resolution), and require comparably large amounts of storage.
Moreover, back-propagating gradients through the existing solvers is often not
straightforward, which poses additional challenges when integrating them into
modern neural architectures. In response to the limitations mentioned above,
this paper takes a fundamentally different perspective on physically-plausible
cloth simulation and re-thinks this long-standing problem: We propose
NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in
which surface evolution is encoded in neural network weights. Our
memory-efficient and differentiable solver operates on a new continuous
coordinate-based representation of dynamic surfaces, i.e., neural deformation
fields (NDFs); it supervises NDF evolution with the rules of the non-linear
Kirchhoff-Love shell theory. NDFs are adaptive in the sense that they 1)
allocate their capacity to the deformation details as the latter arise during
the cloth evolution and 2) allow surface state queries at arbitrary spatial and
temporal resolutions without retraining. We show how to train our
NeuralClothSim solver while imposing hard boundary conditions and demonstrate
multiple applications, such as material interpolation and simulation editing.
The experimental results highlight the effectiveness of our formulation and its
potential impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2008.09312">Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1">Shiliang Zuo</a></p>
<p>I study a stochastic multi-arm bandit problem where rewards are subject to
adversarial corruption. I propose a novel attack strategy that manipulates a
learner employing the UCB algorithm into pulling some non-optimal target arm $T
- o(T)$ times with a cumulative cost that scales as $\widehat{O}(\sqrt{\log
T})$, where $T$ is the number of rounds. I also prove the first lower bound on
the cumulative attack cost. The lower bound matches the upper bound up to
$O(\log \log T)$ factors, showing the proposed attack strategy to be near
optimal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2010.11925">The Polynomial Method is Universal for Distribution-Free Correlational SQ Learning. (arXiv:2010.11925v3 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gollakota_A/0/1/0/all/0/1">Aravind Gollakota</a>, <a href="http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1">Sushrut Karmalkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Klivans_A/0/1/0/all/0/1">Adam Klivans</a></p>
<p>We consider the problem of distribution-free learning for Boolean function
classes in the PAC and agnostic models. Generalizing a beautiful work of Malach
and Shalev-Shwartz (2022) that gave tight correlational SQ (CSQ) lower bounds
for learning DNF formulas, we give new proofs that lower bounds on the
threshold or approximate degree of any function class directly imply CSQ lower
bounds for PAC or agnostic learning respectively. While such bounds implicitly
follow by combining prior results by Feldman (2008, 2012) and Sherstov (2008,
2011), to our knowledge the precise statements we give had not appeared in this
form before. Moreover, our proofs are simple and largely self-contained.
</p>
<p>These lower bounds match corresponding positive results using upper bounds on
the threshold or approximate degree in the SQ model for PAC or agnostic
learning, and in this sense these results show that the polynomial method is a
universal, best-possible approach for distribution-free CSQ learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.12429">BagPipe: Accelerating Deep Recommendation Model Training. (arXiv:2202.12429v3 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Saurabh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1">Chengpo Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1">Shivaram Venkataraman</a></p>
<p>Deep learning based recommendation models (DLRM) are widely used in several
business critical applications. Training such recommendation models efficiently
is challenging because they contain billions of embedding-based parameters,
leading to significant overheads from embedding access. By profiling existing
systems for DLRM training, we observe that around 75\% of the iteration time is
spent on embedding access and model synchronization. Our key insight in this
paper is that embedding access has a specific structure which can be used to
accelerate training. We observe that embedding accesses are heavily skewed,
with around 1\% of embeddings representing more than 92\% of total accesses.
Further, we observe that during offline training we can lookahead at future
batches to determine exactly which embeddings will be needed at what iteration
in the future. Based on these insights, we develop Bagpipe, a system for
training deep recommendation models that uses caching and prefetching to
overlap remote embedding accesses with the computation. We design an Oracle
Cacher, a new component that uses a lookahead algorithm to generate optimal
cache update decisions while providing strong consistency guarantees against
staleness. We also design a logically replicated, physically partitioned cache
and show that our design can reduce synchronization overheads in a distributed
setting. Finally, we propose a disaggregated system architecture and show that
our design can enable low-overhead fault tolerance. Our experiments using three
datasets and four models show that Bagpipe provides a speed up of up to 5.6x
compared to state of the art baselines, while providing the same convergence
and reproducibility guarantees as synchronous training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.16331">FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verwer_S/0/1/0/all/0/1">Sicco Verwer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammerschmidt_C/0/1/0/all/0/1">Christian Hammerschmidt</a></p>
<p>We present the efficient implementations of probabilistic deterministic
finite automaton learning methods available in FlexFringe. These implement
well-known strategies for state-merging including several modifications to
improve their performance in practice. We show experimentally that these
algorithms obtain competitive results and significant improvements over a
default implementation. We also demonstrate how to use FlexFringe to learn
interpretable models from software logs and use these for anomaly detection.
Although less interpretable, we show that learning smaller more convoluted
models improves the performance of FlexFringe on anomaly detection,
outperforming an existing solution based on neural nets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.11418">Riemannian Hamiltonian methods for min-max optimization on manifolds. (arXiv:2204.11418v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Han_A/0/1/0/all/0/1">Andi Han</a>, <a href="http://arxiv.org/find/math/1/au:+Mishra_B/0/1/0/all/0/1">Bamdev Mishra</a>, <a href="http://arxiv.org/find/math/1/au:+Jawanpuria_P/0/1/0/all/0/1">Pratik Jawanpuria</a>, <a href="http://arxiv.org/find/math/1/au:+Kumar_P/0/1/0/all/0/1">Pawan Kumar</a>, <a href="http://arxiv.org/find/math/1/au:+Gao_J/0/1/0/all/0/1">Junbin Gao</a></p>
<p>In this paper, we study min-max optimization problems on Riemannian
manifolds. We introduce a Riemannian Hamiltonian function, minimization of
which serves as a proxy for solving the original min-max problems. Under the
Riemannian Polyak--{\L}ojasiewicz condition on the Hamiltonian function, its
minimizer corresponds to the desired min-max saddle point. We also provide
cases where this condition is satisfied. For geodesic-bilinear optimization in
particular, solving the proxy problem leads to the correct search direction
towards global optimality, which becomes challenging with the min-max
formulation. To minimize the Hamiltonian function, we propose Riemannian
Hamiltonian methods (RHM) and present their convergence analyses. We extend RHM
to include consensus regularization and to the stochastic setting. We
illustrate the efficacy of the proposed RHM in applications such as subspace
robust Wasserstein distance, robust training of neural networks, and generative
adversarial networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.04701">StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random. (arXiv:2205.04701v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chunyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1">Peng Wu</a></p>
<p>In recommender systems, users always choose the favorite items to rate, which
leads to data missing not at random and poses a great challenge for unbiased
evaluation and learning of prediction models. Currently, the doubly robust (DR)
methods have been widely studied and demonstrate superior performance. However,
in this paper, we show that DR methods are unstable and have unbounded bias,
variance, and generalization bounds to extremely small propensities. Moreover,
the fact that DR relies more on extrapolation will lead to suboptimal
performance. To address the above limitations while retaining double
robustness, we propose a stabilized doubly robust (StableDR) learning approach
with a weaker reliance on extrapolation. Theoretical analysis shows that
StableDR has bounded bias, variance, and generalization error bound
simultaneously under inaccurate imputed errors and arbitrarily small
propensities. In addition, we propose a novel learning approach for StableDR
that updates the imputation, propensity, and prediction models cyclically,
achieving more stable and accurate predictions. Extensive experiments show that
our approaches significantly outperform the existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.09107">Leveraging Global Binary Masks for Structure Segmentation in Medical Images. (arXiv:2205.09107v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kazemimoghadam_M/0/1/0/all/0/1">Mahdieh Kazemimoghadam</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zi Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1">Lin Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1">Mingli Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1">Weiguo Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu_X/0/1/0/all/0/1">Xuejun Gu</a></p>
<p>Deep learning (DL) models for medical image segmentation are highly
influenced by intensity variations of input images and lack generalization due
to primarily utilizing pixels' intensity information for inference. Acquiring
sufficient training data is another challenge limiting models' applications. We
proposed to leverage the consistency of organs' anatomical shape and position
information in medical images. We introduced a framework leveraging recurring
anatomical patterns through global binary masks for organ segmentation. Two
scenarios were studied.1) Global binary masks were the only model's (i.e.
U-Net) input, forcing exclusively encoding organs' position and shape
information for segmentation/localization.2) Global binary masks were
incorporated as an additional channel functioning as position/shape clues to
mitigate training data scarcity. Two datasets of the brain and heart CT images
with their ground-truth were split into (26:10:10) and (12:3:5) for training,
validation, and test respectively. Training exclusively on global binary masks
led to Dice scores of 0.77(0.06) and 0.85(0.04), with the average Euclidian
distance of 3.12(1.43)mm and 2.5(0.93)mm relative to the center of mass of the
ground truth for the brain and heart structures respectively. The outcomes
indicate that a surprising degree of position and shape information is encoded
through global binary masks. Incorporating global binary masks led to
significantly higher accuracy relative to the model trained on only CT images
in small subsets of training data; the performance improved by 4.3-125.3% and
1.3-48.1% for 1-8 training cases of the brain and heart datasets respectively.
The findings imply the advantages of utilizing global binary masks for building
generalizable models and to compensate for training data scarcity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14473">Efficient-Adam: Communication-Efficient Distributed Adam. (arXiv:2205.14473v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Congliang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhi-Quan Luo</a></p>
<p>Distributed adaptive stochastic gradient methods have been widely used for
large-scale nonconvex optimization, such as training deep learning models.
However, their communication complexity on finding $\varepsilon$-stationary
points has rarely been analyzed in the nonconvex setting. In this work, we
present a novel communication-efficient distributed Adam in the
parameter-server model for stochastic nonconvex optimization, dubbed {\em
Efficient-Adam}. Specifically, we incorporate a two-way quantization scheme
into Efficient-Adam to reduce the communication cost between the workers and
server. Simultaneously, we adopt a two-way error feedback strategy to reduce
the biases caused by the two-way quantization on both the server and workers,
respectively. In addition, we establish the iteration complexity for the
proposed Efficient-Adam with a class of quantization operators, and further
characterize its communication complexity between the server and workers when
an $\varepsilon$-stationary point is achieved. Finally, we apply Efficient-Adam
to solve a toy stochastic convex optimization problem and train deep learning
models on real-world vision and language tasks. Extensive experiments together
with a theoretical guarantee justify the merits of Efficient Adam.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.07240">Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1">Sayna Ebrahimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1">Sercan O. Arik</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a></p>
<p>For visual document understanding (VDU), self-supervised pretraining has been
shown to successfully generate transferable representations, yet, effective
adaptation of such representations to distribution shifts at test-time remains
to be an unexplored area. We propose DocTTA, a novel test-time adaptation
method for documents, that does source-free domain adaptation using unlabeled
target document data. DocTTA leverages cross-modality self-supervised learning
via masked visual language modeling, as well as pseudo labeling to adapt models
learned on a \textit{source} domain to an unlabeled \textit{target} domain at
test time. We introduce new benchmarks using existing public datasets for
various VDU tasks, including entity recognition, key-value extraction, and
document visual question answering. DocTTA shows significant improvements on
these compared to the source model performance, up to 1.89\% in (F1 score),
3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark
datasets are available at \url{https://saynaebrahimi.github.io/DocTTA.html}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.11723">Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bauer_A/0/1/0/all/0/1">Alexander Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1">Shinichi Nakajima</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a></p>
<p>Deep autoencoders provide an effective tool for learning non-linear
dimensionality reduction in an unsupervised way. Recently, they have been used
for the task of anomaly detection in the visual domain. By optimizing for the
reconstruction error using anomaly-free examples, the common belief is that a
corresponding network should fail to accurately reconstruct anomalous regions
in the application phase. This goal is typically addressed by controlling the
capacity of the network, either by reducing the size of the bottleneck layer or
by enforcing sparsity constraints on the activations. However, neither of these
techniques does explicitly penalize reconstruction of anomalous signals often
resulting in poor detection. We tackle this problem by adapting a
self-supervised learning regime that allows the use of discriminative
information during training but focuses on the data manifold of normal
examples. We emphasize that inference with our approach is very efficient
during training and prediction requiring a single forward pass for each input
image. Our experiments on the MVTec AD dataset demonstrate high detection and
localization performance. On the texture-subset, in particular, our approach
consistently outperforms recent anomaly detection methods by a significant
margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.09755">A temporally and spatially local spike-based backpropagation algorithm to enable training in hardware. (arXiv:2207.09755v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1">Anmol Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Saraswat_V/0/1/0/all/0/1">Vivek Saraswat</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguly_U/0/1/0/all/0/1">Udayan Ganguly</a></p>
<p>Spiking Neural Networks (SNNs) have emerged as a hardware efficient
architecture for classification tasks. The challenge of spike-based encoding
has been the lack of a universal training mechanism performed entirely using
spikes. There have been several attempts to adopt the powerful backpropagation
(BP) technique used in non-spiking artificial neural networks (ANN): (1) SNNs
can be trained by externally computed numerical gradients. (2) A major
advancement towards native spike-based learning has been the use of approximate
Backpropagation using spike-time dependent plasticity (STDP) with phased
forward/backward passes. However, the transfer of information between such
phases for gradient and weight update calculation necessitates external memory
and computational access. This is a challenge for standard neuromorphic
hardware implementations. In this paper, we propose a stochastic SNN based
Back-Prop (SSNN-BP) algorithm that utilizes a composite neuron to
simultaneously compute the forward pass activations and backward pass gradients
explicitly with spikes. Although signed gradient values are a challenge for
spike-based representation, we tackle this by splitting the gradient signal
into positive and negative streams. We show that our method approaches BP ANN
baseline with sufficiently long spike-trains. Finally, we show that the
well-performing softmax cross-entropy loss function can be implemented through
inhibitory lateral connections enforcing a Winner Take All (WTA) rule. Our SNN
with a 2-layer network shows excellent generalization through comparable
performance to ANNs with equivalent architecture and regularization parameters
on static image datasets like MNIST, Fashion-MNIST, Extended MNIST, and
temporally encoded image datasets like Neuromorphic MNIST datasets. Thus,
SSNN-BP enables BP compatible with purely spike-based neuromorphic hardware.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.06228">Unifying Gradients to Improve Real-world Robustness for Deep Networks. (arXiv:2208.06228v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1">Yingwen Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_S/0/1/0/all/0/1">Sizhe Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Fang_K/0/1/0/all/0/1">Kun Fang</a>, <a href="http://arxiv.org/find/stat/1/au:+Huang_X/0/1/0/all/0/1">Xiaolin Huang</a></p>
<p>The wide application of deep neural networks (DNNs) demands an increasing
amount of attention to their real-world robustness, i.e., whether a DNN resists
black-box adversarial attacks, among which score-based query attacks (SQAs) are
most threatening since they can effectively hurt a victim network with the only
access to model outputs. Defending against SQAs requires a slight but artful
variation of outputs due to the service purpose for users, who share the same
output information with SQAs. In this paper, we propose a real-world defense by
Unifying Gradients (UniG) of different data so that SQAs could only probe a
much weaker attack direction that is similar for different samples. Since such
universal attack perturbations have been validated as less aggressive than the
input-specific perturbations, UniG protects real-world DNNs by indicating
attackers a twisted and less informative attack direction. We implement UniG
efficiently by a Hadamard product module which is plug-and-play. According to
extensive experiments on 5 SQAs, 2 adaptive attacks and 7 defense baselines,
UniG significantly improves real-world robustness without hurting clean
accuracy on CIFAR10 and ImageNet. For instance, UniG maintains a model of
77.80% accuracy under 2500-query Square attack while the state-of-the-art
adversarially-trained model only has 67.34% on CIFAR10. Simultaneously, UniG
outperforms all compared baselines in terms of clean accuracy and achieves the
smallest modification of the model output. The code is released at
https://github.com/snowien/UniG-pytorch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11945">Efficient Adaptive Activation Rounding for Post-Training Quantization. (arXiv:2208.11945v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Cong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhanda Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yangjie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yuxian Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiaotian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1">Jingwen Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1">Minyi Guo</a></p>
<p>Post-training quantization attracts increasing attention due to its
convenience in deploying quantized neural networks. Although
rounding-to-nearest remains the prevailing method for DNN quantization, prior
research has demonstrated its suboptimal nature when applied to weight
quantization. They propose optimizing weight rounding schemes by leveraging
output error rather than the traditional weight quantization error. Our study
reveals that similar rounding challenges also extend to activation
quantization. Despite the easy generalization, the challenges lie in the
dynamic nature of activation. Adaptive rounding is expected for varying
activations and the method is subjected to runtime overhead. To tackle this, we
propose the AQuant quantization framework with a novel perspective to reduce
output error by adjusting rounding schemes of activations. Instead of using the
constant rounding border 0.5 of the rounding-to-nearest operation, we make the
border become a function w.r.t. the activation value to change the activation
rounding by the adaptive border. To deal with the runtime overhead, we use a
coarse-grained version of the border function. Finally, we introduce our
framework to optimize the border function. Extensive experiments show that
AQuant achieves notable improvements compared to state-of-the-art works and
pushes the accuracy of ResNet-18 up to 60.31% under the 2-bit weight and
activation quantization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.12263">Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haochen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhiyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1">Xiaoyu Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1">Chen Lv</a></p>
<p>Decision-making for urban autonomous driving is challenging due to the
stochastic nature of interactive traffic participants and the complexity of
road structures. Although reinforcement learning (RL)-based decision-making
scheme is promising to handle urban driving scenarios, it suffers from low
sample efficiency and poor adaptability. In this paper, we propose Scene-Rep
Transformer to improve the RL decision-making capabilities with better scene
representation encoding and sequential predictive latent distillation.
Specifically, a multi-stage Transformer (MST) encoder is constructed to model
not only the interaction awareness between the ego vehicle and its neighbors
but also intention awareness between the agents and their candidate routes. A
sequential latent Transformer (SLT) with self-supervised learning objectives is
employed to distill the future predictive information into the latent scene
representation, in order to reduce the exploration space and speed up training.
The final decision-making module based on soft actor-critic (SAC) takes as
input the refined latent scene representation from the Scene-Rep Transformer
and outputs driving actions. The framework is validated in five challenging
simulated urban scenarios with dense traffic, and its performance is manifested
quantitatively by the substantial improvements in data efficiency and
performance in terms of success rate, safety, and efficiency. The qualitative
results reveal that our framework is able to extract the intentions of neighbor
agents to help make decisions and deliver more diversified driving behaviors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.01566">Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables. (arXiv:2209.01566v4 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jian Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gall_H/0/1/0/all/0/1">Harald C. Gall</a></p>
<p>Deep code generation is a topic of deep learning for software engineering
(DL4SE), which adopts neural models to generate code for the intended
functions. Since end-to-end neural methods lack domain knowledge and software
hierarchy awareness, they tend to perform poorly w.r.t project-level tasks. To
systematically explore the potential improvements of code generation, we let it
participate in the whole top-down development from \emph{expressibles} to
\emph{executables}, which is possible in limited scopes. In the process, it
benefits from massive samples, features, and knowledge. As the foundation, we
suggest building a taxonomy on code data, namely code taxonomy, leveraging the
categorization of code information. Moreover, we introduce a three-layer
semantic pyramid (SP) to associate text data and code data. It identifies the
information of different abstraction levels, and thus introduces the domain
knowledge on development and reveals the hierarchy of software. Furthermore, we
propose a semantic pyramid framework (SPF) as the approach, focusing on
software of high modularity and low complexity. SPF divides the code generation
process into stages and reserves spots for potential interactions. In addition,
we conceived preliminary applications in software development to confirm the
neuro-symbolic framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.10634">Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation. (arXiv:2209.10634v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Lipshutz_D/0/1/0/all/0/1">David Lipshutz</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chklovskii_D/0/1/0/all/0/1">Dmitri B. Chklovskii</a></p>
<p>Early sensory systems in the brain rapidly adapt to fluctuating input
statistics, which requires recurrent communication between neurons.
Mechanistically, such recurrent communication is often indirect and mediated by
local interneurons. In this work, we explore the computational benefits of
mediating recurrent communication via interneurons compared with direct
recurrent connections. To this end, we consider two mathematically tractable
recurrent linear neural networks that statistically whiten their inputs -- one
with direct recurrent connections and the other with interneurons that mediate
recurrent communication. By analyzing the corresponding continuous synaptic
dynamics and numerically simulating the networks, we show that the network with
interneurons is more robust to initialization than the network with direct
recurrent connections in the sense that the convergence time for the synaptic
dynamics in the network with interneurons (resp. direct recurrent connections)
scales logarithmically (resp. linearly) with the spectrum of their
initialization. Our results suggest that interneurons are computationally
useful for rapid adaptation to changing input statistics. Interestingly, the
network with interneurons is an overparameterized solution of the whitening
objective for the network with direct recurrent connections, so our results can
be viewed as a recurrent linear neural network analogue of the implicit
acceleration phenomenon observed in overparameterized feedforward linear neural
networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15596">Individual Privacy Accounting with Gaussian Differential Privacy. (arXiv:2209.15596v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koskela_A/0/1/0/all/0/1">Antti Koskela</a>, <a href="http://arxiv.org/find/cs/1/au:+Tobaben_M/0/1/0/all/0/1">Marlon Tobaben</a>, <a href="http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1">Antti Honkela</a></p>
<p>Individual privacy accounting enables bounding differential privacy (DP) loss
individually for each participant involved in the analysis. This can be
informative as often the individual privacy losses are considerably smaller
than those indicated by the DP bounds that are based on considering worst-case
bounds at each data access. In order to account for the individual privacy
losses in a principled manner, we need a privacy accountant for adaptive
compositions of randomised mechanisms, where the loss incurred at a given data
access is allowed to be smaller than the worst-case loss. This kind of analysis
has been carried out for the R\'enyi differential privacy (RDP) by Feldman and
Zrnic (2021), however not yet for the so-called optimal privacy accountants. We
make first steps in this direction by providing a careful analysis using the
Gaussian differential privacy which gives optimal bounds for the Gaussian
mechanism, one of the most versatile DP mechanisms. This approach is based on
determining a certain supermartingale for the hockey-stick divergence and on
extending the R\'enyi divergence-based fully adaptive composition results by
Feldman and Zrnic. We also consider measuring the individual
$(\varepsilon,\delta)$-privacy losses using the so-called privacy loss
distributions. With the help of the Blackwell theorem, we can then make use of
the RDP analysis to construct an approximative individual
$(\varepsilon,\delta)$-accountant.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.00939">Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v6 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Susung Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gyuseong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1">Wooseok Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungryong Kim</a></p>
<p>Denoising diffusion models (DDMs) have attracted attention for their
exceptional generation quality and diversity. This success is largely
attributed to the use of class- or text-conditional diffusion guidance methods,
such as classifier and classifier-free guidance. In this paper, we present a
more comprehensive perspective that goes beyond the traditional guidance
methods. From this generalized perspective, we introduce novel condition- and
training-free strategies to enhance the quality of generated images. As a
simple solution, blur guidance improves the suitability of intermediate samples
for their fine-scale information and structures, enabling diffusion models to
generate higher quality samples with a moderate guidance scale. Improving upon
this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps
of diffusion models to enhance their stability and efficacy. Specifically, SAG
adversarially blurs only the regions that diffusion models attend to at each
iteration and guides them accordingly. Our experimental results show that our
SAG improves the performance of various diffusion models, including ADM, IDDPM,
Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidance
methods leads to further improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04118">Convergence of the Backward Deep BSDE Method with Applications to Optimal Stopping Problems. (arXiv:2210.04118v3 [math.PR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gao_C/0/1/0/all/0/1">Chengfan Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Gao_S/0/1/0/all/0/1">Siping Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Hu_R/0/1/0/all/0/1">Ruimeng Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhu_Z/0/1/0/all/0/1">Zimu Zhu</a></p>
<p>The optimal stopping problem is one of the core problems in financial
markets, with broad applications such as pricing American and Bermudan options.
The deep BSDE method [Han, Jentzen and E, PNAS, 115(34):8505-8510, 2018] has
shown great power in solving high-dimensional forward-backward stochastic
differential equations (FBSDEs), and inspired many applications. However, the
method solves backward stochastic differential equations (BSDEs) in a forward
manner, which can not be used for optimal stopping problems that in general
require running BSDE backwardly. To overcome this difficulty, a recent paper
[Wang, Chen, Sudjianto, Liu and Shen, <a href="/abs/1807.06622">arXiv:1807.06622</a>, 2018] proposed the
backward deep BSDE method to solve the optimal stopping problem. In this paper,
we provide the rigorous theory for the backward deep BSDE method. Specifically,
1. We derive the a posteriori error estimation, i.e., the error of the
numerical solution can be bounded by the training loss function; and; 2. We
give an upper bound of the loss function, which can be sufficiently small
subject to universal approximations. We give two numerical examples, which
present consistent performance with the proved theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14598">Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Magris_M/0/1/0/all/0/1">Martin Magris</a>, <a href="http://arxiv.org/find/stat/1/au:+Shabani_M/0/1/0/all/0/1">Mostafa Shabani</a>, <a href="http://arxiv.org/find/stat/1/au:+Iosifidis_A/0/1/0/all/0/1">Alexandros Iosifidis</a></p>
<p>We propose an optimization algorithm for Variational Inference (VI) in
complex models. Our approach relies on natural gradient updates where the
variational space is a Riemann manifold. We develop an efficient algorithm for
Gaussian Variational Inference that implicitly satisfies the positive definite
constraint on the variational covariance matrix. Our Exact manifold Gaussian
Variational Bayes (EMGVB) provides exact but simple update rules and is
straightforward to implement. Due to its black-box nature, EMGVB stands as a
ready-to-use solution for VI in complex models. Over five datasets, we
empirically validate our feasible approach on different statistical,
econometric, and deep learning models, discussing its performance with respect
to baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00642">Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks. (arXiv:2211.00642v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hlaing_N/0/1/0/all/0/1">N. Hlaing</a>, <a href="http://arxiv.org/find/cs/1/au:+Morato_P/0/1/0/all/0/1">Pablo G. Morato</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1">F. d. N. Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Weijtjens_W/0/1/0/all/0/1">W. Weijtjens</a>, <a href="http://arxiv.org/find/cs/1/au:+Devriendt_C/0/1/0/all/0/1">C. Devriendt</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigo_P/0/1/0/all/0/1">P. Rigo</a></p>
<p>Offshore wind structures are subject to deterioration mechanisms throughout
their operational lifetime. Even if the deterioration evolution of structural
elements can be estimated through physics-based deterioration models, the
uncertainties involved in the process hurdle the selection of lifecycle
management decisions. In this scenario, the collection of relevant information
through an efficient monitoring system enables the reduction of uncertainties,
ultimately driving more optimal lifecycle decisions. However, a full monitoring
instrumentation implemented on all wind turbines in a farm might become
unfeasible due to practical and economical constraints. Besides, certain load
monitoring systems often become defective after a few years of marine
environment exposure. Addressing the aforementioned concerns, a farm-wide
virtual load monitoring scheme directed by a fleet-leader wind turbine offers
an attractive solution. Fetched with data retrieved from a fully-instrumented
wind turbine, a model can be trained and then deployed, thus yielding load
predictions of non-fully monitored wind turbines, from which only standard data
remains available. In this paper, we propose a virtual load monitoring
framework formulated via Bayesian neural networks (BNNs) and we provide
relevant implementation details needed for the construction, training, and
deployment of BNN data-based virtual monitoring models. As opposed to their
deterministic counterparts, BNNs intrinsically announce the uncertainties
associated with generated load predictions and allow to detect inaccurate load
estimations generated for non-fully monitored wind turbines. The proposed
virtual load monitoring is thoroughly tested through an experimental campaign
in an operational offshore wind farm and the results demonstrate the
effectiveness of BNN models for fleet-leader-based farm-wide virtual
monitoring.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00143">FIESTA: Autoencoders for accurate fiber segmentation in tractography. (arXiv:2212.00143v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dumais_F/0/1/0/all/0/1">F&#xe9;lix Dumais</a>, <a href="http://arxiv.org/find/cs/1/au:+Legarreta_J/0/1/0/all/0/1">Jon Haitz Legarreta</a>, <a href="http://arxiv.org/find/cs/1/au:+Lemaire_C/0/1/0/all/0/1">Carl Lemaire</a>, <a href="http://arxiv.org/find/cs/1/au:+Poulin_P/0/1/0/all/0/1">Philippe Poulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Rheault_F/0/1/0/all/0/1">Fran&#xe7;ois Rheault</a>, <a href="http://arxiv.org/find/cs/1/au:+Petit_L/0/1/0/all/0/1">Laurent Petit</a>, <a href="http://arxiv.org/find/cs/1/au:+Barakovic_M/0/1/0/all/0/1">Muhamed Barakovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Magon_S/0/1/0/all/0/1">Stefano Magon</a>, <a href="http://arxiv.org/find/cs/1/au:+Descoteaux_M/0/1/0/all/0/1">Maxime Descoteaux</a>, <a href="http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1">Pierre-Marc Jodoin</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)</p>
<p>White matter bundle segmentation is a cornerstone of modern tractography to
study the brain's structural connectivity in domains such as neurological
disorders, neurosurgery, and aging. In this study, we present FIESTA (FIbEr
Segmentation in Tractography using Autoencoders), a reliable and robust, fully
automated, and easily semi-automatically calibrated pipeline based on deep
autoencoders that can dissect and fully populate white matter bundles. This
pipeline is built upon previous works that demonstrated how autoencoders can be
used successfully for streamline filtering, bundle segmentation, and streamline
generation in tractography. Our proposed method improves bundle segmentation
coverage by recovering hard-to-track bundles with generative sampling through
the latent space seeding of the subject bundle and the atlas bundle. A latent
space of streamlines is learned using autoencoder-based modeling combined with
contrastive learning. Using an atlas of bundles in standard space (MNI), our
proposed method segments new tractograms using the autoencoder latent distance
between each tractogram streamline and its closest neighbor bundle in the atlas
of bundles. Intra-subject bundle reliability is improved by recovering
hard-to-track streamlines, using the autoencoder to generate new streamlines
that increase the spatial coverage of each bundle while remaining anatomically
correct. Results show that our method is more reliable than state-of-the-art
automated virtual dissection methods such as RecoBundles, RecoBundlesX,
TractSeg, White Matter Analysis and XTRACT. Our framework allows for the
transition from one anatomical bundle definition to another with marginal
calibration efforts. Overall, these results show that our framework improves
the practicality and usability of current state-of-the-art bundle segmentation
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05153">Algorithmic progress in computer vision. (arXiv:2212.05153v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1">Ege Erdil</a>, <a href="http://arxiv.org/find/cs/1/au:+Besiroglu_T/0/1/0/all/0/1">Tamay Besiroglu</a></p>
<p>We investigate algorithmic progress in image classification on ImageNet,
perhaps the most well-known test bed for computer vision. We estimate a model,
informed by work on neural scaling laws, and infer a decomposition of progress
into the scaling of compute, data, and algorithms. Using Shapley values to
attribute performance improvements, we find that algorithmic improvements have
been roughly as important as the scaling of compute for progress computer
vision. Our estimates indicate that algorithmic innovations mostly take the
form of compute-augmenting algorithmic advances (which enable researchers to
get better performance from less compute), not data-augmenting algorithmic
advances. We find that compute-augmenting algorithmic advances are made at a
pace more than twice as fast as the rate usually associated with Moore's law.
In particular, we estimate that compute-augmenting innovations halve compute
requirements every nine months (95\% confidence interval: 4 to 25 months).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01805">Unsupervised Manifold Linearizing and Clustering. (arXiv:2301.01805v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1">Tianjiao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1">Shengbang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xili Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1">Benjamin D. Haeffele</a></p>
<p>We consider the problem of simultaneously clustering and learning a linear
representation of data lying close to a union of low-dimensional manifolds, a
fundamental task in machine learning and computer vision. When the manifolds
are assumed to be linear subspaces, this reduces to the classical problem of
subspace clustering, which has been studied extensively over the past two
decades. Unfortunately, many real-world datasets such as natural images can not
be well approximated by linear subspaces. On the other hand, numerous works
have attempted to learn an appropriate transformation of the data, such that
data is mapped from a union of general non-linear manifolds to a union of
linear subspaces (with points from the same manifold being mapped to the same
subspace). However, many existing works have limitations such as assuming
knowledge of the membership of samples to clusters, requiring high sampling
density, or being shown theoretically to learn trivial representations. In this
paper, we propose to optimize the Maximal Coding Rate Reduction metric with
respect to both the data representation and a novel doubly stochastic cluster
membership, inspired by state-of-the-art subspace clustering results. We give a
parameterization of such a representation and membership, allowing efficient
mini-batching and one-shot initialization. Experiments on CIFAR-10, -20, -100,
and TinyImageNet-200 datasets show that the proposed method is much more
accurate and scalable than state-of-the-art deep clustering methods, and
further learns a latent linear representation of the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.09091">BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Minjung Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1">Yunji Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1">Jeongmin Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Young Sun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1">Hyeran Byun</a>, <a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1">Youngjung Uh</a></p>
<p>3D-aware GANs aim to synthesize realistic 3D scenes such that they can be
rendered in arbitrary perspectives to produce images. Although previous methods
produce realistic images, they suffer from unstable training or degenerate
solutions where the 3D geometry is unnatural. We hypothesize that the 3D
geometry is underdetermined due to the insufficient constraint, i.e., being
classified as real image to the discriminator is not enough. To solve this
problem, we propose to approximate the background as a spherical surface and
represent a scene as a union of the foreground placed in the sphere and the
thin spherical background. It reduces the degree of freedom in the background
field. Accordingly, we modify the volume rendering equation and incorporate
dedicated constraints to design a novel 3D-aware GAN framework named BallGAN.
BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D
geometry; the images of a scene across different viewpoints have better
photometric consistency and fidelity than the state-of-the-art methods. 2) The
training becomes much more stable. 3) The foreground can be separately rendered
on top of different arbitrary backgrounds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00347">Anderson Acceleration For Bioinformatics-Based Machine Learning. (arXiv:2302.00347v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1">Sarwan Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Chourasia_P/0/1/0/all/0/1">Prakash Chourasia</a>, <a href="http://arxiv.org/find/cs/1/au:+Patterson_M/0/1/0/all/0/1">Murray Patterson</a></p>
<p>Anderson acceleration (AA) is a well-known method for accelerating the
convergence of iterative algorithms, with applications in various fields
including deep learning and optimization. Despite its popularity in these
areas, the effectiveness of AA in classical machine learning classifiers has
not been thoroughly studied. Tabular data, in particular, presents a unique
challenge for deep learning models, and classical machine learning models are
known to perform better in these scenarios. However, the convergence analysis
of these models has received limited attention. To address this gap in
research, we implement a support vector machine (SVM) classifier variant that
incorporates AA to speed up convergence. We evaluate the performance of our SVM
with and without Anderson acceleration on several datasets from the biology
domain and demonstrate that the use of AA significantly improves convergence
and reduces the training loss as the number of iterations increases. Our
findings provide a promising perspective on the potential of Anderson
acceleration in the training of simple machine learning classifiers and
underscore the importance of further research in this area. By showing the
effectiveness of AA in this setting, we aim to inspire more studies that
explore the applications of AA in classical machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00747">Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks. (arXiv:2302.00747v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaoyun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ersoy_O/0/1/0/all/0/1">Oguzhan Ersoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1">Stjepan Picek</a></p>
<p>Deep learning models achieve excellent performance in numerous machine
learning tasks. Yet, they suffer from security-related issues such as
adversarial examples and poisoning (backdoor) attacks. A deep learning model
may be poisoned by training with backdoored data or by modifying inner network
parameters. Then, a backdoored model performs as expected when receiving a
clean input, but it misclassifies when receiving a backdoored input stamped
with a pre-designed pattern called "trigger". Unfortunately, it is difficult to
distinguish between clean and backdoored models without prior knowledge of the
trigger. This paper proposes a backdoor detection method by utilizing a special
type of adversarial attack, universal adversarial perturbation (UAP), and its
similarities with a backdoor trigger. We observe an intuitive phenomenon: UAPs
generated from backdoored models need fewer perturbations to mislead the model
than UAPs from clean models. UAPs of backdoored models tend to exploit the
shortcut from all classes to the target class, built by the backdoor trigger.
We propose a novel method called Universal Soldier for Backdoor detection (USB)
and reverse engineering potential backdoor triggers via UAPs. Experiments on
345 models trained on several datasets show that USB effectively detects the
injected backdoor and provides comparable or better results than
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07557">On the Generalization of PINNs outside the training domain and the Hyperparameters influencing it. (arXiv:2302.07557v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonfanti_A/0/1/0/all/0/1">Andrea Bonfanti</a>, <a href="http://arxiv.org/find/cs/1/au:+Santana_R/0/1/0/all/0/1">Roberto Santana</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellero_M/0/1/0/all/0/1">Marco Ellero</a>, <a href="http://arxiv.org/find/cs/1/au:+Gholami_B/0/1/0/all/0/1">Babak Gholami</a></p>
<p>Physics-Informed Neural Networks (PINNs) are Neural Network architectures
trained to emulate solutions of differential equations without the necessity of
solution data. They are currently ubiquitous in the scientific literature due
to their flexible and promising settings. However, very little of the available
research provides practical studies that aim for a better quantitative
understanding of such architecture and its functioning. In this paper, we
perform an empirical analysis of the behavior of PINN predictions outside their
training domain. The primary goal is to investigate the scenarios in which a
PINN can provide consistent predictions outside the training area.
Thereinafter, we assess whether the algorithmic setup of PINNs can influence
their potential for generalization and showcase the respective effect on the
prediction. The results obtained in this study returns insightful and at times
counterintuitive perspectives which can be highly relevant for architectures
which combines PINNs with domain decomposition and/or adaptive training
strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09624">Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy. (arXiv:2302.09624v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Richeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1">Zhonggen Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1">Caijun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1">Tony Quek</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1">Huaiyu Dai</a></p>
<p>We consider a federated data analytics problem in which a server coordinates
the collaborative data analysis of multiple users with privacy concerns and
limited communication capability. The commonly adopted compression schemes
introduce information loss into local data while improving communication
efficiency, and it remains an open problem whether such discrete-valued
mechanisms provide any privacy protection. In this paper, we study the local
differential privacy guarantees of discrete-valued mechanisms with finite
output space through the lens of $f$-differential privacy (DP). More
specifically, we advance the existing literature by deriving tight $f$-DP
guarantees for a variety of discrete-valued mechanisms, including the binomial
noise and the binomial mechanisms that are proposed for privacy preservation,
and the sign-based methods that are proposed for data compression, in
closed-form expressions. We further investigate the amplification in privacy by
sparsification and propose a ternary stochastic compressor. By leveraging
compression for privacy amplification, we improve the existing methods by
removing the dependency of accuracy (in terms of mean square error) on
communication cost in the popular use case of distributed mean estimation,
therefore breaking the three-way tradeoff between privacy, communication, and
accuracy. Finally, we discuss the Byzantine resilience of the proposed
mechanism and its application in federated learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00028">Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jakkala_K/0/1/0/all/0/1">Kalvik Jakkala</a>, <a href="http://arxiv.org/find/cs/1/au:+Akella_S/0/1/0/all/0/1">Srinivas Akella</a></p>
<p>The sensor placement problem is a common problem that arises when monitoring
correlated phenomena, such as temperature and precipitation. Existing
approaches to this problem typically use discrete optimization methods, which
are computationally expensive and cannot scale to large problems. We address
the sensor placement problem in correlated environments by reducing it to a
regression problem that can be efficiently solved using sparse Gaussian
processes (SGPs). Our approach can handle both discrete sensor placement
problems-where sensors are limited to a subset of a given set of locations-and
continuous sensor placement problems-where sensors can be placed anywhere in a
bounded continuous region. We further generalize our approach to handle sensors
with a non-point field of view and integrated observations. Our experimental
results on three real-world datasets show that our approach generates sensor
placements that result in reconstruction quality that is consistently on par or
better than the prior state-of-the-art approach while being significantly
faster. Our computationally efficient approach enables both large-scale sensor
placement and fast robotic sensor placement for informative path planning
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01770">Quantized Radio Map Estimation Using Tensor and Deep Generative Models. (arXiv:2303.01770v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Timilsina_S/0/1/0/all/0/1">Subash Timilsina</a>, <a href="http://arxiv.org/find/eess/1/au:+Shrestha_S/0/1/0/all/0/1">Sagar Shrestha</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1">Xiao Fu</a></p>
<p>Spectrum cartography (SC), also known as radio map estimation (RME), aims at
crafting multi-domain (e.g., frequency and space) radio power propagation maps
from limited sensor measurements. While early methods often lacked theoretical
support, recent works have demonstrated that radio maps can be provably
recovered using low-dimensional models -- such as the block-term tensor
decomposition (BTD) model and certain deep generative models (DGMs) -- of the
high-dimensional multi-domain radio signals. However, these existing provable
SC approaches assume that sensors send real-valued (full-resolution)
measurements to the fusion center, which is unrealistic. This work puts forth a
quantized SC framework that generalizes the BTD and DGM-based SC to scenarios
where heavily quantized sensor measurements are used. A maximum likelihood
estimation (MLE)-based SC framework under a Gaussian quantizer is proposed.
Recoverability of the radio map using the MLE criterion are characterized under
realistic conditions, e.g., imperfect radio map modeling and noisy
measurements. Simulations and real-data experiments are used to showcase the
effectiveness of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05699">Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Saemi Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Seunghyuk Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongwoo Kim</a></p>
<p>We tackle the problem of feature unlearning from a pre-trained image
generative model: GANs and VAEs. Unlike a common unlearning task where an
unlearning target is a subset of the training set, we aim to unlearn a specific
feature, such as hairstyle from facial images, from the pre-trained generative
models. As the target feature is only presented in a local region of an image,
unlearning the entire image from the pre-trained model may result in losing
other details in the remaining region of the image. To specify which features
to unlearn, we collect randomly generated images that contain the target
features. We then identify a latent representation corresponding to the target
feature and then use the representation to fine-tune the pre-trained model.
Through experiments on MNIST and CelebA datasets, we show that target features
are successfully removed while keeping the fidelity of the original models.
Further experiments with an adversarial attack show that the unlearned model is
more robust under the presence of malicious parties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06965">Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation with a Unified Model. (arXiv:2303.06965v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiang_B/0/1/0/all/0/1">Bo Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiran Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yuheng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ningfeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Song Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liangren Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Bo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenming Liu</a></p>
<p>Chemical reactions are the fundamental building blocks of drug design and
organic chemistry research. In recent years, there has been a growing need for
a large-scale deep-learning framework that can efficiently capture the basic
rules of chemical reactions. In this paper, we have proposed a unified
framework that addresses both the reaction representation learning and molecule
generation tasks, which allows for a more holistic approach. Inspired by the
organic chemistry mechanism, we develop a novel pretraining framework that
enables us to incorporate inductive biases into the model. Our framework
achieves state-of-the-art results on challenging downstream tasks. By
possessing chemical knowledge, our generative framework overcome the
limitations of current molecule generation models that rely on a small number
of reaction templates. In the extensive experiments, our model generates
synthesizable drug-like structures of high quality. Overall, our work presents
a significant step toward a large-scale deep-learning framework for a variety
of reaction-based applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08040">Equal Treatment: Measuring Fairness using Explanation Distributions. (arXiv:2303.08040v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mougan_C/0/1/0/all/0/1">Carlos Mougan</a>, <a href="http://arxiv.org/find/cs/1/au:+State_L/0/1/0/all/0/1">Laura State</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1">Antonio Ferrara</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1">Salvatore Ruggieri</a>, <a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1">Steffen Staab</a></p>
<p>Liberalism-oriented political philosophy reasons that all individuals should
be treated equally independently of their protected characteristics. Related
work in machine learning has translated the concept of equal treatment into
terms of equal outcome and measured it as demographic parity (also called
statistical parity). Our analysis reveals that the two concepts of equal
outcome and equal treatment diverge; therefore, demographic parity does not
faithfully represent the notion of equal treatment. We propose a new
formalization for equal treatment by (i) considering the influence of feature
values on predictions, such as computed by Shapley values explaining
classifications, (ii) defining distributions of explanations, and (iii)
comparing explanation distributions between populations with different
protected characteristics. We show the theoretical properties of our notion of
equal treatment and devise a classifier two-sample test based on the AUC of an
equal treatment inspector. We study our formalization of equal treatment on
synthetic and natural data. We release explanationspace, an open-source Python
package with methods and tutorials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01075">Conformal Prediction Regions for Time Series using Linear Complementarity Programming. (arXiv:2304.01075v3 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cleaveland_M/0/1/0/all/0/1">Matthew Cleaveland</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_I/0/1/0/all/0/1">Insup Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Pappas_G/0/1/0/all/0/1">George J. Pappas</a>, <a href="http://arxiv.org/find/eess/1/au:+Lindemann_L/0/1/0/all/0/1">Lars Lindemann</a></p>
<p>Conformal prediction is a statistical tool for producing prediction regions
of machine learning models that are valid with high probability. However,
applying conformal prediction to time series data leads to conservative
prediction regions. In fact, to obtain prediction regions over $T$ time steps
with confidence $1-\delta$, {previous works require that each individual
prediction region is valid} with confidence $1-\delta/T$. We propose an
optimization-based method for reducing this conservatism to enable long horizon
planning and verification when using learning-enabled time series predictors.
Instead of considering prediction errors individually at each time step, we
consider a parameterized prediction error over multiple time steps. By
optimizing the parameters over an additional dataset, we find prediction
regions that are not conservative. We show that this problem can be cast as a
mixed integer linear complementarity program (MILCP), which we then relax into
a linear complementarity program (LCP). Additionally, we prove that the relaxed
LP has the same optimal cost as the original MILCP. Finally, we demonstrate the
efficacy of our method on case studies using pedestrian trajectory predictors
and F16 fighter jet altitude predictors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02169">Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1">Brandon Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Synthetic electronic health records (EHRs) that are both realistic and
preserve privacy can serve as an alternative to real EHRs for machine learning
(ML) modeling and statistical analysis. However, generating high-fidelity and
granular electronic health record (EHR) data in its original,
highly-dimensional form poses challenges for existing methods due to the
complexities inherent in high-dimensional data. In this paper, we propose
Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal
high-dimensional EHR, which preserve the statistical properties of real EHR and
can be used to train accurate ML models without privacy concerns. Our HALO
method, designed as a hierarchical autoregressive model, generates a
probability density function of medical codes, clinical visits, and patient
records, allowing for the generation of realistic EHR data in its original,
unaggregated form without the need for variable selection or aggregation.
Additionally, our model also produces high-quality continuous variables in a
longitudinal and probabilistic manner. We conducted extensive experiments and
demonstrate that HALO can generate high-fidelity EHR data with high-dimensional
disease code probabilities (d &gt; 10,000), disease co-occurrence probabilities
within visits (d &gt; 1,000,000), and conditional probabilities across consecutive
visits (d &gt; 5,000,000) and achieve above 0.9 R2 correlation in comparison to
real EHR data. This performance then enables downstream ML models trained on
its synthetic data to achieve comparable accuracy to models trained on real
data (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using a
combination of real and synthetic data enhances the accuracy of ML models
beyond that achieved by using only real EHR data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03543">HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wydmanski_W/0/1/0/all/0/1">Witold Wydma&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulenok_O/0/1/0/all/0/1">Oleksii Bulenok</a>, <a href="http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1">Marek &#x15a;mieja</a></p>
<p>Deep learning has achieved impressive performance in many domains, such as
computer vision and natural language processing, but its advantage over
classical shallow methods on tabular datasets remains questionable. It is
especially challenging to surpass the performance of tree-like ensembles, such
as XGBoost or Random Forests, on small-sized datasets (less than 1k samples).
To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach
to solving small sample problems on tabular datasets. By combining the
advantages of Random Forests and neural networks, HyperTab generates an
ensemble of neural networks, where each target model is specialized to process
a specific lower-dimensional view of the data. Since each view plays the role
of data augmentation, we virtually increase the number of training samples
while keeping the number of trainable parameters unchanged, which prevents
model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a
varying number of samples and domains of origin, and compared its performance
with shallow and deep learning models representing the current
state-of-the-art. We show that HyperTab consistently outranks other methods on
small data (with a statistically significant difference) and scores comparable
to them on larger datasets.
</p>
<p>We make a python package with the code available to download at
https://pypi.org/project/hypertab/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08134">Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1">Martin Knoche</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1">Gerhard Rigoll</a></p>
<p>Nowadays, face recognition systems surpass human performance on several
datasets. However, there are still edge cases that the machine can't correctly
classify. This paper investigates the effect of a combination of machine and
human operators in the face verification task. First, we look closer at the
edge cases for several state-of-the-art models to discover common datasets'
challenging settings. Then, we conduct a study with 60 participants on these
selected tasks with humans and provide an extensive analysis. Finally, we
demonstrate that combining machine and human decisions can further improve the
performance of state-of-the-art face verification systems on various benchmark
datasets. Code and data are publicly available on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08847">BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naseri_M/0/1/0/all/0/1">Mohammad Naseri</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yufei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cristofaro_E/0/1/0/all/0/1">Emiliano De Cristofaro</a></p>
<p>Federated learning (FL) enables multiple parties to collaboratively train a
machine learning model without sharing their data; rather, they train their own
model locally and send updates to a central server for aggregation. Depending
on how the data is distributed among the participants, FL can be classified
into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the
same set of training instances but only host a different and non-overlapping
subset of the whole feature space. Whereas in HFL, each participant shares the
same set of features while the training set is split into locally owned
training data subsets.
</p>
<p>VFL is increasingly used in applications like financial fraud detection;
nonetheless, very little work has analyzed its security. In this paper, we
focus on robustness in VFL, in particular, on backdoor attacks, whereby an
adversary attempts to manipulate the aggregate model during the training
process to trigger misclassifications. Performing backdoor attacks in VFL is
more challenging than in HFL, as the adversary i) does not have access to the
labels during training and ii) cannot change the labels as she only has access
to the feature embeddings. We present a first-of-its-kind clean-label backdoor
attack in VFL, which consists of two phases: a label inference and a backdoor
phase. We demonstrate the effectiveness of the attack on three different
datasets, investigate the factors involved in its success, and discuss
countermeasures to mitigate its impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09355">To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shwartz_Ziv_R/0/1/0/all/0/1">Ravid Shwartz-Ziv</a>, <a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1">Yann LeCun</a></p>
<p>\begin{abstract} Deep neural networks excel in supervised learning tasks but
are constrained by the need for extensive labeled data. Self-supervised
learning emerges as a promising alternative, allowing models to learn without
explicit labels. Information theory, and notably the information bottleneck
principle, has been pivotal in shaping deep neural networks. This principle
focuses on optimizing the trade-off between compression and preserving relevant
information, providing a foundation for efficient network design in supervised
contexts. However, its precise role and adaptation in self-supervised learning
remain unclear. In this work, we scrutinize various self-supervised learning
approaches from an information-theoretic perspective, introducing a unified
framework that encapsulates the self-supervised information-theoretic learning
problem. We weave together existing research into a cohesive narrative, delve
into contemporary self-supervised methodologies, and spotlight potential
research avenues and inherent challenges. Additionally, we discuss the
empirical evaluation of information-theoretic quantities and their estimation
methods. Overall, this paper furnishes an exhaustive review of the intersection
of information theory, self-supervised learning, and deep neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14343">Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jiawei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chengkai Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wenjun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyuan Wang</a></p>
<p>As deep learning technology advances and more urban spatial-temporal data
accumulates, an increasing number of deep learning models are being proposed to
solve urban spatial-temporal prediction problems. However, there are
limitations in the existing field, including open-source data being in various
formats and difficult to use, few papers making their code and data openly
available, and open-source models often using different frameworks and
platforms, making comparisons challenging. A standardized framework is urgently
needed to implement and evaluate these methods. To address these issues, we
provide a comprehensive review of urban spatial-temporal prediction and propose
a unified storage format for spatial-temporal data called atomic files. We also
propose LibCity, an open-source library that offers researchers a credible
experimental tool and a convenient development framework. In this library, we
have reproduced 65 spatial-temporal prediction models and collected 55
spatial-temporal datasets, allowing researchers to conduct comprehensive
experiments conveniently. Using LibCity, we conducted a series of experiments
to validate the effectiveness of different models and components, and we
summarized promising future technology developments and research directions for
spatial-temporal prediction. By enabling fair model comparisons, designing a
unified data storage format, and simplifying the process of developing new
models, LibCity is poised to make significant contributions to the
spatial-temporal prediction field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01975">A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1">Jiahui Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zongxiong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuandou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Woisetschlaeger_H/0/1/0/all/0/1">Herbert Woisetschlaeger</a>, <a href="http://arxiv.org/find/cs/1/au:+Schimmler_S/0/1/0/all/0/1">Sonja Schimmler</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayer_R/0/1/0/all/0/1">Ruben Mayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhiming Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_C/0/1/0/all/0/1">Chunming Rong</a></p>
<p>Dataset distillation is attracting more attention in machine learning as
training sets continue to grow and the cost of training state-of-the-art models
becomes increasingly high. By synthesizing datasets with high information
density, dataset distillation offers a range of potential applications,
including support for continual learning, neural architecture search, and
privacy protection. Despite recent advances, we lack a holistic understanding
of the approaches and applications. Our survey aims to bridge this gap by first
proposing a taxonomy of dataset distillation, characterizing existing
approaches, and then systematically reviewing the data modalities, and related
applications. In addition, we summarize the challenges and discuss future
directions for this field of research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14706">PruMUX: Augmenting Data Multiplexing with Model Compression. (arXiv:2305.14706v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yushan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Murahari_V/0/1/0/all/0/1">Vishvak Murahari</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kai Li</a></p>
<p>As language models increase in size by the day, methods for efficient
inference are critical to leveraging their capabilities for various
applications. Prior work has investigated techniques like model pruning,
knowledge distillation, and data multiplexing to increase model throughput
without sacrificing accuracy. In this paper, we combine two such methods --
structured pruning and data multiplexing -- to compound the speedup gains
obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X
throughput improvement over BERT-base model with accuracy threshold from 80% to
74%. We further study various combinations of parameters (such as sparsity and
multiplexing factor) in the two techniques to provide a comprehensive analysis
of the tradeoff between accuracy and throughput in the resulting models. We
then propose Auto-PruMUX, a meta-level model that can predict the
high-performance parameters for pruning and multiplexing given a desired
accuracy loss budget, providing a practical method to leverage the combination
effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15490">Symplectic model reduction of Hamiltonian systems using data-driven quadratic manifolds. (arXiv:2305.15490v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Sharma_H/0/1/0/all/0/1">Harsh Sharma</a>, <a href="http://arxiv.org/find/math/1/au:+Mu_H/0/1/0/all/0/1">Hongliang Mu</a>, <a href="http://arxiv.org/find/math/1/au:+Buchfink_P/0/1/0/all/0/1">Patrick Buchfink</a>, <a href="http://arxiv.org/find/math/1/au:+Geelen_R/0/1/0/all/0/1">Rudy Geelen</a>, <a href="http://arxiv.org/find/math/1/au:+Glas_S/0/1/0/all/0/1">Silke Glas</a>, <a href="http://arxiv.org/find/math/1/au:+Kramer_B/0/1/0/all/0/1">Boris Kramer</a></p>
<p>This work presents two novel approaches for the symplectic model reduction of
high-dimensional Hamiltonian systems using data-driven quadratic manifolds.
Classical symplectic model reduction approaches employ linear symplectic
subspaces for representing the high-dimensional system states in a
reduced-dimensional coordinate system. While these approximations respect the
symplectic nature of Hamiltonian systems, linear basis approximations can
suffer from slowly decaying Kolmogorov $N$-width, especially in wave-type
problems, which then requires a large basis size. We propose two different
model reduction methods based on recently developed quadratic manifolds, each
presenting its own advantages and limitations. The addition of quadratic terms
to the state approximation, which sits at the heart of the proposed
methodologies, enables us to better represent intrinsic low-dimensionality in
the problem at hand. Both approaches are effective for issuing predictions in
settings well outside the range of their training data while providing more
accurate solutions than the linear symplectic reduced-order models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16556">LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1">Sayna Ebrahimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1">Sercan O. Arik</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yihe Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a></p>
<p>Multimodal large-scale pretraining has shown impressive performance for
unstructured data including language, image, audio, and video. However, a
prevalent real-world scenario involves the combination of structured data types
(tabular, time-series) with unstructured data which has so far been
understudied. To bridge this gap, we propose LANISTR, an attention-based
framework to learn from LANguage, Image, and STRuctured data. The core of
LANISTR's methodology is rooted in \textit{masking-based} training applied
across both unimodal and multimodal levels. In particular, we introduce a new
similarity-based multimodal masking loss that enables it to learn cross-modal
relations from large-scale multimodal data with missing modalities. On two
real-world datastes, MIMIC-IV (healthcare) and Amazon Product Review (retail),
LANISTR demonstrates remarkable absolute improvements of 6.6\% (AUROC) and up
to 14\% (accuracy) when fine-tuned on 0.1\% and 0.01\% of labeled data,
respectively, compared to the state-of-the-art alternatives. Notably, these
improvements are observed even in the presence of considerable missingness
ratios of 35.7\% and 99.8\%, in the respective datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17058">Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach. (arXiv:2305.17058v2 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaiser_F/0/1/0/all/0/1">Fabian Zaiser</a>, <a href="http://arxiv.org/find/cs/1/au:+Murawski_A/0/1/0/all/0/1">Andrzej S. Murawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1">Luke Ong</a></p>
<p>We present an exact Bayesian inference method for discrete statistical
models, which can find exact solutions to many discrete inference problems,
even with infinite support and continuous priors. To express such models, we
introduce a probabilistic programming language that supports discrete and
continuous sampling, discrete observations, affine functions, (stochastic)
branching, and conditioning on events. Our key tool is probability generating
functions: they provide a compact closed-form representation of distributions
that are definable by programs, thus enabling the exact computation of
posterior probabilities, expectation, variance, and higher moments. Our
inference method is provably correct, fully automated and uses automatic
differentiation (specifically, Taylor polynomials), but does not require
computer algebra. Our experiments show that its performance on a range of
real-world examples is competitive with approximate Monte Carlo methods, while
avoiding approximation errors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02157">Transforming to Yoked Neural Networks to Improve ANN Structure. (arXiv:2306.02157v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinshun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yizhi Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yichao Jiang</a></p>
<p>Most existing classical artificial neural networks (ANN) are designed as a
tree structure to imitate neural networks. In this paper, we argue that the
connectivity of a tree is not sufficient to characterize a neural network. The
nodes of the same level of a tree cannot be connected with each other, i.e.,
these neural unit cannot share information with each other, which is a major
drawback of ANN. Although ANN has been significantly improved in recent years
to more complex structures, such as the directed acyclic graph (DAG), these
methods also have unidirectional and acyclic bias for ANN. In this paper, we
propose a method to build a bidirectional complete graph for the nodes in the
same level of an ANN, which yokes the nodes of the same level to formulate a
neural module. We call our model as YNN in short. YNN promotes the information
transfer significantly which obviously helps in improving the performance of
the method. Our YNN can imitate neural networks much better compared with the
traditional ANN. In this paper, we analyze the existing structural bias of ANN
and propose a model YNN to efficiently eliminate such structural bias. In our
model, nodes also carry out aggregation and transformation of features, and
edges determine the flow of information. We further impose auxiliary sparsity
constraint to the distribution of connectedness, which promotes the learned
structure to focus on critical connections. Finally, based on the optimized
structure, we also design small neural module structure based on the minimum
cut technique to reduce the computational burden of the YNN model. This
learning process is compatible with the existing networks and different tasks.
The obtained quantitative experimental results reflect that the learned
connectivity is superior to the traditional NN structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03398">Minimum intrinsic dimension scaling for entropic optimal transport. (arXiv:2306.03398v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Stromme_A/0/1/0/all/0/1">Austin J. Stromme</a></p>
<p>Motivated by the manifold hypothesis, which states that data with a high
extrinsic dimension may yet have a low intrinsic dimension, we develop refined
statistical bounds for entropic optimal transport that are sensitive to the
intrinsic dimension of the data. Our bounds involve a robust notion of
intrinsic dimension, measured at only a single distance scale depending on the
regularization parameter, and show that it is only the minimum of these
single-scale intrinsic dimensions which governs the rate of convergence. We
call this the Minimum Intrinsic Dimension scaling (MID scaling) phenomenon, and
establish MID scaling with no assumptions on the data distributions so long as
the cost is bounded and Lipschitz, and for various entropic optimal transport
quantities beyond just values, with stronger analogs when one distribution is
supported on a manifold. Our results significantly advance the theoretical
state of the art by showing that MID scaling is a generic phenomenon, and
provide the first rigorous interpretation of the statistical effect of entropic
regularization as a distance scale.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04504">Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1">Israt Jahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1">Md Tahmid Rahman Laskar</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jimmy Huang</a></p>
<p>ChatGPT is a large language model developed by OpenAI. Despite its impressive
performance across various tasks, no prior work has investigated its capability
in the biomedical domain yet. To this end, this paper aims to evaluate the
performance of ChatGPT on various benchmark biomedical tasks, such as relation
extraction, document classification, question answering, and summarization. To
the best of our knowledge, this is the first work that conducts an extensive
evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on
our evaluation that in biomedical datasets that have smaller training sets,
zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative
transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's
pre-training on large text corpora makes it quite specialized even in the
biomedical domain. Our findings demonstrate that ChatGPT has the potential to
be a valuable tool for various tasks in the biomedical domain that lack large
annotated data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04528">PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiaheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zichen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Linyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1">Wei Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1">Neil Zhenqiang Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>The increasing reliance on Large Language Models (LLMs) across academia and
industry necessitates a comprehensive understanding of their robustness to
prompts. In response to this vital need, we introduce PromptBench, a robustness
benchmark designed to measure LLMs' resilience to adversarial prompts. This
study uses a plethora of adversarial textual attacks targeting prompts across
multiple levels: character, word, sentence, and semantic. These prompts are
then employed in diverse tasks, such as sentiment analysis, natural language
inference, reading comprehension, machine translation, and math
problem-solving. Our study generates 4,032 adversarial prompts, meticulously
evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our
findings demonstrate that contemporary LLMs are vulnerable to adversarial
prompts. Furthermore, we present comprehensive analysis to understand the
mystery behind prompt robustness and its transferability. We then offer
insightful robustness analysis and pragmatic recommendations for prompt
composition, beneficial to both researchers and everyday users. We make our
code, prompts, and methodologies to generate adversarial prompts publicly
accessible, thereby enabling and encouraging collaborative exploration in this
pivotal field: https://github.com/microsoft/promptbench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05838">Expectation-Complete Graph Representations with Homomorphisms. (arXiv:2306.05838v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1">Pascal Welke</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiessen_M/0/1/0/all/0/1">Maximilian Thiessen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jogl_F/0/1/0/all/0/1">Fabian Jogl</a>, <a href="http://arxiv.org/find/cs/1/au:+Gartner_T/0/1/0/all/0/1">Thomas G&#xe4;rtner</a></p>
<p>We investigate novel random graph embeddings that can be computed in expected
polynomial time and that are able to distinguish all non-isomorphic graphs in
expectation. Previous graph embeddings have limited expressiveness and either
cannot distinguish all graphs or cannot be computed efficiently for every
graph. To be able to approximate arbitrary functions on graphs, we are
interested in efficient alternatives that become arbitrarily expressive with
increasing resources. Our approach is based on Lov\'asz' characterisation of
graph isomorphism through an infinite dimensional vector of homomorphism
counts. Our empirical evaluation shows competitive results on several benchmark
graph learning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08451">A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias. (arXiv:2306.08451v2 [physics.med-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Mousavi_S/0/1/0/all/0/1">Seyedeh Somayyeh Mousavi</a>, <a href="http://arxiv.org/find/physics/1/au:+Reyna_M/0/1/0/all/0/1">Matthew A. Reyna</a>, <a href="http://arxiv.org/find/physics/1/au:+Clifford_G/0/1/0/all/0/1">Gari D. Clifford</a>, <a href="http://arxiv.org/find/physics/1/au:+Sameni_R/0/1/0/all/0/1">Reza Sameni</a></p>
<p>Regular blood pressure (BP) monitoring in clinical and ambulatory settings
plays a crucial role in the prevention, diagnosis, treatment, and management of
cardiovascular diseases. Recently, the widespread adoption of ambulatory BP
measurement devices has been driven predominantly by the increased prevalence
of hypertension and its associated risks and clinical conditions. Recent
guidelines advocate for regular BP monitoring as part of regular clinical
visits or even at home. This increased utilization of BP measurement
technologies has brought up significant concerns, regarding the accuracy of
reported BP values across settings.
</p>
<p>In this survey, focusing mainly on cuff-based BP monitoring technologies, we
highlight how BP measurements can demonstrate substantial biases and variances
due to factors such as measurement and device errors, demographics, and body
habitus. With these inherent biases, the development of a new generation of
cuff-based BP devices which use artificial-intelligence (AI) has significant
potential. We present future avenues where AI-assisted technologies can
leverage the extensive clinical literature on BP-related studies together with
the large collections of BP records available in electronic health records.
These resources can be combined with machine learning approaches, including
deep learning and Bayesian inference, to remove BP measurement biases and to
provide individualized BP-related cardiovascular risk indexes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10466">Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication. (arXiv:2306.10466v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1">Ajay Jaiswal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Ying Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>Graphs are omnipresent and GNNs are a powerful family of neural networks for
learning over graphs. Despite their popularity, scaling GNNs either by
deepening or widening suffers from prevalent issues of unhealthy gradients,
over-smoothening, information squashing, which often lead to sub-standard
performance. In this work, we are interested in exploring a principled way to
scale GNNs capacity without deepening or widening, which can improve its
performance across multiple small and large graphs. Motivated by the recent
intriguing phenomenon of model soups, which suggest that fine-tuned weights of
multiple large-language pre-trained models can be merged to a better minima, we
argue to exploit the fundamentals of model soups to mitigate the aforementioned
issues of memory bottleneck and trainability during GNNs scaling. More
specifically, we propose not to deepen or widen current GNNs, but instead
present a data-centric perspective of model soups tailored for GNNs, i.e., to
build powerful GNNs. By dividing giant graph data, we build multiple
independently and parallelly trained weaker GNNs (soup ingredient) without any
intermediate communication, and combine their strength using a greedy
interpolation soup procedure to achieve state-of-the-art performance. Compared
to concurrent distributed GNN training works such as Jiong et. al. 2023, we
train each soup ingredient by sampling different subgraphs per epoch and their
respective sub-models are merged only after being fully trained (rather than
intermediately so). Moreover, we provide a wide variety of model soup
preparation techniques by leveraging state-of-the-art graph sampling and graph
partitioning approaches that can handle large graphs. Codes are available at:
\url{https://github.com/VITA-Group/graph_ladling}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10592">Conditional expectation using compactification operators. (arXiv:2306.10592v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Das_S/0/1/0/all/0/1">Suddhasattwa Das</a></p>
<p>The separate tasks of denoising, least squares expectation, and manifold
learning can often be posed in a common setting of finding the conditional
expectations arising from a product of two random variables. This paper focuses
on this more general problem and describes an operator theoretic approach to
estimating the conditional expectation. Kernel integral operators are used as a
compactification tool, to set up the estimation problem as a linear inverse
problem in a reproducing kernel Hilbert space. This equation is shown to have
solutions that allow numerical approximation, thus guaranteeing the convergence
of data-driven implementations. The overall technique is easy to implement, and
their successful application to some real-world problems are also shown.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06886">Min-Max Optimization under Delays. (arXiv:2307.06886v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adibi_A/0/1/0/all/0/1">Arman Adibi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Aritra Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1">Hamed Hassani</a></p>
<p>Delays and asynchrony are inevitable in large-scale machine-learning problems
where communication plays a key role. As such, several works have extensively
analyzed stochastic optimization with delayed gradients. However, as far as we
are aware, no analogous theory is available for min-max optimization, a topic
that has gained recent popularity due to applications in adversarial
robustness, game theory, and reinforcement learning. Motivated by this gap, we
examine the performance of standard min-max optimization algorithms with
delayed gradient updates. First, we show (empirically) that even small delays
can cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge on
simple instances for which \texttt{EG} guarantees convergence in the absence of
delays. Our empirical study thus suggests the need for a careful analysis of
delayed versions of min-max optimization algorithms. Accordingly, under
suitable technical assumptions, we prove that Gradient Descent-Ascent
(\texttt{GDA}) and \texttt{EG} with delayed updates continue to guarantee
convergence to saddle points for convex-concave and strongly convex-strongly
concave settings. Our complexity bounds reveal, in a transparent manner, the
slow-down in convergence caused by delays.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04706">Pareto Invariant Representation Learning for Multimedia Recommendation. (arXiv:2308.04706v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shanshan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qingsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chunyuan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Li Liu</a></p>
<p>Multimedia recommendation involves personalized ranking tasks, where
multimedia content is usually represented using a generic encoder. However,
these generic representations introduce spurious correlations that fail to
reveal users' true preferences. Existing works attempt to alleviate this
problem by learning invariant representations, but overlook the balance between
independent and identically distributed (IID) and out-of-distribution (OOD)
generalization. In this paper, we propose a framework called Pareto Invariant
Representation Learning (PaInvRL) to mitigate the impact of spurious
correlations from an IID-OOD multi-objective optimization perspective, by
learning invariant representations (intrinsic factors that attract user
attention) and variant representations (other factors) simultaneously.
Specifically, PaInvRL includes three iteratively executed modules: (i)
heterogeneous identification module, which identifies the heterogeneous
environments to reflect distributional shifts for user-item interactions; (ii)
invariant mask generation module, which learns invariant masks based on the
Pareto-optimal solutions that minimize the adaptive weighted Invariant Risk
Minimization (IRM) and Empirical Risk (ERM) losses; (iii) convert module, which
generates both variant representations and item-invariant representations for
training a multi-modal recommendation model that mitigates spurious
correlations and balances the generalization performance within and cross the
environmental distributions. We compare the proposed PaInvRL with
state-of-the-art recommendation models on three public multimedia
recommendation datasets (Movielens, Tiktok, and Kwai), and the experimental
results validate the effectiveness of PaInvRL for both within- and
cross-environmental learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06534">Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolf_D/0/1/0/all/0/1">Daniel Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Payer_T/0/1/0/all/0/1">Tristan Payer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1">Catharina Silvia Lisson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1">Christoph Gerhard Lisson</a>, <a href="http://arxiv.org/find/cs/1/au:+Beer_M/0/1/0/all/0/1">Meinrad Beer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1">Timo Ropinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gotz_M/0/1/0/all/0/1">Michael G&#xf6;tz</a></p>
<p>Deep learning in medical imaging has the potential to minimize the risk of
diagnostic errors, reduce radiologist workload, and accelerate diagnosis.
Training such deep learning models requires large and accurate datasets, with
annotations for all training samples. However, in the medical imaging domain,
annotated datasets for specific tasks are often small due to the high
complexity of annotations, limited access, or the rarity of diseases. To
address this challenge, deep learning models can be pre-trained on large image
datasets without annotations using methods from the field of self-supervised
learning. After pre-training, small annotated datasets are sufficient to
fine-tune the models for a specific task. The most popular self-supervised
pre-training approaches in medical imaging are based on contrastive learning.
However, recent studies in natural image processing indicate a strong potential
for masked autoencoder approaches. Our work compares state-of-the-art
contrastive learning methods with the recently introduced masked autoencoder
approach "SparK" for convolutional neural networks (CNNs) on medical images.
Therefore we pre-train on a large unannotated CT image dataset and fine-tune on
several CT classification tasks. Due to the challenge of obtaining sufficient
annotated training data in medical imaging, it is of particular interest to
evaluate how the self-supervised pre-training methods perform when fine-tuning
on small datasets. By experimenting with gradually reducing the training
dataset size for fine-tuning, we find that the reduction has different effects
depending on the type of pre-training chosen. The SparK pre-training method is
more robust to the training dataset size than the contrastive methods. Based on
our results, we propose the SparK pre-training for medical imaging tasks with
only small annotated datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07134">Natural Language is All a Graph Needs. (arXiv:2308.07134v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1">Ruosong Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Caiqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Runhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>The emergence of large-scale pre-trained language models, such as ChatGPT,
has revolutionized various research fields in artificial intelligence.
Transformers-based large language models (LLMs) have gradually replaced CNNs
and RNNs to unify fields of computer vision and natural language processing.
Compared with the data that exists relatively independently such as images,
videos or texts, graph is a type of data that contains rich structural and
relational information. Meanwhile, natural language, as one of the most
expressive mediums, excels in describing complex structures. However, existing
work on incorporating graph learning problems into the generative language
modeling framework remains very limited. As the importance of large language
models continues to grow, it becomes essential to explore whether LLMs can also
replace GNNs as the foundation model for graphs. In this paper, we propose
InstructGLM (Instruction-finetuned Graph Language Model), systematically design
highly scalable prompts based on natural language instructions, and use natural
language to describe the geometric structure and node features of the graph for
instruction tuning an LLM to perform learning and inference on graphs in a
generative manner. Our method exceeds all competitive GNN baselines on
ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of
our method and sheds light on generative large language models as the
foundation model for graph machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09113">Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage. (arXiv:2308.09113v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tang_H/0/1/0/all/0/1">Hewei Tang</a>, <a href="http://arxiv.org/find/stat/1/au:+Kong_Q/0/1/0/all/0/1">Qingkai Kong</a>, <a href="http://arxiv.org/find/stat/1/au:+Morris_J/0/1/0/all/0/1">Joseph P. Morris</a></p>
<p>Deep learning-based surrogate models have been widely applied in geological
carbon storage (GCS) problems to accelerate the prediction of reservoir
pressure and CO2 plume migration. Large amounts of data from physics-based
numerical simulators are required to train a model to accurately predict the
complex physical behaviors associated with this process. In practice, the
available training data are always limited in large-scale 3D problems due to
the high computational cost. Therefore, we propose to use a multi-fidelity
Fourier Neural Operator to solve large-scale GCS problems with more affordable
multi-fidelity training datasets. The Fourier Neural Operator has a desirable
grid-invariant property, which simplifies the transfer learning procedure
between datasets with different discretization. We first test the model
efficacy on a GCS reservoir model being discretized into 110k grid cells. The
multi-fidelity model can predict with accuracy comparable to a high-fidelity
model trained with the same amount of high-fidelity data with 81% less data
generation costs. We further test the generalizability of the multi-fidelity
model on a same reservoir model with a finer discretization of 1 million grid
cells. This case was made more challenging by employing high-fidelity and
low-fidelity datasets generated by different geostatistical models and
reservoir simulators. We observe that the multi-fidelity FNO model can predict
pressure fields with reasonable accuracy even when the high-fidelity data are
extremely limited.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09725">MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v2 [q-bio.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Yang_Z/0/1/0/all/0/1">Ziwei Yang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1">Zheng Chen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Matsubara_Y/0/1/0/all/0/1">Yasuko Matsubara</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sakurai_Y/0/1/0/all/0/1">Yasushi Sakurai</a></p>
<p>Precision medicine fundamentally aims to establish causality between
dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer
subtyping has emerged as a revolutionary approach, as different level of omics
records the biochemical products of multistep processes in cancers. This paper
focuses on fully exploiting the potential of multi-omics data to improve cancer
subtyping outcomes, and hence developed MoCLIM, a representation learning
framework. MoCLIM independently extracts the informative features from distinct
omics modalities. Using a unified representation informed by contrastive
learning of different omics modalities, we can well-cluster the subtypes, given
cancer, into a lower latent space. This contrast can be interpreted as a
projection of inter-omics inference observed in biological networks.
Experimental results on six cancer datasets demonstrate that our approach
significantly improves data fit and subtyping performance in fewer
high-dimensional cancer instances. Moreover, our framework incorporates various
medical evaluations as the final component, providing high interpretability in
medical analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10145">Wasserstein Geodesic Generator for Conditional Distributions. (arXiv:2308.10145v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kim_Y/0/1/0/all/0/1">Young-geun Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1">Kyungbok Lee</a>, <a href="http://arxiv.org/find/stat/1/au:+Choi_Y/0/1/0/all/0/1">Youngwon Choi</a>, <a href="http://arxiv.org/find/stat/1/au:+Won_J/0/1/0/all/0/1">Joong-Ho Won</a>, <a href="http://arxiv.org/find/stat/1/au:+Paik_M/0/1/0/all/0/1">Myunghee Cho Paik</a></p>
<p>Generating samples given a specific label requires estimating conditional
distributions. We derive a tractable upper bound of the Wasserstein distance
between conditional distributions to lay the theoretical groundwork to learn
conditional distributions. Based on this result, we propose a novel conditional
generation algorithm where conditional distributions are fully characterized by
a metric space defined by a statistical distance. We employ optimal transport
theory to propose the Wasserstein geodesic generator, a new conditional
generator that learns the Wasserstein geodesic. The proposed method learns both
conditional distributions for observed domains and optimal transport maps
between them. The conditional distributions given unobserved intermediate
domains are on the Wasserstein geodesic between conditional distributions given
two observed domain labels. Experiments on face images with light conditions as
domain labels demonstrate the efficacy of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11217">Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zengxiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1">Zhaoxiang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tongzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Longfei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1">Chao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chengyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weishan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zelei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Liang Xu</a></p>
<p>Multimodal data, which can comprehensively perceive and recognize the
physical world, has become an essential path towards general artificial
intelligence. However, multimodal large models trained on public datasets often
underperform in specific industrial domains. This paper proposes a multimodal
federated learning framework that enables multiple enterprises to utilize
private domain data to collaboratively train large models for vertical domains,
achieving intelligent services across scenarios. The authors discuss in-depth
the strategic transformation of federated learning in terms of intelligence
foundation and objectives in the era of big model, as well as the new
challenges faced in heterogeneous data, model aggregation, performance and cost
trade-off, data privacy, and incentive mechanism. The paper elaborates a case
study of leading enterprises contributing multimodal data and expert knowledge
to city safety operation management , including distributed deployment and
efficient coordination of the federated learning platform, technical
innovations on data quality improvement based on large model capabilities and
efficient joint fine-tuning approaches. Preliminary experiments show that
enterprises can enhance and accumulate intelligent capabilities through
multimodal model federated learning, thereby jointly creating an smart city
model that provides high-quality intelligent services covering energy
infrastructure safety, residential community security, and urban operation
management. The established federated learning cooperation ecosystem is
expected to further aggregate industry, academia, and research resources,
realize large models in multiple vertical domains, and promote the large-scale
industrial application of artificial intelligence and cutting-edge research on
multimodal federated learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11787">HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials. (arXiv:2308.11787v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cisse_A/0/1/0/all/0/1">Abdoulatif Cisse</a>, <a href="http://arxiv.org/find/cs/1/au:+Evangelopoulos_X/0/1/0/all/0/1">Xenophon Evangelopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Carruthers_S/0/1/0/all/0/1">Sam Carruthers</a>, <a href="http://arxiv.org/find/cs/1/au:+Gusev_V/0/1/0/all/0/1">Vladimir V. Gusev</a>, <a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1">Andrew I. Cooper</a></p>
<p>Robotics and automation offer massive accelerations for solving intractable,
multivariate scientific problems such as materials discovery, but the available
search spaces can be dauntingly large. Bayesian optimization (BO) has emerged
as a popular sample-efficient optimization engine, thriving in tasks where no
analytic form of the target function/property is known. Here we exploit expert
human knowledge in the form of hypotheses to direct Bayesian searches more
quickly to promising regions of chemical space. Previous methods have used
underlying distributions derived from existing experimental measurements, which
is unfeasible for new, unexplored scientific tasks. Also, such distributions
cannot capture intricate hypotheses. Our proposed method, which we call HypBO,
uses expert human hypotheses to generate an improved seed of samples.
Unpromising seeds are automatically discounted, while promising seeds are used
to augment the surrogate model data, thus achieving better-informed sampling.
This process continues in a global versus local search fashion, organized in a
bilevel optimization framework. We validate the performance of our method on a
range of synthetic functions and demonstrate its practical utility on a real
chemical design task where the use of expert hypotheses accelerates the search
performance significantly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11881">Adversarial Training Using Feedback Loops. (arXiv:2308.11881v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rafid_A/0/1/0/all/0/1">Ali Haisam Muhammad Rafid</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandu_A/0/1/0/all/0/1">Adrian Sandu</a></p>
<p>Deep neural networks (DNN) have found wide applicability in numerous fields
due to their ability to accurately learn very complex input-output relations.
Despite their accuracy and extensive use, DNNs are highly susceptible to
adversarial attacks due to limited generalizability. For future progress in the
field, it is essential to build DNNs that are robust to any kind of
perturbations to the data points. In the past, many techniques have been
proposed to robustify DNNs using first-order derivative information of the
network.
</p>
<p>This paper proposes a new robustification approach based on control theory. A
neural network architecture that incorporates feedback control, named Feedback
Neural Networks, is proposed. The controller is itself a neural network, which
is trained using regular and adversarial data such as to stabilize the system
outputs. The novel adversarial training approach based on the feedback control
architecture is called Feedback Looped Adversarial Training (FLAT). Numerical
results on standard test problems empirically show that our FLAT method is more
effective than the state-of-the-art to guard against adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12000">On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1">Po-An Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ariu_K/0/1/0/all/0/1">Kaito Ariu</a>, <a href="http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1">Alexandre Proutiere</a></p>
<p>We study the problem of best-arm identification with fixed budget in
stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly,
there is no algorithm that (i) performs as well as the algorithm sampling each
arm equally (this algorithm is referred to as the {\it uniform sampling}
algorithm) on all instances, and that (ii) strictly outperforms this algorithm
on at least one instance. In short, there is no algorithm better than the
uniform sampling algorithm. Towards this result, we introduce the natural class
of {\it consistent} and {\it stable} algorithms, and show that any algorithm
that performs as well as the uniform sampling algorithm on all instances
belongs to this class. The proof is completed by deriving a lower bound on the
error rate satisfied by any consistent and stable algorithm, and by showing
that the uniform sampling algorithm matches this lower bound. Our results
provide a solution to the two open problems presented in \cite{qin2022open}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12044">A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Amakor_A/0/1/0/all/0/1">Augustina C. Amakor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonntag_K/0/1/0/all/0/1">Konstantin Sonntag</a>, <a href="http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1">Sebastian Peitz</a></p>
<p>Sparsity is a highly desired feature in deep neural networks (DNNs) since it
ensures numerical efficiency, improves the interpretability of models (due to
the smaller number of relevant features), and robustness. In machine learning
approaches based on linear models, it is well known that there exists a
connecting path between the sparsest solution in terms of the $\ell^1$ norm
(i.e., zero weights) and the non-regularized solution, which is called the
regularization path. Very recently, there was a first attempt to extend the
concept of regularization paths to DNNs by means of treating the empirical loss
and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the
resulting multiobjective optimization problem. However, due to the
non-smoothness of the $\ell^1$ norm and the high number of parameters, this
approach is not very efficient from a computational perspective. To overcome
this limitation, we present an algorithm that allows for the approximation of
the entire Pareto front for the above-mentioned objectives in a very efficient
manner. We present numerical examples using both deterministic and stochastic
gradients. We furthermore demonstrate that knowledge of the regularization path
allows for a well-generalizing network parametrization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12126">An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization. (arXiv:2308.12126v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Yang_W/0/1/0/all/0/1">Weifeng Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Min_W/0/1/0/all/0/1">Wenwen Min</a></p>
<p>We propose an accelerated block proximal linear framework with adaptive
momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the
potential causes of the extrapolation step failing in some algorithms, and
resolve this issue by enhancing the comparison process that evaluates the
trade-off between the proximal gradient step and the linear extrapolation step
in our algorithm. Furthermore, we extends our algorithm to any scenario
involving updating block variables with positive integers, allowing each cycle
to randomly shuffle the update order of the variable blocks. Additionally,
under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the
function value without strictly restricting the extrapolation parameters and
step size, demonstrates the viability and effectiveness of updating these
blocks in a random order, and we also more obviously and intuitively
demonstrate that the derivative set of the sequence generated by our algorithm
is a critical point set. Moreover, we demonstrate the global convergence as
well as the linear and sublinear convergence rates of our algorithm by
utilizing the Kurdyka-Lojasiewicz (K{\L}) condition. To enhance the
effectiveness and flexibility of our algorithm, we also expand the study to the
imprecise version of our algorithm and construct an adaptive extrapolation
parameter strategy, which improving its overall performance. We apply our
algorithm to multiple non-negative matrix factorization with the $\ell_0$ norm,
nonnegative tensor decomposition with the $\ell_0$ norm, and perform extensive
numerical experiments to validate its effectiveness and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1910.09642">The SWAX Benchmark: Attacking Biometric Systems with Wax Figures. (arXiv:1910.09642v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1">Rafael Henrique Vareto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandanha_A/0/1/0/all/0/1">Araceli Marcia Sandanha</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1">William Robson Schwartz</a></p>
<p>A face spoofing attack occurs when an intruder attempts to impersonate
someone who carries a gainful authentication clearance. It is a trending topic
due to the increasing demand for biometric authentication on mobile devices,
high-security areas, among others. This work introduces a new database named
Sense Wax Attack dataset (SWAX), comprised of real human and wax figure images
and videos that endorse the problem of face spoofing detection. The dataset
consists of more than 1800 face images and 110 videos of 55 people/waxworks,
arranged in training, validation and test sets with a large range in
expression, illumination and pose variations. Experiments performed with
baseline methods show that despite the progress in recent years, advanced
spoofing methods are still vulnerable to high-quality violation attempts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07445">Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vareto_R/0/1/0/all/0/1">Rafael Henrique Vareto</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1">William Robson Schwartz</a></p>
<p>Open-set face recognition describes a scenario where unknown subjects, unseen
during the training stage, appear on test time. Not only it requires methods
that accurately identify individuals of interest, but also demands approaches
that effectively deal with unfamiliar faces. This work details a scalable
open-set face identification approach to galleries composed of hundreds and
thousands of subjects. It is composed of clustering and an ensemble of binary
learning algorithms that estimates when query face samples belong to the face
gallery and then retrieves their correct identity. The approach selects the
most suitable gallery subjects and uses the ensemble to improve prediction
performance. We carry out experiments on well-known LFW and YTF benchmarks.
Results show that competitive performance can be achieved even when targeting
scalability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12110">Constrained Stein Variational Trajectory Optimization. (arXiv:2308.12110v1 [cs.RO] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Power_T/0/1/0/all/0/1">Thomas Power</a>, <a href="http://arxiv.org/find/cs/1/au:+Berenson_D/0/1/0/all/0/1">Dmitry Berenson</a></p>
<p>We present Constrained Stein Variational Trajectory Optimization (CSVTO), an
algorithm for performing trajectory optimization with constraints on a set of
trajectories in parallel. We frame constrained trajectory optimization as a
novel form of constrained functional minimization over trajectory
distributions, which avoids treating the constraints as a penalty in the
objective and allows us to generate diverse sets of constraint-satisfying
trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find
a set of particles that approximates a distribution over low-cost trajectories
while obeying constraints. CSVTO is applicable to problems with arbitrary
equality and inequality constraints and includes a novel particle resampling
step to escape local minima. By explicitly generating diverse sets of
trajectories, CSVTO is better able to avoid poor local minima and is more
robust to initialization. We demonstrate that CSVTO outperforms baselines in
challenging highly-constrained tasks, such as a 7DoF wrench manipulation task,
where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline. Our
results demonstrate that generating diverse constraint-satisfying trajectories
improves robustness to disturbances and initialization over baselines.
</p>
</p>
</div>

    </div>
    </body>
    