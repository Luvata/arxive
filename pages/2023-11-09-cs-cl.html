<!DOCTYPE html>
<html>
<head>
<title>2023-11-09-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.03365">Leveraging Generative AI: Improving Software Metadata Classification with Generated Code-Comment Pairs. (arXiv:2311.03365v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1">Samah Syed</a>, <a href="http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1">Angel Deborah S</a></p>
<p>In software development, code comments play a crucial role in enhancing code
comprehension and collaboration. This research paper addresses the challenge of
objectively classifying code comments as "Useful" or "Not Useful." We propose a
novel solution that harnesses contextualized embeddings, particularly BERT, to
automate this classification process. We address this task by incorporating
generated code and comment pairs. The initial dataset comprised 9048 pairs of
code and comments written in C, labeled as either Useful or Not Useful. To
augment this dataset, we sourced an additional 739 lines of code-comment pairs
and generated labels using a Large Language Model Architecture, specifically
BERT. The primary objective was to build classification models that can
effectively differentiate between useful and not useful code comments. Various
machine learning algorithms were employed, including Logistic Regression,
Decision Tree, K-Nearest Neighbors (KNN), Support Vector Machine (SVM),
Gradient Boosting, Random Forest, and a Neural Network. Each algorithm was
evaluated using precision, recall, and F1-score metrics, both with the original
seed dataset and the augmented dataset. This study showcases the potential of
generative AI for enhancing binary code comment quality classification models,
providing valuable insights for software developers and researchers in the
field of natural language processing and software engineering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03420">Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19. (arXiv:2311.03420v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Francis_S/0/1/0/all/0/1">Sumam Francis</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a></p>
<p>This paper presents models created for the Social Media Mining for Health
2023 shared task. Our team addressed the first task, classifying tweets that
self-report Covid-19 diagnosis. Our approach involves a classification model
that incorporates diverse textual augmentations and utilizes R-drop to augment
data and mitigate overfitting, boosting model efficacy. Our leading model,
enhanced with R-drop and augmentations like synonym substitution, reserved
words, and back translations, outperforms the task mean and median scores. Our
system achieves an impressive F1 score of 0.877 on the test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03498">In-Context Exemplars as Clues to Retrieving from Large Associative Memory. (arXiv:2311.03498v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiachen Zhao</a></p>
<p>Recently, large language models (LLMs) have made remarkable progress in
natural language processing. The most representative ability of LLMs is
in-context learning (ICL), which enables LLMs to learn patterns from in-context
exemplars without training. The performance of ICL greatly depends on the
exemplars used. However, how to choose exemplars remains unclear due to the
lack of understanding of how in-context learning works. In this paper, we
present a novel perspective on ICL by conceptualizing it as contextual
retrieval from a model of associative memory. We establish a theoretical
framework of ICL based on Hopfield Networks. Based on our framework, we look
into how in-context exemplars influence the performance of ICL and propose more
efficient active exemplar selection. Our study sheds new light on the mechanism
of ICL by connecting it to memory retrieval, with potential implications for
advancing the understanding of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03510">Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation. (arXiv:2311.03510v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kocabiyikoglu_A/0/1/0/all/0/1">Ali Can Kocabiyikoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1">Fran&#xe7;ois Portet</a>, <a href="http://arxiv.org/find/cs/1/au:+Babouchkine_J/0/1/0/all/0/1">Jean-Marc Babouchkine</a>, <a href="http://arxiv.org/find/cs/1/au:+Gibert_P/0/1/0/all/0/1">Prudence Gibert</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanchon_H/0/1/0/all/0/1">Herv&#xe9; Blanchon</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavazzi_G/0/1/0/all/0/1">Ga&#xeb;tan Gavazzi</a></p>
<p>Hospital information systems (HIS) have become an essential part of
healthcare institutions and now incorporate prescribing support software.
Prescription support software allows for structured information capture, which
improves the safety, appropriateness and efficiency of prescriptions and
reduces the number of adverse drug events (ADEs). However, such a system
increases the amount of time physicians spend at a computer entering
information instead of providing medical care. In addition, any new visiting
clinician must learn to manage complex interfaces since each HIS has its own
interfaces. In this paper, we present a natural language interface for
e-prescribing software in the form of a spoken dialogue system accessible on a
smartphone. This system allows prescribers to record their prescriptions
verbally, a form of interaction closer to their usual practice. The system
extracts the formal representation of the prescription ready to be checked by
the prescribing software and uses the dialogue to request mandatory
information, correct errors or warn of particular situations. Since, to the
best of our knowledge, there is no existing voice-based prescription dialogue
system, we present the system developed in a low-resource environment, focusing
on dialogue modeling, semantic extraction and data augmentation. The system was
evaluated in the wild with 55 participants. This evaluation showed that our
system has an average prescription time of 66.15 seconds for physicians and
35.64 seconds for other experts, and a task success rate of 76\% for physicians
and 72\% for other experts. All evaluation data were recorded and annotated to
form PxCorpus, the first spoken drug prescription corpus that has been made
fully available to the community
(\url{https://doi.org/10.5281/zenodo.6524162}).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03533">Quantifying Uncertainty in Natural Language Explanations of Large Language Models. (arXiv:2311.03533v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tanneru_S/0/1/0/all/0/1">Sree Harsha Tanneru</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1">Chirag Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Himabindu Lakkaraju</a></p>
<p>Large Language Models (LLMs) are increasingly used as powerful tools for
several high-stakes natural language processing (NLP) applications. Recent
prompting works claim to elicit intermediate reasoning steps and key tokens
that serve as proxy explanations for LLM predictions. However, there is no
certainty whether these explanations are reliable and reflect the LLMs
behavior. In this work, we make one of the first attempts at quantifying the
uncertainty in explanations of LLMs. To this end, we propose two novel metrics
-- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to
quantify the uncertainty of generated explanations. While verbalized
uncertainty involves prompting the LLM to express its confidence in its
explanations, probing uncertainty leverages sample and model perturbations as a
means to quantify the uncertainty. Our empirical analysis of benchmark datasets
reveals that verbalized uncertainty is not a reliable estimate of explanation
confidence. Further, we show that the probing uncertainty estimates are
correlated with the faithfulness of an explanation, with lower uncertainty
corresponding to explanations with higher faithfulness. Our study provides
insights into the challenges and opportunities of quantifying uncertainty in
LLM explanations, contributing to the broader discussion of the trustworthiness
of foundation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03551">Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models. (arXiv:2311.03551v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Daniel Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kommineni_A/0/1/0/all/0/1">Aditya Kommineni</a>, <a href="http://arxiv.org/find/cs/1/au:+Alshehri_M/0/1/0/all/0/1">Mohammad Alshehri</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohanty_N/0/1/0/all/0/1">Nilamadhab Mohanty</a>, <a href="http://arxiv.org/find/cs/1/au:+Modi_V/0/1/0/all/0/1">Vedant Modi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1">Jonathan Gratch</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1">Shrikanth Narayanan</a></p>
<p>The lack of contextual information in text data can make the annotation
process of text-based emotion classification datasets challenging. As a result,
such datasets often contain labels that fail to consider all the relevant
emotions in the vocabulary. This misalignment between text inputs and labels
can degrade the performance of machine learning models trained on top of them.
As re-annotating entire datasets is a costly and time-consuming task that
cannot be done at scale, we propose to use the expressive capabilities of large
language models to synthesize additional context for input text to increase its
alignment with the annotated emotional labels. In this work, we propose a
formal definition of textual context to motivate a prompting strategy to
enhance such contextual information. We provide both human and empirical
evaluation to demonstrate the efficacy of the enhanced context. Our method
improves alignment between inputs and their human-annotated labels from both an
empirical and human-evaluated standpoint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03566">Measuring Adversarial Datasets. (arXiv:2311.03566v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuanchen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Raoyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1">Vijay Viswanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1">Tzu-Sheng Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tongshuang Wu</a></p>
<p>In the era of widespread public use of AI systems across various domains,
ensuring adversarial robustness has become increasingly vital to maintain
safety and prevent undesirable errors. Researchers have curated various
adversarial datasets (through perturbations) for capturing model deficiencies
that cannot be revealed in standard benchmark datasets. However, little is
known about how these adversarial examples differ from the original data
points, and there is still no methodology to measure the intended and
unintended consequences of those adversarial transformations. In this research,
we conducted a systematic survey of existing quantifiable metrics that describe
text instances in NLP tasks, among dimensions of difficulty, diversity, and
disagreement. We selected several current adversarial effect datasets and
compared the distributions between the original and their adversarial
counterparts. The results provide valuable insights into what makes these
datasets more challenging from a metrics perspective and whether they align
with underlying assumptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03584">Dimensions of Online Conflict: Towards Modeling Agonism. (arXiv:2311.03584v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Canute_M/0/1/0/all/0/1">Matt Canute</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Mali Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+holtzclaw_h/0/1/0/all/0/1">hannah holtzclaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Lusoli_A/0/1/0/all/0/1">Alberto Lusoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Adams_P/0/1/0/all/0/1">Philippa R Adams</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandya_M/0/1/0/all/0/1">Mugdha Pandya</a>, <a href="http://arxiv.org/find/cs/1/au:+Taboada_M/0/1/0/all/0/1">Maite Taboada</a>, <a href="http://arxiv.org/find/cs/1/au:+Maynard_D/0/1/0/all/0/1">Diana Maynard</a>, <a href="http://arxiv.org/find/cs/1/au:+Chun_W/0/1/0/all/0/1">Wendy Hui Kyong Chun</a></p>
<p>Agonism plays a vital role in democratic dialogue by fostering diverse
perspectives and robust discussions. Within the realm of online conflict there
is another type: hateful antagonism, which undermines constructive dialogue.
Detecting conflict online is central to platform moderation and monetization.
It is also vital for democratic dialogue, but only when it takes the form of
agonism. To model these two types of conflict, we collected Twitter
conversations related to trending controversial topics. We introduce a
comprehensive annotation schema for labelling different dimensions of conflict
in the conversations, such as the source of conflict, the target, and the
rhetorical strategies deployed. Using this schema, we annotated approximately
4,000 conversations with multiple labels. We then trained both logistic
regression and transformer-based models on the dataset, incorporating context
from the conversation, including the number of participants and the structure
of the interactions. Results show that contextual labels are helpful in
identifying conflict and make the models robust to variations in topic. Our
research contributes a conceptualization of different dimensions of conflict, a
richly annotated dataset, and promising results that can contribute to content
moderation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03614">STONYBOOK: A System and Resource for Large-Scale Analysis of Novels. (arXiv:2311.03614v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pethe_C/0/1/0/all/0/1">Charuta Pethe</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1">Allen Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhakar_R/0/1/0/all/0/1">Rajesh Prabhakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pial_T/0/1/0/all/0/1">Tanzir Pial</a>, <a href="http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1">Steven Skiena</a></p>
<p>Books have historically been the primary mechanism through which narratives
are transmitted. We have developed a collection of resources for the
large-scale analysis of novels, including: (1) an open source end-to-end NLP
analysis pipeline for the annotation of novels into a standard XML format, (2)
a collection of 49,207 distinct cleaned and annotated novels, and (3) a
database with an associated web interface for the large-scale aggregate
analysis of these literary works. We describe the major functionalities
provided in the annotation system along with their utilities. We present
samples of analysis artifacts from our website, such as visualizations of
character occurrences and interactions, similar books, representative
vocabulary, part of speech statistics, and readability metrics. We also
describe the use of the annotated format in qualitative and quantitative
analysis across large corpora of novels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03627">GNAT: A General Narrative Alignment Tool. (arXiv:2311.03627v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pial_T/0/1/0/all/0/1">Tanzir Pial</a>, <a href="http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1">Steven Skiena</a></p>
<p>Algorithmic sequence alignment identifies similar segments shared between
pairs of documents, and is fundamental to many NLP tasks. But it is difficult
to recognize similarities between distant versions of narratives such as
translations and retellings, particularly for summaries and abridgements which
are much shorter than the original novels.
</p>
<p>We develop a general approach to narrative alignment coupling the
Smith-Waterman algorithm from bioinformatics with modern text similarity
metrics. We show that the background of alignment scores fits a Gumbel
distribution, enabling us to define rigorous p-values on the significance of
any alignment. We apply and evaluate our general narrative alignment tool
(GNAT) on four distinct problem domains differing greatly in both the relative
and absolute length of documents, namely summary-to-book alignment, translated
book alignment, short story alignment, and plagiarism detection --
demonstrating the power and performance of our methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03633">Innovation and Word Usage Patterns in Machine Learning. (arXiv:2311.03633v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borges_V/0/1/0/all/0/1">V&#xed;tor Bandeira Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Cajueiro_D/0/1/0/all/0/1">Daniel Oliveira Cajueiro</a></p>
<p>In this study, we delve into the dynamic landscape of machine learning
research evolution. Initially, through the utilization of Latent Dirichlet
Allocation, we discern pivotal themes and fundamental concepts that have
emerged within the realm of machine learning. Subsequently, we undertake a
comprehensive analysis to track the evolutionary trajectories of these
identified themes. To quantify the novelty and divergence of research
contributions, we employ the Kullback-Leibler Divergence metric. This
statistical measure serves as a proxy for ``surprise'', indicating the extent
of differentiation between the content of academic papers and the subsequent
developments in research. By amalgamating these insights, we gain the ability
to ascertain the pivotal roles played by prominent researchers and the
significance of specific academic venues (periodicals and conferences) within
the machine learning domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03658">The Linear Representation Hypothesis and the Geometry of Large Language Models. (arXiv:2311.03658v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1">Kiho Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1">Yo Joong Choe</a>, <a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1">Victor Veitch</a></p>
<p>Informally, the 'linear representation hypothesis' is the idea that
high-level concepts are represented linearly as directions in some
representation space. In this paper, we address two closely related questions:
What does "linear representation" actually mean? And, how do we make sense of
geometric notions (e.g., cosine similarity or projection) in the representation
space? To answer these, we use the language of counterfactuals to give two
formalizations of "linear representation", one in the output (word)
representation space, and one in the input (sentence) space. We then prove
these connect to linear probing and model steering, respectively. To make sense
of geometric notions, we use the formalization to identify a particular
(non-Euclidean) inner product that respects language structure in a sense we
make precise. Using this causal inner product, we show how to unify all notions
of linear representation. In particular, this allows the construction of probes
and steering vectors using counterfactual pairs. Experiments with LLaMA-2
demonstrate the existence of linear representations of concepts, the connection
to interpretation and control, and the fundamental role of the choice of inner
product.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03663">Generalization of NLP Models: Notion and Causation. (arXiv:2311.03663v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1">Aparna Elangovan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiayuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1">Karin Verspoor</a></p>
<p>The NLP community typically relies on performance of a model on a held-out
test set to assess generalization. Performance drops observed in datasets
outside of official test sets are generally attributed to
"out-of-distribution'' effects. Here, we explore the foundations of
generalizability and study the various factors that affect it, articulating
generalizability lessons from clinical studies. In clinical research
generalizability depends on (a) internal validity of experiments to ensure
controlled measurement of cause and effect, and (b) external validity or
transportability of the results to the wider population. We present the need to
ensure internal validity when building machine learning models in natural
language processing, especially where results may be impacted by spurious
correlations in the data. We demonstrate how spurious factors, such as the
distance between entities in relation extraction tasks, can affect model
internal validity and in turn adversely impact generalization. We also offer
guidance on how to analyze generalization failures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03672">CBSiMT: Mitigating Hallucination in Simultaneous Machine Translation with Weighted Prefix-to-Prefix Training. (arXiv:2311.03672v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengge Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yanzhi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuhang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1">Jian Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuoying Chen</a></p>
<p>Simultaneous machine translation (SiMT) is a challenging task that requires
starting translation before the full source sentence is available.
Prefix-to-prefix framework is often applied to SiMT, which learns to predict
target tokens using only a partial source prefix. However, due to the word
order difference between languages, misaligned prefix pairs would make SiMT
models suffer from serious hallucination problems, i.e. target outputs that are
unfaithful to source inputs. Such problems can not only produce target tokens
that are not supported by the source prefix, but also hinder generating the
correct translation by receiving more source words. In this work, we propose a
Confidence-Based Simultaneous Machine Translation (CBSiMT) framework, which
uses model confidence to perceive hallucination tokens and mitigates their
negative impact with weighted prefix-to-prefix training. Specifically,
token-level and sentence-level weights are calculated based on model confidence
and acted on the loss function. We explicitly quantify the faithfulness of the
generated target tokens using the token-level weight, and employ the
sentence-level weight to alleviate the disturbance of sentence pairs with
serious word order differences on the model. Experimental results on MuST-C
English-to-Chinese and WMT15 German-to-English SiMT tasks demonstrate that our
method can consistently improve translation quality at most latency regimes,
with up to 2 BLEU scores improvement at low latency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03687">Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models. (arXiv:2311.03687v1 [cs.PF])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Longteng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zeyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xinglin Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1">Peijie Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Ruibo Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Rui Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1">Qiong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shaohuai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiaowen Chu</a></p>
<p>Large Language Models (LLMs) have seen great advance in both academia and
industry, and their popularity results in numerous open-source frameworks and
techniques in accelerating LLM pre-training, fine-tuning, and inference.
Training and deploying LLMs are expensive as it requires considerable computing
resources and memory, hence many efficient approaches have been developed for
improving system pipelines as well as operators. However, the runtime
performance can vary significantly across hardware and software stacks, which
makes it difficult to choose the best configuration. In this work, we aim to
benchmark the performance from both macro and micro perspectives. First, we
benchmark the end-to-end performance of pre-training, fine-tuning, and serving
LLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and
70B) on three 8-GPU platforms with and without individual optimization
techniques, including ZeRO, quantization, recomputation, FlashAttention. Then,
we dive deeper to provide a detailed runtime analysis of the sub-modules,
including computing and communication operators in LLMs. For end users, our
benchmark and findings help better understand different optimization
techniques, training and inference frameworks, together with hardware platforms
in choosing configurations for deploying LLMs. For researchers, our in-depth
module-wise analyses discover potential opportunities for future work to
further optimize the runtime performance of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03696">Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation of Lecture Transcripts. (arXiv:2311.03696v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Haiyue Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1">Raj Dabre</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1">Chenhui Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1">Atsushi Fujita</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1">Sadao Kurohashi</a></p>
<p>Lecture transcript translation helps learners understand online courses,
however, building a high-quality lecture machine translation system lacks
publicly available parallel corpora. To address this, we examine a framework
for parallel corpus mining, which provides a quick and effective way to mine a
parallel corpus from publicly available lectures on Coursera. To create the
parallel corpora, we propose a dynamic programming based sentence alignment
algorithm which leverages the cosine similarity of machine-translated
sentences. The sentence alignment F1 score reaches 96%, which is higher than
using the BERTScore, LASER, or sentBERT methods. For both English--Japanese and
English--Chinese lecture translations, we extracted parallel corpora of
approximately 50,000 lines and created development and test sets through manual
filtering for benchmarking translation performance. Through machine translation
experiments, we show that the mined corpora enhance the quality of lecture
transcript translation when used in conjunction with out-of-domain parallel
corpora via multistage fine-tuning. Furthermore, this study also suggests
guidelines for gathering and cleaning corpora, mining parallel sentences,
cleaning noise in the mined data, and creating high-quality evaluation splits.
For the sake of reproducibility, we have released the corpora as well as the
code to create them. The dataset is available at
https://github.com/shyyhs/CourseraParallelCorpusMining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03716">LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators. (arXiv:2311.03716v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1">Allen Roush</a>, <a href="http://arxiv.org/find/cs/1/au:+Zakirov_E/0/1/0/all/0/1">Emil Zakirov</a>, <a href="http://arxiv.org/find/cs/1/au:+Shirokov_A/0/1/0/all/0/1">Artemiy Shirokov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lunina_P/0/1/0/all/0/1">Polina Lunina</a>, <a href="http://arxiv.org/find/cs/1/au:+Gane_J/0/1/0/all/0/1">Jack Gane</a>, <a href="http://arxiv.org/find/cs/1/au:+Duffy_A/0/1/0/all/0/1">Alexander Duffy</a>, <a href="http://arxiv.org/find/cs/1/au:+Basil_C/0/1/0/all/0/1">Charlie Basil</a>, <a href="http://arxiv.org/find/cs/1/au:+Whitcomb_A/0/1/0/all/0/1">Aber Whitcomb</a>, <a href="http://arxiv.org/find/cs/1/au:+Benedetto_J/0/1/0/all/0/1">Jim Benedetto</a>, <a href="http://arxiv.org/find/cs/1/au:+DeWolfe_C/0/1/0/all/0/1">Chris DeWolfe</a></p>
<p>Recent advancements in text-to-image generation have revolutionized numerous
fields, including art and cinema, by automating the generation of high-quality,
context-aware images and video. However, the utility of these technologies is
often limited by the inadequacy of text prompts in guiding the generator to
produce artistically coherent and subject-relevant images. In this paper, We
describe the techniques that can be used to make Large Language Models (LLMs)
act as Art Directors that enhance image and video generation. We describe our
unified system for this called "LaDi". We explore how LaDi integrates multiple
techniques for augmenting the capabilities of text-to-image generators (T2Is)
and text-to-video generators (T2Vs), with a focus on constrained decoding,
intelligent prompting, fine-tuning, and retrieval. LaDi and these techniques
are being used today in apps and platforms developed by Plai Labs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03731">A Survey of Large Language Models Attribution. (arXiv:2311.03731v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongfang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zetian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinshuo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Baotian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1">Aiguo Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a></p>
<p>Open-domain generative systems have gained significant attention in the field
of conversational AI (e.g., generative search engines). This paper presents a
comprehensive review of the attribution mechanisms employed by these systems,
particularly large language models. Though attribution or citation improve the
factuality and verifiability, issues like ambiguous knowledge reservoirs,
inherent biases, and the drawbacks of excessive attribution can hinder the
effectiveness of these systems. The aim of this survey is to provide valuable
insights for researchers, aiding in the refinement of attribution methodologies
to enhance the reliability and veracity of responses generated by open-domain
generative systems. We believe that this field is still in its early stages;
hence, we maintain a repository to keep track of ongoing studies at
https://github.com/HITsz-TMG/awesome-llm-attributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03732">Learning to Learn for Few-shot Continual Active Learning. (arXiv:2311.03732v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1">Stella Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Longxiang Gao</a></p>
<p>Continual learning strives to ensure stability in solving previously seen
tasks while demonstrating plasticity in a novel domain. Recent advances in CL
are mostly confined to a supervised learning setting, especially in NLP domain.
In this work, we consider a few-shot continual active learning (CAL) setting
where labeled data is inadequate, and unlabeled data is abundant but with a
limited annotation budget. We propose a simple but efficient method, called
Meta-Continual Active Learning. Specifically, we employ meta-learning and
experience replay to address the trade-off between stability and plasticity. As
a result, it finds an optimal initialization that efficiently utilizes
annotated information for fast adaptation while preventing catastrophic
forgetting of past tasks. We conduct extensive experiments to validate the
effectiveness of the proposed method and analyze the effect of various active
learning strategies and memory sample selection methods in a few-shot CAL
setup. Our experiment results demonstrate that random sampling is the best
default strategy for both active learning and memory sample selection to solve
few-shot CAL problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03734">Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning. (arXiv:2311.03734v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruosen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xinya Du</a></p>
<p>Neural models, including large language models (LLMs), achieve superior
performance on multi-hop question-answering. To elicit reasoning capabilities
from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to
generate both the reasoning chain and the answer, which enhances the model's
capabilities in conducting multi-hop reasoning. However, several challenges
still remain: such as struggling with inaccurate reasoning, hallucinations, and
lack of interpretability. On the other hand, information extraction (IE)
identifies entities, relations, and events grounded to the text. The extracted
structured information can be easily interpreted by humans and machines
(Grishman, 2019). In this work, we investigate constructing and leveraging
extracted semantic structures (graphs) for multi-hop question answering,
especially the reasoning process. Empirical results and human evaluations show
that our framework: generates more faithful reasoning chains and substantially
improves the QA performance on two benchmark datasets. Moreover, the extracted
structures themselves naturally provide grounded explanations that are
preferred by humans, as compared to the generated reasoning chains and
saliency-based explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03748">Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning. (arXiv:2311.03748v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sarkar Snigdha Sarathi Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ranran Haoran Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1">Peng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1">Wenpeng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a></p>
<p>Unified Sequence Labeling that articulates different sequence labeling
problems such as Named Entity Recognition, Relation Extraction, Semantic Role
Labeling, etc. in a generalized sequence-to-sequence format opens up the
opportunity to make the maximum utilization of large language model knowledge
toward structured prediction. Unfortunately, this requires formatting them into
specialized augmented format unknown to the base pretrained language model
(PLMs) necessitating finetuning to the target format. This significantly bounds
its usefulness in data-limited settings where finetuning large models cannot
properly generalize to the target format. To address this challenge and
leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic
sparse finetuning strategy that selectively focuses on a fraction of
parameters, informed by feedback from highly regressing examples, during the
fine-tuning process. By leveraging the dynamism of sparsity, our approach
mitigates the impact of well-learned samples and prioritizes underperforming
instances for improvement in generalization. Across five tasks of sequence
labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low
resource settings offering upto 40% performance improvements over full
fine-tuning depending on target evaluation settings. Also, compared to
in-context learning and other parameter-efficient fine-tuning approaches,
FISH-DIP performs comparably or better, notably in extreme low-resource
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03753">COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System. (arXiv:2311.03753v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jipeng Han</a></p>
<p>This paper explores the integration of neural networks with logic
programming, addressing the longstanding challenges of combining the
generalization and learning capabilities of neural networks with the precision
of symbolic logic. Traditional attempts at this integration have been hampered
by difficulties in initial data acquisition, the reliability of undertrained
networks, and the complexity of reusing and augmenting trained models. To
overcome these issues, we introduce the COOL (Constraint Object-Oriented Logic)
programming language, an innovative approach that seamlessly combines logical
reasoning with neural network technologies. COOL is engineered to autonomously
handle data collection, mitigating the need for user-supplied initial data. It
incorporates user prompts into the coding process to reduce the risks of
undertraining and enhances the interaction among models throughout their
lifecycle to promote the reuse and augmentation of networks. Furthermore, the
foundational principles and algorithms in COOL's design and its compilation
system could provide valuable insights for future developments in programming
languages and neural network architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03754">Which is better? Exploring Prompting Strategy For LLM-based Metrics. (arXiv:2311.03754v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joonghoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Saeran Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1">Kiyoon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangmin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Seung Hun Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jiyoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1">Pilsung Kang</a></p>
<p>This paper describes the DSBA submissions to the Prompting Large Language
Models as Explainable Metrics shared task, where systems were submitted to two
tracks: small and large summarization tracks. With advanced Large Language
Models (LLMs) such as GPT-4, evaluating the quality of Natural Language
Generation (NLG) has become increasingly paramount. Traditional
similarity-based metrics such as BLEU and ROUGE have shown to misalign with
human evaluation and are ill-suited for open-ended generation tasks. To address
this issue, we explore the potential capability of LLM-based metrics,
especially leveraging open-source LLMs. In this study, wide range of prompts
and prompting techniques are systematically analyzed with three approaches:
prompting strategy, score aggregation, and explainability. Our research focuses
on formulating effective prompt templates, determining the granularity of NLG
quality scores and assessing the impact of in-context examples on LLM-based
evaluation. Furthermore, three aggregation strategies are compared to identify
the most reliable method for aggregating NLG quality scores. To examine
explainability, we devise a strategy that generates rationales for the scores
and analyzes the characteristics of the explanation produced by the open-source
LLMs. Extensive experiments provide insights regarding evaluation capabilities
of open-source LLMs and suggest effective prompting strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03755">Multilingual Mathematical Autoformalization. (arXiv:2311.03755v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1">Albert Q. Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1">Mateja Jamnik</a></p>
<p>Autoformalization is the task of translating natural language materials into
machine-verifiable formalisations. Progress in autoformalization research is
hindered by the lack of a sizeable dataset consisting of informal-formal pairs
expressing the same essence. Existing methods tend to circumvent this challenge
by manually curating small corpora or using few-shot learning with large
language models. But these methods suffer from data scarcity and formal
language acquisition difficulty. In this work, we create $\texttt{MMA}$, a
large, flexible, multilingual, and multi-domain dataset of informal-formal
pairs, by using a language model to translate in the reverse direction, that
is, from formal mathematical statements into corresponding informal ones.
Experiments show that language models fine-tuned on $\texttt{MMA}$ produce
$16-18\%$ of statements acceptable with minimal corrections on the
$\texttt{miniF2F}$ and $\texttt{ProofNet}$ benchmarks, up from $0\%$ with the
base model. We demonstrate that fine-tuning on multilingual formal data results
in more capable autoformalization models even when deployed on monolingual
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03767">Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation. (arXiv:2311.03767v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pushpdeep Singh</a></p>
<p>Neural Machine Translation (NMT) models are state-of-the-art for machine
translation. However, these models are known to have various social biases,
especially gender bias. Most of the work on evaluating gender bias in NMT has
focused primarily on English as the source language. For source languages
different from English, most of the studies use gender-neutral sentences to
evaluate gender bias. However, practically, many sentences that we encounter do
have gender information. Therefore, it makes more sense to evaluate for bias
using such sentences. This allows us to determine if NMT models can identify
the correct gender based on the grammatical gender cues in the source sentence
rather than relying on biased correlations with, say, occupation terms. To
demonstrate our point, in this work, we use Hindi as the source language and
construct two sets of gender-specific sentences: OTSC-Hindi and WinoMT-Hindi
that we use to evaluate different Hindi-English (HI-EN) NMT systems
automatically for gender bias. Our work highlights the importance of
considering the nature of language when designing such extrinsic bias
evaluation datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03780">Ensembling Textual and Structure-Based Models for Knowledge Graph Completion. (arXiv:2311.03780v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nandi_A/0/1/0/all/0/1">Ananjan Nandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaur_N/0/1/0/all/0/1">Navdeep Kaur</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1">Parag Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a></p>
<p>We consider two popular approaches to Knowledge Graph Completion (KGC):
textual models that rely on textual entity descriptions, and structure-based
models that exploit the connectivity structure of the Knowledge Graph (KG).
Preliminary experiments show that these approaches have complementary
strengths: structure-based models perform well when the gold answer is easily
reachable from the query head in the KG, while textual models exploit
descriptions to give good performance even when the gold answer is not
reachable. In response, we explore ensembling as a way of combining the best of
both approaches. We propose a novel method for learning query-dependent
ensemble weights by using the distributions of scores assigned by individual
models to all candidate entities. Our ensemble baseline achieves
state-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR
and 8.3 pt Hits@1 gains over best individual models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03788">Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?. (arXiv:2311.03788v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shaoyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junzhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1">Deyi Xiong</a></p>
<p>Multilingual pretrained language models serve as repositories of multilingual
factual knowledge. Nevertheless, a substantial performance gap of factual
knowledge probing exists between high-resource languages and low-resource
languages, suggesting limited implicit factual knowledge transfer across
languages in multilingual pretrained language models. This paper investigates
the feasibility of explicitly transferring relatively rich factual knowledge
from English to non-English languages. To accomplish this, we propose two
parameter-free $\textbf{L}$anguage $\textbf{R}$epresentation
$\textbf{P}$rojection modules (LRP2). The first module converts non-English
representations into English-like equivalents, while the second module reverts
English-like representations back into representations of the corresponding
non-English language. Experimental results on the mLAMA dataset demonstrate
that LRP2 significantly improves factual knowledge retrieval accuracy and
facilitates knowledge transferability across diverse non-English languages. We
further investigate the working mechanism of LRP2 from the perspectives of
representation space and cross-lingual knowledge neuron.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03792">Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment. (arXiv:2311.03792v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasan_J/0/1/0/all/0/1">Jakir Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1">Shrestha Datta</a>, <a href="http://arxiv.org/find/cs/1/au:+Debnath_A/0/1/0/all/0/1">Ameya Debnath</a></p>
<p>The International Phonetic Alphabet (IPA) is indispensable in language
learning and understanding, aiding users in accurate pronunciation and
comprehension. Additionally, it plays a pivotal role in speech therapy,
linguistic research, accurate transliteration, and the development of
text-to-speech systems, making it an essential tool across diverse fields.
Bangla being 7th as one of the widely used languages, gives rise to the need
for IPA in its domain. Its IPA mapping is too diverse to be captured manually
giving the need for Artificial Intelligence and Machine Learning in this field.
In this study, we have utilized a transformer-based sequence-to-sequence model
at the letter and symbol level to get the IPA of each Bangla word as the
variation of IPA in association of different words is almost null. Our
transformer model only consisted of 8.5 million parameters with only a single
decoder and encoder layer. Additionally, to handle the punctuation marks and
the occurrence of foreign languages in the text, we have utilized manual
mapping as the model won't be able to learn to separate them from Bangla words
while decreasing our required computational resources. Finally, maintaining the
relative position of the sentence component IPAs and generation of the combined
IPA has led us to achieve the top position with a word error rate of 0.10582 in
the public ranking of DataVerse Challenge - ITVerse 2023
(https://www.kaggle.com/competitions/dataverse_2023/).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03798">Noisy Pair Corrector for Dense Retrieval. (arXiv:2311.03798v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yeyun Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xingwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dayiheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daya Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jiancheng Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jian Guo</a></p>
<p>Most dense retrieval models contain an implicit assumption: the training
query-document pairs are exactly matched. Since it is expensive to annotate the
corpus manually, training pairs in real-world applications are usually
collected automatically, which inevitably introduces mismatched-pair noise. In
this paper, we explore an interesting and challenging problem in dense
retrieval, how to train an effective model with mismatched-pair noise. To solve
this problem, we propose a novel approach called Noisy Pair Corrector (NPC),
which consists of a detection module and a correction module. The detection
module estimates noise pairs by calculating the perplexity between annotated
positive and easy negative documents. The correction module utilizes an
exponential moving average (EMA) model to provide a soft supervised signal,
aiding in mitigating the effects of noise. We conduct experiments on
text-retrieval benchmarks Natural Question and TriviaQA, code-search benchmarks
StaQC and SO-DS. Experimental results show that NPC achieves excellent
performance in handling both synthetic and realistic noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03810">Rethinking and Improving Multi-task Learning for End-to-end Speech Translation. (arXiv:2311.03810v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Tong Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chunliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jingbo Zhu</a></p>
<p>Significant improvements in end-to-end speech translation (ST) have been
achieved through the application of multi-task learning. However, the extent to
which auxiliary tasks are highly consistent with the ST task, and how much this
approach truly helps, have not been thoroughly studied. In this paper, we
investigate the consistency between different tasks, considering different
times and modules. We find that the textual encoder primarily facilitates
cross-modal conversion, but the presence of noise in speech impedes the
consistency between text and speech representations. Furthermore, we propose an
improved multi-task learning (IMTL) approach for the ST task, which bridges the
modal gap by mitigating the difference in length and representation. We conduct
experiments on the MuST-C dataset. The results demonstrate that our method
attains state-of-the-art results. Moreover, when additional data is used, we
achieve the new SOTA result on MuST-C English to Spanish task with 20.8% of the
training time required by the current SOTA method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03812">Conversations in Galician: a Large Language Model for an Underrepresented Language. (arXiv:2311.03812v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bao_E/0/1/0/all/0/1">Eliseo Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1">Anxo P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Parapar_J/0/1/0/all/0/1">Javier Parapar</a></p>
<p>The recent proliferation of Large Conversation Language Models has
highlighted the economic significance of widespread access to this type of AI
technologies in the current information age. Nevertheless, prevailing models
have primarily been trained on corpora consisting of documents written in
popular languages. The dearth of such cutting-edge tools for low-resource
languages further exacerbates their underrepresentation in the current economic
landscape, thereby impacting their native speakers. This paper introduces two
novel resources designed to enhance Natural Language Processing (NLP) for the
Galician language. We present a Galician adaptation of the Alpaca dataset,
comprising 52,000 instructions and demonstrations. This dataset proves
invaluable for enhancing language models by fine-tuning them to more accurately
adhere to provided instructions. Additionally, as a demonstration of the
dataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician,
a language not originally supported by the model, by following the Alpaca
format. This work contributes to the research on multilingual models tailored
for low-resource settings, a crucial endeavor in ensuring the inclusion of all
linguistic communities in the development of Large Language Models. Another
noteworthy aspect of this research is the exploration of how knowledge of a
closely related language, in this case, Portuguese, can assist in generating
coherent text when training resources are scarce. Both the Galician Alpaca
dataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we
have made the source code available to facilitate replication of this
experiment and encourage further advancements for underrepresented languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03837">OLaLa: Ontology Matching with Large Language Models. (arXiv:2311.03837v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1">Sven Hertling</a>, <a href="http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1">Heiko Paulheim</a></p>
<p>Ontology (and more generally: Knowledge Graph) Matching is a challenging task
where information in natural language is one of the most important signals to
process. With the rise of Large Language Models, it is possible to incorporate
this knowledge in a better way into the matching pipeline. A number of
decisions still need to be taken, e.g., how to generate a prompt that is useful
to the model, how information in the KG can be formulated in prompts, which
Large Language Model to choose, how to provide existing correspondences to the
model, how to generate candidates, etc. In this paper, we present a prototype
that explores these questions by applying zero-shot and few-shot prompting with
multiple open Large Language Models to different tasks of the Ontology
Alignment Evaluation Initiative (OAEI). We show that with only a handful of
examples and a well-designed prompt, it is possible to achieve results that are
en par with supervised matching systems which use a much larger portion of the
ground truth.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03839">Aspects of human memory and Large Language Models. (arXiv:2311.03839v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1">Romuald A. Janik</a></p>
<p>Large Language Models (LLMs) are huge artificial neural networks which
primarily serve to generate text, but also provide a very sophisticated
probabilistic model of language use. Since generating a semantically consistent
text requires a form of effective memory, we investigate the memory properties
of LLMs and find surprising similarities with key characteristics of human
memory. This result strongly suggests that the biological features of human
memory leave an imprint on the way that we structure our textual narratives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03881">Sparse Contrastive Learning of Sentence Embeddings. (arXiv:2311.03881v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_R/0/1/0/all/0/1">Ruize An</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawei Song</a></p>
<p>Recently, SimCSE has shown the feasibility of contrastive learning in
training sentence embeddings and illustrates its expressiveness in spanning an
aligned and uniform embedding space. However, prior studies have shown that
dense models could contain harmful parameters that affect the model
performance, and it is no wonder that SimCSE can as well be invented with such
parameters. Driven by this, parameter sparsification is applied, where
alignment and uniformity scores are used to measure the contribution of each
parameter to the overall quality of sentence embeddings. Drawing from a
preliminary study, we consider parameters with minimal contributions to be
detrimental, as their sparsification results in improved model performance. To
discuss the ubiquity of detrimental parameters and remove them, more
experiments on the standard semantic textual similarity (STS) tasks and
transfer learning tasks are conducted, and the results show that the proposed
sparsified SimCSE (SparseCSE) has excellent performance in comparison with
SimCSE. Furthermore, through in-depth analysis, we establish the validity and
stability of our sparsification method, showcasing that the embedding space
generated by SparseCSE exhibits improved alignment compared to that produced by
SimCSE. Importantly, the uniformity yet remains uncompromised.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03896">iACOS: Advancing Implicit Sentiment Extraction with Informative and Adaptive Negative Examples. (arXiv:2311.03896v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiancai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jia-Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Lei Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhishang Liu</a></p>
<p>Aspect-based sentiment analysis (ABSA) have been extensively studied, but
little light has been shed on the quadruple extraction consisting of four
fundamental elements: aspects, categories, opinions and sentiments, especially
with implicit aspects and opinions. In this paper, we propose a new method
iACOS for extracting Implicit Aspects with Categories and Opinions with
Sentiments. First, iACOS appends two implicit tokens at the end of a text to
capture the context-aware representation of all tokens including implicit
aspects and opinions. Second, iACOS develops a sequence labeling model over the
context-aware token representation to co-extract explicit and implicit aspects
and opinions. Third, iACOS devises a multi-label classifier with a specialized
multi-head attention for discovering aspect-opinion pairs and predicting their
categories and sentiments simultaneously. Fourth, iACOS leverages informative
and adaptive negative examples to jointly train the multi-label classifier and
the other two classifiers on categories and sentiments by multi-task learning.
Finally, the experimental results show that iACOS significantly outperforms
other quadruple extraction baselines according to the F1 score on two public
benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03928">Improving Korean NLP Tasks with Linguistically Informed Subword Tokenization and Sub-character Decomposition. (arXiv:2311.03928v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeon_T/0/1/0/all/0/1">Taehee Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bongseok Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Changhwan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1">Yoonseob Lim</a></p>
<p>We introduce a morpheme-aware subword tokenization method that utilizes
sub-character decomposition to address the challenges of applying Byte Pair
Encoding (BPE) to Korean, a language characterized by its rich morphology and
unique writing system. Our approach balances linguistic accuracy with
computational efficiency in Pre-trained Language Models (PLMs). Our evaluations
show that this technique achieves good performances overall, notably improving
results in the syntactic task of NIKL-CoLA. This suggests that integrating
morpheme type information can enhance language models' syntactic and semantic
capabilities, indicating that adopting more linguistic insights can further
improve performance beyond standard morphological analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03952">An Analysis of Dialogue Repair in Voice Assistants. (arXiv:2311.03952v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galbraith_M/0/1/0/all/0/1">Matthew Galbraith</a></p>
<p>Spoken dialogue systems have transformed human-machine interaction by
providing real-time responses to queries. However, misunderstandings between
the user and system persist. This study explores the significance of
interactional language in dialogue repair between virtual assistants and users
by analyzing interactions with Google Assistant and Siri, focusing on their
utilization and response to the other-initiated repair strategy "huh?"
prevalent in human-human interaction. Findings reveal several
assistant-generated strategies but an inability to replicate human-like repair
strategies such as "huh?". English and Spanish user acceptability surveys show
differences in users' repair strategy preferences and assistant usage, with
both similarities and disparities among the two surveyed languages. These
results shed light on inequalities between interactional language in
human-human interaction and human-machine interaction, underscoring the need
for further research on the impact of interactional language in human-machine
interaction in English and beyond.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03963">An Expectation-Realization Model for Metaphor Detection. (arXiv:2311.03963v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uduehi_O/0/1/0/all/0/1">Oseremen O. Uduehi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bunescu_R/0/1/0/all/0/1">Razvan C. Bunescu</a></p>
<p>We propose a metaphor detection architecture that is structured around two
main modules: an expectation component that estimates representations of
literal word expectations given a context, and a realization component that
computes representations of actual word meanings in context. The overall
architecture is trained to learn expectation-realization (ER) patterns that
characterize metaphorical uses of words. When evaluated on three metaphor
datasets for within distribution, out of distribution, and novel metaphor
generalization, the proposed method is shown to obtain results that are
competitive or better than state-of-the art. Further increases in metaphor
detection accuracy are obtained through ensembling of ER models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03969">Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media. (arXiv:2311.03969v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ron_G/0/1/0/all/0/1">Gal Ron</a>, <a href="http://arxiv.org/find/cs/1/au:+Levi_E/0/1/0/all/0/1">Effi Levi</a>, <a href="http://arxiv.org/find/cs/1/au:+Oshri_O/0/1/0/all/0/1">Odelia Oshri</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenhav_S/0/1/0/all/0/1">Shaul R. Shenhav</a></p>
<p>In this work we propose a novel annotation scheme which factors hate speech
into five separate discursive categories. To evaluate our scheme, we construct
a corpus of over 2.9M Twitter posts containing hateful expressions directed at
Jews, and annotate a sample dataset of 1,050 tweets. We present a statistical
analysis of the annotated dataset as well as discuss annotation examples, and
conclude by discussing promising directions for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03998">Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals. (arXiv:2311.03998v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1">Sukannya Purkayastha</a>, <a href="http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1">Anne Lauscher</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a></p>
<p>In many domains of argumentation, people's arguments are driven by so-called
attitude roots, i.e., underlying beliefs and world views, and their
corresponding attitude themes. Given the strength of these latent drivers of
arguments, recent work in psychology suggests that instead of directly
countering surface-level reasoning (e.g., falsifying given premises), one
should follow an argumentation style inspired by the Jiu-Jitsu 'soft' combat
system (Hornsey and Fielding, 2017): first, identify an arguer's attitude roots
and themes, and then choose a prototypical rebuttal that is aligned with those
drivers instead of invalidating those. In this work, we are the first to
explore Jiu-Jitsu argumentation for peer review by proposing the novel task of
attitude and theme-guided rebuttal generation. To this end, we enrich an
existing dataset for discourse structure in peer reviews with attitude roots,
attitude themes, and canonical rebuttals. To facilitate this process, we recast
established annotation concepts from the domain of peer reviews (e.g., aspects
a review sentence is relating to) and train domain-specific models. We then
propose strong rebuttal generation strategies, which we benchmark on our novel
dataset for the task of end-to-end attitude and theme-guided rebuttal
generation and two subtasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2004.14254">Hierarchical Reinforcement Learning for Automatic Disease Diagnosis. (arXiv:2004.14254v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1">Cheng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1">Kangenbei Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qianlong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baolin Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jiajie Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhongyu Wei</a></p>
<p>Motivation: Disease diagnosis oriented dialogue system models the interactive
consultation procedure as Markov Decision Process and reinforcement learning
algorithms are used to solve the problem. Existing approaches usually employ a
flat policy structure that treat all symptoms and diseases equally for action
making. This strategy works well in the simple scenario when the action space
is small, however, its efficiency will be challenged in the real environment.
Inspired by the offline consultation process, we propose to integrate a
hierarchical policy structure of two levels into the dialogue systemfor policy
learning. The high-level policy consists of amastermodel that is responsible
for triggering a low-levelmodel, the lowlevel policy consists of several
symptom checkers and a disease classifier. The proposed policy structure is
capable to deal with diagnosis problem including large number of diseases and
symptoms.
</p>
<p>Results: Experimental results on three real-world datasets and a synthetic
dataset demonstrate that our hierarchical framework achieves higher accuracy
and symptom recall in disease diagnosis compared with existing systems. We
construct a benchmark including datasets and implementation of existing
algorithms to encourage follow-up researches.
</p>
<p>Availability: The code and data is available from
https://github.com/FudanDISC/DISCOpen-MedBox-DialoDiagnosis
</p>
<p>Contact: 21210980124@m.fudan.edu.cn
</p>
<p>Supplementary information: Supplementary data are available at Bioinformatics
online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.04874">CompRes: A Dataset for Narrative Structure in News. (arXiv:2007.04874v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levi_E/0/1/0/all/0/1">Effi Levi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mor_G/0/1/0/all/0/1">Guy Mor</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenhav_S/0/1/0/all/0/1">Shaul Shenhav</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheafer_T/0/1/0/all/0/1">Tamir Sheafer</a></p>
<p>This paper addresses the task of automatically detecting narrative structures
in raw texts. Previous works have utilized the oral narrative theory by Labov
and Waletzky to identify various narrative elements in personal stories texts.
Instead, we direct our focus to news articles, motivated by their growing
social impact as well as their role in creating and shaping public opinion.
</p>
<p>We introduce CompRes -- the first dataset for narrative structure in news
media. We describe the process in which the dataset was constructed: first, we
designed a new narrative annotation scheme, better suited for news media, by
adapting elements from the narrative theory of Labov and Waletzky (Complication
and Resolution) and adding a new narrative element of our own (Success); then,
we used that scheme to annotate a set of 29 English news articles (containing
1,099 sentences) collected from news and partisan websites. We use the
annotated dataset to train several supervised models to identify the different
narrative elements, achieving an $F_1$ score of up to 0.7. We conclude by
suggesting several promising directions for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.11104">Hierarchical Text Classification As Sub-Hierarchy Sequence Generation. (arXiv:2111.11104v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1">SangHun Im</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gibaeg Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1">Heung-Seon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1">Seongung Jo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Donghwan Kim</a></p>
<p>Hierarchical text classification (HTC) is essential for various real
applications. However, HTC models are challenging to develop because they often
require processing a large volume of documents and labels with hierarchical
taxonomy. Recent HTC models based on deep learning have attempted to
incorporate hierarchy information into a model structure. Consequently, these
models are challenging to implement when the model parameters increase for a
large-scale hierarchy because the model structure depends on the hierarchy
size. To solve this problem, we formulate HTC as a sub-hierarchy sequence
generation to incorporate hierarchy information into a target label sequence
instead of the model structure. Subsequently, we propose the Hierarchy DECoder
(HiDEC), which decodes a text sequence into a sub-hierarchy sequence using
recursive hierarchy decoding, classifying all parents at the same level into
children at once. In addition, HiDEC is trained to use hierarchical path
information from a root to each leaf in a sub-hierarchy composed of the labels
of a target document via an attention mechanism and hierarchy-aware masking.
HiDEC achieved state-of-the-art performance with significantly fewer model
parameters than existing models on benchmark datasets, such as RCV1-v2, NYT,
and EURLEX57K.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.03897">Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1">Changdae Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1">Junhyuk So</a>, <a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1">Hoyoon Byun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1">YongTaek Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Minchul Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1">Jong-June Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kyungwoo Song</a></p>
<p>Pre-trained multi-modal models, such as CLIP, provide transferable embeddings
and show promising results in diverse applications. However, the analysis of
learned multi-modal embeddings is relatively unexplored, and the embedding
transferability can be improved. In this work, we observe that CLIP holds
separated embedding subspaces for two different modalities, and then we
investigate it through the lens of uniformity-alignment to measure the quality
of learned representation. Both theoretically and empirically, we show that
CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack
of alignment and uniformity might restrict the transferability and robustness
of embeddings. To this end, we devise a new fine-tuning method for robust
representation equipping better alignment and uniformity. First, we propose a
Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to
generate hard negative samples on the hypersphere. Then, we fine-tune the model
on hard negatives as well as original negatives and positives with contrastive
loss. Based on the theoretical analysis about hardness guarantee and limiting
behavior, we justify the use of our method. Extensive experiments on retrieval,
calibration, few- or zero-shot classification (under distribution shift),
embedding arithmetic, and image captioning further show that our method
provides transferable representations, enabling robust model adaptation on
diverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12040">Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent Semantic Communications. (arXiv:2210.12040v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1">Christo Kurisummoottil Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1">Walid Saad</a></p>
<p>Semantic communication (SC) aims to communicate reliably with minimal data
transfer while simultaneously providing seamless connectivity to heterogeneous
services and users. In this paper, a novel emergent SC (ESC) system framework
is proposed and is composed of a signaling game for emergent language design
and a neuro-symbolic (NeSy) artificial intelligence (AI) approach for causal
reasoning. In order to design the language, the signaling game is solved using
an alternating maximization between the communicating node's utilities. The
emergent language helps create a context-aware transmit vocabulary (minimal
semantic representation) and aids the reasoning process (enabling
generalization to unseen scenarios) by splitting complex messages into simpler
reasoning tasks for the receiver. The causal description at the transmitter is
then modeled (a neural component) as a posterior distribution of the relevant
attributes present in the data. Using the reconstructed causal state, the
receiver evaluates a set of logical formulas (symbolic part) to execute its
task. The nodes NeSy reasoning components are implemented by the recently
proposed AI tool called Generative Flow Networks, and they are optimized for
higher semantic reliability. The ESC system is designed to enhance the novel
metrics of semantic information, reliability, distortion and similarity that
are designed using rigorous algebraic properties from category theory thereby
generalizing the metrics beyond Shannon's notion of uncertainty. Simulation
results validate the ability of ESC to communicate efficiently (with reduced
bits) and achieve better semantic reliability than conventional wireless and
state-of-the-art systems that do not exploit causal reasoning capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13709">Undesirable biases in NLP: Addressing challenges of measurement. (arXiv:2211.13709v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1">Oskar van der Wal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachmann_D/0/1/0/all/0/1">Dominik Bachmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1">Alina Leidinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Maanen_L/0/1/0/all/0/1">Leendert van Maanen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1">Willem Zuidema</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1">Katrin Schulz</a></p>
<p>As Large Language Models and Natural Language Processing (NLP) technology
rapidly develop and spread into daily life, it becomes crucial to anticipate
how their use could harm people. One problem that has received a lot of
attention in recent years is that this technology has displayed harmful biases,
from generating derogatory stereotypes to producing disparate outcomes for
different social groups. Although a lot of effort has been invested in
assessing and mitigating these biases, our methods of measuring the biases of
NLP models have serious problems and it is often unclear what they actually
measure. In this paper, we provide an interdisciplinary approach to discussing
the issue of NLP model bias by adopting the lens of psychometrics -- a field
specialized in the measurement of concepts like bias that are not directly
observable. In particular, we will explore two central notions from
psychometrics, the \emph{construct validity} and the \emph{reliability} of
measurement tools, and discuss how they can be applied in the context of
measuring model bias. Our goal is to provide NLP practitioners with
methodological tools for designing better bias measures, and to inspire them
more generally to explore tools from psychometrics when working on bias
measurement tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09462">Latent Diffusion for Language Generation. (arXiv:2212.09462v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lovelace_J/0/1/0/all/0/1">Justin Lovelace</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishore_V/0/1/0/all/0/1">Varsha Kishore</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1">Chao Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shekhtman_E/0/1/0/all/0/1">Eliot Shekhtman</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1">Kilian Q. Weinberger</a></p>
<p>Diffusion models have achieved great success in modeling continuous data
modalities such as images, audio, and video, but have seen limited use in
discrete domains such as language. Recent attempts to adapt diffusion to
language have presented diffusion as an alternative to existing pretrained
language models. We view diffusion and existing language models as
complementary. We demonstrate that encoder-decoder language models can be
utilized to efficiently learn high-quality language autoencoders. We then
demonstrate that continuous diffusion models can be learned in the latent space
of the language autoencoder, enabling us to sample continuous latent
representations that can be decoded into natural language with the pretrained
decoder. We validate the effectiveness of our approach for unconditional,
class-conditional, and sequence-to-sequence language generation. We demonstrate
across multiple diverse data sets that our latent language diffusion models are
significantly more effective than previous diffusion language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10884">Break It Down: Evidence for Structural Compositionality in Neural Networks. (arXiv:2301.10884v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lepori_M/0/1/0/all/0/1">Michael A. Lepori</a>, <a href="http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1">Thomas Serre</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1">Ellie Pavlick</a></p>
<p>Though modern neural networks have achieved impressive performance in both
vision and language tasks, we know little about the functions that they
implement. One possibility is that neural networks implicitly break down
complex tasks into subroutines, implement modular solutions to these
subroutines, and compose them into an overall solution to a task - a property
we term structural compositionality. Another possibility is that they may
simply learn to match new inputs to learned templates, eliding task
decomposition entirely. Here, we leverage model pruning techniques to
investigate this question in both vision and language across a variety of
architectures, tasks, and pretraining regimens. Our results demonstrate that
models often implement solutions to subroutines via modular subnetworks, which
can be ablated while maintaining the functionality of other subnetworks. This
suggests that neural networks may be able to learn compositionality, obviating
the need for specialized symbolic mechanisms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00333">Competence-Based Analysis of Language Models. (arXiv:2303.00333v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1">Adam Davies</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jize Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1">ChengXiang Zhai</a></p>
<p>Despite the recent success of large, pretrained neural language models (LLMs)
on a variety of prompting tasks, these models can be alarmingly brittle to
small changes in inputs or application contexts. To better understand such
behavior and motivate the design of more robust LLMs, we provide a causal
formulation of linguistic competence in the context of LLMs and propose a
general framework to study and measure LLM competence. Our framework, CALM
(Competence-based Analysis of Language Models), establishes the first
quantitative measure of LLM competence, which we study by damaging models'
internal representations of various linguistic properties in the course of
performing various tasks using causal probing and evaluating models' alignment
under these interventions with a given causal model. We also develop a novel
approach for performing causal probing interventions using gradient-based
adversarial attacks, which can target a broader range of properties and
representations than existing techniques. We carry out a case study of CALM
using these interventions to analyze BERT and RoBERTa's competence across a
variety of lexical inference tasks, showing that the CALM framework and
competence metric can be valuable tools for explaining and predicting their
behavior across these tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03283">The AI Ghostwriter Effect: When Users Do Not Perceive Ownership of AI-Generated Text But Self-Declare as Authors. (arXiv:2303.03283v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Draxler_F/0/1/0/all/0/1">Fiona Draxler</a>, <a href="http://arxiv.org/find/cs/1/au:+Werner_A/0/1/0/all/0/1">Anna Werner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehmann_F/0/1/0/all/0/1">Florian Lehmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoppe_M/0/1/0/all/0/1">Matthias Hoppe</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_A/0/1/0/all/0/1">Albrecht Schmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1">Daniel Buschek</a>, <a href="http://arxiv.org/find/cs/1/au:+Welsch_R/0/1/0/all/0/1">Robin Welsch</a></p>
<p>Human-AI interaction in text production increases complexity in authorship.
In two empirical studies (n1 = 30 &amp; n2 = 96), we investigate authorship and
ownership in human-AI collaboration for personalized language generation. We
show an AI Ghostwriter Effect: Users do not consider themselves the owners and
authors of AI-generated text but refrain from publicly declaring AI authorship.
Personalization of AI-generated texts did not impact the AI Ghostwriter Effect,
and higher levels of participants' influence on texts increased their sense of
ownership. Participants were more likely to attribute ownership to supposedly
human ghostwriters than AI ghostwriters, resulting in a higher
ownership-authorship discrepancy for human ghostwriters. Rationalizations for
authorship in AI ghostwriters and human ghostwriters were similar. We discuss
how our findings relate to psychological ownership and human-AI interaction to
lay the foundations for adapting authorship frameworks and user interfaces in
AI in text-generation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10093">Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection. (arXiv:2303.10093v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1">Kyle Buettner</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1">Adriana Kovashka</a></p>
<p>Vision-language alignment learned from image-caption pairs has been shown to
benefit tasks like object recognition and detection. Methods are mostly
evaluated in terms of how well object class names are learned, but captions
also contain rich attribute context that should be considered when learning
object alignment. It is unclear how methods use this context in learning, as
well as whether models succeed when tasks require attribute and object
understanding. To address this gap, we conduct extensive analysis of the role
of attributes in vision-language models. We specifically measure model
sensitivity to the presence and meaning of attribute context, gauging influence
on object embeddings through unsupervised phrase grounding and classification
via description methods. We further evaluate the utility of attribute context
in training for open-vocabulary object detection, fine-grained text-region
retrieval, and attribution tasks. Our results show that attribute context can
be wasted when learning alignment for detection, attribute meaning is not
adequately considered in embeddings, and describing classes by only their
attributes is ineffective. A viable strategy that we find to increase benefits
from attributes is contrastive training with adjective-based negative captions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15714">Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hongyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kangrui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1">Mo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Hongyuan Mei</a></p>
<p>Language models have been shown to perform remarkably well on a wide range of
natural language processing tasks. In this paper, we propose LEAP, a novel
system that uses language models to perform multi-step logical reasoning and
incorporates explicit planning into the inference procedure. Explicit planning
enables the system to make more informed reasoning decisions at each step by
looking ahead into their future effects. Moreover, we propose a training
strategy that safeguards the planning process from being led astray by spurious
features. Our full system significantly outperforms other competing methods on
multiple standard datasets. When using small T5 models as its core selection
and deduction components, our system performs competitively compared to GPT-3
despite having only about 1B parameters (i.e., 175 times smaller than GPT-3).
When using GPT-3.5, it significantly outperforms chain-of-thought prompting on
the challenging PrOntoQA dataset. We have conducted extensive empirical studies
to demonstrate that explicit planning plays a crucial role in the system's
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03353">MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1">Damien Sileo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lernould_A/0/1/0/all/0/1">Antoine Lernould</a></p>
<p>Theory of Mind (ToM) is a critical component of intelligence but its
assessment remains the subject of heated debates. Prior research applied human
ToM assessments to natural language processing models using either
human-created standardized tests or rule-based templates. However, these
methods primarily focus on simplistic reasoning and require further validation.
Here, we leverage dynamic epistemic logic to isolate a particular component of
ToM and to generate controlled problems. We also introduce new verbalization
techniques to express these problems in English natural language. Our findings
indicate that some language model scaling (from 70M to 6B and 350M to 174B)
does not consistently yield results better than random chance. While GPT-4
demonstrates superior epistemic reasoning capabilities, there is still room for
improvement. Our code and datasets are publicly available
(https://huggingface.co/datasets/sileod/mindgames ,
https://github.com/sileod/llm-theory-of-mind )
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14303">QTSumm: Query-Focused Summarization over Tabular Data. (arXiv:2305.14303v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhenting Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1">Linyong Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_B/0/1/0/all/0/1">Boyu Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1">Weijin Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Simeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ruizhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yumo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1">Dragomir Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>People primarily consult tables to conduct data analysis or answer specific
questions. Text generation systems that can provide accurate table summaries
tailored to users' information needs can facilitate more efficient access to
relevant data insights. Motivated by this, we define a new query-focused table
summarization task, where text generation models have to perform human-like
reasoning and analysis over the given table to generate a tailored summary. We
introduce a new benchmark named QTSumm for this task, which contains 7,111
human-annotated query-summary pairs over 2,934 tables covering diverse topics.
We investigate a set of strong baselines on QTSumm, including text generation,
table-to-text generation, and large language models. Experimental results and
manual analysis reveal that the new task presents significant challenges in
table-to-text generation for future research. Moreover, we propose a new
approach named ReFactor, to retrieve and reason over query-relevant information
from tabular data to generate several natural language facts. Experimental
results demonstrate that ReFactor can bring improvements to baselines by
concatenating the generated facts to the model input. Our data and code are
publicly available at https://github.com/yale-nlp/QTSumm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14907">Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shivanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1">Matt Gardner</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a></p>
<p>In-context learning (ICL), the ability of large language models to perform
novel tasks by conditioning on a prompt with a few task examples, requires
these examples to be informative about the test instance. The standard approach
of independently ranking and selecting the most similar examples selects
redundant examples while omitting important information. In this work, we show
that BERTScore-Recall (BSR) selects better examples that demonstrate more of
the salient aspects, e.g. reasoning patterns, of the test input. We further
extend BSR and many standard metrics to easily optimizable set-level metrics,
giving still better coverage of those salient aspects. On 15 datasets spanning
6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric
for in-context example selection across the board, and (2) for compositional
tasks, set selection using Set-BSR outperforms independent ranking by up to 17
points on average and, despite being training-free, surpasses methods that
leverage task or LLM-specific training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19466">The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kazemnejad_A/0/1/0/all/0/1">Amirhossein Kazemnejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1">Inkit Padhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1">Karthikeyan Natesan Ramamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1">Payel Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a></p>
<p>Length generalization, the ability to generalize from small training context
sizes to larger ones, is a critical challenge in the development of
Transformer-based language models. Positional encoding (PE) has been identified
as a major factor influencing length generalization, but the exact impact of
different PE schemes on extrapolation in downstream tasks remains unclear. In
this paper, we conduct a systematic empirical study comparing the length
generalization performance of decoder-only Transformers with five different
position encoding approaches including Absolute Position Embedding (APE), T5's
Relative PE, ALiBi, and Rotary, in addition to Transformers without positional
encoding (NoPE). Our evaluation encompasses a battery of reasoning and
mathematical tasks. Our findings reveal that the most commonly used positional
encoding methods, such as ALiBi, Rotary, and APE, are not well suited for
length generalization in downstream tasks. More importantly, NoPE outperforms
other explicit positional encoding methods while requiring no additional
computation. We theoretically demonstrate that NoPE can represent both absolute
and relative PEs, but when trained with SGD, it mostly resembles T5's relative
PE attention patterns. Finally, we find that scratchpad is not always helpful
to solve length generalization and its format highly impacts the model's
performance. Overall, our work suggests that explicit position embeddings are
not essential for decoder-only Transformers to generalize well to longer
sequences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00802">Birth of a Transformer: A Memory Viewpoint. (arXiv:2306.00802v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1">Alberto Bietti</a>, <a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1">Vivien Cabannes</a>, <a href="http://arxiv.org/find/stat/1/au:+Bouchacourt_D/0/1/0/all/0/1">Diane Bouchacourt</a>, <a href="http://arxiv.org/find/stat/1/au:+Jegou_H/0/1/0/all/0/1">Herve Jegou</a>, <a href="http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1">Leon Bottou</a></p>
<p>Large language models based on transformers have achieved great empirical
successes. However, as they are deployed more widely, there is a growing need
to better understand their internal mechanisms in order to make them more
reliable. These models appear to store vast amounts of knowledge from their
training data, and to adapt quickly to new information provided in their
context or prompt. We study how transformers balance these two types of
knowledge by considering a synthetic setup where tokens are generated from
either global or context-specific bigram distributions. By a careful empirical
analysis of the training process on a simplified two-layer transformer, we
illustrate the fast learning of global bigrams and the slower development of an
"induction head" mechanism for the in-context bigrams. We highlight the role of
weight matrices as associative memories, provide theoretical insights on how
gradients enable their learning during training, and study the role of
data-distributional properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04657">BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jiaming Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mickel Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Juntao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xuehai Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1">Ce Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Ruiyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yizhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>In this paper, we introduce the BeaverTails dataset, aimed at fostering
research on safety alignment in large language models (LLMs). This dataset
uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have gathered safety meta-labels for 333,963
question-answer (QA) pairs and 361,903 pairs of expert comparison data for both
the helpfulness and harmlessness metrics. We further showcase applications of
BeaverTails in content moderation and reinforcement learning with human
feedback (RLHF), emphasizing its potential for practical safety measures in
LLMs. We believe this dataset provides vital resources for the community,
contributing towards the safe development and deployment of LLMs. Our project
page is available at the following URL:
https://sites.google.com/view/pku-beavertails.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07697">Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. (arXiv:2307.07697v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiashuo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chengjin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Lumingyuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Saizhuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yeyun Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1">Lionel M. Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1">Heung-Yeung Shum</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jian Guo</a></p>
<p>Although large language models (LLMs) have achieved significant success in
various tasks, they often struggle with hallucination problems, especially in
scenarios requiring deep and responsible reasoning. These issues could be
partially addressed by introducing external knowledge graphs (KG) in LLM
reasoning. In this paper, we propose a new LLM-KG integrating paradigm
``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to
interactively explore related entities and relations on KGs and perform
reasoning based on the retrieved knowledge. We further implement this paradigm
by introducing a new approach called Think-on-Graph (ToG), in which the LLM
agent iteratively executes beam search on KG, discovers the most promising
reasoning paths, and returns the most likely reasoning results. We use a number
of well-designed experiments to examine and illustrate the following advantages
of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has
the ability of knowledge traceability and knowledge correctability by
leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible
plug-and-play framework for different LLMs, KGs and prompting strategies
without any additional training cost; 4) the performance of ToG with small LLM
models could exceed large LLM such as GPT-4 in certain scenarios and this
reduces the cost of LLM deployment and application. As a training-free method
with lower computational cost and better generality, ToG achieves overall SOTA
in 6 out of 9 datasets where most previous SOTAs rely on additional training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07870">Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1">Grgur Kova&#x10d;</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1">Masataka Sawayama</a>, <a href="http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1">R&#xe9;my Portelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1">C&#xe9;dric Colas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1">Peter Ford Dominey</a>, <a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1">Pierre-Yves Oudeyer</a></p>
<p>Large Language Models (LLMs) are often misleadingly recognized as having a
personality or a set of values. We argue that an LLM can be seen as a
superposition of perspectives with different values and personality traits.
LLMs exhibit context-dependent values and personality traits that change based
on the induced perspective (as opposed to humans, who tend to have more
coherent values and personality traits across contexts). We introduce the
concept of perspective controllability, which refers to a model's affordance to
adopt various perspectives with differing values and personality traits. In our
experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study
how exhibited values and personality traits change based on different
perspectives. Through qualitative experiments, we show that LLMs express
different values when those are (implicitly or explicitly) implied in the
prompt, and that LLMs express different values even when those are not
obviously implied (demonstrating their context-dependent nature). We then
conduct quantitative experiments to study the controllability of different
models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the
effectiveness of various methods for inducing perspectives, and the smoothness
of the models' drivability. We conclude by examining the broader implications
of our work and outline a variety of associated scientific questions. The
project website is available at
https://sites.google.com/view/llm-superpositions .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14132">Detecting Language Model Attacks with Perplexity. (arXiv:2308.14132v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1">Gabriel Alon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1">Michael Kamfonas</a></p>
<p>A novel hack involving Large Language Models (LLMs) has emerged, exploiting
adversarial suffixes to deceive models into generating perilous responses. Such
jailbreaks can trick LLMs into providing intricate instructions to a malicious
user for creating explosives, orchestrating a bank heist, or facilitating the
creation of offensive content. By evaluating the perplexity of queries with
adversarial suffixes using an open-source LLM (GPT-2), we found that they have
exceedingly high perplexity values. As we explored a broad range of regular
(non-adversarial) prompt varieties, we concluded that false positives are a
significant challenge for plain perplexity filtering. A Light-GBM trained on
perplexity and token length resolved the false positives and correctly detected
most adversarial attacks in the test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06827">Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1">Erik Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1">Hamid Palangi</a>, <a href="http://arxiv.org/find/cs/1/au:+Simoes_C/0/1/0/all/0/1">Clarisse Sim&#xf5;es</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1">Varun Chandrasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Subhabrata Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Arindam Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed Awadallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1">Ece Kamar</a></p>
<p>Large language models (LLMs) frequently hallucinate on abstractive
summarization tasks such as document-based question-answering, meeting
summarization, and clinical report generation, even though all necessary
information is included in context. However, optimizing LLMs to hallucinate
less on these tasks is challenging, as hallucination is hard to efficiently
evaluate at each optimization step. In this work, we show that reducing
hallucination on a synthetic task can also reduce hallucination on real-world
downstream tasks. Our method, SynTra, first designs a synthetic task where
hallucinations are easy to elicit and measure. It next optimizes the LLM's
system message via prefix-tuning on the synthetic task, and finally transfers
the system message to realistic, hard-to-optimize tasks. Across three realistic
abstractive summarization tasks, SynTra reduces hallucination for two
13B-parameter LLMs using only a synthetic retrieval task for supervision. We
also find that optimizing the system message rather than the model weights can
be critical; fine-tuning the entire model on the synthetic task can
counterintuitively increase hallucination. Overall, SynTra demonstrates that
the extra flexibility of working with synthetic data can help mitigate
undesired behaviors in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12798">MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sihang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yanchen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1">Hao Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1">Tat-Seng Chua</a></p>
<p>Language Models (LMs) have demonstrated impressive molecule understanding
ability on various 1D text-related tasks. However, they inherently lack 2D
graph perception - a critical ability of human professionals in comprehending
molecules' topological structures. To bridge this gap, we propose MolCA:
Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal
Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and
graph-based molecular contents via the cross-modal projector. Specifically, the
cross-modal projector is implemented as a Q-Former to connect a graph encoder's
representation space and an LM's text space. Further, MolCA employs a uni-modal
adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks.
Unlike previous studies that couple an LM with a graph encoder via cross-modal
contrastive learning, MolCA retains the LM's ability of open-ended text
generation and augments it with 2D graph information. To showcase its
effectiveness, we extensively benchmark MolCA on tasks of molecule captioning,
IUPAC name prediction, and molecule-text retrieval, on which MolCA
significantly outperforms the baselines. Our codes and checkpoints can be found
at https://github.com/acharkq/MolCA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18581">Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE. (arXiv:2310.18581v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1">Neeraj Varshney</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1">Agneet Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1">Mihir Parmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1">Chitta Baral</a></p>
<p>Large Language Models (LLMs) have achieved remarkable performance across a
wide variety of natural language tasks; however, their large size makes their
inference slow and computationally expensive. Focusing on this problem, we
propose to instruction tune LLMs with additional explicit losses from the
intermediate layers (LITE) and show that it enables these layers to acquire
'good' generation ability without affecting the generation ability of the final
layer. We perform 'dynamic confidence-based early exiting' at token level from
the intermediate layers which improves the efficiency of text generation
without compromising the quality of the generation. We conduct comprehensive
experiments by instruction tuning LLaMA-2 models on the Alpaca dataset and
holistically evaluate on four different human-instruction test sets. We show
that dynamic early exiting achieves consistent and considerable inference
computation cost improvements (37.86% for 7B and 46.35% for 13B model) while
maintaining the generation quality of the responses. We further conduct a
thorough analysis of the results over several important aspects, such as
comparing the semantic similarity of the outputs and dissecting the efficiency
improvements by comparing the number of tokens generated in the output. In
summary, our work contributes to improving the efficiency of LLM inference
while maintaining the generation quality, a crucial step en route to enabling
their widespread adoption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20246">Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Nuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zinan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1">Ning Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1">Ming Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a></p>
<p>Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01305">AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haixiao Xu</a></p>
<p>Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01767">PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yiduo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zekai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yaobo Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dongyan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a></p>
<p>Recent evaluations of Large Language Models (LLMs) have centered around
testing their zero-shot/few-shot capabilities for basic natural language tasks
and their ability to translate instructions into tool APIs. However, the
evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal
instructions in a complex multi-modal environment has not been investigated. To
address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark
to assess LLMs' ability to create and edit PPT files based on user
instructions. It contains 279 multi-turn sessions covering diverse topics and
hundreds of instructions involving multi-modal operations. We also propose the
PPTX-Match Evaluation System that evaluates if LLMs finish the instruction
based on the prediction file rather than the label API sequence, thus it
supports various LLM-generated API sequences. We measure 3 closed LLMs and 6
open-source LLMs. The results show that GPT-4 outperforms other LLMs with
75.1\% accuracy in single-turn dialogue testing but faces challenges in
completing entire sessions, achieving just 6\% session accuracy. We find three
main error causes in our benchmark: error accumulation in the multi-turn
session, long PPT template processing, and multi-modality perception. These
pose great challenges for future LLM and agent systems. We release the data,
code, and evaluation system of PPTC at \url{https://github.com/gydpku/PPTC}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02849">Co-training and Co-distillation for Quality Improvement and Compression of Language Models. (arXiv:2311.02849v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hayeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jongpil Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Davis Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_A/0/1/0/all/0/1">Alexander Min</a></p>
<p>Knowledge Distillation (KD) compresses computationally expensive pre-trained
language models (PLMs) by transferring their knowledge to smaller models,
allowing their use in resource-constrained or real-time settings. However, most
smaller models fail to surpass the performance of the original larger model,
resulting in sacrificing performance to improve inference speed. To address
this issue, we propose Co-Training and Co-Distillation (CTCD), a novel
framework that improves performance and inference speed together by co-training
two models while mutually distilling knowledge. The CTCD framework successfully
achieves this based on two significant findings: 1) Distilling knowledge from
the smaller model to the larger model during co-training improves the
performance of the larger model. 2) The enhanced performance of the larger
model further boosts the performance of the smaller model. The CTCD framework
shows promise as it can be combined with existing techniques like architecture
design or data augmentation, replacing one-way KD methods, to achieve further
performance improvement. Extensive ablation studies demonstrate the
effectiveness of CTCD, and the small model distilled by CTCD outperforms the
original larger model by a significant margin of 1.66 on the GLUE benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03287">Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges. (arXiv:2311.03287v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Chenhang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shirley Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Linjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a></p>
<p>While GPT-4V(ision) impressively models both visual and textual information
simultaneously, it's hallucination behavior has not been systematically
assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias
and Interference Challenges in Visual Language Models (Bingo). This benchmark
is designed to evaluate and shed light on the two common types of
hallucinations in visual language models: bias and interference. Here, bias
refers to the model's tendency to hallucinate certain types of responses,
possibly due to imbalance in its training data. Interference pertains to
scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the
text prompt is phrased or how the input image is presented. We identify a
notable regional bias, whereby GPT-4V(ision) is better at interpreting Western
images or images with English writing compared to images from other countries
or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to
leading questions and is often confused when interpreting multiple images
together. Popular mitigation approaches, such as self-correction and
chain-of-thought reasoning, are not effective in resolving these challenges. We
also identified similar biases and interference vulnerabilities with LLaVA and
Bard. Our results characterize the hallucination challenges in GPT-4V(ision)
and state-of-the-art visual-language models, and highlight the need for new
solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.
</p>
</p>
</div>

    </div>
    </body>
    