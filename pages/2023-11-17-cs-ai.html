<!DOCTYPE html>
<html>
<head>
<title>2023-11-17-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.08410">Exploration of the Assessment for AVP Algorithm Training in Underground Parking Garages Simulation Scenario. (arXiv:2311.08410v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenjin Li</a></p>
<p>The autonomous valet parking (AVP) functionality in self-driving vehicles is
currently capable of handling most simple parking tasks. However, further
training is necessary to enable the AVP algorithm to adapt to complex scenarios
and complete parking tasks in any given situation. Training algorithms with
real-world data is time-consuming and labour-intensive, and the current state
of constructing simulation environments is predominantly manual. This paper
introduces an approach to automatically generate 3D underground garage
simulation scenarios of varying difficulty levels based on pre-input 2D
underground parking structure plans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08412">Exploring Large Language Models as a Source of Common-Sense Knowledge for Robots. (arXiv:2311.08412v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ocker_F/0/1/0/all/0/1">Felix Ocker</a>, <a href="http://arxiv.org/find/cs/1/au:+Deigmoller_J/0/1/0/all/0/1">J&#xf6;rg Deigm&#xf6;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Eggert_J/0/1/0/all/0/1">Julian Eggert</a></p>
<p>Service robots need common-sense knowledge to help humans in everyday
situations as it enables them to understand the context of their actions.
However, approaches that use ontologies face a challenge because common-sense
knowledge is often implicit, i.e., it is obvious to humans but not explicitly
stated. This paper investigates if Large Language Models (LLMs) can fill this
gap. Our experiments reveal limited effectiveness in the selective extraction
of contextual action knowledge, suggesting that LLMs may not be sufficient on
their own. However, the large-scale extraction of general, actionable knowledge
shows potential, indicating that LLMs can be a suitable tool for efficiently
creating ontologies for robots. This paper shows that the technique used for
knowledge extraction can be applied to populate a minimalist ontology,
showcasing the potential of LLMs in synergy with formal knowledge
representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08427">Towards a Transportable Causal Network Model Based on Observational Healthcare Data. (arXiv:2311.08427v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bernasconi_A/0/1/0/all/0/1">Alice Bernasconi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanga_A/0/1/0/all/0/1">Alessio Zanga</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucas_P/0/1/0/all/0/1">Peter J.F. Lucas</a>, <a href="http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1">Marco Scutari Fabio Stella</a></p>
<p>Over the last decades, many prognostic models based on artificial
intelligence techniques have been used to provide detailed predictions in
healthcare. Unfortunately, the real-world observational data used to train and
validate these models are almost always affected by biases that can strongly
impact the outcomes validity: two examples are values missing not-at-random and
selection bias. Addressing them is a key element in achieving transportability
and in studying the causal relationships that are critical in clinical decision
making, going beyond simpler statistical approaches based on probabilistic
association.
</p>
<p>In this context, we propose a novel approach that combines selection
diagrams, missingness graphs, causal discovery and prior knowledge into a
single graphical model to estimate the cardiovascular risk of adolescent and
young females who survived breast cancer. We learn this model from data
comprising two different cohorts of patients. The resulting causal network
model is validated by expert clinicians in terms of risk assessment, accuracy
and explainability, and provides a prognostic model that outperforms competing
machine learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08430">Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale. (arXiv:2311.08430v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wei Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kuang-Hung Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorov_I/0/1/0/all/0/1">Igor Fedorov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1">Weiwei Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1">Kaveh Hassani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Mengying Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Lin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Buyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">Dehua Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhengxing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Guang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1">Fangqiu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiyan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1">Yuchen Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Liang Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen-Yen Chen</a></p>
<p>Neural Architecture Search (NAS) has demonstrated its efficacy in computer
vision and potential for ranking systems. However, prior work focused on
academic problems, which are evaluated at small scale under well-controlled
fixed baselines. In industry system, such as ranking system in Meta, it is
unclear whether NAS algorithms from the literature can outperform production
baselines because of: (1) scale - Meta ranking systems serve billions of users,
(2) strong baselines - the baselines are production models optimized by
hundreds to thousands of world-class engineers for years since the rise of deep
learning, (3) dynamic baselines - engineers may have established new and
stronger baselines during NAS search, and (4) efficiency - the search pipeline
must yield results quickly in alignment with the productionization life cycle.
In this paper, we present Rankitect, a NAS software framework for ranking
systems at Meta. Rankitect seeks to build brand new architectures by composing
low level building blocks from scratch. Rankitect implements and improves
state-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under
the same search space, including sampling-based NAS, one-shot NAS, and
Differentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple
production ranking models at Meta. We find that Rankitect can discover new
models from scratch achieving competitive tradeoff between Normalized Entropy
loss and FLOPs. When utilizing search space designed by engineers, Rankitect
can generate better models than engineers, achieving positive offline
evaluation and online A/B test at Meta scale.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08434">Uplift Modeling based on Graph Neural Network Combined with Causal Knowledge. (arXiv:2311.08434v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xinyan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yangze Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Longhan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a></p>
<p>Uplift modeling is a fundamental component of marketing effect modeling,
which is commonly employed to evaluate the effects of treatments on outcomes.
Through uplift modeling, we can identify the treatment with the greatest
benefit. On the other side, we can identify clients who are likely to make
favorable decisions in response to a certain treatment. In the past, uplift
modeling approaches relied heavily on the difference-in-difference (DID)
architecture, paired with a machine learning model as the estimation learner,
while neglecting the link and confidential information between features. We
proposed a framework based on graph neural networks that combine causal
knowledge with an estimate of uplift value. Firstly, we presented a causal
representation technique based on CATE (conditional average treatment effect)
estimation and adjacency matrix structure learning. Secondly, we suggested a
more scalable uplift modeling framework based on graph convolution networks for
combining causal knowledge. Our findings demonstrate that this method works
effectively for predicting uplift values, with small errors in typical
simulated data, and its effectiveness has been verified in actual industry
marketing data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08460">Surrogate Modeling for Computationally Expensive Simulations of Supernovae in High-Resolution Galaxy Simulations. (arXiv:2311.08460v1 [astro-ph.GA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Hirashima_K/0/1/0/all/0/1">Keiya Hirashima</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Moriwaki_K/0/1/0/all/0/1">Kana Moriwaki</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Fujii_M/0/1/0/all/0/1">Michiko S. Fujii</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hirai_Y/0/1/0/all/0/1">Yutaka Hirai</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Saitoh_T/0/1/0/all/0/1">Takayuki R. Saitoh</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Makino_J/0/1/0/all/0/1">Junichiro Makino</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1">Shirley Ho</a></p>
<p>Some stars are known to explode at the end of their lives, called supernovae
(SNe). The substantial amount of matter and energy that SNe release provides
significant feedback to star formation and gas dynamics in a galaxy. SNe
release a substantial amount of matter and energy to the interstellar medium,
resulting in significant feedback to star formation and gas dynamics in a
galaxy. While such feedback has a crucial role in galaxy formation and
evolution, in simulations of galaxy formation, it has only been implemented
using simple {\it sub-grid models} instead of numerically solving the evolution
of gas elements around SNe in detail due to a lack of resolution. We develop a
method combining machine learning and Gibbs sampling to predict how a supernova
(SN) affects the surrounding gas. The fidelity of our model in the thermal
energy and momentum distribution outperforms the low-resolution SN simulations.
Our method can replace the SN sub-grid models and help properly simulate
un-resolved SN feedback in galaxy formation simulations. We find that employing
our new approach reduces the necessary computational cost to $\sim$ 1 percent
compared to directly resolving SN feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08487">Alignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective. (arXiv:2311.08487v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wei Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jia Liu</a></p>
<p>Large Language Models (LLMs) are central to a multitude of applications but
struggle with significant risks, notably in generating harmful content and
biases. Drawing an analogy to the human psyche's conflict between evolutionary
survival instincts and societal norm adherence elucidated in Freud's
psychoanalysis theory, we argue that LLMs suffer a similar fundamental
conflict, arising between their inherent desire for syntactic and semantic
continuity, established during the pre-training phase, and the post-training
alignment with human values. This conflict renders LLMs vulnerable to
adversarial attacks, wherein intensifying the models' desire for continuity can
circumvent alignment efforts, resulting in the generation of harmful
information. Through a series of experiments, we first validated the existence
of the desire for continuity in LLMs, and further devised a straightforward yet
powerful technique, such as incomplete sentences, negative priming, and
cognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle
to prevent the generation of harmful information. In summary, our study
uncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby
questioning the efficacy of solely relying on sophisticated alignment methods,
and further advocates for a new training idea that integrates modal concepts
alongside traditional amodal concepts, aiming to endow LLMs with a more nuanced
understanding of real-world contexts and ethical considerations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08516">LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tyen_G/0/1/0/all/0/1">Gladys Tyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansoor_H/0/1/0/all/0/1">Hassan Mansoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Peter Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mak_T/0/1/0/all/0/1">Tony Mak</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbune_V/0/1/0/all/0/1">Victor C&#x103;rbune</a></p>
<p>While self-correction has shown promise in improving LLM outputs in terms of
style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent
attempts to self-correct logical or reasoning errors often cause correct
answers to become incorrect, resulting in worse performances overall (Huang et
al., 2023). In this paper, we break down the self-correction process into two
core components: mistake finding and output correction. For mistake finding, we
release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought
reasoning traces. We provide benchmark numbers for several state-of-the-art
LLMs, and demonstrate that LLMs generally struggle with finding logical
mistakes. For output correction, we propose a backtracking method which
provides large improvements when given information on mistake location. We
construe backtracking as a lightweight alternative to reinforcement learning
methods, and show that it remains effective with a reward model at 60-70%
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08525">Efficient Rotation Invariance in Deep Neural Networks through Artificial Mental Rotation. (arXiv:2311.08525v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tuggener_L/0/1/0/all/0/1">Lukas Tuggener</a>, <a href="http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1">Thilo Stadelmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1">J&#xfc;rgen Schmidhuber</a></p>
<p>Humans and animals recognize objects irrespective of the beholder's point of
view, which may drastically change their appearances. Artificial pattern
recognizers also strive to achieve this, e.g., through translational invariance
in convolutional neural networks (CNNs). However, both CNNs and vision
transformers (ViTs) perform very poorly on rotated inputs. Here we present
artificial mental rotation (AMR), a novel deep learning paradigm for dealing
with in-plane rotations inspired by the neuro-psychological concept of mental
rotation. Our simple AMR implementation works with all common CNN and ViT
architectures. We test it on ImageNet, Stanford Cars, and Oxford Pet. With a
top-1 error (averaged across datasets and architectures) of $0.743$, AMR
outperforms the current state of the art (rotational data augmentation, average
top-1 error of $0.626$) by $19\%$. We also easily transfer a trained AMR module
to a downstream task to improve the performance of a pre-trained semantic
segmentation model on rotated CoCo from $32.7$ to $55.2$ IoU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08526">GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer. (arXiv:2311.08526v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1">Urchade Zaratiana</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1">Nadi Tomeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1">Pierre Holat</a>, <a href="http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1">Thierry Charnois</a></p>
<p>Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08543">2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection. (arXiv:2311.08543v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jiarui Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Said_K/0/1/0/all/0/1">Karim Said</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_L/0/1/0/all/0/1">Lizhong Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1">Lingjia Liu</a></p>
<p>Orthogonal time frequency space (OTFS) is a promising modulation scheme for
wireless communication in high-mobility scenarios. Recently, a reservoir
computing (RC) based approach has been introduced for online subframe-based
symbol detection in the OTFS system, where only a limited number of
over-the-air (OTA) pilot symbols are utilized for training. However, this
approach does not leverage the domain knowledge specific to the OTFS system.
This paper introduces a novel two-dimensional RC (2D-RC) method that
incorporates the structural knowledge of the OTFS system into the design for
online symbol detection on a subframe basis. Specifically, as the channel
response acts as a two-dimensional (2D) operation over the transmitted
information symbols in the delay-Doppler (DD) domain, the 2D-RC is designed to
have a 2D structure to equalize the channel. With the introduced architecture,
the 2D-RC can benefit from the predictable channel representation in the DD
domain. Moreover, unlike the previous work that requires multiple RCs to learn
the channel feature, the 2D-RC only requires a single neural network for
detection. Experimental results demonstrate the effectiveness of the 2D-RC
approach across different OTFS system variants and modulation orders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08547">DeepThought: An Architecture for Autonomous Self-motivated Systems. (arXiv:2311.08547v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1">Arlindo L. Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Domingos_T/0/1/0/all/0/1">Tiago Domingos</a>, <a href="http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1">M&#xe1;rio Figueiredo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lima_P/0/1/0/all/0/1">Pedro U. Lima</a></p>
<p>The ability of large language models (LLMs) to engage in credible dialogues
with humans, taking into account the training data and the context of the
conversation, has raised discussions about their ability to exhibit intrinsic
motivations, agency, or even some degree of consciousness. We argue that the
internal architecture of LLMs and their finite and volatile state cannot
support any of these properties. By combining insights from complementary
learning systems, global neuronal workspace, and attention schema theories, we
propose to integrate LLMs and other deep learning systems into an architecture
for cognitive language agents able to exhibit properties akin to agency,
self-motivation, even some features of meta-cognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08557">Low-light Pedestrian Detection in Visible and Infrared Image Feeds: Issues and Challenges. (arXiv:2311.08557v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vachhani_H/0/1/0/all/0/1">Hrishikesh Vachhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Akilan_T/0/1/0/all/0/1">Thangarajah Akilan</a>, <a href="http://arxiv.org/find/cs/1/au:+Devmurari_Y/0/1/0/all/0/1">Yash Devmurari</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaik_N/0/1/0/all/0/1">Nisharaff Shaik</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1">Dhruvisha Patel</a></p>
<p>Pedestrian detection has become a cornerstone for several high-level tasks,
including autonomous driving, intelligent transportation, and traffic
surveillance. There are several works focussed on pedestrian detection using
visible images, mainly in the daytime. However, this task is very intriguing
when the environmental conditions change to poor lighting or nighttime.
Recently, new ideas have been spurred to use alternative sources, such as Far
InfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light
conditions. This study comprehensively reviews recent developments in low-light
pedestrian detection approaches. It systematically categorizes and analyses
various algorithms from region-based to non-region-based and graph-based
learning methodologies by highlighting their methodologies, implementation
issues, and challenges. It also outlines the key benchmark datasets that can be
used for research and development of advanced pedestrian detection algorithms,
particularly in low-light situations
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08558">Probabilistic reconstruction of Dark Matter fields from biased tracers using diffusion models. (arXiv:2311.08558v1 [astro-ph.CO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Park_C/0/1/0/all/0/1">Core Francisco Park</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ono_V/0/1/0/all/0/1">Victoria Ono</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Mudur_N/0/1/0/all/0/1">Nayantara Mudur</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ni_Y/0/1/0/all/0/1">Yueying Ni</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Cuesta_Lazaro_C/0/1/0/all/0/1">Carolina Cuesta-Lazaro</a></p>
<p>Galaxies are biased tracers of the underlying cosmic web, which is dominated
by dark matter components that cannot be directly observed. The relationship
between dark matter density fields and galaxy distributions can be sensitive to
assumptions in cosmology and astrophysical processes embedded in the galaxy
formation models, that remain uncertain in many aspects. Based on
state-of-the-art galaxy formation simulation suites with varied cosmological
parameters and sub-grid astrophysics, we develop a diffusion generative model
to predict the unbiased posterior distribution of the underlying dark matter
fields from the given stellar mass fields, while being able to marginalize over
the uncertainties in cosmology and galaxy formation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08568">Adversarial Imitation Learning On Aggregated Data. (arXiv:2311.08568v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Woillemont_P/0/1/0/all/0/1">Pierre Le Pelletier de Woillemont</a>, <a href="http://arxiv.org/find/cs/1/au:+Labory_R/0/1/0/all/0/1">R&#xe9;mi Labory</a>, <a href="http://arxiv.org/find/cs/1/au:+Corruble_V/0/1/0/all/0/1">Vincent Corruble</a></p>
<p>Inverse Reinforcement Learning (IRL) learns an optimal policy, given some
expert demonstrations, thus avoiding the need for the tedious process of
specifying a suitable reward function. However, current methods are constrained
by at least one of the following requirements. The first one is the need to
fully solve a forward Reinforcement Learning (RL) problem in the inner loop of
the algorithm, which might be prohibitively expensive in many complex
environments. The second one is the need for full trajectories from the
experts, which might not be easily available. The third one is the assumption
that the expert data is homogeneous rather than a collection from various
experts or possibly alternative solutions to the same task. Such constraints
make IRL approaches either not scalable or not usable on certain existing
systems. In this work we propose an approach which removes these requirements
through a dynamic, adaptive method called Adversarial Imitation Learning on
Aggregated Data (AILAD). It learns conjointly both a non linear reward function
and the associated optimal policy using an adversarial framework. The reward
learner only uses aggregated data. Moreover, it generates diverse behaviors
producing a distribution over the aggregated data matching that of the experts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08572">Parameter-Efficient Multilingual Summarisation: An Empirical Study. (arXiv:2311.08572v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1">Chenxi Whitehouse</a>, <a href="http://arxiv.org/find/cs/1/au:+Huot_F/0/1/0/all/0/1">Fantine Huot</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1">Jasmijn Bastings</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mostafa Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chu-Cheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1">Mirella Lapata</a></p>
<p>With the increasing prevalence of Large Language Models, traditional full
fine-tuning approaches face growing challenges, especially in memory-intensive
tasks. This paper investigates the potential of Parameter-Efficient
Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), for complex and
under-explored multilingual summarisation tasks. We conduct an extensive study
across different data availability scenarios, including full-data, low-data,
and cross-lingual transfer, leveraging models of different sizes. Our findings
reveal that LoRA lags behind full fine-tuning when trained with full data,
however, it excels in low-data scenarios and cross-lingual transfer.
Interestingly, as models scale up, the performance gap between LoRA and full
fine-tuning diminishes. Additionally, we investigate effective strategies for
few-shot cross-lingual transfer, finding that continued LoRA tuning achieves
the best performance compared to both full fine-tuning and dynamic composition
of language-specific LoRA modules.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08576">Towards Evaluating AI Systems for Moral Status Using Self-Reports. (arXiv:2311.08576v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1">Ethan Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1">Robert Long</a></p>
<p>As AI systems become more advanced and widely deployed, there will likely be
increasing debate over whether AI systems could have conscious experiences,
desires, or other states of potential moral significance. It is important to
inform these discussions with empirical evidence to the extent possible. We
argue that under the right circumstances, self-reports, or an AI system's
statements about its own internal states, could provide an avenue for
investigating whether AI systems have states of moral significance.
Self-reports are the main way such states are assessed in humans ("Are you in
pain?"), but self-reports from current systems like large language models are
spurious for many reasons (e.g. often just reflecting what humans would say).
To make self-reports more appropriate for this purpose, we propose to train
models to answer many kinds of questions about themselves with known answers,
while avoiding or limiting training incentives that bias self-reports. The hope
of this approach is that models will develop introspection-like capabilities,
and that these capabilities will generalize to questions about states of moral
significance. We then propose methods for assessing the extent to which these
techniques have succeeded: evaluating self-report consistency across contexts
and between similar models, measuring the confidence and resilience of models'
self-reports, and using interpretability to corroborate self-reports. We also
discuss challenges for our approach, from philosophical difficulties in
interpreting self-reports to technical reasons why our proposal might fail. We
hope our discussion inspires philosophers and AI researchers to criticize and
improve our proposed methodology, as well as to run experiments to test whether
self-reports can be made reliable enough to provide information about states of
moral significance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08577">Finding AI-Generated Faces in the Wild. (arXiv:2311.08577v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Porcile_G/0/1/0/all/0/1">Gonzalo J. Aniano Porcile</a>, <a href="http://arxiv.org/find/cs/1/au:+Gindi_J/0/1/0/all/0/1">Jack Gindi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mundra_S/0/1/0/all/0/1">Shivansh Mundra</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbus_J/0/1/0/all/0/1">James R. Verbus</a>, <a href="http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1">Hany Farid</a></p>
<p>AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08588">CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. (arXiv:2311.08588v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1">Weixiang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haitian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunkun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tingyu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weishan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Li Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shuiguang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaram_H/0/1/0/all/0/1">Hari Sundaram</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance on
coding related tasks, particularly on assisting humans in programming and
facilitating programming automation. However, existing benchmarks for
evaluating the code understanding and generation capacities of LLMs suffer from
severe limitations. First, most benchmarks are deficient as they focus on a
narrow range of popular programming languages and specific tasks, whereas the
real-world software development scenarios show dire need to implement systems
with multilingual programming environments to satisfy diverse requirements.
Practical programming practices also strongly expect multi-task settings for
testing coding capabilities of LLMs comprehensively and robustly. Second, most
benchmarks also fail to consider the actual executability and the consistency
of execution results of the generated code. To bridge these gaps between
existing benchmarks and expectations from practical applications, we introduce
CodeScope, an execution-based, multilingual, multi-task, multi-dimensional
evaluation benchmark for comprehensively gauging LLM capabilities on coding
tasks. CodeScope covers 43 programming languages and 8 coding tasks. It
evaluates the coding performance of LLMs from three dimensions (perspectives):
difficulty, efficiency, and length. To facilitate execution-based evaluations
of code generation, we develop MultiCodeEngine, an automated code execution
engine that supports 14 programming languages. Finally, we systematically
evaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the
superior breadth and challenges of CodeScope for evaluating LLMs on code
understanding and generation tasks compared to other benchmarks. The CodeScope
benchmark and datasets are publicly available at
https://github.com/WeixiangYAN/CodeScope.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08592">AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications. (arXiv:2311.08592v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Radharapu_B/0/1/0/all/0/1">Bhaktipriya Radharapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1">Kevin Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1">Lora Aroyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahoti_P/0/1/0/all/0/1">Preethi Lahoti</a></p>
<p>Adversarial testing of large language models (LLMs) is crucial for their safe
and responsible deployment. We introduce a novel approach for automated
generation of adversarial evaluation datasets to test the safety of LLM
generations on new downstream applications. We call it AI-assisted Red-Teaming
(AART) - an automated alternative to current manual red-teaming efforts. AART
offers a data generation and augmentation pipeline of reusable and customizable
recipes that reduce human effort significantly and enable integration of
adversarial testing earlier in new product development. AART generates
evaluation datasets with high diversity of content characteristics critical for
effective adversarial testing (e.g. sensitive and harmful concepts, specific to
a wide range of cultural and geographic regions and application scenarios). The
data generation is steered by AI-assisted recipes to define, scope and
prioritize diversity within the application context. This feeds into a
structured LLM-generation process that scales up evaluation priorities.
Compared to some state-of-the-art tools, AART shows promising results in terms
of concept coverage and data quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08605">Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures. (arXiv:2311.08605v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jenny_D/0/1/0/all/0/1">David F. Jenny</a>, <a href="http://arxiv.org/find/cs/1/au:+Billeter_Y/0/1/0/all/0/1">Yann Billeter</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhijing Jin</a></p>
<p>The rapid advancement of Large Language Models (LLMs) has sparked intense
debate regarding their ability to perceive and interpret complex
socio-political landscapes. In this study, we undertake an exploration of
decision-making processes and inherent biases within LLMs, exemplified by
ChatGPT, specifically contextualizing our analysis within political debates. We
aim not to critique or validate LLMs' values, but rather to discern how they
interpret and adjudicate "good arguments." By applying Activity Dependency
Networks (ADNs), we extract the LLMs' implicit criteria for such assessments
and illustrate how normative values influence these perceptions. We discuss the
consequences of our findings for human-AI alignment and bias mitigation. Our
code and data at https://github.com/david-jenny/LLM-Political-Study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08614">XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making. (arXiv:2311.08614v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zichen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaidhani_M/0/1/0/all/0/1">Mitali Gaidhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Ambuj Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sra_M/0/1/0/all/0/1">Misha Sra</a></p>
<p>Large Language Models (LLMs) have recently made impressive strides in natural
language understanding tasks. Despite their remarkable performance,
understanding their decision-making process remains a big challenge. In this
paper, we look into bringing some transparency to this process by introducing a
new explanation dataset for question answering (QA) tasks that integrates
knowledge graphs (KGs) in a novel way. Our dataset includes 12,102
question-answer-explanation (QAE) triples. Each explanation in the dataset
links the LLM's reasoning to entities and relations in the KGs. The explanation
component includes a why-choose explanation, a why-not-choose explanation, and
a set of reason-elements that underlie the LLM's decision. We leverage KGs and
graph attention networks (GAT) to find the reason-elements and transform them
into why-choose and why-not-choose explanations that are comprehensible to
humans. Through quantitative and qualitative evaluations, we demonstrate the
potential of our dataset to improve the in-context learning of LLMs, and
enhance their interpretability and explainability. Our work contributes to the
field of explainable AI by enabling a deeper understanding of the LLMs
decision-making process to make them more transparent and thereby, potentially
more reliable, to researchers and practitioners alike. Our dataset is available
at: https://github.com/chen-zichen/XplainLLM_dataset.git
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08635">Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction. (arXiv:2311.08635v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1">Guangyin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingbo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fuxian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jincai Huang</a></p>
<p>Traffic congestion event prediction is an important yet challenging task in
intelligent transportation systems. Many existing works about traffic
prediction integrate various temporal encoders and graph convolution networks
(GCNs), called spatio-temporal graph-based neural networks, which focus on
predicting dense variables such as flow, speed and demand in time snapshots,
but they can hardly forecast the traffic congestion events that are sparsely
distributed on the continuous time axis. In recent years, neural point process
(NPP) has emerged as an appropriate framework for event prediction in
continuous time scenarios. However, most conventional works about NPP cannot
model the complex spatio-temporal dependencies and congestion evolution
patterns. To address these limitations, we propose a spatio-temporal graph
neural point process framework, named STGNPP for traffic congestion event
prediction. Specifically, we first design the spatio-temporal graph learning
module to fully capture the long-range spatio-temporal dependencies from the
historical traffic state data along with the road network. The extracted
spatio-temporal hidden representation and congestion event information are then
fed into a continuous gated recurrent unit to model the congestion evolution
patterns. In particular, to fully exploit the periodic information, we also
improve the intensity function calculation of the point process with a periodic
gated mechanism. Finally, our model simultaneously predicts the occurrence time
and duration of the next congestion. Extensive experiments on two real-world
datasets demonstrate that our method achieves superior performance in
comparison to existing state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08644">Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations. (arXiv:2311.08644v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yiheng Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juni Jessy Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1">Matthew Lease</a></p>
<p>Can we preserve the accuracy of neural models while also providing faithful
explanations? We present wrapper boxes, a general approach to generate
faithful, example-based explanations for model predictions while maintaining
predictive performance. After training a neural model as usual, its learned
feature representation is input to a classic, interpretable model to perform
the actual prediction. This simple strategy is surprisingly effective, with
results largely comparable to those of the original neural model, as shown
across three large pre-trained language models, two datasets of varying scale,
four classic models, and four evaluation metrics. Moreover, because these
classic models are interpretable by design, the subset of training examples
that determine classic model predictions can be shown directly to users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08648">Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Paiheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bang An</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1">Wei Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a></p>
<p>Language models (LMs) have gained great achievement in various NLP tasks for
both fine-tuning and in-context learning (ICL) methods. Despite its outstanding
performance, evidence shows that spurious correlations caused by imbalanced
label distributions in training data (or exemplars in ICL) lead to robustness
issues. However, previous studies mostly focus on word- and phrase-level
features and fail to tackle it from the concept level, partly due to the lack
of concept labels and subtle and diverse expressions of concepts in text. In
this paper, we first use the LLM to label the concept for each text and then
measure the concept bias of models for fine-tuning or ICL on the test data.
Second, we propose a data rebalancing method to mitigate the spurious
correlations by adding the LLM-generated counterfactual data to make a balanced
label distribution for each concept. We verify the effectiveness of our
mitigation method and show its superiority over the token removal method.
Overall, our results show that there exist label distribution biases in
concepts across multiple text classification datasets, and LMs will utilize
these shortcuts to make predictions in both fine-tuning and ICL methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08649">Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing. (arXiv:2311.08649v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Juyeon Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldt_R/0/1/0/all/0/1">Robert Feldt</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1">Shin Yoo</a></p>
<p>GUI testing checks if a software system behaves as expected when users
interact with its graphical interface, e.g., testing specific functionality or
validating relevant use case scenarios. Currently, deciding what to test at
this high level is a manual task since automated GUI testing tools target lower
level adequacy metrics such as structural code coverage or activity coverage.
We propose DroidAgent, an autonomous GUI testing agent for Android, for
semantic, intent-driven automation of GUI testing. It is based on Large
Language Models and support mechanisms such as long- and short-term memory.
Given an Android app, DroidAgent sets relevant task goals and subsequently
tries to achieve them by interacting with the app. Our empirical evaluation of
DroidAgent using 15 apps from the Themis benchmark shows that it can set up and
perform realistic tasks, with a higher level of autonomy. For example, when
testing a messaging app, DroidAgent created a second account and added a first
account as a friend, testing a realistic use case, without human intervention.
On average, DroidAgent achieved 61% activity coverage, compared to 51% for
current state-of-the-art GUI testing techniques. Further, manual analysis shows
that 317 out of the 374 autonomously created tasks are realistic and relevant
to app functionalities, and also that DroidAgent interacts deeply with the apps
and covers more features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08662">Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets. (arXiv:2311.08662v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vatsal Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandya_P/0/1/0/all/0/1">Pranshu Pandya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kataria_T/0/1/0/all/0/1">Tushar Kataria</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vivek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a></p>
<p>Language models, given their black-box nature, often exhibit sensitivity to
input perturbations, leading to trust issues due to hallucinations. To bolster
trust, it's essential to understand these models' failure modes and devise
strategies to enhance their performance. In this study, we propose a framework
to study the effect of input perturbations on language models of different
scales, from pre-trained models to large language models (LLMs). We use
fine-tuning to train a robust model to perturbations, and we investigate
whether exposure to one perturbation improves or degrades the model's
performance on other perturbations. To address multi-perturbation robustness,
we suggest three distinct training strategies. We also extend the framework to
LLMs via a chain of thought(COT) prompting with exemplars. We instantiate our
framework for the Tabular-NLI task and show that the proposed strategies train
the model robust to different perturbations without losing accuracy on a given
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08685">Safer-Instruct: Aligning Language Models with Automated Preference Data. (arXiv:2311.08685v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1">Taiwei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jieyu Zhao</a></p>
<p>Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for
enhancing model safety in language models. However, annotating preference data
for RLHF is a resource-intensive and creativity-demanding process, while
automatic generation methods face limitations in data diversity and quality. In
response, we present Safer-Instruct, a novel pipeline for semi-automatically
constructing large-scale preference datasets. Our approach leverages reversed
instruction tuning, instruction induction, and expert model evaluation to
efficiently generate high-quality preference data without human annotators. We
evaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an
expert model, generating approximately 10K preference samples. Finetuning an
Alpaca model on this dataset demonstrates improved harmlessness while
maintaining competitive performance on conversation and downstream tasks.
Safer-Instruct addresses the challenges in preference data acquisition,
advancing the development of safer and more responsible AI systems. Our code
and data are available at https://github.com/uscnlp-lime/safer-instruct
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08687">An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping. (arXiv:2311.08687v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Harrigian_K/0/1/0/all/0/1">Keith Harrigian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1">Tina Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzales_A/0/1/0/all/0/1">Anthony Gonzales</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1">Cindy X. Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1">Mark Dredze</a></p>
<p>Diabetic eye disease is a major cause of blindness worldwide. The ability to
monitor relevant clinical trajectories and detect lapses in care is critical to
managing the disease and preventing blindness. Alas, much of the information
necessary to support these goals is found only in the free text of the
electronic medical record. To fill this information gap, we introduce a system
for extracting evidence from clinical text of 19 clinical concepts related to
diabetic eye disease and inferring relevant attributes for each. In developing
this ophthalmology phenotyping system, we are also afforded a unique
opportunity to evaluate the effectiveness of clinical language models at
adapting to new clinical domains. Across multiple training paradigms, we find
that BERT language models pretrained on out-of-distribution clinical data offer
no significant improvement over BERT language models pretrained on non-clinical
data for our domain. Our study tempers recent claims that language models
pretrained on clinical data are necessary for clinical NLP tasks and highlights
the importance of not treating clinical language data as a single homogeneous
domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08698">Artificial General Intelligence, Existential Risk, and Human Risk Perception. (arXiv:2311.08698v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mandel_D/0/1/0/all/0/1">David R. Mandel</a></p>
<p>Artificial general intelligence (AGI) does not yet exist, but given the pace
of technological development in artificial intelligence, it is projected to
reach human-level intelligence within roughly the next two decades. After that,
many experts expect it to far surpass human intelligence and to do so rapidly.
The prospect of superintelligent AGI poses an existential risk to humans
because there is no reliable method for ensuring that AGI goals stay aligned
with human goals. Drawing on publicly available forecaster and opinion data,
the author examines how experts and non-experts perceive risk from AGI. The
findings indicate that the perceived risk of a world catastrophe or extinction
from AGI is greater than for other existential risks. The increase in perceived
risk over the last year is also steeper for AGI than for other existential
threats (e.g., nuclear war or human-caused climate change). That AGI is a
pressing existential risk is something on which experts and non-experts agree,
but the basis for such agreement currently remains obscure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08702">Debate Helps Supervise Unreliable Experts. (arXiv:2311.08702v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1">Julian Michael</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdi_S/0/1/0/all/0/1">Salsabila Mahdi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rein_D/0/1/0/all/0/1">David Rein</a>, <a href="http://arxiv.org/find/cs/1/au:+Petty_J/0/1/0/all/0/1">Jackson Petty</a>, <a href="http://arxiv.org/find/cs/1/au:+Dirani_J/0/1/0/all/0/1">Julien Dirani</a>, <a href="http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1">Vishakh Padmakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1">Samuel R. Bowman</a></p>
<p>As AI systems are used to answer more difficult questions and potentially
help create new knowledge, judging the truthfulness of their outputs becomes
more difficult and more important. How can we supervise unreliable experts,
which have access to the truth but may not accurately report it, to give
answers that are systematically true and don't just superficially seem true,
when the supervisor can't tell the difference between the two on their own? In
this work, we show that debate between two unreliable experts can help a
non-expert judge more reliably identify the truth. We collect a dataset of
human-written debates on hard reading comprehension questions where the judge
has not read the source passage, only ever seeing expert arguments and short
quotes selectively revealed by 'expert' debaters who have access to the
passage. In our debates, one expert argues for the correct answer, and the
other for an incorrect answer. Comparing debate to a baseline we call
consultancy, where a single expert argues for only one answer which is correct
half of the time, we find that debate performs significantly better, with 84%
judge accuracy compared to consultancy's 74%. Debates are also more efficient,
being 68% of the length of consultancies. By comparing human to AI debaters, we
find evidence that with more skilled (in this case, human) debaters, the
performance of debate goes up but the performance of consultancy goes down. Our
error analysis also supports this trend, with 46% of errors in human debate
attributable to mistakes by the honest debater (which should go away with
increased skill); whereas 52% of errors in human consultancy are due to
debaters obfuscating the relevant evidence from the judge (which should become
worse with increased skill). Overall, these results show that debate is a
promising approach for supervising increasingly capable but potentially
unreliable AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08704">Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains. (arXiv:2311.08704v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fonseca_M/0/1/0/all/0/1">Marcio Fonseca</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a></p>
<p>Although large language models (LLMs) exhibit remarkable capacity to leverage
in-context demonstrations, it is still unclear to what extent they can learn
new concepts or facts from ground-truth labels. To address this question, we
examine the capacity of instruction-tuned LLMs to follow in-context concept
guidelines for sentence labeling tasks. We design guidelines that present
different types of factual and counterfactual concept definitions, which are
used as prompts for zero-shot sentence classification tasks. Our results show
that although concept definitions consistently help in task performance, only
the larger models (with 70B parameters or more) have limited ability to work
under counterfactual contexts. Importantly, only proprietary models such as
GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is
due to more sophisticated alignment methods. Finally, we find that
Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which
indicates that careful fine-tuning is more effective than increasing model
scale. Altogether, our simple evaluation method reveals significant gaps in
concept understanding between the most capable open-source language models and
the leading proprietary APIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08706">Aligned: A Platform-based Process for Alignment. (arXiv:2311.08706v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaotran_E/0/1/0/all/0/1">Ethan Shaotran</a>, <a href="http://arxiv.org/find/cs/1/au:+Pesok_I/0/1/0/all/0/1">Ido Pesok</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_S/0/1/0/all/0/1">Sam Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1">Emi Liu</a></p>
<p>We are introducing Aligned, a platform for global governance and alignment of
frontier models, and eventually superintelligence. While previous efforts at
the major AI labs have attempted to gather inputs for alignment, these are
often conducted behind closed doors. We aim to set the foundation for a more
trustworthy, public-facing approach to safety: a constitutional committee
framework. Initial tests with 680 participants result in a 30-guideline
constitution with 93% overall support. We show the platform naturally scales,
instilling confidence and enjoyment from the community. We invite other AI labs
and teams to plug and play into the Aligned ecosystem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08708">Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning. (arXiv:2311.08708v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">Yu Min Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Tun_Y/0/1/0/all/0/1">Yan Kyaw Tun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1">Choong Seon Hong</a></p>
<p>The development of 6G/B5G wireless networks, which have requirements that go
beyond current 5G networks, is gaining interest from academic and industrial.
However, to increase 6G/B5G network quality, conventional cellular networks
that rely on terrestrial base stations are constrained geographically and
economically. Meanwhile, NOMA allows multiple users to share the same
resources, which improves the spectral efficiency of the system and has the
advantage of supporting a larger number of users. Additionally, by
intelligently manipulating the phase and amplitude of both the reflected and
transmitted signals, STAR-RISs can achieve improved coverage, increased
spectral efficiency, and enhanced communication reliability. However, STAR-RISs
must simultaneously optimize the Amplitude and Phase-shift corresponding to
reflection and transmission, which makes the existing terrestiral networks more
complicated and is considered a major challenging issue. Motivated by the
above, we study the joint user pairing for NOMA and beamforming design of
Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization
problem with the objective of maximizing the total throughput of MUs by jointly
optimizing the decoding order, user pairing, active beamforming, and passive
beamforming. However, the formulated problem is a MINLP. To tackle this
challenge, we first introduce the decoding order for NOMA networks. Next, we
decompose the original problem into two subproblems namely: 1) MU pairing and
2) Beamforming optimization under the optimal decoding order. For the first
subproblem, we employ correlation-based K-means clustering to solve the user
pairing problem. Then, to jointly deal with beamforming vector optimizations,
we propose MAPPO, which can make quick decisions in the given environment owing
to its low complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08760">Forms of Understanding of XAI-Explanations. (arXiv:2311.08760v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buschmeier_H/0/1/0/all/0/1">Hendrik Buschmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Buhl_H/0/1/0/all/0/1">Heike M. Buhl</a>, <a href="http://arxiv.org/find/cs/1/au:+Kern_F/0/1/0/all/0/1">Friederike Kern</a>, <a href="http://arxiv.org/find/cs/1/au:+Grimminger_A/0/1/0/all/0/1">Angela Grimminger</a>, <a href="http://arxiv.org/find/cs/1/au:+Beierling_H/0/1/0/all/0/1">Helen Beierling</a>, <a href="http://arxiv.org/find/cs/1/au:+Fisher_J/0/1/0/all/0/1">Josephine Fisher</a>, <a href="http://arxiv.org/find/cs/1/au:+Gross_A/0/1/0/all/0/1">Andr&#xe9; Gro&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Horwath_I/0/1/0/all/0/1">Ilona Horwath</a>, <a href="http://arxiv.org/find/cs/1/au:+Klowait_N/0/1/0/all/0/1">Nils Klowait</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazarov_S/0/1/0/all/0/1">Stefan Lazarov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenke_M/0/1/0/all/0/1">Michael Lenke</a>, <a href="http://arxiv.org/find/cs/1/au:+Lohmer_V/0/1/0/all/0/1">Vivien Lohmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohlfing_K/0/1/0/all/0/1">Katharina Rohlfing</a>, <a href="http://arxiv.org/find/cs/1/au:+Scharlau_I/0/1/0/all/0/1">Ingrid Scharlau</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Amit Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Terfloth_L/0/1/0/all/0/1">Lutz Terfloth</a>, <a href="http://arxiv.org/find/cs/1/au:+Vollmer_A/0/1/0/all/0/1">Anna-Lisa Vollmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilmes_A/0/1/0/all/0/1">Annedore Wilmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Wrede_B/0/1/0/all/0/1">Britta Wrede</a></p>
<p>Explainability has become an important topic in computer science and
artificial intelligence, leading to a subfield called Explainable Artificial
Intelligence (XAI). The goal of providing or seeking explanations is to achieve
(better) 'understanding' on the part of the explainee. However, what it means
to 'understand' is still not clearly defined, and the concept itself is rarely
the subject of scientific investigation. This conceptual article aims to
present a model of forms of understanding in the context of XAI and beyond.
From an interdisciplinary perspective bringing together computer science,
linguistics, sociology, and psychology, a definition of understanding and its
forms, assessment, and dynamics during the process of giving everyday
explanations are explored. Two types of understanding are considered as
possible outcomes of explanations, namely enabledness, 'knowing how' to do or
decide something, and comprehension, 'knowing that' -- both in different
degrees (from shallow to deep). Explanations regularly start with shallow
understanding in a specific domain and can lead to deep comprehension and
enabledness of the explanandum, which we see as a prerequisite for human users
to gain agency. In this process, the increase of comprehension and enabledness
are highly interdependent. Against the background of this systematization,
special challenges of understanding in XAI are discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08764">Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning. (arXiv:2311.08764v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoshuang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhongyi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1">Ke Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Shouhong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongtao Lu</a></p>
<p>Class Incremental Learning (CIL) aims to handle the scenario where data of
novel classes occur continuously and sequentially. The model should recognize
the sequential novel classes while alleviating the catastrophic forgetting. In
the self-supervised manner, it becomes more challenging to avoid the conflict
between the feature embedding spaces of novel classes and old ones without any
class labels. To address the problem, we propose a self-supervised CIL
framework CPPF, meaning Combining Past, Present and Future. In detail, CPPF
consists of a prototype clustering module (PC), an embedding space reserving
module (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the
ESR modules reserve embedding space for subsequent phases at the prototype
level and the feature level respectively to prepare for knowledge learned in
the future. 2) The MTD module maintains the representations of the current
phase without the interference of past knowledge. One of the teacher networks
retains the representations of the past phases, and the other teacher network
distills relation information of the current phase to the student network.
Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our
proposed method boosts the performance of self-supervised class incremental
learning. We will release code in the near future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08768">Three Conjectures on Unexpectedeness. (arXiv:2311.08768v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sileno_G/0/1/0/all/0/1">Giovanni Sileno</a>, <a href="http://arxiv.org/find/cs/1/au:+Dessalles_J/0/1/0/all/0/1">Jean-Louis Dessalles</a></p>
<p>Unexpectedness is a central concept in Simplicity Theory, a theory of
cognition relating various inferential processes to the computation of
Kolmogorov complexities, rather than probabilities. Its predictive power has
been confirmed by several experiments with human subjects, yet its theoretical
basis remains largely unexplored: why does it work? This paper lays the
groundwork for three theoretical conjectures. First, unexpectedness can be seen
as a generalization of Bayes' rule. Second, the frequentist core of
unexpectedness can be connected to the function of tracking ergodic properties
of the world. Third, unexpectedness can be seen as constituent of various
measures of divergence between the entropy of the world (environment) and the
variety of the observer (system). The resulting framework hints to research
directions that go beyond the division between probabilistic and logical
approaches, potentially bringing new insights into the extraction of causal
relations, and into the role of descriptive mechanisms in learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08783">ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home. (arXiv:2311.08783v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Paster_S/0/1/0/all/0/1">Sveta Paster</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_K/0/1/0/all/0/1">Kantwon Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Briggs_G/0/1/0/all/0/1">Gordon Briggs</a>, <a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1">Peter Stone</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirsky_R/0/1/0/all/0/1">Reuth Mirsky</a></p>
<p>With the projected surge in the elderly population, service robots offer a
promising avenue to enhance their well-being in elderly care homes. Such robots
will encounter complex scenarios which will require them to perform decisions
with ethical consequences. In this report, we propose to leverage the
Intelligent Disobedience framework in order to give the robot the ability to
perform a deliberation process over decisions with potential ethical
implications. We list the issues that this framework can assist with, define it
formally in the context of the specific elderly care home scenario, and
delineate the requirements for implementing an intelligently disobeying robot.
We conclude this report with some critical analysis and suggestions for future
work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08788">X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects. (arXiv:2311.08788v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minqian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Ying Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhiyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1">Eunah Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vaibhav Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanadan_R/0/1/0/all/0/1">Reza Ghanadan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lifu Huang</a></p>
<p>Natural Language Generation (NLG) typically involves evaluating the generated
text in various aspects (e.g., consistency and naturalness) to obtain a
comprehensive assessment. However, multi-aspect evaluation remains challenging
as it may require the evaluator to generalize to any given evaluation aspect
even if it's absent during training. In this paper, we introduce X-Eval, a
two-stage instruction tuning framework to evaluate the text in both seen and
unseen aspects customized by end users. X-Eval consists of two learning stages:
the vanilla instruction tuning stage that improves the model's ability to
follow evaluation instructions, and an enhanced instruction tuning stage that
exploits the connections between fine-grained evaluation aspects to better
assess text quality. To support the training of X-Eval, we collect
AspectInstruct, the first instruction tuning dataset tailored for multi-aspect
NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance
task diversity, we devise an augmentation strategy that converts human rating
annotations into diverse forms of NLG evaluation tasks, including scoring,
comparison, ranking, and Boolean question answering. Extensive experiments
across three essential categories of NLG tasks: dialogue generation,
summarization, and data-to-text coupled with 21 aspects in meta-evaluation,
demonstrate that our X-Eval enables even a lightweight language model to
achieve a comparable if not higher correlation with human judgments compared to
the state-of-the-art NLG evaluators, such as GPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08806">SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer. (arXiv:2311.08806v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1">Shanlin Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhiyi Yu</a></p>
<p>As the third-generation neural network, the Spiking Neural Network (SNN) has
the advantages of low power consumption and high energy efficiency, making it
suitable for implementation on edge devices. More recently, the most advanced
SNN, Spikformer, combines the self-attention module from Transformer with SNN
to achieve remarkable performance. However, it adopts larger channel dimensions
in MLP layers, leading to an increased number of redundant model parameters. To
effectively decrease the computational complexity and weight parameters of the
model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very
sparse ($\ge$90%) subnetwork that achieves comparable performance to the
original network. Furthermore, we also design a lightweight token selector
module, which can remove unimportant background information from images based
on the average spike firing rate of neurons, selecting only essential
foreground image tokens to participate in attention calculation. Based on that,
we present SparseSpikformer, a co-design framework aimed at achieving sparsity
in Spikformer through token and weight pruning techniques. Experimental results
demonstrate that our framework can significantly reduce 90% model parameters
and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining
the accuracy of the original model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08815">Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations. (arXiv:2311.08815v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1">Cian Eastwood</a>, <a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1">Julius von K&#xfc;gelgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ericsson_L/0/1/0/all/0/1">Linus Ericsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1">Diane Bouchacourt</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1">Pascal Vincent</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Mark Ibrahim</a></p>
<p>Self-supervised representation learning often uses data augmentations to
induce some invariance to "style" attributes of the data. However, with
downstream tasks generally unknown at training time, it is difficult to deduce
a priori which attributes of the data are indeed "style" and can be safely
discarded. To address this, we introduce a more principled approach that seeks
to disentangle style features rather than discard them. The key idea is to add
multiple style embedding spaces where: (i) each is invariant to all-but-one
augmentation; and (ii) joint entropy is maximized. We formalize our structured
data-augmentation procedure from a causal latent-variable-model perspective,
and prove identifiability of both content and (multiple blocks of) style
variables. We empirically demonstrate the benefits of our approach on synthetic
datasets and then present promising but limited results on ImageNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08817">MAP&#x27;s not dead yet: Uncovering true language model modes by conditioning away degeneracy. (arXiv:2311.08817v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoshida_D/0/1/0/all/0/1">Davis Yoshida</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1">Kartik Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1">Kevin Gimpel</a></p>
<p>It has been widely observed that exact or approximate MAP (mode-seeking)
decoding from natural language generation (NLG) models consistently leads to
degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has
generally been attributed to either a fundamental inadequacy of modes in models
or weaknesses in language modeling. Contrastingly in this work, we emphasize
that degenerate modes can even occur in the absence of any model error, due to
contamination of the training data. Specifically, we show that mixing even a
tiny amount of low-entropy noise with a population text distribution can cause
the data distribution's mode to become degenerate, implying that any models
trained on it will be as well. As the unconditional mode of NLG models will
often be degenerate, we therefore propose to apply MAP decoding to the model's
distribution conditional on avoiding specific degeneracies. Using exact-search,
we empirically verify that the length-conditional modes of machine translation
models and language models are indeed more fluent and topical than their
unconditional modes. For the first time, we also share many examples of exact
modal sequences from these models, and from several variants of the LLaMA-7B
model. Notably, the modes of the LLaMA models are still degenerate, showing
that improvements in modeling have not fixed this issue. Because of the cost of
exact mode finding algorithms, we develop an approximate mode finding approach,
ACBS, which finds sequences that are both high-likelihood and high-quality. We
apply this approach to LLaMA-7B, a model which was not trained for instruction
following, and find that we are able to elicit reasonable outputs without any
finetuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08819">Frequency Domain-based Dataset Distillation. (arXiv:2311.08819v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1">Donghyeok Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Seungjae Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1">Il-Chul Moon</a></p>
<p>This paper presents FreD, a novel parameterization method for dataset
distillation, which utilizes the frequency domain to distill a small-sized
synthetic dataset from a large-sized original dataset. Unlike conventional
approaches that focus on the spatial domain, FreD employs frequency-based
transforms to optimize the frequency representations of each data instance. By
leveraging the concentration of spatial domain information on specific
frequency components, FreD intelligently selects a subset of frequency
dimensions for optimization, leading to a significant reduction in the required
budget for synthesizing an instance. Through the selection of frequency
dimensions based on the explained variance, FreD demonstrates both theoretical
and empirical evidence of its ability to operate efficiently within a limited
budget, while better preserving the information of the original dataset
compared to conventional parameterization methods. Furthermore, based on the
orthogonal compatibility of FreD with existing methods, we confirm that FreD
consistently improves the performances of existing distillation methods over
the evaluation scenarios with different benchmark datasets. We release the code
at https://github.com/sdh0818/FreD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08820">Reinforcement Learning with Model Predictive Control for Highway Ramp Metering. (arXiv:2311.08820v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Airaldi_F/0/1/0/all/0/1">Filippo Airaldi</a>, <a href="http://arxiv.org/find/eess/1/au:+Schutter_B/0/1/0/all/0/1">Bart De Schutter</a>, <a href="http://arxiv.org/find/eess/1/au:+Dabiri_A/0/1/0/all/0/1">Azita Dabiri</a></p>
<p>In the backdrop of an increasingly pressing need for effective urban and
highway transportation systems, this work explores the synergy between
model-based and learning-based strategies to enhance traffic flow management by
use of an innovative approach to the problem of highway ramp metering control
that embeds Reinforcement Learning techniques within the Model Predictive
Control framework. The control problem is formulated as an RL task by crafting
a suitable stage cost function that is representative of the traffic
conditions, variability in the control action, and violations of a
safety-critical constraint on the maximum number of vehicles in queue. An
MPC-based RL approach, which merges the advantages of the two paradigms in
order to overcome the shortcomings of each framework, is proposed to learn to
efficiently control an on-ramp and to satisfy its constraints despite
uncertainties in the system model and variable demands. Finally, simulations
are performed on a benchmark from the literature consisting of a small-scale
highway network. Results show that, starting from an MPC controller that has an
imprecise model and is poorly tuned, the proposed methodology is able to
effectively learn to improve the control policy such that congestion in the
network is reduced and constraints are satisfied, yielding an improved
performance compared to the initial controller.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08832">Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration. (arXiv:2311.08832v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadek_M/0/1/0/all/0/1">Malak Sadek</a>, <a href="http://arxiv.org/find/cs/1/au:+Mougenot_C/0/1/0/all/0/1">C&#xe9;line Mougenot</a></p>
<p>Recent years have seen a steady rise in the popularity and use of
Conversational Agents (CA) for different applications, well before the more
immediate impact of large language models. This rise has been accompanied by an
extensive exploration and documentation of the challenges of designing and
creating conversational agents. Focusing on a recent scoping review of the
socio-technical challenges of CA creation, this opinion paper calls for an
examination of the extent to which interdisciplinary collaboration (IDC)
challenges might contribute towards socio-technical CA design challenges. The
paper proposes a taxonomy of CA design challenges using IDC as a lens, and
proposes practical strategies to overcome them which complement existing design
principles. The paper invites future work to empirically verify suggested
conceptual links and apply the proposed strategies within the space of CA
design to evaluate their effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08834">A* search algorithm for an optimal investment problem in vehicle-sharing systems. (arXiv:2311.08834v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1">Ba Luat Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1">Layla Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Demir_E/0/1/0/all/0/1">Emrah Demir</a>, <a href="http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1">Duc Minh Vu</a></p>
<p>We study an optimal investment problem that arises in the context of the
vehicle-sharing system. Given a set of locations to build stations, we need to
determine i) the sequence of stations to be built and the number of vehicles to
acquire in order to obtain the target state where all stations are built, and
ii) the number of vehicles to acquire and their allocation in order to maximize
the total profit returned by operating the system when some or all stations are
open. The profitability associated with operating open stations, measured over
a specific time period, is represented as a linear optimization problem applied
to a collection of open stations. With operating capital, the owner of the
system can open new stations. This property introduces a set-dependent aspect
to the duration required for opening a new station, and the optimal investment
problem can be viewed as a variant of the Traveling Salesman Problem (TSP) with
set-dependent cost. We propose an A* search algorithm to address this
particular variant of the TSP. Computational experiments highlight the benefits
of the proposed algorithm in comparison to the widely recognized Dijkstra
algorithm and propose future research to explore new possibilities and
applications for both exact and approximate A* algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08836">Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English. (arXiv:2311.08836v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rarrick_S/0/1/0/all/0/1">Spencer Rarrick</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_R/0/1/0/all/0/1">Ranjita Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Poudel_S/0/1/0/all/0/1">Sundar Poudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhary_V/0/1/0/all/0/1">Vishal Chowdhary</a></p>
<p>Machine Translation (MT) continues to improve in quality and adoption, yet
the inadvertent perpetuation of gender bias remains a significant concern.
Despite numerous studies into gender bias in translations from gender-neutral
languages such as Turkish into more strongly gendered languages like English,
there are no benchmarks for evaluating this phenomenon or for assessing
mitigation strategies. To address this gap, we introduce GATE X-E, an extension
to the GATE (Rarrick et al., 2023) corpus, that consists of human translations
from Turkish, Hungarian, Finnish, and Persian into English. Each translation is
accompanied by feminine, masculine, and neutral variants for each possible
gender interpretation. The dataset, which contains between 1250 and 1850
instances for each of the four language pairs, features natural sentences with
a wide range of sentence lengths and domains, challenging translation rewriters
on various linguistic phenomena. Additionally, we present an English gender
rewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We
open source our contributions to encourage further research on gender
debiasing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08856">Advances in ACL2 Proof Debugging Tools. (arXiv:2311.08856v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1">Matt Kaufmann</a> (UT Austin, retired), <a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1">J Strother Moore</a> (UT Austin, retired)</p>
<p>The experience of an ACL2 user generally includes many failed proof attempts.
A key to successful use of the ACL2 prover is the effective use of tools to
debug those failures. We focus on changes made after ACL2 Version 8.5: the
improved break-rewrite utility and the new utility, with-brr-data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08894">Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering. (arXiv:2311.08894v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patidar_M/0/1/0/all/0/1">Mayur Patidar</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Avinash Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawhney_R/0/1/0/all/0/1">Riya Sawhney</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_I/0/1/0/all/0/1">Indrajit Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a></p>
<p>We address the zero-shot transfer learning setting for the knowledge base
question answering (KBQA) problem, where a large volume of labeled training
data is available for the source domain, but no such labeled examples are
available for the target domain. Transfer learning for KBQA makes use of large
volumes of unlabeled data in the target in addition to the labeled data in the
source. More recently, few-shot in-context learning using Black-box Large
Language Models (BLLMs) has been adapted for KBQA without considering any
source domain data. In this work, we show how to meaningfully combine these two
paradigms for KBQA so that their benefits add up. Specifically, we preserve the
two stage retrieve-then-generate pipeline of supervised KBQA and introduce
interaction between in-context learning using BLLMs and transfer learning from
the source for both stages. In addition, we propose execution-guided
self-refinement using BLLMs, decoupled from the transfer setting. With the help
of experiments using benchmark datasets GrailQA as the source and WebQSP as the
target, we show that the proposed combination brings significant improvements
to both stages and also outperforms by a large margin state-of-the-art
supervised KBQA models trained on the source. We also show that in the
in-domain setting, the proposed BLLM augmentation significantly outperforms
state-of-the-art supervised models, when the volume of labeled data is limited,
and also outperforms these marginally even when using the entire large training
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08923">Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery. (arXiv:2311.08923v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emam_A/0/1/0/all/0/1">Ahmed Emam</a>, <a href="http://arxiv.org/find/cs/1/au:+Stomberg_T/0/1/0/all/0/1">Timo T. Stomberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1">Ribana Roscher</a></p>
<p>Natural protected areas are vital for biodiversity, climate change
mitigation, and supporting ecological processes. Despite their significance,
comprehensive mapping is hindered by a lack of understanding of their
characteristics and a missing land cover class definition. This paper aims to
advance the explanation of the designating patterns forming protected and wild
areas. To this end, we propose a novel framework that uses activation
maximization and a generative adversarial model. With this, we aim to generate
satellite images that, in combination with domain knowledge, are capable of
offering complete and valid explanations for the spatial and spectral patterns
that define the natural authenticity of these regions. Our proposed framework
produces more precise attribution maps pinpointing the designating patterns
forming the natural authenticity of protected areas. Our approach fosters our
understanding of the ecological integrity of the protected natural areas and
may contribute to future monitoring and preservation efforts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08935">Supported Trust Region Optimization for Offline Reinforcement Learning. (arXiv:2311.08935v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yixiu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongchang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1">Xiangyang Ji</a></p>
<p>Offline reinforcement learning suffers from the out-of-distribution issue and
extrapolation error. Most policy constraint methods regularize the density of
the trained policy towards the behavior policy, which is too restrictive in
most cases. We propose Supported Trust Region optimization (STR) which performs
trust region policy optimization with the policy constrained within the support
of the behavior policy, enjoying the less restrictive support constraint. We
show that, when assuming no approximation and sampling error, STR guarantees
strict policy improvement until convergence to the optimal support-constrained
policy in the dataset. Further with both errors incorporated, STR still
guarantees safe policy improvement for each step. Empirical results validate
the theory of STR and demonstrate its state-of-the-art performance on MuJoCo
locomotion domains and much more challenging AntMaze domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08941">Reasoning over Description Logic-based Contexts with Transformers. (arXiv:2311.08941v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Poulis_A/0/1/0/all/0/1">Angelos Poulis</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsalapati_E/0/1/0/all/0/1">Eleni Tsalapati</a>, <a href="http://arxiv.org/find/cs/1/au:+Koubarakis_M/0/1/0/all/0/1">Manolis Koubarakis</a></p>
<p>One way that the current state of the art measures the reasoning ability of
transformer-based models is by evaluating accuracy in downstream tasks like
logical question answering or proof generation over synthetic contexts
expressed in natural language. However, most of the contexts used are in
practice very simple; in most cases, they are generated from short first-order
logic sentences with only a few logical operators and quantifiers. In this
work, we seek to answer the question how well a transformer-based model will
perform reasoning over expressive contexts. For this purpose, we construct a
synthetic natural language question-answering dataset, generated by description
logic knowledge bases. For the generation of the knowledge bases, we use the
expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K
examples, and increases in two dimensions: i) reasoning depth, and ii) length
of sentences. We show that the performance of our DeBERTa-based model,
DELTA$_M$, is marginally affected when the reasoning depth is increased and it
is not affected at all when the length of the sentences is increasing. We also
evaluate the generalization ability of the model on reasoning depths unseen at
training, both increasing and decreasing, revealing interesting insights into
the model's adaptive generalization abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08943">Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control. (arXiv:2311.08943v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hobbs_K/0/1/0/all/0/1">Kerianne L. Hobbs</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bernard Li</a></p>
<p>Designing a safe, trusted, and ethical AI may be practically impossible;
however, designing AI with safe, trusted, and ethical use in mind is possible
and necessary in safety and mission-critical domains like aerospace. Safe,
trusted, and ethical use of AI are often used interchangeably; however, a
system can be safely used but not trusted or ethical, have a trusted use that
is not safe or ethical, and have an ethical use that is not safe or trusted.
This manuscript serves as a primer to illuminate the nuanced differences
between these concepts, with a specific focus on applications of Human-AI
teaming in aerospace system control, where humans may be in, on, or
out-of-the-loop of decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08957">I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots. (arXiv:2311.08957v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbo_G/0/1/0/all/0/1">Giulio Antonio Abbo</a>, <a href="http://arxiv.org/find/cs/1/au:+Belpaeme_T/0/1/0/all/0/1">Tony Belpaeme</a></p>
<p>In the rapidly evolving landscape of human-computer interaction, the
integration of vision capabilities into conversational agents stands as a
crucial advancement. This paper presents an initial implementation of a
dialogue manager that leverages the latest progress in Large Language Models
(e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts with
real-time visual input. LLMs are used to interpret both textual prompts and
visual stimuli, creating a more contextually aware conversational agent. The
system's prompt engineering, incorporating dialogue with summarisation of the
images, ensures a balance between context preservation and computational
efficiency. Six interactions with a Furhat robot powered by this system are
reported, illustrating and discussing the results obtained. By implementing
this vision-enabled dialogue system, the paper envisions a future where
conversational agents seamlessly blend textual and visual modalities, enabling
richer, more context-aware dialogues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08968">Identifying Linear Relational Concepts in Large Language Models. (arXiv:2311.08968v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chanin_D/0/1/0/all/0/1">David Chanin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hunter_A/0/1/0/all/0/1">Anthony Hunter</a>, <a href="http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1">Oana-Maria Camburu</a></p>
<p>Transformer language models (LMs) have been shown to represent concepts as
directions in the latent space of hidden activations. However, for any given
human-interpretable concept, how can we find its direction in the latent space?
We present a technique called linear relational concepts (LRC) for finding
concept directions corresponding to human-interpretable concepts at a given
hidden layer in a transformer LM by first modeling the relation between subject
and object as a linear relational embedding (LRE). While the LRE work was
mainly presented as an exercise in understanding model representations, we find
that inverting the LRE while using earlier object layers results in a powerful
technique to find concept directions that both work well as a classifier and
causally influence model outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08987">Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems. (arXiv:2311.08987v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farrell_M/0/1/0/all/0/1">Marie Farrell</a> (University of Manchester, UK), <a href="http://arxiv.org/find/cs/1/au:+Luckcuck_M/0/1/0/all/0/1">Matt Luckcuck</a> (University of Nottingham, UK), <a href="http://arxiv.org/find/cs/1/au:+Gleirscher_M/0/1/0/all/0/1">Mario Gleirscher</a> (University of Bremen, Germany), <a href="http://arxiv.org/find/cs/1/au:+Schwammberger_M/0/1/0/all/0/1">Maike Schwammberger</a> (Karlsruhe Institute of Technology, Germany)</p>
<p>This EPTCS volume contains the proceedings for the Fifth International
Workshop on Formal Methods for Autonomous Systems (FMAS 2023), which was held
on the 15th and 16th of November 2023. FMAS 2023 was co-located with 18th
International Conference on integrated Formal Methods (iFM) (iFM'22), organised
by Leiden Institute of Advanced Computer Science of Leiden University. The
workshop itself was held at Scheltema Leiden, a renovated 19th Century blanket
factory alongside the canal.
</p>
<p>FMAS 2023 received 25 submissions. We received 11 regular papers, 3
experience reports, 6 research previews, and 5 vision papers. The researchers
who submitted papers to FMAS 2023 were from institutions in: Australia, Canada,
Colombia, France, Germany, Ireland, Italy, the Netherlands, Sweden, the United
Kingdom, and the United States of America. Increasing our number of submissions
for the third year in a row is an encouraging sign that FMAS has established
itself as a reputable publication venue for research on the formal modelling
and verification of autonomous systems. After each paper was reviewed by three
members of our Programme Committee we accepted a total of 15 papers: 8 long
papers and 7 short papers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08993">When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks. (arXiv:2311.08993v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaozhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianhui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weikai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yunjia Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zimu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhili Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1">Kaisheng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Lei Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juanzi Li</a></p>
<p>In-context learning (ICL) has become the default method for using large
language models (LLMs), making the exploration of its limitations and
understanding the underlying causes crucial. In this paper, we find that ICL
falls short of handling specification-heavy tasks, which are tasks with
complicated and extensive task specifications, requiring several hours for
ordinary humans to master, such as traditional information extraction tasks.
The performance of ICL on these tasks mostly cannot reach half of the
state-of-the-art results. To explore the reasons behind this failure, we
conduct comprehensive experiments on 18 specification-heavy tasks with various
LLMs and identify three primary reasons: inability to specifically understand
context, misalignment in task schema comprehension with humans, and inadequate
long-text understanding ability. Furthermore, we demonstrate that through
fine-tuning, LLMs can achieve decent performance on these tasks, indicating
that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback
of existing alignment methods that renders LLMs incapable of handling
complicated specification-heavy tasks via ICL. To substantiate this, we perform
dedicated instruction tuning on LLMs for these tasks and observe a notable
improvement. We hope the analyses in this paper could facilitate advancements
in alignment methods enabling LLMs to meet more sophisticated human demands.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08999">Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake. (arXiv:2311.08999v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hackathon_M/0/1/0/all/0/1">Morocco Solidarity Hackathon</a> (Organizers, Speakers, Mentors and Participant teams)</p>
<p>The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023
prompted critical reflections on global disaster management strategies,
resulting in a post-disaster hackathon, using artificial intelligence (AI) to
improve disaster preparedness, response, and recovery. This paper provides (i)
a comprehensive literature review, (ii) an overview of winning projects, (iii)
key insights and challenges, namely real-time open-source data, data scarcity,
and interdisciplinary collaboration barriers, and (iv) a community-call for
further action.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09014">Adversarial Attacks to Reward Machine-based Reinforcement Learning. (arXiv:2311.09014v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nodari_L/0/1/0/all/0/1">Lorenzo Nodari</a></p>
<p>In recent years, Reward Machines (RMs) have stood out as a simple yet
effective automata-based formalism for exposing and exploiting task structure
in reinforcement learning settings. Despite their relevance, little to no
attention has been directed to the study of their security implications and
robustness to adversarial scenarios, likely due to their recent appearance in
the literature. With my thesis, I aim to provide the first analysis of the
security of RM-based reinforcement learning techniques, with the hope of
motivating further research in the field, and I propose and evaluate a novel
class of attacks on RM-based techniques: blinding attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09015">Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach. (arXiv:2311.09015v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1">Zixiao Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ghassami_A/0/1/0/all/0/1">AmirEmad Ghassami</a>, <a href="http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1">Ilya Shpitser</a></p>
<p>We consider the task of identifying and estimating a parameter of interest in
settings where data is missing not at random (MNAR). In general, such
parameters are not identified without strong assumptions on the missing data
model. In this paper, we take an alternative approach and introduce a method
inspired by data fusion, where information in an MNAR dataset is augmented by
information in an auxiliary dataset subject to missingness at random (MAR). We
show that even if the parameter of interest cannot be identified given either
dataset alone, it can be identified given pooled data, under two complementary
sets of assumptions. We derive an inverse probability weighted (IPW) estimator
for identified parameters, and evaluate the performance of our estimation
strategies via simulation studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09027">Assessing the Robustness of Intelligence-Driven Reinforcement Learning. (arXiv:2311.09027v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nodari_L/0/1/0/all/0/1">Lorenzo Nodari</a>, <a href="http://arxiv.org/find/cs/1/au:+Cerutti_F/0/1/0/all/0/1">Federico Cerutti</a></p>
<p>Robustness to noise is of utmost importance in reinforcement learning
systems, particularly in military contexts where high stakes and uncertain
environments prevail. Noise and uncertainty are inherent features of military
operations, arising from factors such as incomplete information, adversarial
actions, or unpredictable battlefield conditions. In RL, noise can critically
impact decision-making, mission success, and the safety of personnel. Reward
machines offer a powerful tool to express complex reward structures in RL
tasks, enabling the design of tailored reinforcement signals that align with
mission objectives. This paper considers the problem of the robustness of
intelligence-driven reinforcement learning based on reward machines. The
preliminary results presented suggest the need for further research in
evidential reasoning and learning to harden current state-of-the-art
reinforcement learning approaches before being mission-critical-ready.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09033">MELA: Multilingual Evaluation of Linguistic Acceptability. (arXiv:2311.09033v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziyin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yikang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weifang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Junyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hai Hu</a></p>
<p>Recent benchmarks for Large Language Models (LLMs) have mostly focused on
application-driven tasks such as complex reasoning and code generation, and
this has led to a scarcity in purely linguistic evaluation of LLMs. Against
this background, we introduce Multilingual Evaluation of Linguistic
Acceptability -- MELA, the first multilingual benchmark on linguistic
acceptability with 48K samples covering 10 languages from a diverse set of
language families. We establish baselines of commonly used LLMs along with
supervised models, and conduct cross-lingual transfer and multi-task learning
experiments with XLM-R. In pursuit of multilingual interpretability, we analyze
the weights of fine-tuned XLM-R to explore the possibility of identifying
transfer difficulty between languages. Our results show that ChatGPT benefits
much from in-context examples but still lags behind fine-tuned XLM-R, while the
performance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting.
Cross-lingual and multi-task learning experiments show that unlike semantic
tasks, in-language training data is crucial in acceptability judgements.
Results in layerwise probing indicate that the upper layers of XLM-R become a
task-specific but language-agnostic region for multilingual acceptability
judgment. We also introduce the concept of conflicting weight, which could be a
potential indicator for the difficulty of cross-lingual transfer between
languages. Our data will be available at https://github.com/sjtu-compling/MELA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09050">Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts. (arXiv:2311.09050v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yunshi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1">Wei Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1">Weining Qian</a></p>
<p>Zero-shot Visual Question Answering (VQA) is a prominent vision-language task
that examines both the visual and textual understanding capability of systems
in the absence of training data. Recently, by converting the images into
captions, information across multi-modalities is bridged and Large Language
Models (LLMs) can apply their strong zero-shot generalization capability to
unseen questions. To design ideal prompts for solving VQA via LLMs, several
studies have explored different strategies to select or generate
question-answer pairs as the exemplar prompts, which guide LLMs to answer the
current questions effectively. However, they totally ignore the role of
question prompts. The original questions in VQA tasks usually encounter
ellipses and ambiguity which require intermediate reasoning. To this end, we
present Reasoning Question Prompts for VQA tasks, which can further activate
the potential of LLMs in zero-shot scenarios. Specifically, for each question,
we first generate self-contained questions as reasoning question prompts via an
unsupervised question edition module considering sentence fluency, semantic
integrity and syntactic invariance. Each reasoning question prompt clearly
indicates the intent of the original question. This results in a set of
candidate answers. Then, the candidate answers associated with their confidence
scores acting as answer heuristics are fed into LLMs and produce the final
answer. We evaluate reasoning question prompts on three VQA challenges,
experimental results demonstrate that they can significantly improve the
results of LLMs on zero-shot setting and outperform existing state-of-the-art
zero-shot methods on three out of four data sets. Our source code is publicly
released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09053">Assessing Knowledge Editing in Language Models via Relation Perspective. (arXiv:2311.09053v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yifan Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiaoyan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Huanhuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1">Fangyu Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1">Yixuan Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Ran Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kang Liu</a></p>
<p>Knowledge Editing (KE) for modifying factual knowledge in Large Language
Models (LLMs) has been receiving increasing attention. However, existing
knowledge editing methods are entity-centric, and it is unclear whether this
approach is suitable for a relation-centric perspective. To address this gap,
this paper constructs a new benchmark named RaKE, which focuses on Relation
based Knowledge Editing. In this paper, we establish a suite of innovative
metrics for evaluation and conduct comprehensive experiments involving various
knowledge editing baselines. We notice that existing knowledge editing methods
exhibit the potential difficulty in their ability to edit relations. Therefore,
we further explore the role of relations in factual triplets within the
transformer. Our research results confirm that knowledge related to relations
is not only stored in the FFN network but also in the attention layers. This
provides experimental support for future relation-based knowledge editing
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09068">Learning Fair Division from Bandit Feedback. (arXiv:2311.09068v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamada_H/0/1/0/all/0/1">Hakuei Yamada</a>, <a href="http://arxiv.org/find/cs/1/au:+Komiyama_J/0/1/0/all/0/1">Junpei Komiyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Abe_K/0/1/0/all/0/1">Kenshi Abe</a>, <a href="http://arxiv.org/find/cs/1/au:+Iwasaki_A/0/1/0/all/0/1">Atsushi Iwasaki</a></p>
<p>This work addresses learning online fair division under uncertainty, where a
central planner sequentially allocates items without precise knowledge of
agents' values or utilities. Departing from conventional online algorithm, the
planner here relies on noisy, estimated values obtained after allocating items.
We introduce wrapper algorithms utilizing \textit{dual averaging}, enabling
gradual learning of both the type distribution of arriving items and agents'
values through bandit feedback. This approach enables the algorithms to
asymptotically achieve optimal Nash social welfare in linear Fisher markets
with agents having additive utilities. We establish regret bounds in Nash
social welfare and empirically validate the superior performance of our
proposed algorithms across synthetic and empirical datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09069">How Well Do Large Language Models Truly Ground?. (arXiv:2311.09069v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyunji Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Joo_S/0/1/0/all/0/1">Sejune Joo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Chaeeun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1">Joel Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Doyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1">Kyoung-Woon On</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1">Minjoon Seo</a></p>
<p>Reliance on the inherent knowledge of Large Language Models (LLMs) can cause
issues such as hallucinations, lack of control, and difficulties in integrating
variable knowledge. To mitigate this, LLMs can be probed to generate responses
by grounding on external context, often given as input (knowledge-augmented
models). Yet, previous research is often confined to a narrow view of the term
"grounding", often only focusing on whether the response contains the correct
answer or not, which does not ensure the reliability of the entire response. To
address this limitation, we introduce a strict definition of grounding: a model
is considered truly grounded when its responses (1) fully utilize necessary
knowledge from the provided context, and (2) don't exceed the knowledge within
the contexts. We introduce a new dataset and a grounding metric to assess this
new definition and perform experiments across 13 LLMs of different sizes and
training methods to provide insights into the factors that influence grounding
performance. Our findings contribute to a better understanding of how to
improve grounding capabilities and suggest an area of improvement toward more
reliable and controllable LLM applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09071">How Multilingual is Multilingual LLM?. (arXiv:2311.09071v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1">Fei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shuai Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a></p>
<p>Large Language Models (LLMs), trained predominantly on extensive English
data, often exhibit limitations when applied to other languages. Current
research is primarily focused on enhancing the multilingual capabilities of
these models by employing various tuning strategies. Despite their
effectiveness in certain languages, the understanding of the multilingual
abilities of LLMs remains incomplete. This study endeavors to evaluate the
multilingual capacity of LLMs by conducting an exhaustive analysis across 101
languages, and classifies languages with similar characteristics into four
distinct quadrants. By delving into each quadrant, we shed light on the
rationale behind their categorization and offer actionable guidelines for
tuning these languages. Extensive experiments reveal that existing LLMs possess
multilingual capabilities that surpass our expectations, and we can
significantly improve the multilingual performance of LLMs by focusing on these
distinct attributes present in each quadrant.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09086">The Uli Dataset: An Exercise in Experience Led Annotation of oGBV. (arXiv:2311.09086v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Arnav Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Jinadoss_M/0/1/0/all/0/1">Maha Jinadoss</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1">Cheshta Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1">Denny George</a>, <a href="http://arxiv.org/find/cs/1/au:+Brindaalakshmi/0/1/0/all/0/1">Brindaalakshmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1">Haseena Dawood Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rawat_K/0/1/0/all/0/1">Kirti Rawat</a>, Div, <a href="http://arxiv.org/find/cs/1/au:+Ritash/0/1/0/all/0/1">Ritash</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathur_S/0/1/0/all/0/1">Seema Mathur</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1">Shivani Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Shora_S/0/1/0/all/0/1">Shehla Rashid Shora</a>, <a href="http://arxiv.org/find/cs/1/au:+Raut_R/0/1/0/all/0/1">Rie Raut</a>, <a href="http://arxiv.org/find/cs/1/au:+Pawar_S/0/1/0/all/0/1">Sumit Pawar</a>, <a href="http://arxiv.org/find/cs/1/au:+Paithane_A/0/1/0/all/0/1">Apurva Paithane</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonia/0/1/0/all/0/1">Sonia</a>, <a href="http://arxiv.org/find/cs/1/au:+Vivek/0/1/0/all/0/1">Vivek</a>, <a href="http://arxiv.org/find/cs/1/au:+Priscilla_D/0/1/0/all/0/1">Dharini Priscilla</a>, <a href="http://arxiv.org/find/cs/1/au:+Khairunnisha/0/1/0/all/0/1">Khairunnisha</a>, <a href="http://arxiv.org/find/cs/1/au:+Banu_G/0/1/0/all/0/1">Grace Banu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tandon_A/0/1/0/all/0/1">Ambika Tandon</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakker_R/0/1/0/all/0/1">Rishav Thakker</a>, <a href="http://arxiv.org/find/cs/1/au:+Korra_R/0/1/0/all/0/1">Rahul Dev Korra</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1">Aatman Vaidya</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhakar_T/0/1/0/all/0/1">Tarunima Prabhakar</a></p>
<p>Online gender based violence has grown concomitantly with adoption of the
internet and social media. Its effects are worse in the Global majority where
many users use social media in languages other than English. The scale and
volume of conversations on the internet has necessitated the need for automated
detection of hate speech, and more specifically gendered abuse. There is,
however, a lack of language specific and contextual data to build such
automated tools. In this paper we present a dataset on gendered abuse in three
languages- Hindi, Tamil and Indian English. The dataset comprises of tweets
annotated along three questions pertaining to the experience of gender abuse,
by experts who identify as women or a member of the LGBTQIA community in South
Asia. Through this dataset we demonstrate a participatory approach to creating
datasets that drive AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09094">Can MusicGen Create Training Data for MIR Tasks?. (arXiv:2311.09094v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kroher_N/0/1/0/all/0/1">Nadine Kroher</a>, <a href="http://arxiv.org/find/cs/1/au:+Cuesta_H/0/1/0/all/0/1">Helena Cuesta</a>, <a href="http://arxiv.org/find/cs/1/au:+Pikrakis_A/0/1/0/all/0/1">Aggelos Pikrakis</a></p>
<p>We are investigating the broader concept of using AI-based generative music
systems to generate training data for Music Information Retrieval (MIR) tasks.
To kick off this line of work, we ran an initial experiment in which we trained
a genre classifier on a fully artificial music dataset created with MusicGen.
We constructed over 50 000 genre- conditioned textual descriptions and
generated a collection of music excerpts that covers five musical genres. Our
preliminary results show that the proposed model can learn genre-specific
characteristics from artificial music tracks that generalise well to real-world
music recordings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09101">Towards A Unified View of Answer Calibration for Multi-Step Reasoning. (arXiv:2311.09101v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Oo_N/0/1/0/all/0/1">Nay Oo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1">Bryan Hooi</a></p>
<p>Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have
broadened the scope for improving multi-step reasoning capabilities. Usually,
answer calibration strategies such as step-level or path-level calibration play
a vital role in multi-step reasoning. While effective, there remains a
significant gap in our understanding of the key factors that drive their
success. In this paper, we break down the design of recent answer calibration
strategies and present a unified view which establishes connections between
them. We then conduct a thorough evaluation on these strategies from a unified
view, systematically scrutinizing step-level and path-level answer calibration
across multiple paths. Our study holds the potential to illuminate key insights
for optimizing multi-step reasoning with answer calibration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09109">Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?. (arXiv:2311.09109v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sakai_Y/0/1/0/all/0/1">Yusuke Sakai</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1">Hidetaka Kamigaito</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1">Katsuhiko Hayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1">Taro Watanabe</a></p>
<p>Knowledge graphs (KGs) consist of links that describe relationships between
entities. Due to the difficulty of manually enumerating all relationships
between entities, automatically completing them is essential for KGs. Knowledge
Graph Completion (KGC) is a task that infers unseen relationships between
entities in a KG. Traditional embedding-based KGC methods, such as RESCAL,
TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using
only the knowledge from training data. In contrast, the recent Pre-trained
Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training.
Therefore, PLM-based KGC can estimate missing links between entities by reusing
memorized knowledge from pre-training without inference. This approach is
problematic because building KGC models aims to infer unseen links between
entities. However, conventional evaluations in KGC do not consider inference
and memorization abilities separately. Thus, a PLM-based KGC method, which
achieves high performance in current KGC evaluations, may be ineffective in
practical applications. To address this issue, we analyze whether PLM-based KGC
methods make inferences or merely access memorized knowledge. For this purpose,
we propose a method for constructing synthetic datasets specified in this
analysis and conclude that PLMs acquire the inference abilities required for
KGC through pre-training, even though the performance improvements mostly come
from textual information of entities and relations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09114">Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification. (arXiv:2311.09114v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Haoqiang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1">Juntong Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating fluent text. However, they often encounter the challenge of
generating inaccurate or hallucinated content. This issue is common in both
non-retrieval-based generation and retrieval-augmented generation approaches,
and existing post-hoc rectification methods may not address the accumulated
hallucination errors that may be caused by the "snowballing" issue, especially
in reasoning tasks. To tackle these challenges, we introduce a novel approach
called Real-time Verification and Rectification (Ever). Instead of waiting
until the end of the generation process to rectify hallucinations, Ever employs
a real-time, step-wise generation and hallucination rectification strategy. The
primary objective is to detect and rectify hallucinations as they occur during
the text generation process. When compared to both retrieval-based and
non-retrieval-based baselines, Ever demonstrates a significant improvement in
generating trustworthy and factually accurate text across a diverse range of
tasks, including short-form QA, biography generation, and multi-hop reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09115">HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data. (arXiv:2311.09115v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hemker_K/0/1/0/all/0/1">Konstantin Hemker</a>, <a href="http://arxiv.org/find/cs/1/au:+Smidjievski_N/0/1/0/all/0/1">Nikola Smidjievski</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1">Mateja Jamnik</a></p>
<p>Technological advances in medical data collection such as high-resolution
histopathology and high-throughput genomic sequencing have contributed to the
rising requirement for multi-modal biomedical modelling, specifically for
image, tabular, and graph data. Most multi-modal deep learning approaches use
modality-specific architectures that are trained separately and cannot capture
the crucial cross-modal information that motivates the integration of different
data sources. This paper presents the Hybrid Early-fusion Attention Learning
Network (HEALNet): a flexible multi-modal fusion architecture, which a)
preserves modality-specific structural information, b) captures the cross-modal
interactions and structural information in a shared latent space, c) can
effectively handle missing modalities during training and inference, and d)
enables intuitive model inspection by learning on the raw data input instead of
opaque embeddings. We conduct multi-modal survival analysis on Whole Slide
Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas
(TCGA). HEALNet achieves state-of-the-art performance, substantially improving
over both uni-modal and recent multi-modal baselines, whilst being robust in
scenarios with missing modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09127">Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts. (arXiv:2311.09127v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuanwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a></p>
<p>Existing work on jailbreak Multimodal Large Language Models (MLLMs) has
focused primarily on adversarial examples in model inputs, with less attention
to vulnerabilities in model APIs. To fill the research gap, we carry out the
following work: 1) We discover a system prompt leakage vulnerability in GPT-4V.
Through carefully designed dialogue, we successfully steal the internal system
prompts of GPT-4V. This finding indicates potential exploitable security risks
in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM
jailbreaking attack method termed SASP (Self-Adversarial Attack via System
Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to
search for potential jailbreak prompts leveraging stolen system prompts.
Furthermore, in pursuit of better performance, we also add human modification
based on GPT-4's analysis, which further improves the attack success rate to
98.7\%; 3) We evaluated the effect of modifying system prompts to defend
against jailbreaking attacks. Results show that appropriately designed system
prompts can significantly reduce jailbreak success rates. Overall, our work
provides new insights into enhancing MLLM security, demonstrating the important
role of system prompts in jailbreaking, which could be leveraged to greatly
facilitate jailbreak success rates while also holding the potential for
defending against jailbreaks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09149">Temporal Knowledge Question Answering via Abstract Reasoning Induction. (arXiv:2311.09149v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongfang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Baotian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a></p>
<p>In this paper, we tackle the significant challenge of temporal knowledge
reasoning in Large Language Models (LLMs), an area where such models frequently
encounter difficulties. These difficulties often result in the generation of
misleading or incorrect information, primarily due to their limited capacity to
process evolving factual knowledge and complex temporal logic. In response, we
propose a novel, constructivism-based approach that advocates for a paradigm
shift in LLM learning towards an active, ongoing process of knowledge synthesis
and customization. At the heart of our proposal is the Abstract Reasoning
Induction ARI framework, which divides temporal reasoning into two distinct
phases: Knowledge-agnostic and Knowledge-based. This division aims to reduce
instances of hallucinations and improve LLMs' capacity for integrating abstract
methodologies derived from historical data. Our approach achieves remarkable
improvements, with relative gains of 29.7\% and 9.27\% on two temporal QA
datasets, underscoring its efficacy in advancing temporal reasoning in LLMs.
The code will be released at https://github.com/czy1999/ARI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09174">AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph. (arXiv:2311.09174v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Haochen Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1">Tianqing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Sehyun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>Cognitive research indicates that abstraction ability is essential in human
intelligence, which remains under-explored in language models. In this paper,
we present AbsPyramid, a unified entailment graph of 221K textual descriptions
of abstraction knowledge. While existing resources only touch nouns or verbs
within simplified events or specific domains, AbsPyramid collects abstract
knowledge for three components of diverse events to comprehensively evaluate
the abstraction ability of language models in the open domain. Experimental
results demonstrate that current LLMs face challenges comprehending abstraction
knowledge in zero-shot and few-shot settings. By training on our rich
abstraction knowledge, we find LLMs can acquire basic abstraction abilities and
generalize to unseen events. In the meantime, we empirically show that our
benchmark is comprehensive to enhance LLMs across two previous abstraction
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09175">Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers. (arXiv:2311.09175v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minghan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1">Honglei Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1">Kai Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jimmy Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagerman_R/0/1/0/all/0/1">Rolf Jagerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuanhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1">Michael Bendersky</a></p>
<p>Query expansion has been proved to be effective in improving recall and
precision of first-stage retrievers, and yet its influence on a complicated,
state-of-the-art cross-encoder ranker remains under-explored. We first show
that directly applying the expansion techniques in the current literature to
state-of-the-art neural rankers can result in deteriorated zero-shot
performance. To this end, we propose GFF, a pipeline that includes a large
language model and a neural ranker, to Generate, Filter, and Fuse query
expansions more effectively in order to improve the zero-shot ranking metrics
such as nDCG@10. Specifically, GFF first calls an instruction-following
language model to generate query-related keywords through a reasoning chain.
Leveraging self-consistency and reciprocal rank weighting, GFF further filters
and combines the ranking results of each expanded query dynamically. By
utilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on
BEIR and TREC DL 2019/2020. We also analyze different modelling choices in the
GFF pipeline and shed light on the future directions in query expansion for
zero-shot neural rankers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09188">Towards Verifiable Text Generation with Symbolic References. (arXiv:2311.09188v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1">Lucas Torroba Hennigen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1">Shannon Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nrusimha_A/0/1/0/all/0/1">Aniruddha Nrusimha</a>, <a href="http://arxiv.org/find/cs/1/au:+Gapp_B/0/1/0/all/0/1">Bernhard Gapp</a>, <a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1">David Sontag</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a></p>
<p>Large language models (LLMs) have demonstrated an impressive ability to
synthesize plausible and fluent text. However they remain vulnerable to
hallucinations, and thus their outputs generally require manual human
verification for high-stakes applications, which can be time-consuming and
difficult. This paper proposes symbolically grounded generation (SymGen) as a
simple approach for enabling easier validation of an LLM's output. SymGen
prompts an LLM to interleave its regular output text with explicit symbolic
references to fields present in some conditioning data (e.g., a table in JSON
format). The references can be used to display the provenance of different
spans of text in the generation, reducing the effort required for manual
verification. Across data-to-text and question answering experiments, we find
that LLMs are able to directly output text that makes use of symbolic
references while maintaining fluency and accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09193">The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task. (arXiv:2311.09193v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yifan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengchuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Wenhan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1">Barlas Oguz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1">James C. Gee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Yixin Nie</a></p>
<p>The study explores the effectiveness of the Chain-of-Thought approach, known
for its proficiency in language tasks by breaking them down into sub-tasks and
intermediate steps, in improving vision-language tasks that demand
sophisticated perception and reasoning. We present the "Description then
Decision" strategy, which is inspired by how humans process signals. This
strategy significantly improves probing task performance by 50%, establishing
the groundwork for future research on reasoning paradigms in complex
vision-language tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09198">Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering. (arXiv:2311.09198v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junqing He</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_K/0/1/0/all/0/1">Kunhao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiaoqun Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhuoyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxin Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qianguo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zejian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxing Zhang</a></p>
<p>While large language models (LLMs) are equipped with longer text input
capabilities than before, they are struggling to seek correct information in
long contexts. The "lost in the middle" problem challenges most LLMs, referring
to the dramatic decline in accuracy when correct information is located in the
middle. To overcome this crucial issue, this paper proposes to enhance the
information searching and reflection ability of LLMs in long contexts via
specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).
Following these tasks, our model excels in focusing more precisely on the
desired information. Experimental results show substantial improvement in
Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%
absolute gain in shuffled settings, by 21.5% in passage retrieval task. We
release our model, Ziya-Reader to promote related research in the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.14201">CNTLS: A Benchmark Dataset for Abstractive or Extractive Chinese Timeline Summarization. (arXiv:2105.14201v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1">Qianren Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiazheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianxin Li</a></p>
<p>Timeline summarization (TLS) involves creating summaries of long-running
events using dated summaries from numerous news articles. However, limited data
availability has significantly slowed down the development of timeline
summarization. In this paper, we introduce the CNTLS dataset, a versatile
resource for Chinese timeline summarization. CNTLS encompasses 77 real-life
topics, each with 2524 documents and summarizes nearly 60\% days duration
compression on average all topics.
</p>
<p>We meticulously analyze the corpus using well-known metrics, focusing on the
style of the summaries and the complexity of the summarization task.
Specifically, we evaluate the performance of various extractive and generative
summarization systems on the CNTLS corpus to provide benchmarks and support
further research. To the best of our knowledge, CNTLS is the first Chinese
timeline summarization dataset. The dataset and source code are
released\footnote{Code and data available at:
\emph{\url{https://github.com/OpenSUM/CNTLS}}.}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.03427">Deep Learning for Two-Sided Matching. (arXiv:2107.03427v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ravindranath_S/0/1/0/all/0/1">Sai Srivatsa Ravindranath</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhe Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shira Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jonathan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kominers_S/0/1/0/all/0/1">Scott D. Kominers</a>, <a href="http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1">David C. Parkes</a></p>
<p>We initiate the study of deep learning for the automated design of two-sided
matching mechanisms. What is of most interest is to use machine learning to
understand the possibility of new tradeoffs between strategy-proofness and
stability. These properties cannot be achieved simultaneously, but the
efficient frontier is not understood. We introduce novel differentiable
surrogates for quantifying ordinal strategy-proofness and stability and use
them to train differentiable matching mechanisms that map discrete preferences
to valid randomized matchings. We demonstrate that the efficient frontier
characterized by these learned mechanisms is substantially better than that
achievable through a convex combination of baselines of deferred acceptance
(stable and strategy-proof for only one side of the market), top trading cycles
(strategy-proof for one side, but not stable), and randomized serial
dictatorship (strategy-proof for both sides, but not stable). This gives a new
target for economic theory and opens up new possibilities for machine learning
pipelines in matching market design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.11986">Gradient Masked Averaging for Federated Learning. (arXiv:2201.11986v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tenison_I/0/1/0/all/0/1">Irene Tenison</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreeramadas_S/0/1/0/all/0/1">Sai Aravind Sreeramadas</a>, <a href="http://arxiv.org/find/cs/1/au:+Mugunthan_V/0/1/0/all/0/1">Vaikkunth Mugunthan</a>, <a href="http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1">Edouard Oyallon</a>, <a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1">Irina Rish</a>, <a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1">Eugene Belilovsky</a></p>
<p>Federated learning (FL) is an emerging paradigm that permits a large number
of clients with heterogeneous data to coordinate learning of a unified global
model without the need to share data amongst each other. A major challenge in
federated learning is the heterogeneity of data across client, which can
degrade the performance of standard FL algorithms. Standard FL algorithms
involve averaging of model parameters or gradient updates to approximate the
global model at the server. However, we argue that in heterogeneous settings,
averaging can result in information loss and lead to poor generalization due to
the bias induced by dominant client gradients. We hypothesize that to
generalize better across non-i.i.d datasets, the algorithms should focus on
learning the invariant mechanism that is constant while ignoring spurious
mechanisms that differ across clients. Inspired from recent works in
Out-of-Distribution generalization, we propose a gradient masked averaging
approach for FL as an alternative to the standard averaging of client updates.
This aggregation technique for client updates can be adapted as a drop-in
replacement in most existing federated algorithms. We perform extensive
experiments on multiple FL algorithms with in-distribution, real-world,
feature-skewed out-of-distribution, and quantity imbalanced datasets and show
that it provides consistent improvements, particularly in the case of
heterogeneous clients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.01251">Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods. (arXiv:2206.01251v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuchen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1">Aristide Baratin</a>, <a href="http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1">Romain Laroche</a>, <a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a>, <a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1">Alessandro Sordoni</a></p>
<p>We address the problem of evaluating the quality of self-supervised learning
(SSL) models without access to supervised labels, while being agnostic to the
architecture, learning algorithm or data manipulation used during training. We
argue that representations can be evaluated through the lens of expressiveness
and learnability. We propose to use the Intrinsic Dimension (ID) to assess
expressiveness and introduce Cluster Learnability (CL) to assess learnability.
CL is measured in terms of the performance of a KNN classifier trained to
predict labels obtained by clustering the representations with K-means. We thus
combine CL and ID into a single predictor -- CLID. Through a large-scale
empirical study with a diverse family of SSL algorithms, we find that CLID
better correlates with in-distribution model performance than other competing
recent evaluation schemes. We also benchmark CLID on out-of-domain
generalization, where CLID serves as a predictor of the transfer performance of
SSL models on several visual classification tasks, yielding improvements with
respect to the competing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.06807">The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Daphne Wang</a> (University College London), <a href="http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1">Mehrnoosh Sadrzadeh</a> (University College London)</p>
<p>Ambiguity is a natural language phenomenon occurring at different levels of
syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,
for instance, we have a variety of competing studies for the human
disambiguation processes. These studies are empirical and based on eye-tracking
measurements. Here we take first steps towards formalizing these processes for
semantic ambiguities where we identified the presence of two features: (1)
joint plausibility degrees of different possible interpretations, (2) causal
structures according to which certain words play a more substantial role in the
processes. The novel sheaf-theoretic model of definite causality developed by
Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these
features. We applied this theory to a dataset of ambiguous phrases extracted
from Psycholinguistics literature and their human plausibility judgements
collected by us using the Amazon Mechanical Turk engine. We measured the causal
fractions of different disambiguation orders within the phrases and discovered
two prominent orders: from subject to verb in the subject-verb and from object
to verb in the verb object phrases. We also found evidence for delay in the
disambiguation of polysemous vs homonymous verbs, again compatible with
Psycholinguistic findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.04053">On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Binkyte_R/0/1/0/all/0/1">R&#x16b;ta Binkyt&#x117;</a>, <a href="http://arxiv.org/find/cs/1/au:+Grozdanovski_L/0/1/0/all/0/1">Ljupcho Grozdanovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhioua_S/0/1/0/all/0/1">Sami Zhioua</a></p>
<p>Besides its common use cases in epidemiology, political, and social sciences,
causality turns out to be crucial in evaluating the fairness of automated
decisions, both in a legal and everyday sense. We provide arguments and
examples, of why causality is particularly important for fairness evaluation.
In particular, we point out the social impact of non-causal predictions and the
legal anti-discrimination process that relies on causal claims. We conclude
with a discussion about the challenges and limitations of applying causality in
practical scenarios as well as possible solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10851">Reward is not Necessary: How to Create a Modular &amp; Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ringstrom_T/0/1/0/all/0/1">Thomas J. Ringstrom</a></p>
<p>Reinforcement Learning views the maximization of rewards and avoidance of
punishments as central to explaining goal-directed behavior. However, over a
life, organisms will need to learn about many different aspects of the world's
structure: the states of the world and state-vector transition dynamics. The
number of combinations of states grows exponentially as an agent incorporates
new knowledge, and there is no obvious weighted combination of pre-existing
rewards or costs defined for a given combination of states, as such a weighting
would need to encode information about good and bad combinations prior to an
agent's experience in the world. Therefore, we must develop more naturalistic
accounts of behavior and motivation in large state-spaces. We show that it is
possible to use only the intrinsic motivation metric of empowerment, which
measures the agent's capacity to realize many possible futures under a
transition operator. We propose to scale empowerment to hierarchical
state-spaces by using Operator Bellman Equations. These equations produce
state-time feasibility functions, which are compositional hierarchical
state-time transition operators that map an initial state and time when an
agent begins a policy to the final states and times of completing a goal.
Because these functions are hierarchical operators we can define hierarchical
empowerment measures on them. An agent can then optimize plans to distant
states and times to maximize its hierarchical empowerment-gain, allowing it to
discover goals that bring about a more favorable coupling of its internal
structure (physiological states) to its external environment (world structure &amp;
spatial state). Life-long agents could therefore be primarily animated by
principles of compositionality and empowerment, exhibiting self-concern for the
growth &amp; maintenance of their own structural integrity without recourse to
reward-maximization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11483">Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1">Matthew Shardlow</a>, <a href="http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1">Piotr Przyby&#x142;a</a></p>
<p>This work is intended as a voice in the discussion over previous claims that
a pretrained large language model (LLM) based on the Transformer model
architecture can be sentient. Such claims have been made concerning the LaMDA
model and also concerning the current wave of LLM-powered chatbots, such as
ChatGPT. This claim, if confirmed, would have serious ramifications in the
Natural Language Processing (NLP) community due to wide-spread use of similar
models. However, here we take the position that such a large language model
cannot be sentient, or conscious, and that LaMDA in particular exhibits no
advances over other similar models that would qualify it. We justify this by
analysing the Transformer architecture through Integrated Information Theory of
consciousness. We see the claims of sentience as part of a wider tendency to
use anthropomorphic language in NLP reporting. Regardless of the veracity of
the claims, we consider this an opportune moment to take stock of progress in
language modelling and consider the ethical implications of the task. In order
to make this work helpful for readers outside the NLP community, we also
present the necessary background in language modelling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10405">Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hyper network to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11562">Arbitrariness and Prediction: The Confounding Role of Variance in Fair Classification. (arXiv:2301.11562v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1">A. Feder Cooper</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Katherine Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choksi_M/0/1/0/all/0/1">Madiha Choksi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barocas_S/0/1/0/all/0/1">Solon Barocas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1">Christopher De Sa</a>, <a href="http://arxiv.org/find/cs/1/au:+Grimmelmann_J/0/1/0/all/0/1">James Grimmelmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1">Jon Kleinberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1">Siddhartha Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Baobao Zhang</a></p>
<p>Variance in predictions across different trained models is a significant,
under-explored source of error in fair binary classification. In practice, the
variance on some data examples is so large that decisions can be effectively
arbitrary. To investigate this problem, we take an experimental approach and
make four overarching contributions: We: 1) Define a metric called
self-consistency, derived from variance, which we use as a proxy for measuring
and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains
from classification when a prediction would be arbitrary; 3) Conduct the
largest to-date empirical study of the role of variance (vis-a-vis
self-consistency and arbitrariness) in fair binary classification; and, 4)
Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA)
datasets easily usable for future research. Altogether, our experiments reveal
shocking insights about the reliability of conclusions on benchmark datasets.
Most fair binary classification benchmarks are close-to-fair when taking into
account the amount of arbitrariness present in predictions -- before we even
try to apply any fairness interventions. This finding calls into question the
practical utility of common algorithmic fairness methods, and in turn suggests
that we should reconsider how we choose to measure fairness in binary
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14516">OVeNet: Offset Vector Network for Semantic Segmentation. (arXiv:2303.14516v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alexandropoulos_S/0/1/0/all/0/1">Stamatis Alexandropoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1">Christos Sakaridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1">Petros Maragos</a></p>
<p>Semantic segmentation is a fundamental task in visual scene understanding. We
focus on the supervised setting, where ground-truth semantic annotations are
available. Based on knowledge about the high regularity of real-world scenes,
we propose a method for improving class predictions by learning to selectively
exploit information from neighboring pixels. In particular, our method is based
on the prior that for each pixel, there is a seed pixel in its close
neighborhood sharing the same prediction with the former. Motivated by this
prior, we design a novel two-head network, named Offset Vector Network
(OVeNet), which generates both standard semantic predictions and a dense 2D
offset vector field indicating the offset from each pixel to the respective
seed pixel, which is used to compute an alternative, seed-based semantic
prediction. The two predictions are adaptively fused at each pixel using a
learnt dense confidence map for the predicted offset vector field. We supervise
offset vectors indirectly via optimizing the seed-based prediction and via a
novel loss on the confidence map. Compared to the baseline state-of-the-art
architectures HRNet and HRNet+OCR on which OVeNet is built, the latter achieves
significant performance gains on three prominent benchmarks for semantic
segmentation, namely Cityscapes, ACDC and ADE20K. Code is available at
https://github.com/stamatisalex/OVeNet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08703">Schema-adaptable Knowledge Graph Construction. (arXiv:2305.08703v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Hongbin Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1">Honghao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>Conventional Knowledge Graph Construction (KGC) approaches typically follow
the static information extraction paradigm with a closed set of pre-defined
schema. As a result, such approaches fall short when applied to dynamic
scenarios or domains, whereas a new type of knowledge emerges. This
necessitates a system that can handle evolving schema automatically to extract
information for KGC. To address this need, we propose a new task called
schema-adaptable KGC, which aims to continually extract entity, relation, and
event based on a dynamically changing schema graph without re-training. We
first split and convert existing datasets based on three principles to build a
benchmark, i.e., horizontal schema expansion, vertical schema expansion, and
hybrid schema expansion; then investigate the schema-adaptable performance of
several well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We
further propose a simple yet effective baseline dubbed \textsc{AdaKGC}, which
contains schema-enriched prefix instructor and schema-conditioned dynamic
decoding to better handle evolving schema. Comprehensive experimental results
illustrate that AdaKGC can outperform baselines but still have room for
improvement. We hope the proposed work can deliver benefits to the community.
Code and datasets available at https://github.com/zjunlp/AdaKGC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09863">Explaining black box text modules in natural language with language models. (arXiv:2305.09863v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1">Chandan Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_A/0/1/0/all/0/1">Aliyah R. Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1">Richard Antonello</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Shailee Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1">Alexander G. Huth</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>Large language models (LLMs) have demonstrated remarkable prediction
performance for a growing array of tasks. However, their rapid proliferation
and increasing opaqueness have created a growing need for interpretability.
Here, we ask whether we can automatically obtain natural language explanations
for black box text modules. A "text module" is any function that maps text to a
scalar continuous value, such as a submodule within an LLM or a fitted model of
a brain region. "Black box" indicates that we only have access to the module's
inputs/outputs.
</p>
<p>We introduce Summarize and Score (SASC), a method that takes in a text module
and returns a natural language explanation of the module's selectivity along
with a score for how reliable the explanation is. We study SASC in 3 contexts.
First, we evaluate SASC on synthetic modules and find that it often recovers
ground truth explanations. Second, we use SASC to explain modules found within
a pre-trained BERT model, enabling inspection of the model's internals.
Finally, we show that SASC can generate explanations for the response of
individual fMRI voxels to language stimuli, with potential applications to
fine-grained brain mapping. All code for using SASC and reproducing results is
made available on Github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13062">GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1">Yuan Sui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mengyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingjie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Large language models (LLMs) are becoming attractive as few-shot reasoners to
solve Natural Language (NL)-related tasks. However, there is still much to
learn about how well LLMs understand structured data, such as tables. While it
is true that tables can be used as inputs to LLMs with serialization, there is
a lack of comprehensive studies examining whether LLMs can truly comprehend
such data. In this paper, we try to understand this by designing a benchmark to
evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark
we create includes seven tasks, each with its own unique challenges, \eg, cell
lookup, row retrieval, and size detection. We conduct a series of evaluations
on GPT-3.5 and GPT-4. We find that the performance varied depending on several
input choices, including table input format, content order, role prompting, and
partition marks. Drawing from the insights gained through the benchmark
evaluations, we propose \textit{self-augmentation} for effective structural
prompting, such as critical value / range identification using LLMs' internal
knowledge. When combined with carefully chosen input choices, these structural
prompting methods lead to promising improvements in LLM performance on a
variety of tabular tasks, \eg, TabFact($\uparrow2.31\%$),
HybridQA($\uparrow2.13\%$), SQA($\uparrow2.72\%$), Feverous($\uparrow0.84\%$),
and ToTTo($\uparrow5.68\%$). We believe that our benchmark and proposed
prompting methods can serve as a simple yet generic selection for future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14463">ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment. (arXiv:2305.14463v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1">Tarek Naous</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryan_M/0/1/0/all/0/1">Michael J. Ryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavrouk_A/0/1/0/all/0/1">Anton Lavrouk</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1">Mohit Chandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a></p>
<p>We present a systematic study and comprehensive evaluation of large language
models for automatic multilingual readability assessment. In particular, we
construct ReadMe++, a multilingual multi-domain dataset with human annotations
of 9757 sentences in Arabic, English, French, Hindi, and Russian collected from
112 different data sources. ReadMe++ offers more domain and language diversity
than existing readability datasets, making it ideal for benchmarking
multilingual and non-English language models (including mBERT, XLM-R, mT5,
Llama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot prompting
settings. Our experiments reveal that models fine-tuned on ReadMe++ outperform
those trained on single-domain datasets, showcasing superior performance on
multi-domain readability assessment and cross-lingual transfer capabilities. We
also compare to traditional readability metrics (such as Flesch-Kincaid Grade
Level and Open Source Metric for Measuring Arabic Narratives), as well as the
state-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will make
our data and code publicly available at: https://github.com/tareknaous/readme.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14613">Selectively Answering Ambiguous Questions. (arXiv:2305.14613v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1">Jeremy R. Cole</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Michael J.Q. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1">Daniel Gillick</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1">Julian Martin Eisenschlos</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1">Bhuwan Dhingra</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1">Jacob Eisenstein</a></p>
<p>Trustworthy language models should abstain from answering questions when they
do not know the answer. However, the answer to a question can be unknown for a
variety of reasons. Prior research has focused on the case in which the
question is clear and the answer is unambiguous but possibly unknown, but the
answer to a question can also be unclear due to uncertainty of the questioner's
intent or context. We investigate question answering from this perspective,
focusing on answering a subset of questions with a high degree of accuracy,
from a set of questions in which many are inherently ambiguous. In this
setting, we find that the most reliable approach to decide when to abstain
involves quantifying repetition within sampled model outputs, rather than the
model's likelihood or self-verification as used in prior work. We find this to
be the case across different types of uncertainty and model scales,and with or
without instruction tuning. Our results suggest that sampling-based confidence
scores help calibrate answers to relatively unambiguous questions, with more
dramatic improvements on ambiguous questions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17208">A Categorical Representation Language and Computational System for Knowledge-Based Planning. (arXiv:2305.17208v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aguinaldo_A/0/1/0/all/0/1">Angeline Aguinaldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Patterson_E/0/1/0/all/0/1">Evan Patterson</a>, <a href="http://arxiv.org/find/cs/1/au:+Fairbanks_J/0/1/0/all/0/1">James Fairbanks</a>, <a href="http://arxiv.org/find/cs/1/au:+Regli_W/0/1/0/all/0/1">William Regli</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_J/0/1/0/all/0/1">Jaime Ruiz</a></p>
<p>Classical planning representation languages based on first-order logic have
preliminarily been used to model and solve robotic task planning problems.
Wider adoption of these representation languages, however, is hindered by the
limitations present when managing implicit world changes with concise action
models. To address this problem, we propose an alternative approach to
representing and managing updates to world states during planning. Based on the
category-theoretic concepts of $\mathsf{C}$-sets and double-pushout rewriting
(DPO), our proposed representation can effectively handle structured knowledge
about world states that support domain abstractions at all levels. It
formalizes the semantics of predicates according to a user-provided ontology
and preserves the semantics when transitioning between world states. This
method provides a formal semantics for using knowledge graphs and relational
databases to model world states and updates in planning. In this paper, we
conceptually compare our category-theoretic representation with the classical
planning representation. We show that our proposed representation has
advantages over the classical representation in terms of handling implicit
preconditions and effects, and provides a more structured framework in which to
model and solve planning problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03530">RLtools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eschmann_J/0/1/0/all/0/1">Jonas Eschmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Albani_D/0/1/0/all/0/1">Dario Albani</a>, <a href="http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1">Giuseppe Loianno</a></p>
<p>Deep Reinforcement Learning (RL) has been demonstrated to yield capable
agents and control policies in several domains but is commonly plagued by
prohibitively long training times. Additionally, in the case of continuous
control problems, the applicability of learned policies on real-world embedded
devices is limited due to the lack of real-time guarantees and portability of
existing deep learning libraries. To address these challenges, we present
RLtools, a dependency-free, header-only, pure C++ library for deep supervised
and reinforcement learning. Leveraging the template meta-programming
capabilities of recent C++ standards, we provide composable components that can
be tightly integrated by the compiler. Its novel architecture allows RLtools to
be used seamlessly on a heterogeneous set of platforms, from HPC clusters over
workstations and laptops to smartphones, smartwatches, and microcontrollers.
Specifically, due to the tight integration of the RL algorithms with simulation
environments, RLtools can solve popular RL problems like the Pendulum-v1
swing-up about 7 to 15 times faster in terms of wall-clock training time
compared to other popular RL frameworks when using TD3. We also provide a
low-overhead and parallelized interface to the MuJoCo simulator, showing that
our PPO implementation achieves state of the art returns in the Ant-v4
environment while being 25%-30% faster in terms of wall-clock training time.
Finally, we also benchmark the policy inference on a diverse set of
microcontrollers and show that in most cases our optimized inference
implementation is much faster than even the manufacturer's DSP libraries. To
the best of our knowledge, RLtools enables the first-ever demonstration of
training a deep RL algorithm directly on a microcontroller, giving rise to the
field of TinyRL. The source code is available through our project page at
https://rl.tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11971">AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gomrokchi_M/0/1/0/all/0/1">Maziar Gomrokchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Levin_O/0/1/0/all/0/1">Owen Levin</a>, <a href="http://arxiv.org/find/cs/1/au:+Roach_J/0/1/0/all/0/1">Jeffrey Roach</a>, <a href="http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1">Jonah White</a></p>
<p>We introduce AdCraft, a novel benchmark environment for the Reinforcement
Learning (RL) community distinguished by its stochastic and non-stationary
properties. The environment simulates bidding and budgeting dynamics within
Search Engine Marketing (SEM), a digital marketing technique utilizing paid
advertising to enhance the visibility of websites on search engine results
pages (SERPs). The performance of SEM advertisement campaigns depends on
several factors, including keyword selection, ad design, bid management, budget
adjustments, and performance monitoring. Deep RL recently emerged as a
potential strategy to optimize campaign profitability within the complex and
dynamic landscape of SEM, but it requires substantial data, which may be costly
or infeasible to acquire in practice. Our customizable environment enables
practitioners to assess and enhance the robustness of RL algorithms pertinent
to SEM bid and budget management without such costs. Through a series of
experiments within the environment, we demonstrate the challenges imposed on
agent convergence and performance by sparsity and non-stationarity. We hope
these challenges further encourage discourse and development around effective
strategies for managing real-world uncertainties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13333">Energy Optimization for HVAC Systems in Multi-VAV Open Offices: A Deep Reinforcement Learning Approach. (arXiv:2306.13333v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiwen Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Vital_N/0/1/0/all/0/1">Natan Vital</a>, <a href="http://arxiv.org/find/eess/1/au:+Duffy_E/0/1/0/all/0/1">Edward.Duffy</a>, <a href="http://arxiv.org/find/eess/1/au:+Razi_A/0/1/0/all/0/1">Abolfazl Razi</a></p>
<p>With more than 32% of the global energy used by commercial and residential
buildings, there is an urgent need to revisit traditional approaches to
Building Energy Management (BEM). With HVAC systems accounting for about 40% of
the total energy cost in the commercial sector, we propose a low-complexity
DRL-based model with multi-input multi-output architecture for the HVAC energy
optimization of open-plan offices, which uses only a handful of controllable
and accessible factors. The efficacy of our solution is evaluated through
extensive analysis of the overall energy consumption and thermal comfort levels
compared to a baseline system based on the existing HVAC schedule in a real
building. This comparison shows that our method achieves 37% savings in energy
consumption with minimum violation (&lt;1%) of the desired temperature range
during work hours. It takes only a total of 40 minutes for 5 epochs (about 7.75
minutes per epoch) to train a network with superior performance and covering
diverse conditions for its low-complexity architecture; therefore, it easily
adapts to changes in the building setups, weather conditions, occupancy rate,
etc. Moreover, by enforcing smoothness on the control strategy, we suppress the
frequent and unpleasant on/off transitions on HVAC units to avoid occupant
discomfort and potential damage to the system. The generalizability of our
model is verified by applying it to different building models and under various
weather conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17833">Resetting the Optimizer in Deep RL: An Empirical Study. (arXiv:2306.17833v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Asadi_K/0/1/0/all/0/1">Kavosh Asadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fakoor_R/0/1/0/all/0/1">Rasool Fakoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabach_S/0/1/0/all/0/1">Shoham Sabach</a></p>
<p>We focus on the task of approximating the optimal value function in deep
reinforcement learning. This iterative process is comprised of solving a
sequence of optimization problems where the loss function changes per
iteration. The common approach to solving this sequence of problems is to
employ modern variants of the stochastic gradient descent algorithm such as
Adam. These optimizers maintain their own internal parameters such as estimates
of the first-order and the second-order moments of the gradient, and update
them over time. Therefore, information obtained in previous iterations is used
to solve the optimization problem in the current iteration. We demonstrate that
this can contaminate the moment estimates because the optimization landscape
can change arbitrarily from one iteration to the next one. To hedge against
this negative effect, a simple idea is to reset the internal parameters of the
optimizer when starting a new iteration. We empirically investigate this
resetting idea by employing various optimizers in conjunction with the Rainbow
algorithm. We demonstrate that this simple modification significantly improves
the performance of deep RL on the Atari benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02185">Citation: A Key to Building Responsible and Accountable Large Language Models. (arXiv:2307.02185v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kevin Chen-Chuan Chang</a></p>
<p>Large Language Models (LLMs) bring transformative benefits alongside unique
challenges, including intellectual property (IP) and ethical concerns. This
position paper explores a novel angle to mitigate these risks, drawing
parallels between LLMs and established web systems. We identify "citation" -
the acknowledgement or reference to a source or evidence - as a crucial yet
missing component in LLMs. Incorporating citation could enhance content
transparency and verifiability, thereby confronting the IP and ethical issues
in the deployment of LLMs. We further propose that a comprehensive citation
mechanism for LLMs should account for both non-parametric and parametric
content. Despite the complexity of implementing such a citation mechanism,
along with the potential pitfalls, we advocate for its development. Building on
this foundation, we outline several research problems in this area, aiming to
guide future explorations towards building more responsible and accountable
LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06507">Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models. (arXiv:2307.06507v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hardy_R/0/1/0/all/0/1">Romain Hardy</a>, <a href="http://arxiv.org/find/cs/1/au:+Klepich_J/0/1/0/all/0/1">Joe Klepich</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1">Ryan Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1">Steve Hall</a>, <a href="http://arxiv.org/find/cs/1/au:+Villareal_J/0/1/0/all/0/1">Jericho Villareal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilin_C/0/1/0/all/0/1">Cornelia Ilin</a></p>
<p>Integrating deep learning with clinical expertise holds great potential for
addressing healthcare challenges and empowering medical professionals with
improved diagnostic tools. However, the need for annotated medical images is
often an obstacle to leveraging the full power of machine learning models. Our
research demonstrates that by combining synthetic images, generated using
diffusion models, with real images, we can enhance nonalcoholic fatty liver
disease (NAFLD) classification performance even in low-data regime settings. We
evaluate the quality of the synthetic images by comparing two metrics:
Inception Score (IS) and Fr\'{e}chet Inception Distance (FID), computed on
diffusion- and generative adversarial network (GAN)-generated images. Our
results show superior performance for the diffusion-generated images, with a
maximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score
of $69.45$ compared to $100.05$ for GANs. Utilizing a partially frozen CNN
backbone (EfficientNet v1), our synthetic augmentation method achieves a
maximum image-level ROC AUC of $0.904$ on a NAFLD prediction task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10103">ASPIRE: Language-Guided Augmentation for Robust Image Classification. (arXiv:2308.10103v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Sreyan Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Evuru_C/0/1/0/all/0/1">Chandra Kiran Reddy Evuru</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sonal Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1">Utkarsh Tyagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sakshi Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1">Sanjoy Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1">Dinesh Manocha</a></p>
<p>Neural image classifiers can often learn to make predictions by overly
relying on non-predictive features that are spuriously correlated with the
class labels in the training data. This leads to poor performance in real-world
atypical scenarios where such features are absent. Supplementing the training
dataset with images without such spurious features can aid robust learning
against spurious correlations via better generalization. This paper presents
ASPIRE (Language-guided data Augmentation for SPurIous correlation REmoval), a
simple yet effective solution for expanding the training dataset with synthetic
images without spurious features. ASPIRE, guided by language, generates these
images without requiring any form of additional supervision or existing
examples. Precisely, we employ LLMs to first extract foreground and background
features from textual descriptions of an image, followed by advanced
language-guided image editing to discover the features that are spuriously
correlated with the class label. Finally, we personalize a text-to-image
generation model to generate diverse in-domain images without spurious
features. We demonstrate the effectiveness of ASPIRE on 4 datasets, including
the very challenging Hard ImageNet dataset, and 9 baselines and show that
ASPIRE improves the classification accuracy of prior methods by 1% - 38%. Code
soon at: https://github.com/Sreyan88/ASPIRE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11809">Expressive probabilistic sampling in recurrent neural networks. (arXiv:2308.11809v3 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1">Shirui Chen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jiang_L/0/1/0/all/0/1">Linxing Preston Jiang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Rao_R/0/1/0/all/0/1">Rajesh P. N. Rao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Shea_Brown_E/0/1/0/all/0/1">Eric Shea-Brown</a></p>
<p>In sampling-based Bayesian models of brain function, neural activities are
assumed to be samples from probability distributions that the brain uses for
probabilistic computation. However, a comprehensive understanding of how
mechanistic models of neural dynamics can sample from arbitrary distributions
is still lacking. We use tools from functional analysis and stochastic
differential equations to explore the minimum architectural requirements for
$\textit{recurrent}$ neural circuits to sample from complex distributions. We
first consider the traditional sampling model consisting of a network of
neurons whose outputs directly represent the samples (sampler-only network). We
argue that synaptic current and firing-rate dynamics in the traditional model
have limited capacity to sample from a complex probability distribution. We
show that the firing rate dynamics of a recurrent neural circuit with a
separate set of output units can sample from an arbitrary probability
distribution. We call such circuits reservoir-sampler networks (RSNs). We
propose an efficient training procedure based on denoising score matching that
finds recurrent and output weights such that the RSN implements Langevin
sampling. We empirically demonstrate our model's ability to sample from several
complex data distributions using the proposed neural dynamics and discuss its
applicability to developing the next generation of sampling-based brain models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15452">When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1">Zhen Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yinuo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guozhou Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>In the realm of embodied artificial intelligence, the reasoning capabilities
of Large Language Models (LLMs) play a pivotal role. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05310">ImitationNet: Unsupervised Human-to-Robot Motion Retargeting via Shared Latent Space. (arXiv:2309.05310v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yashuai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascaro_E/0/1/0/all/0/1">Esteve Valls Mascaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongheui Lee</a></p>
<p>This paper introduces a novel deep-learning approach for human-to-robot
motion retargeting, enabling robots to mimic human poses accurately. Contrary
to prior deep-learning-based works, our method does not require paired
human-to-robot data, which facilitates its translation to new robots. First, we
construct a shared latent space between humans and robots via adaptive
contrastive learning that takes advantage of a proposed cross-domain similarity
metric between the human and robot poses. Additionally, we propose a
consistency term to build a common latent space that captures the similarity of
the poses with precision while allowing direct robot motion control from the
latent space. For instance, we can generate in-between motion through simple
linear interpolation between two projected human poses. We conduct a
comprehensive evaluation of robot control from diverse modalities (i.e., texts,
RGB videos, and key poses), which facilitates robot control for non-expert
users. Our model outperforms existing works regarding human-to-robot
retargeting in terms of efficiency and precision. Finally, we implemented our
method in a real robot with self-collision avoidance through a whole-body
controller to showcase the effectiveness of our approach. More information on
our website https://evm7.github.io/UnsH2R/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06038">GraspGF: Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping. (arXiv:2309.06038v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tianhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Mingdong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yunchong Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a></p>
<p>The use of anthropomorphic robotic hands for assisting individuals in
situations where human hands may be unavailable or unsuitable has gained
significant importance. In this paper, we propose a novel task called
human-assisting dexterous grasping that aims to train a policy for controlling
a robotic hand's fingers to assist users in grasping objects. Unlike
conventional dexterous grasping, this task presents a more complex challenge as
the policy needs to adapt to diverse user intentions, in addition to the
object's geometry. We address this challenge by proposing an approach
consisting of two sub-modules: a hand-object-conditional grasping primitive
called Grasping Gradient Field~(GraspGF), and a history-conditional residual
policy. GraspGF learns `how' to grasp by estimating the gradient from a success
grasping example set, while the residual policy determines `when' and at what
speed the grasping action should be executed based on the trajectory history.
Experimental results demonstrate the superiority of our proposed method
compared to baselines, highlighting the user-awareness and practicality in
real-world applications. The codes and demonstrations can be viewed at
"https://sites.google.com/view/graspgf".
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12632">Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?. (arXiv:2309.12632v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ashames_M/0/1/0/all/0/1">Mohamad M.A. Ashames</a>, <a href="http://arxiv.org/find/cs/1/au:+Demir_A/0/1/0/all/0/1">Ahmet Demir</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerek_O/0/1/0/all/0/1">Omer N. Gerek</a>, <a href="http://arxiv.org/find/cs/1/au:+Fidan_M/0/1/0/all/0/1">Mehmet Fidan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulmezoglu_M/0/1/0/all/0/1">M. Bilginer Gulmezoglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ergin_S/0/1/0/all/0/1">Semih Ergin</a>, <a href="http://arxiv.org/find/cs/1/au:+Koc_M/0/1/0/all/0/1">Mehmet Koc</a>, <a href="http://arxiv.org/find/cs/1/au:+Barkana_A/0/1/0/all/0/1">Atalay Barkana</a>, <a href="http://arxiv.org/find/cs/1/au:+Calisir_C/0/1/0/all/0/1">Cuneyt Calisir</a></p>
<p>Following the great success of various deep learning methods in image and
object classification, the biomedical image processing society is also
overwhelmed with their applications to various automatic diagnosis cases.
Unfortunately, most of the deep learning-based classification attempts in the
literature solely focus on the aim of extreme accuracy scores, without
considering interpretability, or patient-wise separation of training and test
data. For example, most lung nodule classification papers using deep learning
randomly shuffle data and split it into training, validation, and test sets,
causing certain images from the CT scan of a person to be in the training set,
while other images of the exact same person to be in the validation or testing
image sets. This can result in reporting misleading accuracy rates and the
learning of irrelevant features, ultimately reducing the real-life usability of
these models. When the deep neural networks trained on the traditional, unfair
data shuffling method are challenged with new patient images, it is observed
that the trained models perform poorly. In contrast, deep neural networks
trained with strict patient-level separation maintain their accuracy rates even
when new patient images are tested. Heat-map visualizations of the activations
of the deep neural networks trained with strict patient-level separation
indicate a higher degree of focus on the relevant nodules. We argue that the
research question posed in the title has a positive answer only if the deep
neural networks are trained with images of patients that are strictly isolated
from the validation and testing patient sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00239">AdaptNet: Policy Adaptation for Physics-Based Character Control. (arXiv:2310.00239v3 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Pei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kaixiang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrews_S/0/1/0/all/0/1">Sheldon Andrews</a>, <a href="http://arxiv.org/find/cs/1/au:+Kry_P/0/1/0/all/0/1">Paul G. Kry</a>, <a href="http://arxiv.org/find/cs/1/au:+Neff_M/0/1/0/all/0/1">Michael Neff</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuire_M/0/1/0/all/0/1">Morgan McGuire</a>, <a href="http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1">Ioannis Karamouzas</a>, <a href="http://arxiv.org/find/cs/1/au:+Zordan_V/0/1/0/all/0/1">Victor Zordan</a></p>
<p>Motivated by humans' ability to adapt skills in the learning of new ones,
this paper presents AdaptNet, an approach for modifying the latent space of
existing policies to allow new behaviors to be quickly learned from like tasks
in comparison to learning from scratch. Building on top of a given
reinforcement learning controller, AdaptNet uses a two-tier hierarchy that
augments the original state embedding to support modest changes in a behavior
and further modifies the policy network layers to make more substantive
changes. The technique is shown to be effective for adapting existing
physics-based controllers to a wide range of new styles for locomotion, new
task targets, changes in character morphology and extensive changes in
environment. Furthermore, it exhibits significant increase in learning
efficiency, as indicated by greatly reduced training times when compared to
training from scratch or using other approaches that modify existing policies.
Code is available at https://motion-lab.github.io/AdaptNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07570">ChatGPT for Computational Topology. (arXiv:2310.07570v3 [math.AT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1">Jian Liu</a>, <a href="http://arxiv.org/find/math/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/math/1/au:+Wei_G/0/1/0/all/0/1">Guo-Wei Wei</a></p>
<p>ChatGPT represents a significant milestone in the field of artificial
intelligence (AI), finding widespread applications across diverse domains.
However, its effectiveness in mathematical contexts has been somewhat
constrained by its susceptibility to conceptual errors. Concurrently,
topological data analysis (TDA), a relatively new discipline, has garnered
substantial interest in recent years. Nonetheless, the advancement of TDA is
impeded by the limited understanding of computational algorithms and coding
proficiency among theoreticians. This work endeavors to bridge the gap between
theoretical topological concepts and their practical implementation in
computational topology through the utilization of ChatGPT. We showcase how a
pure theoretician, devoid of computational experience and coding skills, can
effectively transform mathematical formulations and concepts into functional
code for computational topology with the assistance of ChatGPT. Our strategy
outlines a productive process wherein a mathematician trains ChatGPT on pure
mathematical concepts, steers ChatGPT towards generating computational topology
code, and subsequently validates the generated code using established examples.
Our specific case studies encompass the computation of Betti numbers, Laplacian
matrices, and Dirac matrices for simplicial complexes, as well as the
persistence of various homologies and Laplacians. Furthermore, we explore the
application of ChatGPT in computing recently developed topological theories for
hypergraphs and digraphs. This work serves as an initial step towards
effectively transforming pure mathematical theories into practical
computational tools, with the ultimate goal of enabling real applications
across diverse fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11305">MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games. (arXiv:2310.11305v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Ti-Rong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guei_H/0/1/0/all/0/1">Hung Guei</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Po-Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1">Pei-Chiun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1">Ting Han Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Shih_C/0/1/0/all/0/1">Chung-Chin Shih</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yun-Jui Tsai</a></p>
<p>This paper presents MiniZero, a zero-knowledge learning framework that
supports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel
AlphaZero, and Gumbel MuZero. While these algorithms have demonstrated
super-human performance in many games, it remains unclear which among them is
most suitable or efficient for specific tasks. Through MiniZero, we
systematically evaluate the performance of each algorithm in two board games,
9x9 Go and 8x8 Othello, as well as 57 Atari games. For two board games, using
more simulations generally results in higher performance. However, the choice
of AlphaZero and MuZero may differ based on game properties. For Atari games,
both MuZero and Gumbel MuZero are worth considering. Since each game has unique
characteristics, different algorithms and simulations yield varying results. In
addition, we introduce an approach, called progressive simulation, which
progressively increases the simulation budget during training to allocate
computation more efficiently. Our empirical results demonstrate that
progressive simulation achieves significantly superior performance in two board
games. By making our framework and trained models publicly available, this
paper contributes a benchmark for future research on zero-knowledge learning
algorithms, assisting researchers in algorithm selection and comparison against
these zero-knowledge learning baselines. Our code and data are available at
https://rlg.iis.sinica.edu.tw/papers/minizero.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12342">Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs&#x27; Non-linear Thinking. (arXiv:2310.12342v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yongqi Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sizhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Simeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a></p>
<p>Chain-of-Thought(CoT) prompting and its variants explore equipping large
language models (LLMs) with high-level reasoning abilities by emulating
human-like linear cognition and logic. However, the human mind is complicated
and mixed with both linear and nonlinear thinking. In this work, we propose
\textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel
prompting that combines the principles of elimination and inference in order to
guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize
Natural Language Inference (NLI) to deduce each possible solution's entailment
relation with context, commonsense, or facts, therefore yielding a broader
perspective by thinking back for inferring. This forward planning and backward
eliminating process allows IEP to better simulate the complex human thinking
processes compared to other CoT-based methods, which only reflect linear
cognitive processes. We conducted a series of empirical studies and have
corroborated that IEP consistently outperforms CoT across various tasks.
Additionally, we observe that integrating IEP and CoT further improves the
LLMs' performance on certain tasks, highlighting the necessity of equipping
LLMs with mixed logic processes. Moreover, to better evaluate comprehensive
features inherent in human logic, we introduce \textbf{M}ental-\textbf{A}bility
\textbf{R}easoning \textbf{B}enchmark (MARB). The benchmark comprises six novel
subtasks with a total of 9,115 questions, among which 1,685 are developed with
hand-crafted rationale references. We believe both \textsc{IEP} and
\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and
verbal reasoning abilities and drive further advancements. \textsc{MARB} will
be available at ~\texttt{anonymity link} soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12963">AutoMix: Automatically Mixing Language Models. (arXiv:2310.12963v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1">Aman Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1">Pranjal Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Ankit Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Potharaju_S/0/1/0/all/0/1">Srividya Pranavi Potharaju</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aditya Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1">Dheeraj Rajagopal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kappaganthu_K/0/1/0/all/0/1">Karthik Kappaganthu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1">Shyam Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a>, <a href="http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1">Manaal Faruqui</a></p>
<p>Large language models (LLMs) are now available in various sizes and
configurations from cloud API providers. While this diversity offers a broad
spectrum of choices, effectively leveraging the options to optimize
computational cost and performance remains challenging. In this work, we
present AutoMix, an approach that strategically routes queries to larger LMs,
based on the approximate correctness of outputs from a smaller LM. Central to
AutoMix is a few-shot self-verification mechanism, which estimates the
reliability of its own outputs without requiring training. Given that
verifications can be noisy, we employ a meta verifier in AutoMix to refine the
accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five
context-grounded reasoning datasets demonstrate that AutoMix surpasses
established baselines, improving the incremental benefit per cost by up to 89%.
Our code and data are available at https://github.com/automix-llm/automix.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13007">A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deck_L/0/1/0/all/0/1">Luca Deck</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1">Jakob Schoeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1">Maria De-Arteaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1">Niklas K&#xfc;hl</a></p>
<p>In this critical survey, we analyze typical claims on the relationship
between explainable AI (XAI) and fairness to disentangle the multidimensional
relationship between these two concepts. Based on a systematic literature
review and a subsequent qualitative content analysis, we identify seven
archetypal claims from 175 papers on the alleged fairness benefits of XAI. We
present crucial caveats with respect to these claims and provide an entry point
for future discussions around the potentials and limitations of XAI for
specific fairness desiderata. While the literature often suggests XAI to be an
enabler for several fairness desiderata, we notice a divide between these
desiderata and the capabilities of XAI. We encourage to conceive XAI as one of
many tools to approach the multidimensional, sociotechnical challenge of
algorithmic fairness and to be more specific about how exactly what kind of XAI
method enables whom to address which fairness desideratum.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14421">On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Horenko_I/0/1/0/all/0/1">Illia Horenko</a></p>
<p>Simply-verifiable mathematical conditions for existence, uniqueness and
explicit analytical computation of minimal adversarial paths (MAP) and minimal
adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for
generalized linear models (GLM), and for entropic AI (EAI) are formulated and
proven. Practical computation of MAP and MAD, their comparison and
interpretations for various classes of AI tools (for neuronal networks, boosted
random forests, GLM and EAI) are demonstrated on the common synthetic
benchmarks: on a double Swiss roll spiral and its extensions, as well as on the
two biomedical data problems (for the health insurance claim predictions, and
for the heart attack lethality classification). On biomedical applications it
is demonstrated how MAP provides unique minimal patient-specific
risk-mitigating interventions in the predefined subsets of accessible control
variables.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17658">Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peiwen_Y/0/1/0/all/0/1">Yuan Peiwen</a>, <a href="http://arxiv.org/find/cs/1/au:+Changsheng_Z/0/1/0/all/0/1">Zhu Changsheng</a></p>
<p>There has been an emergence of various models for long-term time series
forecasting. Recent studies have demonstrated that a single linear layer, using
Channel Dependent (CD) or Channel Independent (CI) modeling, can even
outperform a large number of sophisticated models. However, current research
primarily considers CD and CI as two complementary yet mutually exclusive
approaches, unable to harness these two extremes simultaneously. And it is also
a challenging issue that both CD and CI are static strategies that cannot be
determined to be optimal for a specific dataset without extensive experiments.
In this paper, we reconsider whether the current CI strategy is the best
solution for time series forecasting. First, we propose a simple yet effective
strategy called CSC, which stands for $\mathbf{C}$hannel
$\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel
Self-Clustering (CSC) enhances CI strategy's performance improvements while
reducing parameter size, for exmpale by over 10 times on electricity dataset,
and significantly cutting training time. Second, we further propose Channel
Rearrangement (CR), a method for deep models inspired by the self-clustering.
CR attains competitive performance against baselines. Finally, we also discuss
whether it is best to forecast the future values using the historical values of
the same channel as inputs. We hope our findings and methods could inspire new
solutions beyond CD/CI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18301">Interactive Motion Planning for Autonomous Vehicles with Joint Optimization. (arXiv:2310.18301v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxiao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1">Sushant Veer</a>, <a href="http://arxiv.org/find/cs/1/au:+Karkus_P/0/1/0/all/0/1">Peter Karkus</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a></p>
<p>In highly interactive driving scenarios, the actions of one agent greatly
influences those of its neighbors. Planning safe motions for autonomous
vehicles in such interactive environments, therefore, requires reasoning about
the impact of the ego's intended motion plan on nearby agents' behavior.
Deep-learning-based models have recently achieved great success in trajectory
prediction and many models in the literature allow for ego-conditioned
prediction. However, leveraging ego-conditioned prediction remains challenging
in downstream planning due to the complex nature of neural networks, limiting
the planner structure to simple ones, e.g., sampling-based planner. Despite
their ability to generate fine-grained high-quality motion plans, it is
difficult for gradient-based planning algorithms, such as model predictive
control (MPC), to leverage ego-conditioned prediction due to their iterative
nature and need for gradient. We present Interactive Joint Planning (IJP) that
bridges MPC with learned prediction models in a computationally scalable manner
to provide us the best of both the worlds. In particular, IJP jointly optimizes
over the behavior of the ego and the surrounding agents and leverages
deep-learned prediction models as prediction priors that the join trajectory
optimization tries to stay close to. Furthermore, by leveraging homotopy
classes, our joint optimizer searches over diverse motion plans to avoid
getting stuck at local minima. Closed-loop simulation result shows that IJP
significantly outperforms the baselines that are either without joint
optimization or running sampling-based planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19387">Othello is Solved. (arXiv:2310.19387v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Takizawa_H/0/1/0/all/0/1">Hiroki Takizawa</a></p>
<p>The game of Othello is one of the world's most complex and popular games that
has yet to be computationally solved. Othello has roughly ten octodecillion (10
to the 58th power) possible game records and ten octillion (10 to the 28th
power) possible game positions. The challenge of solving Othello, determining
the outcome of a game with no mistake made by either player, has long been a
grand challenge in computer science. This paper announces a significant
milestone: Othello is now solved. It is computationally proved that perfect
play by both players lead to a draw. Strong Othello software has long been
built using heuristically designed search techniques. Solving a game provides a
solution that enables the software to play the game perfectly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04037">Causal Discovery Under Local Privacy. (arXiv:2311.04037v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Binkyte_R/0/1/0/all/0/1">R&#x16b;ta Binkyt&#x117;</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinzon_C/0/1/0/all/0/1">Carlos Pinz&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Lestyan_S/0/1/0/all/0/1">Szilvia Lesty&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kangsoo Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcolezi_H/0/1/0/all/0/1">H&#xe9;ber H. Arcolezi</a>, <a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1">Catuscia Palamidessi</a></p>
<p>Differential privacy is a widely adopted framework designed to safeguard the
sensitive information of data providers within a data set. It is based on the
application of controlled noise at the interface between the server that stores
and processes the data, and the data consumers. Local differential privacy is a
variant that allows data providers to apply the privatization mechanism
themselves on their data individually. Therefore it provides protection also in
contexts in which the server, or even the data collector, cannot be trusted.
The introduction of noise, however, inevitably affects the utility of the data,
particularly by distorting the correlations between individual data components.
This distortion can prove detrimental to tasks such as causal discovery. In
this paper, we consider various well-known locally differentially private
mechanisms and compare the trade-off between the privacy they provide, and the
accuracy of the causal structure produced by algorithms for causal learning
when applied to data obfuscated by these mechanisms. Our analysis yields
valuable insights for selecting appropriate local differentially private
protocols for causal discovery tasks. We foresee that our findings will aid
researchers and practitioners in conducting locally private causal discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07079">Sample Dominance Aware Framework via Non-Parametric Estimation for Spontaneous Brain-Computer Interface. (arXiv:2311.07079v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Byeong-Hoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_B/0/1/0/all/0/1">Byoung-Hee Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Deep learning has shown promise in decoding brain signals, such as
electroencephalogram (EEG), in the field of brain-computer interfaces (BCIs).
However, the non-stationary characteristics of EEG signals pose challenges for
training neural networks to acquire appropriate knowledge. Inconsistent EEG
signals resulting from these non-stationary characteristics can lead to poor
performance. Therefore, it is crucial to investigate and address sample
inconsistency to ensure robust performance in spontaneous BCIs. In this study,
we introduce the concept of sample dominance as a measure of EEG signal
inconsistency and propose a method to modulate its effect on network training.
We present a two-stage dominance score estimation technique that compensates
for performance degradation caused by sample inconsistencies. Our proposed
method utilizes non-parametric estimation to infer sample inconsistency and
assigns each sample a dominance score. This score is then aggregated with the
loss function during training to modulate the impact of sample inconsistency.
Furthermore, we design a curriculum learning approach that gradually increases
the influence of inconsistent signals during training to improve overall
performance. We evaluate our proposed method using public spontaneous BCI
dataset. The experimental results confirm that our findings highlight the
importance of addressing sample dominance for achieving robust performance in
spontaneous BCIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07861">Overview of the TREC 2023 Product Product Search Track. (arXiv:2311.07861v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1">Daniel Campos</a>, <a href="http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1">Surya Kallumadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosset_C/0/1/0/all/0/1">Corby Rosset</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1">Cheng Xiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnani_A/0/1/0/all/0/1">Alessandro Magnani</a></p>
<p>This is the first year of the TREC Product search track. The focus this year
was the creation of a reusable collection and evaluation of the impact of the
use of metadata and multi-modal data on retrieval accuracy. This year we
leverage the new product search corpus, which includes contextual metadata. Our
analysis shows that in the product search domain, traditional retrieval systems
are highly effective and commonly outperform general-purpose pretrained
embedding models. Our analysis also evaluates the impact of using simplified
and metadata-enhanced collections, finding no clear trend in the impact of the
expanded collection. We also see some surprising outcomes; despite their
widespread adoption and competitive performance on other tasks, we find
single-stage dense retrieval runs can commonly be noncompetitive or generate
low-quality results both in the zero-shot and fine-tuned domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07955">Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle Imagery: Review and Experimental Comparisons. (arXiv:2311.07955v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenjie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ryan Wen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1">Jingxiang Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruobin Gao</a></p>
<p>With the advancement of maritime unmanned aerial vehicles (UAVs) and deep
learning technologies, the application of UAV-based object detection has become
increasingly significant in the fields of maritime industry and ocean
engineering. Endowed with intelligent sensing capabilities, the maritime UAVs
enable effective and efficient maritime surveillance. To further promote the
development of maritime UAV-based object detection, this paper provides a
comprehensive review of challenges, relative methods, and UAV aerial datasets.
Specifically, in this work, we first briefly summarize four challenges for
object detection on maritime UAVs, i.e., object feature diversity, device
limitation, maritime environment variability, and dataset scarcity. We then
focus on computational methods to improve maritime UAV-based object detection
performance in terms of scale-aware, small object detection, view-aware,
rotated object detection, lightweight methods, and others. Next, we review the
UAV aerial image/video datasets and propose a maritime UAV aerial dataset named
MS2ship for ship detection. Furthermore, we conduct a series of experiments to
present the performance evaluation and robustness analysis of object detection
methods on maritime datasets. Eventually, we give the discussion and outlook on
future works for maritime UAV-based object detection. The MS2ship dataset is
available at
\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08360">The Transient Nature of Emergent In-Context Learning in Transformers. (arXiv:2311.08360v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aaditya K. Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1">Stephanie C.Y. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1">Ted Moskovitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1">Erin Grant</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1">Andrew M. Saxe</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1">Felix Hill</a></p>
<p>Transformer neural networks can exhibit a surprising capacity for in-context
learning (ICL) despite not being explicitly trained for it. Prior work has
provided a deeper understanding of how ICL emerges in transformers, e.g.
through the lens of mechanistic interpretability, Bayesian inference, or by
examining the distributional properties of training data. However, in each of
these cases, ICL is treated largely as a persistent phenomenon; namely, once
ICL emerges, it is assumed to persist asymptotically. Here, we show that the
emergence of ICL during transformer training is, in fact, often transient. We
train transformers on synthetic data designed so that both ICL and in-weights
learning (IWL) strategies can lead to correct predictions. We find that ICL
first emerges, then disappears and gives way to IWL, all while the training
loss decreases, indicating an asymptotic preference for IWL. The transient
nature of ICL is observed in transformers across a range of model sizes and
datasets, raising the question of how much to "overtrain" transformers when
seeking compact, cheaper-to-run models. We find that L2 regularization may
offer a path to more persistent ICL that removes the need for early stopping
based on ICL-style validation tasks. Finally, we present initial evidence that
ICL transience may be caused by competition between ICL and IWL circuits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08379">Scheming AIs: Will AIs fake alignment during training in order to get power?. (arXiv:2311.08379v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carlsmith_J/0/1/0/all/0/1">Joe Carlsmith</a></p>
<p>This report examines whether advanced AIs that perform well in training will
be doing so in order to gain power later -- a behavior I call "scheming" (also
sometimes called "deceptive alignment"). I conclude that scheming is a
disturbingly plausible outcome of using baseline machine learning methods to
train goal-directed AIs sophisticated enough to scheme (my subjective
probability on such an outcome, given these conditions, is roughly 25%). In
particular: if performing well in training is a good strategy for gaining power
(as I think it might well be), then a very wide variety of goals would motivate
scheming -- and hence, good training performance. This makes it plausible that
training might either land on such a goal naturally and then reinforce it, or
actively push a model's motivations towards such a goal as an easy way of
improving performance. What's more, because schemers pretend to be aligned on
tests designed to reveal their motivations, it may be quite difficult to tell
whether this has occurred. However, I also think there are reasons for comfort.
In particular: scheming may not actually be such a good strategy for gaining
power; various selection pressures in training might work against schemer-like
goals (for example, relative to non-schemers, schemers need to engage in extra
instrumental reasoning, which might harm their training performance); and we
may be able to increase such pressures intentionally. The report discusses
these and a wide variety of other considerations in detail, and it suggests an
array of empirical research directions for probing the topic further.
</p>
</p>
</div>

    </div>
    </body>
    