<!DOCTYPE html>
<html>
<head>
<title>2023-11-09-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.03365">Leveraging Generative AI: Improving Software Metadata Classification with Generated Code-Comment Pairs. (arXiv:2311.03365v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1">Samah Syed</a>, <a href="http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1">Angel Deborah S</a></p>
<p>In software development, code comments play a crucial role in enhancing code
comprehension and collaboration. This research paper addresses the challenge of
objectively classifying code comments as "Useful" or "Not Useful." We propose a
novel solution that harnesses contextualized embeddings, particularly BERT, to
automate this classification process. We address this task by incorporating
generated code and comment pairs. The initial dataset comprised 9048 pairs of
code and comments written in C, labeled as either Useful or Not Useful. To
augment this dataset, we sourced an additional 739 lines of code-comment pairs
and generated labels using a Large Language Model Architecture, specifically
BERT. The primary objective was to build classification models that can
effectively differentiate between useful and not useful code comments. Various
machine learning algorithms were employed, including Logistic Regression,
Decision Tree, K-Nearest Neighbors (KNN), Support Vector Machine (SVM),
Gradient Boosting, Random Forest, and a Neural Network. Each algorithm was
evaluated using precision, recall, and F1-score metrics, both with the original
seed dataset and the augmented dataset. This study showcases the potential of
generative AI for enhancing binary code comment quality classification models,
providing valuable insights for software developers and researchers in the
field of natural language processing and software engineering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03366">Neural Rankers for Code Generation via Inter-Cluster Modeling. (arXiv:2311.03366v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+To_H/0/1/0/all/0/1">Hung Quoc To</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1">Minh Huynh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1">Nghi D. Q. Bui</a></p>
<p>Code Large Language Models (CodeLLMs) have ushered in a new era of code
generation advancements. However, selecting the best solutions from among all
possible CodeLLM solutions remains a challenge. Previous methods frequently
overlooked the intricate functional similarities and interactions between
clusters, resulting in suboptimal results. In this work, we introduce
\textit{SRank}, a novel reranking strategy for selecting the best solution from
code generation that focuses on modeling inter-cluster relationship. By
quantifying the functional overlap between clusters, our approach provides a
better ranking strategy of code solutions. Empirical results show that our
method achieves a remarkable results on pass@1 score. For instance, on the
Human-Eval benchmark, we achieve 69.66\% in pass@1 with Codex002, 75.31\% for
WizardCoder, 53.99\% for StarCoder and 60.55\% for CodeGen, which surpass the
state-of-the-arts solution ranking methods, such as CodeT and Coder-Reviewer on
the same CodeLLM with significant margin ($\approx 6.1\%$ improvement on
average). Comparing to the random sampling method, we can achieve an average
improvement of $\approx 23.07\%$ on Human-Eval and 17.64\% on MBPP. Even in
scenarios with limited test inputs, our approach demonstrates robustness and
superiority, marking a new state-of-the-arts in code generation reranking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03369">Can We Trust the Similarity Measurement in Federated Learning?. (arXiv:2311.03369v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xukai Zou</a></p>
<p>Is it secure to measure the reliability of local models by similarity in
federated learning (FL)? This paper delves into an unexplored security threat
concerning applying similarity metrics, such as the L_2 norm, Euclidean
distance, and cosine similarity, in protecting FL. We first uncover the
deficiencies of similarity metrics that high-dimensional local models,
including benign and poisoned models, may be evaluated to have the same
similarity while being significantly different in the parameter values. We then
leverage this finding to devise a novel untargeted model poisoning attack,
Faker, which launches the attack by simultaneously maximizing the evaluated
similarity of the poisoned local model and the difference in the parameter
values. Experimental results based on seven datasets and eight defenses show
that Faker outperforms the state-of-the-art benchmark attacks by 1.1-9.0X in
reducing accuracy and 1.2-8.0X in saving time cost, which even holds for the
case of a single malicious client with limited knowledge about the FL system.
Moreover, Faker can degrade the performance of the global model by attacking
only once. We also preliminarily explore extending Faker to other attacks, such
as backdoor attacks and Sybil attacks. Lastly, we provide a model evaluation
strategy, called the similarity of partial parameters (SPP), to defend against
Faker. Given that numerous mechanisms in FL utilize similarity metrics to
assess local models, this work suggests that we should be vigilant regarding
the potential risks of using these metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03370">CMIP X-MOS: Improving Climate Models with Extreme Model Output Statistics. (arXiv:2311.03370v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Morozov_V/0/1/0/all/0/1">Vsevolod Morozov</a>, <a href="http://arxiv.org/find/physics/1/au:+Galliamov_A/0/1/0/all/0/1">Artem Galliamov</a>, <a href="http://arxiv.org/find/physics/1/au:+Lukashevich_A/0/1/0/all/0/1">Aleksandr Lukashevich</a>, <a href="http://arxiv.org/find/physics/1/au:+Kurdukova_A/0/1/0/all/0/1">Antonina Kurdukova</a>, <a href="http://arxiv.org/find/physics/1/au:+Maximov_Y/0/1/0/all/0/1">Yury Maximov</a></p>
<p>Climate models are essential for assessing the impact of greenhouse gas
emissions on our changing climate and the resulting increase in the frequency
and severity of natural disasters. Despite the widespread acceptance of climate
models produced by the Coupled Model Intercomparison Project (CMIP), they still
face challenges in accurately predicting climate extremes, which pose most
significant threats to both people and the environment. To address this
limitation and improve predictions of natural disaster risks, we introduce
Extreme Model Output Statistics (X-MOS). This approach utilizes deep regression
techniques to precisely map CMIP model outputs to real measurements obtained
from weather stations, which results in a more accurate analysis of the XXI
climate extremes. In contrast to previous research, our study places a strong
emphasis on enhancing the estimation of the tails of future climate parameter
distributions. The latter supports decision-makers, enabling them to better
assess climate-related risks across the globe.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03373">Unscrambling the Rectification of Adversarial Attacks Transferability across Computer Networks. (arXiv:2311.03373v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1">Ehsan Nowroozi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghelichkhani_S/0/1/0/all/0/1">Samaneh Ghelichkhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Haider_I/0/1/0/all/0/1">Imran Haider</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehghantanha_A/0/1/0/all/0/1">Ali Dehghantanha</a></p>
<p>Convolutional neural networks (CNNs) models play a vital role in achieving
state-of-the-art performances in various technological fields. CNNs are not
limited to Natural Language Processing (NLP) or Computer Vision (CV) but also
have substantial applications in other technological domains, particularly in
cybersecurity. The reliability of CNN's models can be compromised because of
their susceptibility to adversarial attacks, which can be generated
effortlessly, easily applied, and transferred in real-world scenarios.
</p>
<p>In this paper, we present a novel and comprehensive method to improve the
strength of attacks and assess the transferability of adversarial examples in
CNNs when such strength changes, as well as whether the transferability
property issue exists in computer network applications. In the context of our
study, we initially examined six distinct modes of attack: the Carlini and
Wagner (C&amp;W), Fast Gradient Sign Method (FGSM), Iterative Fast Gradient Sign
Method (I-FGSM), Jacobian-based Saliency Map (JSMA), Limited-memory Broyden
fletcher Goldfarb Shanno (L-BFGS), and Projected Gradient Descent (PGD) attack.
We applied these attack techniques on two popular datasets: the CIC and UNSW
datasets. The outcomes of our experiment demonstrate that an improvement in
transferability occurs in the targeted scenarios for FGSM, JSMA, LBFGS, and
other attacks. Our findings further indicate that the threats to security posed
by adversarial examples, even in computer network applications, necessitate the
development of novel defense mechanisms to enhance the security of DL-based
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03376">Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints. (arXiv:2311.03376v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1">Soumyabrata Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Suggala_A/0/1/0/all/0/1">Arun Sai Suggala</a>, <a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1">Karthikeyan Shanmugam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1">Prateek Jain</a></p>
<p>We consider the problem of \emph{blocked} collaborative bandits where there
are multiple users, each with an associated multi-armed bandit problem. These
users are grouped into \emph{latent} clusters such that the mean reward vectors
of users within the same cluster are identical. Our goal is to design
algorithms that maximize the cumulative reward accrued by all the users over
time, under the \emph{constraint} that no arm of a user is pulled more than
$\mathsf{B}$ times. This problem has been originally considered by
\cite{Bresler:2014}, and designing regret-optimal algorithms for it has since
remained an open problem. In this work, we propose an algorithm called
\texttt{B-LATTICE} (Blocked Latent bAndiTs via maTrIx ComplEtion) that
collaborates across users, while simultaneously satisfying the budget
constraints, to maximize their cumulative rewards. Theoretically, under certain
reasonable assumptions on the latent structure, with $\mathsf{M}$ users,
$\mathsf{N}$ arms, $\mathsf{T}$ rounds per user, and $\mathsf{C}=O(1)$ latent
clusters, \texttt{B-LATTICE} achieves a per-user regret of
$\widetilde{O}(\sqrt{\mathsf{T}(1 + \mathsf{N}\mathsf{M}^{-1})}$ under a budget
constraint of $\mathsf{B}=\Theta(\log \mathsf{T})$. These are the first
sub-linear regret bounds for this problem, and match the minimax regret bounds
when $\mathsf{B}=\mathsf{T}$. Empirically, we demonstrate that our algorithm
has superior performance over baselines even when $\mathsf{B}=1$.
\texttt{B-LATTICE} runs in phases where in each phase it clusters users into
groups and collaborates across users within a group to quickly learn their
reward models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03378">Transferability and explainability of deep learning emulators for regional climate model projections: Perspectives for future applications. (arXiv:2311.03378v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Bano_Medina_J/0/1/0/all/0/1">Jorge Bano-Medina</a>, <a href="http://arxiv.org/find/physics/1/au:+Iturbide_M/0/1/0/all/0/1">Maialen Iturbide</a>, <a href="http://arxiv.org/find/physics/1/au:+Fernandez_J/0/1/0/all/0/1">Jesus Fernandez</a>, <a href="http://arxiv.org/find/physics/1/au:+Gutierrez_J/0/1/0/all/0/1">Jose Manuel Gutierrez</a></p>
<p>Regional climate models (RCMs) are essential tools for simulating and
studying regional climate variability and change. However, their high
computational cost limits the production of comprehensive ensembles of regional
climate projections covering multiple scenarios and driving Global Climate
Models (GCMs) across regions. RCM emulators based on deep learning models have
recently been introduced as a cost-effective and promising alternative that
requires only short RCM simulations to train the models. Therefore, evaluating
their transferability to different periods, scenarios, and GCMs becomes a
pivotal and complex task in which the inherent biases of both GCMs and RCMs
play a significant role. Here we focus on this problem by considering the two
different emulation approaches proposed in the literature (PP and MOS,
following the terminology introduced in this paper). In addition to standard
evaluation techniques, we expand the analysis with methods from the field of
eXplainable Artificial Intelligence (XAI), to assess the physical consistency
of the empirical links learnt by the models. We find that both approaches are
able to emulate certain climatological properties of RCMs for different periods
and scenarios (soft transferability), but the consistency of the emulation
functions differ between approaches. Whereas PP learns robust and physically
meaningful patterns, MOS results are GCM-dependent and lack physical
consistency in some cases. Both approaches face problems when transferring the
emulation function to other GCMs, due to the existence of GCM-dependent biases
(hard transferability). This limits their applicability to build ensembles of
regional climate projections. We conclude by giving some prospects for future
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03380">An attempt to generate new bridge types from latent space of variational autoencoder. (arXiv:2311.03380v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongjun Zhang</a></p>
<p>Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03381">Separating and Learning Latent Confounders to Enhancing User Preferences Modeling. (arXiv:2311.03381v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hangtong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuanbo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongjian Yang</a></p>
<p>Recommender models aim to capture user preferences from historical feedback
and then predict user-specific feedback on candidate items. However, the
presence of various unmeasured confounders causes deviations between the user
preferences in the historical feedback and the true preferences, resulting in
models not meeting their expected performance. Existing debias models either
(1) specific to solving one particular bias or (2) directly obtain auxiliary
information from user historical feedback, which cannot identify whether the
learned preferences are true user preferences or mixed with unmeasured
confounders. Moreover, we find that the former recommender system is not only a
successor to unmeasured confounders but also acts as an unmeasured confounder
affecting user preference modeling, which has always been neglected in previous
studies. To this end, we incorporate the effect of the former recommender
system and treat it as a proxy for all unmeasured confounders. We propose a
novel framework, \textbf{S}eparating and \textbf{L}earning Latent Confounders
\textbf{F}or \textbf{R}ecommendation (\textbf{SLFR}), which obtains the
representation of unmeasured confounders to identify the counterfactual
feedback by disentangling user preferences and unmeasured confounders, then
guides the target model to capture the true preferences of users. Extensive
experiments in five real-world datasets validate the advantages of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03382">Causal Structure Representation Learning of Confounders in Latent Space for Recommendation. (arXiv:2311.03382v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hangtong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuanbo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yongjian Yang</a></p>
<p>Inferring user preferences from the historical feedback of users is a
valuable problem in recommender systems. Conventional approaches often rely on
the assumption that user preferences in the feedback data are equivalent to the
real user preferences without additional noise, which simplifies the problem
modeling. However, there are various confounders during user-item interactions,
such as weather and even the recommendation system itself. Therefore,
neglecting the influence of confounders will result in inaccurate user
preferences and suboptimal performance of the model. Furthermore, the
unobservability of confounders poses a challenge in further addressing the
problem. To address these issues, we refine the problem and propose a more
rational solution. Specifically, we consider the influence of confounders,
disentangle them from user preferences in the latent space, and employ causal
graphs to model their interdependencies without specific labels. By cleverly
combining local and global causal graphs, we capture the user-specificity of
confounders on user preferences. We theoretically demonstrate the
identifiability of the obtained causal graph. Finally, we propose our model
based on Variational Autoencoders, named Causal Structure representation
learning of Confounders in latent space (CSC). We conducted extensive
experiments on one synthetic dataset and five real-world datasets,
demonstrating the superiority of our model. Furthermore, we demonstrate that
the learned causal representations of confounders are controllable, potentially
offering users fine-grained control over the objectives of their recommendation
lists with the learned causal graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03383">Toward Reinforcement Learning-based Rectilinear Macro Placement Under Human Constraints. (arXiv:2311.03383v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Tuyen P. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hieu T. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1">Seungyeol Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taeyoun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungwoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seongjung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunjin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1">Misu Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Daehoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seokyong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1">Daewoo Choi</a></p>
<p>Macro placement is a critical phase in chip design, which becomes more
intricate when involving general rectilinear macros and layout areas.
Furthermore, macro placement that incorporates human-like constraints, such as
design hierarchy and peripheral bias, has the potential to significantly reduce
the amount of additional manual labor required from designers. This study
proposes a methodology that leverages an approach suggested by Google's Circuit
Training (G-CT) to provide a learning-based macro placer that not only supports
placing rectilinear cases, but also adheres to crucial human-like design
principles. Our experimental results demonstrate the effectiveness of our
framework in achieving power-performance-area (PPA) metrics and in obtaining
placements of high quality, comparable to those produced with human
intervention. Additionally, our methodology shows potential as a generalized
model to address diverse macro shapes and layout areas.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03386">A Simple and Efficient Baseline for Data Attribution on Images. (arXiv:2311.03386v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singla_V/0/1/0/all/0/1">Vasu Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandoval_Segura_P/0/1/0/all/0/1">Pedro Sandoval-Segura</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1">Micah Goldblum</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1">Jonas Geiping</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a></p>
<p>Data attribution methods play a crucial role in understanding machine
learning models, providing insight into which training data points are most
responsible for model outputs during deployment. However, current
state-of-the-art approaches require a large ensemble of as many as 300,000
models to accurately attribute model predictions. These approaches therefore
come at a high computational cost, are memory intensive, and are hard to scale
to large models or datasets. In this work, we focus on a minimalist baseline,
utilizing the feature space of a backbone pretrained via self-supervised
learning to perform data attribution. Our method is model-agnostic and scales
easily to large datasets. We show results on CIFAR-10 and ImageNet, achieving
strong performance that rivals or outperforms state-of-the-art approaches at a
fraction of the compute or memory cost. Contrary to prior work, our results
reinforce the intuition that a model's prediction on one image is most impacted
by visually similar training samples. Our approach serves as a simple and
efficient baseline for data attribution on images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03387">Determination of droplet size from wide-angle light scattering image data using convolutional neural networks. (arXiv:2311.03387v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirstein_T/0/1/0/all/0/1">Tom Kirstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Assmann_S/0/1/0/all/0/1">Simon A&#xdf;mann</a>, <a href="http://arxiv.org/find/cs/1/au:+Furat_O/0/1/0/all/0/1">Orkun Furat</a>, <a href="http://arxiv.org/find/cs/1/au:+Will_S/0/1/0/all/0/1">Stefan Will</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_V/0/1/0/all/0/1">Volker Schmidt</a></p>
<p>Wide-angle light scattering (WALS) offers the possibility of a highly
temporally and spatially resolved measurement of droplets in spray-based
methods for nanoparticle synthesis. The size of these droplets is a critical
variable affecting the final properties of synthesized materials such as
hetero-aggregates. However, conventional methods for determining droplet sizes
from WALS image data are labor-intensive and may introduce biases, particularly
when applied to complex systems like spray flame synthesis (SFS). To address
these challenges, we introduce a fully automatic machine learning-based
approach that employs convolutional neural networks (CNNs) in order to
streamline the droplet sizing process. This CNN-based methodology offers
further advantages: it requires few manual labels and can utilize transfer
learning, making it a promising alternative to conventional methods,
specifically with respect to efficiency. To evaluate the performance of our
machine learning models, we consider WALS data from an ethanol spray flame
process at various heights above the burner surface (HABs), where the models
are trained and cross-validated on a large dataset comprising nearly 35000 WALS
images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03388">Attention-based Models for Snow-Water Equivalent Prediction. (arXiv:2311.03388v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thapa_K/0/1/0/all/0/1">Krishu K. Thapa</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1">Bhupinderjeet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Savalkar_S/0/1/0/all/0/1">Supriya Savalkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1">Alan Fern</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajagopalan_K/0/1/0/all/0/1">Kirti Rajagopalan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyanaraman_A/0/1/0/all/0/1">Ananth Kalyanaraman</a></p>
<p>Snow Water-Equivalent (SWE) -- the amount of water available if snowpack is
melted -- is a key decision variable used by water management agencies to make
irrigation, flood control, power generation and drought management decisions.
SWE values vary spatiotemporally -- affected by weather, topography and other
environmental factors. While daily SWE can be measured by Snow Telemetry
(SNOTEL) stations with requisite instrumentation, such stations are spatially
sparse requiring interpolation techniques to create spatiotemporally complete
data. While recent efforts have explored machine learning (ML) for SWE
prediction, a number of recent ML advances have yet to be considered. The main
contribution of this paper is to explore one such ML advance, attention
mechanisms, for SWE prediction. Our hypothesis is that attention has a unique
ability to capture and exploit correlations that may exist across locations or
the temporal spectrum (or both). We present a generic attention-based modeling
framework for SWE prediction and adapt it to capture spatial attention and
temporal attention. Our experimental results on 323 SNOTEL stations in the
Western U.S. demonstrate that our attention-based models outperform other
machine learning approaches. We also provide key results highlighting the
differences between spatial and temporal attention in this context and a
roadmap toward deployment for generating spatially-complete SWE maps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03389">Learning Disentangled Speech Representations. (arXiv:2311.03389v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Brima_Y/0/1/0/all/0/1">Yusuf Brima</a>, <a href="http://arxiv.org/find/eess/1/au:+Krumnack_U/0/1/0/all/0/1">Ulf Krumnack</a>, <a href="http://arxiv.org/find/eess/1/au:+Pika_S/0/1/0/all/0/1">Simone Pika</a>, <a href="http://arxiv.org/find/eess/1/au:+Heidemann_G/0/1/0/all/0/1">Gunther Heidemann</a></p>
<p>Disentangled representation learning from speech remains limited despite its
importance in many application domains. A key challenge is the lack of speech
datasets with known generative factors to evaluate methods. This paper proposes
SynSpeech: a novel synthetic speech dataset with ground truth factors enabling
research on disentangling speech representations. We plan to present a
comprehensive study evaluating supervised techniques using established
supervised disentanglement metrics. This benchmark dataset and framework
address the gap in the rigorous evaluation of state-of-the-art disentangled
speech representation learning methods. Our findings will provide insights to
advance this underexplored area and enable more robust speech representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03396">Differentially Private Pre-Trained Model Fusion using Decentralized Federated Graph Matching. (arXiv:2311.03396v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiqiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xinlong Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Teng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Weiwei Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wuliang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1">Bo Ye</a></p>
<p>Model fusion is becoming a crucial component in the context of
model-as-a-service scenarios, enabling the delivery of high-quality model
services to local users. However, this approach introduces privacy risks and
imposes certain limitations on its applications. Ensuring secure model exchange
and knowledge fusion among users becomes a significant challenge in this
setting. To tackle this issue, we propose PrivFusion, a novel architecture that
preserves privacy while facilitating model fusion under the constraints of
local differential privacy. PrivFusion leverages a graph-based structure,
enabling the fusion of models from multiple parties without necessitating
retraining. By employing randomized mechanisms, PrivFusion ensures privacy
guarantees throughout the fusion process. To enhance model privacy, our
approach incorporates a hybrid local differentially private mechanism and
decentralized federated graph matching, effectively protecting both activation
values and weights. Additionally, we introduce a perturbation filter adapter to
alleviate the impact of randomized noise, thereby preserving the utility of the
fused model. Through extensive experiments conducted on diverse image datasets
and real-world healthcare applications, we provide empirical evidence
showcasing the effectiveness of PrivFusion in maintaining model performance
while preserving privacy. Our contributions offer valuable insights and
practical solutions for secure and collaborative data analysis within the
domain of privacy-preserving model fusion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03401">Enhancing AI Research Paper Analysis: Methodology Component Extraction using Factored Transformer-based Sequence Modeling Approach. (arXiv:2311.03401v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1">Madhusudan Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguly_D/0/1/0/all/0/1">Debasis Ganguly</a>, <a href="http://arxiv.org/find/cs/1/au:+Basuchowdhuri_P/0/1/0/all/0/1">Partha Basuchowdhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1">Sudip Kumar Naskar</a></p>
<p>Research in scientific disciplines evolves, often rapidly, over time with the
emergence of novel methodologies and their associated terminologies. While
methodologies themselves being conceptual in nature and rather difficult to
automatically extract and characterise, in this paper, we seek to develop
supervised models for automatic extraction of the names of the various
constituents of a methodology, e.g., `R-CNN', `ELMo' etc. The main research
challenge for this task is effectively modeling the contexts around these
methodology component names in a few-shot or even a zero-shot setting. The main
contributions of this paper towards effectively identifying new evolving
scientific methodology names are as follows: i) we propose a factored approach
to sequence modeling, which leverages a broad-level category information of
methodology domains, e.g., `NLP', `RL' etc.; ii) to demonstrate the feasibility
of our proposed approach of identifying methodology component names under a
practical setting of fast evolving AI literature, we conduct experiments
following a simulated chronological setup (newer methodologies not seen during
the training process); iii) our experiments demonstrate that the factored
approach outperforms state-of-the-art baselines by margins of up to 9.257\% for
the methodology extraction task with the few-shot setup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03405">Communication Efficient and Privacy-Preserving Federated Learning Based on Evolution Strategies. (arXiv:2311.03405v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1">Guangchen Lan</a></p>
<p>Federated learning (FL) is an emerging paradigm for training deep neural
networks (DNNs) in distributed manners. Current FL approaches all suffer from
high communication overhead and information leakage. In this work, we present a
federated learning algorithm based on evolution strategies (FedES), a
zeroth-order training method. Instead of transmitting model parameters, FedES
only communicates loss values, and thus has very low communication overhead.
Moreover, a third party is unable to estimate gradients without knowing the
pre-shared seed, which protects data privacy. Experimental results demonstrate
FedES can achieve the above benefits while keeping convergence performance the
same as that with back propagation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03408">Training Multi-layer Neural Networks on Ising Machine. (arXiv:2311.03408v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xujie Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengbo Eben Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jingliang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Keqiang Li</a></p>
<p>As a dedicated quantum device, Ising machines could solve large-scale binary
optimization problems in milliseconds. There is emerging interest in utilizing
Ising machines to train feedforward neural networks due to the prosperity of
generative artificial intelligence. However, existing methods can only train
single-layer feedforward networks because of the complex nonlinear network
topology. This paper proposes an Ising learning algorithm to train quantized
neural network (QNN), by incorporating two essential techinques, namely binary
representation of topological network and order reduction of loss function. As
far as we know, this is the first algorithm to train multi-layer feedforward
networks on Ising machines, providing an alternative to gradient-based
backpropagation. Firstly, training QNN is formulated as a quadratic constrained
binary optimization (QCBO) problem by representing neuron connection and
activation function as equality constraints. All quantized variables are
encoded by binary bits based on binary encoding protocol. Secondly, QCBO is
converted to a quadratic unconstrained binary optimization (QUBO) problem, that
can be efficiently solved on Ising machines. The conversion leverages both
penalty function and Rosenberg order reduction, who together eliminate equality
constraints and reduce high-order loss function into a quadratic one. With some
assumptions, theoretical analysis shows the space complexity of our algorithm
is $\mathcal{O}(H^2L + HLN\log H)$, quantifying the required number of Ising
spins. Finally, the algorithm effectiveness is validated with a simulated Ising
machine on MNIST dataset. After annealing 700 ms, the classification accuracy
achieves 98.3%. Among 100 runs, the success probability of finding the optimal
solution is 72%. Along with the increasing number of spins on Ising machine,
our algorithm has the potential to train deeper neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03409">Visualizing DNA reaction trajectories with deep graph embedding approaches. (arXiv:2311.03409v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1">Chenwei Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Duc_K/0/1/0/all/0/1">Khanh Dao Duc</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Condon_A/0/1/0/all/0/1">Anne Condon</a></p>
<p>Synthetic biologists and molecular programmers design novel nucleic acid
reactions, with many potential applications. Good visualization tools are
needed to help domain experts make sense of the complex outputs of folding
pathway simulations of such reactions. Here we present ViDa, a new approach for
visualizing DNA reaction folding trajectories over the energy landscape of
secondary structures. We integrate a deep graph embedding model with common
dimensionality reduction approaches, to map high-dimensional data onto 2D
Euclidean space. We assess ViDa on two well-studied and contrasting DNA
hybridization reactions. Our preliminary results suggest that ViDa's
visualization successfully separates trajectories with different folding
mechanisms, thereby providing useful insight to users, and is a big improvement
over the current state-of-the-art in DNA kinetics visualization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03410">DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for Single-cell Clustering. (arXiv:2311.03410v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huifa Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhili Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaomin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haitao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1">Xinpeng Ling</a></p>
<p>Single-cell RNA sequencing (scRNA-seq) is important to transcriptomic
analysis of gene expression. Recently, deep learning has facilitated the
analysis of high-dimensional single-cell data. Unfortunately, deep learning
models may leak sensitive information about users. As a result, Differential
Privacy (DP) is increasingly used to protect privacy. However, existing DP
methods usually perturb whole neural networks to achieve differential privacy,
and hence result in great performance overheads. To address this challenge, in
this paper, we take advantage of the uniqueness of the autoencoder that it
outputs only the dimension-reduced vector in the middle of the network, and
design a Differentially Private Deep Contrastive Autoencoder Network (DP-DCAN)
by partial network perturbation for single-cell clustering. Since only partial
network is added with noise, the performance improvement is obvious and
twofold: one part of network is trained with less noise due to a bigger privacy
budget, and the other part is trained without any noise. Experimental results
of six datasets have verified that DP-DCAN is superior to the traditional DP
scheme with whole network perturbation. Moreover, DP-DCAN demonstrates strong
robustness to adversarial attacks. The code is available at
https://github.com/LFD-byte/DP-DCAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03411">ViDa: Visualizing DNA hybridization trajectories with biophysics-informed deep graph embeddings. (arXiv:2311.03411v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1">Chenwei Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lovrod_J/0/1/0/all/0/1">Jordan Lovrod</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Beronov_B/0/1/0/all/0/1">Boyan Beronov</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Duc_K/0/1/0/all/0/1">Khanh Dao Duc</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Condon_A/0/1/0/all/0/1">Anne Condon</a></p>
<p>Visualization tools can help synthetic biologists and molecular programmers
understand the complex reactive pathways of nucleic acid reactions, which can
be designed for many potential applications and can be modelled using a
continuous-time Markov chain (CTMC). Here we present ViDa, a new visualization
approach for DNA reaction trajectories that uses a 2D embedding of the
secondary structure state space underlying the CTMC model. To this end, we
integrate a scattering transform of the secondary structure adjacency, a
variational autoencoder, and a nonlinear dimensionality reduction method. We
augment the training loss with domain-specific supervised terms that capture
both thermodynamic and kinetic features. We assess ViDa on two well-studied DNA
hybridization reactions. Our results demonstrate that the domain-specific
features lead to significant quality improvements over the state-of-the-art in
DNA state space visualization, successfully separating different folding
pathways and thus providing useful insights into dominant reaction mechanisms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03413">Discret2Di -- Deep Learning based Discretization for Model-based Diagnosis. (arXiv:2311.03413v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moddemann_L/0/1/0/all/0/1">Lukas Moddemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Steude_H/0/1/0/all/0/1">Henrik Sebastian Steude</a>, <a href="http://arxiv.org/find/cs/1/au:+Diedrich_A/0/1/0/all/0/1">Alexander Diedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Niggemann_O/0/1/0/all/0/1">Oliver Niggemann</a></p>
<p>Consistency-based diagnosis is an established approach to diagnose technical
applications, but suffers from significant modeling efforts, especially for
dynamic multi-modal time series. Machine learning seems to be an obvious
solution, which becomes less obvious when looking at details: Which notion of
consistency can be used? If logical calculi are still to be used, how can
dynamic time series be transferred into the discrete world?
</p>
<p>This paper presents the methodology Discret2Di for automated learning of
logical expressions for consistency-based diagnosis. While these logical
calculi have advantages by providing a clear notion of consistency, they have
the key problem of relying on a discretization of the dynamic system. The
solution presented combines machine learning from both the time series and the
symbolic domain to automate the learning of logical rules for consistency-based
diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03414">A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle. (arXiv:2311.03414v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petroll_C/0/1/0/all/0/1">Christoph Petroll</a>, <a href="http://arxiv.org/find/cs/1/au:+Eilermann_S/0/1/0/all/0/1">Sebastian Eilermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefer_P/0/1/0/all/0/1">Philipp Hoefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Niggemann_O/0/1/0/all/0/1">Oliver Niggemann</a></p>
<p>One of the most promising developments in computer vision in recent years is
the use of generative neural networks for functionality condition-based 3D
design reconstruction and generation. Here, neural networks learn dependencies
between functionalities and a geometry in a very effective way. For a neural
network the functionalities are translated in conditions to a certain geometry.
But the more conditions the design generation needs to reflect, the more
difficult it is to learn clear dependencies. This leads to a multi criteria
design problem due various conditions, which are not considered in the neural
network structure so far.
</p>
<p>In this paper, we address this multi-criteria challenge for a 3D design use
case related to an unmanned aerial vehicle (UAV) motor mount. We generate
10,000 abstract 3D designs and subject them all to simulations for three
physical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we
train a Conditional Variational Autoencoder (CVAE) using the geometry and
corresponding multicriteria functional constraints as input. We use our trained
CVAE as well as the Marching cubes algorithm to generate meshes for simulation
based evaluation. The results are then evaluated with the generated UAV
designs. Subsequently, we demonstrate the ability to generate optimized designs
under self-defined functionality conditions using the trained neural network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03415">PowerFlowNet: Leveraging Message Passing GNNs for Improved Power Flow Approximation. (arXiv:2311.03415v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1">Nan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Orfanoudakis_S/0/1/0/all/0/1">Stavros Orfanoudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardenas_N/0/1/0/all/0/1">Nathan Ordonez Cardenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Giraldo_J/0/1/0/all/0/1">Juan S. Giraldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Vergara_P/0/1/0/all/0/1">Pedro P. Vergara</a></p>
<p>Accurate and efficient power flow (PF) analysis is crucial in modern
electrical networks' efficient operation and planning. Therefore, there is a
need for scalable algorithms capable of handling large-scale power networks
that can provide accurate and fast solutions. Graph Neural Networks (GNNs) have
emerged as a promising approach for enhancing the speed of PF approximations by
leveraging their ability to capture distinctive features from the underlying
power network graph. In this study, we introduce PowerFlowNet, a novel GNN
architecture for PF approximation that showcases similar performance with the
traditional Newton-Raphson method but achieves it 4 times faster in the simple
IEEE 14-bus system and 145 times faster in the realistic case of the French
high voltage network (6470rte). Meanwhile, it significantly outperforms other
traditional approximation methods, such as the DC relaxation method, in terms
of performance and execution time; therefore, making PowerFlowNet a highly
promising solution for real-world PF analysis. Furthermore, we verify the
efficacy of our approach by conducting an in-depth experimental evaluation,
thoroughly examining the performance, scalability, interpretability, and
architectural dependability of PowerFlowNet. The evaluation provides insights
into the behavior and potential applications of GNNs in power system analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03417">Federated Learning for Clinical Structured Data: A Benchmark Comparison of Engineering and Statistical Approaches. (arXiv:2311.03417v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Siqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1">Di Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1">Chuan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+DAgostino_D/0/1/0/all/0/1">Danny D&#x27;Agostino</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1">Yilin Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yuqing Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1">Marcus Eng Hock Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1">Hamed Haddadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nan Liu</a></p>
<p>Federated learning (FL) has shown promising potential in safeguarding data
privacy in healthcare collaborations. While the term "FL" was originally coined
by the engineering community, the statistical field has also explored similar
privacy-preserving algorithms. Statistical FL algorithms, however, remain
considerably less recognized than their engineering counterparts. Our goal was
to bridge the gap by presenting the first comprehensive comparison of FL
frameworks from both engineering and statistical domains. We evaluated five FL
frameworks using both simulated and real-world data. The results indicate that
statistical FL algorithms yield less biased point estimates for model
coefficients and offer convenient confidence interval estimations. In contrast,
engineering-based methods tend to generate more accurate predictions, sometimes
surpassing central pooled and statistical FL models. This study underscores the
relative strengths and weaknesses of both types of methods, emphasizing the
need for increased awareness and their integration in future FL applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03419">Personalizing Keyword Spotting with Speaker Information. (arXiv:2311.03419v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Labrador_B/0/1/0/all/0/1">Beltr&#xe1;n Labrador</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_P/0/1/0/all/0/1">Pai Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1">Guanlong Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Scarpati_A/0/1/0/all/0/1">Angelo Scorza Scarpati</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1">Quan Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lozano_Diez_A/0/1/0/all/0/1">Alicia Lozano-Diez</a>, <a href="http://arxiv.org/find/eess/1/au:+Park_A/0/1/0/all/0/1">Alex Park</a>, <a href="http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1">Ignacio L&#xf3;pez Moreno</a></p>
<p>Keyword spotting systems often struggle to generalize to a diverse population
with various accents and age groups. To address this challenge, we propose a
novel approach that integrates speaker information into keyword spotting using
Feature-wise Linear Modulation (FiLM), a recent method for learning from
multiple sources of information. We explore both Text-Dependent and
Text-Independent speaker recognition systems to extract speaker information,
and we experiment on extracting this information from both the input audio and
pre-enrolled user audio. We evaluate our systems on a diverse dataset and
achieve a substantial improvement in keyword detection accuracy, particularly
among underrepresented speaker groups. Moreover, our proposed approach only
requires a small 1% increase in the number of parameters, with a minimum impact
on latency and computational cost, which makes it a practical solution for
real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03420">Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19. (arXiv:2311.03420v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Francis_S/0/1/0/all/0/1">Sumam Francis</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a></p>
<p>This paper presents models created for the Social Media Mining for Health
2023 shared task. Our team addressed the first task, classifying tweets that
self-report Covid-19 diagnosis. Our approach involves a classification model
that incorporates diverse textual augmentations and utilizes R-drop to augment
data and mitigate overfitting, boosting model efficacy. Our leading model,
enhanced with R-drop and augmentations like synonym substitution, reserved
words, and back translations, outperforms the task mean and median scores. Our
system achieves an impressive F1 score of 0.877 on the test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03421">Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding. (arXiv:2311.03421v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Marin_Llobet_A/0/1/0/all/0/1">Arnau Marin-Llobet</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Manasanch_A/0/1/0/all/0/1">Arnau Manasanch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sanchez_Vives_M/0/1/0/all/0/1">Maria V. Sanchez-Vives</a></p>
<p>The study of brain states, ranging from highly synchronous to asynchronous
neuronal patterns like the sleep-wake cycle, is fundamental for assessing the
brain's spatiotemporal dynamics and their close connection to behavior.
However, the development of new techniques to accurately identify them still
remains a challenge, as these are often compromised by the presence of noise,
artifacts, and suboptimal recording quality. In this study, we propose a
two-stage computational framework combining Hopfield Networks for artifact data
preprocessing with Convolutional Neural Networks (CNNs) for classification of
brain states in rat neural recordings under different levels of anesthesia. To
evaluate the robustness of our framework, we deliberately introduced noise
artifacts into the neural recordings. We evaluated our hybrid Hopfield-CNN
pipeline by benchmarking it against two comparative models: a standalone CNN
handling the same noisy inputs, and another CNN trained and tested on
artifact-free data. Performance across various levels of data compression and
noise intensities showed that our framework can effectively mitigate artifacts,
allowing the model to reach parity with the clean-data CNN at lower noise
levels. Although this study mainly benefits small-scale experiments, the
findings highlight the necessity for advanced deep learning and Hopfield
Network models to improve scalability and robustness in diverse real-world
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03425">An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets. (arXiv:2311.03425v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gulamali_F/0/1/0/all/0/1">Faris F. Gulamali</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawant_A/0/1/0/all/0/1">Ashwin S. Sawant</a>, <a href="http://arxiv.org/find/cs/1/au:+Liharska_L/0/1/0/all/0/1">Lora Liharska</a>, <a href="http://arxiv.org/find/cs/1/au:+Horowitz_C/0/1/0/all/0/1">Carol R. Horowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1">Lili Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovatch_P/0/1/0/all/0/1">Patricia H. Kovatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofer_I/0/1/0/all/0/1">Ira Hofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1">Karandeep Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Richardson_L/0/1/0/all/0/1">Lynne D. Richardson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mensah_E/0/1/0/all/0/1">Emmanuel Mensah</a>, <a href="http://arxiv.org/find/cs/1/au:+Charney_A/0/1/0/all/0/1">Alexander W Charney</a>, <a href="http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1">David L. Reich</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jianying Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nadkarni_G/0/1/0/all/0/1">Girish N. Nadkarni</a></p>
<p>The adoption of diagnosis and prognostic algorithms in healthcare has led to
concerns about the perpetuation of bias against disadvantaged groups of
individuals. Deep learning methods to detect and mitigate bias have revolved
around modifying models, optimization strategies, and threshold calibration
with varying levels of success. Here, we generate a data-centric,
model-agnostic, task-agnostic approach to evaluate dataset bias by
investigating the relationship between how easily different groups are learned
at small sample sizes (AEquity). We then apply a systematic analysis of AEq
values across subpopulations to identify and mitigate manifestations of racial
bias in two known cases in healthcare - Chest X-rays diagnosis with deep
convolutional neural networks and healthcare utilization prediction with
multivariate logistic regression. AEq is a novel and broadly applicable metric
that can be applied to advance equity by diagnosing and remediating bias in
healthcare datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03426">GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values. (arXiv:2311.03426v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javadi_F/0/1/0/all/0/1">Farnoosh Javadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1">Walid Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1">Habib Hajimolahoseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Ataiefard_F/0/1/0/all/0/1">Foozhan Ataiefard</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1">Mohammad Hassanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Asani_S/0/1/0/all/0/1">Saina Asani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1">Austin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Awad_O/0/1/0/all/0/1">Omar Mohamed Awad</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kangling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a></p>
<p>Massive transformer-based models face several challenges, including slow and
computationally intensive pre-training and over-parametrization. This paper
addresses these challenges by proposing a versatile method called GQKVA, which
generalizes query, key, and value grouping techniques. GQKVA is designed to
speed up transformer pre-training while reducing the model size. Our
experiments with various GQKVA variants highlight a clear trade-off between
performance and model size, allowing for customized choices based on resource
and time limitations. Our findings also indicate that the conventional
multi-head attention approach is not always the best choice, as there are
lighter and faster alternatives available. We tested our method on ViT, which
achieved an approximate 0.3% increase in accuracy while reducing the model size
by about 4% in the task of image classification. Additionally, our most
aggressive model reduction experiment resulted in a reduction of approximately
15% in model size, with only around a 1% drop in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03429">ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity. (arXiv:2311.03429v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Zhan_H/0/1/0/all/0/1">Huixin Zhan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_Z/0/1/0/all/0/1">Zijun Zhang</a></p>
<p>Clinical variant classification of pathogenic versus benign genetic variants
remains a pivotal challenge in clinical genetics. Recently, the proposition of
protein language models has improved the generic variant effect prediction
(VEP) accuracy via weakly-supervised or unsupervised training. However, these
VEPs are not disease-specific, limiting their adaptation at point-of-care. To
address this problem, we propose a disease-specific \textsc{pro}tein language
model for variant \textsc{path}ogenicity, termed ProPath, to capture the
pseudo-log-likelihood ratio in rare missense variants through a siamese
network. We evaluate the performance of ProPath against pre-trained language
models, using clinical variant sets in inherited cardiomyopathies and
arrhythmias that were not seen during training. Our results demonstrate that
ProPath surpasses the pre-trained ESM1b with an over $5\%$ improvement in AUC
across both datasets. Furthermore, our model achieved the highest performances
across all baselines for both datasets. Thus, our ProPath offers a potent
disease-specific variant effect prediction, particularly valuable for disease
associations and clinical applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03468">FinA: Fairness of Adverse Effects in Decision-Making of Human-Cyber-Physical-System. (arXiv:2311.03468v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Elmalaki_S/0/1/0/all/0/1">Salma Elmalaki</a></p>
<p>Ensuring fairness in decision-making systems within
Human-Cyber-Physical-Systems (HCPS) is a pressing concern, particularly when
diverse individuals, each with varying behaviors and expectations, coexist
within the same application space, influenced by a shared set of control
actions in the system. The long-term adverse effects of these actions further
pose the challenge, as historical experiences and interactions shape individual
perceptions of fairness. This paper addresses the challenge of fairness from an
equity perspective of adverse effects, taking into account the dynamic nature
of human behavior and evolving preferences while recognizing the lasting impact
of adverse effects. We formally introduce the concept of
Fairness-in-Adverse-Effects (FinA) within the HCPS context. We put forth a
comprehensive set of five formulations for FinA, encompassing both the
instantaneous and long-term aspects of adverse effects. To empirically validate
the effectiveness of our FinA approach, we conducted an evaluation within the
domain of smart homes, a pertinent HCPS application. The outcomes of our
evaluation demonstrate that the adoption of FinA significantly enhances the
overall perception of fairness among individuals, yielding an average
improvement of 66.7% when compared to the state-of-the-art method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03483">Hebbian learning inspired estimation of the linear regression parameters from queries. (arXiv:2311.03483v1 [math.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Schmidt_Hieber_J/0/1/0/all/0/1">Johannes Schmidt-Hieber</a>, <a href="http://arxiv.org/find/math/1/au:+Koolen_W/0/1/0/all/0/1">Wouter M Koolen</a></p>
<p>Local learning rules in biological neural networks (BNNs) are commonly
referred to as Hebbian learning. [26] links a biologically motivated Hebbian
learning rule to a specific zeroth-order optimization method. In this work, we
study a variation of this Hebbian learning rule to recover the regression
vector in the linear regression model. Zeroth-order optimization methods are
known to converge with suboptimal rate for large parameter dimension compared
to first-order methods like gradient descent, and are therefore thought to be
in general inferior. By establishing upper and lower bounds, we show, however,
that such methods achieve near-optimal rates if only queries of the linear
regression loss are available. Moreover, we prove that this Hebbian learning
rule can achieve considerably faster rates than any non-adaptive method that
selects the queries independently of the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03488">Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems. (arXiv:2311.03488v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lilienthal_D/0/1/0/all/0/1">Derek Lilienthal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1">Paul Mello</a>, <a href="http://arxiv.org/find/cs/1/au:+Eirinaki_M/0/1/0/all/0/1">Magdalini Eirinaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1">Stas Tiomkin</a></p>
<p>While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Model (SDRM), which captures the intricate patterns of
real-world datasets required for training highly accurate recommender systems.
SDRM allows for the generation of synthetic data that can replace existing
datasets to preserve user privacy, or augment existing datasets to address
excessive data sparsity. Our method outperforms competing baselines such as
generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$n$ and
4.65% in NDCG@$n$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03489">Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design. (arXiv:2311.03489v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meech_J/0/1/0/all/0/1">James T. Meech</a></p>
<p>We present a new high-level synthesis methodology for using large language
model tools to generate hardware designs. The methodology uses exclusively
open-source tools excluding the large language model. As a case study, we use
our methodology to generate a permuted congruential random number generator
design with a wishbone interface. We verify the functionality and quality of
the random number generator design using large language model-generated
simulations and the Dieharder randomness test suite. We document all the large
language model chat logs, Python scripts, Verilog scripts, and simulation
results used in the case study. We believe that our method of hardware design
generation coupled with the open source silicon 130 nm design tools will
revolutionize application-specific integrated circuit design. Our methodology
significantly lowers the bar to entry when building domain-specific computing
accelerators for the Internet of Things and proof of concept prototypes for
later fabrication in more modern process nodes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03496">Asynchronous Local Computations in Distributed Bayesian Learning. (arXiv:2311.03496v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhar_K/0/1/0/all/0/1">Kinjal Bhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1">He Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+George_J/0/1/0/all/0/1">Jemin George</a>, <a href="http://arxiv.org/find/cs/1/au:+Busart_C/0/1/0/all/0/1">Carl Busart</a></p>
<p>Due to the expanding scope of machine learning (ML) to the fields of sensor
networking, cooperative robotics and many other multi-agent systems,
distributed deployment of inference algorithms has received a lot of attention.
These algorithms involve collaboratively learning unknown parameters from
dispersed data collected by multiple agents. There are two competing aspects in
such algorithms, namely, intra-agent computation and inter-agent communication.
Traditionally, algorithms are designed to perform both synchronously. However,
certain circumstances need frugal use of communication channels as they are
either unreliable, time-consuming, or resource-expensive. In this paper, we
propose gossip-based asynchronous communication to leverage fast computations
and reduce communication overhead simultaneously. We analyze the effects of
multiple (local) intra-agent computations by the active agents between
successive inter-agent communications. For local computations, Bayesian
sampling via unadjusted Langevin algorithm (ULA) MCMC is utilized. The
communication is assumed to be over a connected graph (e.g., as in
decentralized learning), however, the results can be extended to coordinated
communication where there is a central server (e.g., federated learning). We
theoretically quantify the convergence rates in the process. To demonstrate the
efficacy of the proposed algorithm, we present simulations on a toy problem as
well as on real world data sets to train ML models to perform classification
tasks. We observe faster initial convergence and improved performance accuracy,
especially in the low data range. We achieve on average 78% and over 90%
classification accuracy respectively on the Gamma Telescope and mHealth data
sets from the UCI ML repository.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03498">In-Context Exemplars as Clues to Retrieving from Large Associative Memory. (arXiv:2311.03498v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiachen Zhao</a></p>
<p>Recently, large language models (LLMs) have made remarkable progress in
natural language processing. The most representative ability of LLMs is
in-context learning (ICL), which enables LLMs to learn patterns from in-context
exemplars without training. The performance of ICL greatly depends on the
exemplars used. However, how to choose exemplars remains unclear due to the
lack of understanding of how in-context learning works. In this paper, we
present a novel perspective on ICL by conceptualizing it as contextual
retrieval from a model of associative memory. We establish a theoretical
framework of ICL based on Hopfield Networks. Based on our framework, we look
into how in-context exemplars influence the performance of ICL and propose more
efficient active exemplar selection. Our study sheds new light on the mechanism
of ICL by connecting it to memory retrieval, with potential implications for
advancing the understanding of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03520">Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data. (arXiv:2311.03520v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thapaliya_B/0/1/0/all/0/1">Bishal Thapaliya</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1">Esra Akbas</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiayu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1">Raam Sapkota</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1">Bhaskar Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Suresh_P/0/1/0/all/0/1">Pranav Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1">Vince Calhoun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingyu Liu</a></p>
<p>Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful
tool for investigating the relationship between brain function and cognitive
processes as it allows for the functional organization of the brain to be
captured without relying on a specific task or stimuli. In this paper, we
present a novel modeling architecture called BrainRGIN for predicting
intelligence (fluid, crystallized, and total intelligence) using graph neural
networks on rsfMRI derived static functional network connectivity matrices.
Extending from the existing graph convolution networks, our approach
incorporates a clustering-based embedding and graph isomorphism network in the
graph convolutional layer to reflect the nature of the brain sub-network
organization and efficient network expression, in combination with TopK pooling
and attention-based readout functions. We evaluated our proposed architecture
on a large dataset, specifically the Adolescent Brain Cognitive Development
Dataset, and demonstrated its effectiveness in predicting individual
differences in intelligence. Our model achieved lower mean squared errors and
higher correlation scores than existing relevant graph architectures and other
traditional machine learning models for all of the intelligence prediction
tasks. The middle frontal gyrus exhibited a significant contribution to both
fluid and crystallized intelligence, suggesting their pivotal role in these
cognitive processes. Total composite scores identified a diverse set of brain
regions to be relevant which underscores the complex nature of total
intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03524">A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning. (arXiv:2311.03524v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yiyou Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhenmei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixuan Li</a></p>
<p>Open-world semi-supervised learning aims at inferring both known and novel
classes in unlabeled data, by harnessing prior knowledge from a labeled set
with known classes. Despite its importance, there is a lack of theoretical
foundations for this problem. This paper bridges the gap by formalizing a
graph-theoretic framework tailored for the open-world setting, where the
clustering can be theoretically characterized by graph factorization. Our
graph-theoretic framework illuminates practical algorithms and provides
guarantees. In particular, based on our graph formulation, we apply the
algorithm called Spectral Open-world Representation Learning (SORL), and show
that minimizing our loss is equivalent to performing spectral decomposition on
the graph. Such equivalence allows us to derive a provable error bound on the
clustering performance for both known and novel classes, and analyze rigorously
when labeled data helps. Empirically, SORL can match or outperform several
strong baselines on common benchmark datasets, which is appealing for practical
usage while enjoying theoretical guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03532">The Fairness Stitch: Unveiling the Potential of Model Stitching in Neural Network De-Biasing. (arXiv:2311.03532v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sulaiman_M/0/1/0/all/0/1">Modar Sulaiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kallol Roy</a></p>
<p>The pursuit of fairness in machine learning models has emerged as a critical
research challenge in different applications ranging from bank loan approval to
face detection. Despite the widespread adoption of artificial intelligence
algorithms across various domains, concerns persist regarding the presence of
biases and discrimination within these models. To address this pressing issue,
this study introduces a novel method called "The Fairness Stitch (TFS)" to
enhance fairness in deep learning models. This method combines model stitching
and training jointly, while incorporating fairness constraints. In this
research, we assess the effectiveness of our proposed method by conducting a
comprehensive evaluation of two well-known datasets, CelebA and UTKFace. We
systematically compare the performance of our approach with the existing
baseline method. Our findings reveal a notable improvement in achieving a
balanced trade-off between fairness and performance, highlighting the promising
potential of our method to address bias-related challenges and foster equitable
outcomes in machine learning models. This paper poses a challenge to the
conventional wisdom of the effectiveness of the last layer in deep learning
models for de-biasing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03534">PcLast: Discovering Plannable Continuous Latent States. (arXiv:2311.03534v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1">Anurag Koul</a>, <a href="http://arxiv.org/find/cs/1/au:+Sujit_S/0/1/0/all/0/1">Shivakanth Sujit</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shaoru Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1">Ben Evans</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lili Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Byron Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chari_R/0/1/0/all/0/1">Rajan Chari</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1">Riashat Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Seraj_R/0/1/0/all/0/1">Raihan Seraj</a>, <a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1">Yonathan Efroni</a>, <a href="http://arxiv.org/find/cs/1/au:+Molu_L/0/1/0/all/0/1">Lekan Molu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1">Miro Dudik</a>, <a href="http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1">John Langford</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1">Alex Lamb</a></p>
<p>Goal-conditioned planning benefits from learned low-dimensional
representations of rich, high-dimensional observations. While compact latent
representations, typically learned from variational autoencoders or inverse
dynamics, enable goal-conditioned planning they ignore state affordances, thus
hampering their sample-efficient planning capabilities. In this paper, we learn
a representation that associates reachable states together for effective onward
planning. We first learn a latent representation with multi-step inverse
dynamics (to remove distracting information); and then transform this
representation to associate reachable states together in $\ell_2$ space. Our
proposals are rigorously tested in various simulation testbeds. Numerical
results in reward-based and reward-free settings show significant improvements
in sampling efficiency, and yields layered state abstractions that enable
computationally efficient hierarchical planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03547">InterVLS: Interactive Model Understanding and Improvement with Vision-Language Surrogates. (arXiv:2311.03547v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jinbin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wenbin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1">Liang Gou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1">Liu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Bryan_C/0/1/0/all/0/1">Chris Bryan</a></p>
<p>Deep learning models are widely used in critical applications, highlighting
the need for pre-deployment model understanding and improvement. Visual
concept-based methods, while increasingly used for this purpose, face
challenges: (1) most concepts lack interpretability, (2) existing methods
require model knowledge, often unavailable at run time. Additionally, (3) there
lacks a no-code method for post-understanding model improvement. Addressing
these, we present InterVLS. The system facilitates model understanding by
discovering text-aligned concepts, measuring their influence with
model-agnostic linear surrogates. Employing visual analytics, InterVLS offers
concept-based explanations and performance insights. It enables users to adjust
concept influences to update a model, facilitating no-code model improvement.
We evaluate InterVLS in a user study, illustrating its functionality with two
scenarios. Results indicates that InterVLS is effective to help users identify
influential concepts to a model, gain insights and adjust concept influence to
improve the model. We conclude with a discussion based on our study results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03557">Spatio-Temporal Similarity Measure based Multi-Task Learning for Predicting Alzheimer&#x27;s Disease Progression using MRI Data. (arXiv:2311.03557v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xulong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Menghui Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Jun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1">Po Yang</a></p>
<p>Identifying and utilising various biomarkers for tracking Alzheimer's disease
(AD) progression have received many recent attentions and enable helping
clinicians make the prompt decisions. Traditional progression models focus on
extracting morphological biomarkers in regions of interest (ROIs) from MRI/PET
images, such as regional average cortical thickness and regional volume. They
are effective but ignore the relationships between brain ROIs over time, which
would lead to synergistic deterioration. For exploring the synergistic
deteriorating relationship between these biomarkers, in this paper, we propose
a novel spatio-temporal similarity measure based multi-task learning approach
for effectively predicting AD progression and sensitively capturing the
critical relationships between biomarkers. Specifically, we firstly define a
temporal measure for estimating the magnitude and velocity of biomarker change
over time, which indicate a changing trend(temporal). Converting this trend
into the vector, we then compare this variability between biomarkers in a
unified vector space(spatial). The experimental results show that compared with
directly ROI based learning, our proposed method is more effective in
predicting disease progression. Our method also enables performing longitudinal
stability selection to identify the changing relationships between biomarkers,
which play a key role in disease progression. We prove that the synergistic
deteriorating biomarkers between cortical volumes or surface areas have a
significant effect on the cognitive prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03564">Low-Rank MDPs with Continuous Action Spaces. (arXiv:2311.03564v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1">Andrew Bennett</a>, <a href="http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1">Nathan Kallus</a>, <a href="http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1">Miruna Oprescu</a></p>
<p>Low-Rank Markov Decision Processes (MDPs) have recently emerged as a
promising framework within the domain of reinforcement learning (RL), as they
allow for provably approximately correct (PAC) learning guarantees while also
incorporating ML algorithms for representation learning. However, current
methods for low-rank MDPs are limited in that they only consider finite action
spaces, and give vacuous bounds as $|\mathcal{A}| \to \infty$, which greatly
limits their applicability. In this work, we study the problem of extending
such methods to settings with continuous actions, and explore multiple concrete
approaches for performing this extension. As a case study, we consider the
seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic
method for PAC RL with low-rank MDPs. We show that, without any modifications
to the algorithm, we obtain similar PAC bound when actions are allowed to be
continuous. Specifically, when the model for transition functions satisfies a
Holder smoothness condition w.r.t. actions, and either the policy class has a
uniformly bounded minimum density or the reward function is also Holder smooth,
we obtain a polynomial PAC bound that depends on the order of smoothness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03566">Measuring Adversarial Datasets. (arXiv:2311.03566v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuanchen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Raoyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1">Vijay Viswanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1">Tzu-Sheng Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tongshuang Wu</a></p>
<p>In the era of widespread public use of AI systems across various domains,
ensuring adversarial robustness has become increasingly vital to maintain
safety and prevent undesirable errors. Researchers have curated various
adversarial datasets (through perturbations) for capturing model deficiencies
that cannot be revealed in standard benchmark datasets. However, little is
known about how these adversarial examples differ from the original data
points, and there is still no methodology to measure the intended and
unintended consequences of those adversarial transformations. In this research,
we conducted a systematic survey of existing quantifiable metrics that describe
text instances in NLP tasks, among dimensions of difficulty, diversity, and
disagreement. We selected several current adversarial effect datasets and
compared the distributions between the original and their adversarial
counterparts. The results provide valuable insights into what makes these
datasets more challenging from a metrics perspective and whether they align
with underlying assumptions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03578">Generative Diffusion Models for Lattice Field Theory. (arXiv:2311.03578v1 [hep-lat])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-lat/1/au:+Wang_L/0/1/0/all/0/1">Lingxiao Wang</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Aarts_G/0/1/0/all/0/1">Gert Aarts</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Zhou_K/0/1/0/all/0/1">Kai Zhou</a></p>
<p>This study delves into the connection between machine learning and lattice
field theory by linking generative diffusion models (DMs) with stochastic
quantization, from a stochastic differential equation perspective. We show that
DMs can be conceptualized by reversing a stochastic process driven by the
Langevin equation, which then produces samples from an initial distribution to
approximate the target distribution. In a toy model, we highlight the
capability of DMs to learn effective actions. Furthermore, we demonstrate its
feasibility to act as a global sampler for generating configurations in the
two-dimensional $\phi^4$ quantum lattice field theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03583">Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search. (arXiv:2311.03583v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehrabian_A/0/1/0/all/0/1">Abbas Mehrabian</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Ankit Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunjik Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonnerat_N/0/1/0/all/0/1">Nicolas Sonnerat</a>, <a href="http://arxiv.org/find/cs/1/au:+Balog_M/0/1/0/all/0/1">Matej Balog</a>, <a href="http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1">Gheorghe Comanici</a>, <a href="http://arxiv.org/find/cs/1/au:+Berariu_T/0/1/0/all/0/1">Tudor Berariu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1">Andrew Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1">Anian Ruoss</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulanova_A/0/1/0/all/0/1">Anna Bulanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Toyama_D/0/1/0/all/0/1">Daniel Toyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Blackwell_S/0/1/0/all/0/1">Sam Blackwell</a>, <a href="http://arxiv.org/find/cs/1/au:+Paredes_B/0/1/0/all/0/1">Bernardino Romera Paredes</a>, <a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1">Petar Veli&#x10d;kovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Orseau_L/0/1/0/all/0/1">Laurent Orseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joonkyung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Naredla_A/0/1/0/all/0/1">Anurag Murty Naredla</a>, <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1">Adam Zsolt Wagner</a></p>
<p>This work studies a central extremal graph theory problem inspired by a 1975
conjecture of Erd\H{o}s, which aims to find graphs with a given size (number of
nodes) that maximize the number of edges without having 3- or 4-cycles. We
formulate this problem as a sequential decision-making problem and compare
AlphaZero, a neural network-guided tree search, with tabu search, a heuristic
local search method. Using either method, by introducing a curriculum --
jump-starting the search for larger graphs using good graphs found at smaller
sizes -- we improve the state-of-the-art lower bounds for several sizes. We
also propose a flexible graph-generation environment and a
permutation-invariant network architecture for learning to search in the space
of graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03609">Testing RadiX-Nets: Advances in Viable Sparse Topologies. (arXiv:2311.03609v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwak_K/0/1/0/all/0/1">Kevin Kwak</a>, <a href="http://arxiv.org/find/cs/1/au:+West_Z/0/1/0/all/0/1">Zack West</a>, <a href="http://arxiv.org/find/cs/1/au:+Jananthan_H/0/1/0/all/0/1">Hayden Jananthan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1">Jeremy Kepner</a></p>
<p>The exponential growth of data has sparked computational demands on ML
research and industry use. Sparsification of hyper-parametrized deep neural
networks (DNNs) creates simpler representations of complex data. Past research
has shown that some sparse networks achieve similar performance as dense ones,
reducing runtime and storage. RadiX-Nets, a subgroup of sparse DNNs, maintain
uniformity which counteracts their lack of neural connections. Generation,
independent of a dense network, yields faster asymptotic training and removes
the need for costly pruning. However, little work has been done on RadiX-Nets,
making testing challenging. This paper presents a testing suite for RadiX-Nets
in TensorFlow. We test RadiX-Net performance to streamline processing in
scalable models, revealing relationships between network topology,
initialization, and training behavior. We also encounter "strange models" that
train inconsistently and to lower accuracy while models of similar sparsity
train well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03611">Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication. (arXiv:2311.03611v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Chaofei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahn_N/0/1/0/all/0/1">Nick Hahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamdar_F/0/1/0/all/0/1">Foram Kamdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Avansino_D/0/1/0/all/0/1">Donald Avansino</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_G/0/1/0/all/0/1">Guy H. Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hochberg_L/0/1/0/all/0/1">Leigh Hochberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenoy_K/0/1/0/all/0/1">Krishna V. Shenoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1">Jaimie M. Henderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Willett_F/0/1/0/all/0/1">Francis R. Willett</a></p>
<p>Intracortical brain-computer interfaces (iBCIs) have shown promise for
restoring rapid communication to people with neurological disorders such as
amyotrophic lateral sclerosis (ALS). However, to maintain high performance over
time, iBCIs typically need frequent recalibration to combat changes in the
neural recordings that accrue over days. This requires iBCI users to stop using
the iBCI and engage in supervised data collection, making the iBCI system hard
to use. In this paper, we propose a method that enables self-recalibration of
communication iBCIs without interrupting the user. Our method leverages large
language models (LMs) to automatically correct errors in iBCI outputs. The
self-recalibration process uses these corrected outputs ("pseudo-labels") to
continually update the iBCI decoder online. Over a period of more than one year
(403 days), we evaluated our Continual Online Recalibration with Pseudo-labels
(CORP) framework with one clinical trial participant. CORP achieved a stable
decoding accuracy of 93.84% in an online handwriting iBCI task, significantly
outperforming other baseline methods. Notably, this is the longest-running iBCI
stability demonstration involving a human participant. Our results provide the
first evidence for long-term stabilization of a plug-and-play, high-performance
communication iBCI, addressing a major barrier for the clinical translation of
iBCIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03615">CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers. (arXiv:2311.03615v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jieming Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shaolei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jie Xu</a></p>
<p>Training large-scale artificial intelligence (AI) models demands significant
computational power and energy, leading to increased carbon footprint with
potential environmental repercussions. This paper delves into the challenges of
training AI models across geographically distributed (geo-distributed) data
centers, emphasizing the balance between learning performance and carbon
footprint. We consider Federated Learning (FL) as a solution, which prioritizes
model parameter exchange over raw data, ensuring data privacy and compliance
with local regulations. Given the variability in carbon intensity across
regions, we propose a new framework called CAFE (short for Carbon-Aware
Federated Learning) to optimize training within a fixed carbon footprint
budget. Our approach incorporates coreset selection to assess learning
performance, employs the Lyapunov drift-plus-penalty framework to address the
unpredictability of future carbon intensity, and devises an efficient algorithm
to address the combinatorial complexity of the data center selection. Through
extensive simulations using real-world carbon intensity data, we demonstrate
the efficacy of our algorithm, highlighting its superiority over existing
methods in optimizing learning performance while minimizing environmental
impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03621">Exploring Latent Spaces of Tonal Music using Variational Autoencoders. (arXiv:2311.03621v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carvalho_N/0/1/0/all/0/1">N&#xe1;dia Carvalho</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernardes_G/0/1/0/all/0/1">Gilberto Bernardes</a></p>
<p>Variational Autoencoders (VAEs) have proven to be effective models for
producing latent representations of cognitive and semantic value. We assess the
degree to which VAEs trained on a prototypical tonal music corpus of 371 Bach's
chorales define latent spaces representative of the circle of fifths and the
hierarchical relation of each key component pitch as drawn in music cognition.
In detail, we compare the latent space of different VAE corpus encodings --
Piano roll, MIDI, ABC, Tonnetz, DFT of pitch, and pitch class distributions --
in providing a pitch space for key relations that align with cognitive
distances. We evaluate the model performance of these encodings using objective
metrics to capture accuracy, mean square error (MSE), KL-divergence, and
computational cost. The ABC encoding performs the best in reconstructing the
original data, while the Pitch DFT seems to capture more information from the
latent space. Furthermore, an objective evaluation of 12 major or minor
transpositions per piece is adopted to quantify the alignment of 1) intra- and
inter-segment distances per key and 2) the key distances to cognitive pitch
spaces. Our results show that Pitch DFT VAE latent spaces align best with
cognitive spaces and provide a common-tone space where overlapping objects
within a key are fuzzy clusters, which impose a well-defined order of
structural significance or stability -- i.e., a tonal hierarchy. Tonal
hierarchies of different keys can be used to measure key distances and the
relationships of their in-key components at multiple hierarchies (e.g., notes
and chords). The implementation of our VAE and the encodings framework are made
available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03622">TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer. (arXiv:2311.03622v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamada_J/0/1/0/all/0/1">Jun Yamada</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigter_M/0/1/0/all/0/1">Marc Rigter</a>, <a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1">Jack Collins</a>, <a href="http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1">Ingmar Posner</a></p>
<p>Model-based RL is a promising approach for real-world robotics due to its
improved sample efficiency and generalization capabilities compared to
model-free RL. However, effective model-based RL solutions for vision-based
real-world applications require bridging the sim-to-real gap for any world
model learnt. Due to its significant computational cost, standard domain
randomisation does not provide an effective solution to this problem. This
paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real
Transfer) to achieve efficient sim-to-real transfer of vision-based model-based
RL using distillation. Specifically, TWIST leverages state observations as
readily accessible, privileged information commonly garnered from a simulator
to significantly accelerate sim-to-real transfer. Specifically, a teacher world
model is trained efficiently on state information. At the same time, a matching
dataset is collected of domain-randomised image observations. The teacher world
model then supervises a student world model that takes the domain-randomised
image observations as input. By distilling the learned latent dynamics model
from the teacher to the student model, TWIST achieves efficient and effective
sim-to-real transfer for vision-based model-based RL tasks. Experiments in
simulated and real robotics tasks demonstrate that our approach outperforms
naive domain randomisation and model-free methods in terms of sample efficiency
and task performance of sim-to-real transfer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03624">Are Words Enough? On the semantic conditioning of affective music generation. (arXiv:2311.03624v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Forero_J/0/1/0/all/0/1">Jorge Forero</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernardes_G/0/1/0/all/0/1">Gilberto Bernardes</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendes_M/0/1/0/all/0/1">M&#xf3;nica Mendes</a></p>
<p>Music has been commonly recognized as a means of expressing emotions. In this
sense, an intense debate emerges from the need to verbalize musical emotions.
This concern seems highly relevant today, considering the exponential growth of
natural language processing using deep learning models where it is possible to
prompt semantic propositions to generate music automatically. This scoping
review aims to analyze and discuss the possibilities of music generation
conditioned by emotions. To address this topic, we propose a historical
perspective that encompasses the different disciplines and methods contributing
to this topic. In detail, we review two main paradigms adopted in automatic
music generation: rules-based and machine-learning models. Of note are the deep
learning architectures that aim to generate high-fidelity music from textual
descriptions. These models raise fundamental questions about the expressivity
of music, including whether emotions can be represented with words or expressed
through them. We conclude that overcoming the limitation and ambiguity of
language to express emotions through music, some of the use of deep learning
with natural language has the potential to impact the creative industries by
providing powerful tools to prompt and generate new musical works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03629">Random Field Augmentations for Self-Supervised Representation Learning. (arXiv:2311.03629v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1">Philip Andrew Mansfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Afkanpour_A/0/1/0/all/0/1">Arash Afkanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Morningstar_W/0/1/0/all/0/1">Warren Richard Morningstar</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a></p>
<p>Self-supervised representation learning is heavily dependent on data
augmentations to specify the invariances encoded in representations. Previous
work has shown that applying diverse data augmentations is crucial to
downstream performance, but augmentation techniques remain under-explored. In
this work, we propose a new family of local transformations based on Gaussian
random fields to generate image augmentations for self-supervised
representation learning. These transformations generalize the well-established
affine and color transformations (translation, rotation, color jitter, etc.)
and greatly increase the space of augmentations by allowing transformation
parameter values to vary from pixel to pixel. The parameters are treated as
continuous functions of spatial coordinates, and modeled as independent
Gaussian random fields. Empirical results show the effectiveness of the new
transformations for self-supervised representation learning. Specifically, we
achieve a 1.7% top-1 accuracy improvement over baseline on ImageNet downstream
classification, and a 3.6% improvement on out-of-distribution iNaturalist
downstream classification. However, due to the flexibility of the new
transformations, learned representations are sensitive to hyperparameters.
While mild transformations improve representations, we observe that strong
transformations can degrade the structure of an image, indicating that
balancing the diversity and strength of augmentations is important for
improving generalization of learned representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03630">Counterfactual Data Augmentation with Contrastive Learning. (arXiv:2311.03630v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aloui_A/0/1/0/all/0/1">Ahmed Aloui</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Juncheng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1">Cat P. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1">Vahid Tarokh</a></p>
<p>Statistical disparity between distinct treatment groups is one of the most
significant challenges for estimating Conditional Average Treatment Effects
(CATE). To address this, we introduce a model-agnostic data augmentation method
that imputes the counterfactual outcomes for a selected subset of individuals.
Specifically, we utilize contrastive learning to learn a representation space
and a similarity measure such that in the learned representation space close
individuals identified by the learned similarity measure have similar potential
outcomes. This property ensures reliable imputation of counterfactual outcomes
for the individuals with close neighbors from the alternative treatment group.
By augmenting the original dataset with these reliable imputations, we can
effectively reduce the discrepancy between different treatment groups, while
inducing minimal imputation error. The augmented dataset is subsequently
employed to train CATE estimation models. Theoretical analysis and experimental
studies on synthetic and semi-synthetic benchmarks demonstrate that our method
achieves significant improvements in both performance and robustness to
overfitting across state-of-the-art models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03633">Innovation and Word Usage Patterns in Machine Learning. (arXiv:2311.03633v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borges_V/0/1/0/all/0/1">V&#xed;tor Bandeira Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Cajueiro_D/0/1/0/all/0/1">Daniel Oliveira Cajueiro</a></p>
<p>In this study, we delve into the dynamic landscape of machine learning
research evolution. Initially, through the utilization of Latent Dirichlet
Allocation, we discern pivotal themes and fundamental concepts that have
emerged within the realm of machine learning. Subsequently, we undertake a
comprehensive analysis to track the evolutionary trajectories of these
identified themes. To quantify the novelty and divergence of research
contributions, we employ the Kullback-Leibler Divergence metric. This
statistical measure serves as a proxy for ``surprise'', indicating the extent
of differentiation between the content of academic papers and the subsequent
developments in research. By amalgamating these insights, we gain the ability
to ascertain the pivotal roles played by prominent researchers and the
significance of specific academic venues (periodicals and conferences) within
the machine learning domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03636">Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach. (arXiv:2311.03636v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Md Rabiul Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1">Nahian Ismail Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Md Hadisur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Syed_M/0/1/0/all/0/1">Md Asif Bin Syed</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1">JuHyeong Ryu</a></p>
<p>The integration of Artificial Intelligence (AI) into education is a recent
development, with chatbots emerging as a noteworthy addition to this
transformative landscape. As online learning platforms rapidly advance,
students need to adapt swiftly to excel in this dynamic environment.
Consequently, understanding the acceptance of chatbots, particularly those
employing Large Language Model (LLM) such as Chat Generative Pretrained
Transformer (ChatGPT), Google Bard, and other interactive AI technologies, is
of paramount importance. However, existing research on chatbots in education
has overlooked key behavior-related aspects, such as Optimism, Innovativeness,
Discomfort, Insecurity, Transparency, Ethics, Interaction, Engagement, and
Accuracy, creating a significant literature gap. To address this gap, this
study employs Partial Least Squares Structural Equation Modeling (PLS-SEM) to
investigate the determinant of chatbots adoption in education among students,
considering the Technology Readiness Index (TRI) and Technology Acceptance
Model (TAM). Utilizing a five-point Likert scale for data collection, we
gathered a total of 185 responses, which were analyzed using R-Studio software.
We established 12 hypotheses to achieve its objectives. The results showed that
Optimism and Innovativeness are positively associated with Perceived Ease of
Use (PEOU) and Perceived Usefulness (PU). Conversely, Discomfort and Insecurity
negatively impact PEOU, with only Insecurity negatively affecting PU. These
findings provide insights for future technology designers, elucidating critical
user behavior factors influencing chatbots adoption and utilization in
educational contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03639">A Physics-Guided Bi-Fidelity Fourier-Featured Operator Learning Framework for Predicting Time Evolution of Drag and Lift Coefficients. (arXiv:2311.03639v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mollaali_A/0/1/0/all/0/1">Amirhossein Mollaali</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahin_I/0/1/0/all/0/1">Izzet Sahin</a>, <a href="http://arxiv.org/find/cs/1/au:+Raza_I/0/1/0/all/0/1">Iqrar Raza</a>, <a href="http://arxiv.org/find/cs/1/au:+Moya_C/0/1/0/all/0/1">Christian Moya</a>, <a href="http://arxiv.org/find/cs/1/au:+Paniagua_G/0/1/0/all/0/1">Guillermo Paniagua</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guang Lin</a></p>
<p>In the pursuit of accurate experimental and computational data while
minimizing effort, there is a constant need for high-fidelity results. However,
achieving such results often requires significant computational resources. To
address this challenge, this paper proposes a deep operator learning-based
framework that requires a limited high-fidelity dataset for training. We
introduce a novel physics-guided, bi-fidelity, Fourier-featured Deep Operator
Network (DeepONet) framework that effectively combines low and high-fidelity
datasets, leveraging the strengths of each. In our methodology, we began by
designing a physics-guided Fourier-featured DeepONet, drawing inspiration from
the intrinsic physical behavior of the target solution. Subsequently, we train
this network to primarily learn the low-fidelity solution, utilizing an
extensive dataset. This process ensures a comprehensive grasp of the
foundational solution patterns. Following this foundational learning, the
low-fidelity deep operator network's output is enhanced using a physics-guided
Fourier-featured residual deep operator network. This network refines the
initial low-fidelity output, achieving the high-fidelity solution by employing
a small high-fidelity dataset for training. Notably, in our framework, we
employ the Fourier feature network as the Trunk network for the DeepONets,
given its proficiency in capturing and learning the oscillatory nature of the
target solution with high precision. We validate our approach using a
well-known 2D benchmark cylinder problem, which aims to predict the time
trajectories of lift and drag coefficients. The results highlight that the
physics-guided Fourier-featured deep operator network, serving as a
foundational building block of our framework, possesses superior predictive
capability for the lift and drag coefficients compared to its data-driven
counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03651">SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations. (arXiv:2311.03651v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Chan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jaekyung Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Bobda_C/0/1/0/all/0/1">Christophe Bobda</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1">Seung-Woo Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seong-Woo Kim</a></p>
<p>Robotic agents trained using reinforcement learning have the problem of
taking unreliable actions in an out-of-distribution (OOD) state. Agents can
easily become OOD in real-world environments because it is almost impossible
for them to visit and learn the entire state space during training.
Unfortunately, unreliable actions do not ensure that agents perform their
original tasks successfully. Therefore, agents should be able to recognize
whether they are in OOD states and learn how to return to the learned state
distribution rather than continue to take unreliable actions. In this study, we
propose a novel method for retraining agents to recover from OOD situations in
a self-supervised manner when they fall into OOD states. Our in-depth
experimental results demonstrate that our method substantially improves the
agent's ability to recover from OOD situations in terms of sample efficiency
and restoration of the performance for the original tasks. Moreover, we show
that our method can retrain the agent to recover from OOD situations even when
in-distribution states are difficult to visit through exploration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03658">The Linear Representation Hypothesis and the Geometry of Large Language Models. (arXiv:2311.03658v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1">Kiho Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1">Yo Joong Choe</a>, <a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1">Victor Veitch</a></p>
<p>Informally, the 'linear representation hypothesis' is the idea that
high-level concepts are represented linearly as directions in some
representation space. In this paper, we address two closely related questions:
What does "linear representation" actually mean? And, how do we make sense of
geometric notions (e.g., cosine similarity or projection) in the representation
space? To answer these, we use the language of counterfactuals to give two
formalizations of "linear representation", one in the output (word)
representation space, and one in the input (sentence) space. We then prove
these connect to linear probing and model steering, respectively. To make sense
of geometric notions, we use the formalization to identify a particular
(non-Euclidean) inner product that respects language structure in a sense we
make precise. Using this causal inner product, we show how to unify all notions
of linear representation. In particular, this allows the construction of probes
and steering vectors using counterfactual pairs. Experiments with LLaMA-2
demonstrate the existence of linear representations of concepts, the connection
to interpretation and control, and the fundamental role of the choice of inner
product.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03661">Graph Neural Networks for Power Grid Operational Risk Assessment. (arXiv:2311.03661v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yadong Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Karve_P/0/1/0/all/0/1">Pranav M Karve</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahadevan_S/0/1/0/all/0/1">Sankaran Mahadevan</a></p>
<p>In this article, the utility of graph neural network (GNN) surrogates for
Monte Carlo (MC) sampling-based risk quantification in daily operations of
power grid is investigated. The MC simulation process necessitates solving a
large number of optimal power flow (OPF) problems corresponding to the sample
values of stochastic grid variables (power demand and renewable generation),
which is computationally prohibitive. Computationally inexpensive surrogates of
the OPF problem provide an attractive alternative for expedited MC simulation.
GNN surrogates are especially suitable due to their superior ability to handle
graph-structured data. Therefore, GNN surrogates of OPF problem are trained
using supervised learning. They are then used to obtain Monte Carlo (MC)
samples of the quantities of interest (operating reserve, transmission line
flow) given the (hours-ahead) probabilistic wind generation and load forecast.
The utility of GNN surrogates is evaluated by comparing OPF-based and GNN-based
grid reliability and risk for IEEE Case118 synthetic grid. It is shown that the
GNN surrogates are sufficiently accurate for predicting the (bus-level,
branch-level and system-level) grid state and enable fast as well as accurate
operational risk quantification for power grids. The article thus develops
various tools for fast reliability and risk quantification for real-world power
grids using GNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03669">Stable Modular Control via Contraction Theory for Reinforcement Learning. (arXiv:2311.03669v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1">Bing Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1">Jean-Jacques Slotine</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1">Quang-Cuong Pham</a></p>
<p>We propose a novel way to integrate control techniques with reinforcement
learning (RL) for stability, robustness, and generalization: leveraging
contraction theory to realize modularity in neural control, which ensures that
combining stable subsystems can automatically preserve the stability. We
realize such modularity via signal composition and dynamic decomposition.
Signal composition creates the latent space, within which RL applies to
maximizing rewards. Dynamic decomposition is realized by coordinate
transformation that creates an auxiliary space, within which the latent signals
are coupled in the way that their combination can preserve stability provided
each signal, that is, each subsystem, has stable self-feedbacks. Leveraging
modularity, the nonlinear stability problem is deconstructed into algebraically
solvable ones, the stability of the subsystems in the auxiliary space, yielding
linear constraints on the input gradients of control networks that can be as
simple as switching the signs of network weights. This minimally invasive
method for stability allows arguably easy integration into the modular neural
architectures in machine learning, like hierarchical RL, and improves their
performance. We demonstrate in simulation the necessity and the effectiveness
of our method: the necessity for robustness and generalization, and the
effectiveness in improving hierarchical RL for manipulation learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03683">Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks. (arXiv:2311.03683v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1">Ahmad Rashid</a>, <a href="http://arxiv.org/find/cs/1/au:+Hacker_S/0/1/0/all/0/1">Serena Hacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guojun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1">Agustinus Kristiadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1">Pascal Poupart</a></p>
<p>Discriminatively trained, deterministic neural networks are the de facto
choice for classification problems. However, even though they achieve
state-of-the-art results on in-domain test sets, they tend to be overconfident
on out-of-distribution (OOD) data. For instance, ReLU networks -- a popular
class of neural network architectures -- have been shown to almost always yield
high confidence predictions when the test data are far away from the training
set, even when they are trained with OOD data. We overcome this problem by
adding a term to the output of the neural network that corresponds to the logit
of an extra class, that we design to dominate the logits of the original
classes as we move away from the training data.This technique provably prevents
arbitrarily high confidence on far-away test data while maintaining a simple
discriminative point-estimate training. Evaluation on various benchmarks
demonstrates strong performance against competitive baselines on both far-away
and realistic OOD data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03685">Dynamic Non-monotone Submodular Maximization. (arXiv:2311.03685v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banihashem_K/0/1/0/all/0/1">Kiarash Banihashem</a>, <a href="http://arxiv.org/find/cs/1/au:+Biabani_L/0/1/0/all/0/1">Leyla Biabani</a>, <a href="http://arxiv.org/find/cs/1/au:+Goudarzi_S/0/1/0/all/0/1">Samira Goudarzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jabbarzade_P/0/1/0/all/0/1">Peyman Jabbarzade</a>, <a href="http://arxiv.org/find/cs/1/au:+Monemizadeh_M/0/1/0/all/0/1">Morteza Monemizadeh</a></p>
<p>Maximizing submodular functions has been increasingly used in many
applications of machine learning, such as data summarization, recommendation
systems, and feature selection. Moreover, there has been a growing interest in
both submodular maximization and dynamic algorithms. In 2020, Monemizadeh and
Lattanzi, Mitrovic, Norouzi{-}Fard, Tarnawski, and Zadimoghaddam initiated
developing dynamic algorithms for the monotone submodular maximization problem
under the cardinality constraint $k$. Recently, there have been some
improvements on the topic made by Banihashem, Biabani, Goudarzi, Hajiaghayi,
Jabbarzade, and Monemizadeh. In 2022, Chen and Peng studied the complexity of
this problem and raised an important open question: "Can we extend [fully
dynamic] results (algorithm or hardness) to non-monotone submodular
maximization?". We affirmatively answer their question by demonstrating a
reduction from maximizing a non-monotone submodular function under the
cardinality constraint $k$ to maximizing a monotone submodular function under
the same constraint. Through this reduction, we obtain the first dynamic
algorithms to solve the non-monotone submodular maximization problem under the
cardinality constraint $k$. Our algorithms maintain an
$(8+\epsilon)$-approximate of the solution and use expected amortized
$O(\epsilon^{-3}k^3\log^3(n)\log(k))$ or $O(\epsilon^{-1}k^2\log^3(k))$ oracle
queries per update, respectively. Furthermore, we showcase the benefits of our
dynamic algorithm for video summarization and max-cut problems on several
real-world data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03687">Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models. (arXiv:2311.03687v1 [cs.PF])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Longteng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zeyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xinglin Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1">Peijie Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Ruibo Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Rui Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1">Qiong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shaohuai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiaowen Chu</a></p>
<p>Large Language Models (LLMs) have seen great advance in both academia and
industry, and their popularity results in numerous open-source frameworks and
techniques in accelerating LLM pre-training, fine-tuning, and inference.
Training and deploying LLMs are expensive as it requires considerable computing
resources and memory, hence many efficient approaches have been developed for
improving system pipelines as well as operators. However, the runtime
performance can vary significantly across hardware and software stacks, which
makes it difficult to choose the best configuration. In this work, we aim to
benchmark the performance from both macro and micro perspectives. First, we
benchmark the end-to-end performance of pre-training, fine-tuning, and serving
LLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and
70B) on three 8-GPU platforms with and without individual optimization
techniques, including ZeRO, quantization, recomputation, FlashAttention. Then,
we dive deeper to provide a detailed runtime analysis of the sub-modules,
including computing and communication operators in LLMs. For end users, our
benchmark and findings help better understand different optimization
techniques, training and inference frameworks, together with hardware platforms
in choosing configurations for deploying LLMs. For researchers, our in-depth
module-wise analyses discover potential opportunities for future work to
further optimize the runtime performance of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03695">Context Shift Reduction for Offline Meta-Reinforcement Learning. (arXiv:2311.03695v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yunkai Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiaming Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1">Qi Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Shaohui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1">Siming Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ruizhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1">Zidong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xing Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Ling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yunji Chen</a></p>
<p>Offline meta-reinforcement learning (OMRL) utilizes pre-collected offline
datasets to enhance the agent's generalization ability on unseen tasks.
However, the context shift problem arises due to the distribution discrepancy
between the contexts used for training (from the behavior policy) and testing
(from the exploration policy). The context shift problem leads to incorrect
task inference and further deteriorates the generalization ability of the
meta-policy. Existing OMRL methods either overlook this problem or attempt to
mitigate it with additional information. In this paper, we propose a novel
approach called Context Shift Reduction for OMRL (CSRO) to address the context
shift problem with only offline datasets. The key insight of CSRO is to
minimize the influence of policy in context during both the meta-training and
meta-test phases. During meta-training, we design a max-min mutual information
representation learning mechanism to diminish the impact of the behavior policy
on task representation. In the meta-test phase, we introduce the non-prior
context collection strategy to reduce the effect of the exploration policy.
Experimental results demonstrate that CSRO significantly reduces the context
shift and improves the generalization ability, surpassing previous methods
across various challenging domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03698">A Novel Variational Lower Bound for Inverse Reinforcement Learning. (arXiv:2311.03698v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gui_Y/0/1/0/all/0/1">Yikang Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1">Prashant Doshi</a></p>
<p>Inverse reinforcement learning (IRL) seeks to learn the reward function from
expert trajectories, to understand the task for imitation or collaboration
thereby removing the need for manual reward engineering. However, IRL in the
context of large, high-dimensional problems with unknown dynamics has been
particularly challenging. In this paper, we present a new Variational Lower
Bound for IRL (VLB-IRL), which is derived under the framework of a
probabilistic graphical model with an optimality node. Our method
simultaneously learns the reward function and policy under the learned reward
function by maximizing the lower bound, which is equivalent to minimizing the
reverse Kullback-Leibler divergence between an approximated distribution of
optimality given the reward function and the true distribution of optimality
given trajectories. This leads to a new IRL method that learns a valid reward
function such that the policy under the learned reward achieves expert-level
performance on several known domains. Importantly, the method outperforms the
existing state-of-the-art IRL algorithms on these domains by demonstrating
better reward from the learned policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03701">Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation. (arXiv:2311.03701v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacobson_M/0/1/0/all/0/1">Maxwell Joseph Jacobson</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yexiang Xue</a></p>
<p>Meta Reinforcement Learning (Meta RL) trains agents that adapt to
fast-changing environments and tasks. Current strategies often lose adaption
efficiency due to the passive nature of model exploration, causing delayed
understanding of new transition dynamics. This results in particularly
fast-evolving tasks being impossible to solve. We propose a novel approach,
Hypothesis Network Planned Exploration (HyPE), that integrates an active and
planned exploration process via the hypothesis network to optimize adaptation
speed. HyPE uses a generative hypothesis network to form potential models of
state transition dynamics, then eliminates incorrect models through
strategically devised experiments. Evaluated on a symbolic version of the
Alchemy game, HyPE outpaces baseline methods in adaptation speed and model
accuracy, validating its potential in enhancing reinforcement learning
adaptation in rapidly evolving settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03703">Pipeline Parallelism for DNN Inference with Practical Performance Guarantees. (arXiv:2311.03703v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Archer_A/0/1/0/all/0/1">Aaron Archer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahrbach_M/0/1/0/all/0/1">Matthew Fahrbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kuikui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhu_P/0/1/0/all/0/1">Prakash Prabhu</a></p>
<p>We optimize pipeline parallelism for deep neural network (DNN) inference by
partitioning model graphs into $k$ stages and minimizing the running time of
the bottleneck stage, including communication. We design practical algorithms
for this NP-hard problem and show that they are nearly optimal in practice by
comparing against strong lower bounds obtained via novel mixed-integer
programming (MIP) formulations. We apply these algorithms and lower-bound
methods to production models to achieve substantially improved approximation
guarantees compared to standard combinatorial lower bounds. For example,
evaluated via geometric means across production data with $k=16$ pipeline
stages, our MIP formulations more than double the lower bounds, improving the
approximation ratio from $2.175$ to $1.058$. This work shows that while
max-throughput partitioning is theoretically hard, we have a handle on the
algorithmic side of the problem in practice and much of the remaining challenge
is in developing more accurate cost models to feed into the partitioning
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03707">The NeurIPS 2022 Neural MMO Challenge: A Massively Multiagent Competition with Specialization and Trade. (arXiv:2311.03707v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1">Enhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Suarez_J/0/1/0/all/0/1">Joseph Suarez</a>, <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chenhui You</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bo Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bingcheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jun Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaolong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Clare Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1">Julian Togelius</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1">Sharada Mohanty</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1">Weijun Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1">Rui Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yibing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qinwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuejia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hanhui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shiqi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a></p>
<p>In this paper, we present the results of the NeurIPS-2022 Neural MMO
Challenge, which attracted 500 participants and received over 1,600
submissions. Like the previous IJCAI-2022 Neural MMO Challenge, it involved
agents from 16 populations surviving in procedurally generated worlds by
collecting resources and defeating opponents. This year's competition runs on
the latest v1.6 Neural MMO, which introduces new equipment, combat, trading,
and a better scoring system. These elements combine to pose additional
robustness and generalization challenges not present in previous competitions.
This paper summarizes the design and results of the challenge, explores the
potential of this environment as a benchmark for learning methods, and presents
some practical reinforcement learning training approaches for complex tasks
with sparse rewards. Additionally, we have open-sourced our baselines,
including environment wrappers, benchmarks, and visualization tools for future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03711">Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning. (arXiv:2311.03711v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1">Junmin Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruofan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1">Jennie Si</a></p>
<p>We address the issue of estimation bias in deep reinforcement learning (DRL)
by introducing solution mechanisms that include a new, twin TD-regularized
actor-critic (TDR) method. It aims at reducing both over and under-estimation
errors. With TDR and by combining good DRL improvements, such as distributional
learning and long N-step surrogate stage reward (LNSS) method, we show that our
new TDR-based actor-critic learning has enabled DRL methods to outperform their
respective baselines in challenging environments in DeepMind Control Suite.
Furthermore, they elevate TD3 and SAC respectively to a level of performance
comparable to that of D4PG (the current SOTA), and they also improve the
performance of D4PG to a new SOTA level measured by mean reward, convergence
speed, learning success rate, and learning variance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03713">Multimodal deep representation learning for quantum cross-platform verification. (arXiv:2311.03713v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1">Yang Qian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1">Yuxuan Du</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+He_Z/0/1/0/all/0/1">Zhenliang He</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Hsieh_M/0/1/0/all/0/1">Min-hsiu Hsieh</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Cross-platform verification, a critical undertaking in the realm of
early-stage quantum computing, endeavors to characterize the similarity of two
imperfect quantum devices executing identical algorithms, utilizing minimal
measurements. While the random measurement approach has been instrumental in
this context, the quasi-exponential computational demand with increasing qubit
count hurdles its feasibility in large-qubit scenarios. To bridge this
knowledge gap, here we introduce an innovative multimodal learning approach,
recognizing that the formalism of data in this task embodies two distinct
modalities: measurement outcomes and classical description of compiled circuits
on explored quantum devices, both enriched with unique information. Building
upon this insight, we devise a multimodal neural network to independently
extract knowledge from these modalities, followed by a fusion operation to
create a comprehensive data representation. The learned representation can
effectively characterize the similarity between the explored quantum devices
when executing new quantum algorithms not present in the training data. We
evaluate our proposal on platforms featuring diverse noise models, encompassing
system sizes up to 50 qubits. The achieved results demonstrate a
three-orders-of-magnitude improvement in prediction accuracy compared to the
random measurements and offer compelling evidence of the complementary roles
played by each modality in cross-platform verification. These findings pave the
way for harnessing the power of multimodal learning to overcome challenges in
wider quantum system learning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03714">Loss Balancing for Fair Supervised Learning. (arXiv:2311.03714v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalili_M/0/1/0/all/0/1">Mohammad Mahdi Khalili</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xueru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Abroshan_M/0/1/0/all/0/1">Mahed Abroshan</a></p>
<p>Supervised learning models have been used in various domains such as lending,
college admission, face recognition, natural language processing, etc. However,
they may inherit pre-existing biases from training data and exhibit
discrimination against protected social groups. Various fairness notions have
been proposed to address unfairness issues. In this work, we focus on Equalized
Loss (EL), a fairness notion that requires the expected loss to be
(approximately) equalized across different groups. Imposing EL on the learning
process leads to a non-convex optimization problem even if the loss function is
convex, and the existing fair learning algorithms cannot properly be adopted to
find the fair predictor under the EL constraint. This paper introduces an
algorithm that can leverage off-the-shelf convex programming tools (e.g.,
CVXPY) to efficiently find the global optimum of this non-convex optimization.
In particular, we propose the ELminimizer algorithm, which finds the optimal
fair predictor under EL by reducing the non-convex optimization to a sequence
of convex optimization problems. We theoretically prove that our algorithm
finds the global optimal solution under certain conditions. Then, we support
our theoretical results through several empirical studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03721">ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning. (arXiv:2311.03721v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaltenborn_J/0/1/0/all/0/1">Julia Kaltenborn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_C/0/1/0/all/0/1">Charlotte E. E. Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1">Venkatesh Ramesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1">Philippe Brouillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurwicz_Y/0/1/0/all/0/1">Yaniv Gurwicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagda_C/0/1/0/all/0/1">Chandni Nagda</a>, <a href="http://arxiv.org/find/cs/1/au:+Runge_J/0/1/0/all/0/1">Jakob Runge</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowack_P/0/1/0/all/0/1">Peer Nowack</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1">David Rolnick</a></p>
<p>Climate models have been key for assessing the impact of climate change and
simulating future climate scenarios. The machine learning (ML) community has
taken an increased interest in supporting climate scientists' efforts on
various tasks such as climate model emulation, downscaling, and prediction
tasks. Many of those tasks have been addressed on datasets created with single
climate models. However, both the climate science and ML communities have
suggested that to address those tasks at scale, we need large, consistent, and
ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset
containing the inputs and outputs of 36 climate models from the Input4MIPs and
CMIP6 archives. In addition, we provide a modular dataset pipeline for
retrieving and preprocessing additional climate models and scenarios. We
showcase the potential of our dataset by using it as a benchmark for ML-based
climate model emulation. We gain new insights about the performance and
generalization capabilities of the different ML models by analyzing their
performance across different climate models. Furthermore, the dataset can be
used to train an ML emulator on several climate models instead of just one.
Such a "super emulator" can quickly project new climate change scenarios,
complementing existing scenarios already provided to policymakers. We believe
ClimateSet will create the basis needed for the ML community to tackle
climate-related tasks at scale.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03732">Learning to Learn for Few-shot Continual Active Learning. (arXiv:2311.03732v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1">Stella Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Longxiang Gao</a></p>
<p>Continual learning strives to ensure stability in solving previously seen
tasks while demonstrating plasticity in a novel domain. Recent advances in CL
are mostly confined to a supervised learning setting, especially in NLP domain.
In this work, we consider a few-shot continual active learning (CAL) setting
where labeled data is inadequate, and unlabeled data is abundant but with a
limited annotation budget. We propose a simple but efficient method, called
Meta-Continual Active Learning. Specifically, we employ meta-learning and
experience replay to address the trade-off between stability and plasticity. As
a result, it finds an optimal initialization that efficiently utilizes
annotated information for fast adaptation while preventing catastrophic
forgetting of past tasks. We conduct extensive experiments to validate the
effectiveness of the proposed method and analyze the effect of various active
learning strategies and memory sample selection methods in a few-shot CAL
setup. Our experiment results demonstrate that random sampling is the best
default strategy for both active learning and memory sample selection to solve
few-shot CAL problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03733">Improved weight initialization for deep and narrow feedforward neural network. (arXiv:2311.03733v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyunwoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Seungyeop Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1">Hayoung Choi</a></p>
<p>Appropriate weight initialization settings, along with the ReLU activation
function, have been a cornerstone of modern deep learning, making it possible
to train and deploy highly effective and efficient neural network models across
diverse artificial intelligence. The problem of dying ReLU, where ReLU neurons
become inactive and yield zero output, presents a significant challenge in the
training of deep neural networks with ReLU activation function. Theoretical
research and various methods have been introduced to address the problem.
However, even with these methods and research, training remains challenging for
extremely deep and narrow feedforward networks with ReLU activation function.
In this paper, we propose a new weight initialization method to address this
issue. We prove the properties of the proposed initial weight matrix and
demonstrate how these properties facilitate the effective propagation of signal
vectors. Through a series of experiments and comparisons with existing methods,
we demonstrate the effectiveness of the new initialization method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03736">Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning. (arXiv:2311.03736v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suarez_J/0/1/0/all/0/1">Joseph Su&#xe1;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a>, <a href="http://arxiv.org/find/cs/1/au:+Choe_K/0/1/0/all/0/1">Kyoung Whan Choe</a>, <a href="http://arxiv.org/find/cs/1/au:+Bloomin_D/0/1/0/all/0/1">David Bloomin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinnaparaju_N/0/1/0/all/0/1">Nikhil Pinnaparaju</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanna_N/0/1/0/all/0/1">Nishaanth Kanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Scott_D/0/1/0/all/0/1">Daniel Scott</a>, <a href="http://arxiv.org/find/cs/1/au:+Sullivan_R/0/1/0/all/0/1">Ryan Sullivan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shuman_R/0/1/0/all/0/1">Rose S. Shuman</a>, <a href="http://arxiv.org/find/cs/1/au:+Alcantara_L/0/1/0/all/0/1">Lucas de Alc&#xe2;ntara</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1">Herbie Bradley</a>, <a href="http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1">Louis Castricato</a>, <a href="http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1">Kirsty You</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qimai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaolong Zhu</a></p>
<p>Neural MMO 2.0 is a massively multi-agent environment for reinforcement
learning research. The key feature of this new version is a flexible task
system that allows users to define a broad range of objectives and reward
signals. We challenge researchers to train agents capable of generalizing to
tasks, maps, and opponents never seen during training. Neural MMO features
procedurally generated maps with 128 agents in the standard setting and support
for up to. Version 2.0 is a complete rewrite of its predecessor with three-fold
improved performance and compatibility with CleanRL. We release the platform as
free and open-source software with comprehensive documentation available at
neuralmmo.github.io and an active community Discord. To spark initial research
on this new platform, we are concurrently running a competition at NeurIPS
2023.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03745">Unsupervised Video Summarization. (arXiv:2311.03745v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hanqing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1">Diego Klabjan</a>, <a href="http://arxiv.org/find/cs/1/au:+Utke_J/0/1/0/all/0/1">Jean Utke</a></p>
<p>This paper introduces a new, unsupervised method for automatic video
summarization using ideas from generative adversarial networks but eliminating
the discriminator, having a simple loss function, and separating training of
different parts of the model. An iterative training strategy is also applied by
alternately training the reconstructor and the frame selector for multiple
iterations. Furthermore, a trainable mask vector is added to the model in
summary generation during training and evaluation. The method also includes an
unsupervised model selection algorithm. Results from experiments on two public
datasets (SumMe and TVSum) and four datasets we created (Soccer, LoL, MLB, and
ShortMLB) demonstrate the effectiveness of each component on the model
performance, particularly the iterative training strategy. Evaluations and
comparisons with the state-of-the-art methods highlight the advantages of the
proposed method in performance, stability, and training efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03746">Enhanced physics-informed neural networks with domain scaling and residual correction methods for multi-frequency elliptic problems. (arXiv:2311.03746v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Jang_D/0/1/0/all/0/1">Deok-Kyu Jang</a>, <a href="http://arxiv.org/find/math/1/au:+Kim_H/0/1/0/all/0/1">Hyea Hyun Kim</a>, <a href="http://arxiv.org/find/math/1/au:+Kim_K/0/1/0/all/0/1">Kyungsoo Kim</a></p>
<p>In this paper, neural network approximation methods are developed for
elliptic partial differential equations with multi-frequency solutions. Neural
network work approximation methods have advantages over classical approaches in
that they can be applied without much concerns on the form of the differential
equations or the shape or dimension of the problem domain. When applied to
problems with multi-frequency solutions, the performance and accuracy of neural
network approximation methods are strongly affected by the contrast of the
high- and low-frequency parts in the solutions. To address this issue, domain
scaling and residual correction methods are proposed. The efficiency and
accuracy of the proposed methods are demonstrated for multi-frequency model
problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03755">Multilingual Mathematical Autoformalization. (arXiv:2311.03755v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1">Albert Q. Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1">Mateja Jamnik</a></p>
<p>Autoformalization is the task of translating natural language materials into
machine-verifiable formalisations. Progress in autoformalization research is
hindered by the lack of a sizeable dataset consisting of informal-formal pairs
expressing the same essence. Existing methods tend to circumvent this challenge
by manually curating small corpora or using few-shot learning with large
language models. But these methods suffer from data scarcity and formal
language acquisition difficulty. In this work, we create $\texttt{MMA}$, a
large, flexible, multilingual, and multi-domain dataset of informal-formal
pairs, by using a language model to translate in the reverse direction, that
is, from formal mathematical statements into corresponding informal ones.
Experiments show that language models fine-tuned on $\texttt{MMA}$ produce
$16-18\%$ of statements acceptable with minimal corrections on the
$\texttt{miniF2F}$ and $\texttt{ProofNet}$ benchmarks, up from $0\%$ with the
base model. We demonstrate that fine-tuning on multilingual formal data results
in more capable autoformalization models even when deployed on monolingual
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03756">Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning. (arXiv:2311.03756v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhiwen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1">Tom H. Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1">Bin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuen_C/0/1/0/all/0/1">Chau Yuen</a></p>
<p>This paper considers optimal traffic signal control in smart cities, which
has been taken as a complex networked system control problem. Given the
interacting dynamics among traffic lights and road networks, attaining
controller adaptivity and scalability stands out as a primary challenge.
Capturing the spatial-temporal correlation among traffic lights under the
framework of Multi-Agent Reinforcement Learning (MARL) is a promising solution.
Nevertheless, existing MARL algorithms ignore effective information aggregation
which is fundamental for improving the learning capacity of decentralized
agents. In this paper, we design a new decentralized control architecture with
improved environmental observability to capture the spatial-temporal
correlation. Specifically, we first develop a topology-aware information
aggregation strategy to extract correlation-related information from
unstructured data gathered in the road network. Particularly, we transfer the
road network topology into a graph shift operator by forming a diffusion
process on the topology, which subsequently facilitates the construction of
graph signals. A diffusion convolution module is developed, forming a new MARL
algorithm, which endows agents with the capabilities of graph learning.
Extensive experiments based on both synthetic and real-world datasets verify
that our proposal outperforms existing decentralized algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03757">Manifold learning: what, how, and why. (arXiv:2311.03757v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1">Marina Meil&#x103;</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1">Hanyu Zhang</a></p>
<p>Manifold learning (ML), known also as non-linear dimension reduction, is a
set of methods to find the low dimensional structure of data. Dimension
reduction for large, high dimensional data is not merely a way to reduce the
data; the new representations and descriptors obtained by ML reveal the
geometric shape of high dimensional point clouds, and allow one to visualize,
de-noise and interpret them. This survey presents the principles underlying ML,
the representative methods, as well as their statistical foundations from a
practicing statistician's perspective. It describes the trade-offs, and what
theory tells us about the parameter and algorithmic choices we make in order to
obtain reliable conclusions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03760">Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds. (arXiv:2311.03760v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Takeno_S/0/1/0/all/0/1">Shion Takeno</a>, <a href="http://arxiv.org/find/cs/1/au:+Inatsu_Y/0/1/0/all/0/1">Yu Inatsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Karasuyama_M/0/1/0/all/0/1">Masayuki Karasuyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a></p>
<p>Among various acquisition functions (AFs) in Bayesian optimization (BO),
Gaussian process upper confidence bound (GP-UCB) and Thompson sampling (TS) are
well-known options with established theoretical properties regarding Bayesian
cumulative regret (BCR). Recently, it has been shown that a randomized variant
of GP-UCB achieves a tighter BCR bound compared with GP-UCB, which we call the
tighter BCR bound for brevity. Inspired by this study, this paper first shows
that TS achieves the tighter BCR bound. On the other hand, GP-UCB and TS often
practically suffer from manual hyperparameter tuning and over-exploration
issues, respectively. To overcome these difficulties, we propose yet another AF
called a probability of improvement from the maximum of a sample path (PIMS).
We show that PIMS achieves the tighter BCR bound and avoids the hyperparameter
tuning, unlike GP-UCB. Furthermore, we demonstrate a wide range of experiments,
focusing on the effectiveness of PIMS that mitigates the practical issues of
GP-UCB and TS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03761">Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation Recognition. (arXiv:2311.03761v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shilian Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1">Kunfeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Luxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1">Qi Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoniu Yang</a></p>
<p>The use of deep learning for radio modulation recognition has become
prevalent in recent years. This approach automatically extracts
high-dimensional features from large datasets, facilitating the accurate
classification of modulation schemes. However, in real-world scenarios, it may
not be feasible to gather sufficient training data in advance. Data
augmentation is a method used to increase the diversity and quantity of
training dataset and to reduce data sparsity and imbalance. In this paper, we
propose data augmentation methods that involve replacing detail coefficients
decomposed by discrete wavelet transform for reconstructing to generate new
samples and expand the training set. Different generation methods are used to
generate replacement sequences. Simulation results indicate that our proposed
methods significantly outperform the other augmentation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03764">Neuro-GPT: Developing A Foundation Model for EEG. (arXiv:2311.03764v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1">Wenhui Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1">Woojae Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tholke_P/0/1/0/all/0/1">Philipp Th&#xf6;lke</a>, <a href="http://arxiv.org/find/cs/1/au:+Medani_T/0/1/0/all/0/1">Takfarinas Medani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jerbi_K/0/1/0/all/0/1">Karim Jerbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Anand A. Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Leahy_R/0/1/0/all/0/1">Richard M. Leahy</a></p>
<p>To handle the scarcity and heterogeneity of electroencephalography (EEG) data
in Brain-Computer Interface (BCI) tasks, and to harness the vast public data,
we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT
model. The foundation model is pre-trained on a large-scale public EEG dataset,
using a self-supervised task which learns how to reconstruct the masked chunk
in EEG. We then fine-tune the foundation model on a Motor Imagery
Classification task where only 9 subjects are available. Experiments
demonstrated that applying foundation model can significantly improve
classification performance compared to the model trained from scratch, which
provides evidence for the advanced generalizability of foundation model and the
ability to address the challenges of data scarcity and heterogeneity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03768">PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning. (arXiv:2311.03768v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1">Jinrui Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xiaoxuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chuanxian Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1">Guangxin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yucheng Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Changwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Huan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhenyu Guo</a></p>
<p>Self-supervised learning has been actively studied in time series domain
recently, especially for masked reconstruction. Most of these methods follow
the "Pre-training + Fine-tuning" paradigm in which a new decoder replaces the
pre-trained decoder to fit for a specific downstream task, leading to
inconsistency of upstream and downstream tasks. In this paper, we first point
out that the unification of task objectives and adaptation for task difficulty
are critical for bridging the gap between time series masked reconstruction and
forecasting. By reserving the pre-trained mask token during fine-tuning stage,
the forecasting task can be taken as a special case of masked reconstruction,
where the future values are masked and reconstructed based on history values.
It guarantees the consistency of task objectives but there is still a gap in
task difficulty. Because masked reconstruction can utilize contextual
information while forecasting can only use historical information to
reconstruct. To further mitigate the existed gap, we propose a simple yet
effective prompt token tuning (PT-Tuning) paradigm, in which all pre-trained
parameters are frozen and only a few trainable prompt tokens are added to
extended mask tokens in element-wise manner. Extensive experiments on
real-world datasets demonstrate the superiority of our proposed paradigm with
state-of-the-art performance compared to representation learning and end-to-end
supervised forecasting methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03780">Ensembling Textual and Structure-Based Models for Knowledge Graph Completion. (arXiv:2311.03780v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nandi_A/0/1/0/all/0/1">Ananjan Nandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaur_N/0/1/0/all/0/1">Navdeep Kaur</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1">Parag Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a></p>
<p>We consider two popular approaches to Knowledge Graph Completion (KGC):
textual models that rely on textual entity descriptions, and structure-based
models that exploit the connectivity structure of the Knowledge Graph (KG).
Preliminary experiments show that these approaches have complementary
strengths: structure-based models perform well when the gold answer is easily
reachable from the query head in the KG, while textual models exploit
descriptions to give good performance even when the gold answer is not
reachable. In response, we explore ensembling as a way of combining the best of
both approaches. We propose a novel method for learning query-dependent
ensemble weights by using the distributions of scores assigned by individual
models to all candidate entities. Our ensemble baseline achieves
state-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR
and 8.3 pt Hits@1 gains over best individual models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03784">UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields. (arXiv:2311.03784v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1">Injae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minhyuk Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunwoo J. Kim</a></p>
<p>Neural Radiance Field (NeRF) has enabled novel view synthesis with high
fidelity given images and camera poses. Subsequent works even succeeded in
eliminating the necessity of pose priors by jointly optimizing NeRF and camera
pose. However, these works are limited to relatively simple settings such as
photometrically consistent and occluder-free image collections or a sequence of
images from a video. So they have difficulty handling unconstrained images with
varying illumination and transient occluders. In this paper, we propose
\textbf{UP-NeRF} (\textbf{U}nconstrained \textbf{P}ose-prior-free
\textbf{Ne}ural \textbf{R}adiance \textbf{F}ields) to optimize NeRF with
unconstrained image collections without camera pose prior. We tackle these
challenges with surrogate tasks that optimize color-insensitive feature fields
and a separate module for transient occluders to block their influence on pose
estimation. In addition, we introduce a candidate head to enable more robust
pose estimation and transient-aware depth supervision to minimize the effect of
incorrect prior. Our experiments verify the superior performance of our method
compared to the baselines including BARF and its variants in a challenging
internet photo collection, \textit{Phototourism} dataset. The code of UP-NeRF
is available at \url{https://github.com/mlvlab/UP-NeRF}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03797">User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates. (arXiv:2311.03797v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1">Hilal Asi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Daogao Liu</a></p>
<p>We study differentially private stochastic convex optimization (DP-SCO) under
user-level privacy, where each user may hold multiple data items. Existing work
for user-level DP-SCO either requires super-polynomial runtime [Ghazi et al.
(2023)] or requires the number of users to grow polynomially with the
dimensionality of the problem with additional strict assumptions [Bassily et
al. (2023)]. We develop new algorithms for user-level DP-SCO that obtain
optimal rates for both convex and strongly convex functions in polynomial time
and require the number of users to grow only logarithmically in the dimension.
Moreover, our algorithms are the first to obtain optimal rates for non-smooth
functions in polynomial time. These algorithms are based on multiple-pass
DP-SGD, combined with a novel private mean estimation procedure for
concentrated data, which applies an outlier removal step before estimating the
mean of the gradients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03839">Aspects of human memory and Large Language Models. (arXiv:2311.03839v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1">Romuald A. Janik</a></p>
<p>Large Language Models (LLMs) are huge artificial neural networks which
primarily serve to generate text, but also provide a very sophisticated
probabilistic model of language use. Since generating a semantically consistent
text requires a form of effective memory, we investigate the memory properties
of LLMs and find surprising similarities with key characteristics of human
memory. This result strongly suggests that the biological features of human
memory leave an imprint on the way that we structure our textual narratives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03852">Improved MDL Estimators Using Fiber Bundle of Local Exponential Families for Non-exponential Families. (arXiv:2311.03852v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miyamoto_K/0/1/0/all/0/1">Kohei Miyamoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Barron_A/0/1/0/all/0/1">Andrew R. Barron</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeuchi_J/0/1/0/all/0/1">Jun&#x27;ichi Takeuchi</a></p>
<p>Minimum Description Length (MDL) estimators, using two-part codes for
universal coding, are analyzed. For general parametric families under certain
regularity conditions, we introduce a two-part code whose regret is close to
the minimax regret, where regret of a code with respect to a target family M is
the difference between the code length of the code and the ideal code length
achieved by an element in M. This is a generalization of the result for
exponential families by Gr\"unwald. Our code is constructed by using an
augmented structure of M with a bundle of local exponential families for data
description, which is not needed for exponential families. This result gives a
tight upper bound on risk and loss of the MDL estimators based on the theory
introduced by Barron and Cover in 1991. Further, we show that we can apply the
result to mixture families, which are a typical example of non-exponential
families.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03863">An Explainable Framework for Machine learning-Based Reactive Power Optimization of Distribution Network. (arXiv:2311.03863v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1">Wenlong Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Schafer_B/0/1/0/all/0/1">Benjamin Sch&#xe4;fer</a>, <a href="http://arxiv.org/find/eess/1/au:+Qin_D/0/1/0/all/0/1">Dalin Qin</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1">Gonghao Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zhixian Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zhe Yang</a></p>
<p>To reduce the heavy computational burden of reactive power optimization of
distribution networks, machine learning models are receiving increasing
attention. However, most machine learning models (e.g., neural networks) are
usually considered as black boxes, making it challenging for power system
operators to identify and comprehend potential biases or errors in the
decision-making process of machine learning models. To address this issue, an
explainable machine-learning framework is proposed to optimize the reactive
power in distribution networks. Firstly, a Shapley additive explanation
framework is presented to measure the contribution of each input feature to the
solution of reactive power optimizations generated from machine learning
models. Secondly, a model-agnostic approximation method is developed to
estimate Shapley values, so as to avoid the heavy computational burden
associated with direct calculations of Shapley values. The simulation results
show that the proposed explainable framework can accurately explain the
solution of the machine learning model-based reactive power optimization by
using visual analytics, from both global and instance perspectives. Moreover,
the proposed explainable framework is model-agnostic, and thus applicable to
various models (e.g., neural networks).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03865">FD-MIA: Efficient Attacks on Fairness-enhanced Models. (arXiv:2311.03865v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1">Huan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guangsheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1">Tianqing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wanlei Zhou</a></p>
<p>Previous studies have developed fairness methods for biased models that
exhibit discriminatory behaviors towards specific subgroups. While these models
have shown promise in achieving fair predictions, recent research has
identified their potential vulnerability to score-based membership inference
attacks (MIAs). In these attacks, adversaries can infer whether a particular
data sample was used during training by analyzing the model's prediction
scores. However, our investigations reveal that these score-based MIAs are
ineffective when targeting fairness-enhanced models in binary classifications.
The attack models trained to launch the MIAs degrade into simplistic threshold
models, resulting in lower attack performance. Meanwhile, we observe that
fairness methods often lead to prediction performance degradation for the
majority subgroups of the training data. This raises the barrier to successful
attacks and widens the prediction gaps between member and non-member data.
Building upon these insights, we propose an efficient MIA method against
fairness-enhanced models based on fairness discrepancy results (FD-MIA). It
leverages the difference in the predictions from both the original and
fairness-enhanced models and exploits the observed prediction gaps as attack
clues. We also explore potential strategies for mitigating privacy leakages.
Extensive experiments validate our findings and demonstrate the efficacy of the
proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03886">Formulating Discrete Probability Flow Through Optimal Transport. (arXiv:2311.03886v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengze Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hubery Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaohua Xie</a></p>
<p>Continuous diffusion models are commonly acknowledged to display a
deterministic probability flow, whereas discrete diffusion models do not. In
this paper, we aim to establish the fundamental theory for the probability flow
of discrete diffusion models. Specifically, we first prove that the continuous
probability flow is the Monge optimal transport map under certain conditions,
and also present an equivalent evidence for discrete cases. In view of these
findings, we are then able to define the discrete probability flow in line with
the principles of optimal transport. Finally, drawing upon our newly
established definitions, we propose a novel sampling method that surpasses
previous discrete diffusion models in its ability to generate more certain
outcomes. Extensive experiments on the synthetic toy dataset and the CIFAR-10
dataset have validated the effectiveness of our proposed discrete probability
flow. Code is released at:
https://github.com/PangzeCheung/Discrete-Probability-Flow.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03897">Temporal Graph Representation Learning with Adaptive Augmentation Contrastive. (arXiv:2311.03897v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongjiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1">Pengfei Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Huijun Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Huaming Wu</a></p>
<p>Temporal graph representation learning aims to generate low-dimensional
dynamic node embeddings to capture temporal information as well as structural
and property information. Current representation learning methods for temporal
networks often focus on capturing fine-grained information, which may lead to
the model capturing random noise instead of essential semantic information.
While graph contrastive learning has shown promise in dealing with noise, it
only applies to static graphs or snapshots and may not be suitable for handling
time-dependent noise. To alleviate the above challenge, we propose a novel
Temporal Graph representation learning with Adaptive augmentation Contrastive
(TGAC) model. The adaptive augmentation on the temporal graph is made by
combining prior knowledge with temporal information, and the contrastive
objective function is constructed by defining the augmented inter-view contrast
and intra-view contrast. To complement TGAC, we propose three adaptive
augmentation strategies that modify topological features to reduce noise from
the network. Our extensive experiments on various real networks demonstrate
that the proposed model outperforms other temporal graph representation
learning methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03899">Learning-Based Latency-Constrained Fronthaul Compression Optimization in C-RAN. (arXiv:2311.03899v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gronland_A/0/1/0/all/0/1">Axel Gr&#xf6;nland</a>, <a href="http://arxiv.org/find/cs/1/au:+Klaiqi_B/0/1/0/all/0/1">Bleron Klaiqi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gelabert_X/0/1/0/all/0/1">Xavier Gelabert</a></p>
<p>The evolution of wireless mobile networks towards cloudification, where Radio
Access Network (RAN) functions can be hosted at either a central or distributed
locations, offers many benefits like low cost deployment, higher capacity, and
improved hardware utilization. Nevertheless, the flexibility in the functional
deployment comes at the cost of stringent fronthaul (FH) capacity and latency
requirements. One possible approach to deal with these rigorous constraints is
to use FH compression techniques. To ensure that FH capacity and latency
requirements are met, more FH compression is applied during high load, while
less compression is applied during medium and low load to improve FH
utilization and air interface performance. In this paper, a model-free deep
reinforcement learning (DRL) based FH compression (DRL-FC) framework is
proposed that dynamically controls FH compression through various configuration
parameters such as modulation order, precoder granularity, and precoder weight
quantization that affect both FH load and air interface performance. Simulation
results show that DRL-FC exhibits significantly higher FH utilization (68.7% on
average) and air interface throughput than a reference scheme (i.e. with no
applied compression) across different FH load levels. At the same time, the
proposed DRL-FC framework is able to meet the predefined FH latency constraints
(in our case set to 260 $\mu$s) under various FH loads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03910">Structure of universal formulas. (arXiv:2311.03910v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yarotsky_D/0/1/0/all/0/1">Dmitry Yarotsky</a></p>
<p>By universal formulas we understand parameterized analytic expressions that
have a fixed complexity, but nevertheless can approximate any continuous
function on a compact set. There exist various examples of such formulas,
including some in the form of neural networks. In this paper we analyze the
essential structural elements of these highly expressive models. We introduce a
hierarchy of expressiveness classes connecting the global approximability
property to the weaker property of infinite VC dimension, and prove a series of
classification results for several increasingly complex functional families. In
particular, we introduce a general family of
polynomially-exponentially-algebraic functions that, as we prove, is subject to
polynomial constraints. As a consequence, we show that fixed-size neural
networks with not more than one layer of neurons having transcendental
activations (e.g., sine or standard sigmoid) cannot in general approximate
functions on arbitrary finite sets. On the other hand, we give examples of
functional families, including two-hidden-layer neural networks, that
approximate functions on arbitrary finite sets, but fail to do that on the
whole domain of definition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03912">FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer. (arXiv:2311.03912v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chi-Chih Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1">Yuan-Yao Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shixing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1">Ning-Chi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1">Diana Marculescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kai-Chiang Wu</a></p>
<p>Vision Transformers (ViT) have recently demonstrated success across a myriad
of computer vision tasks. However, their elevated computational demands pose
significant challenges for real-world deployment. While low-rank approximation
stands out as a renowned method to reduce computational loads, efficiently
automating the target rank selection in ViT remains a challenge. Drawing from
the notable similarity and alignment between the processes of rank selection
and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based
on NAS. To overcome the design challenge of supernet posed by vast search
space, FLORA employs a low-rank aware candidate filtering strategy. This method
adeptly identifies and eliminates underperforming candidates, effectively
alleviating potential undertraining and interference among subnetworks. To
further enhance the quality of low-rank supernets, we design a low-rank
specific training paradigm. First, we propose weight inheritance to construct
supernet and enable gradient sharing among low-rank modules. Secondly, we adopt
low-rank aware sampling to strategically allocate training resources, taking
into account inherited information from pre-trained models. Empirical results
underscore FLORA's efficacy. With our method, a more fine-grained rank
configuration can be generated automatically and yield up to 33% extra FLOPs
reduction compared to a simple uniform configuration. More specific,
FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without
performance degradtion. Importantly, FLORA boasts both versatility and
orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with
leading compression techniques or compact hybrid structures. Our code is
publicly available at https://github.com/shadowpa0327/FLORA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03956">Cup Curriculum: Curriculum Learning on Model Capacity. (arXiv:2311.03956v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scharr_L/0/1/0/all/0/1">Luca Scharr</a>, <a href="http://arxiv.org/find/cs/1/au:+Toborek_V/0/1/0/all/0/1">Vanessa Toborek</a></p>
<p>Curriculum learning (CL) aims to increase the performance of a learner on a
given task by applying a specialized learning strategy. This strategy focuses
on either the dataset, the task, or the model. There is little to no work
analysing the possibilities to apply CL on the model capacity in natural
language processing. To close this gap, we propose the cup curriculum. In a
first phase of training we use a variation of iterative magnitude pruning to
reduce model capacity. These weights are reintroduced in a second phase,
resulting in the model capacity to show a cup-shaped curve over the training
iterations. We empirically evaluate different strategies of the cup curriculum
and show that it outperforms early stopping reliably while exhibiting a high
resilience to overfitting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03976">Its All Graph To Me: Foundational Topology Models with Contrastive Learning on Multiple Domains. (arXiv:2311.03976v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1">Alex O. Davies</a>, <a href="http://arxiv.org/find/cs/1/au:+Green_R/0/1/0/all/0/1">Riku W. Green</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajmeri_N/0/1/0/all/0/1">Nirav S. Ajmeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Filho_T/0/1/0/all/0/1">Telmo M. Silva Filho</a></p>
<p>Representations and embeddings of graph data have been essential in many
domains of research.
</p>
<p>The principle benefit of learning such representations is that the
pre-trained model can be fine-tuned on smaller datasets where data or labels
are scarse.
</p>
<p>Existing models, however, are domain specific; for example a model trained on
molecular graphs is fine-tuned on other molecular graphs.
</p>
<p>This means that in many application cases the choice of pre-trained model can
be arbitrary, and novel domains may lack an appropriate pre-trained model.
</p>
<p>This is of particular issue where data is scarse, precluding traditional
supervised methods.
</p>
<p>In this work we use adversarial contrastive learning to present a \method, a
model pre-trained on many graph domains.
</p>
<p>We train the model only on topologies but include node labels in evaluation.
</p>
<p>We evaluate the efficacy of its learnt representations on various downstream
tasks.
</p>
<p>Against baseline models pre-trained on single domains, as well as un-trained
models and non-transferred models, we show that performance is equal or better
using our single model.
</p>
<p>This includes when node labels are used in evaluation, where performance is
consistently superior to single-domain or non-pre-trained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03989">Learned Causal Method Prediction. (arXiv:2311.03989v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shantanu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilmkil_A/0/1/0/all/0/1">Agrin Hilmkil</a></p>
<p>For a given causal question, it is important to efficiently decide which
causal inference method to use for a given dataset. This is challenging because
causal methods typically rely on complex and difficult-to-verify assumptions,
and cross-validation is not applicable since ground truth causal quantities are
unobserved.In this work, we propose CAusal Method Predictor (CAMP), a framework
for predicting the best method for a given dataset. To this end, we generate
datasets from a diverse set of synthetic causal models, score the candidate
methods, and train a model to directly predict the highest-scoring method for
that dataset. Next, by formulating a self-supervised pre-training objective
centered on dataset assumptions relevant for causal inference, we significantly
reduce the need for costly labeled data and enhance training efficiency. Our
strategy learns to map implicit dataset properties to the best method in a
data-driven manner. In our experiments, we focus on method prediction for
causal discovery. CAMP outperforms selecting any individual candidate method
and demonstrates promising generalization to unseen semi-synthetic and
real-world benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03992">Bandit Pareto Set Identification: the Fixed Budget Setting. (arXiv:2311.03992v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kone_C/0/1/0/all/0/1">Cyrille Kone</a>, <a href="http://arxiv.org/find/stat/1/au:+Kaufmann_E/0/1/0/all/0/1">Emilie Kaufmann</a>, <a href="http://arxiv.org/find/stat/1/au:+Richert_L/0/1/0/all/0/1">Laura Richert</a></p>
<p>We study a multi-objective pure exploration problem in a multi-armed bandit
model. Each arm is associated to an unknown multi-variate distribution and the
goal is to identify the distributions whose mean is not uniformly worse than
that of another distribution: the Pareto optimal set. We propose and analyze
the first algorithms for the \emph{fixed budget} Pareto Set Identification
task. We propose Empirical Gap Elimination, a family of algorithms combining a
careful estimation of the ``hardness to classify'' each arm in or out of the
Pareto set with a generic elimination scheme. We prove that two particular
instances, EGE-SR and EGE-SH, have a probability of error that decays
exponentially fast with the budget, with an exponent supported by an
information theoretic lower-bound. We complement these findings with an
empirical study using real-world and synthetic datasets, which showcase the
good performance of our algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03996">An Initialization Schema for Neuronal Networks on Tabular Data. (arXiv:2311.03996v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1">Wolfgang Fuhl</a></p>
<p>Nowadays, many modern applications require heterogeneous tabular data, which
is still a challenging task in terms of regression and classification. Many
approaches have been proposed to adapt neural networks for this task, but
still, boosting and bagging of decision trees are the best-performing methods
for this task. In this paper, we show that a binomial initialized neural
network can be used effectively on tabular data. The proposed approach shows a
simple but effective approach for initializing the first hidden layer in neural
networks. We also show that this initializing schema can be used to jointly
train ensembles by adding gradient masking to batch entries and using the
binomial initialization for the last layer in a neural network. For this
purpose, we modified the hinge binary loss and the soft max loss to make them
applicable for joint ensemble training. We evaluate our approach on multiple
public datasets and showcase the improved performance compared to other neural
network-based approaches. In addition, we discuss the limitations and possible
further research of our approach for improving the applicability of neural
networks to tabular data.
</p>
<p>Link:
https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FInitializationNeuronalNetworksTabularData&amp;mode=list
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.04634">Accurate 3D Object Detection using Energy-Based Models. (arXiv:2012.04634v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1">Fredrik K. Gustafsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1">Martin Danelljan</a>, <a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1">Thomas B. Sch&#xf6;n</a></p>
<p>Accurate 3D object detection (3DOD) is crucial for safe navigation of complex
environments by autonomous robots. Regressing accurate 3D bounding boxes in
cluttered environments based on sparse LiDAR data is however a highly
challenging problem. We address this task by exploring recent advances in
conditional energy-based models (EBMs) for probabilistic regression. While
methods employing EBMs for regression have demonstrated impressive performance
on 2D object detection in images, these techniques are not directly applicable
to 3D bounding boxes. In this work, we therefore design a differentiable
pooling operator for 3D bounding boxes, serving as the core module of our EBM
network. We further integrate this general approach into the state-of-the-art
3D object detector SA-SSD. On the KITTI dataset, our proposed approach
consistently outperforms the SA-SSD baseline across all 3DOD metrics,
demonstrating the potential of EBM-based regression for highly accurate 3DOD.
Code is available at https://github.com/fregu856/ebms_3dod.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.12697">Structural Causal Models Reveal Confounder Bias in Linear Program Modelling. (arXiv:2105.12697v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1">Matej Ze&#x10d;evi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1">Devendra Singh Dhami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>The recent years have been marked by extended research on adversarial
attacks, especially on deep neural networks. With this work we intend on posing
and investigating the question of whether the phenomenon might be more general
in nature, that is, adversarial-style attacks outside classical classification
tasks. Specifically, we investigate optimization problems as they constitute a
fundamental part of modern AI research. To this end, we consider the base class
of optimizers namely Linear Programs (LPs). On our initial attempt of a na\"ive
mapping between the formalism of adversarial examples and LPs, we quickly
identify the key ingredients missing for making sense of a reasonable notion of
adversarial examples for LPs. Intriguingly, the formalism of Pearl's notion to
causality allows for the right description of adversarial like examples for
LPs. Characteristically, we show the direct influence of the Structural Causal
Model (SCM) onto the subsequent LP optimization, which ultimately exposes a
notion of confounding in LPs (inherited by said SCM) that allows for
adversarial-style attacks. We provide both the general proof formally alongside
existential proofs of such intriguing LP-parameterizations based on SCM for
three combinatorial problems, namely Linear Assignment, Shortest Path and a
real world problem of energy systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.13703">Task-Driven Detection of Distribution Shifts with Statistical Guarantees for Robot Learning. (arXiv:2106.13703v6 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farid_A/0/1/0/all/0/1">Alec Farid</a>, <a href="http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1">Sushant Veer</a>, <a href="http://arxiv.org/find/cs/1/au:+Pachisia_D/0/1/0/all/0/1">Divyanshu Pachisia</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1">Anirudha Majumdar</a></p>
<p>Our goal is to perform out-of-distribution (OOD) detection, i.e., to detect
when a robot is operating in environments drawn from a different distribution
than the ones used to train the robot. We leverage Probably Approximately
Correct (PAC)-Bayes theory to train a policy with a guaranteed bound on
performance on the training distribution. Our idea for OOD detection relies on
the following intuition: violation of the performance bound on test
environments provides evidence that the robot is operating OOD. We formalize
this via statistical techniques based on p-values and concentration
inequalities. The approach provides guaranteed confidence bounds on OOD
detection including bounds on both the false positive and false negative rates
of the detector and is task-driven and only sensitive to changes that impact
the robot's performance. We demonstrate our approach in simulation and hardware
for a grasping task using objects with unfamiliar shapes or poses and a drone
performing vision-based obstacle avoidance in environments with wind
disturbances and varied obstacle densities. Our examples demonstrate that we
can perform task-driven OOD detection within just a handful of trials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.00604">Inexact bilevel stochastic gradient methods for constrained and unconstrained lower-level problems. (arXiv:2110.00604v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Giovannelli_T/0/1/0/all/0/1">Tommaso Giovannelli</a>, <a href="http://arxiv.org/find/math/1/au:+Kent_G/0/1/0/all/0/1">Griffin Dean Kent</a>, <a href="http://arxiv.org/find/math/1/au:+Vicente_L/0/1/0/all/0/1">Luis Nunes Vicente</a></p>
<p>Two-level stochastic optimization formulations have become instrumental in a
number of machine learning contexts such as continual learning, neural
architecture search, adversarial learning, and hyperparameter tuning. Practical
stochastic bilevel optimization problems become challenging in optimization or
learning scenarios where the number of variables is high or there are
constraints.
</p>
<p>In this paper, we introduce a bilevel stochastic gradient method for bilevel
problems with nonlinear and possibly nonconvex lower-level constraints. We also
present a comprehensive convergence theory that addresses both the lower-level
unconstrained and constrained cases and covers all inexact calculations of the
adjoint gradient (also called hypergradient), such as the inexact solution of
the lower-level problem, inexact computation of the adjoint formula (due to the
inexact solution of the adjoint equation or use of a truncated Neumann series),
and noisy estimates of the gradients, Hessians, and Jacobians involved. To
promote the use of bilevel optimization in large-scale learning, we have
developed new low-rank practical bilevel stochastic gradient methods (BSG-N-FD
and~BSG-1) that do not require second-order derivatives and, in the lower-level
unconstrained case, dismiss any matrix-vector products.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.11948">Learning Proposals for Practical Energy-Based Regression. (arXiv:2110.11948v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1">Fredrik K. Gustafsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1">Martin Danelljan</a>, <a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1">Thomas B. Sch&#xf6;n</a></p>
<p>Energy-based models (EBMs) have experienced a resurgence within machine
learning in recent years, including as a promising alternative for
probabilistic regression. However, energy-based regression requires a proposal
distribution to be manually designed for training, and an initial estimate has
to be provided at test-time. We address both of these issues by introducing a
conceptually simple method to automatically learn an effective proposal
distribution, which is parameterized by a separate network head. To this end,
we derive a surprising result, leading to a unified training objective that
jointly minimizes the KL divergence from the proposal to the EBM, and the
negative log-likelihood of the EBM. At test-time, we can then employ importance
sampling with the trained proposal to efficiently evaluate the learned EBM and
produce stand-alone predictions. Furthermore, we utilize our derived training
objective to learn mixture density networks (MDNs) with a jointly trained
energy-based teacher, consistently outperforming conventional MDN training on
four real-world regression tasks within computer vision. Code is available at
https://github.com/fregu856/ebms_proposals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08440">Climate-Invariant Machine Learning. (arXiv:2112.08440v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beucler_T/0/1/0/all/0/1">Tom Beucler</a>, <a href="http://arxiv.org/find/cs/1/au:+Gentine_P/0/1/0/all/0/1">Pierre Gentine</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuval_J/0/1/0/all/0/1">Janni Yuval</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankitesh Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Liran Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jerry Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Sungduk Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasp_S/0/1/0/all/0/1">Stephan Rasp</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1">Fiaz Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+OGorman_P/0/1/0/all/0/1">Paul A. O&#x27;Gorman</a>, <a href="http://arxiv.org/find/cs/1/au:+Neelin_J/0/1/0/all/0/1">J. David Neelin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lutsko_N/0/1/0/all/0/1">Nicholas J. Lutsko</a>, <a href="http://arxiv.org/find/cs/1/au:+Pritchard_M/0/1/0/all/0/1">Michael Pritchard</a></p>
<p>Projecting climate change is a generalization problem: we extrapolate the
recent past using physical models across past, present, and future climates.
Current climate models require representations of processes that occur at
scales smaller than model grid size, which have been the main source of model
projection uncertainty. Recent machine learning (ML) algorithms hold promise to
improve such process representations, but tend to extrapolate poorly to climate
regimes they were not trained on. To get the best of the physical and
statistical worlds, we propose a new framework -- termed "climate-invariant" ML
-- incorporating knowledge of climate processes into ML algorithms, and show
that it can maintain high offline accuracy across a wide range of climate
conditions and configurations in three distinct atmospheric models. Our results
suggest that explicitly incorporating physical knowledge into data-driven
models of Earth system processes can improve their consistency, data
efficiency, and generalizability across climate regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.03897">Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1">Changdae Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1">Junhyuk So</a>, <a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1">Hoyoon Byun</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1">YongTaek Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Minchul Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1">Jong-June Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kyungwoo Song</a></p>
<p>Pre-trained multi-modal models, such as CLIP, provide transferable embeddings
and show promising results in diverse applications. However, the analysis of
learned multi-modal embeddings is relatively unexplored, and the embedding
transferability can be improved. In this work, we observe that CLIP holds
separated embedding subspaces for two different modalities, and then we
investigate it through the lens of uniformity-alignment to measure the quality
of learned representation. Both theoretically and empirically, we show that
CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack
of alignment and uniformity might restrict the transferability and robustness
of embeddings. To this end, we devise a new fine-tuning method for robust
representation equipping better alignment and uniformity. First, we propose a
Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to
generate hard negative samples on the hypersphere. Then, we fine-tune the model
on hard negatives as well as original negatives and positives with contrastive
loss. Based on the theoretical analysis about hardness guarantee and limiting
behavior, we justify the use of our method. Extensive experiments on retrieval,
calibration, few- or zero-shot classification (under distribution shift),
embedding arithmetic, and image captioning further show that our method
provides transferable representations, enabling robust model adaptation on
diverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.13908">On efficient algorithms for computing near-best polynomial approximations to high-dimensional, Hilbert-valued functions from limited samples. (arXiv:2203.13908v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Adcock_B/0/1/0/all/0/1">Ben Adcock</a>, <a href="http://arxiv.org/find/math/1/au:+Brugiapaglia_S/0/1/0/all/0/1">Simone Brugiapaglia</a>, <a href="http://arxiv.org/find/math/1/au:+Dexter_N/0/1/0/all/0/1">Nick Dexter</a>, <a href="http://arxiv.org/find/math/1/au:+Moraga_S/0/1/0/all/0/1">Sebastian Moraga</a></p>
<p>Sparse polynomial approximation has become indispensable for approximating
smooth, high- or infinite-dimensional functions from limited samples. This is a
key task in computational science and engineering, e.g., surrogate modelling in
uncertainty quantification where the function is the solution map of a
parametric or stochastic differential equation (DE). Yet, sparse polynomial
approximation lacks a complete theory. On the one hand, there is a
well-developed theory of best $s$-term polynomial approximation, which asserts
exponential or algebraic rates of convergence for holomorphic functions. On the
other, there are increasingly mature methods such as (weighted)
$\ell^1$-minimization for computing such approximations. While the sample
complexity of these methods has been analyzed with compressed sensing, whether
they achieve best $s$-term approximation rates is not fully understood.
Furthermore, these methods are not algorithms per se, as they involve exact
minimizers of nonlinear optimization problems.
</p>
<p>This paper closes these gaps. Specifically, we consider the following
question: are there robust, efficient algorithms for computing approximations
to finite- or infinite-dimensional, holomorphic and Hilbert-valued functions
from limited samples that achieve best $s$-term rates? We answer this
affirmatively by introducing algorithms and theoretical guarantees that assert
exponential or algebraic rates of convergence, along with robustness to
sampling, algorithmic, and physical discretization errors. We tackle both
scalar- and Hilbert-valued functions, this being key to parametric or
stochastic DEs. Our results involve significant developments of existing
techniques, including a novel restarted primal-dual iteration for solving
weighted $\ell^1$-minimization problems in Hilbert spaces. Our theory is
supplemented by numerical experiments demonstrating the efficacy of these
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.10581">FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1">Tuan Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1">Matthias Lenga</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrurier_A/0/1/0/all/0/1">Antoine Serrurier</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1">Sadegh Mohammadi</a></p>
<p>Audio-based classification techniques on body sounds have long been studied
to aid in the diagnosis of respiratory diseases. While most research is
centered on the use of cough as the main biomarker, other body sounds also have
the potential to detect respiratory diseases. Recent studies on COVID-19 have
shown that breath and speech sounds, in addition to cough, correlate with the
disease. Our study proposes Fused Audio Instance and Representation (FAIR) as a
method for respiratory disease detection. FAIR relies on constructing a joint
feature vector from various body sounds represented in waveform and spectrogram
form. We conducted experiments on the use case of COVID-19 detection by
combining waveform and spectrogram representation of body sounds. Our findings
show that the use of self-attention to combine extracted features from cough,
breath, and speech sounds leads to the best performance with an Area Under the
Receiver Operating Characteristic Curve (AUC) score of 0.8658, a sensitivity of
0.8057, and a specificity of 0.7958. Compared to models trained solely on
spectrograms or waveforms, the use of both representations results in an
improved AUC score, demonstrating that combining spectrogram and waveform
representation helps to enrich the extracted features and outperforms the
models that use only one representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15623">k-Means Maximum Entropy Exploration. (arXiv:2205.15623v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nedergaard_A/0/1/0/all/0/1">Alexander Nedergaard</a>, <a href="http://arxiv.org/find/cs/1/au:+Cook_M/0/1/0/all/0/1">Matthew Cook</a></p>
<p>Exploration in high-dimensional, continuous spaces with sparse rewards is an
open problem in reinforcement learning. Artificial curiosity algorithms address
this by creating rewards that lead to exploration. Given a reinforcement
learning algorithm capable of maximizing rewards, the problem reduces to
finding an optimization objective consistent with exploration. Maximum entropy
exploration uses the entropy of the state visitation distribution as such an
objective. However, efficiently estimating the entropy of the state visitation
distribution is challenging in high-dimensional, continuous spaces. We
introduce an artificial curiosity algorithm based on lower bounding an
approximation to the entropy of the state visitation distribution. The bound
relies on a result we prove for non-parametric density estimation in arbitrary
dimensions using k-means. We show that our approach is both computationally
efficient and competitive on benchmarks for exploration in high-dimensional,
continuous spaces, especially on tasks where reinforcement learning algorithms
are unable to find rewards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.09798">Using Sum-Product Networks to Assess Uncertainty in Deep Active Learning. (arXiv:2206.09798v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khosravani_M/0/1/0/all/0/1">Mohamadsadegh Khosravani</a>, <a href="http://arxiv.org/find/cs/1/au:+Zilles_S/0/1/0/all/0/1">Sandra Zilles</a></p>
<p>The success of deep active learning hinges on the choice of an effective
acquisition function, which ranks not yet labeled data points according to
their expected informativeness. Many acquisition functions are (partly) based
on the uncertainty that the current model has about the class label of a point,
yet there is no generally agreed upon strategy for computing such uncertainty.
This paper proposes a new and very simple approach to computing uncertainty in
deep active learning with a Convolutional Neural Network (CNN). The main idea
is to use the feature representation extracted by the CNN as data for training
a Sum-Product Network (SPN). Since SPNs are typically used for estimating the
distribution of a dataset, they are well suited to the task of estimating class
probabilities that can be used directly by standard acquisition functions such
as max entropy and variational ratio. The effectiveness of our method is
demonstrated in an experimental study on several standard benchmark datasets
for image classification, where we compare it to various state-of-the-art
methods for assessing uncertainty in deep active learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.10540">Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1">Yoshitomo Matsubara</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1">Naoya Chiba</a>, <a href="http://arxiv.org/find/cs/1/au:+Igarashi_R/0/1/0/all/0/1">Ryo Igarashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1">Yoshitaka Ushiku</a></p>
<p>This paper revisits datasets and evaluation criteria for Symbolic Regression
(SR), specifically focused on its potential for scientific discovery. Focused
on a set of formulas used in the existing datasets based on Feynman Lectures on
Physics, we recreate 120 datasets to discuss the performance of symbolic
regression for scientific discovery (SRSD). For each of the 120 SRSD datasets,
we carefully review the properties of the formula and its variables to design
reasonably realistic sampling ranges of values so that our new SRSD datasets
can be used for evaluating the potential of SRSD such as whether or not an SR
method can (re)discover physical laws from such datasets. We also create
another 120 datasets that contain dummy variables to examine whether SR methods
can choose necessary variables only. Besides, we propose to use normalized edit
distances (NED) between a predicted equation and the true equation trees for
addressing a critical issue that existing SR metrics are either binary or
errors between the target values and an SR model's predicted values for a given
input. We conduct benchmark experiments on our new SRSD datasets using various
representative SR methods. The experimental results show that we provide a more
realistic performance evaluation, and our user study shows that the NED
correlates with human judges significantly more than an existing SR metric.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.02062">Image Amodal Completion: A Survey. (arXiv:2207.02062v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1">Jiayang Ao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1">Qiuhong Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1">Krista A. Ehinger</a></p>
<p>Existing computer vision systems can compete with humans in understanding the
visible parts of objects, but still fall far short of humans when it comes to
depicting the invisible parts of partially occluded objects. Image amodal
completion aims to equip computers with human-like amodal completion functions
to understand an intact object despite it being partially occluded. The main
purpose of this survey is to provide an intuitive understanding of the research
hotspots, key technologies and future trends in the field of image amodal
completion. Firstly, we present a comprehensive review of the latest literature
in this emerging field, exploring three key tasks in image amodal completion,
including amodal shape completion, amodal appearance completion, and order
perception. Then we examine popular datasets related to image amodal completion
along with their common data collection methods and evaluation metrics.
Finally, we discuss real-world applications and future research directions for
image amodal completion, facilitating the reader's understanding of the
challenges of existing technologies and upcoming research trends.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.14960">Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I: the compact case. (arXiv:2208.14960v3 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Azangulov_I/0/1/0/all/0/1">Iskander Azangulov</a>, <a href="http://arxiv.org/find/stat/1/au:+Smolensky_A/0/1/0/all/0/1">Andrei Smolensky</a>, <a href="http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1">Alexander Terenin</a>, <a href="http://arxiv.org/find/stat/1/au:+Borovitskiy_V/0/1/0/all/0/1">Viacheslav Borovitskiy</a></p>
<p>Gaussian processes are arguably the most important class of spatiotemporal
models within machine learning. They encode prior information about the modeled
function and can be used for exact or approximate Bayesian learning. In many
applications, particularly in physical sciences and engineering, but also in
areas such as geostatistics and neuroscience, invariance to symmetries is one
of the most fundamental forms of prior information one can consider. The
invariance of a Gaussian process' covariance to such symmetries gives rise to
the most natural generalization of the concept of stationarity to such spaces.
In this work, we develop constructive and practical techniques for building
stationary Gaussian processes on a very large class of non-Euclidean spaces
arising in the context of symmetries. Our techniques make it possible to (i)
calculate covariance kernels and (ii) sample from prior and posterior Gaussian
processes defined on such spaces, both in a practical manner. This work is
split into two parts, each involving different technical considerations: part I
studies compact spaces, while part II studies non-compact spaces possessing
certain structure. Our contributions make the non-Euclidean Gaussian process
models we study compatible with well-understood computational techniques
available in standard Gaussian process software packages, thereby making them
accessible to practitioners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.09326">Sparse Interaction Additive Networks via Feature Interaction Detection and Sparse Selection. (arXiv:2209.09326v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Enouen_J/0/1/0/all/0/1">James Enouen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a></p>
<p>There is currently a large gap in performance between the statistically
rigorous methods like linear regression or additive splines and the powerful
deep methods using neural networks. Previous works attempting to close this gap
have failed to fully investigate the exponentially growing number of feature
combinations which deep networks consider automatically during training. In
this work, we develop a tractable selection algorithm to efficiently identify
the necessary feature combinations by leveraging techniques in feature
interaction detection. Our proposed Sparse Interaction Additive Networks (SIAN)
construct a bridge from these simple and interpretable models to fully
connected neural networks. SIAN achieves competitive performance against
state-of-the-art methods across multiple large-scale tabular datasets and
consistently finds an optimal tradeoff between the modeling capacity of neural
networks and the generalizability of simpler methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15042">Generalizability of Adversarial Robustness Under Distribution Shifts. (arXiv:2209.15042v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alhamoud_K/0/1/0/all/0/1">Kumail Alhamoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1">Hasan Abed Al Kader Hammoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1">Motasem Alfarra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a></p>
<p>Recent progress in empirical and certified robustness promises to deliver
reliable and deployable Deep Neural Networks (DNNs). Despite that success, most
existing evaluations of DNN robustness have been done on images sampled from
the same distribution on which the model was trained. However, in the real
world, DNNs may be deployed in dynamic environments that exhibit significant
distribution shifts. In this work, we take a first step towards thoroughly
investigating the interplay between empirical and certified adversarial
robustness on one hand and domain generalization on another. To do so, we train
robust models on multiple domains and evaluate their accuracy and robustness on
an unseen domain. We observe that: (1) both empirical and certified robustness
generalize to unseen domains, and (2) the level of generalizability does not
correlate well with input visual similarity, measured by the FID between source
and target domains. We also extend our study to cover a real-world medical
application, in which adversarial augmentation significantly boosts the
generalization of robustness with minimal effect on clean data accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.02671">A Logic for Expressing Log-Precision Transformers. (arXiv:2210.02671v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1">William Merrill</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1">Ashish Sabharwal</a></p>
<p>One way to interpret the reasoning power of transformer-based language models
is to describe the types of logical rules they can resolve over some input
text. Recently, Chiang et al. (2023) showed that finite-precision transformers
can be equivalently expressed in a generalization of first-order logic.
However, finite-precision transformers are a weak transformer variant because,
as we show, a single head can only attend to a constant number of tokens and,
in particular, cannot represent uniform attention. Since attending broadly is a
core capability for transformers, we ask whether a minimally more expressive
model that can attend universally can also be characterized in logic. To this
end, we analyze transformers whose forward pass is computed in $\log n$
precision on contexts of length $n$. We prove that any log-precision
transformer can be equivalently expressed as a first-order logic sentence that,
in addition to standard universal and existential quantifiers, may also contain
majority-vote quantifiers. This is the tightest known upper bound and first
logical characterization of log-precision transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.05431">Non-Asymptotic Analysis of a UCB-based Top Two Algorithm. (arXiv:2210.05431v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Jourdan_M/0/1/0/all/0/1">Marc Jourdan</a>, <a href="http://arxiv.org/find/stat/1/au:+Degenne_R/0/1/0/all/0/1">R&#xe9;my Degenne</a></p>
<p>A Top Two sampling rule for bandit identification is a method which selects
the next arm to sample from among two candidate arms, a leader and a
challenger. Due to their simplicity and good empirical performance, they have
received increased attention in recent years. However, for fixed-confidence
best arm identification, theoretical guarantees for Top Two methods have only
been obtained in the asymptotic regime, when the error level vanishes. In this
paper, we derive the first non-asymptotic upper bound on the expected sample
complexity of a Top Two algorithm, which holds for any error level. Our
analysis highlights sufficient properties for a regret minimization algorithm
to be used as leader. These properties are satisfied by the UCB algorithm, and
our proposed UCB-based Top Two algorithm simultaneously enjoys non-asymptotic
guarantees and competitive empirical performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09364">Probabilistic Categorical Adversarial Attack &amp; Adversarial Training. (arXiv:2210.09364v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Han Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pengfei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1">Yuxuan Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zitao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a></p>
<p>The existence of adversarial examples brings huge concern for people to apply
Deep Neural Networks (DNNs) in safety-critical tasks. However, how to generate
adversarial examples with categorical data is an important problem but lack of
extensive exploration. Previously established methods leverage greedy search
method, which can be very time-consuming to conduct successful attack. This
also limits the development of adversarial training and potential defenses for
categorical data. To tackle this problem, we propose Probabilistic Categorical
Adversarial Attack (PCAA), which transfers the discrete optimization problem to
a continuous problem that can be solved efficiently by Projected Gradient
Descent. In our paper, we theoretically analyze its optimality and time
complexity to demonstrate its significant advantage over current greedy based
attacks. Moreover, based on our attack, we propose an efficient adversarial
training framework. Through a comprehensive empirical study, we justify the
effectiveness of our proposed attack and defense algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.10514">The Future of Consumer Edge-AI Computing. (arXiv:2210.10514v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1">Stefanos Laskaridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1">Stylianos I. Venieris</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouris_A/0/1/0/all/0/1">Alexandros Kouris</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1">Nicholas D. Lane</a></p>
<p>In the last decade, Deep Learning has rapidly infiltrated the consumer end,
mainly thanks to hardware acceleration across devices. However, as we look
towards the future, it is evident that isolated hardware will be insufficient.
Increasingly complex AI tasks demand shared resources, cross-device
collaboration, and multiple data types, all without compromising user privacy
or quality of experience. To address this, we introduce a novel paradigm
centered around EdgeAI-Hub devices, designed to reorganise and optimise compute
resources and data access at the consumer edge. To this end, we lay a holistic
foundation for the transition from on-device to Edge-AI serving systems in
consumer environments, detailing their components, structure, challenges and
opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12040">Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent Semantic Communications. (arXiv:2210.12040v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1">Christo Kurisummoottil Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1">Walid Saad</a></p>
<p>Semantic communication (SC) aims to communicate reliably with minimal data
transfer while simultaneously providing seamless connectivity to heterogeneous
services and users. In this paper, a novel emergent SC (ESC) system framework
is proposed and is composed of a signaling game for emergent language design
and a neuro-symbolic (NeSy) artificial intelligence (AI) approach for causal
reasoning. In order to design the language, the signaling game is solved using
an alternating maximization between the communicating node's utilities. The
emergent language helps create a context-aware transmit vocabulary (minimal
semantic representation) and aids the reasoning process (enabling
generalization to unseen scenarios) by splitting complex messages into simpler
reasoning tasks for the receiver. The causal description at the transmitter is
then modeled (a neural component) as a posterior distribution of the relevant
attributes present in the data. Using the reconstructed causal state, the
receiver evaluates a set of logical formulas (symbolic part) to execute its
task. The nodes NeSy reasoning components are implemented by the recently
proposed AI tool called Generative Flow Networks, and they are optimized for
higher semantic reliability. The ESC system is designed to enhance the novel
metrics of semantic information, reliability, distortion and similarity that
are designed using rigorous algebraic properties from category theory thereby
generalizing the metrics beyond Shannon's notion of uncertainty. Simulation
results validate the ability of ESC to communicate efficiently (with reduced
bits) and achieve better semantic reliability than conventional wireless and
state-of-the-art systems that do not exploit causal reasoning capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10955">When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method. (arXiv:2211.10955v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Manyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weiran Huang</a></p>
<p>Real-world large-scale datasets are both noisily labeled and
class-imbalanced. The issues seriously hurt the generalization of trained
models. It is hence significant to address the simultaneous incorrect labeling
and class-imbalance, i.e., the problem of learning with noisy labels on
long-tailed data. Previous works develop several methods for the problem.
However, they always rely on strong assumptions that are invalid or hard to be
checked in practice. In this paper, to handle the problem and address the
limitations of prior works, we propose a representation calibration method
RCAL. Specifically, RCAL works with the representations extracted by
unsupervised contrastive learning. We assume that without incorrect labeling
and class imbalance, the representations of instances in each class conform to
a multivariate Gaussian distribution, which is much milder and easier to be
checked. Based on the assumption, we recover underlying representation
distributions from polluted ones resulting from mislabeled and class-imbalanced
data. Additional data points are then sampled from the recovered distributions
to help generalization. Moreover, during classifier training, representation
learning takes advantage of representation robustness brought by contrastive
learning, which further improves the classifier performance. We derive
theoretical results to discuss the effectiveness of our representation
calibration. Experiments on multiple benchmarks justify our claims and confirm
the superiority of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16412">Procedural Image Programs for Representation Learning. (arXiv:2211.16412v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baradad_M/0/1/0/all/0/1">Manel Baradad</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chun-Fu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1">Jonas Wulff</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tongzhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1">Rogerio Feris</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a></p>
<p>Learning image representations using synthetic data allows training neural
networks without some of the concerns associated with real images, such as
privacy and bias. Existing work focuses on a handful of curated generative
processes which require expert knowledge to design, making it hard to scale up.
To overcome this, we propose training with a large dataset of twenty-one
thousand programs, each one generating a diverse set of synthetic images. These
programs are short code snippets, which are easy to modify and fast to execute
using OpenGL. The proposed dataset can be used for both supervised and
unsupervised representation learning, and reduces the gap between pre-training
with real and procedurally generated images by 38%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16583">Offline Policy Evaluation and Optimization under Confounding. (arXiv:2211.16583v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kausik_C/0/1/0/all/0/1">Chinmaya Kausik</a>, <a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1">Yangyi Lu</a>, <a href="http://arxiv.org/find/stat/1/au:+Tan_K/0/1/0/all/0/1">Kevin Tan</a>, <a href="http://arxiv.org/find/stat/1/au:+Makar_M/0/1/0/all/0/1">Maggie Makar</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1">Yixin Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1">Ambuj Tewari</a></p>
<p>Evaluating and optimizing policies in the presence of unobserved confounders
is a problem of growing interest in offline reinforcement learning. Using
conventional methods for offline RL in the presence of confounding can not only
lead to poor decisions and poor policies, but also have disastrous effects in
critical applications such as healthcare and education. We map out the
landscape of offline policy evaluation for confounded MDPs, distinguishing
assumptions on confounding based on whether they are memoryless and on their
effect on the data-collection policies. We characterize settings where
consistent value estimates are provably not achievable, and provide algorithms
with guarantees to instead estimate lower bounds on the value. When consistent
estimates are achievable, we provide algorithms for value estimation with
sample complexity guarantees. We also present new algorithms for offline policy
improvement and prove local convergence guarantees. Finally, we experimentally
evaluate our algorithms on both a gridworld environment and a simulated
healthcare setting of managing sepsis patients. In gridworld, our model-based
method provides tighter lower bounds than existing methods, while in the sepsis
simulator, our methods significantly outperform confounder-oblivious
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01346">Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v8 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1">Kaustubh Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1">Souradeep Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1">James Weimer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Insup Lee</a></p>
<p>Deep neural networks have emerged as the workhorse for a large section of
robotics and control applications, especially as models for dynamical systems.
Such data-driven models are in turn used for designing and verifying autonomous
systems. They are particularly useful in modeling medical systems where data
can be leveraged to individualize treatment. In safety-critical applications,
it is important that the data-driven model is conformant to established
knowledge from the natural sciences. Such knowledge is often available or can
often be distilled into a (possibly black-box) model. For instance, an F1
racing car should conform to Newton's laws (which are encoded within a unicycle
model). In this light, we consider the following problem - given a model $M$
and a state transition dataset, we wish to best approximate the system model
while being a bounded distance away from $M$. We propose a method to guarantee
this conformance. Our first step is to distill the dataset into a few
representative samples called memories, using the idea of a growing neural gas.
Next, using these memories we partition the state space into disjoint subsets
and compute bounds that should be respected by the neural network in each
subset. This serves as a symbolic wrapper for guaranteed conformance. We argue
theoretically that this only leads to a bounded increase in approximation
error; which can be controlled by increasing the number of memories. We
experimentally show that on three case studies (Car Model, Drones, and
Artificial Pancreas), our constrained neurosymbolic models conform to specified
models (each encoding various constraints) with order-of-magnitude improvements
compared to the augmented Lagrangian and vanilla training methods. Our code can
be found at: https://github.com/kaustubhsridhar/Constrained_Models
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04922">Doubly Robust Kernel Statistics for Testing Distributional Treatment Effects. (arXiv:2212.04922v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Fawkes_J/0/1/0/all/0/1">Jake Fawkes</a>, <a href="http://arxiv.org/find/stat/1/au:+Hu_R/0/1/0/all/0/1">Robert Hu</a>, <a href="http://arxiv.org/find/stat/1/au:+Evans_R/0/1/0/all/0/1">Robin J. Evans</a>, <a href="http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1">Dino Sejdinovic</a></p>
<p>With the widespread application of causal inference, it is increasingly
important to have tools which can test for the presence of causal effects in a
diverse array of circumstances. In this vein we focus on the problem of testing
for \emph{distributional} causal effects, where the treatment affects not just
the mean, but also higher order moments of the distribution, as well as
multidimensional or structured outcomes. We build upon a previously introduced
framework, Counterfactual Mean Embeddings, for representing causal
distributions within Reproducing Kernel Hilbert Spaces (RKHS) by proposing new,
improved, estimators for the distributional embeddings. These improved
estimators are inspired by doubly robust estimators of the causal mean, using a
similar form within the kernel space. We analyse these estimators, proving they
retain the doubly robust property and have improved convergence rates compared
to the original estimators. This leads to new permutation based tests for
distributional causal effects, using the estimators we propose as tests
statistics. We experimentally and theoretically demonstrate the validity of our
tests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08324">Mobile Augmented Reality with Federated Learning in the Metaverse. (arXiv:2212.08324v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xinyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a></p>
<p>The Metaverse is deemed the next evolution of the Internet and has received
much attention recently. Metaverse applications via mobile augmented reality
(MAR) require rapid and accurate object detection to mix digital data with the
real world. As mobile devices evolve, their computational capabilities are
increasing, and thus their computational resources can be leveraged to train
machine learning models. In light of the increasing concerns of user privacy
and data security, federated learning (FL) has become a promising distributed
learning framework for privacy-preserving analytics. In this article, FL and
MAR are brought together in the Metaverse. We discuss the necessity and
rationality of the combination of FL and MAR. The prospective technologies that
support FL and MAR in the Metaverse are also discussed. In addition, existing
challenges that prevent the fulfillment of FL and MAR in the Metaverse and
several application scenarios are presented. Finally, three case studies of
Metaverse FL-MAR systems are demonstrated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09462">Latent Diffusion for Language Generation. (arXiv:2212.09462v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lovelace_J/0/1/0/all/0/1">Justin Lovelace</a>, <a href="http://arxiv.org/find/cs/1/au:+Kishore_V/0/1/0/all/0/1">Varsha Kishore</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1">Chao Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shekhtman_E/0/1/0/all/0/1">Eliot Shekhtman</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1">Kilian Q. Weinberger</a></p>
<p>Diffusion models have achieved great success in modeling continuous data
modalities such as images, audio, and video, but have seen limited use in
discrete domains such as language. Recent attempts to adapt diffusion to
language have presented diffusion as an alternative to existing pretrained
language models. We view diffusion and existing language models as
complementary. We demonstrate that encoder-decoder language models can be
utilized to efficiently learn high-quality language autoencoders. We then
demonstrate that continuous diffusion models can be learned in the latent space
of the language autoencoder, enabling us to sample continuous latent
representations that can be decoded into natural language with the pretrained
decoder. We validate the effectiveness of our approach for unconditional,
class-conditional, and sequence-to-sequence language generation. We demonstrate
across multiple diverse data sets that our latent language diffusion models are
significantly more effective than previous diffusion language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.03338">Topologically Regularized Data Embeddings. (arXiv:2301.03338v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Heiter_E/0/1/0/all/0/1">Edith Heiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Vandaele_R/0/1/0/all/0/1">Robin Vandaele</a>, <a href="http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1">Tijl De Bie</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeys_Y/0/1/0/all/0/1">Yvan Saeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Lijffijt_J/0/1/0/all/0/1">Jefrey Lijffijt</a></p>
<p>Unsupervised representation learning methods are widely used for gaining
insight into high-dimensional, unstructured, or structured data. In some cases,
users may have prior topological knowledge about the data, such as a known
cluster structure or the fact that the data is known to lie along a tree- or
graph-structured topology. However, generic methods to ensure such structure is
salient in the low-dimensional representations are lacking. This negatively
impacts the interpretability of low-dimensional embeddings, and plausibly
downstream learning tasks. To address this issue, we introduce topological
regularization: a generic approach based on algebraic topology to incorporate
topological prior knowledge into low-dimensional embeddings. We introduce a
class of topological loss functions, and show that jointly optimizing an
embedding loss with such a topological loss function as a regularizer yields
embeddings that reflect not only local proximities but also the desired
topological structure. We include a self-contained overview of the required
foundational concepts in algebraic topology, and provide intuitive guidance on
how to design topological loss functions for a variety of shapes, such as
clusters, cycles, and bifurcations. We empirically evaluate the proposed
approach on computational efficiency, robustness, and versatility in
combination with linear and non-linear dimensionality reduction and graph
embedding methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.09702">Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1">Jiaqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Reibman_A/0/1/0/all/0/1">Amy R. Reibman</a>, <a href="http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1">Edward J. Delp</a></p>
<p>Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to
learn identity information from labeled images in source domains and apply it
to unlabeled images in a target domain. One major issue with many unsupervised
re-identification methods is that they do not perform well relative to large
domain variations such as illumination, viewpoint, and occlusions. In this
paper, we propose a Synthesis Model Bank (SMB) to deal with illumination
variation in unsupervised person re-ID. The proposed SMB consists of several
convolutional neural networks (CNN) for feature extraction and Mahalanobis
matrices for distance metrics. They are trained using synthetic data with
different illumination conditions such that their synergistic effect makes the
SMB robust against illumination variation. To better quantify the illumination
intensity and improve the quality of synthetic images, we introduce a new 3D
virtual-human dataset for GAN-based image synthesis. From our experiments, the
proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03679">How Reliable is Your Regression Model&#x27;s Uncertainty Under Real-World Distribution Shifts?. (arXiv:2302.03679v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1">Fredrik K. Gustafsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1">Martin Danelljan</a>, <a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1">Thomas B. Sch&#xf6;n</a></p>
<p>Many important computer vision applications are naturally formulated as
regression problems. Within medical imaging, accurate regression models have
the potential to automate various tasks, helping to lower costs and improve
patient outcomes. Such safety-critical deployment does however require reliable
estimation of model uncertainty, also under the wide variety of distribution
shifts that might be encountered in practice. Motivated by this, we set out to
investigate the reliability of regression uncertainty estimation methods under
various real-world distribution shifts. To that end, we propose an extensive
benchmark of 8 image-based regression datasets with different types of
challenging distribution shifts. We then employ our benchmark to evaluate many
of the most common uncertainty estimation methods, as well as two
state-of-the-art uncertainty scores from the task of out-of-distribution
detection. We find that while methods are well calibrated when there is no
distribution shift, they all become highly overconfident on many of the
benchmark datasets. This uncovers important limitations of current uncertainty
estimation methods, and the proposed benchmark therefore serves as a challenge
to the research community. We hope that our benchmark will spur more work on
how to develop truly reliable regression uncertainty estimation methods. Code
is available at https://github.com/fregu856/regression_uncertainty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04178">DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atanackovic_L/0/1/0/all/0/1">Lazar Atanackovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1">Alexander Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1">Leo J. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartford_J/0/1/0/all/0/1">Jason Hartford</a></p>
<p>One of the grand challenges of cell biology is inferring the gene regulatory
network (GRN) which describes interactions between genes and their products
that control gene expression and cellular function. We can treat this as a
causal discovery problem but with two non-standard challenges: (1) regulatory
networks are inherently cyclic so we should not model a GRN as a directed
acyclic graph (DAG), and (2) observations have significant measurement noise,
so for typical sample sizes there will always be a large equivalence class of
graphs that are likely given the data, and we want methods that capture this
uncertainty. Existing methods either focus on challenge (1), identifying cyclic
structure from dynamics, or on challenge (2) learning complex Bayesian
posteriors over DAGs, but not both. In this paper we leverage the fact that it
is possible to estimate the "velocity" of gene expression with RNA velocity
techniques to develop an approach that addresses both challenges. Because we
have access to velocity information, we can treat the Bayesian structure
learning problem as a problem of sparse identification of a dynamical system,
capturing cyclic feedback loops through time. Since our objective is to model
uncertainty over discrete structures, we leverage Generative Flow Networks
(GFlowNets) to estimate the posterior distribution over the combinatorial space
of possible sparse dependencies. Our results indicate that our method learns
posteriors that better encapsulate the distributions of cyclic structures
compared to counterpart state-of-the-art Bayesian structure learning
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07025">Optimal Transport for Change Detection on LiDAR Point Clouds. (arXiv:2302.07025v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1">Marco Fiorucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Naylor_P/0/1/0/all/0/1">Peter Naylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1">Makoto Yamada</a></p>
<p>Unsupervised change detection between airborne LiDAR data points, taken at
separate times over the same location, can be difficult due to unmatching
spatial support and noise from the acquisition system. Most current approaches
to detect changes in point clouds rely heavily on the computation of Digital
Elevation Models (DEM) images and supervised methods. Obtaining a DEM leads to
LiDAR informational loss due to pixelisation, and supervision requires large
amounts of labelled data often unavailable in real-world scenarios. We propose
an unsupervised approach based on the computation of the transport of 3D LiDAR
points over two temporal supports. The method is based on unbalanced optimal
transport and can be generalised to any change detection problem with LiDAR
data. We apply our approach to publicly available datasets for monitoring urban
sprawling in various noise and resolution configurations that mimic several
sensors used in practice. Our method allows for unsupervised multi-class
classification and outperforms the previous state-of-the-art unsupervised
approaches by a significant margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12000">Graph Construction using Principal Axis Trees for Simple Graph Convolution. (arXiv:2302.12000v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alshammari_M/0/1/0/all/0/1">Mashaan Alshammari</a>, <a href="http://arxiv.org/find/cs/1/au:+Stavrakakis_J/0/1/0/all/0/1">John Stavrakakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1">Adel F. Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Takatsuka_M/0/1/0/all/0/1">Masahiro Takatsuka</a></p>
<p>Graph Neural Networks (GNNs) are increasingly becoming the favorite method
for graph learning. They exploit the semi-supervised nature of deep learning,
and they bypass computational bottlenecks associated with traditional graph
learning methods. In addition to the feature matrix $X$, GNNs need an adjacency
matrix $A$ to perform feature propagation. In many cases, the adjacency matrix
$A$ is missing. We introduce a graph construction scheme that constructs the
adjacency matrix $A$ using unsupervised and supervised information.
Unsupervised information characterizes the neighborhood around points. We used
Principal Axis trees (PA-trees) as a source for unsupervised information, where
we create edges between points falling onto the same leaf node. For supervised
information, we used the concept of penalty and intrinsic graphs. A penalty
graph connects points with different class labels, whereas an intrinsic graph
connects points with the same class labels. We used the penalty and intrinsic
graphs to remove or add edges to the graph constructed via PA-tree. We tested
this graph construction scheme on two well-known GNNs: 1) Graph Convolutional
Network (GCN) and 2) Simple Graph Convolution (SGC). The experiments show that
it is better to use SGC because it is faster and delivers better or the same
results as GCN. We also test the effect of oversmoothing on both GCN and SGC.
We found out that the level of smoothing has to be carefully selected for SGC
to avoid oversmoothing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06596">Amodal Intra-class Instance Segmentation: Synthetic Datasets and Benchmark. (arXiv:2303.06596v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1">Jiayang Ao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1">Qiuhong Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1">Krista A. Ehinger</a></p>
<p>Images of realistic scenes often contain intra-class objects that are heavily
occluded from each other, making the amodal perception task that requires
parsing the occluded parts of the objects challenging. Although important for
downstream tasks such as robotic grasping systems, the lack of large-scale
amodal datasets with detailed annotations makes it difficult to model
intra-class occlusions explicitly. This paper introduces two new amodal
datasets for image amodal completion tasks, which contain a total of over 267K
images of intra-class occlusion scenarios, annotated with multiple masks,
amodal bounding boxes, dual order relations and full appearance for instances
and background. We also present a point-supervised scheme with layer priors for
amodal instance segmentation specifically designed for intra-class occlusion
scenarios. Experiments show that our weakly supervised approach outperforms the
SOTA fully supervised methods, while our layer priors design exhibits
remarkable performance improvements in the case of intra-class occlusion in
both synthetic and real images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09354">The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schacherer_D/0/1/0/all/0/1">Daniela P. Schacherer</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrmann_M/0/1/0/all/0/1">Markus D. Herrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Clunie_D/0/1/0/all/0/1">David A. Clunie</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofener_H/0/1/0/all/0/1">Henning H&#xf6;fener</a>, <a href="http://arxiv.org/find/cs/1/au:+Clifford_W/0/1/0/all/0/1">William Clifford</a>, <a href="http://arxiv.org/find/cs/1/au:+Longabaugh_W/0/1/0/all/0/1">William J.R. Longabaugh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pieper_S/0/1/0/all/0/1">Steve Pieper</a>, <a href="http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1">Ron Kikinis</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1">Andrey Fedorov</a>, <a href="http://arxiv.org/find/cs/1/au:+Homeyer_A/0/1/0/all/0/1">Andr&#xe9; Homeyer</a></p>
<p>Background and Objectives: Reproducibility is a major challenge in developing
machine learning (ML)-based solutions in computational pathology (CompPath).
The NCI Imaging Data Commons (IDC) provides &gt;120 cancer image collections
according to the FAIR principles and is designed to be used with cloud ML
services. Here, we explore its potential to facilitate reproducibility in
CompPath research.
</p>
<p>Methods: Using the IDC, we implemented two experiments in which a
representative ML-based method for classifying lung tumor tissue was trained
and/or evaluated on different datasets. To assess reproducibility, the
experiments were run multiple times with separate but identically configured
instances of common ML services.
</p>
<p>Results: The AUC values of different runs of the same experiment were
generally consistent. However, we observed small variations in AUC values of up
to 0.045, indicating a practical limit to reproducibility.
</p>
<p>Conclusions: We conclude that the IDC facilitates approaching the
reproducibility limit of CompPath research (i) by enabling researchers to reuse
exactly the same datasets and (ii) by integrating with cloud ML services so
that experiments can be run in identically configured computing environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10093">Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection. (arXiv:2303.10093v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1">Kyle Buettner</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1">Adriana Kovashka</a></p>
<p>Vision-language alignment learned from image-caption pairs has been shown to
benefit tasks like object recognition and detection. Methods are mostly
evaluated in terms of how well object class names are learned, but captions
also contain rich attribute context that should be considered when learning
object alignment. It is unclear how methods use this context in learning, as
well as whether models succeed when tasks require attribute and object
understanding. To address this gap, we conduct extensive analysis of the role
of attributes in vision-language models. We specifically measure model
sensitivity to the presence and meaning of attribute context, gauging influence
on object embeddings through unsupervised phrase grounding and classification
via description methods. We further evaluate the utility of attribute context
in training for open-vocabulary object detection, fine-grained text-region
retrieval, and attribution tasks. Our results show that attribute context can
be wasted when learning alignment for detection, attribute meaning is not
adequately considered in embeddings, and describing classes by only their
attributes is ineffective. A viable strategy that we find to increase benefits
from attributes is contrastive training with adjective-based negative captions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15714">Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hongyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kangrui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1">Mo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1">Hongyuan Mei</a></p>
<p>Language models have been shown to perform remarkably well on a wide range of
natural language processing tasks. In this paper, we propose LEAP, a novel
system that uses language models to perform multi-step logical reasoning and
incorporates explicit planning into the inference procedure. Explicit planning
enables the system to make more informed reasoning decisions at each step by
looking ahead into their future effects. Moreover, we propose a training
strategy that safeguards the planning process from being led astray by spurious
features. Our full system significantly outperforms other competing methods on
multiple standard datasets. When using small T5 models as its core selection
and deduction components, our system performs competitively compared to GPT-3
despite having only about 1B parameters (i.e., 175 times smaller than GPT-3).
When using GPT-3.5, it significantly outperforms chain-of-thought prompting on
the challenging PrOntoQA dataset. We have conducted extensive empirical studies
to demonstrate that explicit planning plays a crucial role in the system's
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17963">Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lefringhausen_R/0/1/0/all/0/1">Robert Lefringhausen</a>, <a href="http://arxiv.org/find/eess/1/au:+Srithasan_S/0/1/0/all/0/1">Supitsana Srithasan</a>, <a href="http://arxiv.org/find/eess/1/au:+Lederer_A/0/1/0/all/0/1">Armin Lederer</a>, <a href="http://arxiv.org/find/eess/1/au:+Hirche_S/0/1/0/all/0/1">Sandra Hirche</a></p>
<p>As control engineering methods are applied to increasingly complex systems,
data-driven approaches for system identification appear as a promising
alternative to physics-based modeling. While the Bayesian approaches prevalent
for safety-critical applications usually rely on the availability of state
measurements, the states of a complex system are often not directly measurable.
It may then be necessary to jointly estimate the dynamics and the latent state,
making the quantification of uncertainties and the design of controllers with
formal performance guarantees considerably more challenging. This paper
proposes a novel method for the computation of an optimal input trajectory for
unknown nonlinear systems with latent states based on a combination of particle
Markov chain Monte Carlo methods and scenario theory. Probabilistic performance
guarantees are derived for the resulting input trajectory, and an approach to
validate the performance of arbitrary control laws is presented. The
effectiveness of the proposed method is demonstrated in a numerical simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03408">Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1">Blake Bordelon</a>, <a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a></p>
<p>We analyze the dynamics of finite width effects in wide but finite feature
learning neural networks. Starting from a dynamical mean field theory
description of infinite width deep neural network kernel and prediction
dynamics, we provide a characterization of the $O(1/\sqrt{\text{width}})$
fluctuations of the DMFT order parameters over random initializations of the
network weights. Our results, while perturbative in width, unlike prior
analyses, are non-perturbative in the strength of feature learning. In the lazy
limit of network training, all kernels are random but static in time and the
prediction variance has a universal form. However, in the rich, feature
learning regime, the fluctuations of the kernels and predictions are
dynamically coupled with a variance that can be computed self-consistently. In
two layer networks, we show how feature learning can dynamically reduce the
variance of the final tangent kernel and final network predictions. We also
show how initialization variance can slow down online learning in wide but
finite networks. In deeper networks, kernel variance can dramatically
accumulate through subsequent layers at large feature learning strengths, but
feature learning continues to improve the signal-to-noise ratio of the feature
kernels. In discrete time, we demonstrate that large learning rate phenomena
such as edge of stability effects can be well captured by infinite width
dynamics and that initialization variance can decrease dynamically. For CNNs
trained on CIFAR-10, we empirically find significant corrections to both the
bias and variance of network dynamics due to finite width.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13105">Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li-Hsiang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Kuan-I Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsiao_A/0/1/0/all/0/1">An-Hung Hsiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1">Kai-Ten Feng</a></p>
<p>Accurate detection of human presence in indoor environments is important for
various applications, such as energy management and security. In this paper, we
propose a novel system for human presence detection using the channel state
information (CSI) of WiFi signals. Our system named attention-enhanced deep
learning for presence detection (ALPD) employs an attention mechanism to
automatically select informative subcarriers from the CSI data and a
bidirectional long short-term memory (LSTM) network to capture temporal
dependencies in CSI. Additionally, we utilize a static feature to improve the
accuracy of human presence detection in static states. We evaluate the proposed
ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI
dataset, which is further compared with several benchmarks. The results
demonstrate that our ALPD system outperforms the benchmarks in terms of
accuracy, especially in the presence of interference. Moreover, bidirectional
transmission data is beneficial to training improving stability and accuracy,
as well as reducing the costs of data collection for training. Overall, our
proposed ALPD system shows promising results for human presence detection using
WiFi CSI signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13972">Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Li_H/0/1/0/all/0/1">Haochuan Li</a>, <a href="http://arxiv.org/find/math/1/au:+Rakhlin_A/0/1/0/all/0/1">Alexander Rakhlin</a>, <a href="http://arxiv.org/find/math/1/au:+Jadbabaie_A/0/1/0/all/0/1">Ali Jadbabaie</a></p>
<p>In this paper, we provide a rigorous proof of convergence of the Adaptive
Moment Estimate (Adam) algorithm for a wide class of optimization objectives.
Despite the popularity and efficiency of the Adam algorithm in training deep
neural networks, its theoretical properties are not yet fully understood, and
existing convergence proofs require unrealistically strong assumptions, such as
globally bounded gradients, to show the convergence to stationary points. In
this paper, we show that Adam provably converges to $\epsilon$-stationary
points with ${O}(\epsilon^{-4})$ gradient complexity under far more realistic
conditions. The key to our analysis is a new proof of boundedness of gradients
along the optimization trajectory of Adam, under a generalized smoothness
assumption according to which the local smoothness (i.e., Hessian norm when it
exists) is bounded by a sub-quadratic function of the gradient norm. Moreover,
we propose a variance-reduced version of Adam with an accelerated gradient
complexity of ${O}(\epsilon^{-3})$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08463">Convergence Analysis of Mean Shift. (arXiv:2305.08463v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Yamasaki_R/0/1/0/all/0/1">Ryoya Yamasaki</a>, <a href="http://arxiv.org/find/stat/1/au:+Tanaka_T/0/1/0/all/0/1">Toshiyuki Tanaka</a></p>
<p>The mean shift (MS) algorithm seeks a mode of the kernel density estimate
(KDE). This study presents a convergence guarantee of the mode estimate
sequence generated by the MS algorithm and an evaluation of the convergence
rate, under fairly mild conditions, with the help of the argument concerning
the {\L}ojasiewicz inequality. Our findings extend existing ones covering
analytic kernels and the Epanechnikov kernel. Those are significant in that
they cover the biweight kernel, which is optimal among non-negative kernels in
terms of the asymptotic statistical efficiency for the KDE-based mode
estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08573">A graph convolutional autoencoder approach to model order reduction for parametrized PDEs. (arXiv:2305.08573v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Pichi_F/0/1/0/all/0/1">Federico Pichi</a>, <a href="http://arxiv.org/find/math/1/au:+Moya_B/0/1/0/all/0/1">Beatriz Moya</a>, <a href="http://arxiv.org/find/math/1/au:+Hesthaven_J/0/1/0/all/0/1">Jan S. Hesthaven</a></p>
<p>The present work proposes a framework for nonlinear model order reduction
based on a Graph Convolutional Autoencoder (GCA-ROM). In the reduced order
modeling (ROM) context, one is interested in obtaining real-time and many-query
evaluations of parametric Partial Differential Equations (PDEs). Linear
techniques such as Proper Orthogonal Decomposition (POD) and Greedy algorithms
have been analyzed thoroughly, but they are more suitable when dealing with
linear and affine models showing a fast decay of the Kolmogorov n-width. On one
hand, the autoencoder architecture represents a nonlinear generalization of the
POD compression procedure, allowing one to encode the main information in a
latent set of variables while extracting their main features. On the other
hand, Graph Neural Networks (GNNs) constitute a natural framework for studying
PDE solutions defined on unstructured meshes. Here, we develop a non-intrusive
and data-driven nonlinear reduction approach, exploiting GNNs to encode the
reduced manifold and enable fast evaluations of parametrized PDEs. We show the
capabilities of the methodology for several models: linear/nonlinear and
scalar/vector problems with fast/slow decay in the physically and geometrically
parametrized setting. The main properties of our approach consist of (i) high
generalizability in the low-data regime even for complex regimes, (ii) physical
compliance with general unstructured grids, and (iii) exploitation of pooling
and un-pooling operations to learn from scattered data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11531">Generalizing to new geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v4 [physics.ins-det] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1">Junze Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Ghosh_A/0/1/0/all/0/1">Aishik Ghosh</a>, <a href="http://arxiv.org/find/physics/1/au:+Smith_D/0/1/0/all/0/1">Dylan Smith</a>, <a href="http://arxiv.org/find/physics/1/au:+Baldi_P/0/1/0/all/0/1">Pierre Baldi</a>, <a href="http://arxiv.org/find/physics/1/au:+Whiteson_D/0/1/0/all/0/1">Daniel Whiteson</a></p>
<p>Generation of simulated detector response to collision products is crucial to
data analysis in particle physics, but computationally very expensive. One
subdetector, the calorimeter, dominates the computational time due to the high
granularity of its cells and complexity of the interactions. Generative models
can provide more rapid sample production, but currently require significant
effort to optimize performance for specific detector geometries, often
requiring many models to describe the varying cell sizes and arrangements,
without the ability to generalize to other geometries. We develop a
$\textit{geometry-aware}$ autoregressive model, which learns how the
calorimeter response varies with geometry, and is capable of generating
simulated responses to unseen geometries without additional training. The
geometry-aware model outperforms a baseline unaware model by over $50\%$ in
several metrics such as the Wasserstein distance between the generated and the
true distributions of key quantities which summarize the simulated response. A
single geometry-aware model could replace the hundreds of generative models
currently designed for calorimeter simulation by physicists analyzing data
collected at the Large Hadron Collider. This proof-of-concept study motivates
the design of a foundational model that will be a crucial tool for the study of
future detectors, dramatically reducing the large upfront investment usually
needed to develop generative calorimeter models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11915">PINNs error estimates for nonlinear equations in $\mathbb{R}$-smooth Banach spaces. (arXiv:2305.11915v2 [math.FA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gao_J/0/1/0/all/0/1">Jiexing Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Zakharian_Y/0/1/0/all/0/1">Yurii Zakharian</a></p>
<p>In the paper, we describe in operator form classes of PDEs that admit PINN's
error estimation. Also, for $L^p$ spaces, we obtain a Bramble-Hilbert type
lemma that is a tool for PINN's residuals bounding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14517">CongFu: Conditional Graph Fusion for Drug Synergy Prediction. (arXiv:2305.14517v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsepa_O/0/1/0/all/0/1">Oleksii Tsepa</a>, <a href="http://arxiv.org/find/cs/1/au:+Naida_B/0/1/0/all/0/1">Bohdan Naida</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1">Anna Goldenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a></p>
<p>Drug synergy, characterized by the amplified combined effect of multiple
drugs, is critically important for optimizing therapeutic outcomes. Limited
data on drug synergy, arising from the vast number of possible drug
combinations and testing costs, motivate the need for predictive methods. In
this work, we introduce CongFu, a novel Conditional Graph Fusion Layer,
designed to predict drug synergy. CongFu employs an attention mechanism and a
bottleneck to extract local graph contexts and conditionally fuse graph data
within a global context. Its modular architecture enables flexible replacement
of layer modules, including readouts and graph encoders, facilitating
customization for diverse applications. To evaluate the performance of CongFu,
we conduct comprehensive experiments on four datasets, encompassing three
distinct setups for drug synergy prediction. CongFu achieves state-of-the-art
results on 11 out of 12 benchmark datasets, demonstrating its ability to
capture intricate patterns of drug synergy. Through ablation studies, we
validate the significance of individual layer components, affirming their
contributions to overall predictive performance. Finally, we propose an
explainability strategy for elucidating the effect of drugs on genes. By
addressing the challenge of predicting drug synergy in untested drug pairs and
utilizing our proposed explainability approach, CongFu opens new avenues for
optimizing drug combinations and advancing personalized medicine.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15947">Online learning of long-range dependencies. (arXiv:2305.15947v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zucchet_N/0/1/0/all/0/1">Nicolas Zucchet</a>, <a href="http://arxiv.org/find/cs/1/au:+Meier_R/0/1/0/all/0/1">Robert Meier</a>, <a href="http://arxiv.org/find/cs/1/au:+Schug_S/0/1/0/all/0/1">Simon Schug</a>, <a href="http://arxiv.org/find/cs/1/au:+Mujika_A/0/1/0/all/0/1">Asier Mujika</a>, <a href="http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1">Jo&#xe3;o Sacramento</a></p>
<p>Online learning holds the promise of enabling efficient long-term credit
assignment in recurrent neural networks. However, current algorithms fall short
of offline backpropagation by either not being scalable or failing to learn
long-range dependencies. Here we present a high-performance online learning
algorithm that merely doubles the memory and computational requirements of a
single inference pass. We achieve this by leveraging independent recurrent
modules in multi-layer networks, an architectural motif that has recently been
shown to be particularly powerful. Experiments on synthetic memory problems and
on the challenging long-range arena benchmark suite reveal that our algorithm
performs competitively, establishing a new standard for what can be achieved
through online learning. This ability to learn long-range dependencies offers a
new perspective on learning in the brain and opens a promising avenue in
neuromorphic computing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17058">Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach. (arXiv:2305.17058v3 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaiser_F/0/1/0/all/0/1">Fabian Zaiser</a>, <a href="http://arxiv.org/find/cs/1/au:+Murawski_A/0/1/0/all/0/1">Andrzej S. Murawski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1">Luke Ong</a></p>
<p>We present an exact Bayesian inference method for discrete statistical
models, which can find exact solutions to a large class of discrete inference
problems, even with infinite support and continuous priors. To express such
models, we introduce a probabilistic programming language that supports
discrete and continuous sampling, discrete observations, affine functions,
(stochastic) branching, and conditioning on discrete events. Our key tool is
probability generating functions: they provide a compact closed-form
representation of distributions that are definable by programs, thus enabling
the exact computation of posterior probabilities, expectation, variance, and
higher moments. Our inference method is provably correct and fully automated in
a tool called Genfer, which uses automatic differentiation (specifically,
Taylor polynomials), but does not require computer algebra. Our experiments
show that Genfer is often faster than the existing exact inference tools PSI,
Dice, and Prodigy. On a range of real-world inference problems that none of
these exact tools can solve, Genfer's performance is competitive with
approximate Monte Carlo methods, while avoiding approximation errors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17275">Local Convergence of Gradient Methods for Min-Max Games: Partial Curvature Generically Suffices. (arXiv:2305.17275v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wang_G/0/1/0/all/0/1">Guillaume Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Chizat_L/0/1/0/all/0/1">L&#xe9;na&#xef;c Chizat</a></p>
<p>We study the convergence to local Nash equilibria of gradient methods for
two-player zero-sum differentiable games. It is well-known that such dynamics
converge locally when $S \succ 0$ and may diverge when $S=0$, where $S\succeq
0$ is the symmetric part of the Jacobian at equilibrium that accounts for the
"potential" component of the game. We show that these dynamics also converge as
soon as $S$ is nonzero (partial curvature) and the eigenvectors of the
antisymmetric part $A$ are in general position with respect to the kernel of
$S$. We then study the convergence rates when $S \ll A$ and prove that they
typically depend on the average of the eigenvalues of $S$, instead of the
minimum as an analogy with minimization problems would suggest. To illustrate
our results, we consider the problem of computing mixed Nash equilibria of
continuous games. We show that, thanks to partial curvature, conic particle
methods -- which optimize over both weights and supports of the mixed
strategies -- generically converge faster than fixed-support methods. For
min-max games, it is thus beneficial to add degrees of freedom "with
curvature": this can be interpreted as yet another benefit of
over-parameterization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19303">MAGNet: Motif-Agnostic Generation of Molecules from Shapes. (arXiv:2305.19303v2 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Hetzel_L/0/1/0/all/0/1">Leon Hetzel</a>, <a href="http://arxiv.org/find/physics/1/au:+Sommer_J/0/1/0/all/0/1">Johanna Sommer</a>, <a href="http://arxiv.org/find/physics/1/au:+Rieck_B/0/1/0/all/0/1">Bastian Rieck</a>, <a href="http://arxiv.org/find/physics/1/au:+Theis_F/0/1/0/all/0/1">Fabian Theis</a>, <a href="http://arxiv.org/find/physics/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Recent advances in machine learning for molecules exhibit great potential for
facilitating drug discovery from in silico predictions. Most models for
molecule generation rely on the decomposition of molecules into frequently
occurring substructures (motifs), from which they generate novel compounds.
While motif representations greatly aid in learning molecular distributions,
such methods struggle to represent substructures beyond their known motif set.
To alleviate this issue and increase flexibility across datasets, we propose
MAGNet, a graph-based model that generates abstract shapes before allocating
atom and bond types. To this end, we introduce a novel factorisation of the
molecules' data distribution that accounts for the molecules' global context
and facilitates learning adequate assignments of atoms and bonds onto shapes.
Despite the added complexity of shape abstractions, MAGNet outperforms most
other graph-based approaches on standard benchmarks. Importantly, we
demonstrate that MAGNet's improved expressivity leads to molecules with more
topologically distinct structures and, at the same time, diverse atom and bond
assignments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19466">The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kazemnejad_A/0/1/0/all/0/1">Amirhossein Kazemnejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1">Inkit Padhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1">Karthikeyan Natesan Ramamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1">Payel Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a></p>
<p>Length generalization, the ability to generalize from small training context
sizes to larger ones, is a critical challenge in the development of
Transformer-based language models. Positional encoding (PE) has been identified
as a major factor influencing length generalization, but the exact impact of
different PE schemes on extrapolation in downstream tasks remains unclear. In
this paper, we conduct a systematic empirical study comparing the length
generalization performance of decoder-only Transformers with five different
position encoding approaches including Absolute Position Embedding (APE), T5's
Relative PE, ALiBi, and Rotary, in addition to Transformers without positional
encoding (NoPE). Our evaluation encompasses a battery of reasoning and
mathematical tasks. Our findings reveal that the most commonly used positional
encoding methods, such as ALiBi, Rotary, and APE, are not well suited for
length generalization in downstream tasks. More importantly, NoPE outperforms
other explicit positional encoding methods while requiring no additional
computation. We theoretically demonstrate that NoPE can represent both absolute
and relative PEs, but when trained with SGD, it mostly resembles T5's relative
PE attention patterns. Finally, we find that scratchpad is not always helpful
to solve length generalization and its format highly impacts the model's
performance. Overall, our work suggests that explicit position embeddings are
not essential for decoder-only Transformers to generalize well to longer
sequences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00802">Birth of a Transformer: A Memory Viewpoint. (arXiv:2306.00802v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1">Alberto Bietti</a>, <a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1">Vivien Cabannes</a>, <a href="http://arxiv.org/find/stat/1/au:+Bouchacourt_D/0/1/0/all/0/1">Diane Bouchacourt</a>, <a href="http://arxiv.org/find/stat/1/au:+Jegou_H/0/1/0/all/0/1">Herve Jegou</a>, <a href="http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1">Leon Bottou</a></p>
<p>Large language models based on transformers have achieved great empirical
successes. However, as they are deployed more widely, there is a growing need
to better understand their internal mechanisms in order to make them more
reliable. These models appear to store vast amounts of knowledge from their
training data, and to adapt quickly to new information provided in their
context or prompt. We study how transformers balance these two types of
knowledge by considering a synthetic setup where tokens are generated from
either global or context-specific bigram distributions. By a careful empirical
analysis of the training process on a simplified two-layer transformer, we
illustrate the fast learning of global bigrams and the slower development of an
"induction head" mechanism for the in-context bigrams. We highlight the role of
weight matrices as associative memories, provide theoretical insights on how
gradients enable their learning during training, and study the role of
data-distributional properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00904">Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions. (arXiv:2306.00904v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1">Zhaolu Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Peach_R/0/1/0/all/0/1">Robert L. Peach</a>, <a href="http://arxiv.org/find/stat/1/au:+Mediano_P/0/1/0/all/0/1">Pedro A.M. Mediano</a>, <a href="http://arxiv.org/find/stat/1/au:+Barahona_M/0/1/0/all/0/1">Mauricio Barahona</a></p>
<p>Models that rely solely on pairwise relationships often fail to capture the
complete statistical structure of the complex multivariate data found in
diverse domains, such as socio-economic, ecological, or biomedical systems.
Non-trivial dependencies between groups of more than two variables can play a
significant role in the analysis and modelling of such systems, yet extracting
such high-order interactions from data remains challenging. Here, we introduce
a hierarchy of $d$-order ($d \geq 2$) interaction measures, increasingly
inclusive of possible factorisations of the joint probability distribution, and
define non-parametric, kernel-based tests to establish systematically the
statistical significance of $d$-order interactions. We also establish
mathematical links with lattice theory, which elucidate the derivation of the
interaction measures and their composite permutation tests; clarify the
connection of simplicial complexes with kernel matrix centring; and provide a
means to enhance computational efficiency. We illustrate our results
numerically with validations on synthetic data, and through an application to
neuroimaging data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02063">Exploring the Optimal Choice for Generative Processes in Diffusion Models: Ordinary vs Stochastic Differential Equations. (arXiv:2306.02063v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingrun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yixin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiang Zhou</a></p>
<p>The diffusion model has shown remarkable success in computer vision, but it
remains unclear whether the ODE-based probability flow or the SDE-based
diffusion model is more superior and under what circumstances. Comparing the
two is challenging due to dependencies on data distributions, score training,
and other numerical issues. In this paper, we study the problem mathematically
for two limiting scenarios: the zero diffusion (ODE) case and the large
diffusion case. We first introduce a pulse-shape error to perturb the score
function and analyze error accumulation of sampling quality, followed by a
thorough analysis for generalization to arbitrary error. Our findings indicate
that when the perturbation occurs at the end of the generative process, the ODE
model outperforms the SDE model with a large diffusion coefficient. However,
when the perturbation occurs earlier, the SDE model outperforms the ODE model,
and we demonstrate that the error of sample generation due to such a
pulse-shape perturbation is exponentially suppressed as the diffusion term's
magnitude increases to infinity. Numerical validation of this phenomenon is
provided using Gaussian, Gaussian mixture, and Swiss roll distribution, as well
as realistic datasets like MNIST and CIFAR-10.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02866">Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance. (arXiv:2306.02866v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tien Dat Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Suleymanzade_A/0/1/0/all/0/1">Ayhan Suleymanzade</a>, <a href="http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1">Hyeokjun An</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Seunghoon Hong</a></p>
<p>We present a novel framework to overcome the limitations of equivariant
architectures in learning functions with group symmetries. In contrary to
equivariant architectures, we use an arbitrary base model such as an MLP or a
transformer and symmetrize it to be equivariant to the given group by employing
a small equivariant network that parameterizes the probabilistic distribution
underlying the symmetrization. The distribution is end-to-end trained with the
base model which can maximize performance while reducing sample complexity of
symmetrization. We show that this approach ensures not only equivariance to
given group but also universal approximation capability in expectation. We
implement our method on various base models, including patch-based transformers
that can be initialized from pretrained vision transformers, and test them for
a wide range of symmetry groups including permutation and Euclidean groups and
their combinations. Empirical tests show competitive results against tailored
equivariant architectures, suggesting the potential for learning equivariant
functions for diverse groups using a non-equivariant universal base
architecture. We further show evidence of enhanced learning in symmetric
modalities, like graphs, when pretrained from non-symmetric modalities, like
vision. Code is available at https://github.com/jw9730/lps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04096">An enrichment approach for enhancing the expressivity of neural operators with applications to seismology. (arXiv:2306.04096v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1">Ehsan Haghighat</a>, <a href="http://arxiv.org/find/cs/1/au:+Waheed_U/0/1/0/all/0/1">Umair bin Waheed</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Karniadakis</a></p>
<p>The Eikonal equation plays a central role in seismic wave propagation and
hypocenter localization, a crucial aspect of efficient earthquake early warning
systems. Despite recent progress, real-time earthquake localization remains
challenging due to the need to learn a generalizable Eikonal operator. We
introduce a novel deep learning architecture, Enriched-DeepONet (En-DeepONet),
addressing the limitations of current operator learning models in dealing with
moving-solution operators. Leveraging addition and subtraction operations and a
novel `root' network, En-DeepONet is particularly suitable for learning such
operators and achieves up to four orders of magnitude improved accuracy without
increased training cost. We demonstrate the effectiveness of En-DeepONet in
earthquake localization under variable velocity and arrival time conditions.
Our results indicate that En-DeepONet paves the way for real-time hypocenter
localization for velocity models of practical interest. The proposed method
represents a significant advancement in operator learning that is applicable to
a gamut of scientific problems, including those in seismology, fracture
mechanics, and phase-field problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04186">Self-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks. (arXiv:2306.04186v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xian Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Shao_N/0/1/0/all/0/1">Nian Shao</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaofei Li</a></p>
<p>Self-supervised learning (SSL) has emerged as a popular approach for learning
audio representations. One goal of audio self-supervised pre-training is to
transfer knowledge to downstream audio tasks, generally including clip-level
and frame-level tasks. While frame-level tasks are important for fine-grained
acoustic scene/event understanding, prior studies primarily evaluate on
clip-level downstream tasks. In order to tackle both clip-level and frame-level
tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a
clip-level version (named ATST-Clip) and a frame-level version (named
ATST-Frame), responsible for learning clip-level and frame-level
representations, respectively. Both methods use a Transformer encoder and a
teacher-student training scheme. We have carefully designed the view creation
strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses
segment-wise data augmentations, and ATST-Frame integrates frame-wise data
augmentations and masking. Experimental results show that our ATST-Frame model
obtains state-of-the-art (SOTA) performances on most of the clip-level and
frame-level downstream tasks. Especially, it outperforms other models by a
large margin on the frame-level sound event detection task. In addition, the
performance can be further improved by combining the two models through
knowledge distillation. Our code is available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05284">Simple and Controllable Music Generation. (arXiv:2306.05284v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1">Jade Copet</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1">Felix Kreuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1">Itai Gat</a>, <a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1">Tal Remez</a>, <a href="http://arxiv.org/find/cs/1/au:+Kant_D/0/1/0/all/0/1">David Kant</a>, <a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1">Gabriel Synnaeve</a>, <a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1">Yossi Adi</a>, <a href="http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1">Alexandre D&#xe9;fossez</a></p>
<p>We tackle the task of conditional music generation. We introduce MusicGen, a
single Language Model (LM) that operates over several streams of compressed
discrete music representation, i.e., tokens. Unlike prior work, MusicGen is
comprised of a single-stage transformer LM together with efficient token
interleaving patterns, which eliminates the need for cascading several models,
e.g., hierarchically or upsampling. Following this approach, we demonstrate how
MusicGen can generate high-quality samples, both mono and stereo, while being
conditioned on textual description or melodic features, allowing better
controls over the generated output. We conduct extensive empirical evaluation,
considering both automatic and human studies, showing the proposed approach is
superior to the evaluated baselines on a standard text-to-music benchmark.
Through ablation studies, we shed light over the importance of each of the
components comprising MusicGen. Music samples, code, and models are available
at https://github.com/facebookresearch/audiocraft
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10045">Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v9 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1">Yuchao Lin</a>, <a href="http://arxiv.org/find/physics/1/au:+Yan_K/0/1/0/all/0/1">Keqiang Yan</a>, <a href="http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1">Youzhi Luo</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Qian_X/0/1/0/all/0/1">Xiaoning Qian</a>, <a href="http://arxiv.org/find/physics/1/au:+Ji_S/0/1/0/all/0/1">Shuiwang Ji</a></p>
<p>We study property prediction for crystal materials. A crystal structure
consists of a minimal unit cell that is repeated infinitely in 3D space. How to
accurately represent such repetitive structures in machine learning models
remains unresolved. Current methods construct graphs by establishing edges only
between nearby nodes, thereby failing to faithfully capture infinite repeating
patterns and distant interatomic interactions. In this work, we propose several
innovations to overcome these limitations. First, we propose to model
physics-principled interatomic potentials directly instead of only using
distances as in many existing methods. These potentials include the Coulomb
potential, London dispersion potential, and Pauli repulsion potential. Second,
we model the complete set of potentials among all atoms, instead of only
between nearby atoms as in existing methods. This is enabled by our
approximations of infinite potential summations, where we extend the Ewald
summation for several potential series approximations with provable error
bounds. Finally, we propose to incorporate our computations of complete
interatomic potentials into message passing neural networks for representation
learning. We perform experiments on the JARVIS and Materials Project benchmarks
for evaluation. Results show that the use of interatomic potentials and
complete interatomic potentials leads to consistent performance improvements
with reasonable computational costs. Our code is publicly available as part of
the AIRS library (https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10280">OpenGSL: A Comprehensive Benchmark for Graph Structure Learning. (arXiv:2306.10280v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhiyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_B/0/1/0/all/0/1">Bochao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xuanyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1">Qiaoyu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Can Wang</a></p>
<p>Graph Neural Networks (GNNs) have emerged as the de facto standard for
representation learning on graphs, owing to their ability to effectively
integrate graph topology and node attributes. However, the inherent suboptimal
nature of node connections, resulting from the complex and contingent formation
process of graphs, presents significant challenges in modeling them
effectively. To tackle this issue, Graph Structure Learning (GSL), a family of
data-centric learning approaches, has garnered substantial attention in recent
years. The core concept behind GSL is to jointly optimize the graph structure
and the corresponding GNN models. Despite the proposal of numerous GSL methods,
the progress in this field remains unclear due to inconsistent experimental
protocols, including variations in datasets, data processing techniques, and
splitting strategies. In this paper, we introduce OpenGSL, the first
comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables
a fair comparison among state-of-the-art GSL methods by evaluating them across
various popular datasets using uniform data processing and splitting
strategies. Through extensive experiments, we observe that existing GSL methods
do not consistently outperform vanilla GNN counterparts. We also find that
there is no significant correlation between the homophily of the learned
structure and task performance, challenging the common belief. Moreover, we
observe that the learned graph structure demonstrates a strong generalization
ability across different GNN models, despite the high computational and space
consumption. We hope that our open-sourced library will facilitate rapid and
equitable evaluation and inspire further innovative research in this field. The
code of the benchmark can be found in https://github.com/OpenGSL/OpenGSL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11412">Size Matters: Large Graph Generation with HiGGs. (arXiv:2306.11412v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1">Alex O. Davies</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajmeri_N/0/1/0/all/0/1">Nirav S. Ajmeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Filho_T/0/1/0/all/0/1">Telmo M. Silva Filho</a></p>
<p>Large graphs are present in a variety of domains, including social networks,
civil infrastructure, and the physical sciences to name a few. Graph generation
is similarly widespread, with applications in drug discovery, network analysis
and synthetic datasets among others. While GNN (Graph Neural Network) models
have been applied in these domains their high in-memory costs restrict them to
small graphs. Conversely less costly rule-based methods struggle to reproduce
complex structures. We propose HIGGS (Hierarchical Generation of Graphs) as a
model-agnostic framework of producing large graphs with realistic local
structures. HIGGS uses GNN models with conditional generation capabilities to
sample graphs in hierarchies of resolution. As a result HIGGS has the capacity
to extend the scale of generated graphs from a given GNN model by quadratic
order. As a demonstration we implement HIGGS using DiGress, a recent
graph-diffusion model, including a novel edge-predictive-diffusion variant
edge-DiGress. We use this implementation to generate categorically attributed
graphs with tens of thousands of nodes. These HIGGS generated graphs are far
larger than any previously produced using GNNs. Despite this jump in scale we
demonstrate that the graphs produced by HIGGS are, on the local scale, more
realistic than those from the rule-based model BTER.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11681">MoleCLUEs: Molecular Conformers Maximally In-Distribution for Predictive Models. (arXiv:2306.11681v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maser_M/0/1/0/all/0/1">Michael Maser</a>, <a href="http://arxiv.org/find/cs/1/au:+Tagasovska_N/0/1/0/all/0/1">Natasa Tagasovska</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jae Hyeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Watkins_A/0/1/0/all/0/1">Andrew Watkins</a></p>
<p>Structure-based molecular ML (SBML) models can be highly sensitive to input
geometries and give predictions with large variance. We present an approach to
mitigate the challenge of selecting conformations for such models by generating
conformers that explicitly minimize predictive uncertainty. To achieve this, we
compute estimates of aleatoric and epistemic uncertainties that are
differentiable w.r.t. latent posteriors. We then iteratively sample new latents
in the direction of lower uncertainty by gradient descent. As we train our
predictive models jointly with a conformer decoder, the new latent embeddings
can be mapped to their corresponding inputs, which we call \textit{MoleCLUEs},
or (molecular) counterfactual latent uncertainty explanations
\citep{antoran2020getting}. We assess our algorithm for the task of predicting
drug properties from 3D structure with maximum confidence. We additionally
analyze the structure trajectories obtained from conformer optimizations, which
provide insight into the sources of uncertainty in SBML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14351">Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions. (arXiv:2306.14351v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ibeling_D/0/1/0/all/0/1">Duligur Ibeling</a>, <a href="http://arxiv.org/find/stat/1/au:+Icard_T/0/1/0/all/0/1">Thomas Icard</a></p>
<p>The aim of this paper is to make clear and precise the relationship between
the Rubin causal model (RCM) and structural causal model (SCM) frameworks for
causal inference. Adopting a neutral logical perspective, and drawing on
previous work, we show what is required for an RCM to be representable by an
SCM. A key result then shows that every RCM -- including those that violate
algebraic principles implied by the SCM framework -- emerges as an abstraction
of some representable RCM. Finally, we illustrate the power of this
conciliatory perspective by pinpointing an important role for SCM principles in
classic applications of RCMs; conversely, we offer a characterization of the
algebraic constraints implied by a graph, helping to substantiate further
comparisons between the two frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14411">Score-based Source Separation with Applications to Digital Communication Signals. (arXiv:2306.14411v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jayashankar_T/0/1/0/all/0/1">Tejas Jayashankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gary C.F. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lancho_A/0/1/0/all/0/1">Alejandro Lancho</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_A/0/1/0/all/0/1">Amir Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Polyanskiy_Y/0/1/0/all/0/1">Yury Polyanskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wornell_G/0/1/0/all/0/1">Gregory W. Wornell</a></p>
<p>We propose a new method for separating superimposed sources using
diffusion-based generative models. Our method relies only on separately trained
statistical priors of independent sources to establish a new objective function
guided by maximum a posteriori estimation with an $\alpha$-posterior, across
multiple levels of Gaussian smoothing. Motivated by applications in
radio-frequency (RF) systems, we are interested in sources with underlying
discrete nature and the recovery of encoded bits from a signal of interest, as
measured by the bit error rate (BER). Experimental results with RF mixtures
demonstrate that our method results in a BER reduction of 95% over classical
and existing learning-based methods. Our analysis demonstrates that our
proposed method yields solutions that asymptotically approach the modes of an
underlying discrete distribution. Furthermore, our method can be viewed as a
multi-source extension to the recently proposed score distillation sampling
scheme, shedding additional light on its use beyond conditional sampling. The
project webpage is available at https://alpha-rgs.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14770">ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion. (arXiv:2306.14770v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yingjun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1">Zehao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1">Shengcai Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1">Cees Snoek</a></p>
<p>Prototype-based meta-learning has emerged as a powerful technique for
addressing few-shot learning challenges. However, estimating a deterministic
prototype using a simple average function from a limited number of examples
remains a fragile process. To overcome this limitation, we introduce ProtoDiff,
a novel framework that leverages a task-guided diffusion model during the
meta-training phase to gradually generate prototypes, thereby providing
efficient class representations. Specifically, a set of prototypes is optimized
to achieve per-task prototype overfitting, enabling accurately obtaining the
overfitted prototypes for individual tasks. Furthermore, we introduce a
task-guided diffusion process within the prototype space, enabling the
meta-learning of a generative process that transitions from a vanilla prototype
to an overfitted prototype. ProtoDiff gradually generates task-specific
prototypes from random noise during the meta-test stage, conditioned on the
limited samples available for the new task. Furthermore, to expedite training
and enhance ProtoDiff's performance, we propose the utilization of residual
prototype learning, which leverages the sparsity of the residual prototype. We
conduct thorough ablation studies to demonstrate its ability to accurately
capture the underlying prototype distribution and enhance generalization. The
new state-of-the-art performance on within-domain, cross-domain, and few-task
few-shot classification further substantiates the benefit of ProtoDiff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01050">Transport meets Variational Inference: Controlled Monte Carlo Diffusions. (arXiv:2307.01050v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1">Francisco Vargas</a>, <a href="http://arxiv.org/find/stat/1/au:+Padhy_S/0/1/0/all/0/1">Shreyas Padhy</a>, <a href="http://arxiv.org/find/stat/1/au:+Blessing_D/0/1/0/all/0/1">Denis Blessing</a>, <a href="http://arxiv.org/find/stat/1/au:+Nusken_N/0/1/0/all/0/1">Nikolas N&#xfc;sken</a></p>
<p>Connecting optimal transport and variational inference, we present a
principled and systematic framework for sampling and generative modelling
centred around divergences on path space. Our work culminates in the
development of the \emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for
Bayesian computation, a score-based annealing technique that crucially adapts
both forward and backward dynamics in a diffusion model. On the way, we clarify
the relationship between the EM-algorithm and iterative proportional fitting
(IPF) for Schr{\"o}dinger bridges, deriving as well a regularised objective
that bypasses the iterative bottleneck of standard IPF-updates. Finally, we
show that CMCD has a strong foundation in the Jarzinsky and Crooks identities
from statistical physics, and that it convincingly outperforms competing
approaches across a wide array of experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04841">Loss Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1">Blake Bordelon</a>, <a href="http://arxiv.org/find/stat/1/au:+Masset_P/0/1/0/all/0/1">Paul Masset</a>, <a href="http://arxiv.org/find/stat/1/au:+Kuo_H/0/1/0/all/0/1">Henry Kuo</a>, <a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a></p>
<p>Reinforcement learning has been successful across several applications in
which agents have to learn to act in environments with sparse feedback.
However, despite this empirical success there is still a lack of theoretical
understanding of how the parameters of reinforcement learning models and the
features used to represent states interact to control the dynamics of learning.
In this work, we use concepts from statistical physics, to study the typical
case learning curves for temporal difference learning of a value function with
linear function approximators. Our theory is derived under a Gaussian
equivalence hypothesis where averages over the random trajectories are replaced
with temporally correlated Gaussian feature averages and we validate our
assumptions on small scale Markov Decision Processes. We find that the
stochastic semi-gradient noise due to subsampling the space of possible
episodes leads to significant plateaus in the value error, unlike in
traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and
reward function. We then analyze how strategies like learning rate annealing
and reward shaping can favorably alter learning dynamics and plateaus. To
conclude, our work introduces new tools to open a new direction towards
developing a theory of learning dynamics in reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05857">FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tianyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Taherisadr_M/0/1/0/all/0/1">Mojtaba Taherisadr</a>, <a href="http://arxiv.org/find/cs/1/au:+Elmalaki_S/0/1/0/all/0/1">Salma Elmalaki</a></p>
<p>Achieving fairness in sequential-decision making systems within
Human-in-the-Loop (HITL) environments is a critical concern, especially when
multiple humans with different behavior and expectations are affected by the
same adaptation decisions in the system. This human variability factor adds
more complexity since policies deemed fair at one point in time may become
discriminatory over time due to variations in human preferences resulting from
inter- and intra-human variability. This paper addresses the fairness problem
from an equity lens, considering human behavior variability, and the changes in
human preferences over time. We propose FAIRO, a novel algorithm for
fairness-aware sequential-decision making in HITL adaptation, which
incorporates these notions into the decision-making process. In particular,
FAIRO decomposes this complex fairness task into adaptive sub-tasks based on
individual human preferences through leveraging the Options reinforcement
learning framework. We design FAIRO to generalize to three types of HITL
application setups that have the shared adaptation decision problem.
Furthermore, we recognize that fairness-aware policies can sometimes conflict
with the application's utility. To address this challenge, we provide a
fairness-utility tradeoff in FAIRO, allowing system designers to balance the
objectives of fairness and utility based on specific application requirements.
Extensive evaluations of FAIRO on the three HITL applications demonstrate its
generalizability and effectiveness in promoting fairness while accounting for
human variability. On average, FAIRO can improve fairness compared with other
methods across all three applications by 35.36%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07870">Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1">Grgur Kova&#x10d;</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1">Masataka Sawayama</a>, <a href="http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1">R&#xe9;my Portelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1">C&#xe9;dric Colas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1">Peter Ford Dominey</a>, <a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1">Pierre-Yves Oudeyer</a></p>
<p>Large Language Models (LLMs) are often misleadingly recognized as having a
personality or a set of values. We argue that an LLM can be seen as a
superposition of perspectives with different values and personality traits.
LLMs exhibit context-dependent values and personality traits that change based
on the induced perspective (as opposed to humans, who tend to have more
coherent values and personality traits across contexts). We introduce the
concept of perspective controllability, which refers to a model's affordance to
adopt various perspectives with differing values and personality traits. In our
experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study
how exhibited values and personality traits change based on different
perspectives. Through qualitative experiments, we show that LLMs express
different values when those are (implicitly or explicitly) implied in the
prompt, and that LLMs express different values even when those are not
obviously implied (demonstrating their context-dependent nature). We then
conduct quantitative experiments to study the controllability of different
models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the
effectiveness of various methods for inducing perspectives, and the smoothness
of the models' drivability. We conclude by examining the broader implications
of our work and outline a variety of associated scientific questions. The
project website is available at
https://sites.google.com/view/llm-superpositions .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09212">How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1">Itay Safran</a>, <a href="http://arxiv.org/find/cs/1/au:+Reichman_D/0/1/0/all/0/1">Daniel Reichman</a>, <a href="http://arxiv.org/find/cs/1/au:+Valiant_P/0/1/0/all/0/1">Paul Valiant</a></p>
<p>We study the size of a neural network needed to approximate the maximum
function over $d$ inputs, in the most basic setting of approximating with
respect to the $L_2$ norm, for continuous distributions, for a network that
uses ReLU activations. We provide new lower and upper bounds on the width
required for approximation across various depths. Our results establish new
depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as
providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$
construction which approximates the maximum function. Our depth separation
results are facilitated by a new lower bound for depth 2 networks approximating
the maximum function over the uniform distribution, assuming an exponential
upper bound on the size of the weights. Furthermore, we are able to use this
depth 2 lower bound to provide tight bounds on the number of neurons needed to
approximate the maximum by a depth 3 network. Our lower bounds are of
potentially broad interest as they apply to the widely studied and used
\emph{max} function, in contrast to many previous results that base their
bounds on specially constructed or pathological functions and distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11620">Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization. (arXiv:2307.11620v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangsen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haoran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yinan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1">Xianyuan Zhan</a></p>
<p>Offline reinforcement learning (RL) has received considerable attention in
recent years due to its attractive capability of learning policies from offline
datasets without environmental interactions. Despite some success in the
single-agent setting, offline multi-agent RL (MARL) remains to be a challenge.
The large joint state-action space and the coupled multi-agent behaviors pose
extra complexities for offline policy optimization. Most existing offline MARL
studies simply apply offline data-related regularizations on individual agents,
without fully considering the multi-agent system at the global level. In this
work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit
global-to-local v alue regularization. OMIGA provides a principled framework to
convert global-level value regularization into equivalent implicit local value
regularizations and simultaneously enables in-sample learning, thus elegantly
bridging multi-agent value decomposition and policy learning with offline
regularizations. Based on comprehensive experiments on the offline multi-agent
MuJoCo and StarCraft II micro-management tasks, we show that OMIGA achieves
superior performance over the state-of-the-art offline MARL methods in almost
all tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13214">FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Huy Q. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1">Minh N. H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Thwal_C/0/1/0/all/0/1">Chu Myaet Thwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1">Choong Seon Hong</a></p>
<p>Federated learning (FL) enables a decentralized machine learning paradigm for
multiple clients to collaboratively train a generalized global model without
sharing their private data. Most existing works simply propose typical FL
systems for single-modal data, thus limiting its potential on exploiting
valuable multimodal data for future personalized applications. Furthermore, the
majority of FL approaches still rely on the labeled data at the client side,
which is limited in real-world applications due to the inability of
self-annotation from users. In light of these limitations, we propose a novel
multimodal FL framework that employs a semi-supervised learning approach to
leverage the representations from different modalities. Bringing this concept
into a system, we develop a distillation-based multimodal embedding knowledge
transfer mechanism, namely FedMEKT, which allows the server and clients to
exchange the joint knowledge of their learning models extracted from a small
multimodal proxy dataset. Our FedMEKT iteratively updates the generalized
global encoders with the joint embedding knowledge from the participating
clients. Thereby, to address the modality discrepancy and labeled data
constraint in existing FL systems, our proposed FedMEKT comprises local
multimodal autoencoder learning, generalized multimodal autoencoder
construction, and generalized classifier learning. Through extensive
experiments on three multimodal human activity recognition datasets, we
demonstrate that FedMEKT achieves superior global encoder performance on linear
evaluation and guarantees user privacy for personal data and model parameters
while demanding less communication cost than other baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13813">How to Scale Your EMA. (arXiv:2307.13813v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Busbridge_D/0/1/0/all/0/1">Dan Busbridge</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramapuram_J/0/1/0/all/0/1">Jason Ramapuram</a>, <a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1">Pierre Ablin</a>, <a href="http://arxiv.org/find/stat/1/au:+Likhomanenko_T/0/1/0/all/0/1">Tatiana Likhomanenko</a>, <a href="http://arxiv.org/find/stat/1/au:+Dhekane_E/0/1/0/all/0/1">Eeshan Gunesh Dhekane</a>, <a href="http://arxiv.org/find/stat/1/au:+Suau_X/0/1/0/all/0/1">Xavier Suau</a>, <a href="http://arxiv.org/find/stat/1/au:+Webb_R/0/1/0/all/0/1">Russ Webb</a></p>
<p>Preserving training dynamics across batch sizes is an important tool for
practical machine learning as it enables the trade-off between batch size and
wall-clock time. This trade-off is typically enabled by a scaling rule, for
example, in stochastic gradient descent, one should scale the learning rate
linearly with the batch size. Another important machine learning tool is the
model EMA, a functional copy of a target model, whose parameters move towards
those of its target model according to an Exponential Moving Average (EMA) at a
rate parameterized by a momentum hyperparameter. This model EMA can improve the
robustness and generalization of supervised learning, stabilize
pseudo-labeling, and provide a learning signal for Self-Supervised Learning
(SSL). Prior works have not considered the optimization of the model EMA when
performing scaling, leading to different training dynamics across batch sizes
and lower model performance. In this work, we provide a scaling rule for
optimization in the presence of a model EMA and demonstrate the rule's validity
across a range of architectures, optimizers, and data modalities. We also show
the rule's validity where the model EMA contributes to the optimization of the
target model, enabling us to train EMA-based pseudo-labeling and SSL methods at
small and large batch sizes. For SSL, we enable training of BYOL up to batch
size 24,576 without sacrificing performance, a 6$\times$ wall-clock time
reduction under idealized hardware settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09000">DealMVC: Dual Contrastive Calibration for Multi-view Clustering. (arXiv:2308.09000v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xihong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jiaqi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Ke Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Suyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Sihang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinwang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1">En Zhu</a></p>
<p>Benefiting from the strong view-consistent information mining capacity,
multi-view contrastive clustering has attracted plenty of attention in recent
years. However, we observe the following drawback, which limits the clustering
performance from further improvement. The existing multi-view models mainly
focus on the consistency of the same samples in different views while ignoring
the circumstance of similar but different samples in cross-view scenarios. To
solve this problem, we propose a novel Dual contrastive calibration network for
Multi-View Clustering (DealMVC). Specifically, we first design a fusion
mechanism to obtain a global cross-view feature. Then, a global contrastive
calibration loss is proposed by aligning the view feature similarity graph and
the high-confidence pseudo-label graph. Moreover, to utilize the diversity of
multi-view information, we propose a local contrastive calibration loss to
constrain the consistency of pair-wise view features. The feature structure is
regularized by reliable class information, thus guaranteeing similar samples
have similar features in different views. During the training procedure, the
interacted cross-view feature is jointly optimized at both local and global
levels. In comparison with other state-of-the-art approaches, the comprehensive
experimental results obtained from eight benchmark datasets provide substantial
validation of the effectiveness and superiority of our algorithm. We release
the code of DealMVC at https://github.com/xihongyang1999/DealMVC on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10364">SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Midgley_L/0/1/0/all/0/1">Laurence I. Midgley</a>, <a href="http://arxiv.org/find/cs/1/au:+Stimper_V/0/1/0/all/0/1">Vincent Stimper</a>, <a href="http://arxiv.org/find/cs/1/au:+Antoran_J/0/1/0/all/0/1">Javier Antor&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathieu_E/0/1/0/all/0/1">Emile Mathieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a></p>
<p>Coupling normalizing flows allow for fast sampling and density evaluation,
making them the tool of choice for probabilistic modeling of physical systems.
However, the standard coupling architecture precludes endowing flows that
operate on the Cartesian coordinates of atoms with the SE(3) and permutation
invariances of physical systems. This work proposes a coupling flow that
preserves SE(3) and permutation equivariance by performing coordinate splits
along additional augmented dimensions. At each layer, the flow maps atoms'
positions into learned SE(3) invariant bases, where we apply standard flow
transformations, such as monotonic rational-quadratic splines, before returning
to the original basis. Crucially, our flow preserves fast sampling and density
evaluation, and may be used to produce unbiased estimates of expectations with
respect to the target distribution via importance sampling. When trained on the
DW4, LJ13, and QM9-positional datasets, our flow is competitive with
equivariant continuous normalizing flows, while allowing sampling more than an
order of magnitude faster. Moreover, to the best of our knowledge, we are the
first to learn the full Boltzmann distribution of alanine dipeptide by only
modeling the Cartesian positions of its atoms. Lastly, we demonstrate that our
flow can be trained to approximately sample from the Boltzmann distribution of
the DW4 and LJ13 particle systems using only their energy functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10544">Towards Accelerated Model Training via Bayesian Data Selection. (arXiv:2308.10544v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhijie Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1">Peng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a></p>
<p>Mislabeled, duplicated, or biased data in real-world scenarios can lead to
prolonged training and even hinder model convergence. Traditional solutions
prioritizing easy or hard samples lack the flexibility to handle such a variety
simultaneously. Recent work has proposed a more reasonable data selection
principle by examining the data's impact on the model's generalization loss.
However, its practical adoption relies on less principled approximations and
additional holdout data. This work solves these problems by leveraging a
lightweight Bayesian treatment and incorporating off-the-shelf zero-shot
predictors built on large-scale pre-trained models. The resulting algorithm is
efficient and easy to implement. We perform extensive empirical studies on
challenging benchmarks with considerable data noise and imbalance in the online
batch selection scenario, and observe superior training efficiency over
competitive baselines. Notably, on the challenging WebVision benchmark, our
method can achieve similar predictive performance with significantly fewer
training iterations than leading data selection methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11849">A Mobile Data-Driven Hierarchical Deep Reinforcement Learning Approach for Real-time Demand-Responsive Railway Rescheduling and Station Overcrowding Mitigation. (arXiv:2308.11849v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_E/0/1/0/all/0/1">Enze Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1">Zhiyuan Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Judith Y.T. Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a></p>
<p>Real-time railway rescheduling is an important technique to enable
operational recovery in response to unexpected and dynamic conditions in a
timely and flexible manner. Current research relies mostly on OD based data and
model-based methods for estimating train passenger demands. These approaches
primarily focus on averaged disruption patterns, often overlooking the
immediate uneven distribution of demand over time. In reality, passenger demand
deviates significantly from predictions, especially during a disaster.
Disastrous situations such as flood in Zhengzhou, China in 2022 has created not
only unprecedented effect on Zhengzhou railway station itself, which is a major
railway hub in China, but also other major hubs connected to Zhengzhou, e.g.,
Xi'an, the closest hub west of Zhengzhou. In this study, we define a real-time
demand-responsive (RTDR) railway rescheduling problem focusing two specific
aspects, namely, volatility of the demand, and management of station
crowdedness. For the first time, we propose a data-driven approach using
real-time mobile data (MD) to deal with this RTDR problem. A hierarchical deep
reinforcement learning (HDRL) framework is designed to perform real-time
rescheduling in a demand-responsive manner. The use of MD has enabled the
modelling of passenger dynamics in response to train delays and station
crowdedness, and a real-time optimisation for rescheduling of train services in
view of the change in demand as a result of passengers' behavioural response to
disruption. Results show that the agent can steadily satisfy over 62% of the
demand with only 61% of the original rolling stock, ensuring continuous
operations without overcrowding. Moreover, the agent exhibits adaptability when
transferred to a new environment with increased demand, highlighting its
effectiveness in addressing unforeseen disruptions in real-time settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14132">Detecting Language Model Attacks with Perplexity. (arXiv:2308.14132v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1">Gabriel Alon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1">Michael Kamfonas</a></p>
<p>A novel hack involving Large Language Models (LLMs) has emerged, exploiting
adversarial suffixes to deceive models into generating perilous responses. Such
jailbreaks can trick LLMs into providing intricate instructions to a malicious
user for creating explosives, orchestrating a bank heist, or facilitating the
creation of offensive content. By evaluating the perplexity of queries with
adversarial suffixes using an open-source LLM (GPT-2), we found that they have
exceedingly high perplexity values. As we explored a broad range of regular
(non-adversarial) prompt varieties, we concluded that false positives are a
significant challenge for plain perplexity filtering. A Light-GBM trained on
perplexity and token length resolved the false positives and correctly detected
most adversarial attacks in the test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15873">Minimum Width for Deep, Narrow MLP: A Diffeomorphism Approach. (arXiv:2308.15873v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1">Geonho Hwang</a></p>
<p>Recently, there has been a growing focus on determining the minimum width
requirements for achieving the universal approximation property in deep, narrow
Multi-Layer Perceptrons (MLPs). Among these challenges, one particularly
challenging task is approximating a continuous function under the uniform norm,
as indicated by the significant disparity between its lower and upper bounds.
To address this problem, we propose a framework that simplifies finding the
minimum width for deep, narrow MLPs into determining a purely geometrical
function denoted as $w(d_x, d_y)$. This function relies solely on the input and
output dimensions, represented as $d_x$ and $d_y$, respectively. Two key steps
support this framework. First, we demonstrate that deep, narrow MLPs, when
provided with a small additional width, can approximate a $C^2$-diffeomorphism.
Subsequently, using this result, we prove that $w(d_x, d_y)$ equates to the
optimal minimum width required for deep, narrow MLPs to achieve universality.
By employing the aforementioned framework and the Whitney embedding theorem, we
provide an upper bound for the minimum width, given by
$\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$, where $0 \leq
\alpha(\sigma) \leq 2$ represents a constant depending on the activation
function. Furthermore, we provide a lower bound of $4$ for the minimum width in
cases where the input and output dimensions are both equal to two.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00543">Curating Naturally Adversarial Datasets for Learning-Enabled Medical Cyber-Physical Systems. (arXiv:2309.00543v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pugh_S/0/1/0/all/0/1">Sydney Pugh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruchkin_I/0/1/0/all/0/1">Ivan Ruchkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Insup Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1">James Weimer</a></p>
<p>Deep learning models have shown promising predictive accuracy for time-series
healthcare applications. However, ensuring the robustness of these models is
vital for building trustworthy AI systems. Existing research predominantly
focuses on robustness to synthetic adversarial examples, crafted by adding
imperceptible perturbations to clean input data. However, these synthetic
adversarial examples do not accurately reflect the most challenging real-world
scenarios, especially in the context of healthcare data. Consequently,
robustness to synthetic adversarial examples may not necessarily translate to
robustness against naturally occurring adversarial examples, which is highly
desirable for trustworthy AI. We propose a method to curate datasets comprised
of natural adversarial examples to evaluate model robustness. The method relies
on probabilistic labels obtained from automated weakly-supervised labeling that
combines noisy and cheap-to-obtain labeling heuristics. Based on these labels,
our method adversarially orders the input data and uses this ordering to
construct a sequence of increasingly adversarial datasets. Our evaluation on
six medical case studies and three non-medical case studies demonstrates the
efficacy and statistical validity of our approach to generating naturally
adversarial datasets
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02422">Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. (arXiv:2309.02422v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Paik_S/0/1/0/all/0/1">Seunghoon Paik</a>, <a href="http://arxiv.org/find/stat/1/au:+Celentano_M/0/1/0/all/0/1">Michael Celentano</a>, <a href="http://arxiv.org/find/stat/1/au:+Green_A/0/1/0/all/0/1">Alden Green</a>, <a href="http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1">Ryan J. Tibshirani</a></p>
<p>Maximum mean discrepancy (MMD) refers to a general class of nonparametric
two-sample tests that are based on maximizing the mean difference over samples
from one distribution $P$ versus another $Q$, over all choices of data
transformations $f$ living in some function space $\mathcal{F}$. Inspired by
recent work that connects what are known as functions of $\textit{Radon bounded
variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study
the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of
a given smoothness order $k \geq 0$. This test, which we refer to as the
$\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a
generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to
multiple dimensions and higher orders of smoothness. It is also intimately
connected to neural networks: we prove that the witness in the RKS test -- the
function $f$ achieving the maximum mean difference -- is always a ridge spline
of degree $k$, i.e., a single neuron in a neural network. This allows us to
leverage the power of modern deep learning toolkits to (approximately) optimize
the criterion that underlies the RKS test. We prove that the RKS test has
asymptotically full power at distinguishing any distinct pair $P \not= Q$ of
distributions, derive its asymptotic null distribution, and carry out extensive
experiments to elucidate the strengths and weakenesses of the RKS test versus
the more traditional kernel MMD test.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04037">SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks. (arXiv:2309.04037v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1">Sheng Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Sian Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1">Kai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xin Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zizhong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1">Franck Cappello</a></p>
<p>The fast growth of computational power and scales of modern super-computing
systems have raised great challenges for the management of exascale scientific
data. To maintain the usability of scientific data, error-bound lossy
compression is proposed and developed as an essential technique for the size
reduction of scientific data with constrained data distortion. Among the
diverse datasets generated by various scientific simulations, certain datasets
cannot be effectively compressed by existing error-bounded lossy compressors
with traditional techniques. The recent success of Artificial Intelligence has
inspired several researchers to integrate neural networks into error-bounded
lossy compressors. However, those works still suffer from limited compression
ratios and/or extremely low efficiencies. To address those issues and improve
the compression on the hard-to-compress datasets, in this paper, we propose
SRN-SZ, which is a deep learning-based scientific error-bounded lossy
compressor leveraging the hierarchical data grid expansion paradigm implemented
by super-resolution neural networks. SRN-SZ applies the most advanced
super-resolution network HAT for its compression, which is free of time-costing
per-data training. In experiments compared with various state-of-the-art
compressors, SRN-SZ achieves up to 75% compression ratio improvements under the
same error bound and up to 80% compression ratio improvements under the same
PSNR than the second-best compressor.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06782">Improved particle-flow event reconstruction with scalable neural networks for current and future particle detectors. (arXiv:2309.06782v3 [physics.data-an] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Pata_J/0/1/0/all/0/1">Joosep Pata</a>, <a href="http://arxiv.org/find/physics/1/au:+Wulff_E/0/1/0/all/0/1">Eric Wulff</a>, <a href="http://arxiv.org/find/physics/1/au:+Mokhtar_F/0/1/0/all/0/1">Farouk Mokhtar</a>, <a href="http://arxiv.org/find/physics/1/au:+Southwick_D/0/1/0/all/0/1">David Southwick</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhang_M/0/1/0/all/0/1">Mengke Zhang</a>, <a href="http://arxiv.org/find/physics/1/au:+Girone_M/0/1/0/all/0/1">Maria Girone</a>, <a href="http://arxiv.org/find/physics/1/au:+Duarte_J/0/1/0/all/0/1">Javier Duarte</a></p>
<p>We study scalable machine learning models for full event reconstruction in
high-energy electron-positron collisions based on a highly granular detector
simulation. Particle-flow reconstruction can be formulated as a supervised
learning task using tracks and calorimeter clusters or hits. We compare a graph
neural network and kernel-based transformer and demonstrate that both avoid
quadratic memory allocation and computational cost while achieving realistic
reconstruction. We show that hyperparameter tuning on a supercomputer
significantly enhances the physics performance of the models, improving the jet
transverse momentum resolution by up to 50% compared to the baseline. The
resulting model is highly portable across hardware processors. Finally, we
demonstrate that the model can be trained on highly granular inputs consisting
of tracks and calorimeter hits, resulting in a competitive physics performance
with the baseline. Datasets and software to reproduce the studies are published
following the findable, accessible, interoperable, and reusable principles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07289">User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bicer_Y/0/1/0/all/0/1">Yunus Bicer</a>, <a href="http://arxiv.org/find/cs/1/au:+Smedemark_Margulies_N/0/1/0/all/0/1">Niklas Smedemark-Margulies</a>, <a href="http://arxiv.org/find/cs/1/au:+Celik_B/0/1/0/all/0/1">Basak Celik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunger_E/0/1/0/all/0/1">Elifnur Sunger</a>, <a href="http://arxiv.org/find/cs/1/au:+Orendorff_R/0/1/0/all/0/1">Ryan Orendorff</a>, <a href="http://arxiv.org/find/cs/1/au:+Naufel_S/0/1/0/all/0/1">Stephanie Naufel</a>, <a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1">Tales Imbiriba</a>, <a href="http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1">Deniz Erdo&#x11f;mu&#x15f;</a>, <a href="http://arxiv.org/find/cs/1/au:+Tunik_E/0/1/0/all/0/1">Eugene Tunik</a>, <a href="http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1">Mathew Yarossi</a></p>
<p>We designed and tested a system for real-time control of a user interface by
extracting surface electromyographic (sEMG) activity from eight electrodes in a
wrist-band configuration. sEMG data were streamed into a machine-learning
algorithm that classified hand gestures in real-time. After an initial model
calibration, participants were presented with one of three types of feedback
during a human-learning stage: veridical feedback, in which predicted
probabilities from the gesture classification algorithm were displayed without
alteration, modified feedback, in which we applied a hidden augmentation of
error to these probabilities, and no feedback. User performance was then
evaluated in a series of minigames, in which subjects were required to use
eight gestures to manipulate their game avatar to complete a task. Experimental
results indicated that, relative to baseline, the modified feedback condition
led to significantly improved accuracy and improved gesture class separation.
These findings suggest that real-time feedback in a gamified user interface
with manipulation of feedback may enable intuitive, rapid, and accurate task
acquisition for sEMG-based gesture recognition applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00496">The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks. (arXiv:2310.00496v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shinn_C/0/1/0/all/0/1">Cameron Shinn</a>, <a href="http://arxiv.org/find/cs/1/au:+McCarthy_C/0/1/0/all/0/1">Collin McCarthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1">Saurav Muralidharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Osama_M/0/1/0/all/0/1">Muhammad Osama</a>, <a href="http://arxiv.org/find/cs/1/au:+Owens_J/0/1/0/all/0/1">John D. Owens</a></p>
<p>We introduce the Sparsity Roofline, a visual performance model for evaluating
sparsity in neural networks. The Sparsity Roofline jointly models network
accuracy, sparsity, and theoretical inference speedup. Our approach does not
require implementing and benchmarking optimized kernels, and the theoretical
speedup becomes equal to the actual speedup when the corresponding dense and
sparse kernels are well-optimized. We achieve this through a novel analytical
model for predicting sparse network performance, and validate the predicted
speedup using several real-world computer vision architectures pruned across a
range of sparsity patterns and degrees. We demonstrate the utility and
ease-of-use of our model through two case studies: (1) we show how machine
learning researchers can predict the performance of unimplemented or
unoptimized block-structured sparsity patterns, and (2) we show how hardware
designers can predict the performance implications of new sparsity patterns and
sparse data formats in hardware. In both scenarios, the Sparsity Roofline helps
performance experts identify sparsity regimes with the highest performance
potential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01258">MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device. (arXiv:2310.01258v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rozendaal_T/0/1/0/all/0/1">Ties van Rozendaal</a>, <a href="http://arxiv.org/find/eess/1/au:+Singhal_T/0/1/0/all/0/1">Tushar Singhal</a>, <a href="http://arxiv.org/find/eess/1/au:+Le_H/0/1/0/all/0/1">Hoang Le</a>, <a href="http://arxiv.org/find/eess/1/au:+Sautiere_G/0/1/0/all/0/1">Guillaume Sautiere</a>, <a href="http://arxiv.org/find/eess/1/au:+Said_A/0/1/0/all/0/1">Amir Said</a>, <a href="http://arxiv.org/find/eess/1/au:+Buska_K/0/1/0/all/0/1">Krishna Buska</a>, <a href="http://arxiv.org/find/eess/1/au:+Raha_A/0/1/0/all/0/1">Anjuman Raha</a>, <a href="http://arxiv.org/find/eess/1/au:+Kalatzis_D/0/1/0/all/0/1">Dimitris Kalatzis</a>, <a href="http://arxiv.org/find/eess/1/au:+Mehta_H/0/1/0/all/0/1">Hitarth Mehta</a>, <a href="http://arxiv.org/find/eess/1/au:+Mayer_F/0/1/0/all/0/1">Frank Mayer</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Liang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Nagel_M/0/1/0/all/0/1">Markus Nagel</a>, <a href="http://arxiv.org/find/eess/1/au:+Wiggers_A/0/1/0/all/0/1">Auke Wiggers</a></p>
<p>Neural video codecs have recently become competitive with standard codecs
such as HEVC in the low-delay setting. However, most neural codecs are large
floating-point networks that use pixel-dense warping operations for temporal
modeling, making them too computationally expensive for deployment on mobile
devices. Recent work has demonstrated that running a neural decoder in real
time on mobile is feasible, but shows this only for 720p RGB video. This work
presents the first neural video codec that decodes 1080p YUV420 video in real
time on a mobile device. Our codec relies on two major contributions. First, we
design an efficient codec that uses a block-based motion compensation algorithm
available on the warping core of the mobile accelerator, and we show how to
quantize this model to integer precision. Second, we implement a fast decoder
pipeline that concurrently runs neural network components on the neural signal
processor, parallel entropy coding on the mobile GPU, and warping on the
warping core. Our codec outperforms the previous on-device codec by a large
margin with up to 48% BD-rate savings, while reducing the MAC count on the
receiver side by $10 \times$. We perform a careful ablation to demonstrate the
effect of the introduced motion compensation scheme, and ablate the effect of
model quantization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04411">Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL. (arXiv:2310.04411v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yang Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1">Rui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1">Bingyi Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shiji Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a></p>
<p>The divergence of the Q-value estimation has been a prominent issue in
offline RL, where the agent has no access to real dynamics. Traditional beliefs
attribute this instability to querying out-of-distribution actions when
bootstrapping value targets. Though this issue can be alleviated with policy
constraints or conservative Q estimation, a theoretical understanding of the
underlying mechanism causing the divergence has been absent. In this work, we
aim to thoroughly comprehend this mechanism and attain an improved solution. We
first identify a fundamental pattern, self-excitation, as the primary cause of
Q-value estimation divergence in offline RL. Then, we propose a novel
Self-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel
(NTK) to measure the evolving property of Q-network at training, which provides
an intriguing explanation of the emergence of divergence. For the first time,
our theory can reliably decide whether the training will diverge at an early
stage, and even predict the order of the growth for the estimated Q-value, the
model's norm, and the crashing step when an SGD optimizer is used. The
experiments demonstrate perfect alignment with this theoretic analysis.
Building on our insights, we propose to resolve divergence from a novel
perspective, namely improving the model's architecture for better extrapolating
behavior. Through extensive empirical studies, we identify LayerNorm as a good
solution to effectively avoid divergence without introducing detrimental bias,
leading to superior performance. Experimental results prove that it can still
work in some most challenging settings, i.e. using only 1 transitions of the
dataset, where all previous methods fail. Moreover, it can be easily plugged
into modern offline RL methods and achieve SOTA results on many challenging
tasks. We also give unique insights into its effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05166">A Corrected Expected Improvement Acquisition Function Under Noisy Observations. (arXiv:2310.05166v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xingchen Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew B Blaschko</a></p>
<p>Sequential maximization of expected improvement (EI) is one of the most
widely used policies in Bayesian optimization because of its simplicity and
ability to handle noisy observations. In particular, the improvement function
often uses the best posterior mean as the best incumbent in noisy settings.
However, the uncertainty associated with the incumbent solution is often
neglected in many analytic EI-type methods: a closed-form acquisition function
is derived in the noise-free setting, but then applied to the setting with
noisy observations. To address this limitation, we propose a modification of EI
that corrects its closed-form expression by incorporating the covariance
information provided by the Gaussian Process (GP) model. This acquisition
function specializes to the classical noise-free result, and we argue should
replace that formula in Bayesian optimization software packages, tutorials, and
textbooks. This enhanced acquisition provides good generality for noisy and
noiseless settings. We show that our method achieves a sublinear convergence
rate on the cumulative regret bound under heteroscedastic observation noise.
Our empirical results demonstrate that our proposed acquisition function can
outperform EI in the presence of noisy observations on benchmark functions for
black-box optimization, as well as on parameter search for neural network model
compression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05309">Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods. (arXiv:2310.05309v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1">Constantine Caramanis</a>, <a href="http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1">Dimitris Fotakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1">Alkis Kalavasis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kontonis_V/0/1/0/all/0/1">Vasilis Kontonis</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1">Christos Tzamos</a></p>
<p>Deep Neural Networks and Reinforcement Learning methods have empirically
shown great promise in tackling challenging combinatorial problems. In those
methods a deep neural network is used as a solution generator which is then
trained by gradient-based methods (e.g., policy gradient) to successively
obtain better solution distributions. In this work we introduce a novel
theoretical framework for analyzing the effectiveness of such methods. We ask
whether there exist generative models that (i) are expressive enough to
generate approximately optimal solutions; (ii) have a tractable, i.e,
polynomial in the size of the input, number of parameters; (iii) their
optimization landscape is benign in the sense that it does not contain
sub-optimal stationary points. Our main contribution is a positive answer to
this question. Our result holds for a broad class of combinatorial problems
including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and
the Traveling Salesman Problem. As a byproduct of our analysis we introduce a
novel regularization process over vanilla gradient descent and provide
theoretical and experimental evidence that it helps address vanishing-gradient
issues and escape bad stationary points.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06827">Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1">Erik Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1">Hamid Palangi</a>, <a href="http://arxiv.org/find/cs/1/au:+Simoes_C/0/1/0/all/0/1">Clarisse Sim&#xf5;es</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1">Varun Chandrasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Subhabrata Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Arindam Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed Awadallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1">Ece Kamar</a></p>
<p>Large language models (LLMs) frequently hallucinate on abstractive
summarization tasks such as document-based question-answering, meeting
summarization, and clinical report generation, even though all necessary
information is included in context. However, optimizing LLMs to hallucinate
less on these tasks is challenging, as hallucination is hard to efficiently
evaluate at each optimization step. In this work, we show that reducing
hallucination on a synthetic task can also reduce hallucination on real-world
downstream tasks. Our method, SynTra, first designs a synthetic task where
hallucinations are easy to elicit and measure. It next optimizes the LLM's
system message via prefix-tuning on the synthetic task, and finally transfers
the system message to realistic, hard-to-optimize tasks. Across three realistic
abstractive summarization tasks, SynTra reduces hallucination for two
13B-parameter LLMs using only a synthetic retrieval task for supervision. We
also find that optimizing the system message rather than the model weights can
be critical; fine-tuning the entire model on the synthetic task can
counterintuitively increase hallucination. Overall, SynTra demonstrates that
the extra flexibility of working with synthetic data can help mitigate
undesired behaviors in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08812">A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model. (arXiv:2310.08812v3 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Gui_Z/0/1/0/all/0/1">Zhengtao Gui</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1">Haoyuan Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1">Sijie Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a></p>
<p>Time series forecasting represents a significant and challenging task across
various fields. Recently, methods based on mode decomposition have dominated
the forecasting of complex time series because of the advantages of capturing
local characteristics and extracting intrinsic modes from data. Unfortunately,
most models fail to capture the implied volatilities that contain significant
information. To enhance the forecasting of current, rapidly evolving, and
volatile time series, we propose a novel decomposition-ensemble paradigm, the
VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed
to decompose the time series into K sub-modes. Subsequently, the GARCH model
extracts the volatility information from these sub-modes, which serve as the
input for the LSTM. The numerical and volatility information of each sub-mode
is utilized to train a Long Short-Term Memory network. This network predicts
the sub-mode, and then we aggregate the predictions from all sub-modes to
produce the output. By integrating econometric and artificial intelligence
methods, and taking into account both the numerical and volatility information
of the time series, our proposed model demonstrates superior performance in
time series forecasting, as evidenced by the significant decrease in MSE, RMSE,
and MAPE in our comparative experimental results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12425">Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Md Rashedul Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1">Iftekhar Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagheri_H/0/1/0/all/0/1">Hamid Bagheri</a></p>
<p>The growing adoption of declarative software specification languages, coupled
with their inherent difficulty in debugging, has underscored the need for
effective and automated repair techniques applicable to such languages.
Researchers have recently explored various methods to automatically repair
declarative software specifications, such as template-based repair,
feedback-driven iterative repair, and bounded exhaustive approaches. The latest
developments in large language models provide new opportunities for the
automatic repair of declarative specifications. In this study, we assess the
effectiveness of utilizing OpenAI's ChatGPT to repair software specifications
written in the Alloy declarative language. Unlike imperative languages,
specifications in Alloy are not executed but rather translated into logical
formulas and evaluated using backend constraint solvers to identify
specification instances and counterexamples to assertions. Our evaluation
focuses on ChatGPT's ability to improve the correctness and completeness of
Alloy declarative specifications through automatic repairs. We analyze the
results produced by ChatGPT and compare them with those of leading automatic
Alloy repair methods. Our study revealed that while ChatGPT falls short in
comparison to existing techniques, it was able to successfully repair bugs that
no other technique could address. Our analysis also identified errors in
ChatGPT's generated repairs, including improper operator usage, type errors,
higher-order logic misuse, and relational arity mismatches. Additionally, we
observed instances of hallucinations in ChatGPT-generated repairs and
inconsistency in its results. Our study provides valuable insights for software
practitioners, researchers, and tool builders considering ChatGPT for
declarative specification repairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12567">Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jiaming Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Borong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiayi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xuehai Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Weidong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Ruiyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1">Yiran Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yifan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Juntao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15015">Leveraging Deep Learning for Abstractive Code Summarization of Unofficial Documentation. (arXiv:2310.15015v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naghshzan_A/0/1/0/all/0/1">AmirHossein Naghshzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerrouj_L/0/1/0/all/0/1">Latifa Guerrouj</a>, <a href="http://arxiv.org/find/cs/1/au:+Baysal_O/0/1/0/all/0/1">Olga Baysal</a></p>
<p>Usually, programming languages have official documentation to guide
developers with APIs, methods, and classes. However, researchers identified
insufficient or inadequate documentation examples and flaws with the API's
complex structure as barriers to learning an API. As a result, developers may
consult other sources (StackOverflow, GitHub, etc.) to learn more about an API.
Recent research studies have shown that unofficial documentation is a valuable
source of information for generating code summaries. We, therefore, have been
motivated to leverage such a type of documentation along with deep learning
techniques towards generating high-quality summaries for APIs discussed in
informal documentation. This paper proposes an automatic approach using the
BART algorithm, a state-of-the-art transformer model, to generate summaries for
APIs discussed in StackOverflow. We built an oracle of human-generated
summaries to evaluate our approach against it using ROUGE and BLEU metrics
which are the most widely used evaluation metrics in text summarization.
Furthermore, we evaluated our summaries empirically against a previous work in
terms of quality. Our findings demonstrate that using deep learning algorithms
can improve summaries' quality and outperform the previous work by an average
of %57 for Precision, %66 for Recall, and %61 for F-measure, and it runs 4.4
times faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17273">Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adachi_M/0/1/0/all/0/1">Masaki Adachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Planden_B/0/1/0/all/0/1">Brady Planden</a>, <a href="http://arxiv.org/find/cs/1/au:+Howey_D/0/1/0/all/0/1">David A. Howey</a>, <a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1">Krikamol Muandet</a>, <a href="http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1">Michael A. Osborne</a>, <a href="http://arxiv.org/find/cs/1/au:+Chau_S/0/1/0/all/0/1">Siu Lun Chau</a></p>
<p>Like many optimizers, Bayesian optimization often falls short of gaining user
trust due to opacity. While attempts have been made to develop human-centric
optimizers, they typically assume user knowledge is well-specified and
error-free, employing users mainly as supervisors of the optimization process.
We relax these assumptions and propose a more balanced human-AI partnership
with our Collaborative and Explainable Bayesian Optimization (CoExBO)
framework. Instead of explicitly requiring a user to provide a knowledge model,
CoExBO employs preference learning to seamlessly integrate human insights into
the optimization, resulting in algorithmic suggestions that resonate with user
preference. CoExBO explains its candidate selection every iteration to foster
trust, empowering users with a clearer grasp of the optimization. Furthermore,
CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with
extreme adversarial interventions, the algorithm converges asymptotically to a
vanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI
teaming experiments in lithium-ion battery design, highlighting substantial
improvements over conventional methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18725">The Evolution of the Interplay Between Input Distributions and Linear Regions in Networks. (arXiv:2310.18725v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yi Wei</a></p>
<p>It is commonly recognized that the expressiveness of deep neural networks is
contingent upon a range of factors, encompassing their depth, width, and other
relevant considerations. Currently, the practical performance of the majority
of deep neural networks remains uncertain. For ReLU (Rectified Linear Unit)
networks with piecewise linear activations, the number of linear convex regions
serves as a natural metric to gauge the network's expressivity. In this paper,
we count the number of linear convex regions in deep neural networks based on
ReLU. In particular, we prove that for any one-dimensional input, there exists
a minimum threshold for the number of neurons required to express it. We also
empirically observe that for the same network, intricate inputs hinder its
capacity to express linear regions. Furthermore, we unveil the iterative
refinement process of decision boundaries in ReLU networks during training. We
aspire for our research to serve as an inspiration for network optimization
endeavors and aids in the exploration and analysis of the behaviors exhibited
by deep networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19005">Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals. (arXiv:2310.19005v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Alizade_M/0/1/0/all/0/1">Mohamad H. Alizade</a>, <a href="http://arxiv.org/find/eess/1/au:+Einizade_A/0/1/0/all/0/1">Aref Einizade</a>, <a href="http://arxiv.org/find/eess/1/au:+Giraldo_J/0/1/0/all/0/1">Jhony H. Giraldo</a></p>
<p>Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is
concerned with the inference of the graph's underlying structure from nodal
observations. However, real-world data often contains diverse information,
necessitating the simultaneous clustering and learning of multiple graphs. In
practical applications, valuable node-specific covariates, represented as
kernels, have been underutilized by existing graph signal clustering methods.
In this letter, we propose a new framework, named Kernel-based joint Multiple
GL and clustering of graph signals (KMGL), that leverages a multi-convex
optimization approach. This allows us to integrate node-side information,
construct low-pass filters, and efficiently solve the optimization problem. The
experiments demonstrate that KMGL significantly enhances the robustness of GL
and clustering, particularly in scenarios with high noise levels and a
substantial number of clusters. These findings underscore the potential of KMGL
for improving the performance of GSP methods in diverse, real-world
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19102">Atom: Low-bit Quantization for Efficient and Accurate LLM Serving. (arXiv:2310.19102v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chien-Yu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Zihao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lequn Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Size Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ceze_L/0/1/0/all/0/1">Luis Ceze</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1">Arvind Krishnamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasikci_B/0/1/0/all/0/1">Baris Kasikci</a></p>
<p>The growing demand for Large Language Models (LLMs) in applications such as
content generation, intelligent chatbots, and sentiment analysis poses
considerable challenges for LLM service providers. To efficiently use GPU
resources and boost throughput, batching multiple requests has emerged as a
popular paradigm; to further speed up batching, LLM quantization techniques
reduce memory consumption and increase computing capacity. However, prevalent
quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully
leverage the capabilities of modern GPUs, such as 4-bit integer operators,
resulting in sub-optimal performance.
</p>
<p>To maximize LLMs' serving throughput, we introduce Atom, a low-bit
quantization method that achieves high throughput improvements with negligible
accuracy loss. Atom significantly boosts serving throughput by using low-bit
operators and considerably reduces memory consumption via low-bit quantization.
It attains high accuracy by applying a novel mixed-precision and fine-grained
quantization process. We evaluate Atom on 4-bit weight-activation quantization
setups in the serving context. Atom improves end-to-end throughput by up to
$7.73\times$ compared to the FP16 and by $2.53\times$ compared to INT8
quantization, while maintaining the same latency target.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19253">Flow-based distributionally robust optimization. (arXiv:2310.19253v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jonghyeok Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiuyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yao Xie</a></p>
<p>We present a computationally efficient framework, called FlowDRO, for solving
flow-based distributionally robust optimization (DRO) problems with Wasserstein
uncertainty sets while aiming to find continuous worst-case distribution (also
called the Least Favorable Distribution, LFD). The requirement for LFD to be
continuous is so that the algorithm can be scalable to problems with larger
sample sizes and achieve better generalization capability for the induced
robust algorithms. To tackle the computationally challenging infinitely
dimensional optimization problem, we leverage flow-based models and
continuous-time invertible transport maps between the data distribution and the
target distribution. We also develop a Wasserstein proximal gradient flow type
of algorithm. In theory, we establish the equivalence of the solution by
optimal transport map to the original formulation, as well as the dual form of
the problem through Wasserstein calculus and Brenier theorem. In practice, we
parameterize the transport maps by a sequence of neural networks progressively
trained in blocks by gradient descent. Our computational framework is general,
can handle high-dimensional data with large sample sizes, and can be useful for
various applications. We demonstrate its usage in adversarial learning,
distributionally robust hypothesis testing, and a new mechanism for data-driven
distribution perturbation differential privacy, where the proposed method gives
strong empirical performance on real high-dimensional data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19685">DGFN: Double Generative Flow Networks. (arXiv:2310.19685v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1">Elaine Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Vemgal_N/0/1/0/all/0/1">Nikhil Vemgal</a>, <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1">Emmanuel Bengio</a></p>
<p>Deep learning is emerging as an effective tool in drug discovery, with
potential applications in both predictive and generative models. Generative
Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for
the ability to generate diverse candidates, in particular in small molecule
generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing
inspiration from reinforcement learning and Double Deep Q-Learning, we
introduce a target network used to sample trajectories, while updating the main
network with these sampled trajectories. Empirical results confirm that DGFNs
effectively enhance exploration in sparse reward domains and high-dimensional
state spaces, both challenging aspects of de-novo design in drug discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19695">Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions. (arXiv:2310.19695v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alonso_Monsalve_S/0/1/0/all/0/1">Sa&#xfa;l Alonso-Monsalve</a>, <a href="http://arxiv.org/find/cs/1/au:+Sgalaberna_D/0/1/0/all/0/1">Davide Sgalaberna</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xingyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Molines_A/0/1/0/all/0/1">Adrien Molines</a>, <a href="http://arxiv.org/find/cs/1/au:+McGrew_C/0/1/0/all/0/1">Clark McGrew</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubbia_A/0/1/0/all/0/1">Andr&#xe9; Rubbia</a></p>
<p>Image decomposition plays a crucial role in various computer vision tasks,
enabling the analysis and manipulation of visual content at a fundamental
level. Overlapping images, which occur when multiple objects or scenes
partially occlude each other, pose unique challenges for decomposition
algorithms. The task intensifies when working with sparse images, where the
scarcity of meaningful information complicates the precise extraction of
components. This paper presents a solution that leverages the power of deep
learning to accurately extract individual objects within multi-dimensional
overlapping-sparse images, with a direct application in high-energy physics
with decomposition of overlaid elementary particles obtained from imaging
detectors. In particular, the proposed approach tackles a highly complex yet
unsolved problem: identifying and measuring independent particles at the vertex
of neutrino interactions, where one expects to observe detector images with
multiple indiscernible overlapping charged particles. By decomposing the image
of the detector activity at the vertex through deep learning, it is possible to
infer the kinematic parameters of the identified low-momentum particles - which
otherwise would remain neglected - and enhance the reconstructed energy
resolution of the neutrino event. We also present an additional step - that can
be tuned directly on detector data - combining the above method with a
fully-differentiable generative model to improve the image decomposition
further and, consequently, the resolution of the measured parameters, achieving
unprecedented results. This improvement is crucial for precisely measuring the
parameters that govern neutrino flavour oscillations and searching for
asymmetries between matter and antimatter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20060">AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces. (arXiv:2310.20060v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Mata_J/0/1/0/all/0/1">Jo&#xe3;o Victor Galv&#xe3;o da Mata</a>, <a href="http://arxiv.org/find/math/1/au:+Andersen_M/0/1/0/all/0/1">Martin S. Andersen</a></p>
<p>We introduce AdaSub, a stochastic optimization algorithm that computes a
search direction based on second-order information in a low-dimensional
subspace that is defined adaptively based on available current and past
information. Compared to first-order methods, second-order methods exhibit
better convergence characteristics, but the need to compute the Hessian matrix
at each iteration results in excessive computational expenses, making them
impractical. To address this issue, our approach enables the management of
computational expenses and algorithm efficiency by enabling the selection of
the subspace dimension for the search. Our code is freely available on GitHub,
and our preliminary numerical results demonstrate that AdaSub surpasses popular
stochastic optimizers in terms of time and number of iterations required to
reach a given accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20587">Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning. (arXiv:2310.20587v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1">Ruizhe Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuyao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ze_Y/0/1/0/all/0/1">Yanjie Ze</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon S. Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a></p>
<p>Offline reinforcement learning (RL) aims to find a near-optimal policy using
pre-collected datasets. In real-world scenarios, data collection could be
costly and risky; therefore, offline RL becomes particularly challenging when
the in-domain data is limited. Given recent advances in Large Language Models
(LLMs) and their few-shot learning prowess, this paper introduces
$\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a
general framework based on Decision Transformers to effectively use pre-trained
Language Models (LMs) for offline RL. Our framework highlights four crucial
components: (1) Initializing Decision Transformers with sequentially
pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to
full-weight fine-tuning, to combine the pre-trained knowledge from LMs and
in-domain knowledge effectively, (3) using the non-linear MLP transformation
instead of linear projections, to generate embeddings, and (4) integrating an
auxiliary language prediction loss during fine-tuning to stabilize the LMs and
retain their original abilities on languages. Empirical results indicate
$\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks
and closes the gap between value-based offline RL methods and decision
transformers in dense-reward tasks. In particular, our method demonstrates
superior performance in scenarios with limited data samples. Our project
website is $\href{https://lamo2023.github.io}{\text{this https URL}}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00136">Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Antoniades_A/0/1/0/all/0/1">Antonis Antoniades</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yu_Y/0/1/0/all/0/1">Yiyi Yu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Canzano_J/0/1/0/all/0/1">Joseph Canzano</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1">William Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Smith_S/0/1/0/all/0/1">Spencer LaVere Smith</a></p>
<p>State-of-the-art systems neuroscience experiments yield large-scale
multimodal data, and these data sets require new tools for analysis. Inspired
by the success of large pretrained models in vision and language domains, we
reframe the analysis of large-scale, cellular-resolution neuronal spiking data
into an autoregressive spatiotemporal generation problem. Neuroformer is a
multimodal, multitask generative pretrained transformer (GPT) model that is
specifically designed to handle the intricacies of data in systems
neuroscience. It scales linearly with feature size, can process an arbitrary
number of modalities, and is adaptable to downstream tasks, such as predicting
behavior. We first trained Neuroformer on simulated datasets, and found that it
both accurately predicted simulated neuronal circuit activity, and also
intrinsically inferred the underlying neural circuit connectivity, including
direction. When pretrained to decode neural responses, the model predicted the
behavior of a mouse with only few-shot fine-tuning, suggesting that the model
begins learning how to do so directly from the neural representations
themselves, without any explicit supervision. We used an ablation study to show
that joint training on neuronal responses and behavior boosted performance,
highlighting the model's ability to associate behavioral and neural
representations in an unsupervised manner. These findings show that Neuroformer
can analyze neural datasets and their emergent properties, informing the
development of models and hypotheses associated with the brain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00855">A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1">Dinesh Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Ankit Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalappa_C/0/1/0/all/0/1">Chaitra Gopalappa</a></p>
<p>Human immunodeficiency virus (HIV) is a major public health concern in the
United States, with about 1.2 million people living with HIV and 35,000 newly
infected each year. There are considerable geographical disparities in HIV
burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE)
initiative aims to reduce new infections by 90% by 2030, by improving coverage
of diagnoses, treatment, and prevention interventions and prioritizing
jurisdictions with high HIV prevalence. Identifying optimal scale-up of
intervention combinations will help inform resource allocation. Existing HIV
decision analytic models either evaluate specific cities or the overall
national population, thus overlooking jurisdictional interactions or
differences. In this paper, we propose a multi-agent reinforcement learning
(MARL) model, that enables jurisdiction-specific decision analyses but in an
environment with cross-jurisdictional epidemiological interactions. In
experimental analyses, conducted on jurisdictions within California and
Florida, optimal policies from MARL were significantly different than those
generated from single-agent RL, highlighting the influence of jurisdictional
variations and interactions. By using comprehensive modeling of HIV and
formulations of state space, action space, and reward functions, this work
helps demonstrate the strengths and applicability of MARL for informing public
health policies, and provides a framework for expanding to the national-level
to inform the EHE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00975">Autonomous Learning of Generative Models with Chemical Reaction Network Ensembles. (arXiv:2311.00975v2 [q-bio.MN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Poole_W/0/1/0/all/0/1">William Poole</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ouldridge_T/0/1/0/all/0/1">Thomas E. Ouldridge</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gopalkrishnan_M/0/1/0/all/0/1">Manoj Gopalkrishnan</a></p>
<p>Can a micron sized sack of interacting molecules autonomously learn an
internal model of a complex and fluctuating environment? We draw insights from
control theory, machine learning theory, chemical reaction network theory, and
statistical physics to develop a general architecture whereby a broad class of
chemical systems can autonomously learn complex distributions. Our construction
takes the form of a chemical implementation of machine learning's optimization
workhorse: gradient descent on the relative entropy cost function. We show how
this method can be applied to optimize any detailed balanced chemical reaction
network and that the construction is capable of using hidden units to learn
complex distributions. This result is then recast as a form of integral
feedback control. Finally, due to our use of an explicit physical model of
learning, we are able to derive thermodynamic costs and trade-offs associated
to this process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01305">AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Baisong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haixiao Xu</a></p>
<p>Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01442">Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models. (arXiv:2311.01442v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Assandri_V/0/1/0/all/0/1">Valentino Assandri</a>, <a href="http://arxiv.org/find/cs/1/au:+Heshmati_S/0/1/0/all/0/1">Sam Heshmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaman_B/0/1/0/all/0/1">Burhaneddin Yaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Iakovlev_A/0/1/0/all/0/1">Anton Iakovlev</a>, <a href="http://arxiv.org/find/cs/1/au:+Repetur_A/0/1/0/all/0/1">Ariel Emiliano Repetur</a></p>
<p>Deep learning models, particularly Transformers, have achieved impressive
results in various domains, including time series forecasting. While existing
time series literature primarily focuses on model architecture modifications
and data augmentation techniques, this paper explores the training schema of
deep learning models for time series; how models are trained regardless of
their architecture. We perform extensive experiments to investigate the
occurrence of deep double descent in several Transformer models trained on
public time series data sets. We demonstrate epoch-wise deep double descent and
that overfitting can be reverted using more epochs. Leveraging these findings,
we achieve state-of-the-art results for long sequence time series forecasting
in nearly 70% of the 72 benchmarks tested. This suggests that many models in
the literature may possess untapped potential. Additionally, we introduce a
taxonomy for classifying training schema modifications, covering data
augmentation, model inputs, model targets, time series per model, and
computational budget.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01479">Detecting Out-of-Distribution Through the Lens of Neural Collapse. (arXiv:2311.01479v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Litian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yao Qin</a></p>
<p>Out-of-distribution (OOD) detection is essential for the safe deployment of
AI. Particularly, OOD detectors should generalize effectively across diverse
scenarios. To improve upon the generalizability of existing OOD detectors, we
introduce a highly versatile OOD detector, called Neural Collapse inspired OOD
detector (NC-OOD). We extend the prevalent observation that in-distribution
(ID) features tend to form clusters, whereas OOD features are far away.
Particularly, based on the recent observation, Neural Collapse, we further
demonstrate that ID features tend to cluster in proximity to weight vectors.
From our extended observation, we propose to detect OOD based on feature
proximity to weight vectors. To further rule out OOD samples, we leverage the
observation that OOD features tend to reside closer to the origin than ID
features. Extensive experiments show that our approach enhances the
generalizability of existing work and can consistently achieve state-of-the-art
OOD detection performance across a wide range of OOD Benchmarks over different
classification tasks, training losses, and model architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01522">An Efficient Detection and Control System for Underwater Docking using Machine Learning and Realistic Simulation: A Comprehensive Approach. (arXiv:2311.01522v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chavez_Galaviz_J/0/1/0/all/0/1">Jalil Chavez-Galaviz</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergman_M/0/1/0/all/0/1">Matthew Bergman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mengdibayev_M/0/1/0/all/0/1">Miras Mengdibayev</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmoudian_N/0/1/0/all/0/1">Nina Mahmoudian</a></p>
<p>Underwater docking is critical to enable the persistent operation of
Autonomous Underwater Vehicles (AUVs). For this, the AUV must be capable of
detecting and localizing the docking station, which is complex due to the
highly dynamic undersea environment. Image-based solutions offer a high
acquisition rate and versatile alternative to adapt to this environment;
however, the underwater environment presents challenges such as low visibility,
high turbidity, and distortion. In addition to this, field experiments to
validate underwater docking capabilities can be costly and dangerous due to the
specialized equipment and safety considerations required to conduct the
experiments. This work compares different deep-learning architectures to
perform underwater docking detection and classification. The architecture with
the best performance is then compressed using knowledge distillation under the
teacher-student paradigm to reduce the network's memory footprint, allowing
real-time implementation. To reduce the simulation-to-reality gap, a Generative
Adversarial Network (GAN) is used to do image-to-image translation, converting
the Gazebo simulation image into a realistic underwater-looking image. The
obtained image is then processed using an underwater image formation model to
simulate image attenuation over distance under different water types. The
proposed method is finally evaluated according to the AUV docking success rate
and compared with classical vision methods. The simulation results show an
improvement of 20% in the high turbidity scenarios regardless of the underwater
currents. Furthermore, we show the performance of the proposed approach by
showing experimental results on the off-the-shelf AUV Iver3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02117">Cooperative Network Learning for Large-Scale and Decentralized Graphs. (arXiv:2311.02117v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiming Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yujie Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yijie Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Fang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1">Linyuan L&#xfc;</a></p>
<p>Graph research, the systematic study of interconnected data points
represented as graphs, plays a vital role in capturing intricate relationships
within networked systems. However, in the real world, as graphs scale up,
concerns about data security among different data-owning agencies arise,
hindering information sharing and, ultimately, the utilization of graph data.
Therefore, establishing a mutual trust mechanism among graph agencies is
crucial for unlocking the full potential of graphs. Here, we introduce a
Cooperative Network Learning (CNL) framework to ensure secure graph computing
for various graph tasks. Essentially, this CNL framework unifies the local and
global perspectives of GNN computing with distributed data for an agency by
virtually connecting all participating agencies as a global graph without a
fixed central coordinator. Inter-agency computing is protected by various
technologies inherent in our framework, including homomorphic encryption and
secure transmission. Moreover, each agency has a fair right to design or employ
various graph learning models from its local or global perspective. Thus, CNL
can collaboratively train GNN models based on decentralized graphs inferred
from local and global graphs. Experiments on contagion dynamics prediction and
traditional graph tasks (i.e., node classification and link prediction)
demonstrate that our CNL architecture outperforms state-of-the-art GNNs
developed at individual sites, revealing that CNL can provide a reliable, fair,
secure, privacy-preserving, and global perspective to build effective and
personalized models for network applications. We hope this framework will
address privacy concerns in graph-related research and integrate decentralized
graph data structures to benefit the network research community in cooperation
and innovation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02171">Emergence of Abstract State Representations in Embodied Sequence Modeling. (arXiv:2311.02171v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1">Tian Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zilai Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Handa_K/0/1/0/all/0/1">Kunal Handa</a>, <a href="http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1">Ashish V. Thapliyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1">Bo Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1">Ellie Pavlick</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a></p>
<p>Decision making via sequence modeling aims to mimic the success of language
models, where actions taken by an embodied agent are modeled as tokens to
predict. Despite their promising performance, it remains unclear if embodied
sequence modeling leads to the emergence of internal representations that
represent the environmental state information. A model that lacks abstract
state representations would be liable to make decisions based on surface
statistics which fail to generalize. We take the BabyAI environment, a grid
world in which language-conditioned navigation tasks are performed, and build a
sequence modeling Transformer, which takes a language instruction, a sequence
of actions, and environmental observations as its inputs. In order to
investigate the emergence of abstract state representations, we design a
"blindfolded" navigation task, where only the initial environmental layout, the
language instruction, and the action sequence to complete the task are
available for training. Our probing results show that intermediate
environmental layouts can be reasonably reconstructed from the internal
activations of a trained model, and that language instructions play a role in
the reconstruction accuracy. Our results suggest that many key features of
state representations can emerge via embodied sequence modeling, supporting an
optimistic outlook for applications of sequence modeling objectives to more
complex embodied decision-making domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03285">S-LoRA: Serving Thousands of Concurrent LoRA Adapters. (arXiv:2311.03285v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1">Ying Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shiyi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dacheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1">Coleman Hooper</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Nicholas Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1">Christopher Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lianmin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1">Kurt Keutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a></p>
<p>The "pretrain-then-finetune" paradigm is commonly adopted in the deployment
of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient
fine-tuning method, is often employed to adapt a base model to a multitude of
tasks, resulting in a substantial collection of LoRA adapters derived from one
base model. We observe that this paradigm presents significant opportunities
for batched inference during serving. To capitalize on these opportunities, we
present S-LoRA, a system designed for the scalable serving of many LoRA
adapters. S-LoRA stores all adapters in the main memory and fetches the
adapters used by the currently running queries to the GPU memory. To
efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes
Unified Paging. Unified Paging uses a unified memory pool to manage dynamic
adapter weights with different ranks and KV cache tensors with varying sequence
lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and
highly optimized custom CUDA kernels for heterogeneous batching of LoRA
computation. Collectively, these features enable S-LoRA to serve thousands of
LoRA adapters on a single GPU or across multiple GPUs with a small overhead.
Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with
naive support of LoRA serving), S-LoRA can improve the throughput by up to 4
times and increase the number of served adapters by several orders of
magnitude. As a result, S-LoRA enables scalable serving of many task-specific
fine-tuned models and offers the potential for large-scale customized
fine-tuning services. The code is available at https://github.com/S-LoRA/S-LoRA
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03287">Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges. (arXiv:2311.03287v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Chenhang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yiyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shirley Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Linjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a></p>
<p>While GPT-4V(ision) impressively models both visual and textual information
simultaneously, it's hallucination behavior has not been systematically
assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias
and Interference Challenges in Visual Language Models (Bingo). This benchmark
is designed to evaluate and shed light on the two common types of
hallucinations in visual language models: bias and interference. Here, bias
refers to the model's tendency to hallucinate certain types of responses,
possibly due to imbalance in its training data. Interference pertains to
scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the
text prompt is phrased or how the input image is presented. We identify a
notable regional bias, whereby GPT-4V(ision) is better at interpreting Western
images or images with English writing compared to images from other countries
or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to
leading questions and is often confused when interpreting multiple images
together. Popular mitigation approaches, such as self-correction and
chain-of-thought reasoning, are not effective in resolving these challenges. We
also identified similar biases and interference vulnerabilities with LLaVA and
Bard. Our results characterize the hallucination challenges in GPT-4V(ision)
and state-of-the-art visual-language models, and highlight the need for new
solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.
</p>
</p>
</div>

    </div>
    </body>
    