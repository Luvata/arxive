<!DOCTYPE html>
<html>
<head>
<title>2023-11-24-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.12799">A Fine-Grained Image Description Generation Method Based on Joint Objectives. (arXiv:2311.12799v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chunzhen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1">Donglin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dazhen Lin</a></p>
<p>The goal of fine-grained image description generation techniques is to learn
detailed information from images and simulate human-like descriptions that
provide coherent and comprehensive textual details about the image content.
Currently, most of these methods face two main challenges: description
repetition and omission. Moreover, the existing evaluation metrics cannot
clearly reflect the performance of models on these two issues. To address these
challenges, we propose an innovative Fine-grained Image Description Generation
model based on Joint Objectives. Furthermore, we introduce new object-based
evaluation metrics to more intuitively assess the model's performance in
handling description repetition and omission. This novel approach combines
visual features at both the image level and object level to maximize their
advantages and incorporates an object penalty mechanism to reduce description
repetition. Experimental results demonstrate that our proposed method
significantly improves the CIDEr evaluation metric, indicating its excellent
performance in addressing description repetition and omission issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12800">Understanding Data Augmentation from a Robustness Perspective. (arXiv:2311.12800v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhendong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qiangqiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chongjun Wang</a></p>
<p>In the realm of visual recognition, data augmentation stands out as a pivotal
technique to amplify model robustness. Yet, a considerable number of existing
methodologies lean heavily on heuristic foundations, rendering their intrinsic
mechanisms ambiguous. This manuscript takes both a theoretical and empirical
approach to understanding the phenomenon. Theoretically, we frame the discourse
around data augmentation within game theory's constructs. Venturing deeper, our
empirical evaluations dissect the intricate mechanisms of emblematic data
augmentation strategies, illuminating that these techniques primarily stimulate
mid- and high-order game interactions. Beyond the foundational exploration, our
experiments span multiple datasets and diverse augmentation techniques,
underscoring the universal applicability of our findings. Recognizing the vast
array of robustness metrics with intricate correlations, we unveil a
streamlined proxy. This proxy not only simplifies robustness assessment but
also offers invaluable insights, shedding light on the inherent dynamics of
model game interactions and their relation to overarching system robustness.
These insights provide a novel lens through which we can re-evaluate model
safety and robustness in visual recognition tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12801">End-to-end Phase Field Model Discovery Combining Experimentation, Crowdsourcing, Simulation and Learning. (arXiv:2311.12801v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1">Md Nasim</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Azab_A/0/1/0/all/0/1">Anter El-Azab</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinghang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yexiang Xue</a></p>
<p>The availability of tera-byte scale experiment data calls for AI driven
approaches which automatically discover scientific models from data.
Nonetheless, significant challenges present in AI-driven scientific discovery:
(i) The annotation of large scale datasets requires fundamental re-thinking in
developing scalable crowdsourcing tools. (ii) The learning of scientific models
from data calls for innovations beyond black-box neural nets. (iii) Novel
visualization and diagnosis tools are needed for the collaboration of
experimental and theoretical physicists, and computer scientists. We present
Phase-Field-Lab platform for end-to-end phase field model discovery, which
automatically discovers phase field physics models from experiment data,
integrating experimentation, crowdsourcing, simulation and learning.
Phase-Field-Lab combines (i) a streamlined annotation tool which reduces the
annotation time (by ~50-75%), while increasing annotation accuracy compared to
baseline; (ii) an end-to-end neural model which automatically learns phase
field models from data by embedding phase field simulation and existing domain
knowledge into learning; and (iii) novel interfaces and visualizations to
integrate our platform into the scientific discovery cycle of domain
scientists. Our platform is deployed in the analysis of nano-structure
evolution in materials under extreme conditions (high temperature and
irradiation). Our approach reveals new properties of nano-void defects, which
otherwise cannot be detected via manual analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12802">A general Framework for Utilizing Metaheuristic Optimization for Sustainable Unrelated Parallel Machine Scheduling: A concise overview. (arXiv:2311.12802v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ezugwu_A/0/1/0/all/0/1">Absalom E. Ezugwu</a></p>
<p>Sustainable development has emerged as a global priority, and industries are
increasingly striving to align their operations with sustainable practices.
Parallel machine scheduling (PMS) is a critical aspect of production planning
that directly impacts resource utilization and operational efficiency. In this
paper, we investigate the application of metaheuristic optimization algorithms
to address the unrelated parallel machine scheduling problem (UPMSP) through
the lens of sustainable development goals (SDGs). The primary objective of this
study is to explore how metaheuristic optimization algorithms can contribute to
achieving sustainable development goals in the context of UPMSP. We examine a
range of metaheuristic algorithms, including genetic algorithms, particle swarm
optimization, ant colony optimization, and more, and assess their effectiveness
in optimizing the scheduling problem. The algorithms are evaluated based on
their ability to improve resource utilization, minimize energy consumption,
reduce environmental impact, and promote socially responsible production
practices. To conduct a comprehensive analysis, we consider UPMSP instances
that incorporate sustainability-related constraints and objectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12803">Investigating Copyright Issues of Diffusion Models under Practical Scenarios. (arXiv:2311.12803v1 [cs.MM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzun_T/0/1/0/all/0/1">Teoh Tze Tzun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hern_L/0/1/0/all/0/1">Lim Wei Hern</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>The issue of copyright in generative models, particularly diffusion models,
has become a prominent concern in recent years. Previous studies have
predominantly focused on copyright violation at the image level, where
generative models replicate copyrighted images entirely. Furthermore, these
earlier studies have examined copyright infringements mainly using prompts that
are semantically similar to target topics. However, copyright infringement can
be more nuanced than mere replication of whole images and can be triggered with
prompts that are less directly related to copyright topics. In our work, we
tackle the limitations of previous studies by delving into partial copyright
infringement, which treats parts of images as copyrighted content, using
prompts that are considerably different from copyrighted topics. We develop a
data generation pipeline that facilitates the creation of datasets for
copyright research in diffusion models. Using our pipeline, we create datasets
containing copyright infringement samples for different diffusion models. We
conduct evaluations on generated data under various criteria. Our results show
the prevalence of generating copyright-infringing content across a range of
diffusion models, including the latest Stable Diffusion XL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12805">DeepCompass: AI-driven Location-Orientation Synchronization for Navigating Platforms. (arXiv:2311.12805v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">SP Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1">Bumsoo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Seok_H/0/1/0/all/0/1">Hyekyoung Seok</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1">Hyoungseok Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1">Sanghee Jung</a></p>
<p>In current navigating platforms, the user's orientation is typically
estimated based on the difference between two consecutive locations. In other
words, the orientation cannot be identified until the second location is taken.
This asynchronous location-orientation identification often leads to our
real-life question: Why does my navigator tell the wrong direction of my car at
the beginning? We propose DeepCompass to identify the user's orientation by
bridging the gap between the street-view and the user-view images. First, we
explore suitable model architectures and design corresponding input
configuration. Second, we demonstrate artificial transformation techniques
(e.g., style transfer and road segmentation) to minimize the disparity between
the street-view and the user's real-time experience. We evaluate DeepCompass
with extensive evaluation in various driving conditions. DeepCompass does not
require additional hardware and is also not susceptible to external
interference, in contrast to magnetometer-based navigator. This highlights the
potential of DeepCompass as an add-on to existing sensor-based orientation
detection methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12812">Personalization of Affective Models to Enable Neuropsychiatric Digital Precision Health Interventions: A Feasibility Study. (arXiv:2311.12812v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kargarandehkordi_A/0/1/0/all/0/1">Ali Kargarandehkordi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaisti_M/0/1/0/all/0/1">Matti Kaisti</a>, <a href="http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1">Peter Washington</a></p>
<p>Mobile digital therapeutics for autism spectrum disorder (ASD) often target
emotion recognition and evocation, which is a challenge for children with ASD.
While such mobile applications often use computer vision machine learning (ML)
models to guide the adaptive nature of the digital intervention, a single model
is usually deployed and applied to all children. Here, we explore the potential
of model personalization, or training a single emotion recognition model per
person, to improve the performance of these underlying emotion recognition
models used to guide digital health therapies for children with ASD. We
conducted experiments on the Emognition dataset, a video dataset of human
subjects evoking a series of emotions. For a subset of 10 individuals in the
dataset with a sufficient representation of at least two ground truth emotion
labels, we trained a personalized version of three classical ML models on a set
of 51 features extracted from each video frame. We measured the importance of
each facial feature for all personalized models and observed differing ranked
lists of top features across subjects, motivating the need for model
personalization. We then compared the personalized models against a generalized
model trained using data from all 10 participants. The mean F1-scores achieved
by the personalized models were 90.48%, 92.66%, and 86.40%, respectively. By
contrast, the mean F1-scores reached by non-personalized models trained on
different human subjects and evaluated using the same test set were 88.55%,
91.78%, and 80.42%, respectively. The personalized models outperformed the
generalized models for 7 out of 10 participants. PCA analyses on the remaining
3 participants revealed relatively facial configuration differences between
emotion labels within each subject, suggesting that personalized ML will fail
when the variation among data points within a subjects data is too low.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12814">HydraScreen: A Generalizable Structure-Based Deep Learning Approach to Drug Discovery. (arXiv:2311.12814v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Prat_A/0/1/0/all/0/1">Alvaro Prat</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Aty_H/0/1/0/all/0/1">Hisham Abdel Aty</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kamuntavicius_G/0/1/0/all/0/1">Gintautas Kamuntavi&#x10d;ius</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Paquet_T/0/1/0/all/0/1">Tanya Paquet</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Norvaisas_P/0/1/0/all/0/1">Povilas Norvai&#x161;as</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gasparotto_P/0/1/0/all/0/1">Piero Gasparotto</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tal_R/0/1/0/all/0/1">Roy Tal</a></p>
<p>We propose HydraScreen, a deep-learning approach that aims to provide a
framework for more robust machine-learning-accelerated drug discovery.
HydraScreen utilizes a state-of-the-art 3D convolutional neural network,
designed for the effective representation of molecular structures and
interactions in protein-ligand binding. We design an end-to-end pipeline for
high-throughput screening and lead optimization, targeting applications in
structure-based drug design. We assess our approach using established public
benchmarks based on the CASF 2016 core set, achieving top-tier results in
affinity and pose prediction (Pearson's r = 0.86, RMSE = 1.15, Top-1 = 0.95).
Furthermore, we utilize a novel interaction profiling approach to identify
potential biases in the model and dataset to boost interpretability and support
the unbiased nature of our method. Finally, we showcase HydraScreen's capacity
to generalize across unseen proteins and ligands, offering directions for
future development of robust machine learning scoring functions. HydraScreen
(accessible at https://hydrascreen.ro5.ai) provides a user-friendly GUI and a
public API, facilitating easy assessment of individual protein-ligand
complexes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12815">Proposing an intelligent mesh smoothing method with graph neural networks. (arXiv:2311.12815v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhichao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinhai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junjun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jie Liu</a></p>
<p>In CFD, mesh smoothing methods are commonly utilized to refine the mesh
quality to achieve high-precision numerical simulations. Specifically,
optimization-based smoothing is used for high-quality mesh smoothing, but it
incurs significant computational overhead. Pioneer works improve its smoothing
efficiency by adopting supervised learning to learn smoothing methods from
high-quality meshes. However, they pose difficulty in smoothing the mesh nodes
with varying degrees and also need data augmentation to address the node input
sequence problem. Additionally, the required labeled high-quality meshes
further limit the applicability of the proposed method. In this paper, we
present GMSNet, a lightweight neural network model for intelligent mesh
smoothing. GMSNet adopts graph neural networks to extract features of the
node's neighbors and output the optimal node position. During smoothing, we
also introduce a fault-tolerance mechanism to prevent GMSNet from generating
negative volume elements. With a lightweight model, GMSNet can effectively
smoothing mesh nodes with varying degrees and remain unaffected by the order of
input data. A novel loss function, MetricLoss, is also developed to eliminate
the need for high-quality meshes, which provides a stable and rapid convergence
during training. We compare GMSNet with commonly used mesh smoothing methods on
two-dimensional triangle meshes. The experimental results show that GMSNet
achieves outstanding mesh smoothing performances with 5% model parameters of
the previous model, and attains 8.62 times faster than optimization-based
smoothing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12817">Semantic Face Compression for Metaverse: A Compact 3D Descriptor Based Approach. (arXiv:2311.12817v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Binzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bolin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yan Ye</a></p>
<p>In this letter, we envision a new metaverse communication paradigm for
virtual avatar faces, and develop the semantic face compression with compact 3D
facial descriptors. The fundamental principle is that the communication of
virtual avatar faces primarily emphasizes the conveyance of semantic
information. In light of this, the proposed scheme offers the advantages of
being highly flexible, efficient and semantically meaningful. The semantic face
compression, which allows the communication of the descriptors for artificial
intelligence based understanding, could facilitate numerous applications
without the involvement of humans in metaverse. The promise of the proposed
paradigm is also demonstrated by performance comparisons with the
state-of-the-art video coding standard, Versatile Video Coding. A significant
improvement in terms of rate-accuracy performance has been achieved. The
proposed scheme is expected to enable numerous applications, such as digital
human communication based on machine analysis, and to form the cornerstone of
interaction and communication in the metaverse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12819">Fixing the problems of deep neural networks will require better training data and learning algorithms. (arXiv:2311.12819v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1">Drew Linsley</a>, <a href="http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1">Thomas Serre</a></p>
<p>Bowers and colleagues argue that DNNs are poor models of biological vision
because they often learn to rival human accuracy by relying on strategies that
differ markedly from those of humans. We show that this problem is worsening as
DNNs are becoming larger-scale and increasingly more accurate, and prescribe
methods for building DNNs that can reliably model biological vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12820">MSG-BART: Multi-granularity Scene Graph-Enhanced Encoder-Decoder Language Model for Video-grounded Dialogue Generation. (arXiv:2311.12820v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongcheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pingjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a></p>
<p>Generating dialogue grounded in videos requires a high level of understanding
and reasoning about the visual scenes in the videos. However, existing large
visual-language models are not effective due to their latent features and
decoder-only structure, especially with respect to spatio-temporal relationship
reasoning. In this paper, we propose a novel approach named MSG-BART, which
enhances the integration of video information by incorporating a
multi-granularity spatio-temporal scene graph into an encoder-decoder
pre-trained language model. Specifically, we integrate the global and local
scene graph into the encoder and decoder, respectively, to improve both overall
perception and target reasoning capability. To further improve the information
selection capability, we propose a multi-pointer network to facilitate
selection between text and video. Extensive experiments are conducted on three
video-grounded dialogue benchmarks, which show the significant superiority of
the proposed MSG-BART compared to a range of state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12823">EWasteNet: A Two-Stream Data Efficient Image Transformer Approach for E-Waste Classification. (arXiv:2311.12823v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_N/0/1/0/all/0/1">Niful Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jony_M/0/1/0/all/0/1">Md. Mehedi Hasan Jony</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_E/0/1/0/all/0/1">Emam Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutradhar_S/0/1/0/all/0/1">Sunny Sutradhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1">Atikur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md. Motaharul Islam</a></p>
<p>Improper disposal of e-waste poses global environmental and health risks,
raising serious concerns. The accurate classification of e-waste images is
critical for efficient management and recycling. In this paper, we have
presented a comprehensive dataset comprised of eight different classes of
images of electronic devices named the E-Waste Vision Dataset. We have also
presented EWasteNet, a novel two-stream approach for precise e-waste image
classification based on a data-efficient image transformer (DeiT). The first
stream of EWasteNet passes through a sobel operator that detects the edges
while the second stream is directed through an Atrous Spatial Pyramid Pooling
and attention block where multi-scale contextual information is captured. We
train both of the streams simultaneously and their features are merged at the
decision level. The DeiT is used as the backbone of both streams. Extensive
analysis of the e-waste dataset indicates the usefulness of our method,
providing 96% accuracy in e-waste classification. The proposed approach
demonstrates significant usefulness in addressing the global concern of e-waste
management. It facilitates efficient waste management and recycling by
accurately classifying e-waste images, reducing health and safety hazards
associated with improper disposal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12824">Comparative Analysis of Shear Strength Prediction Models for Reinforced Concrete Slab-Column Connections. (arXiv:2311.12824v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wahab_S/0/1/0/all/0/1">Sarmed Wahab</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmoudabadi_N/0/1/0/all/0/1">Nasim Shakouri Mahmoudabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Waqas_S/0/1/0/all/0/1">Sarmad Waqas</a>, <a href="http://arxiv.org/find/cs/1/au:+Herl_N/0/1/0/all/0/1">Nouman Herl</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_M/0/1/0/all/0/1">Muhammad Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_K/0/1/0/all/0/1">Khurshid Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1">Afaq Ahmad</a></p>
<p>This research aims at comparative analysis of shear strength prediction at
slab-column connection, unifying machine learning, design codes and Finite
Element Analysis. Current design codes (CDCs) of ACI 318-19 (ACI), Eurocode 2
(EC2), Compressive Force Path (CFP) method, Feed Forward Neural Network (FNN)
based Artificial Neural Network (ANN), PSO-based FNN (PSOFNN), and BAT
algorithm-based BATFNN are used. The study is complemented with FEA of slab for
validating the experimental results and machine learning predictions.In the
case of hybrid models of PSOFNN and BATFNN, mean square error is used as an
objective function to obtain the optimized values of the weights, that are used
by Feed Forward Neural Network to perform predictions on the slab data. Seven
different models of PSOFNN, BATFNN, and FNN are trained on this data and the
results exhibited that PSOFNN is the best model overall. PSOFNN has the best
results for SCS=1 with highest value of R as 99.37% and lowest of MSE, and MAE
values of 0.0275%, and 1.214% respectively which are better than the best FNN
model for SCS=4 having the values of R, MSE, and MAE as 97.464%, 0.0492%, and
1.43%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12825">A PSO Based Method to Generate Actionable Counterfactuals for High Dimensional Data. (arXiv:2311.12825v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1">Shashank Shekhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Salim_A/0/1/0/all/0/1">Asif Salim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansode_A/0/1/0/all/0/1">Adesh Bansode</a>, <a href="http://arxiv.org/find/cs/1/au:+Jinturkar_V/0/1/0/all/0/1">Vivaswan Jinturkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1">Anirudha Nayak</a></p>
<p>Counterfactual explanations (CFE) are methods that explain a machine learning
model by giving an alternate class prediction of a data point with some minimal
changes in its features. It helps the users to identify their data attributes
that caused an undesirable prediction like a loan or credit card rejection. We
describe an efficient and an actionable counterfactual (CF) generation method
based on particle swarm optimization (PSO). We propose a simple objective
function for the optimization of the instance-centric CF generation problem.
The PSO brings in a lot of flexibility in terms of carrying out multi-objective
optimization in large dimensions, capability for multiple CF generation, and
setting box constraints or immutability of data attributes. An algorithm is
proposed that incorporates these features and it enables greater control over
the proximity and sparsity properties over the generated CFs. The proposed
algorithm is evaluated with a set of action-ability metrics in real-world
datasets, and the results were superior compared to that of the
state-of-the-arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12826">LiveChat: Video Comment Generation from Audio-Visual Multimodal Contexts. (arXiv:2311.12826v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lalanne_J/0/1/0/all/0/1">Julien Lalanne</a>, <a href="http://arxiv.org/find/cs/1/au:+Bournet_R/0/1/0/all/0/1">Raphael Bournet</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yi Yu</a></p>
<p>Live commenting on video, a popular feature of live streaming platforms,
enables viewers to engage with the content and share their comments, reactions,
opinions, or questions with the streamer or other viewers while watching the
video or live stream. It presents a challenging testbed for AI agents, which
involves the simultaneous understanding of audio-visual multimodal contexts
from live streams and the ability to interact with human viewers through
dialogue. As existing live streaming-based comments datasets contain limited
categories and lack a diversity, we create a large-scale audio-visual
multimodal dialogue dataset to facilitate the development of live commenting
technologies. The data is collected from Twitch, with 11 different categories
and 575 streamers for a total of 438 hours of video and 3.2 million comments.
Moreover, we propose a novel multimodal generation model capable of generating
live comments that align with the temporal and spatial events within the video,
as well as with the ongoing multimodal dialogue context. Our initial results
have demonstrated the effectiveness of the proposed model, providing a robust
foundation for further research and practical applications in the field of live
video interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12829">Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile. (arXiv:2311.12829v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tashakori_A/0/1/0/all/0/1">Arvin Tashakori</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zenan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Servati_A/0/1/0/all/0/1">Amir Servati</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayana_H/0/1/0/all/0/1">Harishkumar Narayana</a>, <a href="http://arxiv.org/find/cs/1/au:+Soltanian_S/0/1/0/all/0/1">Saeid Soltanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeap_R/0/1/0/all/0/1">Rou Yi Yeap</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Meng Han Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Toy_L/0/1/0/all/0/1">Lauren Toy</a>, <a href="http://arxiv.org/find/cs/1/au:+Servati_P/0/1/0/all/0/1">Peyman Servati</a></p>
<p>The kinematics of human movements and locomotion are closely linked to the
activation and contractions of muscles. To investigate this, we present a
multimodal dataset with benchmarks collected using a novel pair of Intelligent
Knee Sleeves (Texavie MarsWear Knee Sleeves) for human pose estimation. Our
system utilizes synchronized datasets that comprise time-series data from the
Knee Sleeves and the corresponding ground truth labels from the visualized
motion capture camera system. We employ these to generate 3D human models
solely based on the wearable data of individuals performing different
activities. We demonstrate the effectiveness of this camera-free system and
machine learning algorithms in the assessment of various movements and
exercises, including extension to unseen exercises and individuals. The results
show an average error of 7.21 degrees across all eight lower body joints when
compared to the ground truth, indicating the effectiveness and reliability of
the Knee Sleeve system for the prediction of different lower body joints beyond
the knees. The results enable human pose estimation in a seamless manner
without being limited by visual occlusion or the field of view of cameras. Our
results show the potential of multimodal wearable sensing in a variety of
applications from home fitness to sports, healthcare, and physical
rehabilitation focusing on pose and movement estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12830">Nature Inspired Evolutionary Swarm Optimizers for Biomedical Image and Signal Processing -- A Systematic Review. (arXiv:2311.12830v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adhikary_S/0/1/0/all/0/1">Subhrangshu Adhikary</a></p>
<p>The challenge of finding a global optimum in a solution search space with
limited resources and higher accuracy has given rise to several optimization
algorithms. Generally, the gradient-based optimizers converge to the global
solution very accurately, but they often require a large number of iterations
to find the solution. Researchers took inspiration from different natural
phenomena and behaviours of many living organisms to develop algorithms that
can solve optimization problems much quicker with high accuracy. These
algorithms are called nature-inspired meta-heuristic optimization algorithms.
These can be used for denoising signals, updating weights in a deep neural
network, and many other cases. In the state-of-the-art, there are no systematic
reviews available that have discussed the applications of nature-inspired
algorithms on biomedical signal processing. The paper solves that gap by
discussing the applications of such algorithms in biomedical signal processing
and also provides an updated survey of the application of these algorithms in
biomedical image processing. The paper reviews 28 latest peer-reviewed relevant
articles and 26 nature-inspired algorithms and segregates them into thoroughly
explored, lesser explored and unexplored categories intending to help readers
understand the reliability and exploration stage of each of these algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12832">Toward effective protection against diffusion based mimicry through score distillation. (arXiv:2311.12832v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1">Haotian Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1">Chumeng Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yongxin Chen</a></p>
<p>While generative diffusion models excel in producing high-quality images,
they can also be misused to mimic authorized images, posing a significant
threat to AI systems. Efforts have been made to add calibrated perturbations to
protect images from diffusion-based mimicry pipelines. However, most of the
existing methods are too ineffective and even impractical to be used by
individual users due to their high computation and memory requirements. In this
work, we present novel findings on attacking latent diffusion models (LDM) and
propose new plug-and-play strategies for more effective protection. In
particular, we explore the bottleneck in attacking an LDM, discovering that the
encoder module rather than the denoiser module is the vulnerable point. Based
on this insight, we present our strategy using Score Distillation Sampling
(SDS) to double the speed of protection and reduce memory occupation by half
without compromising its strength. Additionally, we provide a robust protection
strategy by counterintuitively minimizing the semantic loss, which can assist
in generating more natural perturbations. Finally, we conduct extensive
experiments to substantiate our findings and comprehensively evaluate our newly
proposed strategies. We hope our insights and protective measures can
contribute to better defense against malicious diffusion-based mimicry,
advancing the development of secure AI systems. The code is available in
https://github.com/xavihart/Diff-Protect
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12833">HPC-GPT: Integrating Large Language Model for High-Performance Computing. (arXiv:2311.12833v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xianzhong Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Le Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Emani_M/0/1/0/all/0/1">Murali Emani</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Chunhua Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1">Pei-Hung Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanderbruggen_T/0/1/0/all/0/1">Tristan Vanderbruggen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhen Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cerpa_A/0/1/0/all/0/1">Alberto E. Cerpa</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Wan Du</a></p>
<p>Large Language Models (LLMs), including the LLaMA model, have exhibited their
efficacy across various general-domain natural language processing (NLP) tasks.
However, their performance in high-performance computing (HPC) domain tasks has
been less than optimal due to the specialized expertise required to interpret
the model responses. In response to this challenge, we propose HPC-GPT, a novel
LLaMA-based model that has been supervised fine-tuning using generated QA
(Question-Answer) instances for the HPC domain. To evaluate its effectiveness,
we concentrate on two HPC tasks: managing AI models and datasets for HPC, and
data race detection. By employing HPC-GPT, we demonstrate comparable
performance with existing methods on both tasks, exemplifying its excellence in
HPC-related scenarios. Our experiments on open-source benchmarks yield
extensive results, underscoring HPC-GPT's potential to bridge the performance
gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way
for LLMs to excel in HPC domains, simplifying the utilization of language
models in complex computing applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12836">AI-based association analysis for medical imaging using latent-space geometric confounder correction. (arXiv:2311.12836v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianjing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Vernooij_M/0/1/0/all/0/1">Meike W. Vernooij</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolvius_E/0/1/0/all/0/1">Eppo B. Wolvius</a>, <a href="http://arxiv.org/find/cs/1/au:+Roshchupkin_G/0/1/0/all/0/1">Gennady V. Roshchupkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1">Esther E. Bron</a></p>
<p>AI has greatly enhanced medical image analysis, yet its use in
epidemiological population imaging studies remains limited due to visualization
challenges in non-linear models and lack of confounder control. Addressing
this, we introduce an AI method emphasizing semantic feature interpretation and
resilience against multiple confounders. Our approach's merits are tested in
three scenarios: extracting confounder-free features from a 2D synthetic
dataset; examining the association between prenatal alcohol exposure and
children's facial shapes using 3D mesh data; exploring the relationship between
global cognition and brain images with a 3D MRI dataset. Results confirm our
method effectively reduces confounder influences, establishing less confounded
associations. Additionally, it provides a unique visual representation,
highlighting specific image alterations due to identified correlations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12839">A Review of Deep Reinforcement Learning in Serverless Computing: Function Scheduling and Resource Auto-Scaling. (arXiv:2311.12839v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Majid_A/0/1/0/all/0/1">Amjad Yousef Majid</a>, <a href="http://arxiv.org/find/cs/1/au:+Marin_E/0/1/0/all/0/1">Eduard Marin</a></p>
<p>In the rapidly evolving field of serverless computing, efficient function
scheduling and resource scaling are critical for optimizing performance and
cost. This paper presents a comprehensive review of the application of Deep
Reinforcement Learning (DRL) techniques in these areas. We begin by providing
an overview of serverless computing, highlighting its benefits and challenges,
with a particular focus on function scheduling and resource scaling. We then
delve into the principles of deep reinforcement learning (DRL) and its
potential for addressing these challenges. A systematic review of recent
studies applying DRL to serverless computing is presented, covering various
algorithms, models, and performances. Our analysis reveals that DRL, with its
ability to learn and adapt from an environment, shows promising results in
improving the efficiency of function scheduling and resource scaling in
serverless computing. However, several challenges remain, including the need
for more realistic simulation environments, handling of cold starts, and the
trade-off between learning time and scheduling performance. We conclude by
discussing potential future directions for this research area, emphasizing the
need for more robust DRL models, better benchmarking methods, and the
exploration of multi-agent reinforcement learning for more complex serverless
architectures. This review serves as a valuable resource for researchers and
practitioners aiming to understand and advance the application of DRL in
serverless computing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12840">Wafer Map Defect Patterns Semi-Supervised Classification Using Latent Vector Representation. (arXiv:2311.12840v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1">Qiyu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoyan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zeng Zeng</a></p>
<p>As the globalization of semiconductor design and manufacturing processes
continues, the demand for defect detection during integrated circuit
fabrication stages is becoming increasingly critical, playing a significant
role in enhancing the yield of semiconductor products. Traditional wafer map
defect pattern detection methods involve manual inspection using electron
microscopes to collect sample images, which are then assessed by experts for
defects. This approach is labor-intensive and inefficient. Consequently, there
is a pressing need to develop a model capable of automatically detecting
defects as an alternative to manual operations. In this paper, we propose a
method that initially employs a pre-trained VAE model to obtain the fault
distribution information of the wafer map. This information serves as guidance,
combined with the original image set for semi-supervised model training. During
the semi-supervised training, we utilize a teacher-student network for
iterative learning. The model presented in this paper is validated on the
benchmark dataset WM-811K wafer dataset. The experimental results demonstrate
superior classification accuracy and detection performance compared to
state-of-the-art models, fulfilling the requirements for industrial
applications. Compared to the original architecture, we have achieved
significant performance improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12848">Lightweight Knowledge Representations for Automating Data Analysis. (arXiv:2311.12848v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sterbentz_M/0/1/0/all/0/1">Marko Sterbentz</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrie_C/0/1/0/all/0/1">Cameron Barrie</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooshmand_D/0/1/0/all/0/1">Donna Hooshmand</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahi_S/0/1/0/all/0/1">Shubham Shahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1">Abhratanu Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Pack_H/0/1/0/all/0/1">Harper Pack</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1">Andong Li Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Paley_A/0/1/0/all/0/1">Andrew Paley</a>, <a href="http://arxiv.org/find/cs/1/au:+Einarsson_A/0/1/0/all/0/1">Alexander Einarsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammond_K/0/1/0/all/0/1">Kristian Hammond</a></p>
<p>The principal goal of data science is to derive meaningful information from
data. To do this, data scientists develop a space of analytic possibilities and
from it reach their information goals by using their knowledge of the domain,
the available data, the operations that can be performed on those data, the
algorithms/models that are fed the data, and how all of these facets
interweave. In this work, we take the first steps towards automating a key
aspect of the data science pipeline: data analysis. We present an extensible
taxonomy of data analytic operations that scopes across domains and data, as
well as a method for codifying domain-specific knowledge that links this
analytics taxonomy to actual data. We validate the functionality of our
analytics taxonomy by implementing a system that leverages it, alongside domain
labelings for 8 distinct domains, to automatically generate a space of
answerable questions and associated analytic plans. In this way, we produce
information spaces over data that enable complex analyses and search over this
data and pave the way for fully automated data analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12854">Enhancing Robotic Manipulation: Harnessing the Power of Multi-Task Reinforcement Learning and Single Life Reinforcement Learning in Meta-World. (arXiv:2311.12854v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nehme_G/0/1/0/all/0/1">Ghadi Nehme</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabane_I/0/1/0/all/0/1">Ishan Sabane</a>, <a href="http://arxiv.org/find/cs/1/au:+Deo_T/0/1/0/all/0/1">Tejas Y. Deo</a></p>
<p>At present, robots typically require extensive training to successfully
accomplish a single task. However, to truly enhance their usefulness in
real-world scenarios, robots should possess the capability to perform multiple
tasks effectively. To address this need, various multi-task reinforcement
learning (RL) algorithms have been developed, including multi-task proximal
policy optimization (PPO), multi-task trust region policy optimization (TRPO),
and multi-task soft-actor critic (SAC). Nevertheless, these algorithms
demonstrate optimal performance only when operating within an environment or
observation space that exhibits a similar distribution. In reality, such
conditions are often not the norm, as robots may encounter scenarios or
observations that differ from those on which they were trained. Addressing this
challenge, algorithms like Q-Weighted Adversarial Learning (QWALE) attempt to
tackle the issue by training the base algorithm (generating prior data) solely
for a particular task, rendering it unsuitable for generalization across tasks.
So, the aim of this research project is to enable a robotic arm to successfully
execute seven distinct tasks within the Meta World environment. To achieve
this, a multi-task soft actor-critic (MT-SAC) is employed to train the robotic
arm. Subsequently, the trained model will serve as a source of prior data for
the single-life RL algorithm. The effectiveness of this MT-QWALE algorithm will
be assessed by conducting tests on various target positions (novel positions).
In the end, a comparison is provided between the trained MT-SAC and the
MT-QWALE algorithm where the MT-QWALE performs better. An ablation study
demonstrates that MT-QWALE successfully completes tasks with a slightly larger
number of steps even after hiding the final goal position.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12856">Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer. (arXiv:2311.12856v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Lee_N/0/1/0/all/0/1">Namkyeong Lee</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Noh_H/0/1/0/all/0/1">Heewoong Noh</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kim_S/0/1/0/all/0/1">Sungwon Kim</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Hyun_D/0/1/0/all/0/1">Dongmin Hyun</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Na_G/0/1/0/all/0/1">Gyoung S. Na</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Park_C/0/1/0/all/0/1">Chanyoung Park</a></p>
<p>The density of states (DOS) is a spectral property of crystalline materials,
which provides fundamental insights into various characteristics of the
materials. While previous works mainly focus on obtaining high-quality
representations of crystalline materials for DOS prediction, we focus on
predicting the DOS from the obtained representations by reflecting the nature
of DOS: DOS determines the general distribution of states as a function of
energy. That is, DOS is not solely determined by the crystalline material but
also by the energy levels, which has been neglected in previous works. In this
paper, we propose to integrate heterogeneous information obtained from the
crystalline materials and the energies via a multi-modal transformer, thereby
modeling the complex relationships between the atoms in the crystalline
materials and various energy levels for DOS prediction. Moreover, we propose to
utilize prompts to guide the model to learn the crystal structural
system-specific interactions between crystalline materials and energies.
Extensive experiments on two types of DOS, i.e., Phonon DOS and Electron DOS,
with various real-world scenarios demonstrate the superiority of
DOSTransformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12858">RAEDiff: Denoising Diffusion Probabilistic Models Based Reversible Adversarial Examples Self-Generation and Self-Recovery. (arXiv:2311.12858v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1">Fan Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiaoyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xuefeng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhuo Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yan Zhao</a></p>
<p>Collected and annotated datasets, which are obtained through extensive
efforts, are effective for training Deep Neural Network (DNN) models. However,
these datasets are susceptible to be misused by unauthorized users, resulting
in infringement of Intellectual Property (IP) rights owned by the dataset
creators. Reversible Adversarial Exsamples (RAE) can help to solve the issues
of IP protection for datasets. RAEs are adversarial perturbed images that can
be restored to the original. As a cutting-edge approach, RAE scheme can serve
the purposes of preventing unauthorized users from engaging in malicious model
training, as well as ensuring the legitimate usage of authorized users.
Nevertheless, in the existing work, RAEs still rely on the embedded auxiliary
information for restoration, which may compromise their adversarial abilities.
In this paper, a novel self-generation and self-recovery method, named as
RAEDiff, is introduced for generating RAEs based on a Denoising Diffusion
Probabilistic Models (DDPM). It diffuses datasets into a Biased Gaussian
Distribution (BGD) and utilizes the prior knowledge of the DDPM for generating
and recovering RAEs. The experimental results demonstrate that RAEDiff
effectively self-generates adversarial perturbations for DNN models, including
Artificial Intelligence Generated Content (AIGC) models, while also exhibiting
significant self-recovery capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12860">On the stability, correctness and plausibility of visual explanation methods based on feature importance. (arXiv:2311.12860v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Darme_R/0/1/0/all/0/1">Romain Xu-Darme</a> (LSL, LIG), <a href="http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1">Jenny Benois-Pineau</a> (LaBRI), <a href="http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1">Romain Giot</a> (LaBRI), <a href="http://arxiv.org/find/cs/1/au:+Quenot_G/0/1/0/all/0/1">Georges Qu&#xe9;not</a> (LIG), <a href="http://arxiv.org/find/cs/1/au:+Chihani_Z/0/1/0/all/0/1">Zakaria Chihani</a> (LSL), <a href="http://arxiv.org/find/cs/1/au:+Rousset_M/0/1/0/all/0/1">Marie-Christine Rousset</a> (LIG), <a href="http://arxiv.org/find/cs/1/au:+Zhukov_A/0/1/0/all/0/1">Alexey Zhukov</a> (LaBRI)</p>
<p>In the field of Explainable AI, multiples evaluation metrics have been
proposed in order to assess the quality of explanation methods w.r.t. a set of
desired properties. In this work, we study the articulation between the
stability, correctness and plausibility of explanations based on feature
importance for image classifiers. We show that the existing metrics for
evaluating these properties do not always agree, raising the issue of what
constitutes a good evaluation metric for explanations. Finally, in the
particular case of stability and correctness, we show the possible limitations
of some evaluation metrics and propose new ones that take into account the
local behaviour of the model under test.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12861">A versatile circuit for emulating active biological dendrites applied to sound localisation and neuron imitation. (arXiv:2311.12861v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mannion_D/0/1/0/all/0/1">Daniel John Mannion</a></p>
<p>Sophisticated machine learning struggles to transition onto battery-operated
devices due to the high-power consumption of neural networks. Researchers have
turned to neuromorphic engineering, inspired by biological neural networks, for
more efficient solutions. While previous research focused on artificial neurons
and synapses, an essential component has been overlooked: dendrites. Dendrites
transmit inputs from synapses to the neuron's soma, applying both passive and
active transformations. However, neuromorphic circuits replace these
sophisticated computational channels with metallic interconnects. In this
study, we introduce a versatile circuit that emulates a segment of a dendrite
which exhibits gain, introduces delays, and performs integration. We show how
sound localisation - a biological example of dendritic computation - is not
possible with the existing passive dendrite circuits but can be achieved using
this proposed circuit. We also find that dendrites can form bursting neurons.
This significant discovery suggests the potential to fabricate neural networks
solely comprised of dendrite circuits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12866">Modular Blended Attention Network for Video Question Answering. (arXiv:2311.12866v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingjie Zhou</a></p>
<p>In multimodal machine learning tasks, it is due to the complexity of the
assignments that the network structure, in most cases, is assembled in a
sophisticated way. The holistic architecture can be separated into several
logical parts according to the respective ends that the modules are devised to
achieve. As the number of modalities of information representation increases,
constructing ad hoc subnetworks for processing the data from divergent
modalities while mediating the fusion of different information types has become
a cumbersome and expensive problem. In this paper, we present an approach to
facilitate the question with a reusable and composable neural unit; by
connecting the units in series or parallel, the arduous network constructing of
multimodal machine learning tasks will be accomplished in a much
straightforward way. Additionally, through parameter sharing (weights
replication) among the units, the space complexity will be significantly
reduced. We have conducted experiments on three commonly used datasets; our
method achieves impressive performance compared to several video QA baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12869">Progression and Challenges of IoT in Healthcare: A Short Review. (arXiv:2311.12869v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1">S M Atikur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibtisum_S/0/1/0/all/0/1">Sifat Ibtisum</a>, <a href="http://arxiv.org/find/cs/1/au:+Podder_P/0/1/0/all/0/1">Priya Podder</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1">S. M. Saokat Hossain</a></p>
<p>Smart healthcare, an integral element of connected living, plays a pivotal
role in fulfilling a fundamental human need. The burgeoning field of smart
healthcare is poised to generate substantial revenue in the foreseeable future.
Its multifaceted framework encompasses vital components such as the Internet of
Things (IoT), medical sensors, artificial intelligence (AI), edge and cloud
computing, as well as next-generation wireless communication technologies. Many
research papers discuss smart healthcare and healthcare more broadly. Numerous
nations have strategically deployed the Internet of Medical Things (IoMT)
alongside other measures to combat the propagation of COVID-19. This combined
effort has not only enhanced the safety of frontline healthcare workers but has
also augmented the overall efficacy in managing the pandemic, subsequently
reducing its impact on human lives and mortality rates. Remarkable strides have
been made in both applications and technology within the IoMT domain. However,
it is imperative to acknowledge that this technological advancement has
introduced certain challenges, particularly in the realm of security. The rapid
and extensive adoption of IoMT worldwide has magnified issues related to
security and privacy. These encompass a spectrum of concerns, ranging from
replay attacks, man-in-the-middle attacks, impersonation, privileged insider
threats, remote hijacking, password guessing, and denial of service (DoS)
attacks, to malware incursions. In this comprehensive review, we undertake a
comparative analysis of existing strategies designed for the detection and
prevention of malware in IoT environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12871">An Embodied Generalist Agent in 3D World. (arXiv:2311.12871v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiangyong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1">Silong Yong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Linghu_X/0/1/0/all/0/1">Xiongkun Linghu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Puhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1">Baoxiong Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Siyuan Huang</a></p>
<p>Leveraging massive knowledge and learning schemes from large language models
(LLMs), recent machine learning models show notable successes in building
generalist agents that exhibit the capability of general-purpose task solving
in diverse domains, including natural language processing, computer vision, and
robotics. However, a significant challenge remains as these models exhibit
limited ability in understanding and interacting with the 3D world. We argue
this limitation significantly hinders the current models from performing
real-world tasks and further achieving general intelligence. To this end, we
introduce an embodied multi-modal and multi-task generalist agent that excels
in perceiving, grounding, reasoning, planning, and acting in the 3D world. Our
proposed agent, referred to as LEO, is trained with shared LLM-based model
architectures, objectives, and weights in two stages: (i) 3D vision-language
alignment and (ii) 3D vision-language-action instruction tuning. To facilitate
the training, we meticulously curate and generate an extensive dataset
comprising object-level and scene-level multi-modal tasks with exceeding scale
and complexity, necessitating a deep understanding of and interaction with the
3D world. Through rigorous experiments, we demonstrate LEO's remarkable
proficiency across a wide spectrum of tasks, including 3D captioning, question
answering, embodied reasoning, embodied navigation, and robotic manipulation.
Our ablation results further provide valuable insights for the development of
future embodied generalist agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12872">The Case for Universal Basic Computing Power. (arXiv:2311.12872v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yue Zhu</a></p>
<p>The Universal Basic Computing Power (UBCP) initiative ensures global, free
access to a set amount of computing power specifically for AI research and
development (R&amp;D). This initiative comprises three key elements. First, UBCP
must be cost free, with its usage limited to AI R&amp;D and minimal additional
conditions. Second, UBCP should continually incorporate the state of the art AI
advancements, including efficiently distilled, compressed, and deployed
training data, foundational models, benchmarks, and governance tools. Lastly,
it's essential for UBCP to be universally accessible, ensuring convenience for
all users. We urge major stakeholders in AI development large platforms, open
source contributors, and policymakers to prioritize the UBCP initiative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12875">Nav-Q: Quantum Deep Reinforcement Learning for Collision-Free Navigation of Self-Driving Cars. (arXiv:2311.12875v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Sinha_A/0/1/0/all/0/1">Akash Sinha</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Macaluso_A/0/1/0/all/0/1">Antonio Macaluso</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Klusch_M/0/1/0/all/0/1">Matthias Klusch</a></p>
<p>The challenge of collision-free navigation (CFN) for self-driving cars is an
NP-hard problem addressed through Deep Reinforcement Learning (DRL). Despite
the effectiveness of DRL methods, their application demands significant
computing resources and prolonged training periods to establish a resilient
agent. On the other hand, quantum reinforcement learning algorithms have
recently demonstrated faster convergence and improved stability in simple,
non-real-world environments. However, their application in the real-world CFN
domain has not been explored, and their direct adaptation would require a
quantum computing device onboard the vehicle for testing.
</p>
<p>In this work, we propose Nav-Q, the first quantum-supported DRL algorithm for
CFN of self-driving cars, that leverages quantum computation for improving the
training performance without the requirement for onboard quantum hardware.
Nav-Q is based on the actor-critic approach, where the critic is implemented
using a hybrid quantum-classical algorithm suitable for near-term quantum
devices. We assess the performance of Nav-Q using the CARLA driving simulator,
a de facto standard benchmark for evaluating state-of-the-art DRL methods. Our
empirical evaluations showcase that Nav-Q surpasses its classical counterpart
not only in terms of training stability but also, in certain instances, with
respect to the convergence rate when analyzing the Reward vs. Episode curve.
This enhancement is accomplished without negatively impacting the learned
policy by the agent. Furthermore, we assess Nav-Q in relation to effective
dimension, unveiling that the incorporation of a quantum component results in a
model possessing greater descriptive power compared to classical baselines.
Finally, we evaluate the performance of Nav-Q using noisy quantum simulation,
observing that the quantum noise enhances the exploratory tendencies of the
agent during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12876">Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification. (arXiv:2311.12876v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Corral_J/0/1/0/all/0/1">Jos&#xe9; Mar&#xed;a Rodr&#xed;guez Corral</a>, <a href="http://arxiv.org/find/eess/1/au:+Civit_Masot_J/0/1/0/all/0/1">Javier Civit-Masot</a>, <a href="http://arxiv.org/find/eess/1/au:+Luna_Perejon_F/0/1/0/all/0/1">Francisco Luna-Perej&#xf3;n</a>, <a href="http://arxiv.org/find/eess/1/au:+Diaz_Cano_I/0/1/0/all/0/1">Ignacio D&#xed;az-Cano</a>, <a href="http://arxiv.org/find/eess/1/au:+Morgado_Estevez_A/0/1/0/all/0/1">Arturo Morgado-Est&#xe9;vez</a>, <a href="http://arxiv.org/find/eess/1/au:+Dominguez_Morales_M/0/1/0/all/0/1">Manuel Dom&#xed;nguez-Morales</a></p>
<p>In this work, we evaluate the energy usage of fully embedded medical
diagnosis aids based on both segmentation and classification of medical images
implemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis
based on color fundus images as an example to show the possibility of
performing segmentation and classification in real time on embedded boards and
to highlight the different energy requirements of the studied implementations.
</p>
<p>Several other works develop the use of segmentation and feature extraction
techniques to detect glaucoma, among many other pathologies, with deep neural
networks. Memory limitations and low processing capabilities of embedded
accelerated systems (EAS) limit their use for deep network-based system
training. However, including specific acceleration hardware, such as NVIDIA's
Maxwell GPU or Google's Edge TPU, enables them to perform inferences using
complex pre-trained networks in very reasonable times.
</p>
<p>In this study, we evaluate the timing and energy performance of two EAS
equipped with Machine Learning (ML) accelerators executing an example
diagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)
segmentation, the obtained prediction times per image are under 29 and 43 ms
using Edge TPUs and Maxwell GPUs, respectively. Prediction times for the
classification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell
GPUs, respectively. Regarding energy usage, in approximate terms, for OD
segmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,
respectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and
70 mJ, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12889">Enhancing Scene Graph Generation with Hierarchical Relationships and Commonsense Knowledge. (arXiv:2311.12889v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bowen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zhijun Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1">Camillo Jose Taylor</a></p>
<p>This work presents an enhanced approach to generating scene graphs by
incorporating a relationship hierarchy and commonsense knowledge. Specifically,
we propose a Bayesian classification head that exploits an informative
hierarchical structure. It jointly predicts the super-category or type of
relationship between the two objects, along with the detailed relationship
under each super-category. We design a commonsense validation pipeline that
uses a large language model to critique the results from the scene graph
prediction system and then use that feedback to enhance the model performance.
The system requires no external large language model assistance at test time,
making it more convenient for practical applications. Experiments on the Visual
Genome and the OpenImage V6 datasets demonstrate that harnessing hierarchical
relationships enhances the model performance by a large margin. The proposed
Bayesian head can also be incorporated as a portable module in existing scene
graph generation algorithms to improve their results. In addition, the
commonsense validation enables the model to generate an extensive set of
reasonable predictions beyond dataset annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12893">A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs. (arXiv:2311.12893v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1">Jiageng Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Ming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yinliang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zihang Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haoran Shen</a></p>
<p>For intelligent quadcopter UAVs, a robust and reliable autonomous planning
system is crucial. Most current trajectory planning methods for UAVs are
suitable for static environments but struggle to handle dynamic obstacles,
which can pose challenges and even dangers to flight. To address this issue,
this paper proposes a vision-based planning system that combines tracking and
trajectory prediction of dynamic obstacles to achieve efficient and reliable
autonomous flight. We use a lightweight object detection algorithm to identify
dynamic obstacles and then use Kalman Filtering to track and estimate their
motion states. During the planning phase, we not only consider static obstacles
but also account for the potential movements of dynamic obstacles. For
trajectory generation, we use a B-spline-based trajectory search algorithm,
which is further optimized with various constraints to enhance safety and
alignment with the UAV's motion characteristics. We conduct experiments in both
simulation and real-world environments, and the results indicate that our
approach can successfully detect and avoid obstacles in dynamic environments in
real-time, offering greater reliability compared to existing approaches.
Furthermore, with the advancements in Natural Language Processing (NLP)
technology demonstrating exceptional zero-shot generalization capabilities,
more user-friendly human-machine interactions have become feasible, and this
study also explores the integration of autonomous planning systems with Large
Language Models (LLMs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12905">Revisiting the Domain Shift and Sample Uncertainty in Multi-source Active Domain Transfer. (arXiv:2311.12905v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenqiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1">Zheqi Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jia-Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juncheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mengze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yueting Zhuang</a></p>
<p>Active Domain Adaptation (ADA) aims to maximally boost model adaptation in a
new target domain by actively selecting a limited number of target data to
annotate.This setting neglects the more practical scenario where training data
are collected from multiple sources. This motivates us to target a new and
challenging setting of knowledge transfer that extends ADA from a single source
domain to multiple source domains, termed Multi-source Active Domain Adaptation
(MADA). Not surprisingly, we find that most traditional ADA methods cannot work
directly in such a setting, mainly due to the excessive domain gap introduced
by all the source domains and thus their uncertainty-aware sample selection can
easily become miscalibrated under the multi-domain shifts. Considering this, we
propose a Dynamic integrated uncertainty valuation framework(Detective) that
comprehensively consider the domain shift between multi-source domains and
target domain to detect the informative target samples. Specifically, the
leverages a dynamic Domain Adaptation(DA) model that learns how to adapt the
model's parameters to fit the union of multi-source domains. This enables an
approximate single-source domain modeling by the dynamic model. We then
comprehensively measure both domain uncertainty and predictive uncertainty in
the target domain to detect informative target samples using evidential deep
learning, thereby mitigating uncertainty miscalibration. Furthermore, we
introduce a contextual diversity-aware calculator to enhance the diversity of
the selected samples. Experiments demonstrate that our solution outperforms
existing methods by a considerable margin on three domain adaptation
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12908">Diffusion Model Alignment Using Direct Preference Optimization. (arXiv:2311.12908v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1">Bram Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1">Meihua Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1">Rafael Rafailov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Linqi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1">Aaron Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1">Senthil Purushwalkam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1">Nikhil Naik</a></p>
<p>Large language models (LLMs) are fine-tuned using human comparison data with
Reinforcement Learning from Human Feedback (RLHF) methods to make them better
aligned with users' preferences. In contrast to LLMs, human preference learning
has not been widely explored in text-to-image diffusion models; the best
existing approach is to fine-tune a pretrained model using carefully curated
high quality images and captions to improve visual appeal and text alignment.
We propose Diffusion-DPO, a method to align diffusion models to human
preferences by directly optimizing on human comparison data. Diffusion-DPO is
adapted from the recently developed Direct Preference Optimization (DPO), a
simpler alternative to RLHF which directly optimizes a policy that best
satisfies human preferences under a classification objective. We re-formulate
DPO to account for a diffusion model notion of likelihood, utilizing the
evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic
dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model
of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with
Diffusion-DPO. Our fine-tuned base model significantly outperforms both base
SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement
model in human evaluation, improving visual appeal and prompt alignment. We
also develop a variant that uses AI feedback and has comparable performance to
training on human preferences, opening the door for scaling of diffusion model
alignment methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12917">Orchard: building large cancer phylogenies using stochastic combinatorial search. (arXiv:2311.12917v1 [q-bio.PE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Kulman_E/0/1/0/all/0/1">E. Kulman</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kuang_R/0/1/0/all/0/1">R. Kuang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Morris_Q/0/1/0/all/0/1">Q. Morris</a></p>
<p>Phylogenies depicting the evolutionary history of genetically heterogeneous
subpopulations of cells from the same cancer i.e., cancer phylogenies, provide
useful insights about cancer development and inform treatment. Cancer
phylogenies can be reconstructed using data obtained from bulk DNA sequencing
of multiple tissue samples from the same cancer. We introduce Orchard, a fast
algorithm that reconstructs cancer phylogenies using point mutations detected
in bulk DNA sequencing data. Orchard constructs cancer phylogenies
progressively, one point mutation at a time, ultimately sampling complete
phylogenies from a posterior distribution implied by the bulk DNA data. Orchard
reconstructs more plausible phylogenies than state-of-the-art cancer phylogeny
reconstruction methods on 90 simulated cancers and 14 B-progenitor acute
lymphoblastic leukemias (B-ALLs). These results demonstrate that Orchard
accurately reconstructs cancer phylogenies with up to 300 mutations. We then
introduce a simple graph based clustering algorithm that uses a reconstructed
phylogeny to infer unique groups of mutations i.e., mutation clusters, that
characterize the genetic differences between cancer cell populations, and show
that this approach is competitive with state-of-the-art mutation clustering
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12919">SPOT! Revisiting Video-Language Models for Event Understanding. (arXiv:2311.12919v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gengyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1">Jinhe Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a></p>
<p>Understanding videos is an important research topic for multimodal learning.
Leveraging large-scale datasets of web-crawled video-text pairs as weak
supervision has become a pre-training paradigm for learning joint
representations and showcased remarkable potential in video understanding
tasks. However, videos can be multi-event and multi-grained, while these
video-text pairs usually contain only broad-level video captions. This raises a
question: with such weak supervision, can video representation in
video-language models gain the ability to distinguish even factual
discrepancies in textual description and understand fine-grained events? To
address this, we introduce SPOT Prober, to benchmark existing video-language
models's capacities of distinguishing event-level discrepancies as an indicator
of models' event understanding ability. Our approach involves extracting events
as tuples (&lt;Subject, Predicate, Object, Attribute, Timestamps&gt;) from videos and
generating false event tuples by manipulating tuple components systematically.
We reevaluate the existing video-language models with these positive and
negative captions and find they fail to distinguish most of the manipulated
events. Based on our findings, we propose to plug in these manipulated event
captions as hard negative samples and find them effective in enhancing models
for event understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12943">InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions. (arXiv:2311.12943v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kedia_K/0/1/0/all/0/1">Kushal Kedia</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1">Atiksh Bhardwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_P/0/1/0/all/0/1">Prithwish Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1">Sanjiban Choudhury</a></p>
<p>In collaborative human-robot manipulation, a robot must predict human intents
and adapt its actions accordingly to smoothly execute tasks. However, the
human's intent in turn depends on actions the robot takes, creating a
chicken-or-egg problem. Prior methods ignore such inter-dependency and instead
train marginal intent prediction models independent of robot actions. This is
because training conditional models is hard given a lack of paired human-robot
interaction datasets.
</p>
<p>Can we instead leverage large-scale human-human interaction data that is more
easily accessible? Our key insight is to exploit a correspondence between human
and robot actions that enables transfer learning from human-human to
human-robot data. We propose a novel architecture, InteRACT, that pre-trains a
conditional intent prediction model on large human-human datasets and
fine-tunes on a small human-robot dataset. We evaluate on a set of real-world
collaborative human-robot manipulation tasks and show that our conditional
model improves over various marginal baselines. We also introduce new
techniques to tele-operate a 7-DoF robot arm and collect a diverse range of
human-robot collaborative manipulation data, which we open-source.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12944">DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks. (arXiv:2311.12944v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1">Daksh Dave</a>, <a href="http://arxiv.org/find/cs/1/au:+Chamola_V/0/1/0/all/0/1">Vinay Chamola</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Sandeep Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeadally_S/0/1/0/all/0/1">Sherali Zeadally</a></p>
<p>The power requirements posed by the fifth-generation and beyond cellular
networks are an important constraint in network deployment and require
energy-efficient solutions. In this work, we propose a novel user load transfer
approach using airborne base stations (BS), mounted on drones, for reliable and
secure power redistribution across the micro-grid network comprising green
small cell BSs. Depending on the user density and the availability of an aerial
BS, the energy requirement of a cell with an energy deficit is accommodated by
migrating the aerial BS from a high-energy to a low-energy cell. The proposed
hybrid drone-based framework integrates long short-term memory with unique cost
functions using an evolutionary neural network for drones and BSs, and
efficiently manages energy and load redistribution. The proposed algorithm
reduces power outages at BSs and maintains consistent throughput stability,
thereby demonstrating its capability to boost the reliability and robustness of
wireless communication systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12947">PINNs-Based Uncertainty Quantification for Transient Stability Analysis. (arXiv:2311.12947v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ren Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1">Ming Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_Cortes_L/0/1/0/all/0/1">Lola Gir&#xe1;ldez S&#xe1;nchez-Cort&#xe9;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerra_I/0/1/0/all/0/1">Ignacio de Cominges Guerra</a></p>
<p>This paper addresses the challenge of transient stability in power systems
with missing parameters and uncertainty propagation in swing equations. We
introduce a novel application of Physics-Informed Neural Networks (PINNs),
specifically an Ensemble of PINNs (E-PINNs), to estimate critical parameters
like rotor angle and inertia coefficient with enhanced accuracy and reduced
computational load. E-PINNs capitalize on the underlying physical principles of
swing equations to provide a robust solution. Our approach not only facilitates
efficient parameter estimation but also quantifies uncertainties, delivering
probabilistic insights into the system behavior. The efficacy of E-PINNs is
demonstrated through the analysis of $1$-bus and $2$-bus systems, highlighting
the model's ability to handle parameter variability and data scarcity. The
study advances the application of machine learning in power system stability,
paving the way for reliable and computationally efficient transient stability
analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12956">Innovative Horizons in Aerial Imagery: LSKNet Meets DiffusionDet for Advanced Object Detection. (arXiv:2311.12956v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharshar_A/0/1/0/all/0/1">Ahmed Sharshar</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsun_A/0/1/0/all/0/1">Aleksandr Matsun</a></p>
<p>In the realm of aerial image analysis, object detection plays a pivotal role,
with significant implications for areas such as remote sensing, urban planning,
and disaster management. This study addresses the inherent challenges in this
domain, notably the detection of small objects, managing densely packed
elements, and accounting for diverse orientations. We present an in-depth
evaluation of an object detection model that integrates the Large Selective
Kernel Network (LSKNet)as its backbone with the DiffusionDet head, utilizing
the iSAID dataset for empirical analysis. Our approach encompasses the
introduction of novel methodologies and extensive ablation studies. These
studies critically assess various aspects such as loss functions, box
regression techniques, and classification strategies to refine the model's
precision in object detection. The paper details the experimental application
of the LSKNet backbone in synergy with the DiffusionDet heads, a combination
tailored to meet the specific challenges in aerial image object detection. The
findings of this research indicate a substantial enhancement in the model's
performance, especially in the accuracy-time tradeoff. The proposed model
achieves a mean average precision (MAP) of approximately 45.7%, which is a
significant improvement, outperforming the RCNN model by 4.7% on the same
dataset. This advancement underscores the effectiveness of the proposed
modifications and sets a new benchmark in aerial image analysis, paving the way
for more accurate and efficient object detection methodologies. The code is
publicly available at https://github.com/SashaMatsun/LSKDiffDet
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12967">Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model. (arXiv:2311.12967v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouasfi_A/0/1/0/all/0/1">Amine Ouasfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1">Adnane Boukhayma</a></p>
<p>Feedforward generalizable models for implicit shape reconstruction from
unoriented point cloud present multiple advantages, including high performance
and inference speed. However, they still suffer from generalization issues,
ranging from underfitting the input point cloud, to misrepresenting samples
outside of the training data distribution, or with toplogies unseen at
training. We propose here an efficient mechanism to remedy some of these
limitations at test time. We combine the inter-shape data prior of the network
with an intra-shape regularization prior of a Nystr\"om Kernel Ridge
Regression, that we further adapt by fitting its hyperprameters to the current
shape. The resulting shape function defined in a shape specific Reproducing
Kernel Hilbert Space benefits from desirable stability and efficiency
properties and grants a shape adaptive expressiveness-robustness trade-off. We
demonstrate the improvement obtained through our method with respect to
baselines and the state-of-the-art using synthetic and real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12970">Clustered Policy Decision Ranking. (arXiv:2311.12970v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levin_M/0/1/0/all/0/1">Mark Levin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1">Hana Chockler</a></p>
<p>Policies trained via reinforcement learning (RL) are often very complex even
for simple tasks. In an episode with n time steps, a policy will make n
decisions on actions to take, many of which may appear non-intuitive to the
observer. Moreover, it is not clear which of these decisions directly
contribute towards achieving the reward and how significant their contribution
is. Given a trained policy, we propose a black-box method based on statistical
covariance estimation that clusters the states of the environment and ranks
each cluster according to the importance of decisions made in its states. We
compare our measure against a previous statistical fault localization based
ranking procedure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12975">Neural Approximate Dynamic Programming for the Ultra-fast Order Dispatching Problem. (arXiv:2311.12975v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Dehghan_A/0/1/0/all/0/1">Arash Dehghan</a>, <a href="http://arxiv.org/find/math/1/au:+Cevik_M/0/1/0/all/0/1">Mucahit Cevik</a>, <a href="http://arxiv.org/find/math/1/au:+Bodur_M/0/1/0/all/0/1">Merve Bodur</a></p>
<p>Same-Day Delivery (SDD) services aim to maximize the fulfillment of online
orders while minimizing delivery delays but are beset by operational
uncertainties such as those in order volumes and courier planning. Our work
aims to enhance the operational efficiency of SDD by focusing on the ultra-fast
Order Dispatching Problem (ODP), which involves matching and dispatching orders
to couriers within a centralized warehouse setting, and completing the delivery
within a strict timeline (e.g., within minutes). We introduce important
extensions to ultra-fast ODP such as order batching and explicit courier
assignments to provide a more realistic representation of dispatching
operations and improve delivery efficiency. As a solution method, we primarily
focus on NeurADP, a methodology that combines Approximate Dynamic Programming
(ADP) and Deep Reinforcement Learning (DRL), and our work constitutes the first
application of NeurADP outside of the ride-pool matching problem. NeurADP is
particularly suitable for ultra-fast ODP as it addresses complex one-to-many
matching and routing intricacies through a neural network-based VFA that
captures high-dimensional problem dynamics without requiring manual feature
engineering as in generic ADP methods. We test our proposed approach using four
distinct realistic datasets tailored for ODP and compare the performance of
NeurADP against myopic and DRL baselines by also making use of non-trivial
bounds to assess the quality of the policies. Our numerical results indicate
that the inclusion of order batching and courier queues enhances the efficiency
of delivery operations and that NeurADP significantly outperforms other
methods. Detailed sensitivity analysis with important parameters confirms the
robustness of NeurADP under different scenarios, including variations in
courier numbers, spatial setup, vehicle capacity, and permitted delay time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12983">GAIA: a benchmark for General AI Assistants. (arXiv:2311.12983v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mialon_G/0/1/0/all/0/1">Gr&#xe9;goire Mialon</a>, <a href="http://arxiv.org/find/cs/1/au:+Fourrier_C/0/1/0/all/0/1">Cl&#xe9;mentine Fourrier</a>, <a href="http://arxiv.org/find/cs/1/au:+Swift_C/0/1/0/all/0/1">Craig Swift</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1">Thomas Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1">Yann LeCun</a>, <a href="http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1">Thomas Scialom</a></p>
<p>We introduce GAIA, a benchmark for General AI Assistants that, if solved,
would represent a milestone in AI research. GAIA proposes real-world questions
that require a set of fundamental abilities such as reasoning, multi-modality
handling, web browsing, and generally tool-use proficiency. GAIA questions are
conceptually simple for humans yet challenging for most advanced AIs: we show
that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins.
This notable performance disparity contrasts with the recent trend of LLMs
outperforming humans on tasks requiring professional skills in e.g. law or
chemistry. GAIA's philosophy departs from the current trend in AI benchmarks
suggesting to target tasks that are ever more difficult for humans. We posit
that the advent of Artificial General Intelligence (AGI) hinges on a system's
capability to exhibit similar robustness as the average human does on such
questions. Using GAIA's methodology, we devise 466 questions and their answer.
We release our questions while retaining answers to 300 of them to power a
leader-board available at https://huggingface.co/gaia-benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12986">Unsupervised Graph Attention Autoencoder for Attributed Networks using K-means Loss. (arXiv:2311.12986v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bekkaira_A/0/1/0/all/0/1">Abdelfateh Bekkaira</a>, <a href="http://arxiv.org/find/cs/1/au:+Bellaouar_S/0/1/0/all/0/1">Slimane Bellaouar</a>, <a href="http://arxiv.org/find/cs/1/au:+Oulad_Naoui_S/0/1/0/all/0/1">Slimane Oulad-Naoui</a></p>
<p>Multimodal Sentiment Analysis (MSA) has recently become a centric research
direction for many real-world applications. This proliferation is due to the
fact that opinions are central to almost all human activities and are key
influencers of our behaviors. In addition, the recent deployment of Deep
Learning-based (DL) models has proven their high efficiency for a wide range of
Western languages. In contrast, Arabic DL-based multimodal sentiment analysis
(MSA) is still in its infantile stage due, mainly, to the lack of standard
datasets. % The contribution In this paper, our investigation is twofold.
First, we design a pipeline that helps building our Arabic Multimodal dataset
leveraging both state-of-the-art transformers and feature extraction tools
within word alignment techniques. Thereafter, we validate our dataset using
state-of-the-art transformer-based model dealing with multimodality. Despite
the small size of the outcome dataset, experiments show that Arabic
multimodality is very promising.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12990">NERIF: GPT-4V for Automatic Scoring of Drawn Models. (arXiv:2311.12990v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gyeong-Geon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaoming Zhai</a></p>
<p>Scoring student-drawn models is time-consuming. Recently released GPT-4V
provides a unique opportunity to advance scientific modeling practices by
leveraging the powerful image processing capability. To test this ability
specifically for automatic scoring, we developed a method NERIF
(Notation-Enhanced Rubric Instruction for Few-shot Learning) employing
instructional note and rubrics to prompt GPT-4V to score students' drawn models
for science phenomena. We randomly selected a set of balanced data (N = 900)
that includes student-drawn models for six modeling assessment tasks. Each
model received a score from GPT-4V ranging at three levels: 'Beginning,'
'Developing,' or 'Proficient' according to scoring rubrics. GPT-4V scores were
compared with human experts' scores to calculate scoring accuracy. Results show
that GPT-4V's average scoring accuracy was mean =.51, SD = .037. Specifically,
average scoring accuracy was .64 for the 'Beginning' class, .62 for the
'Developing' class, and .26 for the 'Proficient' class, indicating that more
proficient models are more challenging to score. Further qualitative study
reveals how GPT-4V retrieves information from image input, including problem
context, example evaluations provided by human coders, and students' drawing
models. We also uncovered how GPT-4V catches the characteristics of
student-drawn models and narrates them in natural language. At last, we
demonstrated how GPT-4V assigns scores to student-drawn models according to the
given scoring rubric and instructional notes. Our findings suggest that the
NERIF is an effective approach for employing GPT-4V to score drawn models. Even
though there is space for GPT-4V to improve scoring accuracy, some mis-assigned
scores seemed interpretable to experts. The results of this study show that
utilizing GPT-4V for automatic scoring of student-drawn models is promising.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12996">RLIF: Interactive Imitation Learning as Reinforcement Learning. (arXiv:2311.12996v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jianlan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1">Perry Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuexiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>Although reinforcement learning methods offer a powerful framework for
automatic skill acquisition, for practical learning-based control problems in
domains such as robotics, imitation learning often provides a more convenient
and accessible alternative. In particular, an interactive imitation learning
method such as DAgger, which queries a near-optimal expert to intervene online
to collect correction data for addressing the distributional shift challenges
that afflict na\"ive behavioral cloning, can enjoy good performance both in
theory and practice without requiring manually specified reward functions and
other components of full reinforcement learning methods. In this paper, we
explore how off-policy reinforcement learning can enable improved performance
under assumptions that are similar but potentially even more practical than
those of interactive imitation learning. Our proposed method uses reinforcement
learning with user intervention signals themselves as rewards. This relaxes the
assumption that intervening experts in interactive imitation learning should be
near-optimal and enables the algorithm to learn behaviors that improve over the
potential suboptimal human expert. We also provide a unified framework to
analyze our RL method and DAgger; for which we present the asymptotic analysis
of the suboptimal gap for both methods as well as the non-asymptotic sample
complexity bound of our method. We then evaluate our method on challenging
high-dimensional continuous control simulation benchmarks as well as real-world
robotic vision-based manipulation tasks. The results show that it strongly
outperforms DAgger-like approaches across the different tasks, especially when
the intervening experts are suboptimal. Code and videos can be found on the
project website: rlif-page.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12999">CovarNav: Machine Unlearning via Model Inversion and Covariance Navigation. (arXiv:2311.12999v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1">Ali Abbasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Thrash_C/0/1/0/all/0/1">Chayne Thrash</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbari_E/0/1/0/all/0/1">Elaheh Akbari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Daniel Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1">Soheil Kolouri</a></p>
<p>The rapid progress of AI, combined with its unprecedented public adoption and
the propensity of large neural networks to memorize training data, has given
rise to significant data privacy concerns. To address these concerns, machine
unlearning has emerged as an essential technique to selectively remove the
influence of specific training data points on trained models. In this paper, we
approach the machine unlearning problem through the lens of continual learning.
Given a trained model and a subset of training data designated to be forgotten
(i.e., the "forget set"), we introduce a three-step process, named CovarNav, to
facilitate this forgetting. Firstly, we derive a proxy for the model's training
data using a model inversion attack. Secondly, we mislabel the forget set by
selecting the most probable class that deviates from the actual ground truth.
Lastly, we deploy a gradient projection method to minimize the cross-entropy
loss on the modified forget set (i.e., learn incorrect labels for this set)
while preventing forgetting of the inverted samples. We rigorously evaluate
CovarNav on the CIFAR-10 and Vggface2 datasets, comparing our results with
recent benchmarks in the field and demonstrating the efficacy of our proposed
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13018">Attention: Large Multimodal Model is Watching your Geo-privacy. (arXiv:2311.13018v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Daoyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shuju Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Junhong Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junzhou He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a></p>
<p>Geographic privacy, a crucial aspect of personal security, often goes
unnoticed in daily activities. This paper addresses the underestimation of this
privacy in the context of increasing online data sharing and the advancements
in information gathering technologies. With the surge in the use of Large
Multimodal Models, such as GPT-4, for Open Source Intelligence (OSINT), the
potential risks associated with geographic privacy breaches have intensified.
This study highlights the criticality of these developments, focusing on their
implications for individual privacy. The primary objective is to demonstrate
the capabilities of advanced AI tools, specifically a GPT-4 based model named
"Dr. Watson," in identifying and potentially compromising geographic privacy
through online shared content. We developed "Dr. Watson" to analyze and extract
geographic information from publicly available data sources. The study involved
five experimental cases, each offering different perspectives on the tool's
application in extracting precise location data from partial images and social
media content. The experiments revealed that "Dr. Watson" could successfully
identify specific geographic details, thereby exposing the vulnerabilities in
current geo-privacy measures. These findings underscore the ease with which
geographic information can be unintentionally disclosed. The paper concludes
with a discussion on the broader implications of these findings for individuals
and the community at large. It emphasizes the urgency for enhanced awareness
and protective measures against geo-privacy leakage in the era of advanced AI
and widespread social media usage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13028">DMLR: Data-centric Machine Learning Research -- Past, Present and Future. (arXiv:2311.13028v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oala_L/0/1/0/all/0/1">Luis Oala</a>, <a href="http://arxiv.org/find/cs/1/au:+Maskey_M/0/1/0/all/0/1">Manil Maskey</a>, <a href="http://arxiv.org/find/cs/1/au:+Bat_Leah_L/0/1/0/all/0/1">Lilith Bat-Leah</a>, <a href="http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1">Alicia Parrish</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurel_N/0/1/0/all/0/1">Nezihe Merve G&#xfc;rel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1">Tzu-Sheng Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1">Rotem Dror</a>, <a href="http://arxiv.org/find/cs/1/au:+Brajovic_D/0/1/0/all/0/1">Danilo Brajovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1">Xiaozhe Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1">Max Bartolo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojas_W/0/1/0/all/0/1">William A Gaviria Rojas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hileman_R/0/1/0/all/0/1">Ryan Hileman</a>, <a href="http://arxiv.org/find/cs/1/au:+Aliment_R/0/1/0/all/0/1">Rainier Aliment</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1">Michael W. Mahoney</a>, <a href="http://arxiv.org/find/cs/1/au:+Risdal_M/0/1/0/all/0/1">Meg Risdal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1">Matthew Lease</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_D/0/1/0/all/0/1">Debojyoti Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Northcutt_C/0/1/0/all/0/1">Curtis G Northcutt</a>, <a href="http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1">Cody Coleman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hancock_B/0/1/0/all/0/1">Braden Hancock</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_B/0/1/0/all/0/1">Bernard Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1">Girmaw Abebe Tadesse</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1">Bojan Karla&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaa_A/0/1/0/all/0/1">Ahmed Alaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Dieng_A/0/1/0/all/0/1">Adji Bousso Dieng</a>, <a href="http://arxiv.org/find/cs/1/au:+Noy_N/0/1/0/all/0/1">Natasha Noy</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1">Vijay Janapa Reddi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Paritosh_P/0/1/0/all/0/1">Praveen Paritosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bollacker_K/0/1/0/all/0/1">Kurt Bollacker</a>, <a href="http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1">Lora Aroyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1">Joaquin Vanschoren</a>, <a href="http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1">Isabelle Guyon</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattson_P/0/1/0/all/0/1">Peter Mattson</a></p>
<p>Drawing from discussions at the inaugural DMLR workshop at ICML 2023 and
meetings prior, in this report we outline the relevance of community engagement
and infrastructure development for the creation of next-generation public
datasets that will advance machine learning science. We chart a path forward as
a collective effort to sustain the creation and maintenance of these datasets
and methods towards positive scientific, societal and business impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13038">Synaptic Sampling of Neural Networks. (arXiv:2311.13038v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aimone_J/0/1/0/all/0/1">James B. Aimone</a>, <a href="http://arxiv.org/find/cs/1/au:+Severa_W/0/1/0/all/0/1">William Severa</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1">J. Darby Smith</a></p>
<p>Probabilistic artificial neural networks offer intriguing prospects for
enabling the uncertainty of artificial intelligence methods to be described
explicitly in their function; however, the development of techniques that
quantify uncertainty by well-understood methods such as Monte Carlo sampling
has been limited by the high costs of stochastic sampling on deterministic
computing hardware. Emerging computing systems that are amenable to
hardware-level probabilistic computing, such as those that leverage stochastic
devices, may make probabilistic neural networks more feasible in the
not-too-distant future. This paper describes the scANN technique --
\textit{sampling (by coinflips) artificial neural networks} -- which enables
neural networks to be sampled directly by treating the weights as Bernoulli
coin flips. This method is natively well suited for probabilistic computing
techniques that focus on tunable stochastic devices, nearly matches fully
deterministic performance while also describing the uncertainty of correct and
incorrect neural network outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13046">Do we listen to what we are told? An empirical study on human behaviour during the COVID-19 pandemic: neural networks vs. regression analysis. (arXiv:2311.13046v1 [econ.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Heluo_Y/0/1/0/all/0/1">Yuxi Heluo</a>, <a href="http://arxiv.org/find/econ/1/au:+Wang_K/0/1/0/all/0/1">Kexin Wang</a>, <a href="http://arxiv.org/find/econ/1/au:+Robson_C/0/1/0/all/0/1">Charles W. Robson</a></p>
<p>In this work, we contribute the first visual open-source empirical study on
human behaviour during the COVID-19 pandemic, in order to investigate how
compliant a general population is to mask-wearing-related public-health policy.
Object-detection-based convolutional neural networks, regression analysis and
multilayer perceptrons are combined to analyse visual data of the Viennese
public during 2020. We find that mask-wearing-related government regulations
and public-transport announcements encouraged correct mask-wearing-behaviours
during the COVID-19 pandemic. Importantly, changes in announcement and
regulation contents led to heterogeneous effects on people's behaviour.
Comparing the predictive power of regression analysis and neural networks, we
demonstrate that the latter produces more accurate predictions of population
reactions during the COVID-19 pandemic. Our use of regression modelling also
allows us to unearth possible causal pathways underlying societal behaviour.
Since our findings highlight the importance of appropriate communication
contents, our results will facilitate more effective non-pharmaceutical
interventions to be developed in future. Adding to the literature, we
demonstrate that regression modelling and neural networks are not mutually
exclusive but instead complement each other.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13051">Latent Lab: Large Language Models for Knowledge Exploration. (arXiv:2311.13051v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dunnell_K/0/1/0/all/0/1">Kevin Dunnell</a>, <a href="http://arxiv.org/find/cs/1/au:+Painter_T/0/1/0/all/0/1">Trudy Painter</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoddard_A/0/1/0/all/0/1">Andrew Stoddard</a>, <a href="http://arxiv.org/find/cs/1/au:+Lippman_A/0/1/0/all/0/1">Andy Lippman</a></p>
<p>This paper investigates the potential of AI models, particularly large
language models (LLMs), to support knowledge exploration and augment human
creativity during ideation. We present "Latent Lab" an interactive tool for
discovering connections among MIT Media Lab research projects, emphasizing
"exploration" over search. The work offers insights into collaborative AI
systems by addressing the challenges of organizing, searching, and synthesizing
content. In a user study, the tool's success was evaluated based on its ability
to introduce users to an unfamiliar knowledge base, ultimately setting the
groundwork for the ongoing advancement of human-AI knowledge exploration
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13063">From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models. (arXiv:2311.13063v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Englhardt_Z/0/1/0/all/0/1">Zachary Englhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chengqian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1">Margaret E. Morris</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xuhai &quot;Orson&quot; Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chun-Cheng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">Lianhui Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Shwetak Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1">Vikram Iyer</a></p>
<p>Passively collected behavioral health data from ubiquitous sensors holds
significant promise to provide mental health professionals insights from
patient's daily lives; however, developing analysis tools to use this data in
clinical practice requires addressing challenges of generalization across
devices and weak or ambiguous correlations between the measured signals and an
individual's mental health. To address these challenges, we take a novel
approach that leverages large language models (LLMs) to synthesize clinically
useful insights from multi-sensor data. We develop chain of thought prompting
methods that use LLMs to generate reasoning about how trends in data such as
step count and sleep relate to conditions like depression and anxiety. We first
demonstrate binary depression classification with LLMs achieving accuracies of
61.1% which exceed the state of the art. While it is not robust for clinical
use, this leads us to our key finding: even more impactful and valued than
classification is a new human-AI collaboration approach in which clinician
experts interactively query these tools and combine their domain expertise and
context about the patient with AI generated reasoning to support clinical
decision-making. We find models like GPT-4 correctly reference numerical data
75% of the time, and clinician participants express strong interest in using
this approach to interpret self-tracking data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13081">Learning to Fly in Seconds. (arXiv:2311.13081v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eschmann_J/0/1/0/all/0/1">Jonas Eschmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Albani_D/0/1/0/all/0/1">Dario Albani</a>, <a href="http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1">Giuseppe Loianno</a></p>
<p>Learning-based methods, particularly Reinforcement Learning (RL), hold great
promise for streamlining deployment, enhancing performance, and achieving
generalization in the control of autonomous multirotor aerial vehicles. Deep RL
has been able to control complex systems with impressive fidelity and agility
in simulation but the simulation-to-reality transfer often brings a
hard-to-bridge reality gap. Moreover, RL is commonly plagued by prohibitively
long training times. In this work, we propose a novel asymmetric
actor-critic-based architecture coupled with a highly reliable RL-based
training paradigm for end-to-end quadrotor control. We show how curriculum
learning and a highly optimized simulator enhance sample complexity and lead to
fast training times. To precisely discuss the challenges related to
low-level/end-to-end multirotor control, we also introduce a taxonomy that
classifies the existing levels of control abstractions as well as
non-linearities and domain parameters. Our framework enables
Simulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18
seconds of training on a consumer-grade laptop as well as its deployment on
microcontrollers to control a multirotor under real-time guarantees. Finally,
our solution exhibits competitive performance in trajectory tracking, as
demonstrated through various experimental comparisons with existing
state-of-the-art control solutions using a real Crazyflie nano quadrotor. We
open source the code including a very fast multirotor dynamics simulator that
can simulate about 5 months of flight per second on a laptop GPU. The fast
training times and deployment to a cheap, off-the-shelf quadrotor lower the
barriers to entry and help democratize the research and development of these
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13087">Predict-Then-Optimize by Proxy: Learning Joint Models of Prediction and Optimization. (arXiv:2311.13087v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotary_J/0/1/0/all/0/1">James Kotary</a>, <a href="http://arxiv.org/find/cs/1/au:+Vito_V/0/1/0/all/0/1">Vincenzo Di Vito</a>, <a href="http://arxiv.org/find/cs/1/au:+Christopher_J/0/1/0/all/0/1">Jacob Christopher</a>, <a href="http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1">Pascal Van Hentenryck</a>, <a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1">Ferdinando Fioretto</a></p>
<p>Many real-world decision processes are modeled by optimization problems whose
defining parameters are unknown and must be inferred from observable data. The
Predict-Then-Optimize framework uses machine learning models to predict unknown
parameters of an optimization problem from features before solving. Recent
works show that decision quality can be improved in this setting by solving and
differentiating the optimization problem in the training loop, enabling
end-to-end training with loss functions defined directly on the resulting
decisions. However, this approach can be inefficient and requires handcrafted,
problem-specific rules for backpropagation through the optimization step. This
paper proposes an alternative method, in which optimal solutions are learned
directly from the observable features by predictive models. The approach is
generic, and based on an adaptation of the Learning-to-Optimize paradigm, from
which a rich variety of existing techniques can be employed. Experimental
evaluations show the ability of several Learning-to-Optimize methods to provide
efficient, accurate, and flexible solutions to an array of challenging
Predict-Then-Optimize problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13090">On the Limitation of Diffusion Models for Synthesizing Training Datasets. (arXiv:2311.13090v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1">Shin&#x27;ya Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fukuda_T/0/1/0/all/0/1">Takuma Fukuda</a></p>
<p>Synthetic samples from diffusion models are promising for leveraging in
training discriminative models as replications of real training datasets.
However, we found that the synthetic datasets degrade classification
performance over real datasets even when using state-of-the-art diffusion
models. This means that modern diffusion models do not perfectly represent the
data distribution for the purpose of replicating datasets for training
discriminative tasks. This paper investigates the gap between synthetic and
real samples by analyzing the synthetic samples reconstructed from real samples
through the diffusion and reverse process. By varying the time steps starting
the reverse process in the reconstruction, we can control the trade-off between
the information in the original real data and the information added by
diffusion models. Through assessing the reconstructed samples and trained
models, we found that the synthetic data are concentrated in modes of the
training data distribution as the reverse step increases, and thus, they are
difficult to cover the outer edges of the distribution. Our findings imply that
modern diffusion models are insufficient to replicate training data
distribution perfectly, and there is room for the improvement of generative
modeling in the replication of training datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13095">Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications. (arXiv:2311.13095v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Ha-Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fungwacharakorn_W/0/1/0/all/0/1">Wachara Fungwacharakorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1">Ken Satoh</a></p>
<p>Language serves as a vehicle for conveying thought, enabling communication
among individuals. The ability to distinguish between diverse concepts,
identify fairness and injustice, and comprehend a range of legal notions
fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt
to emulate human language understanding and generation, but their competency in
logical reasoning remains limited. This paper seeks to address the
philosophical question: How can we effectively teach logical reasoning to LLMs
while maintaining a deep understanding of the intricate relationship between
language and logic? By focusing on bolstering LLMs' capabilities in logical
reasoning, we aim to expand their applicability in law and other
logic-intensive disciplines. To this end, we propose a Reinforcement Learning
from Logical Feedback (RLLF) approach, which serves as a potential framework
for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation
methodology, we explore new avenues for research in this domain and contribute
to the development of LLMs capable of handling complex legal reasoning tasks
while acknowledging the fundamental connection between language and logic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13099">PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF. (arXiv:2311.13099v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yintong Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1">Tianjia Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a></p>
<p>We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13118">Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based Methodology to Analyze the Language in Online Advertisements. (arXiv:2311.13118v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1">Alejandro Rodriguez Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivas_P/0/1/0/all/0/1">Pablo Rivas</a></p>
<p>This project tackles the pressing issue of human trafficking in online C2C
marketplaces through advanced Natural Language Processing (NLP) techniques. We
introduce a novel methodology for generating pseudo-labeled datasets with
minimal supervision, serving as a rich resource for training state-of-the-art
NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and
Organized Activity Detection (OAD), we employ cutting-edge Transformer models
for analysis. A key contribution is the implementation of an interpretability
framework using Integrated Gradients, providing explainable insights crucial
for law enforcement. This work not only fills a critical gap in the literature
but also offers a scalable, machine learning-driven approach to combat human
exploitation online. It serves as a foundation for future research and
practical applications, emphasizing the role of machine learning in addressing
complex social issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13127">Toward Robust Imperceptible Perturbation against Unauthorized Text-to-image Diffusion-based Synthesis. (arXiv:2311.13127v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Chenrui Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yutong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a></p>
<p>Text-to-image diffusion models allow seamless generation of personalized
images from scant reference photos. Yet, these tools, in the wrong hands, can
fabricate misleading or harmful content, endangering individuals. To address
this problem, existing poisoning-based approaches perturb user images in an
imperceptible way to render them "unlearnable" from malicious uses. We identify
two limitations of these defending approaches: i) sub-optimal due to the
hand-crafted heuristics for solving the intractable bilevel optimization and
ii) lack of robustness against simple data transformations like Gaussian
filtering. To solve these challenges, we propose MetaCloak, which solves the
bi-level poisoning problem with a meta-learning framework with an additional
transformation sampling process to craft transferable and robust perturbation.
Specifically, we employ a pool of surrogate diffusion models to craft
transferable and model-agnostic perturbation. Furthermore, by incorporating an
additional transformation process, we design a simple denoising-error
maximization loss that is sufficient for causing transformation-robust semantic
distortion and degradation in a personalized generation. Extensive experiments
on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing
approaches. Notably, MetaCloak can successfully fool online training services
like Replicate, in a black-box manner, demonstrating the effectiveness of
MetaCloak in real-world scenarios. Our code is available at
https://github.com/liuyixin-louis/MetaCloak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13133">LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms. (arXiv:2311.13133v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1">Aditi Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Havens_S/0/1/0/all/0/1">Sam Havens</a>, <a href="http://arxiv.org/find/cs/1/au:+Dohmann_J/0/1/0/all/0/1">Jeremey Dohmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1">Alex Trott</a>, <a href="http://arxiv.org/find/cs/1/au:+Portes_J/0/1/0/all/0/1">Jacob Portes</a></p>
<p>Large Language Models are traditionally finetuned on large instruction
datasets. However recent studies suggest that small, high-quality datasets can
suffice for general purpose instruction following. This lack of consensus
surrounding finetuning best practices is in part due to rapidly diverging
approaches to LLM evaluation. In this study, we ask whether a small amount of
diverse finetuning samples can improve performance on both traditional
perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We
finetune open-source MPT-7B and MPT-30B models on instruction finetuning
datasets of various sizes ranging from 1k to 60k samples. We find that subsets
of 1k-6k instruction finetuning samples are sufficient to achieve good
performance on both (1) traditional NLP benchmarks and (2) model-based
evaluation. Finally, we show that mixing textbook-style and open-ended QA
finetuning datasets optimizes performance on both evaluation paradigms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13148">Building the Future of Responsible AI: A Pattern-Oriented Reference Architecture for Designing Large Language Model based Agents. (arXiv:2311.13148v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrer_S/0/1/0/all/0/1">Stefan Harrer</a>, <a href="http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1">Jon Whittle</a></p>
<p>Large language models (LLMs) have been widely recognized as transformative
technology due to their capabilities to understand and generate natural
language text, including plans with some limited reasoning capabilities.
LLM-based agents derive their autonomy from the capabilities of LLMs, which
enable them to autonomously break down the given goal into a set of manageable
tasks and orchestrate the task execution to fulfill the goal. Despite the huge
efforts put into building LLM-based autonomous agents, the architecture design
of the agents has not yet been systematically explored. Also, while there are
significant benefits of using autonomous agents for planning and execution,
there are serious considerations regarding responsible AI related software
quality attributes, such as security and accountability. Therefore, this paper
presents a pattern-oriented reference architecture that serves as architecture
design guidelines and enables responsible-AI-by-design when designing LLM-based
autonomous agents. We evaluate the completeness and utility of the proposed
reference architecture by mapping it to the architecture of two real-world
agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13160">Large Language Models in Education: Vision and Opportunities. (arXiv:2311.13160v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1">Wensheng Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhenlian Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiayang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jerry Chun-Wei Lin</a></p>
<p>With the rapid development of artificial intelligence technology, large
language models (LLMs) have become a hot research topic. Education plays an
important role in human social development and progress. Traditional education
faces challenges such as individual student differences, insufficient
allocation of teaching resources, and assessment of teaching effectiveness.
Therefore, the applications of LLMs in the field of digital/smart education
have broad prospects. The research on educational large models (EduLLMs) is
constantly evolving, providing new methods and approaches to achieve
personalized learning, intelligent tutoring, and educational assessment goals,
thereby improving the quality of education and the learning experience. This
article aims to investigate and summarize the application of LLMs in smart
education. It first introduces the research background and motivation of LLMs
and explains the essence of LLMs. It then discusses the relationship between
digital education and EduLLMs and summarizes the current research status of
educational large models. The main contributions are the systematic summary and
vision of the research background, motivation, and application of large models
for education (LLM4Edu). By reviewing existing research, this article provides
guidance and insights for educators, researchers, and policy-makers to gain a
deep understanding of the potential and challenges of LLM4Edu. It further
provides guidance for further advancing the development and application of
LLM4Edu, while still facing technical, ethical, and practical challenges
requiring further research and exploration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13165">Multimodal Large Language Models: A Survey. (arXiv:2311.13165v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiayang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1">Wensheng Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zefeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1">Shicheng Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a></p>
<p>The exploration of multimodal language models integrates multiple data types,
such as images, text, language, audio, and other heterogeneity. While the
latest large language models excel in text-based tasks, they often struggle to
understand and process other data types. Multimodal models address this
limitation by combining various modalities, enabling a more comprehensive
understanding of diverse data. This paper begins by defining the concept of
multimodal and examining the historical development of multimodal algorithms.
Furthermore, we introduce a range of multimodal products, focusing on the
efforts of major technology companies. A practical guide is provided, offering
insights into the technical aspects of multimodal models. Moreover, we present
a compilation of the latest algorithms and commonly used datasets, providing
researchers with valuable resources for experimentation and evaluation. Lastly,
we explore the applications of multimodal models and discuss the challenges
associated with their development. By addressing these aspects, this paper aims
to facilitate a deeper understanding of multimodal models and their potential
in various domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13169">SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape. (arXiv:2311.13169v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Hua Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kuang-Hung Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorov_I/0/1/0/all/0/1">Igor Fedorov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen-Yen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wei Wen</a></p>
<p>Neural Architecture Search (NAS) has become a widely used tool for automating
neural network design. While one-shot NAS methods have successfully reduced
computational requirements, they often require extensive training. On the other
hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate
architecture's test performance but has two limitations: (1) inability to use
the information gained as a network improves with training and (2) unreliable
performance, particularly in complex domains like RecSys, due to the
multi-modal data inputs and complex architecture configurations. To synthesize
the benefits of both methods, we introduce a "sub-one-shot" paradigm that
serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the
supernet is trained using only a small subset of the training data, a phase we
refer to as "warm-up." Within this framework, we present SiGeo, a proxy founded
on a novel theoretical framework that connects the supernet warm-up with the
efficacy of the proxy. Extensive experiments have shown that SiGeo, with the
benefit of warm-up, consistently outperforms state-of-the-art NAS proxies on
various established NAS benchmarks. When a supernet is warmed up, it can
achieve comparable performance to weight-sharing one-shot NAS methods, but with
a significant reduction ($\sim 60$\%) in computational costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13171">ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization. (arXiv:2311.13171v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1">Prateek Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1">Leshem Choshen</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Parameter-efficient fine-tuning (PEFT) techniques make it possible to
efficiently adapt a language model to create "expert" models that specialize to
new tasks or domains. Recent techniques in model merging and compositional
generalization leverage these expert models by dynamically composing modules to
improve zero/few-shot generalization. Despite the efficiency of PEFT methods,
the size of expert models can make it onerous to retrieve expert models per
query over high-latency networks like the Internet or serve multiple experts on
a single GPU. To address these issues, we present ComPEFT, a novel method for
compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT
employs sparsification and ternary quantization to reduce the size of the PEFT
module without performing any additional retraining while preserving or
enhancing model performance. In extensive evaluation across T5, T0, and
LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression
ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale -
stronger models exhibit higher compressibility and better performance. For
example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on
MMLU with a storage size reduction of up to 26x. In addition, we show that the
compressed experts produced by ComPEFT maintain few-shot compositional
generalization capabilities, facilitate efficient communication and
computation, and exhibit enhanced performance when merged. Lastly, we provide
an analysis of different method components, compare it with other PEFT methods,
and test ComPEFT's efficacy for compressing the residual of full-finetuning.
Our code is available at https://github.com/prateeky2806/compeft.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13188">Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation. (arXiv:2311.13188v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1">Chung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taesan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1">Taekyoon Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Junui Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yelim Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1">Mincheol Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyunam Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1">Sungil Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1">Hyungjun Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minsung Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1">Jaegul Choo</a></p>
<p>This paper investigates Cross-Domain Sequential Recommendation (CDSR), a
promising method that uses information from multiple domains (more than three)
to generate accurate and diverse recommendations, and takes into account the
sequential nature of user interactions. The effectiveness of these systems
often depends on the complex interplay among the multiple domains. In this
dynamic landscape, the problem of negative transfer arises, where heterogeneous
knowledge between dissimilar domains leads to performance degradation due to
differences in user preferences across these domains. As a remedy, we propose a
new CDSR framework that addresses the problem of negative transfer by assessing
the extent of negative transfer from one domain to another and adaptively
assigning low weight values to the corresponding prediction losses. To this
end, the amount of negative transfer is estimated by measuring the marginal
contribution of each domain to model performance based on a cooperative game
theory. In addition, a hierarchical contrastive learning approach that
incorporates information from the sequence of coarse-level categories into that
of fine-level categories (e.g., item level) when implementing contrastive
learning was developed to mitigate negative transfer. Despite the potentially
low relevance between domains at the fine-level, there may be higher relevance
at the category level due to its generalised and broader preferences. We show
that our model is superior to prior works in terms of model performance on two
real-world datasets across ten different domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13206">Breast Cancer classification by adaptive weighted average ensemble of previously trained models. (arXiv:2311.13206v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farea_M/0/1/0/all/0/1">Mosab S. M. Farea</a>, <a href="http://arxiv.org/find/cs/1/au:+chen_z/0/1/0/all/0/1">zhe chen</a></p>
<p>Breast cancer is a serious disease that inflicts millions of people each
year, and the number of cases is increasing. Early detection is the best way to
reduce the impact of the disease. Researchers have developed many techniques to
detect breast cancer, including the use of histopathology images in CAD
systems. This research proposes a technique that combine already fully trained
model using adaptive average ensemble, this is different from the literature
which uses average ensemble before training and the average ensemble is trained
simultaneously. Our approach is different because it used adaptive average
ensemble after training which has increased the performance of evaluation
metrics. It averages the outputs of every trained model, and every model will
have weight according to its accuracy. The accuracy in the adaptive weighted
ensemble model has achieved 98% where the accuracy has increased by 1 percent
which is better than the best participating model in the ensemble which was
97%. Also, it decreased the numbers of false positive and false negative and
enhanced the performance metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13213">Artificial Intelligence in the Service of Entrepreneurial Finance: Knowledge Structure and the Foundational Algorithmic Paradigm. (arXiv:2311.13213v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kudelic_R/0/1/0/all/0/1">Robert Kudeli&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Smaguc_T/0/1/0/all/0/1">Tamara &#x160;maguc</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_S/0/1/0/all/0/1">Sherry Robinson</a></p>
<p>While the application of Artificial Intelligence in Finance has a long
tradition, its potential in Entrepreneurship has been intensively explored only
recently. In this context, Entrepreneurial Finance is a particularly fertile
ground for future Artificial Intelligence proliferation. To support the latter,
the study provides a bibliometric review of Artificial Intelligence
applications in (1) entrepreneurial finance literature, and (2) corporate
finance literature with implications for Entrepreneurship. Rigorous search and
screening procedures of the scientific database Web of Science Core Collection
resulted in the identification of 1890 relevant journal articles subjected to
analysis. The bibliometric analysis gives a rich insight into the knowledge
field's conceptual, intellectual, and social structure, indicating nascent and
underdeveloped research directions. As far as we were able to identify, this is
the first study to map and bibliometrically analyze the academic field
concerning the relationship between Artificial Intelligence, Entrepreneurship,
and Finance, and the first review that deals with Artificial Intelligence
methods in Entrepreneurship. According to the results, Artificial Neural
Network, Deep Neural Network and Support Vector Machine are highly represented
in almost all identified topic niches. At the same time, applying Topic
Modeling, Fuzzy Neural Network and Growing Hierarchical Self-organizing Map is
quite rare. As an element of the research, and before final remarks, the
article deals as well with a discussion of certain gaps in the relationship
between Computer Science and Economics. These gaps do represent problems in the
application of Artificial Intelligence in Economic Science. As a way to at
least in part remedy this situation, the foundational paradigm and the bespoke
demonstration of the Monte Carlo randomized algorithm are presented.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13226">Robot at the Mirror: Learning to Imitate via Associating Self-supervised Models. (arXiv:2311.13226v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lucny_A/0/1/0/all/0/1">Andrej L&#xfa;&#x10d;ny</a>, <a href="http://arxiv.org/find/cs/1/au:+Malinovska_K/0/1/0/all/0/1">Krist&#xed;na Malinovsk&#xe1;</a>, <a href="http://arxiv.org/find/cs/1/au:+Farkas_I/0/1/0/all/0/1">Igor Farka&#x161;</a></p>
<p>We introduce an approach to building a custom model from ready-made
self-supervised models via their associating instead of training and
fine-tuning. We demonstrate it with an example of a humanoid robot looking at
the mirror and learning to detect the 3D pose of its own body from the image it
perceives. To build our model, we first obtain features from the visual input
and the postures of the robot's body via models prepared before the robot's
operation. Then, we map their corresponding latent spaces by a sample-efficient
robot's self-exploration at the mirror. In this way, the robot builds the
solicited 3D pose detector, which quality is immediately perfect on the
acquired samples instead of obtaining the quality gradually. The mapping, which
employs associating the pairs of feature vectors, is then implemented in the
same way as the key-value mechanism of the famous transformer models. Finally,
deploying our model for imitation to a simulated robot allows us to study, tune
up, and systematically evaluate its hyperparameters without the involvement of
the human counterpart, advancing our previous research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13230">Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus. (arXiv:2311.13230v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1">Lin Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qipeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Cheng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chenghu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinbing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1">Luoyi Fu</a></p>
<p>Large Language Models (LLMs) have gained significant popularity for their
impressive performance across diverse fields. However, LLMs are prone to
hallucinate untruthful or nonsensical outputs that fail to meet user
expectations in many real-world applications. Existing works for detecting
hallucinations in LLMs either rely on external knowledge for reference
retrieval or require sampling multiple responses from the LLM for consistency
verification, making these methods costly and inefficient. In this paper, we
propose a novel reference-free, uncertainty-based method for detecting
hallucinations in LLMs. Our approach imitates human focus in factuality
checking from three aspects: 1) focus on the most informative and important
keywords in the given text; 2) focus on the unreliable tokens in historical
context which may lead to a cascade of hallucinations; and 3) focus on the
token properties such as token type and token frequency. Experimental results
on relevant datasets demonstrate the effectiveness of our proposed method,
which achieves state-of-the-art performance across all the evaluation metrics
and eliminates the need for additional information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13231">Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model. (arXiv:2311.13231v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1">Jian Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiafei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1">Chunjiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qimai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Weihan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaolong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiu Li</a></p>
<p>Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13233">A Survey of Adversarial CAPTCHAs on its History, Classification and Generation. (arXiv:2311.13233v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zisheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qiao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">F. Richard Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Leung_V/0/1/0/all/0/1">Victor C. M. Leung</a></p>
<p>Completely Automated Public Turing test to tell Computers and Humans Apart,
short for CAPTCHA, is an essential and relatively easy way to defend against
malicious attacks implemented by bots. The security and usability trade-off
limits the use of massive geometric transformations to interfere deep model
recognition and deep models even outperformed humans in complex CAPTCHAs. The
discovery of adversarial examples provides an ideal solution to the security
and usability trade-off by integrating adversarial examples and CAPTCHAs to
generate adversarial CAPTCHAs that can fool the deep models. In this paper, we
extend the definition of adversarial CAPTCHAs and propose a classification
method for adversarial CAPTCHAs. Then we systematically review some commonly
used methods to generate adversarial examples and methods that are successfully
used to generate adversarial CAPTCHAs. Also, we analyze some defense methods
that can be used to defend adversarial CAPTCHAs, indicating potential threats
to adversarial CAPTCHAs. Finally, we discuss some possible future research
directions for adversarial CAPTCHAs at the end of this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13234">TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer. (arXiv:2311.13234v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Huimin Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kunle Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1">Kaiyuan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jin Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1">Haochao Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuozhu Liu</a></p>
<p>Optical Intraoral Scanners (IOS) are widely used in digital dentistry to
provide detailed 3D information of dental crowns and the gingiva. Accurate 3D
tooth segmentation in IOSs is critical for various dental applications, while
previous methods are error-prone at complicated boundaries and exhibit
unsatisfactory results across patients. In this paper, we propose TSegFormer
which captures both local and global dependencies among different teeth and the
gingiva in the IOS point clouds with a multi-task 3D transformer architecture.
Moreover, we design a geometry-guided loss based on a novel point curvature to
refine boundaries in an end-to-end manner, avoiding time-consuming
post-processing to reach clinically applicable segmentation. In addition, we
create a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of
our knowledge. The experimental results demonstrate that our TSegFormer
consistently surpasses existing state-of-the-art baselines. The superiority of
TSegFormer is corroborated by extensive analysis, visualizations and real-world
clinical applicability tests. Our code is available at
https://github.com/huiminxiong/TSegFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13254">DA-STC: Domain Adaptive Video Semantic Segmentation via Spatio-Temporal Consistency. (arXiv:2311.13254v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Gaochang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1">Tianyou Chai</a></p>
<p>Video semantic segmentation is a pivotal aspect of video representation
learning. However, significant domain shifts present a challenge in effectively
learning invariant spatio-temporal features across the labeled source domain
and unlabeled target domain for video semantic segmentation. To solve the
challenge, we propose a novel DA-STC method for domain adaptive video semantic
segmentation, which incorporates a bidirectional multi-level spatio-temporal
fusion module and a category-aware spatio-temporal feature alignment module to
facilitate consistent learning for domain-invariant features. Firstly, we
perform bidirectional spatio-temporal fusion at the image sequence level and
shallow feature level, leading to the construction of two fused intermediate
video domains. This prompts the video semantic segmentation model to
consistently learn spatio-temporal features of shared patch sequences which are
influenced by domain-specific contexts, thereby mitigating the feature gap
between the source and target domain. Secondly, we propose a category-aware
feature alignment module to promote the consistency of spatio-temporal
features, facilitating adaptation to the target domain. Specifically, we
adaptively aggregate the domain-specific deep features of each category along
spatio-temporal dimensions, which are further constrained to achieve
cross-domain intra-class feature alignment and inter-class feature separation.
Extensive experiments demonstrate the effectiveness of our method, which
achieves state-of-the-art mIOUs on multiple challenging benchmarks.
Furthermore, we extend the proposed DA-STC to the image domain, where it also
exhibits superior performance for domain adaptive semantic segmentation. The
source code and models will be made available at
\url{https://github.com/ZHE-SAPI/DA-STC}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13262">The Rise of Creative Machines: Exploring the Impact of Generative AI. (arXiv:2311.13262v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1">Saad Shaikh</a>, <a href="http://arxiv.org/find/cs/1/au:+bendre_R/0/1/0/all/0/1">Rajat bendre</a>, <a href="http://arxiv.org/find/cs/1/au:+Mhaske_S/0/1/0/all/0/1">Sakshi Mhaske</a></p>
<p>This study looks at how generative artificial intelligence (AI) can
revolutionize marketing, product development, and research. It discusses the
latest developments in the field, easy-to-use resources, and moral and social
hazards. In addition to addressing mitigating techniques for issues like
prejudice and disinformation, the debate emphasizes the significance of
responsible development through continual stakeholder communication and ethical
principles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13265">Improved identification accuracy in equation learning via comprehensive $\boldsymbol{R^2}$-elimination and Bayesian model selection. (arXiv:2311.13265v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Nickelsen_D/0/1/0/all/0/1">Daniel Nickelsen</a>, <a href="http://arxiv.org/find/stat/1/au:+Bah_B/0/1/0/all/0/1">Bubacarr Bah</a></p>
<p>In the field of equation learning, exhaustively considering all possible
equations derived from a basis function dictionary is infeasible. Sparse
regression and greedy algorithms have emerged as popular approaches to tackle
this challenge. However, the presence of multicollinearity poses difficulties
for sparse regression techniques, and greedy steps may inadvertently exclude
terms of the true equation, leading to reduced identification accuracy. In this
article, we present an approach that strikes a balance between
comprehensiveness and efficiency in equation learning. Inspired by stepwise
regression, our approach combines the coefficient of determination, $R^2$, and
the Bayesian model evidence, $p(\boldsymbol y|\mathcal M)$, in a novel way. Our
procedure is characterized by a comprehensive search with just a minor
reduction of the model space at each iteration step. With two flavors of our
approach and the adoption of $p(\boldsymbol y|\mathcal M)$ for bi-directional
stepwise regression, we present a total of three new avenues for equation
learning. Through three extensive numerical experiments involving random
polynomials and dynamical systems, we compare our approach against four
state-of-the-art methods and two standard approaches. The results demonstrate
that our comprehensive search approach surpasses all other methods in terms of
identification accuracy. In particular, the second flavor of our approach
establishes an efficient overfitting penalty solely based on $R^2$, which
achieves highest rates of exact equation recovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13267">FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem in Federated Learning. (arXiv:2311.13267v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seongyoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Federated Learning (FL) is a collaborative method for training models while
preserving data privacy in decentralized settings. However, FL encounters
challenges related to data heterogeneity, which can result in performance
degradation. In our study, we observe that as data heterogeneity increases,
feature representation in the FedAVG model deteriorates more significantly
compared to classifier weight. Additionally, we observe that as data
heterogeneity increases, the gap between higher feature norms for observed
classes, obtained from local models, and feature norms of unobserved classes
widens, in contrast to the behavior of classifier weight norms. This widening
gap extends to encompass the feature norm disparities between local and the
global models. To address these issues, we introduce Federated Averaging with
Feature Normalization Update (FedFN), a straightforward learning method. We
demonstrate the superior performance of FedFN through extensive experiments,
even when applied to pretrained ResNet18. Subsequently, we confirm the
applicability of FedFN to foundation models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13286">Algorithmic Transparency and Manipulation. (arXiv:2311.13286v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klenk_M/0/1/0/all/0/1">Michael Klenk</a></p>
<p>A series of recent papers raises worries about the manipulative potential of
algorithmic transparency. But while the concern is apt and relevant, it is
based on a fraught understanding of manipulation. Therefore, this paper draws
attention to the indifference view of manipulation, which explains better than
the vulnerability view why algorithmic transparency has manipulative potential.
The paper also raises pertinent research questions for future studies of
manipulation in the context of algorithmic transparency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13294">Probabilistic Inference in Reinforcement Learning Done Right. (arXiv:2311.13294v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tarbouriech_J/0/1/0/all/0/1">Jean Tarbouriech</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1">Tor Lattimore</a>, <a href="http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1">Brendan O&#x27;Donoghue</a></p>
<p>A popular perspective in Reinforcement learning (RL) casts the problem as
probabilistic inference on a graphical model of the Markov decision process
(MDP). The core object of study is the probability of each state-action pair
being visited under the optimal policy. Previous approaches to approximate this
quantity can be arbitrarily poor, leading to algorithms that do not implement
genuine statistical inference and consequently do not perform well in
challenging problems. In this work, we undertake a rigorous Bayesian treatment
of the posterior probability of state-action optimality and clarify how it
flows through the MDP. We first reveal that this quantity can indeed be used to
generate a policy that explores efficiently, as measured by regret.
Unfortunately, computing it is intractable, so we derive a new variational
Bayesian approximation yielding a tractable convex optimization problem and
establish that the resulting policy also explores efficiently. We call our
approach VAPOR and show that it has strong connections to Thompson sampling,
K-learning, and maximum entropy exploration. We conclude with some experiments
demonstrating the performance advantage of a deep RL version of VAPOR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13326">Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1">Woosung Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1">Insu Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yuntae Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Gimin Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woo Chang Kim</a></p>
<p>Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13335">Quantum learning and essential cognition under the traction of meta-characteristics in an open world. (arXiv:2311.13335v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Changlin Song</a></p>
<p>Artificial intelligence has made significant progress in the Close World
problem, being able to accurately recognize old knowledge through training and
classification. However, AI faces significant challenges in the Open World
problem, as it involves a new and unknown exploration journey. AI is not
inherently proactive in exploration, and its challenge lies in not knowing how
to approach and adapt to the unknown world. How do humans acquire knowledge of
the unknown world. Humans identify new knowledge through intrinsic cognition.
In the process of recognizing new colors, the cognitive cues are different from
known color features and involve hue, saturation, brightness, and other
characteristics. When AI encounters objects with different features in the new
world, it faces another challenge: where are the distinguishing features
between influential features of new and old objects? AI often mistakes a new
world's brown bear for a known dog because it has not learned the differences
in feature distributions between knowledge systems. This is because things in
the new and old worlds have different units and dimensions for their features.
This paper proposes an open-world model and elemental feature system that
focuses on fundamentally recognizing the distribution differences in objective
features between the new and old worlds. The quantum tunneling effect of
learning ability in the new and old worlds is realized through the tractive
force of meta-characteristic. The outstanding performance of the model system
in learning new knowledge (using pedestrian re-identification datasets as an
example) demonstrates that AI has acquired the ability to recognize the new
world with an accuracy of $96.71\%$ at most and has gained the capability to
explore new knowledge, similar to humans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13341">Learning principle and mathematical realization of the learning mechanism in the brain. (arXiv:2311.13341v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katayose_T/0/1/0/all/0/1">Taisuke Katayose</a></p>
<p>While deep learning has achieved remarkable success, there is no clear
explanation about why it works so well. In order to discuss this question
quantitatively, we need a mathematical framework that explains what learning is
in the first place. After several considerations, we succeeded in constructing
a mathematical framework that can provide a unified understanding of all types
of learning, including deep learning and learning in the brain. We call it
learning principle, and it follows that all learning is equivalent to
estimating the probability of input data. We not only derived this principle,
but also mentioned its application to actual machine learning models. For
example, we found that conventional supervised learning is equivalent to
estimating conditional probabilities, and succeeded in making supervised
learning more effective and generalized. We also proposed a new method of
defining the values of estimated probability using differentiation, and showed
that unsupervised learning can be performed on arbitrary dataset without any
prior knowledge. Namely, this method is a general-purpose machine learning in
the true sense. Moreover, we succeeded in describing the learning mechanism in
the brain by considering the time evolution of a fully or partially connected
model and applying this new method. The learning principle provides solutions
to many unsolved problems in deep learning and cognitive neuroscience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13350">Fact-based Court Judgment Prediction. (arXiv:2311.13350v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nigam_S/0/1/0/all/0/1">Shubham Kumar Nigam</a>, <a href="http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1">Aniket Deroy</a></p>
<p>This extended abstract extends the research presented in "ILDC for CJPE:
Indian Legal Documents Corpus for Court Judgment Prediction and Explanation"
\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within
the context of Indian legal documents. We introduce two distinct problem
variations: one based solely on facts, and another combining facts with rulings
from lower courts (RLC). Our research aims to enhance early-phase case outcome
prediction, offering significant benefits to legal professionals and the
general public. The results, however, indicated a performance decline compared
to the original ILDC for CJPE study, even after implementing various weightage
schemes in our DELSumm algorithm. Additionally, using only facts for legal
judgment prediction with different transformer models yielded results inferior
to the state-of-the-art outcomes reported in the "ILDC for CJPE" study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13356">Uncertainty Estimation in Multi-Agent Distributed Learning. (arXiv:2311.13356v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Radchenko_G/0/1/0/all/0/1">Gleb Radchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Fill_V/0/1/0/all/0/1">Victoria Andrea Fill</a></p>
<p>Traditionally, IoT edge devices have been perceived primarily as low-power
components with limited capabilities for autonomous operations. Yet, with
emerging advancements in embedded AI hardware design, a foundational shift
paves the way for future possibilities. Thus, the aim of the KDT NEUROKIT2E
project is to establish a new open-source framework to further facilitate AI
applications on edge devices by developing new methods in quantization,
pruning-aware training, and sparsification. These innovations hold the
potential to expand the functional range of such devices considerably, enabling
them to manage complex Machine Learning (ML) tasks utilizing local resources
and laying the groundwork for innovative learning approaches.
</p>
<p>In the context of 6G's transformative potential, distributed learning among
independent agents emerges as a pivotal application, attributed to 6G networks'
support for ultra-reliable low-latency communication, enhanced data rates, and
advanced edge computing capabilities.
</p>
<p>Our research focuses on the mechanisms and methodologies that allow edge
network-enabled agents to engage in collaborative learning in distributed
environments. Particularly, one of the key issues within distributed
collaborative learning is determining the degree of confidence in the learning
results, considering the spatio-temporal locality of data sets perceived by
independent agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13361">Applying Large Language Models to Power Systems: Potential Security Threats. (arXiv:2311.13361v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1">Jiaqi Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1">Gaoqi Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Huan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guolong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junhua Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1">Fushuan Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhao Yang Dong</a></p>
<p>Applying large language models (LLMs) to power systems presents a promising
avenue for enhancing decision-making and operational efficiency. However, this
action may also incur potential security threats, which have not been fully
recognized so far. To this end, this letter analyzes potential threats incurred
by applying LLMs to power systems, emphasizing the need for urgent research and
development of countermeasures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13373">Large Language Model is a Good Policy Teacher for Training Reinforcement Learning Agents. (arXiv:2311.13373v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zihao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Bin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a></p>
<p>Recent studies have shown that Large Language Models (LLMs) can be utilized
for solving complex sequential decision-making tasks by providing high-level
instructions. However, LLM-based agents face limitations in real-time dynamic
environments due to their lack of specialization in solving specific target
problems. Moreover, the deployment of such LLM-based agents is both costly and
time-consuming in practical scenarios. In this paper, we introduce a novel
framework that addresses these challenges by training a smaller scale
specialized student agent using instructions from an LLM-based teacher agent.
By leveraging guided actions provided by the teachers, the prior knowledge of
the LLM is distilled into the local student model. Consequently, the student
agent can be trained with significantly less data. Furthermore, subsequent
training with environment feedback empowers the student agents to surpass the
capabilities of their teachers. We conducted experiments on three challenging
MiniGrid environments to evaluate the effectiveness of our framework. The
results demonstrate that our approach enhances sample efficiency and achieves
superior performance compared to baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13379">Deriving Comprehensible Theories from Probabilistic Circuits. (arXiv:2311.13379v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bocklandt_S/0/1/0/all/0/1">Sieben Bocklandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1">Wannes Meert</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanderstraeten_K/0/1/0/all/0/1">Koen Vanderstraeten</a>, <a href="http://arxiv.org/find/cs/1/au:+Pijpops_W/0/1/0/all/0/1">Wouter Pijpops</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaspers_K/0/1/0/all/0/1">Kurt Jaspers</a></p>
<p>The field of Explainable AI (XAI) is seeking to shed light on the inner
workings of complex AI models and uncover the rationale behind their decisions.
One of the models gaining attention are probabilistic circuits (PCs), which are
a general and unified framework for tractable probabilistic models that support
efficient computation of various probabilistic queries. Probabilistic circuits
guarantee inference that is polynomial in the size of the circuit. In this
paper, we improve the explainability of probabilistic circuits by computing a
comprehensible, readable logical theory that covers the high-density regions
generated by a PC. To achieve this, pruning approaches based on generative
significance are used in a new method called PUTPUT (Probabilistic circuit
Understanding Through Pruning Underlying logical Theories). The method is
applied to a real world use case where music playlists are automatically
generated and expressed as readable (database) queries. Evaluation shows that
this approach can effectively produce a comprehensible logical theory that
describes the high-density regions of a PC and outperforms state of the art
methods when exploring the performance-comprehensibility trade-off.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13381">Confidant: Customizing Transformer-based LLMs via Collaborative Edge Training. (arXiv:2311.13381v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuxuan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qianqian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1">Yuanchao Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shibo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiming Chen</a></p>
<p>Transformer-based large language models (LLMs) have demonstrated impressive
capabilities in a variety of natural language processing (NLP) tasks.
Nonetheless, it is challenging to deploy and fine-tune LLMs on mobile edge
devices with limited computing, memory, and energy budgets. In this paper, we
propose Confidant, a multi-backend collaborative training framework for
customizing state-of-the-art LLMs on commodity mobile devices like smartphones.
Confidant partitions an LLM into several sub-models so that each fits into a
mobile device's memory. A pipeline parallel training mechanism is further
developed to ensure fast and efficient distributed training. In addition, we
propose a novel backend scheduler to allocate different attention heads to
heterogeneous compute hardware, including mobile CPU and GPUs, to maximize the
compute resource utilization on each edge device. Our preliminary experimental
results show that Confidant achieves at most 45.3% memory reduction and 8.03x
inference speedup in practical settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13414">From Images to Connections: Can DQN with GNNs learn the Strategic Game of Hex?. (arXiv:2311.13414v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1">Yannik Keller</a>, <a href="http://arxiv.org/find/cs/1/au:+Bluml_J/0/1/0/all/0/1">Jannis Bl&#xfc;ml</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudhakaran_G/0/1/0/all/0/1">Gopika Sudhakaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>The gameplay of strategic board games such as chess, Go and Hex is often
characterized by combinatorial, relational structures -- capturing distinct
interactions and non-local patterns -- and not just images. Nonetheless, most
common self-play reinforcement learning (RL) approaches simply approximate
policy and value functions using convolutional neural networks (CNN). A key
feature of CNNs is their relational inductive bias towards locality and
translational invariance. In contrast, graph neural networks (GNN) can encode
more complicated and distinct relational structures. Hence, we investigate the
crucial question: Can GNNs, with their ability to encode complex connections,
replace CNNs in self-play reinforcement learning? To this end, we do a
comparison with Hex -- an abstract yet strategically rich board game -- serving
as our experimental platform. Our findings reveal that GNNs excel at dealing
with long range dependency situations in game states and are less prone to
overfitting, but also showing a reduced proficiency in discerning local
patterns. This suggests a potential paradigm shift, signaling the use of
game-specific structures to reshape self-play reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13435">PG-Video-LLaVA: Pixel Grounding Large Video-Language Models. (arXiv:2311.13435v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Munasinghe_S/0/1/0/all/0/1">Shehan Munasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Thushara_R/0/1/0/all/0/1">Rusiru Thushara</a>, <a href="http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1">Muhammad Maaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1">Hanoona Abdul Rasheed</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Khan</a></p>
<p>Extending image-based Large Multimodal Models (LMM) to videos is challenging
due to the inherent complexity of video data. The recent approaches extending
image-based LMM to videos either lack the grounding capabilities (e.g.,
VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for
better video understanding (e.g., Video-ChatGPT). Addressing these gaps, we
propose Video-LLaVA, the first LMM with pixel-level grounding capability,
integrating audio cues by transcribing them into text to enrich video-context
understanding. Our framework uses an off-the-shelf tracker and a novel
grounding module, enabling it to spatially and temporally localize objects in
videos following user instructions. We evaluate Video-LLaVA using video-based
generative and question-answering benchmarks and introduce new benchmarks
specifically designed to measure prompt-based object grounding performance in
videos. Further, we propose the use of Vicuna over GPT-3.5, as utilized in
Video-ChatGPT, for video-based conversation benchmarking, ensuring
reproducibility of results which is a concern with the proprietary nature of
GPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its
advantages to the video domain, delivering promising gains on video-based
conversation and grounding tasks. Project Page:
https://github.com/mbzuai-oryx/Video-LLaVA
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13443">Guided Flows for Generative Modeling and Decision Making. (arXiv:2311.13443v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qinqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1">Matt Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaul_N/0/1/0/all/0/1">Neta Shaul</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1">Yaron Lipman</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ricky T. Q. Chen</a></p>
<p>Classifier-free guidance is a key component for improving the performance of
conditional generative models for many downstream tasks. It drastically
improves the quality of samples produced, but has so far only been used for
diffusion models. Flow Matching (FM), an alternative simulation-free approach,
trains Continuous Normalizing Flows (CNFs) based on regressing vector fields.
It remains an open question whether classifier-free guidance can be performed
for Flow Matching models, and to what extent does it improve performance. In
this paper, we explore the usage of Guided Flows for a variety of downstream
applications involving conditional image generation, speech synthesis, and
reinforcement learning. In particular, we are the first to apply flow models to
the offline reinforcement learning setting. We also show that Guided Flows
significantly improves the sample quality in image generation and zero-shot
text-to-speech synthesis, and can make use of drastically low amounts of
computation without affecting the agent's overall performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13455">Generation of Explanations for Logic Reasoning. (arXiv:2311.13455v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yanyi Pu</a></p>
<p>This thesis delves into a fortiori arguments in deductive reasoning,
underscoring their relevance in various domains such as law, philosophy, and
artificial intelligence. The research is centred on employing GPT-3.5-turbo to
automate the analysis of these arguments, with a focus on understanding
intricate reasoning processes, generating clear and coherent explanations, and
creating novel arguments. The methodology encompasses a series of tasks
including detailed reasoning, interpretation, and the augmentation of a
fortiori arguments. It involves meticulously identifying these arguments in
diverse contexts, differentiating comparative elements, and categorizing them
based on their logical structure.
</p>
<p>Extensive experiments reveals the challenges encountered by GPT-3.5-turbo in
accurately detecting and classifying a fortiori arguments. Nevertheless, the
model demonstrates a performance that rivals specialized models, particularly
in extracting key components and interpreting underlying properties. The
integration of external information into the model's processing significantly
elevates the quality of the generated explanations. Additionally, the model
exhibits a noteworthy capability in augmenting arguments, thus contributing to
the enrichment of the data set.
</p>
<p>Despite facing certain limitations, this thesis makes significant
contributions to the fields of artificial intelligence and logical reasoning.
It introduces novel methodologies, establishes a rigorous evaluation framework,
and provides deep insights that set the stage for future advancements in
automated logical reasoning. The findings and methodologies presented herein
not only underscore the potential of AI in complex reasoning tasks but also
highlight areas for future research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13472">Complexity-Guided Curriculum Learning for Text Graphs. (arXiv:2311.13472v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vakil_N/0/1/0/all/0/1">Nidhi Vakil</a>, <a href="http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1">Hadi Amiri</a></p>
<p>Curriculum learning provides a systematic approach to training. It refines
training progressively, tailors training to task requirements, and improves
generalization through exposure to diverse examples. We present a curriculum
learning approach that builds on existing knowledge about text and graph
complexity formalisms for training with text graph data. The core part of our
approach is a novel data scheduler, which employs "spaced repetition" and
complexity formalisms to guide the training process. We demonstrate the
effectiveness of the proposed approach on several text graph tasks and graph
neural network architectures. The proposed model gains more and uses less data;
consistently prefers text over graph complexity indices throughout training,
while the best curricula derived from text and graph complexity indices are
equally effective; and it learns transferable curricula across GNN models and
datasets. In addition, we find that both node-level (local) and graph-level
(global) graph complexity indices, as well as shallow and traditional text
complexity indices play a crucial role in effective curriculum learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13502">Bitformer: An efficient Transformer with bitwise operation-based attention for Big Data Analytics at low-cost low-precision devices. (arXiv:2311.13502v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_G/0/1/0/all/0/1">Gaoxiang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junkai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoying Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yongxin Zhu</a></p>
<p>In the current landscape of large models, the Transformer stands as a
cornerstone, playing a pivotal role in shaping the trajectory of modern models.
However, its application encounters challenges attributed to the substantial
computational intricacies intrinsic to its attention mechanism. Moreover, its
reliance on high-precision floating-point operations presents specific hurdles,
particularly evident in computation-intensive scenarios such as edge computing
environments. These environments, characterized by resource-constrained devices
and a preference for lower precision, necessitate innovative solutions.
</p>
<p>To tackle the exacting data processing demands posed by edge devices, we
introduce the Bitformer model, an inventive extension of the Transformer
paradigm. Central to this innovation is a novel attention mechanism that
adeptly replaces conventional floating-point matrix multiplication with bitwise
operations. This strategic substitution yields dual advantages. Not only does
it maintain the attention mechanism's prowess in capturing intricate long-range
information dependencies, but it also orchestrates a profound reduction in the
computational complexity inherent in the attention operation. The transition
from an $O(n^2d)$ complexity, typical of floating-point operations, to an
$O(n^2T)$ complexity characterizing bitwise operations, substantiates this
advantage. Notably, in this context, the parameter $T$ remains markedly smaller
than the conventional dimensionality parameter $d$.
</p>
<p>The Bitformer model in essence endeavors to reconcile the indomitable
requirements of modern computing landscapes with the constraints posed by edge
computing scenarios. By forging this innovative path, we bridge the gap between
high-performing models and resource-scarce environments, thus unveiling a
promising trajectory for further advancements in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13534">LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1">Shitao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peitian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xingrun Xing</a></p>
<p>The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose a novel method which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging (namely LM-Cocktail), where the fine-tuned language model
is merged with the pre-trained base model or the peer models from other domains
through weighted average. Despite simplicity, LM-Cocktail is surprisingly
effective: the resulted model is able to achieve a strong empirical performance
in the whole scope of general tasks while preserving a superior capacity in its
targeted domain. We conduct comprehensive experiments with LLama and BGE model
on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13538">Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhicheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yinya Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jing Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jing Tang</a></p>
<p>Existing work has found that the prompt engineering heavily influences the
performance of large language models (LLMs). Chain-of-thought (CoT), as a
popular prompt engineering technique, prompted LLMs using in-context examples
with reasoning steps. In current studies, the few-shot examples of CoT are
generally handcrafted by humans. However, how the text style of in-context
examples influence the outputs of LLMs still remains under-explored. This paper
presents a novel and effective approach, named \textbf{AlignCoT}, to improve
the reasoning capability of LLMs by aligning the in-context examples with the
native style of LLMs. ``Native'' refers to the inherent characteristic style of
LLMs which can be probed by original zero-shot scenarios. AlignCoT is
orthogonal to other prompt engineering methods, making it easy to combine with
state-of-the-art techniques to further improve the LLMs' performance. We
conduct extensive and comprehensive experiments on several benchmarks. The
empirical results demonstrate that our AlignCoTsignificantly improves
performance over the carefully handcrafted in-context examples. For instance,
with GPT-3.5-turbo, we observed a +2.5\% improvement on GSM8K. Furthermore, our
AlignCoT consistently improve the performance when combined with other
state-of-the-art prompt engineering methods. The source code and dataset will
be available at
\href{https://github.com/yangzhch6/AlignCoT}{https://github.com/yangzhch6/AlignCoT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13541">Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nahshan_Y/0/1/0/all/0/1">Yury Nahshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kampeas_J/0/1/0/all/0/1">Joseph Kampeas</a>, <a href="http://arxiv.org/find/cs/1/au:+Haleva_E/0/1/0/all/0/1">Emir Haleva</a></p>
<p>Transformer models have achieved remarkable results in a wide range of
applications. However, their scalability is hampered by the quadratic time and
memory complexity of the self-attention mechanism concerning the sequence
length. This limitation poses a substantial obstacle when dealing with long
documents or high-resolution images. In this work, we study the self-attention
mechanism by analyzing the distribution of the attention matrix and its
concentration ability. Furthermore, we propose instruments to measure these
quantities and introduce a novel self-attention mechanism, Linear Log-Normal
Attention, designed to emulate the distribution and concentration behavior of
the original self-attention. Our experimental results on popular natural
language benchmarks reveal that our proposed Linear Log-Normal Attention
outperforms other linearized attention alternatives, offering a promising
avenue for enhancing the scalability of transformer models. Our code is
available in supplementary materials.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13546">Enigma: Privacy-Preserving Execution of QAOA on Untrusted Quantum Computers. (arXiv:2311.13546v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Ayanzadeh_R/0/1/0/all/0/1">Ramin Ayanzadeh</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Mousavi_A/0/1/0/all/0/1">Ahmad Mousavi</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Alavisamani_N/0/1/0/all/0/1">Narges Alavisamani</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Qureshi_M/0/1/0/all/0/1">Moinuddin Qureshi</a></p>
<p>Quantum computers can solve problems that are beyond the capabilities of
conventional computers. As quantum computers are expensive and hard to
maintain, the typical model for performing quantum computation is to send the
circuit to a quantum cloud provider. This leads to privacy concerns for
commercial entities as an untrusted server can learn protected information from
the provided circuit. Current proposals for Secure Quantum Computing (SQC)
either rely on emerging technologies (such as quantum networks) or incur
prohibitive overheads (for Quantum Homomorphic Encryption). The goal of our
paper is to enable low-cost privacy-preserving quantum computation that can be
used with current systems.
</p>
<p>We propose Enigma, a suite of privacy-preserving schemes specifically
designed for the Quantum Approximate Optimization Algorithm (QAOA). Unlike
previous SQC techniques that obfuscate quantum circuits, Enigma transforms the
input problem of QAOA, such that the resulting circuit and the outcomes are
unintelligible to the server. We introduce three variants of Enigma. Enigma-I
protects the coefficients of QAOA using random phase flipping and fudging of
values. Enigma-II protects the nodes of the graph by introducing decoy qubits,
which are indistinguishable from primary ones. Enigma-III protects the edge
information of the graph by modifying the graph such that each node has an
identical number of connections. For all variants of Enigma, we demonstrate
that we can still obtain the solution for the original problem. We evaluate
Enigma using IBM quantum devices and show that the privacy improvements of
Enigma come at only a small reduction in fidelity (1%-13%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13547">Medical Image Retrieval Using Pretrained Embeddings. (arXiv:2311.13547v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jush_F/0/1/0/all/0/1">Farnaz Khun Jush</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1">Tuan Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogler_S/0/1/0/all/0/1">Steffen Vogler</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1">Matthias Lenga</a></p>
<p>A wide range of imaging techniques and data formats available for medical
images make accurate retrieval from image databases challenging.
</p>
<p>Efficient retrieval systems are crucial in advancing medical research,
enabling large-scale studies and innovative diagnostic tools. Thus, addressing
the challenges of medical image retrieval is essential for the continued
enhancement of healthcare and research.
</p>
<p>In this study, we evaluated the feasibility of employing four
state-of-the-art pretrained models for medical image retrieval at modality,
body region, and organ levels and compared the results of two similarity
indexing approaches. Since the employed networks take 2D images, we analyzed
the impacts of weighting and sampling strategies to incorporate 3D information
during retrieval of 3D volumes. We showed that medical image retrieval is
feasible using pretrained networks without any additional training or
fine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for
various tasks at modality, body region, and organ level.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13559">Transfer Learning-based Real-time Handgun Detection. (arXiv:2311.13559v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elmir_Y/0/1/0/all/0/1">Youssef Elmir</a>, <a href="http://arxiv.org/find/cs/1/au:+Laouar_S/0/1/0/all/0/1">Sid Ahmed Laouar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamdaoui_L/0/1/0/all/0/1">Larbi Hamdaoui</a></p>
<p>Traditional surveillance systems rely on human attention, limiting their
effectiveness. This study employs convolutional neural networks and transfer
learning to develop a real-time computer vision system for automatic handgun
detection. Comprehensive analysis of online handgun detection methods is
conducted, emphasizing reducing false positives and learning time. Transfer
learning is demonstrated as an effective approach. Despite technical
challenges, the proposed system achieves a precision rate of 84.74%,
demonstrating promising performance comparable to related works, enabling
faster learning and accurate automatic handgun detection for enhanced security.
This research advances security measures by reducing human monitoring
dependence, showcasing the potential of transfer learning-based approaches for
efficient and reliable handgun detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13562">Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object. (arXiv:2311.13562v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_P/0/1/0/all/0/1">Peng Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingbo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1">Hongwu Lv</a></p>
<p>Image style transfer occupies an important place in both computer graphics
and computer vision. However, most current methods require reference to
stylized images and cannot individually stylize specific objects. To overcome
this limitation, we propose the "Soulstyler" framework, which allows users to
guide the stylization of specific objects in an image through simple textual
descriptions. We introduce a large language model to parse the text and
identify stylization goals and specific styles. Combined with a CLIP-based
semantic visual embedding encoder, the model understands and matches text and
image content. We also introduce a novel localized text-image block matching
loss that ensures that style transfer is performed only on specified target
objects, while non-target regions remain in their original style. Experimental
results demonstrate that our model is able to accurately perform style transfer
on target objects according to textual descriptions without affecting the style
of background regions. Our code will be available at
https://github.com/yisuanwang/Soulstyler.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13565">Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering. (arXiv:2311.13565v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nair_I/0/1/0/all/0/1">Inderjeet Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Somasundaram_S/0/1/0/all/0/1">Shwetha Somasundaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1">Apoorv Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1">Koustava Goswami</a></p>
<p>We address the task of evidence retrieval for long document question
answering, which involves locating relevant paragraphs within a document to
answer a question. We aim to assess the applicability of large language models
(LLMs) in the task of zero-shot long document evidence retrieval, owing to
their unprecedented performance across various NLP tasks. However, currently
the LLMs can consume limited context lengths as input, thus providing document
chunks as inputs might overlook the global context while missing out on
capturing the inter-segment dependencies. Moreover, directly feeding the large
input sets can incur significant computational costs, particularly when
processing the entire document (and potentially incurring monetary expenses
with enterprise APIs like OpenAI's GPT variants). To address these challenges,
we propose a suite of techniques that exploit the discourse structure commonly
found in documents. By utilizing this structure, we create a condensed
representation of the document, enabling a more comprehensive understanding and
analysis of relationships between different parts. We retain $99.6\%$ of the
best zero-shot approach's performance, while processing only $26\%$ of the
total tokens used by the best approach in the information seeking evidence
retrieval setup. We also show how our approach can be combined with
\textit{self-ask} reasoning agent to achieve best zero-shot performance in
complex multi-hop question answering, just $\approx 4\%$ short of zero-shot
performance using gold evidence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13569">Combinatorial Optimization with Policy Adaptation using Latent Space Search. (arXiv:2311.13569v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chalumeau_F/0/1/0/all/0/1">Felix Chalumeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Surana_S/0/1/0/all/0/1">Shikha Surana</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonnet_C/0/1/0/all/0/1">Clement Bonnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1">Nathan Grinsztajn</a>, <a href="http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1">Arnu Pretorius</a>, <a href="http://arxiv.org/find/cs/1/au:+Laterre_A/0/1/0/all/0/1">Alexandre Laterre</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrett_T/0/1/0/all/0/1">Thomas D. Barrett</a></p>
<p>Combinatorial Optimization underpins many real-world applications and yet,
designing performant algorithms to solve these complex, typically NP-hard,
problems remains a significant research challenge. Reinforcement Learning (RL)
provides a versatile framework for designing heuristics across a broad spectrum
of problem domains. However, despite notable progress, RL has not yet
supplanted industrial solvers as the go-to solution. Current approaches
emphasize pre-training heuristics that construct solutions but often rely on
search procedures with limited variance, such as stochastically sampling
numerous solutions from a single policy or employing computationally expensive
fine-tuning of the policy on individual problem instances. Building on the
intuition that performant search at inference time should be anticipated during
pre-training, we propose COMPASS, a novel RL approach that parameterizes a
distribution of diverse and specialized policies conditioned on a continuous
latent space. We evaluate COMPASS across three canonical problems - Travelling
Salesman, Capacitated Vehicle Routing, and Job-Shop Scheduling - and
demonstrate that our search strategy (i) outperforms state-of-the-art
approaches on 11 standard benchmarking tasks and (ii) generalizes better,
surpassing all other approaches on a set of 18 procedurally transformed
instance distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13571">Ball Mill Fault Prediction Based on Deep Convolutional Auto-Encoding Network. (arXiv:2311.13571v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1">Xinkun Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yonggang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xinwu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peilong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">LiYe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">JanFeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yuan Pan</a></p>
<p>Ball mills play a critical role in modern mining operations, making their
bearing failures a significant concern due to the potential loss of production
efficiency and economic consequences. This paper presents an anomaly detection
method based on Deep Convolutional Auto-encoding Neural Networks (DCAN) for
addressing the issue of ball mill bearing fault detection. The proposed
approach leverages vibration data collected during normal operation for
training, overcoming challenges such as labeling issues and data imbalance
often encountered in supervised learning methods. DCAN includes the modules of
convolutional feature extraction and transposed convolutional feature
reconstruction, demonstrating exceptional capabilities in signal processing and
feature extraction. Additionally, the paper describes the practical deployment
of the DCAN-based anomaly detection model for bearing fault detection,
utilizing data from the ball mill bearings of Wuhan Iron &amp; Steel Resources
Group and fault data from NASA's bearing vibration dataset. Experimental
results validate the DCAN model's reliability in recognizing fault vibration
patterns. This method holds promise for enhancing bearing fault detection
efficiency, reducing production interruptions, and lowering maintenance costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13577">Physical Reasoning and Object Planning for Household Embodied Agents. (arXiv:2311.13577v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1">Ayush Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhakar_R/0/1/0/all/0/1">Raghav Prabhakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dianbo Liu</a></p>
<p>In this study, we explore the sophisticated domain of task planning for
robust household embodied agents, with a particular emphasis on the intricate
task of selecting substitute objects. We introduce the CommonSense Object
Affordance Task (COAT), a novel framework designed to analyze reasoning
capabilities in commonsense scenarios. This approach is centered on
understanding how these agents can effectively identify and utilize alternative
objects when executing household tasks, thereby offering insights into the
complexities of practical decision-making in real-world environments.Drawing
inspiration from human decision-making, we explore how large language models
tackle this challenge through three meticulously crafted commonsense
question-and-answer datasets, featuring refined rules and human annotations.
Our evaluation of state-of-the-art language models on these datasets sheds
light on three pivotal considerations: 1) aligning an object's inherent utility
with the task at hand, 2) navigating contextual dependencies (societal norms,
safety, appropriateness, and efficiency), and 3) accounting for the current
physical state of the object. To maintain accessibility, we introduce five
abstract variables reflecting an object's physical condition, modulated by
human insights to simulate diverse household scenarios. Our contributions
include insightful Object-Utility mappings addressing the first consideration
and two extensive QA datasets (15k and 130k questions) probing the intricacies
of contextual dependencies and object states. The datasets, along with our
findings, are accessible at: \url{https://github.com/com-phy-affordance/COAT}.
This research not only advances our understanding of physical commonsense
reasoning in language models but also paves the way for future improvements in
household agent intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13580">$\sigma$-PCA: a unified neural model for linear and nonlinear principal component analysis. (arXiv:2311.13580v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1">Fahdi Kanavati</a>, <a href="http://arxiv.org/find/cs/1/au:+Katsnith_L/0/1/0/all/0/1">Lucy Katsnith</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsuneki_M/0/1/0/all/0/1">Masayuki Tsuneki</a></p>
<p>Linear principal component analysis (PCA), nonlinear PCA, and linear
independent component analysis (ICA) -- those are three methods with
single-layer autoencoder formulations for learning linear transformations from
data. Linear PCA learns orthogonal transformations (rotations) that orient axes
to maximise variance, but it suffers from a subspace rotational indeterminacy:
it fails to find a unique rotation for axes that share the same variance. Both
nonlinear PCA and linear ICA reduce the subspace indeterminacy from rotational
to permutational by maximising statistical independence under the assumption of
unit variance. The main difference between them is that nonlinear PCA only
learns rotations while linear ICA learns not just rotations but any linear
transformation with unit variance. The relationship between all three can be
understood by the singular value decomposition of the linear ICA transformation
into a sequence of rotation, scale, rotation. Linear PCA learns the first
rotation; nonlinear PCA learns the second. The scale is simply the inverse of
the standard deviations. The problem is that, in contrast to linear PCA,
conventional nonlinear PCA cannot be used directly on the data to learn the
first rotation, the first being special as it reduces dimensionality and orders
by variances. In this paper, we have identified the cause, and as a solution we
propose $\sigma$-PCA: a unified neural model for linear and nonlinear PCA as
single-layer autoencoders. One of its key ingredients: modelling not just the
rotation but also the scale -- the variances. This model bridges the disparity
between linear and nonlinear PCA. And so, like linear PCA, it can learn a
semi-orthogonal transformation that reduces dimensionality and orders by
variances, but, unlike linear PCA, it does not suffer from rotational
indeterminacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13594">Labeling Neural Representations with Inverse Recognition. (arXiv:2311.13594v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kopf_L/0/1/0/all/0/1">Laura Kopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1">Shinichi Nakajima</a>, <a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1">Marius Kloft</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a></p>
<p>Deep Neural Networks (DNNs) demonstrated remarkable capabilities in learning
complex hierarchical data representations, but the nature of these
representations remains largely unknown. Existing global explainability
methods, such as Network Dissection, face limitations such as reliance on
segmentation masks, lack of statistical significance testing, and high
computational demands. We propose Inverse Recognition (INVERT), a scalable
approach for connecting learned representations with human-understandable
concepts by leveraging their capacity to discriminate between these concepts.
In contrast to prior work, INVERT is capable of handling diverse types of
neurons, exhibits less computational complexity, and does not rely on the
availability of segmentation masks. Moreover, INVERT provides an interpretable
metric assessing the alignment between the representation and its corresponding
explanation and delivering a measure of statistical significance, emphasizing
its utility and credibility. We demonstrate the applicability of INVERT in
various scenarios, including the identification of representations affected by
spurious correlations, and the interpretation of the hierarchical structure of
decision-making within the models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13601">Visual In-Context Prompting. (arXiv:2311.13601v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1">Qing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1">Tianhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shilong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xueyan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huaizhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>In-context prompting in large language models (LLMs) has become a prevalent
approach to improve zero-shot capabilities, but this idea is less explored in
the vision domain. Existing visual prompting methods focus on referring
segmentation to segment the most relevant object, falling short of addressing
many generic vision tasks like open-set segmentation and detection. In this
paper, we introduce a universal visual in-context prompting framework for both
tasks. In particular, we build on top of an encoder-decoder architecture, and
develop a versatile prompt encoder to support a variety of prompts like
strokes, boxes, and points. We further enhance it to take an arbitrary number
of reference image segments as the context. Our extensive explorations show
that the proposed visual in-context prompting elicits extraordinary referring
and generic segmentation capabilities to refer and detect, yielding competitive
performance to close-set in-domain datasets and showing promising results on
many open-set segmentation datasets. By joint training on COCO and SA-1B, our
model achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be
available at https://github.com/UX-Decoder/DINOv.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.04819">Deep Rank-Consistent Pyramid Model for Enhanced Crowd Counting. (arXiv:2201.04819v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiaqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhizhong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1">Yiming Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1">Hongming Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">James Z. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei-Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junping Zhang</a></p>
<p>Most conventional crowd counting methods utilize a fully-supervised learning
framework to establish a mapping between scene images and crowd density maps.
They usually rely on a large quantity of costly and time-intensive pixel-level
annotations for training supervision. One way to mitigate the intensive
labeling effort and improve counting accuracy is to leverage large amounts of
unlabeled images. This is attributed to the inherent self-structural
information and rank consistency within a single image, offering additional
qualitative relation supervision during training. Contrary to earlier methods
that utilized the rank relations at the original image level, we explore such
rank-consistency relation within the latent feature spaces. This approach
enables the incorporation of numerous pyramid partial orders, strengthening the
model representation capability. A notable advantage is that it can also
increase the utilization ratio of unlabeled samples. Specifically, we propose a
Deep Rank-consistEnt pyrAmid Model (DREAM), which makes full use of rank
consistency across coarse-to-fine pyramid features in latent spaces for
enhanced crowd counting with massive unlabeled images. In addition, we have
collected a new unlabeled crowd counting dataset, FUDAN-UCC, comprising 4,000
images for training purposes. Extensive experiments on four benchmark datasets,
namely UCF-QNRF, ShanghaiTech PartA and PartB, and UCF-CC-50, show the
effectiveness of our method compared with previous semi-supervised methods. The
codes are available at https://github.com/bridgeqiqi/DREAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05326">Scalable Real-Time Recurrent Learning Using Columnar-Constructive Networks. (arXiv:2302.05326v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Javed_K/0/1/0/all/0/1">Khurram Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1">Haseeb Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1">Rich Sutton</a>, <a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1">Martha White</a></p>
<p>Constructing states from sequences of observations is an important component
of reinforcement learning agents. One solution for state construction is to use
recurrent neural networks. Back-propagation through time (BPTT), and real-time
recurrent learning (RTRL) are two popular gradient-based methods for recurrent
learning. BPTT requires complete trajectories of observations before it can
compute the gradients and is unsuitable for online updates. RTRL can do online
updates but scales poorly to large networks. In this paper, we propose two
constraints that make RTRL scalable. We show that by either decomposing the
network into independent modules or learning the network in stages, we can make
RTRL scale linearly with the number of parameters. Unlike prior scalable
gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms
do not add noise or bias to the gradient estimate. Instead, they trade off the
functional capacity of the network for computationally efficient learning. We
demonstrate the effectiveness of our approach over Truncated-BPTT on a
prediction benchmark inspired by animal learning and by doing policy evaluation
of pre-trained policies for Atari 2600 games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07061">AutoDroid-0shot: A Simple Baseline for GPT-powered UI-grounded Smartphone Task Automation in Android. (arXiv:2304.07061v3 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaxuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanchun Li</a></p>
<p>This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large
language models (LLMs) to automate the interactions with Android mobile
applications. Given a natural language description of a desired task,
DroidBot-GPT can automatically generate and execute actions that navigate the
app to complete the task. It works by translating the app GUI state information
and the available actions on the smartphone screen to natural language prompts
and asking the LLM to make a choice of actions. Since the LLM is typically
trained on a large amount of data including the how-to manuals of diverse
software applications, it has the ability to make reasonable choices of actions
based on the provided information. We evaluate DroidBot-GPT with a self-created
dataset that contains 33 tasks collected from 17 Android applications spanning
10 categories. It can successfully complete 39.39% of the tasks, and the
average partial completion progress is about 66.76%. Given the fact that our
method is fully unsupervised (no modification required from both the app and
the LLM), we believe there is great potential to enhance automation performance
with better app development paradigms and/or custom model training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11731">Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically-Generated Misspellings. (arXiv:2305.11731v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mohammad Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1">Heshaam Faili</a></p>
<p>Spelling correction is a remarkable challenge in the field of natural
language processing. The objective of spelling correction tasks is to recognize
and rectify spelling errors automatically. The development of applications that
can effectually diagnose and correct Persian spelling and grammatical errors
has become more important in order to improve the quality of Persian text. The
Typographical Error Type Detection in Persian is a relatively understudied
area. Therefore, this paper presents a compelling approach for detecting
typographical errors in Persian texts. Our work includes the presentation of a
publicly available dataset called FarsTypo, which comprises 3.4 million words
arranged in chronological order and tagged with their corresponding
part-of-speech. These words cover a wide range of topics and linguistic styles.
We develop an algorithm designed to apply Persian-specific errors to a scalable
portion of these words, resulting in a parallel dataset of correct and
incorrect words. By leveraging FarsTypo, we establish a strong foundation and
conduct a thorough comparison of various methodologies employing different
architectures. Additionally, we introduce a groundbreaking Deep Sequential
Neural Network that utilizes both word and character embeddings, along with
bidirectional LSTM layers, for token classification aimed at detecting
typographical errors across 51 distinct classes. Our approach is contrasted
with highly advanced industrial systems that, unlike this study, have been
developed using a diverse range of resources. The outcomes of our final method
proved to be highly competitive, achieving an accuracy of 97.62%, precision of
98.83%, recall of 98.61%, and surpassing others in terms of speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14264">Active Learning Principles for In-Context Learning with Large Language Models. (arXiv:2305.14264v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1">Katerina Margatina</a>, <a href="http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1">Timo Schick</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a>, <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1">Jane Dwivedi-Yu</a></p>
<p>The remarkable advancements in large language models (LLMs) have
significantly enhanced the performance in few-shot learning settings. By using
only a small number of labeled examples, referred to as demonstrations, LLMs
can effectively grasp the task at hand through in-context learning. However,
the process of selecting appropriate demonstrations has received limited
attention in prior work. This paper addresses the issue of identifying the most
informative demonstrations for few-shot learning by approaching it as a
pool-based Active Learning (AL) problem over a single iteration. Our objective
is to investigate how AL algorithms can serve as effective demonstration
selection methods for in-context learning. We compare various standard AL
algorithms based on uncertainty, diversity, and similarity, and consistently
observe that the latter outperforms all other methods, including random
sampling. Notably, uncertainty sampling, despite its success in conventional
supervised learning scenarios, performs poorly in this context. Our extensive
experimentation involving a diverse range of GPT and OPT models across $24$
classification and multi-choice tasks, coupled with thorough analysis,
unambiguously demonstrates that in-context example selection through AL
prioritizes high-quality examples that exhibit low uncertainty and bear
similarity to the test examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01665">SourceP: Detecting Ponzi Schemes on Ethereum with Source Code. (arXiv:2306.01665v5 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1">Pengcheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1">Liang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1">Keting Yin</a></p>
<p>As blockchain technology becomes more and more popular, a typical financial
scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.
This Ponzi scheme deployed through smart contracts, also known as the smart
Ponzi scheme, has caused a lot of economic losses and negative impacts.
Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on
bytecode features, opcode features, account features, and transaction behavior
features of smart contracts, which are unable to truly characterize the
behavioral features of Ponzi schemes, and thus generally perform poorly in
terms of detection accuracy and false alarm rates. In this paper, we propose
SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using
pre-trained models and data flow, which only requires using the source code of
smart contracts as features. SourceP reduces the difficulty of data acquisition
and feature extraction of existing detection methods. Specifically, we first
convert the source code of a smart contract into a data flow graph and then
introduce a pre-trained model based on learning code representations to build a
classification model to identify Ponzi schemes in smart contracts. The
experimental results show that SourceP achieves 87.2\% recall and 90.7\%
F-score for detecting smart Ponzi schemes within Ethereum's smart contract
dataset, outperforming state-of-the-art methods in terms of performance and
sustainability. We also demonstrate through additional experiments that
pre-trained models and data flow play an important contribution to SourceP, as
well as proving that SourceP has a good generalization ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06202">NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Said_A/0/1/0/all/0/1">Anwar Said</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayrak_R/0/1/0/all/0/1">Roza G. Bayrak</a>, <a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1">Tyler Derr</a>, <a href="http://arxiv.org/find/cs/1/au:+Shabbir_M/0/1/0/all/0/1">Mudassir Shabbir</a>, <a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1">Daniel Moyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Catie Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutsoukos_X/0/1/0/all/0/1">Xenofon Koutsoukos</a></p>
<p>Machine learning provides a valuable tool for analyzing high-dimensional
functional neuroimaging data, and is proving effective in predicting various
neurological conditions, psychiatric disorders, and cognitive patterns. In
functional magnetic resonance imaging (MRI) research, interactions between
brain regions are commonly modeled using graph-based representations. The
potency of graph machine learning methods has been established across myriad
domains, marking a transformative step in data interpretation and predictive
modeling. Yet, despite their promise, the transposition of these techniques to
the neuroimaging domain has been challenging due to the expansive number of
potential preprocessing pipelines and the large parameter search space for
graph-based dataset construction. In this paper, we introduce NeuroGraph, a
collection of graph-based neuroimaging datasets, and demonstrated its utility
for predicting multiple categories of behavioral and cognitive traits. We delve
deeply into the dataset generation search space by crafting 35 datasets that
encompass static and dynamic brain connectivity, running in excess of 15
baseline methods for benchmarking. Additionally, we provide generic frameworks
for learning on both static and dynamic graphs. Our extensive experiments lead
to several key observations. Notably, using correlation vectors as node
features, incorporating larger number of regions of interest, and employing
sparser graphs lead to improved performance. To foster further advancements in
graph-based data driven neuroimaging analysis, we offer a comprehensive
open-source Python package that includes the benchmark datasets, baseline
implementations, model training, and standard evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03817">Exploring and Characterizing Large Language Models For Embedded System Development and Debugging. (arXiv:2307.03817v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Englhardt_Z/0/1/0/all/0/1">Zachary Englhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Richard Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nissanka_D/0/1/0/all/0/1">Dilini Nissanka</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayanswamy_G/0/1/0/all/0/1">Girish Narayanswamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Breda_J/0/1/0/all/0/1">Joseph Breda</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Shwetak Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1">Vikram Iyer</a></p>
<p>Large language models (LLMs) have shown remarkable abilities to generate
code, however their ability to develop software for embedded systems, which
requires cross-domain knowledge of hardware and software has not been studied.
In this paper we develop an extensible, open source hardware-in-the-loop
framework to systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to
assess their capabilities and limitations for embedded system development. We
observe through our study that even when these tools fail to produce working
code, they consistently generate helpful reasoning about embedded design tasks.
We leverage this finding to study how human programmers interact with these
tools, and develop an human-AI based software engineering workflow for building
embedded systems.
</p>
<p>Our evaluation platform for verifying LLM generated programs uses sensor
actuator pairs for physical evaluation. We compare all three models with N=450
experiments and find surprisingly that GPT-4 especially shows an exceptional
level of cross-domain understanding and reasoning, in some cases generating
fully correct programs from a single prompt. In N=50 trials, GPT-4 produces
functional I2C interfaces 66% of the time. GPT-4 also produces register-level
drivers, code for LoRa communication, and context-specific power optimizations
for an nRF52 program resulting in over 740x current reduction to 12.2uA. We
also characterize the models' limitations to develop a generalizable human-AI
workflow for using LLMs in embedded system development. We evaluate our
workflow with 15 users including novice and expert programmers. We find that
our workflow improves productivity for all users and increases the success rate
for building a LoRa environmental sensor from 25% to 100%, including for users
with zero hardware or C/C++ experience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08430">Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks. (arXiv:2307.08430v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zijie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qiuting He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Kun He</a></p>
<p>Utilizing long-range dependency, though extensively studied in homogeneous
graphs, is rarely studied in large-scale heterogeneous information networks
(HINs), whose main challenge is the high costs and the difficulty in utilizing
effective information. To this end, we investigate the importance of different
meta-paths and propose an automatic framework for utilizing long-range
dependency in HINs, called Long-range Meta-path Search through Progressive
Sampling (LMSPS). Specifically, to discover meta-paths for various datasets or
tasks without prior, we develop a search space with all target-node-related
meta-paths. With a progressive sampling algorithm, we dynamically shrink the
search space with hop-independent time complexity, leading to a compact search
space driven by the current HIN and task. Utilizing a sampling evaluation
strategy as the guidance, we conduct a specialized and expressive meta-path
selection. Extensive experiments on eight heterogeneous datasets demonstrate
that LMSPS discovers effective long-range meta-paths and outperforms
state-of-the-art models. Besides, it ranks top-1 on the leaderboards of
ogbn-mag in Open Graph Benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11494">Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1">Marcel Kollovieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1">Abdul Fatir Ansari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohlke_Schneider_M/0/1/0/all/0/1">Michael Bohlke-Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Zschiegner_J/0/1/0/all/0/1">Jasper Zschiegner</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a></p>
<p>Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally-trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01897">Inferring Actual Treatment Pathways from Patient Records. (arXiv:2309.01897v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilkins_Caruana_A/0/1/0/all/0/1">Adrian Wilkins-Caruana</a>, <a href="http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1">Madhushi Bandara</a>, <a href="http://arxiv.org/find/cs/1/au:+Musial_K/0/1/0/all/0/1">Katarzyna Musial</a>, <a href="http://arxiv.org/find/cs/1/au:+Catchpoole_D/0/1/0/all/0/1">Daniel Catchpoole</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_P/0/1/0/all/0/1">Paul J. Kennedy</a></p>
<p>Treatment pathways are step-by-step plans outlining the recommended medical
care for specific diseases; they get revised when different treatments are
found to improve patient outcomes. Examining health records is an important
part of this revision process, but inferring patients' actual treatments from
health data is challenging due to complex event-coding schemes and the absence
of pathway-related annotations. This study aims to infer the actual treatment
steps for a particular patient group from administrative health records (AHR) -
a common form of tabular healthcare data - and address several technique- and
methodology-based gaps in treatment pathway-inference research. We introduce
Defrag, a method for examining AHRs to infer the real-world treatment steps for
a particular patient group. Defrag learns the semantic and temporal meaning of
healthcare event sequences, allowing it to reliably infer treatment steps from
complex healthcare data. To our knowledge, Defrag is the first
pathway-inference method to utilise a neural network (NN), an approach made
possible by a novel, self-supervised learning objective. We also developed a
testing and validation framework for pathway inference, which we use to
characterise and evaluate Defrag's pathway inference ability and compare
against baselines. We demonstrate Defrag's effectiveness by identifying
best-practice pathway fragments for breast cancer, lung cancer, and melanoma in
public healthcare records. Additionally, we use synthetic data experiments to
demonstrate the characteristics of the Defrag method, and to compare Defrag to
several baselines where it significantly outperforms non-NN-based methods.
Defrag significantly outperforms several existing pathway-inference methods and
offers an innovative and effective approach for inferring treatment pathways
from AHRs. Open-source code is provided to encourage further research in this
area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02185">BEVTrack: A Simple and Strong Baseline for 3D Single Object Tracking in Bird&#x27;s-Eye View. (arXiv:2309.02185v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuxiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yingqi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jiahao Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1">Zheng-Jun Zha</a></p>
<p>3D Single Object Tracking (SOT) is a fundamental task of computer vision,
proving essential for applications like autonomous driving. It remains
challenging to localize the target from surroundings due to appearance
variations, distractors, and the high sparsity of point clouds. The spatial
information indicating objects' spatial adjacency across consecutive frames is
crucial for effective object tracking. However, existing trackers typically
employ point-wise representation with irregular formats, leading to
insufficient use of this important spatial knowledge. As a result, these
trackers usually require elaborate designs and solving multiple subtasks. In
this paper, we propose BEVTrack, a simple yet effective baseline that performs
tracking in Bird's-Eye View (BEV). This representation greatly retains spatial
information owing to its ordered structure and inherently encodes the implicit
motion relations of the target as well as distractors. To achieve accurate
regression for targets with diverse attributes (\textit{e.g.}, sizes and motion
patterns), BEVTrack constructs the likelihood function with the learned
underlying distributions adapted to different targets, rather than making a
fixed Laplace or Gaussian assumption as in previous works. This provides
valuable priors for tracking and thus further boosts performance. While only
using a single regression loss with a plain convolutional architecture,
BEVTrack achieves state-of-the-art performance on three large-scale datasets,
KITTI, NuScenes, and Waymo Open Dataset while maintaining a high inference
speed of about 200 FPS. The code will be released at
https://github.com/xmm-prio/BEVTrack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08549">HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Van_M/0/1/0/all/0/1">Minh-Hao Van</a>, <a href="http://arxiv.org/find/cs/1/au:+Carey_A/0/1/0/all/0/1">Alycia N. Carey</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xintao Wu</a></p>
<p>While numerous defense methods have been proposed to prohibit potential
poisoning attacks from untrusted data sources, most research works only defend
against specific attacks, which leaves many avenues for an adversary to
exploit. In this work, we propose an efficient and robust training approach to
defend against data poisoning attacks based on influence functions, named
Healthy Influential-Noise based Training. Using influence functions, we craft
healthy noise that helps to harden the classification model against poisoning
attacks without significantly affecting the generalization ability on test
data. In addition, our method can perform effectively when only a subset of the
training data is modified, instead of the current method of adding noise to all
examples that has been used in several previous works. We conduct comprehensive
evaluations over two image datasets with state-of-the-art poisoning attacks
under different realistic attack scenarios. Our empirical results show that
HINT can efficiently protect deep learning models against the effect of both
untargeted and targeted poisoning attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13289">USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation. (arXiv:2309.13289v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaofan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bo Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Changyou Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Daipeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhuyang Xie</a></p>
<p>Unsupervised skin lesion segmentation offers several benefits, including
conserving expert human resources, reducing discrepancies due to subjective
human labeling, and adapting to novel environments. However, segmenting
dermoscopic images without manual labeling guidance presents significant
challenges due to dermoscopic image artifacts such as hair noise, blister
noise, and subtle edge differences. To address these challenges, we introduce
an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin
lesion segmentation. The USL-Net can effectively segment a range of lesions,
eliminating the need for manual labeling guidance. Initially, features are
extracted using contrastive learning, followed by the generation of Class
Activation Maps (CAMs) as saliency maps using these features. The different CAM
locations correspond to the importance of the lesion region based on their
saliency. High-saliency regions in the map serve as pseudo-labels for lesion
regions while low-saliency regions represent the background. However,
intermediate regions can be hard to classify, often due to their proximity to
lesion edges or interference from hair or blisters. Rather than risk potential
pseudo-labeling errors or learning confusion by forcefully classifying these
regions, we consider them as uncertainty regions, exempting them from
pseudo-labeling and allowing the network to self-learn. Further, we employ
connectivity detection and centrality detection to refine foreground
pseudo-labels and reduce noise-induced errors. The application of cycle
refining enhances performance further. Our method underwent thorough
experimental validation on the ISIC-2017, ISIC-2018, and PH2 datasets,
demonstrating that its performance is on par with weakly supervised and
supervised methods, and exceeds that of other existing unsupervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13972">Audio classification with Dilated Convolution with Learnable Spacings. (arXiv:2309.13972v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khalfaoui_Hassani_I/0/1/0/all/0/1">Ismail Khalfaoui-Hassani</a>, <a href="http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1">Timoth&#xe9;e Masquelier</a>, <a href="http://arxiv.org/find/cs/1/au:+Pellegrini_T/0/1/0/all/0/1">Thomas Pellegrini</a></p>
<p>Dilated convolution with learnable spacings (DCLS) is a recent convolution
method in which the positions of the kernel elements are learned throughout
training by backpropagation. Its interest has recently been demonstrated in
computer vision (ImageNet classification and downstream tasks). Here we show
that DCLS is also useful for audio tagging using the AudioSet classification
benchmark. We took two state-of-the-art convolutional architectures using
depthwise separable convolutions (DSC), ConvNeXt and ConvFormer, and a hybrid
one using attention in addition, FastViT, and drop-in replaced all the DSC
layers by DCLS ones. This significantly improved the mean average precision
(mAP) with the three architectures without increasing the number of parameters
and with only a low cost on the throughput. The method code is based on PyTorch
and is available at https://github.com/K-H-Ismail/DCLS-Audio
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00603">Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals. (arXiv:2310.00603v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gat_Y/0/1/0/all/0/1">Yair Gat</a>, <a href="http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1">Nitay Calderon</a>, <a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1">Amir Feder</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapanin_A/0/1/0/all/0/1">Alexander Chapanin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Amit Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1">Roi Reichart</a></p>
<p>Causal explanations of the predictions of NLP systems are essential to ensure
safety and establish trust. Yet, existing methods often fall short of
explaining model predictions effectively or efficiently and are often
model-specific. In this paper, we address model-agnostic explanations,
proposing two approaches for counterfactual (CF) approximation. The first
approach is CF generation, where a large language model (LLM) is prompted to
change a specific text concept while keeping confounding concepts unchanged.
While this approach is demonstrated to be very effective, applying LLM at
inference-time is costly. We hence present a second approach based on matching,
and propose a method that is guided by an LLM at training-time and learns a
dedicated embedding space. This space is faithful to a given causal graph and
effectively serves to identify matches that approximate CFs. After showing
theoretically that approximating CFs is required in order to construct faithful
explanations, we benchmark our approaches and explain several models, including
LLMs with billions of parameters. Our empirical results demonstrate the
excellent performance of CF generation models as model-agnostic explainers.
Moreover, our matching approach, which requires far less test-time resources,
also provides effective explanations, surpassing many baselines. We also find
that Top-K techniques universally improve every tested method. Finally, we
showcase the potential of LLMs in constructing new benchmarks for model
explanation and subsequently validate our conclusions. Our work illuminates new
pathways for efficient and accurate approaches to interpreting NLP systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01012">Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chapman_J/0/1/0/all/0/1">James Chapman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wells_L/0/1/0/all/0/1">Lennie Wells</a>, <a href="http://arxiv.org/find/cs/1/au:+Aguila_A/0/1/0/all/0/1">Ana Lawry Aguila</a></p>
<p>The Canonical Correlation Analysis (CCA) family of methods is foundational in
multi-view learning. Regularised linear CCA methods can be seen to generalise
Partial Least Squares (PLS) and be unified with a Generalized Eigenvalue
Problem (GEP) framework. However, classical algorithms for these linear methods
are computationally infeasible for large-scale data. Extensions to Deep CCA
show great promise, but current training procedures are slow and complicated.
First we propose a novel unconstrained objective that characterizes the top
subspace of GEPs. Our core contribution is a family of fast algorithms for
stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying
stochastic gradient descent (SGD) to the corresponding CCA objectives. These
methods show far faster convergence and recover higher correlations than the
previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This
speed allows us to perform a first-of-its-kind PLS analysis of an extremely
large biomedical dataset from the UK Biobank, with over 33,000 individuals and
500,000 variants. Finally, we not only match the performance of `CCA-family'
Self-Supervised Learning (SSL) methods on CIFAR-10 and CIFAR-100 with minimal
hyper-parameter tuning, but also establish the first solid theoretical links to
classical CCA, laying the groundwork for future insights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09886">Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1">Chengwei Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a></p>
<p>Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12036">A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azar_M/0/1/0/all/0/1">Mohammad Gheshlaghi Azar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1">Mark Rowland</a>, <a href="http://arxiv.org/find/cs/1/au:+Piot_B/0/1/0/all/0/1">Bilal Piot</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daniel Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Calandriello_D/0/1/0/all/0/1">Daniele Calandriello</a>, <a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>, <a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a></p>
<p>The prevalent deployment of learning from human preferences through
reinforcement learning (RLHF) relies on two important approximations: the first
assumes that pairwise preferences can be substituted with pointwise rewards.
The second assumes that a reward model trained on these pointwise rewards can
generalize from collected data to out-of-distribution data sampled by the
policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an
approach that bypasses the second approximation and learn directly a policy
from collected data without the reward modelling stage. However, this method
still heavily relies on the first approximation.
</p>
<p>In this paper we try to gain a deeper theoretical understanding of these
practical algorithms. In particular we derive a new general objective called
$\Psi$PO for learning from human preferences that is expressed in terms of
pairwise preferences and therefore bypasses both approximations. This new
general objective allows us to perform an in-depth analysis of the behavior of
RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential
pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$
simply to Identity, for which we can derive an efficient optimisation
procedure, prove performance guarantees and demonstrate its empirical
superiority to DPO on some illustrative examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19680">Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Soon-Jae Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1">Chang-Sung Jeong</a></p>
<p>Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies have been
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes PLM-integrated NMT
(PiNMT) model to overcome the identified problems. PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieve
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01483">FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1">Zihan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yue Gao</a></p>
<p>Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01588">Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v2 [astro-ph.CO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Roncoli_A/0/1/0/all/0/1">Andrea Roncoli</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ciprijanovic_A/0/1/0/all/0/1">Aleksandra &#x106;iprijanovi&#x107;</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Voetberg_M/0/1/0/all/0/1">Maggie Voetberg</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1">Francisco Villaescusa-Navarro</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Nord_B/0/1/0/all/0/1">Brian Nord</a></p>
<p>Deep learning models have been shown to outperform methods that rely on
summary statistics, like the power spectrum, in extracting information from
complex cosmological data sets. However, due to differences in the subgrid
physics implementation and numerical approximations across different simulation
suites, models trained on data from one cosmological simulation show a drop in
performance when tested on another. Similarly, models trained on any of the
simulations would also likely experience a drop in performance when applied to
observational data. Training on data from two different suites of the CAMELS
hydrodynamic cosmological simulations, we examine the generalization
capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing
GNNs, we capitalize on their capacity to capture structured scale-free
cosmological information from galaxy distributions. Moreover, by including
unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable
our models to extract domain-invariant features. We demonstrate that DA-GNN
achieves higher accuracy and robustness on cross-dataset tasks (up to $28\%$
better relative error and up to almost an order of magnitude better $\chi^2$).
Using data visualizations, we show the effects of domain adaptation on proper
latent space data alignment. This shows that DA-GNNs are a promising method for
extracting domain-independent cosmological information, a vital step toward
robust deep learning for real cosmic survey data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03105">Pelvic floor MRI segmentation based on semi-supervised deep learning. (arXiv:2311.03105v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1">Jianwei Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1">Fei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhuhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ashton_Miller_J/0/1/0/all/0/1">James A. Ashton-Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Delancey_J/0/1/0/all/0/1">John O.L. Delancey</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiajia Luo</a></p>
<p>The semantic segmentation of pelvic organs via MRI has important clinical
significance. Recently, deep learning-enabled semantic segmentation has
facilitated the three-dimensional geometric reconstruction of pelvic floor
organs, providing clinicians with accurate and intuitive diagnostic results.
However, the task of labeling pelvic floor MRI segmentation, typically
performed by clinicians, is labor-intensive and costly, leading to a scarcity
of labels. Insufficient segmentation labels limit the precise segmentation and
reconstruction of pelvic floor organs. To address these issues, we propose a
semi-supervised framework for pelvic organ segmentation. The implementation of
this framework comprises two stages. In the first stage, it performs
self-supervised pre-training using image restoration tasks. Subsequently,
fine-tuning of the self-supervised model is performed, using labeled data to
train the segmentation model. In the second stage, the self-supervised
segmentation model is used to generate pseudo labels for unlabeled data.
Ultimately, both labeled and unlabeled data are utilized in semi-supervised
training. Upon evaluation, our method significantly enhances the performance in
the semantic segmentation and geometric reconstruction of pelvic organs, Dice
coefficient can increase by 2.65% averagely. Especially for organs that are
difficult to segment, such as the uterus, the accuracy of semantic segmentation
can be improved by up to 3.70%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03738">deep-REMAP: Parameterization of Stellar Spectra Using Regularized Multi-Task Learning. (arXiv:2311.03738v2 [astro-ph.SR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Gilda_S/0/1/0/all/0/1">Sankalp Gilda</a></p>
<p>Traditional spectral analysis methods are increasingly challenged by the
exploding volumes of data produced by contemporary astronomical surveys. In
response, we develop deep-Regularized Ensemble-based Multi-task Learning with
Asymmetric Loss for Probabilistic Inference ($\rm{deep-REMAP}$), a novel
framework that utilizes the rich synthetic spectra from the PHOENIX library and
observational data from the MARVELS survey to accurately predict stellar
atmospheric parameters. By harnessing advanced machine learning techniques,
including multi-task learning and an innovative asymmetric loss function,
$\rm{deep-REMAP}$ demonstrates superior predictive capabilities in determining
effective temperature, surface gravity, and metallicity from observed spectra.
Our results reveal the framework's effectiveness in extending to other stellar
libraries and properties, paving the way for more sophisticated and automated
techniques in stellar characterization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05997">JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models. (arXiv:2311.05997v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1">Shaofei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anji Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yonggang Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Jinbing Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haowei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhaofeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zilong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yitao Liang</a></p>
<p>Achieving human-like planning and control with multimodal observations in an
open world is a key milestone for more functional generalist agents. Existing
approaches can handle certain long-horizon tasks in an open world. However,
they still struggle when the number of open-world tasks could potentially be
infinite and lack the capability to progressively enhance task completion as
game time progresses. We introduce JARVIS-1, an open-world agent that can
perceive multimodal input (visual observations and human instructions),
generate sophisticated plans, and perform embodied control, all within the
popular yet challenging open-world Minecraft universe. Specifically, we develop
JARVIS-1 on top of pre-trained multimodal language models, which map visual
observations and textual instructions to plans. The plans will be ultimately
dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a
multimodal memory, which facilitates planning using both pre-trained knowledge
and its actual game survival experiences. JARVIS-1 is the existing most general
agent in Minecraft, capable of completing over 200 different tasks using
control and observation space similar to humans. These tasks range from
short-horizon tasks, e.g., "chopping trees" to long-horizon tasks, e.g.,
"obtaining a diamond pickaxe". JARVIS-1 performs exceptionally well in
short-horizon tasks, achieving nearly perfect performance. In the classic
long-term task of $\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the
reliability of current state-of-the-art agents by 5 times and can successfully
complete longer-horizon and more challenging tasks. The project page is
available at https://craftjarvis-jarvis1.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10242">Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers. (arXiv:2311.10242v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bengesi_S/0/1/0/all/0/1">Staphord Bengesi</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Sayed_H/0/1/0/all/0/1">Hoda El-Sayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1">Md Kamruzzaman Sarker</a>, <a href="http://arxiv.org/find/cs/1/au:+Houkpati_Y/0/1/0/all/0/1">Yao Houkpati</a>, <a href="http://arxiv.org/find/cs/1/au:+Irungu_J/0/1/0/all/0/1">John Irungu</a>, <a href="http://arxiv.org/find/cs/1/au:+Oladunni_T/0/1/0/all/0/1">Timothy Oladunni</a></p>
<p>The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10500">From Principle to Practice: Vertical Data Minimization for Machine Learning. (arXiv:2311.10500v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Staab_R/0/1/0/all/0/1">Robin Staab</a>, <a href="http://arxiv.org/find/cs/1/au:+Jovanovic_N/0/1/0/all/0/1">Nikola Jovanovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1">Mislav Balunovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a></p>
<p>Aiming to train and deploy predictive models, organizations collect large
amounts of detailed client data, risking the exposure of private information in
the event of a breach. To mitigate this, policymakers increasingly demand
compliance with the data minimization (DM) principle, restricting data
collection to only that data which is relevant and necessary for the task.
Despite regulatory pressure, the problem of deploying machine learning models
that obey DM has so far received little attention. In this work, we address
this challenge in a comprehensive manner. We propose a novel vertical DM (vDM)
workflow based on data generalization, which by design ensures that no
full-resolution client data is collected during training and deployment of
models, benefiting client privacy by reducing the attack surface in case of a
breach. We formalize and study the corresponding problem of finding
generalizations that both maximize data utility and minimize empirical privacy
risk, which we quantify by introducing a diverse set of policy-aligned
adversarial scenarios. Finally, we propose a range of baseline vDM algorithms,
as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that
outperforms all baselines across several settings. We plan to release our code
as a publicly available library, helping advance the standardization of DM for
machine learning. Overall, we believe our work can help lay the foundation for
further exploration and adoption of DM principles in real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10747">Safety-aware Causal Representation for Trustworthy Reinforcement Learning in Autonomous Driving. (arXiv:2311.10747v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haohong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenhao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yaru Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiacheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yuming Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a></p>
<p>In the domain of autonomous driving, the Learning from Demonstration (LfD)
paradigm has exhibited notable efficacy in addressing sequential
decision-making problems. However, consistently achieving safety in varying
traffic contexts, especially in safety-critical scenarios, poses a significant
challenge due to the long-tailed and unforeseen scenarios absent from offline
datasets. In this paper, we introduce the saFety-aware strUctured Scenario
representatION (FUSION), a pioneering methodology conceived to facilitate the
learning of an adaptive end-to-end driving policy by leveraging structured
scenario information. FUSION capitalizes on the causal relationships between
decomposed reward, cost, state, and action space, constructing a framework for
structured sequential reasoning under dynamic traffic environments. We conduct
rigorous evaluations in two typical real-world settings of distribution shift
in autonomous vehicles, demonstrating the good balance between safety cost and
utility reward of FUSION compared to contemporary state-of-the-art safety-aware
LfD baselines. Empirical evidence under diverse driving scenarios attests that
FUSION significantly enhances the safety and generalizability of autonomous
driving agents, even in the face of challenging and unseen environments.
Furthermore, our ablation studies reveal noticeable improvements in the
integration of causal representation into the safe offline RL problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10813">A Language Agent for Autonomous Driving. (arXiv:2311.10813v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jiageng Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Junjie Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yuxi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a></p>
<p>Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Code will be released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10840">Integration and Implementation Strategies for AI Algorithm Deployment with Smart Routing Rules and Workflow Management. (arXiv:2311.10840v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erdal_B/0/1/0/all/0/1">Barbaros Selnur Erdal</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vikash Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Demirer_M/0/1/0/all/0/1">Mutlu Demirer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fair_K/0/1/0/all/0/1">Kim H. Fair</a>, <a href="http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1">Richard D. White</a>, <a href="http://arxiv.org/find/cs/1/au:+Blair_J/0/1/0/all/0/1">Jeff Blair</a>, <a href="http://arxiv.org/find/cs/1/au:+Deichert_B/0/1/0/all/0/1">Barbara Deichert</a>, <a href="http://arxiv.org/find/cs/1/au:+Lafleur_L/0/1/0/all/0/1">Laurie Lafleur</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1">Ming Melvin Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bericat_D/0/1/0/all/0/1">David Bericat</a>, <a href="http://arxiv.org/find/cs/1/au:+Genereaux_B/0/1/0/all/0/1">Brad Genereaux</a></p>
<p>This paper reviews the challenges hindering the widespread adoption of
artificial intelligence (AI) solutions in the healthcare industry, focusing on
computer vision applications for medical imaging, and how interoperability and
enterprise-grade scalability can be used to address these challenges. The
complex nature of healthcare workflows, intricacies in managing large and
secure medical imaging data, and the absence of standardized frameworks for AI
development pose significant barriers and require a new paradigm to address
them.
</p>
<p>The role of interoperability is examined in this paper as a crucial factor in
connecting disparate applications within healthcare workflows. Standards such
as DICOM, Health Level 7 (HL7), and Integrating the Healthcare Enterprise (IHE)
are highlighted as foundational for common imaging workflows. A specific focus
is placed on the role of DICOM gateways, with Smart Routing Rules and Workflow
Management leading transformational efforts in this area.
</p>
<p>To drive enterprise scalability, new tools are needed. Project MONAI,
established in 2019, is introduced as an initiative aiming to redefine the
development of medical AI applications. The MONAI Deploy App SDK, a component
of Project MONAI, is identified as a key tool in simplifying the packaging and
deployment process, enabling repeatable, scalable, and standardized deployment
patterns for AI applications.
</p>
<p>The abstract underscores the potential impact of successful AI adoption in
healthcare, offering physicians both life-saving and time-saving insights and
driving efficiencies in radiology department workflows. The collaborative
efforts between academia and industry, are emphasized as essential for
advancing the adoption of healthcare AI solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10863">Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks. (arXiv:2311.10863v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haojun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zihe Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1">Yiannis Kantaros</a></p>
<p>Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11045">Orca 2: Teaching Small Language Models How to Reason. (arXiv:2311.11045v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1">Arindam Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Corro_L/0/1/0/all/0/1">Luciano Del Corro</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1">Shweti Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Codas_A/0/1/0/all/0/1">Andres Codas</a>, <a href="http://arxiv.org/find/cs/1/au:+Simoes_C/0/1/0/all/0/1">Clarisse Simoes</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Sahaj Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuxi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Razdaibiedina_A/0/1/0/all/0/1">Anastasia Razdaibiedina</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1">Erik Jones</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1">Kriti Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1">Hamid Palangi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guoqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosset_C/0/1/0/all/0/1">Corby Rosset</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanpour_H/0/1/0/all/0/1">Hamed Khanpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed Awadallah</a></p>
<p>Orca 1 learns from rich signals, such as explanation traces, allowing it to
outperform conventional instruction-tuned models on benchmarks like BigBench
Hard and AGIEval. In Orca 2, we continue exploring how improved training
signals can enhance smaller LMs' reasoning abilities. Research on training
small LMs has often relied on imitation learning to replicate the output of
more capable models. We contend that excessive emphasis on imitation may
restrict the potential of smaller models. We seek to teach small LMs to employ
different solution strategies for different tasks, potentially different from
the one used by the larger model. For example, while larger models might
provide a direct answer to a complex task, smaller models may not have the same
capacity. In Orca 2, we teach the model various reasoning techniques
(step-by-step, recall then generate, recall-reason-generate, direct answer,
etc.). More crucially, we aim to help the model learn to determine the most
effective solution strategy for each task. We evaluate Orca 2 using a
comprehensive set of 15 diverse benchmarks (corresponding to approximately 100
tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of
similar size and attains performance levels similar or better to those of
models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings. make Orca 2 weights publicly available at
aka.ms/orca-lm to support research on the development, evaluation, and
alignment of smaller LMs
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11655">Peeking Inside the Schufa Blackbox: Explaining the German Housing Scoring System. (arXiv:2311.11655v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kern_D/0/1/0/all/0/1">Dean-Robin Kern</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_G/0/1/0/all/0/1">Gunnar Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Dethier_E/0/1/0/all/0/1">Erik Dethier</a>, <a href="http://arxiv.org/find/cs/1/au:+Naveed_S/0/1/0/all/0/1">Sidra Naveed</a>, <a href="http://arxiv.org/find/cs/1/au:+Alizadeh_F/0/1/0/all/0/1">Fatemeh Alizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1">Delong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1">Md Shajalal</a></p>
<p>Explainable Artificial Intelligence is a concept aimed at making complex
algorithms transparent to users through a uniform solution. Researchers have
highlighted the importance of integrating domain specific contexts to develop
explanations tailored to end users. In this study, we focus on the Schufa
housing scoring system in Germany and investigate how users information needs
and expectations for explanations vary based on their roles. Using the
speculative design approach, we asked business information students to imagine
user interfaces that provide housing credit score explanations from the
perspectives of both tenants and landlords. Our preliminary findings suggest
that although there are general needs that apply to all users, there are also
conflicting needs that depend on the practical realities of their roles and how
credit scores affect them. We contribute to Human centered XAI research by
proposing future research directions that examine users explanatory needs
considering their roles and agencies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11819">Generalized super-resolution 4D Flow MRI $\unicode{x2013}$ using ensemble learning to extend across the cardiovascular system. (arXiv:2311.11819v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ericsson_L/0/1/0/all/0/1">Leon Ericsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Hjalmarsson_A/0/1/0/all/0/1">Adam Hjalmarsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Akbar_M/0/1/0/all/0/1">Muhammad Usman Akbar</a>, <a href="http://arxiv.org/find/eess/1/au:+Ferdian_E/0/1/0/all/0/1">Edward Ferdian</a>, <a href="http://arxiv.org/find/eess/1/au:+Bonini_M/0/1/0/all/0/1">Mia Bonini</a>, <a href="http://arxiv.org/find/eess/1/au:+Hardy_B/0/1/0/all/0/1">Brandon Hardy</a>, <a href="http://arxiv.org/find/eess/1/au:+Schollenberger_J/0/1/0/all/0/1">Jonas Schollenberger</a>, <a href="http://arxiv.org/find/eess/1/au:+Aristova_M/0/1/0/all/0/1">Maria Aristova</a>, <a href="http://arxiv.org/find/eess/1/au:+Winter_P/0/1/0/all/0/1">Patrick Winter</a>, <a href="http://arxiv.org/find/eess/1/au:+Burris_N/0/1/0/all/0/1">Nicholas Burris</a>, <a href="http://arxiv.org/find/eess/1/au:+Fyrdahl_A/0/1/0/all/0/1">Alexander Fyrdahl</a>, <a href="http://arxiv.org/find/eess/1/au:+Sigfridsson_A/0/1/0/all/0/1">Andreas Sigfridsson</a>, <a href="http://arxiv.org/find/eess/1/au:+Schnell_S/0/1/0/all/0/1">Susanne Schnell</a>, <a href="http://arxiv.org/find/eess/1/au:+Figueroa_C/0/1/0/all/0/1">C. Alberto Figueroa</a>, <a href="http://arxiv.org/find/eess/1/au:+Nordsletten_D/0/1/0/all/0/1">David Nordsletten</a>, <a href="http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1">Alistair A. Young</a>, <a href="http://arxiv.org/find/eess/1/au:+Marlevi_D/0/1/0/all/0/1">David Marlevi</a></p>
<p>4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive
measurement technique capable of quantifying blood flow across the
cardiovascular system. While practical use is limited by spatial resolution and
image noise, incorporation of trained super-resolution (SR) networks has
potential to enhance image quality post-scan. However, these efforts have
predominantly been restricted to narrowly defined cardiovascular domains, with
limited exploration of how SR performance extends across the cardiovascular
system; a task aggravated by contrasting hemodynamic conditions apparent across
the cardiovasculature. The aim of our study was to explore the generalizability
of SR 4D Flow MRI using a combination of heterogeneous training sets and
dedicated ensemble learning. With synthetic training data generated across
three disparate domains (cardiac, aortic, cerebrovascular), varying
convolutional base and ensemble learners were evaluated as a function of domain
and architecture, quantifying performance on both in-silico and acquired
in-vivo data from the same three domains. Results show that both bagging and
stacking ensembling enhance SR performance across domains, accurately
predicting high-resolution velocities from low-resolution input data in-silico.
Likewise, optimized networks successfully recover native resolution velocities
from downsampled in-vivo data, as well as show qualitative potential in
generating denoised SR-images from clinical level input data. In conclusion,
our work presents a viable approach for generalized SR 4D Flow MRI, with
ensemble learning extending utility across various clinical areas of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12068">Enhancing Novel Object Detection via Cooperative Foundational Models. (arXiv:2311.12068v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1">Rohit Bharadwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a></p>
<p>In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12198">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. (arXiv:2311.12198v2 [cs.GR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianyi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1">Zeshun Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yuxing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1">Chenfanfu Jiang</a></p>
<p>We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or
any other geometry embedding, highlighting the principle of "what you see is
what you simulate (WS$^2$)." Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12207">Defense semantics of argumentation: revisit. (arXiv:2311.12207v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1">Beishui Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Torre_L/0/1/0/all/0/1">Leendert van der Torre</a></p>
<p>In this paper we introduce a novel semantics, called defense semantics, for
Dung's abstract argumentation frameworks in terms of a notion of (partial)
defence, which is a triple encoding that one argument is (partially) defended
by another argument via attacking the attacker of the first argument. In terms
of defense semantics, we show that defenses related to self-attacked arguments
and arguments in 3-cycles are unsatifiable under any situation and therefore
can be removed without affecting the defense semantics of an AF. Then, we
introduce a new notion of defense equivalence of AFs, and compare defense
equivalence with standard equivalence and strong equivalence, respectively.
Finally, by exploiting defense semantics, we define two kinds of reasons for
accepting arguments, i.e., direct reasons and root reasons, and a notion of
root equivalence of AFs that can be used in argumentation summarization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12526">Neural Network Pruning by Gradient Descent. (arXiv:2311.12526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Ruyi Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiang Zhang</a></p>
<p>The rapid increase in the parameters of deep learning models has led to
significant costs, challenging computational efficiency and model
interpretability. In this paper, we introduce a novel and straightforward
neural network pruning framework that incorporates the Gumbel-Softmax
technique. This framework enables the simultaneous optimization of a network's
weights and topology in an end-to-end process using stochastic gradient
descent. Empirical results demonstrate its exceptional compression capability,
maintaining high accuracy on the MNIST dataset with only 0.15\% of the original
network parameters. Moreover, our framework enhances neural network
interpretability, not only by allowing easy extraction of feature importance
directly from the pruned network but also by enabling visualization of feature
symmetry and the pathways of information propagation from features to outcomes.
Although the pruning strategy is learned through deep learning, it is
surprisingly intuitive and understandable, focusing on selecting key
representative features and exploiting data patterns to achieve extreme sparse
pruning. We believe our method opens a promising new avenue for deep learning
pruning and the creation of interpretable machine learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12538">In-Context Learning Functions with Varying Number of Minima. (arXiv:2311.12538v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1">David Oniani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanshan Wang</a></p>
<p>Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06054">Refining the ONCE Benchmark with Hyperparameter Tuning. (arXiv:2311.06054v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golyadkin_M/0/1/0/all/0/1">Maksim Golyadkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gambashidze_A/0/1/0/all/0/1">Alexander Gambashidze</a>, <a href="http://arxiv.org/find/cs/1/au:+Nurgaliev_I/0/1/0/all/0/1">Ildar Nurgaliev</a>, <a href="http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1">Ilya Makarov</a></p>
<p>In response to the growing demand for 3D object detection in applications
such as autonomous driving, robotics, and augmented reality, this work focuses
on the evaluation of semi-supervised learning approaches for point cloud data.
The point cloud representation provides reliable and consistent observations
regardless of lighting conditions, thanks to advances in LiDAR sensors. Data
annotation is of paramount importance in the context of LiDAR applications, and
automating 3D data annotation with semi-supervised methods is a pivotal
challenge that promises to reduce the associated workload and facilitate the
emergence of cost-effective LiDAR solutions. Nevertheless, the task of
semi-supervised learning in the context of unordered point cloud data remains
formidable due to the inherent sparsity and incomplete shapes that hinder the
generation of accurate pseudo-labels. In this study, we consider these
challenges by posing the question: "To what extent does unlabelled data
contribute to the enhancement of model performance?" We show that improvements
from previous semi-supervised methods may not be as profound as previously
thought. Our results suggest that simple grid search hyperparameter tuning
applied to a supervised model can lead to state-of-the-art performance on the
ONCE dataset, while the contribution of unlabelled data appears to be
comparatively less exceptional.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11642">Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging. (arXiv:2311.11642v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muqeet_A/0/1/0/all/0/1">Abdul Muqeet</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyuchul Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Bumsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yohan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungrae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woonggon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kwang Hee Lee</a></p>
<p>Video face re-aging deals with altering the apparent age of a person to the
target age in videos. This problem is challenging due to the lack of paired
video datasets maintaining temporal consistency in identity and age. Most
re-aging methods process each image individually without considering the
temporal consistency of videos. While some existing works address the issue of
temporal coherence through video facial attribute manipulation in latent space,
they often fail to deliver satisfactory performance in age transformation. To
tackle the issues, we propose (1) a novel synthetic video dataset that features
subjects across a diverse range of age groups; (2) a baseline architecture
designed to validate the effectiveness of our proposed dataset, and (3) the
development of three novel metrics tailored explicitly for evaluating the
temporal consistency of video re-aging techniques. Our comprehensive
experiments on public datasets, such as VFHQ and CelebV-HQ, show that our
method outperforms the existing approaches in terms of both age transformation
and temporal consistency.
</p>
</p>
</div>

    </div>
    </body>
    