<!DOCTYPE html>
<html>
<head>
<title>2023-11-13-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.04905">Detecting Relevant Information in High-Volume Chat Logs: Keyphrase Extraction for Grooming and Drug Dealing Forensic Analysis. (arXiv:2311.04905v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alves_J/0/1/0/all/0/1">Jeovane Hon&#xf3;rio Alves</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedroso_H/0/1/0/all/0/1">Hor&#xe1;cio A. C. G. Pedroso</a>, <a href="http://arxiv.org/find/cs/1/au:+Venetikides_R/0/1/0/all/0/1">Rafael Honorio Venetikides</a>, <a href="http://arxiv.org/find/cs/1/au:+Koster_J/0/1/0/all/0/1">Joel E. M. K&#xf6;ster</a>, <a href="http://arxiv.org/find/cs/1/au:+Grochocki_L/0/1/0/all/0/1">Luiz Rodrigo Grochocki</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitas_C/0/1/0/all/0/1">Cinthia O. A. Freitas</a>, <a href="http://arxiv.org/find/cs/1/au:+Barddal_J/0/1/0/all/0/1">Jean Paul Barddal</a></p>
<p>The growing use of digital communication platforms has given rise to various
criminal activities, such as grooming and drug dealing, which pose significant
challenges to law enforcement and forensic experts. This paper presents a
supervised keyphrase extraction approach to detect relevant information in
high-volume chat logs involving grooming and drug dealing for forensic
analysis. The proposed method, JointKPE++, builds upon the JointKPE keyphrase
extractor by employing improvements to handle longer texts effectively. We
evaluate JointKPE++ using BERT-based pre-trained models on grooming and drug
dealing datasets, including BERT, RoBERTa, SpanBERT, and BERTimbau. The results
show significant improvements over traditional approaches and demonstrate the
potential for JointKPE++ to aid forensic experts in efficiently detecting
keyphrases related to criminal activities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04906">FlaCGEC: A Chinese Grammatical Error Correction Dataset with Fine-grained Linguistic Annotation. (arXiv:2311.04906v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1">Hanyue Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yike Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qingyuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiani Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yunshi Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xuesong Lu</a></p>
<p>Chinese Grammatical Error Correction (CGEC) has been attracting growing
attention from researchers recently. In spite of the fact that multiple CGEC
datasets have been developed to support the research, these datasets lack the
ability to provide a deep linguistic topology of grammar errors, which is
critical for interpreting and diagnosing CGEC approaches. To address this
limitation, we introduce FlaCGEC, which is a new CGEC dataset featured with
fine-grained linguistic annotation. Specifically, we collect raw corpus from
the linguistic schema defined by Chinese language experts, conduct edits on
sentences via rules, and refine generated samples manually, which results in
10k sentences with 78 instantiated grammar points and 3 types of edits. We
evaluate various cutting-edge CGEC methods on the proposed FlaCGEC dataset and
their unremarkable results indicate that this dataset is challenging in
covering a large range of grammatical errors. In addition, we also treat
FlaCGEC as a diagnostic dataset for testing generalization skills and conduct a
thorough evaluation of existing CGEC models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04907">In nomine patris... Elements for a semantics of medieval paternity. (arXiv:2311.04907v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perreaux_N/0/1/0/all/0/1">Nicolas Perreaux</a> (LAMOP)</p>
<p>This article examines medieval concepts of paternity and father-son
relationships through the digital analysis of medieval textual corpora.
Although historians have access to enormous digital collections in 2023, they
have rarely fully exploited these resources. The author proposes a historical
semantic approach to this theme, using modeling tools and text mining in
general, to analyze the evolution of terms related to paternity. The study
proposes three conclusions: 1. a semantic break occurred in the semantic field
of paternity at the turn of Antiquity and the Early Middle Ages. The meaning of
pater and its derivatives changed radically over the course of the 4th-6th
centuries, particularly as a result of the influence of the dogma of the
Christian Trinity. Medieval fatherhood was multidimensional, encompassing both
biological and spiritual aspects, in other words, complex relationships between
multiple carnal and spiritual (i.e. divine) fathers. 2. The role of spiritual
kinship is crucial to understanding medieval fatherhood, as the work of Anita
Guerreau-Jalabert and J{\'e}r{\^o}me Baschet has already shown. Initially
attributed to God, this ''ideal paternity'' (paternitas) gradually extended to
members of the Church (popes, bishops, abbots), underlining at the same time
the growing importance of spiritual kinship over biological kinship over the
centuries studied. 3. To reveal these structures, invisible to the naked eye,
an interdisciplinary approach is rigorously required. Complementary
investigations into the lemmas mater, filia, frater and other family terms are
required. The use of digital tools and historical semantic analysis opens up
new perspectives for researchers in history, anthropology, linguistics and data
mining, enabling them to explore the representation systems of ancient
societies in depth and with nuance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04910">Ontology-Driven Processing of Transdisciplinary Domain Knowledge. (arXiv:2311.04910v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Palagin_O/0/1/0/all/0/1">Oleksandr Palagin</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrenko_M/0/1/0/all/0/1">Mykola Petrenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Kryvyi_S/0/1/0/all/0/1">Sergii Kryvyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyko_M/0/1/0/all/0/1">Mykola Boyko</a>, <a href="http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1">Kyrylo Malakhov</a></p>
<p>The monograph discusses certain aspects of modern real-world problems facing
humanity, which are much more challenging than scientific ones. Modern science
is unable to solve them in a fundamental way. Vernadsky's noosphere thesis, in
fact, appeals to the scientific worldview that needs to be built in a way that
overcomes the interdisciplinary barriers and increases the effectiveness of
interdisciplinary interaction and modern science overall. We are talking about
the general transdisciplinary knowledge. In world practice, there is still no
systematic methodology and a specific form of generally accepted valid
scientific theory that would provide transdisciplinary knowledge. Non-linear
interdisciplinary interaction is the standard of evolution of modern science.
At the same time, a new transdisciplinary theory (domain of scientific
research) is being de facto created and the process is repeated many times:
from an individual or group of disciplines, through interdisciplinary
interaction, in a direction that brings us closer to creating a holistic
general scientific worldview.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04911">From Text to Structure: Using Large Language Models to Support the Development of Legal Expert Systems. (arXiv:2311.04911v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Janatian_S/0/1/0/all/0/1">Samyar Janatian</a>, <a href="http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1">Hannes Westermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Jinzhe Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1">Jaromir Savelka</a>, <a href="http://arxiv.org/find/cs/1/au:+Benyekhlef_K/0/1/0/all/0/1">Karim Benyekhlef</a></p>
<p>Encoding legislative text in a formal representation is an important
prerequisite to different tasks in the field of AI &amp; Law. For example,
rule-based expert systems focused on legislation can support laypeople in
understanding how legislation applies to them and provide them with helpful
context and information. However, the process of analyzing legislation and
other sources to encode it in the desired formal representation can be
time-consuming and represents a bottleneck in the development of such systems.
Here, we investigate to what degree large language models (LLMs), such as
GPT-4, are able to automatically extract structured representations from
legislation. We use LLMs to create pathways from legislation, according to the
JusticeBot methodology for legal decision support systems, evaluate the
pathways and compare them to manually created pathways. The results are
promising, with 60% of generated pathways being rated as equivalent or better
than manually created ones in a blind comparison. The approach suggests a
promising path to leverage the capabilities of LLMs to ease the costly
development of systems based on symbolic approaches that are transparent and
explainable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04913">An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach. (arXiv:2311.04913v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1">Suhaima Jamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimmer_H/0/1/0/all/0/1">Hayden Wimmer</a></p>
<p>Phishing and spam detection is long standing challenge that has been the
subject of much academic research. Large Language Models (LLM) have vast
potential to transform society and provide new and innovative approaches to
solve well-established challenges. Phishing and spam have caused financial
hardships and lost time and resources to email users all over the world and
frequently serve as an entry point for ransomware threat actors. While
detection approaches exist, especially heuristic-based approaches, LLMs offer
the potential to venture into a new unexplored area for understanding and
solving this challenge. LLMs have rapidly altered the landscape from business,
consumers, and throughout academia and demonstrate transformational potential
for the potential of society. Based on this, applying these new and innovative
approaches to email detection is a rational next step in academic research. In
this work, we present IPSDM, our model based on fine-tuning the BERT family of
models to specifically detect phishing and spam email. We demonstrate our
fine-tuned version, IPSDM, is able to better classify emails in both unbalanced
and balanced datasets. This work serves as an important first step towards
employing LLMs to improve the security of our information systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04915">Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models. (arXiv:2311.04915v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yoon Kyung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Inju Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1">Minjung Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1">Seoyeon Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1">Sowon Hahn</a></p>
<p>We present a novel method, the Chain of Empathy (CoE) prompting, that
utilizes insights from psychotherapy to induce Large Language Models (LLMs) to
reason about human emotional states. This method is inspired by various
psychotherapy approaches including Cognitive Behavioral Therapy (CBT),
Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality
Therapy (RT), each leading to different patterns of interpreting clients'
mental states. LLMs without reasoning generated predominantly exploratory
responses. However, when LLMs used CoE reasoning, we found a more comprehensive
range of empathetic responses aligned with the different reasoning patterns of
each psychotherapy model. The CBT based CoE resulted in the most balanced
generation of empathetic responses. The findings underscore the importance of
understanding the emotional context and how it affects human and AI
communication. Our research contributes to understanding how psychotherapeutic
models can be incorporated into LLMs, facilitating the development of
context-specific, safer, and empathetic AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04916">Explainable Identification of Hate Speech towards Islam using Graph Neural Networks. (arXiv:2311.04916v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1">Azmine Toushik Wasi</a></p>
<p>Islamophobic language is a prevalent challenge on online social interaction
platforms. Identifying and eliminating such hatred is a crucial step towards a
future of harmony and peace. This study presents a novel paradigm for
identifying and explaining hate speech towards Islam using graph neural
networks. Utilizing the intrinsic ability of graph neural networks to find,
extract, and use relationships across disparate data points, our model
consistently achieves outstanding performance while offering explanations for
the underlying correlations and causation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04917">Adapting Fake News Detection to the Era of Large Language Models. (arXiv:2311.04917v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jinyan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1">Claire Cardie</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1">Preslav Nakov</a></p>
<p>In the age of large language models (LLMs) and the widespread adoption of
AI-driven content creation, the landscape of information dissemination has
witnessed a paradigm shift. With the proliferation of both human-written and
machine-generated real and fake news, robustly and effectively discerning the
veracity of news articles has become an intricate challenge. While substantial
research has been dedicated to fake news detection, this either assumes that
all news articles are human-written or abruptly assumes that all
machine-generated news are fake. Thus, a significant gap exists in
understanding the interplay between machine-(paraphrased) real news,
machine-generated fake news, human-written fake news, and human-written real
news. In this paper, we study this gap by conducting a comprehensive evaluation
of fake news detectors trained in various scenarios. Our primary objectives
revolve around the following pivotal question: How to adapt fake news detectors
to the era of LLMs? Our experiments reveal an interesting pattern that
detectors trained exclusively on human-written articles can indeed perform well
at detecting machine-generated fake news, but not vice versa. Moreover, due to
the bias of detectors against machine-generated texts \cite{su2023fake}, they
should be trained on datasets with a lower machine-generated news ratio than
the test set. Building on our findings, we provide a practical strategy for the
development of robust fake news detectors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04918">Low-Resource Named Entity Recognition: Can One-vs-All AUC Maximization Help?. (arXiv:2311.04918v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Ngoc Dang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Wei Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Lan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1">Wray Buntine</a>, <a href="http://arxiv.org/find/cs/1/au:+Beare_R/0/1/0/all/0/1">Richard Beare</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changyou Chen</a></p>
<p>Named entity recognition (NER), a task that identifies and categorizes named
entities such as persons or organizations from text, is traditionally framed as
a multi-class classification problem. However, this approach often overlooks
the issues of imbalanced label distributions, particularly in low-resource
settings, which is common in certain NER contexts, like biomedical NER
(bioNER). To address these issues, we propose an innovative reformulation of
the multi-class problem as a one-vs-all (OVA) learning problem and introduce a
loss function based on the area under the receiver operating characteristic
curve (AUC). To enhance the efficiency of our OVA-based approach, we propose
two training strategies: one groups labels with similar linguistic
characteristics, and another employs meta-learning. The superiority of our
approach is confirmed by its performance, which surpasses traditional NER
learning in varying NER settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04919">The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization. (arXiv:2311.04919v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gooding_S/0/1/0/all/0/1">Sian Gooding</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansoor_H/0/1/0/all/0/1">Hassan Mansoor</a></p>
<p>Reinforcement Learning from Human Feedback (RLHF) can be used to capture
complex and nuanced properties of text generation quality. As a result, the
task of text summarization has been identified as a good candidate for this
process. In this paper, we explore how preference agreement impacts the
efficacy of RLHF for summarization. We show that sampling human preferences to
include a range of annotator agreement results in (1) higher accuracy reward
models and (2) alters the characteristics of quality captured. We additionally
show improvements in downstream generation when using a reward model trained
with a range of preference agreements. Our contributions have implications for
the design of synthetic datasets as well as the importance of considering
quality differentials in comparison-based data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04921">Successor Features for Efficient Multisubject Controlled Text Generation. (arXiv:2311.04921v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1">Meng Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1">Mehdi Fatemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Jackie Chi Kit Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1">Samira Shabanian</a></p>
<p>While large language models (LLMs) have achieved impressive performance in
generating fluent and realistic text, controlling the generated text so that it
exhibits properties such as safety, factuality, and non-toxicity remains
challenging. % such as DExperts, GeDi, and rectification Existing
decoding-based methods are static in terms of the dimension of control; if the
target subject is changed, they require new training. Moreover, it can quickly
become prohibitive to concurrently control multiple subjects. In this work, we
introduce SF-GEN, which is grounded in two primary concepts: successor features
(SFs) to decouple the LLM's dynamics from task-specific rewards, and language
model rectification to proportionally adjust the probability of selecting a
token based on the likelihood that the finished text becomes undesired. SF-GEN
seamlessly integrates the two to enable dynamic steering of text generation
with no need to alter the LLM's parameters. Thanks to the decoupling effect
induced by successor features, our method proves to be memory-wise and
computationally efficient for training as well as decoding, especially when
dealing with multiple target subjects. To the best of our knowledge, our
research represents the first application of successor features in text
generation. In addition to its computational efficiency, the resultant language
produced by our method is comparable to the SOTA (and outperforms baselines) in
both control measures as well as language quality, which we demonstrate through
a series of experiments in various controllable text generation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04922">Are cascade dialogue state tracking models speaking out of turn in spoken dialogues?. (arXiv:2311.04922v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Druart_L/0/1/0/all/0/1">Lucas Druart</a> (LIA), <a href="http://arxiv.org/find/cs/1/au:+Jacqmin_L/0/1/0/all/0/1">L&#xe9;o Jacqmin</a> (LIS), <a href="http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1">Beno&#xee;t Favre</a> (LIS), <a href="http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1">Lina Maria Rojas-Barahona</a>, <a href="http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1">Valentin Vielzeuf</a></p>
<p>In Task-Oriented Dialogue (TOD) systems, correctly updating the system's
understanding of the user's needs is key to a smooth interaction. Traditionally
TOD systems are composed of several modules that interact with one another.
While each of these components is the focus of active research communities,
their behavior in interaction can be overlooked. This paper proposes a
comprehensive analysis of the errors of state of the art systems in complex
settings such as Dialogue State Tracking which highly depends on the dialogue
context. Based on spoken MultiWoz, we identify that errors on non-categorical
slots' values are essential to address in order to bridge the gap between
spoken and chat-based dialogue systems. We explore potential solutions to
improve transcriptions and help dialogue state tracking generative models
correct such errors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04923">Is one brick enough to break the wall of spoken dialogue state tracking?. (arXiv:2311.04923v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Druart_L/0/1/0/all/0/1">Lucas Druart</a> (LIA), <a href="http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1">Valentin Vielzeuf</a>, <a href="http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1">Yannick Est&#xe8;ve</a> (LIA)</p>
<p>In Task-Oriented Dialogue (TOD) systems, correctly updating the system's
understanding of the user's needs (a.k.a dialogue state tracking) is key to a
smooth interaction. Traditionally, TOD systems perform this update in three
steps: transcription of the user's utterance, semantic extraction of the key
concepts, and contextualization with the previously identified concepts. Such
cascade approaches suffer from cascading errors and separate optimization.
End-to-End approaches have been proved helpful up to the semantic extraction
step. This paper goes one step further paving the path towards completely
neural spoken dialogue state tracking by comparing three approaches: (1) a
state of the art cascade approach, (2) a locally E2E approach with rule-based
contextualization and (3) a completely neural approach. Our study highlights
that although they all outperform the recent DSTC11 best model, especially with
a filtering post-processing step, (1) remains the most accurate approach.
Indeed, both (2) and (3) have trouble propagating context as dialogues unfold
showing that context propagation in completely neural approaches is an open
challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04924">Tuning-less Object Naming with a Foundation Model. (arXiv:2311.04924v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lucny_A/0/1/0/all/0/1">Andrej Lucny</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrovic_P/0/1/0/all/0/1">Pavel Petrovic</a></p>
<p>We implement a real-time object naming system that enables learning a set of
named entities never seen. Our approach employs an existing foundation model
that we consider ready to see anything before starting. It turns seen images
into relatively small feature vectors that we associate with index to a
gradually built vocabulary without any training of fine-tuning of the model.
Our contribution is using the association mechanism known from transformers as
attention. It has features that support generalization from irrelevant
information for distinguishing the entities and potentially enable associating
with much more than indices to vocabulary. As a result, the system can work in
a one-shot manner and correctly name objects named in different contents. We
also outline implementation details of the system modules integrated by a
blackboard architecture. Finally, we investigate the system's quality, mainly
how many objects it can handle in this way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04925">Investigating Deep-Learning NLP for Automating the Extraction of Oncology Efficacy Endpoints from Scientific Literature. (arXiv:2311.04925v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gendrin_Brokmann_A/0/1/0/all/0/1">Aline Gendrin-Brokmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Harrison_E/0/1/0/all/0/1">Eden Harrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Noveras_J/0/1/0/all/0/1">Julianne Noveras</a>, <a href="http://arxiv.org/find/cs/1/au:+Souliotis_L/0/1/0/all/0/1">Leonidas Souliotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vince_H/0/1/0/all/0/1">Harris Vince</a>, <a href="http://arxiv.org/find/cs/1/au:+Smit_I/0/1/0/all/0/1">Ines Smit</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_F/0/1/0/all/0/1">Francisco Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Milward_D/0/1/0/all/0/1">David Milward</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrievska_S/0/1/0/all/0/1">Sashka Dimitrievska</a>, <a href="http://arxiv.org/find/cs/1/au:+Metcalfe_P/0/1/0/all/0/1">Paul Metcalfe</a>, <a href="http://arxiv.org/find/cs/1/au:+Louvet_E/0/1/0/all/0/1">Emilie Louvet</a></p>
<p>Benchmarking drug efficacy is a critical step in clinical trial design and
planning. The challenge is that much of the data on efficacy endpoints is
stored in scientific papers in free text form, so extraction of such data is
currently a largely manual task. Our objective is to automate this task as much
as possible. In this study we have developed and optimised a framework to
extract efficacy endpoints from text in scientific papers, using a machine
learning approach. Our machine learning model predicts 25 classes associated
with efficacy endpoints and leads to high F1 scores (harmonic mean of precision
and recall) of 96.4% on the test set, and 93.9% and 93.7% on two case studies.
These methods were evaluated against - and showed strong agreement with -
subject matter experts and show significant promise in the future of automating
the extraction of clinical endpoints from free text. Clinical information
extraction from text data is currently a laborious manual task which scales
poorly and is prone to human error. Demonstrating the ability to extract
efficacy endpoints automatically shows great promise for accelerating clinical
trial design moving forwards.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04926">More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems. (arXiv:2311.04926v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_I/0/1/0/all/0/1">Irene Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Man_O/0/1/0/all/0/1">Owen Man</a>, <a href="http://arxiv.org/find/cs/1/au:+Mettille_S/0/1/0/all/0/1">Sophie Mettille</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_S/0/1/0/all/0/1">Sebastian Gutierrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Angelikas_K/0/1/0/all/0/1">Kenneth Angelikas</a>, <a href="http://arxiv.org/find/cs/1/au:+MacNeil_S/0/1/0/all/0/1">Stephen MacNeil</a></p>
<p>The advent of large language models is reshaping computing education. Recent
research has demonstrated that these models can produce better explanations
than students, answer multiple-choice questions at or above the class average,
and generate code that can pass automated tests in introductory courses. These
capabilities have prompted instructors to rapidly adapt their courses and
assessment methods to accommodate changes in learning objectives and the
potential for academic integrity violations. While some scholars have advocated
for the integration of visual problems as a safeguard against the capabilities
of language models, new multimodal language models now have vision and language
capabilities that may allow them to analyze and solve visual problems. In this
paper, we evaluate the performance of two large multimodal models on visual
assignments, with a specific focus on Parsons problems presented across diverse
visual representations. Our results show that GPT-4V solved 96.7\% of these
visual problems, struggling minimally with a single Parsons problem.
Conversely, Bard performed poorly by only solving 69.2\% of problems,
struggling with common issues like hallucinations and refusals. These findings
suggest that merely transitioning to visual programming problems might not be a
panacea to issues of academic integrity in the generative AI era.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04927">Contextualizing the Limits of Model &amp; Evaluation Dataset Curation on Semantic Similarity Classification Tasks. (arXiv:2311.04927v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theron_D/0/1/0/all/0/1">Daniel Theron</a></p>
<p>This paper demonstrates how the limitations of pre-trained models and open
evaluation datasets factor into assessing the performance of binary semantic
similarity classification tasks. As (1) end-user-facing documentation around
the curation of these datasets and pre-trained model training regimes is often
not easily accessible and (2) given the lower friction and higher demand to
quickly deploy such systems in real-world contexts, our study reinforces prior
work showing performance disparities across datasets, embedding techniques and
distance metrics, while highlighting the importance of understanding how data
is collected, curated and analyzed in semantic similarity classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04928">Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1">Marios Papachristou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Longqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1">Chin-Chia Hsu</a></p>
<p>In various work contexts, such as meeting scheduling, collaborating, and
project planning, collective decision-making is essential but often challenging
due to diverse individual preferences, varying work focuses, and power dynamics
among members. To address this, we propose a system leveraging Large Language
Models (LLMs) to facilitate group decision-making by managing conversations and
balancing preferences among individuals. Our system extracts individual
preferences and suggests options that satisfy a significant portion of the
members. We apply this system to corporate meeting scheduling. We create
synthetic employee profiles and simulate conversations at scale, leveraging
LLMs to evaluate the system. Our results indicate efficient coordination with
reduced interactions between members and the LLM-based system. The system also
effectively refines proposed options over time, ensuring their quality and
equity. Finally, we conduct a survey study involving human participants to
assess our system's ability to aggregate preferences and reasoning. Our
findings show that the system exhibits strong performance in both dimensions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04929">An Interdisciplinary Outlook on Large Language Models for Scientific Research. (arXiv:2311.04929v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boyko_J/0/1/0/all/0/1">James Boyko</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1">Joseph Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_N/0/1/0/all/0/1">Nathan Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+Veiga_M/0/1/0/all/0/1">Maria Han Veiga</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jennifer I-Hsiu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Modenesi_B/0/1/0/all/0/1">Bernardo Modenesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rauch_A/0/1/0/all/0/1">Andreas H. Rauch</a>, <a href="http://arxiv.org/find/cs/1/au:+Reid_K/0/1/0/all/0/1">Kenneth N. Reid</a>, <a href="http://arxiv.org/find/cs/1/au:+Tribedi_S/0/1/0/all/0/1">Soumi Tribedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Visheratina_A/0/1/0/all/0/1">Anastasia Visheratina</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xin Xie</a></p>
<p>In this paper, we describe the capabilities and constraints of Large Language
Models (LLMs) within disparate academic disciplines, aiming to delineate their
strengths and limitations with precision. We examine how LLMs augment
scientific inquiry, offering concrete examples such as accelerating literature
review by summarizing vast numbers of publications, enhancing code development
through automated syntax correction, and refining the scientific writing
process. Simultaneously, we articulate the challenges LLMs face, including
their reliance on extensive and sometimes biased datasets, and the potential
ethical dilemmas stemming from their use. Our critical discussion extends to
the varying impacts of LLMs across fields, from the natural sciences, where
they help model complex biological sequences, to the social sciences, where
they can parse large-scale qualitative data. We conclude by offering a nuanced
perspective on how LLMs can be both a boon and a boundary to scientific
progress.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04930">Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language. (arXiv:2311.04930v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hosseini_E/0/1/0/all/0/1">Eghbal A. Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1">Evelina Fedorenko</a></p>
<p>Predicting upcoming events is critical to our ability to interact with our
environment. Transformer models, trained on next-word prediction, appear to
construct representations of linguistic input that can support diverse
downstream tasks. But how does a predictive objective shape such
representations? Inspired by recent work in vision (Henaff et al., 2019), we
test a hypothesis about predictive representations of autoregressive
transformers. In particular, we test whether the neural trajectory of a
sentence becomes progressively straighter as it passes through the network
layers. The key insight is that straighter trajectories should facilitate
prediction via linear extrapolation. We quantify straightness using a
1-dimensional curvature metric, and present four findings in support of the
trajectory straightening hypothesis: i) In trained models, the curvature
decreases from the early to the deeper layers of the network. ii) Models that
perform better on the next-word prediction objective exhibit greater decreases
in curvature, suggesting that this improved ability to straighten sentence
trajectories may be the driver of better language modeling performance. iii)
Given the same linguistic context, the sequences that are generated by the
model have lower curvature than the actual continuations observed in a language
corpus, suggesting that the model favors straighter trajectories for making
predictions. iv) A consistent relationship holds between the average curvature
and the average surprisal of sentences in the deep model layers, such that
sentences with straighter trajectories also have lower surprisal. Importantly,
untrained models do not exhibit these behaviors. In tandem, these results
support the trajectory straightening hypothesis and provide a possible
mechanism for how the geometry of the internal representations of
autoregressive models supports next word prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04931">GPT4All: An Ecosystem of Open Source Compressed Language Models. (arXiv:2311.04931v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anand_Y/0/1/0/all/0/1">Yuvanesh Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1">Zach Nussbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Treat_A/0/1/0/all/0/1">Adam Treat</a>, <a href="http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1">Aaron Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1">Richard Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_B/0/1/0/all/0/1">Ben Schmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Community_G/0/1/0/all/0/1">GPT4All Community</a>, <a href="http://arxiv.org/find/cs/1/au:+Duderstadt_B/0/1/0/all/0/1">Brandon Duderstadt</a>, <a href="http://arxiv.org/find/cs/1/au:+Mulyar_A/0/1/0/all/0/1">Andriy Mulyar</a></p>
<p>Large language models (LLMs) have recently achieved human-level performance
on a range of professional and academic benchmarks. The accessibility of these
models has lagged behind their performance. State-of-the-art LLMs require
costly infrastructure; are only accessible via rate-limited, geo-locked, and
censored web interfaces; and lack publicly available code and technical
reports. In this paper, we tell the story of GPT4All, a popular open source
repository that aims to democratize access to LLMs. We outline the technical
details of the original GPT4All model family, as well as the evolution of the
GPT4All project from a single model into a fully fledged open source ecosystem.
It is our hope that this paper acts as both a technical overview of the
original GPT4All models as well as a case study on the subsequent growth of the
GPT4All open source ecosystem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04933">Evaluating Large Language Models in Ophthalmology. (arXiv:2311.04933v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1">Jason Holmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1">Shuyuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shi-Nan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jinyu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Huan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Hong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">Jie Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1">Yi Shao</a></p>
<p>Purpose: The performance of three different large language models (LLMS)
(GPT-3.5, GPT-4, and PaLM2) in answering ophthalmology professional questions
was evaluated and compared with that of three different professional
populations (medical undergraduates, medical masters, and attending
physicians). Methods: A 100-item ophthalmology single-choice test was
administered to three different LLMs (GPT-3.5, GPT-4, and PaLM2) and three
different professional levels (medical undergraduates, medical masters, and
attending physicians), respectively. The performance of LLM was comprehensively
evaluated and compared with the human group in terms of average score,
stability, and confidence. Results: Each LLM outperformed undergraduates in
general, with GPT-3.5 and PaLM2 being slightly below the master's level, while
GPT-4 showed a level comparable to that of attending physicians. In addition,
GPT-4 showed significantly higher answer stability and confidence than GPT-3.5
and PaLM2. Conclusion: Our study shows that LLM represented by GPT-4 performs
better in the field of ophthalmology. With further improvements, LLM will bring
unexpected benefits in medical education and clinical decision making in the
near future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04934">Prompt Cache: Modular Attention Reuse for Low-Latency Inference. (arXiv:2311.04934v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gim_I/0/1/0/all/0/1">In Gim</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guojun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seung-seob Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarda_N/0/1/0/all/0/1">Nikhil Sarda</a>, <a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1">Anurag Khandelwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1">Lin Zhong</a></p>
<p>We present Prompt Cache, an approach for accelerating inference for large
language models (LLM) by reusing attention states across different LLM prompts.
Many input prompts have overlapping text segments, such as system messages,
prompt templates, and documents provided for context. Our key insight is that
by precomputing and storing the attention states of these frequently occurring
text segments on the inference server, we can efficiently reuse them when these
segments appear in user prompts. Prompt Cache employs a schema to explicitly
define such reusable text segments, called prompt modules. The schema ensures
positional accuracy during attention state reuse and provides users with an
interface to access cached states in their prompt. Using a prototype
implementation, we evaluate Prompt Cache across several LLMs. We show that
Prompt Cache significantly reduce latency in time-to-first-token, especially
for longer prompts such as document-based question answering and
recommendations. The improvements range from 8x for GPU-based inference to 60x
for CPU-based inference, all while maintaining output accuracy and without the
need for model parameter modifications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04936">A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition. (arXiv:2311.04936v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barcovschi_A/0/1/0/all/0/1">Andrei Barcovschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rishabh Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1">Peter Corcoran</a></p>
<p>Automatic Speech Recognition (ASR) systems have progressed significantly in
their performance on adult speech data; however, transcribing child speech
remains challenging due to the acoustic differences in the characteristics of
child and adult voices. This work aims to explore the potential of adapting
state-of-the-art Conformer-transducer models to child speech to improve child
speech recognition performance. Furthermore, the results are compared with
those of self-supervised wav2vec2 models and semi-supervised multi-domain
Whisper models that were previously finetuned on the same data. We demonstrate
that finetuning Conformer-transducer models on child speech yields significant
improvements in ASR performance on child speech, compared to the non-finetuned
models. We also show Whisper and wav2vec2 adaptation on different child speech
datasets. Our detailed comparative analysis shows that wav2vec2 provides the
most consistent performance improvements among the three methods studied.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04939">LooGLE: Can Long-Context Language Models Understand Long Contexts?. (arXiv:2311.04939v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengmeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zilong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a></p>
<p>Large language models (LLMs), despite their impressive performance in various
language tasks, are typically limited to processing texts within context-window
size. This limitation has spurred significant research efforts to enhance LLMs'
long-context understanding with high-quality long-sequence benchmarks. However,
prior datasets in this regard suffer from shortcomings, such as short context
length compared to the context window of modern LLMs; outdated documents that
have data leakage problems; and an emphasis on short dependency tasks rather
than long dependency tasks. In this paper, we present LooGLE, a Long Context
Generic Language Evaluation benchmark for LLMs' long context understanding.
LooGLE features relatively new documents post-2022, with over 24,000 tokens per
document and 6,000 newly generated questions spanning diverse domains. Human
annotators meticulously crafted more than 1,100 high-quality question-answer
pairs to meet the long dependency requirements. These pairs underwent thorough
cross-validation, yielding the most precise assessment of LLMs' long dependency
capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed
key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs
excelled in short dependency tasks like short question-answering and cloze
tasks but struggled with more intricate long dependency tasks; (iii) in-context
learning and chaining thoughts offered only marginal improvements; (iv)
retrieval-based techniques demonstrated substantial benefits for short
question-answering, while strategies for extending context window length had
limited impact on long context understanding. As such, LooGLE not only provides
a systematic and comprehensive evaluation schema on long-context LLMs, but also
sheds light on future development of enhanced models towards "true long-context
understanding".
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04948">Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?. (arXiv:2311.04948v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Novoa_Paradela_D/0/1/0/all/0/1">David Novoa-Paradela</a>, <a href="http://arxiv.org/find/cs/1/au:+Fontenla_Romero_O/0/1/0/all/0/1">Oscar Fontenla-Romero</a>, <a href="http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1">Bertha Guijarro-Berdi&#xf1;as</a></p>
<p>This paper presents a pipeline to detect and explain anomalous reviews in
online platforms. The pipeline is made up of three modules and allows the
detection of reviews that do not generate value for users due to either
worthless or malicious composition. The classifications are accompanied by a
normality score and an explanation that justifies the decision made. The
pipeline's ability to solve the anomaly detection task was evaluated using
different datasets created from a large Amazon database. Additionally, a study
comparing three explainability techniques involving 241 participants was
conducted to assess the explainability module. The study aimed to measure the
impact of explanations on the respondents' ability to reproduce the
classification model and their perceived usefulness. This work can be useful to
automate tasks in review online platforms, such as those for electronic
commerce, and offers inspiration for addressing similar problems in the field
of anomaly detection in textual data. We also consider it interesting to have
carried out a human evaluation of the capacity of different explainability
techniques in a real and infrequent scenario such as the detection of anomalous
reviews, as well as to reflect on whether it is possible to explain tasks as
humanly subjective as this one.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04954">Prompt Sketching for Large Language Models. (arXiv:2311.04954v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beurer_Kellner_L/0/1/0/all/0/1">Luca Beurer-Kellner</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Mark Niklas M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1">Marc Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a></p>
<p>Many recent prompting strategies for large language models (LLMs) query the
model multiple times sequentially -- first to produce intermediate results and
then the final answer. However, using these methods, both decoder and model are
unaware of potential follow-up prompts, leading to disconnected and undesirably
wordy intermediate responses. In this work, we address this issue by proposing
prompt sketching, a new prompting paradigm in which an LLM does not only
respond by completing a prompt, but by predicting values for multiple variables
in a template. This way, sketching grants users more control over the
generation process, e.g., by providing a reasoning framework via intermediate
instructions, leading to better overall results. The key idea enabling
sketching with existing, autoregressive models is to adapt the decoding
procedure to also score follow-up instructions during text generation, thus
optimizing overall template likelihood in inference. Our experiments show that
in a zero-shot setting, prompt sketching outperforms existing, sequential
prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM
benchmarking tasks, including state tracking, arithmetic reasoning, and general
question answering. To facilitate future use, we release a number of generic,
yet effective sketches applicable to many tasks, and an open source library
called dclib, powering our sketch-aware decoders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04978">On the steerability of large language models toward data-driven personas. (arXiv:2311.04978v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1">Ninareh Mehrabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1">Charith Peris</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Palash Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1">Aram Galstyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rahul Gupta</a></p>
<p>The recent surge in Large Language Model (LLM) related applications has led
to a concurrent escalation in expectations for LLMs to accommodate a myriad of
personas and encompass a broad spectrum of perspectives. An important first
step towards addressing this demand is to align language models with specific
personas, be it groups of users or individuals. Towards this goal, we first
present a new conceptualization of a persona. Moving beyond the traditional
reliance on demographics like age, gender, or political party affiliation, we
introduce a data-driven persona definition methodology built on
collaborative-filtering. In this methodology, users are embedded into a
continuous vector space based on their opinions and clustered into cohorts that
manifest coherent views across specific inquiries. This methodology allows for
a more nuanced understanding of different latent social groups present in the
overall population (as opposed to simply using demographic groups) and enhances
the applicability of model steerability. Finally, we present an efficient
method to steer LLMs towards a particular persona. We learn a soft-prompting
model to map the continuous representation of users into sequences of virtual
tokens which, when prepended to the LLM input, enables the LLM to produce
responses aligned with a given user. Our results show that our steerability
algorithm is superior in performance compared to a collection of baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05014">Interpreting Pretrained Language Models via Concept Bottlenecks. (arXiv:2311.05014v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zhen Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Song Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1">Yuan Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jundong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>Pretrained language models (PLMs) have made significant strides in various
natural language processing tasks. However, the lack of interpretability due to
their ``black-box'' nature poses challenges for responsible implementation.
Although previous studies have attempted to improve interpretability by using,
e.g., attention weights in self-attention layers, these weights often lack
clarity, readability, and intuitiveness. In this research, we propose a novel
approach to interpreting PLMs by employing high-level, meaningful concepts that
are easily understandable for humans. For example, we learn the concept of
``Food'' and investigate how it influences the prediction of a model's
sentiment towards a restaurant review. We introduce C$^3$M, which combines
human-annotated and machine-generated concepts to extract hidden neurons
designed to encapsulate semantically meaningful and task-specific concepts.
Through empirical evaluations on real-world datasets, we manifest that our
approach offers valuable insights to interpret PLM behavior, helps diagnose
model failures, and enhances model robustness amidst noisy concept labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05020">First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models. (arXiv:2311.05020v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1">Naomi Saphra</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1">Adam Lopez</a></p>
<p>Many NLP researchers are experiencing an existential crisis triggered by the
astonishing success of ChatGPT and other systems based on large language models
(LLMs). After such a disruptive change to our understanding of the field, what
is left to do? Taking a historical lens, we look for guidance from the first
era of LLMs, which began in 2005 with large $n$-gram models for machine
translation. We identify durable lessons from the first era, and more
importantly, we identify evergreen problems where NLP researchers can continue
to make meaningful contributions in areas where LLMs are ascendant. Among these
lessons, we discuss the primacy of hardware advancement in shaping the
availability and importance of scale, as well as the urgent challenge of
quality evaluation, both automated and human. We argue that disparities in
scale are transient and that researchers can work to reduce them; that data,
rather than hardware, is still a bottleneck for many meaningful applications;
that meaningful evaluation informed by actual use is still an open problem; and
that there is still room for speculative approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05043">Zero-shot Translation of Attention Patterns in VQA Models to Natural Language. (arXiv:2311.05043v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1">Leonard Salewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1">A. Sophia Koepke</a>, <a href="http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1">Hendrik P. A. Lensch</a>, <a href="http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1">Zeynep Akata</a></p>
<p>Converting a model's internals to text can yield human-understandable
insights about the model. Inspired by the recent success of training-free
approaches for image captioning, we propose ZS-A2T, a zero-shot framework that
translates the transformer attention of a given model into natural language
without requiring any training. We consider this in the context of Visual
Question Answering (VQA). ZS-A2T builds on a pre-trained large language model
(LLM), which receives a task prompt, question, and predicted answer, as inputs.
The LLM is guided to select tokens which describe the regions in the input
image that the VQA model attended to. Crucially, we determine this similarity
by exploiting the text-image matching capabilities of the underlying VQA model.
Our framework does not require any training and allows the drop-in replacement
of different guiding sources (e.g. attribution instead of attention maps), or
language models. We evaluate this novel task on textual explanation datasets
for VQA, giving state-of-the-art performances for the zero-shot setting on
GQA-REX and VQA-X. Our code is available at:
https://github.com/ExplainableML/ZS-A2T.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05047">DeepLearningBrasil@LT-EDI-2023: Exploring Deep Learning Techniques for Detecting Depression in Social Media Text. (arXiv:2311.05047v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcia_E/0/1/0/all/0/1">Eduardo Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1">Juliana Gomes</a>, <a href="http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1">Adalberto Barbosa J&#xfa;nior</a>, <a href="http://arxiv.org/find/cs/1/au:+Borges_C/0/1/0/all/0/1">Cardeque Borges</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1">N&#xe1;dia da Silva</a></p>
<p>In this paper, we delineate the strategy employed by our team,
DeepLearningBrasil, which secured us the first place in the shared task
DepSign-LT-EDI@RANLP-2023, achieving a 47.0% Macro F1-Score and a notable 2.4%
advantage. The task was to classify social media texts into three distinct
levels of depression - "not depressed," "moderately depressed," and "severely
depressed." Leveraging the power of the RoBERTa and DeBERTa models, we further
pre-trained them on a collected Reddit dataset, specifically curated from
mental health-related Reddit's communities (Subreddits), leading to an enhanced
understanding of nuanced mental health discourse. To address lengthy textual
data, we used truncation techniques that retained the essence of the content by
focusing on its beginnings and endings. Our model was robust against unbalanced
data by incorporating sample weights into the loss. Cross-validation and
ensemble techniques were then employed to combine our k-fold trained models,
delivering an optimal solution. The accompanying code is made available for
transparency and further development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05051">Deep Learning Brasil at ABSAPT 2022: Portuguese Transformer Ensemble Approaches. (arXiv:2311.05051v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1">Juliana Resplande Santanna Gomes</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_E/0/1/0/all/0/1">Eduardo Augusto Santos Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1">Adalberto Ferreira Barbosa Junior</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodrigues_R/0/1/0/all/0/1">Ruan Chaves Rodrigues</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1">Diogo Fernandes Costa Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Maia_D/0/1/0/all/0/1">Dyonnatan Ferreira Maia</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1">N&#xe1;dia F&#xe9;lix Felipe da Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Filho_A/0/1/0/all/0/1">Arlindo Rodrigues Galv&#xe3;o Filho</a>, <a href="http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1">Anderson da Silva Soares</a></p>
<p>Aspect-based Sentiment Analysis (ABSA) is a task whose objective is to
classify the individual sentiment polarity of all entities, called aspects, in
a sentence. The task is composed of two subtasks: Aspect Term Extraction (ATE),
identify all aspect terms in a sentence; and Sentiment Orientation Extraction
(SOE), given a sentence and its aspect terms, the task is to determine the
sentiment polarity of each aspect term (positive, negative or neutral). This
article presents we present our participation in Aspect-Based Sentiment
Analysis in Portuguese (ABSAPT) 2022 at IberLEF 2022. We submitted the best
performing systems, achieving new state-of-the-art results on both subtasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05074">A Framework to Assess (Dis)agreement Among Diverse Rater Groups. (arXiv:2311.05074v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1">Vinodkumar Prabhakaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1">Christopher Homan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1">Lora Aroyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1">Alicia Parrish</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1">Alex Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1">Mark D&#xed;az</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Ding Wang</a></p>
<p>Recent advancements in conversational AI have created an urgent need for
safety guardrails that prevent users from being exposed to offensive and
dangerous content. Much of this work relies on human ratings and feedback, but
does not account for the fact that perceptions of offense and safety are
inherently subjective and that there may be systematic disagreements between
raters that align with their socio-demographic identities. Instead, current
machine learning approaches largely ignore rater subjectivity and use gold
standards that obscure disagreements (e.g., through majority voting). In order
to better understand the socio-cultural leanings of such tasks, we propose a
comprehensive disagreement analysis framework to measure systematic diversity
in perspectives among different rater subgroups. We then demonstrate its
utility by applying this framework to a dataset of human-chatbot conversations
rated by a demographically diverse pool of raters. Our analysis reveals
specific rater groups that have more diverse perspectives than the rest, and
informs demographic axes that are crucial to consider for safety annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05075">Mental Health Diagnosis in the Digital Age: Harnessing Sentiment Analysis on Social Media Platforms upon Ultra-Sparse Feature Content. (arXiv:2311.05075v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1">Haijian Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Ming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1">Shengjie Zhai</a></p>
<p>Amid growing global mental health concerns, particularly among vulnerable
groups, natural language processing offers a tremendous potential for early
detection and intervention of people's mental disorders via analyzing their
postings and discussions on social media platforms. However, ultra-sparse
training data, often due to vast vocabularies and low-frequency words, hinders
the analysis accuracy. Multi-labeling and Co-occurrences of symptoms may also
blur the boundaries in distinguishing similar/co-related disorders. To address
these issues, we propose a novel semantic feature preprocessing technique with
a three-folded structure: 1) mitigating the feature sparsity with a weak
classifier, 2) adaptive feature dimension with modulus loops, and 3)
deep-mining and extending features among the contexts. With enhanced semantic
features, we train a machine learning model to predict and classify mental
disorders. We utilize the Reddit Mental Health Dataset 2022 to examine
conditions such as Anxiety, Borderline Personality Disorder (BPD), and
Bipolar-Disorder (BD) and present solutions to the data sparsity challenge,
highlighted by 99.81% non-zero elements. After applying our preprocessing
technique, the feature sparsity decreases to 85.4%. Overall, our methods, when
compared to seven benchmark models, demonstrate significant performance
improvements: 8.0% in accuracy, 0.069 in precision, 0.093 in recall, 0.102 in
F1 score, and 0.059 in AUC. This research provides foundational insights for
mental health prediction and monitoring, providing innovative solutions to
navigate challenges associated with ultra-sparse data feature and intricate
multi-label classification in the domain of mental health analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05085">Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks. (arXiv:2311.05085v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Aditi Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1">Sajjadur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hannah Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_K/0/1/0/all/0/1">Kushan Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1">Estevam Hruschka</a></p>
<p>Large language models (LLMs) are proficient at generating fluent text with
minimal task-specific supervision. Yet, their ability to provide well-grounded
rationalizations for knowledge-intensive tasks remains under-explored. Such
tasks, like commonsense multiple-choice questions, require rationales based on
world knowledge to support predictions and refute alternate options. We
consider the task of generating knowledge-guided rationalization in natural
language by using expert-written examples in a few-shot manner. Surprisingly,
crowd-workers preferred knowledge-grounded rationales over crowdsourced
rationalizations, citing their factuality, sufficiency, and comprehensive
refutations. Although LLMs-generated rationales were preferable, further
improvements in conciseness and novelty are required. In another study, we show
how rationalization of incorrect model predictions erodes humans' trust in
LLM-generated rationales. Motivated by these observations, we create a
two-stage pipeline to review task predictions and eliminate potential incorrect
decisions before rationalization, enabling trustworthy rationale generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05089">Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform. (arXiv:2311.05089v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giofre_D/0/1/0/all/0/1">Daniele Giofr&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghantasala_S/0/1/0/all/0/1">Sneha Ghantasala</a></p>
<p>Since its introduction, the transformers architecture has seen great adoption
in NLP applications, but it also has limitations. Although the self-attention
mechanism allows for generating very rich representations of the input text,
its effectiveness may be limited in specialized domains such as legal, where,
for example, language models often have to process very long texts. In this
paper, we explore alternatives to replace the attention-based layers with
simpler token-mixing mechanisms: Hartley and Fourier transforms. Using these
non-parametric techniques, we train models with long input documents from
scratch in the legal domain setting. We also introduce a new hybrid Seq2Seq
architecture, a no-attention-based encoder connected with an attention-based
decoder, which performs quite well on existing summarization tasks with much
less compute and memory requirements. We believe that similar, if not better
performance, as in the case of long correlations of abstractive text
summarization tasks, can be achieved by adopting these simpler infrastructures.
This not only makes training models from scratch accessible to more people, but
also contributes to the reduction of the carbon footprint during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05112">A Survey of Large Language Models in Medicine: Progress, Application, and Challenge. (arXiv:2311.05112v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hongjian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1">Boyang Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xinyu Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiru Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sam S. Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Peilin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junling Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yining Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1">Chengfeng Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a></p>
<p>Large language models (LLMs), such as ChatGPT, have achieved substantial
attention due to their impressive human language understanding and generation
capabilities. Therefore, the application of LLMs in medicine to assist
physicians and patient care emerges as a promising research direction in both
artificial intelligence and clinical medicine. To this end, this survey
provides a comprehensive overview of the current progress, applications, and
challenges faced by LLMs in medicine. Specifically, we aim to address the
following questions: 1) What are LLMs and how can medical LLMs be built? 2)
What are the downstream performances of medical LLMs? 3) How can medical LLMs
be utilized in real-world clinical practice? 4) What challenges arise from the
use of medical LLMs? 5) How can we better construct and utilize medical LLMs?
As a result, this survey aims to provide insights into the opportunities and
challenges of LLMs in medicine and serve as a valuable resource for
constructing practical and effective medical LLMs. A regularly updated list of
practical guide resources of medical LLMs can be found at
https://github.com/AI-in-Health/MedLLMsPracticalGuide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05113">Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset. (arXiv:2311.05113v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_W/0/1/0/all/0/1">Wenyang Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yezeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weiqi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1">Kewei Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yi Zhou</a></p>
<p>Mathematical understanding and reasoning are crucial tasks for assessing the
capabilities of artificial intelligence (AI). However, existing benchmarks
either require just a few steps of reasoning, or only contain a small amount of
data in one specific topic, making it hard to analyse AI's behaviour with
reference to different problems within a specific topic in detail. In this
work, we propose Conic10K, a challenging math problem dataset on conic sections
in Chinese senior high school education. Our dataset contains various problems
with different reasoning depths, while only the knowledge from conic sections
is required. Since the dataset only involves a narrow range of knowledge, it is
easy to separately analyse the knowledge a model possesses and the reasoning
ability it has. For each problem, we provide a high-quality formal
representation, the reasoning steps, and the final solution. Experiments show
that existing large language models, including GPT-4, exhibit weak performance
on complex reasoning. We hope that our findings could inspire more advanced
techniques for precise natural language understanding and reasoning. Our
dataset and codes are available at https://github.com/whyNLP/Conic10K.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05117">Unsupervised Translation Quality Estimation Exploiting Synthetic Data and Pre-trained Multilingual Encoder. (arXiv:2311.05117v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuroda_Y/0/1/0/all/0/1">Yuto Kuroda</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1">Atsushi Fujita</a>, <a href="http://arxiv.org/find/cs/1/au:+Kajiwara_T/0/1/0/all/0/1">Tomoyuki Kajiwara</a>, <a href="http://arxiv.org/find/cs/1/au:+Ninomiya_T/0/1/0/all/0/1">Takashi Ninomiya</a></p>
<p>Translation quality estimation (TQE) is the task of predicting translation
quality without reference translations. Due to the enormous cost of creating
training data for TQE, only a few translation directions can benefit from
supervised training. To address this issue, unsupervised TQE methods have been
studied. In this paper, we extensively investigate the usefulness of synthetic
TQE data and pre-trained multilingual encoders in unsupervised sentence-level
TQE, both of which have been proven effective in the supervised training
scenarios. Our experiment on WMT20 and WMT21 datasets revealed that this
approach can outperform other unsupervised TQE methods on high- and
low-resource translation directions in predicting post-editing effort and human
evaluation score, and some zero-resource translation directions in predicting
post-editing effort.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05120">Quranic Conversations: Developing a Semantic Search tool for the Quran using Arabic NLP Techniques. (arXiv:2311.05120v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shohoud_Y/0/1/0/all/0/1">Yasser Shohoud</a>, <a href="http://arxiv.org/find/cs/1/au:+Shoman_M/0/1/0/all/0/1">Maged Shoman</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelazim_S/0/1/0/all/0/1">Sarah Abdelazim</a></p>
<p>The Holy Book of Quran is believed to be the literal word of God (Allah) as
revealed to the Prophet Muhammad (PBUH) over a period of approximately 23
years. It is the book where God provides guidance on how to live a righteous
and just life, emphasizing principles like honesty, compassion, charity and
justice, as well as providing rules for personal conduct, family matters,
business ethics and much more. However, due to constraints related to the
language and the Quran organization, it is challenging for Muslims to get all
relevant ayahs (verses) pertaining to a matter or inquiry of interest. Hence,
we developed a Quran semantic search tool which finds the verses pertaining to
the user inquiry or prompt. To achieve this, we trained several models on a
large dataset of over 30 tafsirs, where typically each tafsir corresponds to
one verse in the Quran and, using cosine similarity, obtained the tafsir tensor
which is most similar to the prompt tensor of interest, which was then used to
index for the corresponding ayah in the Quran. Using the SNxLM model, we were
able to achieve a cosine similarity score as high as 0.97 which corresponds to
the abdu tafsir for a verse relating to financial matters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05155">Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages. (arXiv:2311.05155v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1">Koustava Goswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1">Priya Rani</a>, <a href="http://arxiv.org/find/cs/1/au:+Fransen_T/0/1/0/all/0/1">Theodorus Fransen</a>, <a href="http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1">John P. McCrae</a></p>
<p>Exploiting cognates for transfer learning in under-resourced languages is an
exciting opportunity for language understanding tasks, including unsupervised
machine translation, named entity recognition and information retrieval.
Previous approaches mainly focused on supervised cognate detection tasks based
on orthographic, phonetic or state-of-the-art contextual language models, which
under-perform for most under-resourced languages. This paper proposes a novel
language-agnostic weakly-supervised deep cognate detection framework for
under-resourced languages using morphological knowledge from closely related
languages. We train an encoder to gain morphological knowledge of a language
and transfer the knowledge to perform unsupervised and weakly-supervised
cognate detection tasks with and without the pivot language for the
closely-related languages. While unsupervised, it overcomes the need for
hand-crafted annotation of cognates. We performed experiments on different
published cognate detection datasets across language families and observed not
only significant improvement over the state-of-the-art but also our method
outperformed the state-of-the-art supervised and unsupervised methods. Our
model can be extended to a wide range of languages from any language family as
it overcomes the requirement of the annotation of the cognate pairs for
training. The code and dataset building scripts can be found at
https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05161">Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization. (arXiv:2311.05161v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jangwhan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1">Seungcheol Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Seok Joong Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1">Wonyong Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jungwook Choi</a></p>
<p>Large Language Models (LLMs) are proficient in natural language processing
tasks, but their deployment is often restricted by extensive parameter sizes
and computational demands. This paper focuses on post-training quantization
(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)
quantization, to enhance computational efficiency -- a topic less explored
compared to weight-only quantization. We present two innovative techniques:
activation-quantization-aware scaling (AQAS) and sequence-length-aware
calibration (SLAC) to enhance PTQ by considering the combined effects on
weights and activations and aligning calibration sequence lengths to target
tasks. Moreover, we introduce dINT, a hybrid data format combining integer and
denormal representations, to address the underflow issue in W4A8 quantization,
where small values are rounded to zero. Through rigorous evaluations of LLMs,
including OPT and LLaMA, we demonstrate that our techniques significantly boost
task accuracies to levels comparable with full-precision models. By developing
arithmetic units compatible with dINT, we further confirm that our methods
yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC
unit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05169">Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation. (arXiv:2311.05169v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1">Diego Moll&#xe1;</a></p>
<p>This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05195">PRODIGy: a PROfile-based DIalogue Generation dataset. (arXiv:2311.05195v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Occhipinti_D/0/1/0/all/0/1">Daniela Occhipinti</a>, <a href="http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1">Serra Sinem Tekiroglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1">Marco Guerini</a></p>
<p>Providing dialogue agents with a profile representation can improve their
consistency and coherence, leading to better conversations. However, current
profile-based dialogue datasets for training such agents contain either
explicit profile representations that are simple and dialogue-specific, or
implicit representations that are difficult to collect. In this work, we
propose a unified framework in which we bring together both standard and more
sophisticated profile representations by creating a new resource where each
dialogue is aligned with all possible speaker representations such as
communication style, biographies, and personality. This framework allows to
test several baselines built using generative language models with several
profile configurations. The automatic evaluation shows that profile-based
models have better generalisation capabilities than models trained on dialogues
only, both in-domain and cross-domain settings. These results are consistent
for fine-tuned models and instruction-based LLMs. Additionally, human
evaluation demonstrates a clear preference for generations consistent with both
profile and context. Finally, to account for possible privacy concerns, all
experiments are done under two configurations: inter-character and
intra-character. In the former, the LM stores the information about the
character in its internal representation, while in the latter, the LM does not
retain any personal information but uses it only at inference time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05232">A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. (arXiv:2311.05232v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Weijiang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Weitao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1">Weihong Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhangyin Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haotian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qianglong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Weihua Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xiaocheng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bing Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a></p>
<p>The emergence of large language models (LLMs) has marked a significant
breakthrough in natural language processing (NLP), leading to remarkable
advancements in text understanding and generation. Nevertheless, alongside
these strides, LLMs exhibit a critical tendency to produce hallucinations,
resulting in content that is inconsistent with real-world facts or user inputs.
This phenomenon poses substantial challenges to their practical deployment and
raises concerns over the reliability of LLMs in real-world scenarios, which
attracts increasing attention to detect and mitigate these hallucinations. In
this survey, we aim to provide a thorough and in-depth overview of recent
advances in the field of LLM hallucinations. We begin with an innovative
taxonomy of LLM hallucinations, then delve into the factors contributing to
hallucinations. Subsequently, we present a comprehensive overview of
hallucination detection methods and benchmarks. Additionally, representative
approaches designed to mitigate hallucinations are introduced accordingly.
Finally, we analyze the challenges that highlight the current limitations and
formulate open questions, aiming to delineate pathways for future research on
hallucinations in LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05263">Model-Based Minimum Bayes Risk Decoding. (arXiv:2311.05263v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jinnai_Y/0/1/0/all/0/1">Yuu Jinnai</a>, <a href="http://arxiv.org/find/cs/1/au:+Morimura_T/0/1/0/all/0/1">Tetsuro Morimura</a>, <a href="http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1">Ukyo Honda</a>, <a href="http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1">Kaito Ariu</a>, <a href="http://arxiv.org/find/cs/1/au:+Abe_K/0/1/0/all/0/1">Kenshi Abe</a></p>
<p>Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative
to beam search decoding in a variety of text generation tasks. MBR decoding
selects a hypothesis from a pool of hypotheses that has the least expected risk
under a probability model according to a given utility function. Since it is
impractical to compute the expected risk exactly over all possible hypotheses,
two approximations are commonly used in MBR. First, it integrates over a
sampled set of hypotheses rather than over all possible hypotheses. Second, it
estimates the probability of each hypothesis using a Monte Carlo estimator.
While the first approximation is necessary to make it computationally feasible,
the second is not essential since we typically have access to the model
probability at inference time. We propose Model-Based MBR (MBMBR), a variant of
MBR that uses the model probability itself as the estimate of the probability
distribution instead of the Monte Carlo estimate. We show analytically and
empirically that the model-based estimate is more promising than the Monte
Carlo estimate in text generation tasks. Our experiments show that MBMBR
outperforms MBR in several text generation tasks, both with encoder-decoder
models and with large language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05265">Don&#x27;t Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels. (arXiv:2311.05265v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Ben Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1">Yida Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1">Carolina Scarton</a>, <a href="http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1">Kalina Bontcheva</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xingyi Song</a></p>
<p>In this paper, we address the limitations of the common data annotation and
training methods for objective single-label classification tasks. Typically,
when annotating such tasks annotators are only asked to provide a single label
for each sample and annotator disagreement is discarded when a final hard label
is decided through majority voting. We challenge this traditional approach,
acknowledging that determining the appropriate label can be difficult due to
the ambiguity and lack of context in the data samples. Rather than discarding
the information from such ambiguous annotations, our soft label method makes
use of them for training. Our findings indicate that additional annotator
information, such as confidence, secondary label and disagreement, can be used
to effectively generate soft labels. Training classifiers with these soft
labels then leads to improved performance and calibration on the hard label
test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05268">Modelling prospective memory and resilient situated communications via Wizard of Oz. (arXiv:2311.05268v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanzhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Broz_F/0/1/0/all/0/1">Frank Broz</a>, <a href="http://arxiv.org/find/cs/1/au:+Neerincx_M/0/1/0/all/0/1">Mark Neerincx</a></p>
<p>This abstract presents a scenario for human-robot action in a home setting
involving an older adult and a robot. The scenario is designed to explore the
envisioned modelling of memory for communication with a socially assistive
robots (SAR). The scenario will enable the gathering of data on failures of
speech technology and human-robot communication involving shared memory that
may occur during daily activities such as a music-listening activity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05286">Causal Inference from Text: Unveiling Interactions between Variables. (arXiv:2311.05286v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuxiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yulan He</a></p>
<p>Adjusting for latent covariates is crucial for estimating causal effects from
observational textual data. Most existing methods only account for confounding
covariates that affect both treatment and outcome, potentially leading to
biased causal effects. This bias arises from insufficient consideration of
non-confounding covariates, which are relevant only to either the treatment or
the outcome. In this work, we aim to mitigate the bias by unveiling
interactions between different variables to disentangle the non-confounding
covariates when estimating causal effects from text. The disentangling process
ensures covariates only contribute to their respective objectives, enabling
independence between variables. Additionally, we impose a constraint to balance
representations from the treatment group and control group to alleviate
selection bias. We conduct experiments on two different treatment factors under
various scenarios, and the proposed model significantly outperforms recent
strong baselines. Furthermore, our thorough analysis on earnings call
transcripts demonstrates that our model can effectively disentangle the
variables, and further investigations into real-world scenarios provide
guidance for investors to make informed decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05296">DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings. (arXiv:2311.05296v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a></p>
<p>Recent studies have proposed using large language models (LLMs) for sentence
embeddings. However, most existing LLMs are built with an autoregressive
architecture that primarily captures forward dependencies while neglecting
backward dependencies. Previous work has highlighted the importance of backward
dependencies in improving sentence embeddings. To address this issue, in this
paper, we first present quantitative evidence demonstrating the limited
learning of backward dependencies in LLMs. Then, we propose a novel approach
called Dependency-Enhanced Large Language Model (DeeLM) to improve sentence
embeddings. Specifically, we found a turning point in LLMs, where surpassing
specific LLM layers leads to a significant performance drop in the semantic
textual similarity (STS) task. STS is a crucial task for evaluating sentence
embeddings. We then extract the layers after the turning point to make them
bidirectional, allowing for the learning of backward dependencies. Extensive
experiments demonstrate that DeeLM outperforms baselines and achieves
state-of-the-art performance across various STS tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05297">Do personality tests generalize to Large Language Models?. (arXiv:2311.05297v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dorner_F/0/1/0/all/0/1">Florian E. Dorner</a>, <a href="http://arxiv.org/find/cs/1/au:+Suhr_T/0/1/0/all/0/1">Tom S&#xfc;hr</a>, <a href="http://arxiv.org/find/cs/1/au:+Samadi_S/0/1/0/all/0/1">Samira Samadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kelava_A/0/1/0/all/0/1">Augustin Kelava</a></p>
<p>With large language models (LLMs) appearing to behave increasingly human-like
in text-based interactions, it has become popular to attempt to evaluate
various properties of these models using tests originally designed for humans.
While re-using existing tests is a resource-efficient way to evaluate LLMs,
careful adjustments are usually required to ensure that test results are even
valid across human sub-populations. Thus, it is not clear to what extent
different tests' validity generalizes to LLMs. In this work, we provide
evidence that LLMs' responses to personality tests systematically deviate from
typical human responses, implying that these results cannot be interpreted in
the same way as human test results. Concretely, reverse-coded items (e.g. "I am
introverted" vs "I am extraverted") are often both answered affirmatively by
LLMs. In addition, variation across different prompts designed to "steer" LLMs
to simulate particular personality types does not follow the clear separation
into five independent personality factors from human samples. In light of these
results, we believe it is important to pay more attention to tests' validity
for LLMs before drawing strong conclusions about potentially ill-defined
concepts like LLMs' "personality".
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05332">On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving. (arXiv:2311.05332v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1">Licheng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuemeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Daocheng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1">Pinlong Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Linran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_D/0/1/0/all/0/1">Dengke Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shaoyan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yeqi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xinyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1">Min Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shuanglu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Botian Shi</a></p>
<p>The pursuit of autonomous driving technology hinges on the sophisticated
integration of perception, decision-making, and control systems. Traditional
approaches, both data-driven and rule-based, have been hindered by their
inability to grasp the nuance of complex driving environments and the
intentions of other road users. This has been a significant bottleneck,
particularly in the development of common sense reasoning and nuanced scene
understanding necessary for safe and reliable autonomous driving. The advent of
Visual Language Models (VLM) represents a novel frontier in realizing fully
autonomous vehicle driving. This report provides an exhaustive evaluation of
the latest state-of-the-art VLM, \modelnamefull, and its application in
autonomous driving scenarios. We explore the model's abilities to understand
and reason about driving scenes, make decisions, and ultimately act in the
capacity of a driver. Our comprehensive tests span from basic scene recognition
to complex causal reasoning and real-time decision-making under varying
conditions. Our findings reveal that \modelname demonstrates superior
performance in scene understanding and causal reasoning compared to existing
autonomous systems. It showcases the potential to handle out-of-distribution
scenarios, recognize intentions, and make informed decisions in real driving
contexts. However, challenges remain, particularly in direction discernment,
traffic light recognition, vision grounding, and spatial reasoning tasks. These
limitations underscore the need for further research and development. Project
is now available on GitHub for interested parties to access and utilize:
\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05350">There&#x27;s no Data Like Better Data: Using QE Metrics for MT Data Filtering. (arXiv:2311.05350v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peter_J/0/1/0/all/0/1">Jan-Thorsten Peter</a>, <a href="http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1">David Vilar</a>, <a href="http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1">Daniel Deutsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Finkelstein_M/0/1/0/all/0/1">Mara Finkelstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1">Juraj Juraska</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1">Markus Freitag</a></p>
<p>Quality Estimation (QE), the evaluation of machine translation output without
the need of explicit references, has seen big improvements in the last years
with the use of neural metrics. In this paper we analyze the viability of using
QE metrics for filtering out bad quality sentence pairs in the training data of
neural machine translation systems~(NMT). While most corpus filtering methods
are focused on detecting noisy examples in collections of texts, usually huge
amounts of web crawled data, QE models are trained to discriminate more
fine-grained quality differences. We show that by selecting the highest quality
sentence pairs in the training data, we can improve translation quality while
reducing the training size by half. We also provide a detailed analysis of the
filtering results, which highlights the differences between both approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05374">TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs. (arXiv:2311.05374v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shuyi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wenlin Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaobo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Donlin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lifeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xinhua Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1">Pengzhi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yujie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhichao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengyou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jing Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuhong Liu</a></p>
<p>Large language models (LLMs) have shown impressive capabilities across
various natural language tasks. However, evaluating their alignment with human
preferences remains a challenge. To this end, we propose a comprehensive human
evaluation framework to assess LLMs' proficiency in following instructions on
diverse real-world tasks. We construct a hierarchical task tree encompassing 7
major areas covering over 200 categories and over 800 tasks, which covers
diverse capabilities such as question answering, reasoning, multiturn dialogue,
and text generation, to evaluate LLMs in a comprehensive and in-depth manner.
We also design detailed evaluation standards and processes to facilitate
consistent, unbiased judgments from human evaluators. A test set of over 3,000
instances is released, spanning different difficulty levels and knowledge
domains. Our work provides a standardized methodology to evaluate human
alignment in LLMs for both English and Chinese. We also analyze the feasibility
of automating parts of evaluation with a strong LLM (GPT-4). Our framework
supports a thorough assessment of LLMs as they are integrated into real-world
applications. We have made publicly available the task tree, TencentLLMEval
dataset, and evaluation methodology which have been demonstrated as effective
in assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to
facilitate the benchmarking of advances in the development of safe and
human-aligned LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05379">Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation. (arXiv:2311.05379v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1">Verna Dankers</a>, <a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1">Ivan Titov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1">Dieuwke Hupkes</a></p>
<p>When training a neural network, it will quickly memorise some source-target
mappings from your dataset but never learn some others. Yet, memorisation is
not easily expressed as a binary feature that is good or bad: individual
datapoints lie on a memorisation-generalisation continuum. What determines a
datapoint's position on that spectrum, and how does that spectrum influence
neural models' performance? We address these two questions for neural machine
translation (NMT) models. We use the counterfactual memorisation metric to (1)
build a resource that places 5M NMT datapoints on a memorisation-generalisation
map, (2) illustrate how the datapoints' surface-level characteristics and a
models' per-datum training signals are predictive of memorisation in NMT, (3)
and describe the influence that subsets of that map have on NMT systems'
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05419">Mirror: A Universal Framework for Various Information Extraction Tasks. (arXiv:2311.05419v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1">Tong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Junfei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zijian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Mengsong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guoliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1">Xiaoye Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenliang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhefeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1">Baoxing Huai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a></p>
<p>Sharing knowledge between information extraction tasks has always been a
challenge due to the diverse data formats and task variations. Meanwhile, this
divergence leads to information waste and increases difficulties in building
complex applications in real scenarios. Recent studies often formulate IE tasks
as a triplet extraction problem. However, such a paradigm does not support
multi-span and n-ary extraction, leading to weak versatility. To this end, we
reorganize IE problems into unified multi-slot tuples and propose a universal
framework for various IE tasks, namely Mirror. Specifically, we recast existing
IE tasks as a multi-span cyclic graph extraction problem and devise a
non-autoregressive graph decoding algorithm to extract all spans in a single
step. It is worth noting that this graph structure is incredibly versatile, and
it supports not only complex IE tasks, but also machine reading comprehension
and classification tasks. We manually construct a corpus containing 57 datasets
for model pretraining, and conduct experiments on 30 datasets across 8
downstream tasks. The experimental results demonstrate that our model has
decent compatibility and outperforms or reaches competitive performance with
SOTA systems under few-shot and zero-shot settings. The code, model weights,
and pretraining corpus are available at https://github.com/Spico197/Mirror .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05437">LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents. (arXiv:2311.05437v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shilong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haotian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1">Tianhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xueyan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a></p>
<p>LLaVA-Plus is a general-purpose multimodal assistant that expands the
capabilities of large multimodal models. It maintains a skill repository of
pre-trained vision and vision-language models and can activate relevant tools
based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on
multimodal instruction-following data to acquire the ability to use tools,
covering visual understanding, generation, external knowledge retrieval, and
compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in
existing capabilities and exhibits new ones. It is distinct in that the image
query is directly grounded and actively engaged throughout the entire human-AI
interaction sessions, significantly improving tool use performance and enabling
new scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05450">Cognitively Inspired Components for Social Conversational Agents. (arXiv:2311.05450v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clay_A/0/1/0/all/0/1">Alex Clay</a>, <a href="http://arxiv.org/find/cs/1/au:+Alonso_E/0/1/0/all/0/1">Eduardo Alonso</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondragon_E/0/1/0/all/0/1">Esther Mondrag&#xf3;n</a></p>
<p>Current conversational agents (CA) have seen improvement in conversational
quality in recent years due to the influence of large language models (LLMs)
like GPT3. However, two key categories of problem remain. Firstly there are the
unique technical problems resulting from the approach taken in creating the CA,
such as scope with retrieval agents and the often nonsensical answers of former
generative agents. Secondly, humans perceive CAs as social actors, and as a
result expect the CA to adhere to social convention. Failure on the part of the
CA in this respect can lead to a poor interaction and even the perception of
threat by the user. As such, this paper presents a survey highlighting a
potential solution to both categories of problem through the introduction of
cognitively inspired additions to the CA. Through computational facsimiles of
semantic and episodic memory, emotion, working memory, and the ability to
learn, it is possible to address both the technical and social problems
encountered by CAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05451">All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation. (arXiv:2311.05451v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1">Pragyan Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1">Abhinav Java</a>, <a href="http://arxiv.org/find/cs/1/au:+Jandial_S/0/1/0/all/0/1">Surgan Jandial</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahid_S/0/1/0/all/0/1">Simra Shahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Furniturewala_S/0/1/0/all/0/1">Shaz Furniturewala</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1">Balaji Krishnamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1">Sumit Bhatia</a></p>
<p>Fairness in Language Models (LMs) remains a longstanding challenge, given the
inherent biases in training data that can be perpetuated by models and affect
the downstream tasks. Recent methods employ expensive retraining or attempt
debiasing during inference by constraining model outputs to contrast from a
reference set of biased templates or exemplars. Regardless, they dont address
the primary goal of fairness to maintain equitability across different
demographic groups. In this work, we posit that inferencing LMs to generate
unbiased output for one demographic under a context ensues from being aware of
outputs for other demographics under the same context. To this end, we propose
Counterfactually Aware Fair InferencE (CAFIE), a framework that dynamically
compares the model understanding of diverse demographics to generate more
equitable sentences. We conduct an extensive empirical evaluation using base
LMs of varying sizes and across three diverse datasets and found that CAFIE
outperforms strong baselines. CAFIE produces fairer text and strikes the best
balance between fairness and language modeling capability
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05472">Text Representation Distillation via Information Bottleneck Principle. (arXiv:2311.05472v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanzhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1">Dingkun Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zehan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Pengjun Xie</a></p>
<p>Pre-trained language models (PLMs) have recently shown great success in text
representation field. However, the high computational cost and high-dimensional
representation of PLMs pose significant challenges for practical applications.
To make models more accessible, an effective method is to distill large models
into smaller representation models. In order to relieve the issue of
performance degradation after distillation, we propose a novel Knowledge
Distillation method called IBKD. This approach is motivated by the Information
Bottleneck principle and aims to maximize the mutual information between the
final representation of the teacher and student model, while simultaneously
reducing the mutual information between the student model's representation and
the input data. This enables the student model to preserve important learned
information while avoiding unnecessary information, thus reducing the risk of
over-fitting. Empirical studies on two main downstream applications of text
representation (Semantic Textual Similarity and Dense Retrieval tasks)
demonstrate the effectiveness of our proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05550">Towards End-to-End Spoken Grammatical Error Correction. (arXiv:2311.05550v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banno_S/0/1/0/all/0/1">Stefano Bann&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Rao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1">Mengjie Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1">Kate M. Knill</a>, <a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1">Mark J.F. Gales</a></p>
<p>Grammatical feedback is crucial for L2 learners, teachers, and testers.
Spoken grammatical error correction (GEC) aims to supply feedback to L2
learners on their use of grammar when speaking. This process usually relies on
a cascaded pipeline comprising an ASR system, disfluency removal, and GEC, with
the associated concern of propagating errors between these individual modules.
In this paper, we introduce an alternative "end-to-end" approach to spoken GEC,
exploiting a speech recognition foundation model, Whisper. This foundation
model can be used to replace the whole framework or part of it, e.g., ASR and
disfluency removal. These end-to-end approaches are compared to more standard
cascaded approaches on the data obtained from a free-speaking spoken language
assessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC is
possible within this architecture, but the lack of available data limits
current performance compared to a system using large quantities of text-based
GEC data. Conversely, end-to-end disfluency detection and removal, which is
easier for the attention-based Whisper to learn, does outperform cascaded
approaches. Additionally, the paper discusses the challenges of providing
feedback to candidates when using end-to-end systems for spoken GEC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05552">The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation. (arXiv:2311.05552v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1">Tyler Loakman</a>, <a href="http://arxiv.org/find/cs/1/au:+Maladry_A/0/1/0/all/0/1">Aaron Maladry</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Human evaluation is often considered to be the gold standard method of
evaluating a Natural Language Generation system. However, whilst its importance
is accepted by the community at large, the quality of its execution is often
brought into question. In this position paper, we argue that the generation of
more esoteric forms of language - humour, irony and sarcasm - constitutes a
subdomain where the characteristics of selected evaluator panels are of utmost
importance, and every effort should be made to report demographic
characteristics wherever possible, in the interest of transparency and
replicability. We support these claims with an overview of each language form
and an analysis of examples in terms of how their interpretation is affected by
different participant variables. We additionally perform a critical survey of
recent works in NLG to assess how well evaluation procedures are reported in
this subdomain, and note a severe lack of open reporting of evaluator
demographic information, and a significant reliance on crowdsourcing platforms
for recruitment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05553">Removing RLHF Protections in GPT-4 via Fine-Tuning. (arXiv:2311.05553v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1">Qiusi Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1">Richard Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bindu_R/0/1/0/all/0/1">Rohan Bindu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Akul Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1">Tatsunori Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Daniel Kang</a></p>
<p>As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks.
</p>
<p>In this work, we show the contrary: fine-tuning allows attackers to remove
RLHF protections with as few as 340 examples and a 95% success rate. These
training examples can be automatically generated with weaker models. We further
show that removing RLHF protections does not decrease usefulness on
non-censored outputs, providing evidence that our fine-tuning strategy does not
decrease usefulness despite using weaker models to generate training data. Our
results show the need for further research on protections on LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05584">Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations. (arXiv:2311.05584v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Joey Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca Dragan</a></p>
<p>Large language models (LLMs) have emerged as powerful and general solutions
to many natural language tasks. However, many of the most important
applications of language generation are interactive, where an agent has to talk
to a person to reach a desired outcome. For example, a teacher might try to
understand their student's current comprehension level to tailor their
instruction accordingly, and a travel agent might ask questions of their
customer to understand their preferences in order to recommend activities they
might enjoy. LLMs trained with supervised fine-tuning or "single-step" RL, as
with standard RLHF, might struggle which tasks that require such goal-directed
behavior, since they are not trained to optimize for overall conversational
outcomes after multiple turns of interaction. In this work, we explore a new
method for adapting LLMs with RL for such goal-directed dialogue. Our key
insight is that, though LLMs might not effectively solve goal-directed dialogue
tasks out of the box, they can provide useful data for solving such tasks by
simulating suboptimal but human-like behaviors. Given a textual description of
a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic
rollouts of hypothetical in-domain human-human interactions. Our algorithm then
utilizes this dataset with offline reinforcement learning to train an
interactive conversational agent that can optimize goal-directed objectives
over multiple turns. In effect, the LLM produces examples of possible
interactions, and RL then processes these examples to learn to perform more
optimal interactions. Empirically, we show that our proposed approach achieves
state-of-the-art performance in various goal-directed dialogue tasks that
include teaching and preference elicitation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05591">Accuracy of a Vision-Language Model on Challenging Medical Cases. (arXiv:2311.05591v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buckley_T/0/1/0/all/0/1">Thomas Buckley</a>, <a href="http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1">James A. Diao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodman_A/0/1/0/all/0/1">Adam Rodman</a>, <a href="http://arxiv.org/find/cs/1/au:+Manrai_A/0/1/0/all/0/1">Arjun K. Manrai</a></p>
<p>Background: General-purpose large language models that utilize both text and
images have not been evaluated on a diverse array of challenging medical cases.
</p>
<p>Methods: Using 934 cases from the NEJM Image Challenge published between 2005
and 2023, we evaluated the accuracy of the recently released Generative
Pre-trained Transformer 4 with Vision model (GPT-4V) compared to human
respondents overall and stratified by question difficulty, image type, and skin
tone. We further conducted a physician evaluation of GPT-4V on 69 NEJM
clinicopathological conferences (CPCs). Analyses were conducted for models
utilizing text alone, images alone, and both text and images.
</p>
<p>Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%)
compared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at
all levels of difficulty and disagreement, skin tones, and image types; the
exception was radiographic images, where performance was equivalent between
GPT-4V and human respondents. Longer, more informative captions were associated
with improved performance for GPT-4V but similar performance for human
respondents. GPT-4V included the correct diagnosis in its differential for 80%
(95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45
to 70%) of CPCs when using both images and text.
</p>
<p>Conclusions: GPT-4V outperformed human respondents on challenging medical
cases and was able to synthesize information from both images and text, but
performance deteriorated when images were added to highly informative text.
Overall, our results suggest that multimodal AI models may be useful in medical
diagnostic reasoning but that their accuracy may depend heavily on context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05601">FAMuS: Frames Across Multiple Sources. (arXiv:2311.05601v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vashishtha_S/0/1/0/all/0/1">Siddharth Vashishtha</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1">Alexander Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1">William Gantt</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a>, <a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1">Aaron Steven White</a></p>
<p>Understanding event descriptions is a central aspect of language processing,
but current approaches focus overwhelmingly on single sentences or documents.
Aggregating information about an event \emph{across documents} can offer a much
richer understanding. To this end, we present FAMuS, a new corpus of Wikipedia
passages that \emph{report} on some event, paired with underlying,
genre-diverse (non-Wikipedia) \emph{source} articles for the same event. Events
and (cross-sentence) arguments in both report and source are annotated against
FrameNet, providing broad coverage of different event types. We present results
on two key event understanding tasks enabled by FAMuS: \emph{source validation}
-- determining whether a document is a valid source for a target report event
-- and \emph{cross-document argument extraction} -- full-document argument
extraction for a target event from both its report and the correct source
article. We release both FAMuS and our models to support further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05608">FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts. (arXiv:2311.05608v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yichen Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ran_D/0/1/0/all/0/1">Delong Ran</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Conglei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_T/0/1/0/all/0/1">Tianshuo Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Anyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1">Sisi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyun Wang</a></p>
<p>Large vision-language models (VLMs) like GPT-4V represent an unprecedented
revolution in the field of artificial intelligence (AI). Compared to
single-modal large language models (LLMs), VLMs possess more versatile
capabilities by incorporating additional modalities (e.g., images). Meanwhile,
there's a rising enthusiasm in the AI community to develop open-source VLMs,
such as LLaVA and MiniGPT4, which, however, have not undergone rigorous safety
assessment. In this paper, to demonstrate that more modalities lead to
unforeseen AI safety issues, we propose FigStep, a novel jailbreaking framework
against VLMs. FigStep feeds harmful instructions into VLMs through the image
channel and then uses benign text prompts to induce VLMs to output contents
that violate common AI safety policies. Our experimental results show that
FigStep can achieve an average attack success rate of 94.8% across 2 families
of popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover,
we demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which
already leverages several system-level mechanisms to filter harmful queries.
Above all, our experimental results reveal that VLMs are vulnerable to
jailbreaking attacks, which highlights the necessity of novel safety alignments
between visual and textual modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.00225">Learning From How Humans Correct. (arXiv:2102.00225v15 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tong Guo</a></p>
<p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and re-label
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we re-label the
noisy data in our dataset for our industry application. The experiment result
shows that our method improve the classification accuracy from 91.7% to 92.5%
in test dataset. The 91.7% accuracy is trained on the corrected dataset, which
improve the baseline from 83.3% to 91.7% in test dataset. The accuracy under
human evaluation achieves more than 97%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.07505">Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models. (arXiv:2104.07505v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1">Karolina Sta&#x144;czak</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1">Sagnik Ray Choudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1">Tiago Pimentel</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a></p>
<p>Recent research has demonstrated that large pre-trained language models
reflect societal biases expressed in natural language. The present paper
introduces a simple method for probing language models to conduct a
multilingual study of gender bias towards politicians. We quantify the usage of
adjectives and verbs generated by language models surrounding the names of
politicians as a function of their gender. To this end, we curate a dataset of
250k politicians worldwide, including their names and gender. Our study is
conducted in seven languages across six different language modeling
architectures. The results demonstrate that pre-trained language models' stance
towards politicians varies strongly across analyzed languages. We find that
while some words such as dead, and designated are associated with both male and
female politicians, a few specific words such as beautiful and divorced are
predominantly associated with female politicians. Finally, and contrary to
previous findings, our study suggests that larger language models do not tend
to be significantly more gender-biased than smaller ones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.09885">An Attention-Based Model for Predicting Contextual Informativeness and Curriculum Learning Applications. (arXiv:2204.09885v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1">Sungjin Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1">David Jurgens</a>, <a href="http://arxiv.org/find/cs/1/au:+Frishkoff_G/0/1/0/all/0/1">Gwen Frishkoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Collins_Thompson_K/0/1/0/all/0/1">Kevyn Collins-Thompson</a></p>
<p>Both humans and machines learn the meaning of unknown words through
contextual information in a sentence, but not all contexts are equally helpful
for learning. We introduce an effective method for capturing the level of
contextual informativeness with respect to a given target word. Our study makes
three main contributions. First, we develop models for estimating contextual
informativeness, focusing on the instructional aspect of sentences. Our
attention-based approach using pre-trained embeddings demonstrates
state-of-the-art performance on our single-context dataset and an existing
multi-sentence context dataset. Second, we show how our model identifies key
contextual elements in a sentence that are likely to contribute most to a
reader's understanding of the target word. Third, we examine how our contextual
informativeness model, originally developed for vocabulary learning
applications for students, can be used for developing better training curricula
for word embedding models in batch learning and few-shot machine learning
settings. We believe our results open new possibilities for applications that
support language learning for both human and machine learners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.00445">Interpreting Embedding Spaces by Conceptualization. (arXiv:2209.00445v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simhi_A/0/1/0/all/0/1">Adi Simhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1">Shaul Markovitch</a></p>
<p>One of the main methods for computational interpretation of a text is mapping
it into a vector in some embedding space. Such vectors can then be used for a
variety of textual processing tasks. Recently, most embedding spaces are a
product of training large language models (LLMs). One major drawback of this
type of representation is their incomprehensibility to humans. Understanding
the embedding space is crucial for several important needs, including the need
to debug the embedding method and compare it to alternatives, and the need to
detect biases hidden in the model. In this paper, we present a novel method of
understanding embeddings by transforming a latent embedding space into a
comprehensible conceptual space. We present an algorithm for deriving a
conceptual space with dynamic on-demand granularity. We devise a new evaluation
method, using either human rater or LLM-based raters, to show that the
conceptualized vectors indeed represent the semantics of the original latent
ones. We show the use of our method for various tasks, including comparing the
semantics of alternative models and tracing the layers of the LLM. The code is
available online
https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12534">Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weerasooriya_T/0/1/0/all/0/1">Tharindu Cyril Weerasooriya</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1">Sujan Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1">Tharindu Ranasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1">Marcos Zampieri</a>, <a href="http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1">Christopher M. Homan</a>, <a href="http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1">Ashiqur R. KhudaBukhsh</a></p>
<p>Offensive speech detection is a key component of content moderation. However,
what is offensive can be highly subjective. This paper investigates how machine
and human moderators disagree on what is offensive when it comes to real-world
social web political discourse. We show that (1) there is extensive
disagreement among the moderators (humans and machines); and (2) human and
large-language-model classifiers are unable to predict how other human raters
will respond, based on their political leanings. For (1), we conduct a noise
audit at an unprecedented scale that combines both machine and human responses.
For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our
noise audit reveals that moderation outcomes vary wildly across different
machine moderators. Our experiments with human moderators suggest that
political leanings combined with sensitive issues affect both first-person and
vicarious offense. The dataset is available through
https://github.com/Homan-Lab/voiced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00856">idT5: Indonesian Version of Multilingual T5 Transformer. (arXiv:2302.00856v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fuadi_M/0/1/0/all/0/1">Mukhlish Fuadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wibawa_A/0/1/0/all/0/1">Adhi Dharma Wibawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sumpeno_S/0/1/0/all/0/1">Surya Sumpeno</a></p>
<p>Indonesian language is spoken by almost 200 million people and is the 10th
most spoken language in the world, but it is under-represented in NLP (Natural
Language Processing) research. A sparsity of language resources has hampered
previous work on Indonesian. The Transformer is a new architecture rapidly
becoming dominant for NLP, surpassing alternatives like convolutional and
recurrent neural networks. T5 (Text-to-Text Transfer Transformer) is a
Transformer model that converts all text-based language problems to
text-to-text format for English. The multilingual variant is mT5 (multilingual
T5) which has shown promising results on many NLP tasks across languages.
However, the size of this multilingual model is a drawback for its application
in real production applications, which sometimes require only one language. In
this study, the mT5 model was adapted for only one language, Indonesian,
resulting in a pre-trained T5 model that was specific only for Indonesian with
a smaller size. For performance comparison, we fine-tuned this model and the
mT5 model to the Sentiment Analysis (SA), Question Generation (QG), and
Question Answering (QA) tasks with the exact mechanism and dataset. Fine-tuned
model based on our model achieved 77.18% accuracy on SA, 8% higher than the
mT5-based model, and obtained nearly the same score as the mT5-based model on
QG and QA. The results confirm that it is possible to produce a smaller
pre-trained model that maintains comparable yields while reducing the model
size by up to 58%. In addition, the resulting model requires less memory, loads
faster, and inference times faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04023">A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. (arXiv:2302.04023v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1">Yejin Bang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1">Samuel Cahyawijaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Nayeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1">Wenliang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1">Dan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1">Bryan Wilie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1">Holy Lovenia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Ziwei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tiezheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1">Willy Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1">Quyet V. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1">Pascale Fung</a></p>
<p>This paper proposes a framework for quantitatively evaluating interactive
LLMs such as ChatGPT using publicly available data sets. We carry out an
extensive technical evaluation of ChatGPT using 23 data sets covering 8
different common NLP application tasks. We evaluate the multitask, multilingual
and multi-modal aspects of ChatGPT based on these data sets and a newly
designed multimodal dataset. We find that ChatGPT outperforms LLMs with
zero-shot learning on most tasks and even outperforms fine-tuned models on some
tasks. We find that it is better at understanding non-Latin script languages
than generating them. It is able to generate multimodal content from textual
prompts, via an intermediate code generation step. Moreover, we find that
ChatGPT is 63.41% accurate on average in 10 different reasoning categories
under logical reasoning, non-textual reasoning, and commonsense reasoning,
hence making it an unreliable reasoner. It is, for example, better at deductive
than inductive reasoning. ChatGPT suffers from hallucination problems like
other LLMs and it generates more extrinsic hallucinations from its parametric
memory as it does not have access to an external knowledge base. Finally, the
interactive feature of ChatGPT enables human collaboration with the underlying
LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++
on machine translation, in a multi-turn "prompt engineering" fashion. We also
release codebase for evaluation set extraction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07687">MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Poel_S/0/1/0/all/0/1">Sam van der Poel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_D/0/1/0/all/0/1">Dakotah Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostyszyn_K/0/1/0/all/0/1">Kalina Kostyszyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tiantian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1">Rahul Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Andersen_D/0/1/0/all/0/1">Derek Andersen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1">Joanne Chau</a>, <a href="http://arxiv.org/find/cs/1/au:+Peterson_E/0/1/0/all/0/1">Emily Peterson</a>, <a href="http://arxiv.org/find/cs/1/au:+Clair_C/0/1/0/all/0/1">Cody St. Clair</a>, <a href="http://arxiv.org/find/cs/1/au:+Fodor_P/0/1/0/all/0/1">Paul Fodor</a>, <a href="http://arxiv.org/find/cs/1/au:+Shibata_C/0/1/0/all/0/1">Chihiro Shibata</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinz_J/0/1/0/all/0/1">Jeffrey Heinz</a></p>
<p>Evaluating machine learning (ML) systems on their ability to learn known
classifiers allows fine-grained examination of the patterns they can learn,
which builds confidence when they are applied to the learning of unknown
classifiers. This article presents a new benchmark for ML systems on sequence
classification called MLRegTest, which contains training, development, and test
sets from 1,800 regular languages. Different kinds of formal languages
represent different kinds of long-distance dependencies, and correctly
identifying long-distance dependencies in sequences is a known challenge for ML
systems to generalize successfully. MLRegTest organizes its languages according
to their logical complexity (monadic second order, first order, propositional,
or monomial expressions) and the kind of logical literals (string, tier-string,
subsequence, or combinations thereof). The logical complexity and choice of
literal provides a systematic way to understand different kinds of
long-distance dependencies in regular languages, and therefore to understand
the capacities of different ML systems to learn such long-distance
dependencies. Finally, the performance of different neural networks (simple
RNN, LSTM, GRU, transformer) on MLRegTest is examined. The main conclusion is
that their performance depends significantly on the kind of test set, the class
of language, and the neural network architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07880">Sabi\&#x27;a: Portuguese Large Language Models. (arXiv:2304.07880v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1">Ramon Pires</a>, <a href="http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1">Hugo Abonizio</a>, <a href="http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1">Thales Sales Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1">Rodrigo Nogueira</a></p>
<p>As the capabilities of language models continue to advance, it is conceivable
that "one-size-fits-all" model will remain as the main paradigm. For instance,
given the vast number of languages worldwide, many of which are low-resource,
the prevalent practice is to pretrain a single model on multiple languages. In
this paper, we add to the growing body of evidence that challenges this
practice, demonstrating that monolingual pretraining on the target language
significantly improves models already extensively trained on diverse corpora.
More specifically, we further pretrain GPT-J and LLaMA models on Portuguese
texts using 3% or less of their original pretraining budget. Few-shot
evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models
outperform English-centric and multilingual counterparts by a significant
margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By
evaluating on datasets originally conceived in the target language as well as
translated ones, we study the contributions of language-specific pretraining in
terms of 1) capturing linguistic nuances and structures inherent to the target
language, and 2) enriching the model's knowledge about a domain or culture. Our
results indicate that the majority of the benefits stem from the
domain-specific knowledge acquired through monolingual pretraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12894">Leveraging Human Feedback to Scale Educational Datasets: Combining Crowdworkers and Comparative Judgement. (arXiv:2305.12894v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Henkel_O/0/1/0/all/0/1">Owen Henkel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hills_L/0/1/0/all/0/1">Libby Hills</a></p>
<p>Machine Learning models have many potentially beneficial applications in
education settings, but a key barrier to their development is securing enough
data to train these models. Labelling educational data has traditionally relied
on highly skilled raters using complex, multi-class rubrics, making the process
expensive and difficult to scale. An alternative, more scalable approach could
be to use non-expert crowdworkers to evaluate student work, however,
maintaining sufficiently high levels of accuracy and inter-rater reliability
when using non-expert workers is challenging. This paper reports on two
experiments investigating using non-expert crowdworkers and comparative
judgement to evaluate complex student data. Crowdworkers were hired to evaluate
student responses to open-ended reading comprehension questions. Crowdworkers
were randomly assigned to one of two conditions: the control, where they were
asked to decide whether answers were correct or incorrect (i.e., a categorical
judgement), or the treatment, where they were shown the same question and
answers, but were instead asked to decide which of two candidate answers was
more correct (i.e., a comparative/preference-based judgement). We found that
using comparative judgement substantially improved inter-rater reliability on
both tasks. These results are in-line with well-established literature on the
benefits of comparative judgement in the field of educational assessment, as
well as with recent trends in artificial intelligence research, where
comparative judgement is becoming the preferred method for providing human
feedback on model outputs when working with non-expert crowdworkers. However,
to our knowledge, these results are novel and important in demonstrating the
beneficial effects of using the combination of comparative judgement and
crowdworkers to evaluate educational data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13903">Let&#x27;s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought. (arXiv:2305.13903v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1">Vaishnavi Himakunthala</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1">Andy Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1">Daniel Rose</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ryan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1">Alex Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yujie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1">Chinmay Sonar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1">Michael Saxon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>Despite exciting recent results showing vision-language systems' capacity to
reason about images using natural language, their capacity for video reasoning
remains under-explored. We motivate framing video reasoning as the sequential
understanding of a small number of keyframes, thereby leveraging the power and
robustness of vision-language while alleviating the computational complexities
of processing videos. To evaluate this novel application, we introduce VIP, an
inference-time challenge dataset designed to explore models' reasoning
capabilities through video chain-of-thought. Inspired by visually descriptive
scene plays, we propose two formats for keyframe description: unstructured
dense captions and structured scene descriptions that identify the focus,
action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video
reasoning, we propose two tasks: Video Infilling and Video Prediction, which
test abilities to generate multiple intermediate keyframes and predict future
keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP,
demonstrate the performance gap in these complex video reasoning tasks, and
encourage future work to prioritize language models for efficient and
generalized video reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14734">Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation. (arXiv:2305.14734v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1">Bashar Alhafni</a>, <a href="http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1">Go Inoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Khairallah_C/0/1/0/all/0/1">Christian Khairallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1">Nizar Habash</a></p>
<p>Grammatical error correction (GEC) is a well-explored problem in English with
many existing models and datasets. However, research on GEC in morphologically
rich languages has been limited due to challenges such as data scarcity and
language complexity. In this paper, we present the first results on Arabic GEC
using two newly developed Transformer-based pretrained sequence-to-sequence
models. We also define the task of multi-class Arabic grammatical error
detection (GED) and present the first results on multi-class Arabic GED. We
show that using GED information as an auxiliary input in GEC models improves
GEC performance across three datasets spanning different genres. Moreover, we
also investigate the use of contextual morphological preprocessing in aiding
GEC systems. Our models achieve SOTA results on two Arabic GEC shared task
datasets and establish a strong benchmark on a recently created dataset. We
make our code, data, and pretrained models publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11167">Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naeini_S/0/1/0/all/0/1">Saeid Naeini</a>, <a href="http://arxiv.org/find/cs/1/au:+Saqur_R/0/1/0/all/0/1">Raeid Saqur</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1">Mozhgan Saeidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1">John Giorgi</a>, <a href="http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1">Babak Taati</a></p>
<p>The quest for human imitative AI has been an enduring topic in AI research
since its inception. The technical evolution and emerging capabilities of the
latest cohort of large language models (LLMs) have reinvigorated the subject
beyond academia to the cultural zeitgeist. While recent NLP evaluation
benchmark tasks test some aspects of human-imitative behaviour (e.g.,
BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative
problem solving abilities. Creative problem solving in humans is a well-studied
topic in cognitive neuroscience with standardized tests that predominantly use
the ability to associate (heterogeneous) connections among clue words as a
metric for creativity. Exposure to misleading stimuli - distractors dubbed red
herrings - impede human performance in such tasks via the fixation effect and
Einstellung paradigm. In cognitive neuroscience studies, such fixations are
experimentally induced by pre-exposing participants to orthographically similar
incorrect words to subsequent word-fragments or clues. The popular British quiz
show Only Connect's Connecting Wall segment essentially mimics Mednick's Remote
Associates Test (RAT) formulation with built-in, deliberate red herrings, which
makes it an ideal proxy dataset to explore and study fixation effect and
Einstellung paradigm from cognitive neuroscience in LLMs. In this paper we
present the novel Only Connect Wall (OCW) dataset and report results from our
evaluation of selected pre-trained language models and LLMs on creative problem
solving tasks like grouping clue words by heterogeneous connections, and
identifying correct open knowledge domain connections in respective groups. We
synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to
further analyze our red-herrings hypothesis in language models. The code and
link to the dataset are available at https://github.com/TaatiTeam/OCW.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12929">Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1">Yelysei Bondarenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1">Markus Nagel</a>, <a href="http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1">Tijmen Blankevoort</a></p>
<p>Transformer models have been widely adopted in various domains over the last
years, and especially large language models have advanced the field of AI
significantly. Due to their size, the capability of these networks has
increased tremendously, but this has come at the cost of a significant increase
in necessary compute. Quantization is one of the most effective ways to reduce
the computational time and memory consumption of neural networks. Many studies
have shown, however, that modern transformer models tend to learn strong
outliers in their activations, making them difficult to quantize. To retain
acceptable performance, the existence of these outliers requires activations to
be in higher bitwidth or the use of different numeric formats, extra
fine-tuning, or other workarounds. We show that strong outliers are related to
very specific behavior of attention heads that try to learn a "no-op" or just a
partial update of the residual. To achieve the exact zeros needed in the
attention matrix for a no-update, the input to the softmax is pushed to be
larger and larger during training, causing outliers in other parts of the
network. Based on these observations, we propose two simple (independent)
modifications to the attention mechanism - clipped softmax and gated attention.
We empirically show that models pre-trained using our methods learn
significantly smaller outliers while maintaining and sometimes even improving
the floating-point task performance. This enables us to quantize transformers
to full INT8 quantization of the activations without any additional effort. We
demonstrate the effectiveness of our methods on both language models (BERT,
OPT) and vision transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16900">Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research. (arXiv:2306.16900v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Ji-Ung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1">Haritz Puerto</a>, <a href="http://arxiv.org/find/cs/1/au:+Aken_B/0/1/0/all/0/1">Betty van Aken</a>, <a href="http://arxiv.org/find/cs/1/au:+Arase_Y/0/1/0/all/0/1">Yuki Arase</a>, <a href="http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1">Jessica Zosa Forde</a>, <a href="http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1">Leon Derczynski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1">Andreas R&#xfc;ckl&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1">Roy Schwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a>, <a href="http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1">Jesse Dodge</a></p>
<p>Many recent improvements in NLP stem from the development and use of large
pre-trained language models (PLMs) with billions of parameters. Large model
sizes makes computational cost one of the main limiting factors for training
and evaluating such models; and has raised severe concerns about the
sustainability, reproducibility, and inclusiveness for researching PLMs. These
concerns are often based on personal experiences and observations. However,
there had not been any large-scale surveys that investigate them. In this work,
we provide a first attempt to quantify these concerns regarding three topics,
namely, environmental impact, equity, and impact on peer reviewing. By
conducting a survey with 312 participants from the NLP community, we capture
existing (dis)parities between different and within groups with respect to
seniority, academia, and industry; and their impact on the peer reviewing
process. For each topic, we provide an analysis and devise recommendations to
mitigate found disparities, some of which already successfully implemented.
Finally, we discuss additional concerns raised by many participants in
free-text responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02697">Strahler Number of Natural Language Sentences in Comparison with Random Trees. (arXiv:2307.02697v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tanaka_Ishii_K/0/1/0/all/0/1">Kumiko Tanaka-Ishii</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_A/0/1/0/all/0/1">Akira Tanaka</a></p>
<p>The Strahler number was originally proposed to characterize the complexity of
river bifurcation and has found various applications. This article proposes
computation of the Strahler number's upper and lower limits for natural
language sentence tree structures. Through empirical measurements across
grammatically annotated data, the Strahler number of natural language sentences
is shown to be almost 3 or 4, similarly to the case of river bifurcation as
reported by Strahler (1957). From the theory behind the number, we show that it
is one kind of lower limit on the amount of memory required to process
sentences. We consider the Strahler number to provide reasoning that explains
reports showing that the number of required memory areas to process sentences
is 3 to 4 for parsing (Schuler et al., 2010), and reports indicating a
psychological "magical number" of 3 to 5 (Cowan, 2001). An analytical and
empirical analysis shows that the Strahler number is not constant but grows
logarithmically; therefore, the Strahler number of sentences derives from the
range of sentence lengths. Furthermore, the Strahler number is not different
for random trees, which could suggest that its origin is not specific to
natural language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09949">Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models. (arXiv:2310.09949v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wenqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1">Marco Zeller</a>, <a href="http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1">Roger Waleffe</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1">Torsten Hoefler</a>, <a href="http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1">Gustavo Alonso</a></p>
<p>A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10378">Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Jirui Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1">Raquel Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1">Arianna Bisazza</a></p>
<p>Multilingual large-scale Pretrained Language Models (PLMs) have been shown to
store considerable amounts of factual knowledge, but large variations are
observed across languages. With the ultimate goal of ensuring that users with
different language backgrounds obtain consistent feedback from the same model,
we study the cross-lingual consistency (CLC) of factual knowledge in various
multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)
metric to evaluate knowledge consistency across languages independently from
accuracy. Using this metric, we conduct an in-depth analysis of the determining
factors for CLC, both at model level and at language-pair level. Among other
results, we find that increasing model size leads to higher factual probing
accuracy in most languages, but does not improve cross-lingual consistency.
Finally, we conduct a case study on CLC when new factual associations are
inserted in the PLMs via model editing. Results on a small sample of facts
inserted in English reveal a clear pattern whereby the new piece of knowledge
transfers only to languages with which English has a high RankC score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13620">Bridging Information-Theoretic and Geometric Compression in Language Models. (arXiv:2310.13620v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1">Emily Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1">Corentin Kervadec</a>, <a href="http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1">Marco Baroni</a></p>
<p>For a language model (LM) to faithfully model human language, it must
compress vast, potentially infinite information into relatively few dimensions.
We propose analyzing compression in (pre-trained) LMs from two points of view:
geometric and information-theoretic. We demonstrate that the two views are
highly correlated, such that the intrinsic geometric dimension of linguistic
data predicts their coding length under the LM. We then show that, in turn,
high compression of a linguistic dataset predicts rapid adaptation to that
dataset, confirming that being able to compress linguistic information is an
important part of successful LM performance. As a practical byproduct of our
analysis, we evaluate a battery of intrinsic dimension estimators for the first
time on linguistic data, showing that only some encapsulate the relationship
between information-theoretic compression, geometric compression, and
ease-of-adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14360">Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques. (arXiv:2310.14360v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhengcong Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Diya Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_D/0/1/0/all/0/1">Daniel W. Goldberg</a></p>
<p>The remarkable success of GPT models across various tasks, including toponymy
recognition motivates us to assess the performance of the GPT-3 model in the
geocoding address parsing task. To ensure that the evaluation more accurately
mirrors performance in real-world scenarios with diverse user input qualities
and resolve the pressing need for a 'gold standard' evaluation dataset for
geocoding systems, we introduce a benchmark dataset of low-quality address
descriptions synthesized based on human input patterns mining from actual input
logs of a geocoding system in production. This dataset has 21 different input
errors and variations; contains over 239,000 address records that are uniquely
selected from streets across all U.S. 50 states and D.C.; and consists of three
subsets to be used as training, validation, and testing sets. Building on this,
we train and gauge the performance of the GPT-3 model in extracting address
components, contrasting its performance with transformer-based and LSTM-based
models. The evaluation results indicate that Bidirectional LSTM-CRF model has
achieved the best performance over these transformer-based models and GPT-3
model. Transformer-based models demonstrate very comparable results compared to
the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in
performance, showcases potential in the address parsing task with few-shot
examples, exhibiting room for improvement with additional fine-tuning. We open
source the code and data of this presented benchmark so that researchers can
utilize it for future model development or extend it to evaluate similar tasks,
such as document geocoding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15720">Ensemble of Task-Specific Language Models for Brain Encoding. (arXiv:2310.15720v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1">Arvindh Arun</a>, <a href="http://arxiv.org/find/cs/1/au:+John_J/0/1/0/all/0/1">Jerrin John</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumaran_S/0/1/0/all/0/1">Sanjai Kumaran</a></p>
<p>Language models have been shown to be rich enough to encode fMRI activations
of certain Regions of Interest in our Brains. Previous works have explored
transfer learning from representations learned for popular natural language
processing tasks for predicting brain responses. In our work, we improve the
performance of such encoders by creating an ensemble model out of 10 popular
Language Models (2 syntactic and 8 semantic). We beat the current baselines by
10% on average across all ROIs through our ensembling methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02271">FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization. (arXiv:2311.02271v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Nan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yusen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1">Prasenjit Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a></p>
<p>Summaries of medical text shall be faithful by being consistent and factual
with source inputs, which is an important but understudied topic for safety and
efficiency in healthcare. In this paper, we investigate and improve
faithfulness in summarization on a broad range of medical summarization tasks.
Our investigation reveals that current summarization models often produce
unfaithful outputs for medical input text. We then introduce FaMeSumm, a
framework to improve faithfulness by fine-tuning pre-trained language models
based on medical knowledge. FaMeSumm performs contrastive learning on designed
sets of faithful and unfaithful summaries, and it incorporates medical terms
and their contexts to encourage faithful generation of medical terms. We
conduct comprehensive experiments on three datasets in two languages: health
question and radiology report summarization datasets in English, and a
patient-doctor dialogue dataset in Chinese. Results demonstrate that FaMeSumm
is flexible and effective by delivering consistent improvements over mainstream
language models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art
performances on metrics for faithfulness and general quality. Human evaluation
by doctors also shows that FaMeSumm generates more faithful outputs. Our code
is available at https://github.com/psunlpgroup/FaMeSumm .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02408">Citance-Contextualized Summarization of Scientific Papers. (arXiv:2311.02408v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1">Shahbaz Syed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hakimi_A/0/1/0/all/0/1">Ahmad Dawar Hakimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1">Khalid Al-Khatib</a>, <a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1">Martin Potthast</a></p>
<p>Current approaches to automatic summarization of scientific papers generate
informative summaries in the form of abstracts. However, abstracts are not
intended to show the relationship between a paper and the references cited in
it. We propose a new contextualized summarization approach that can generate an
informative summary conditioned on a given sentence containing the citation of
a reference (a so-called ``citance''). This summary outlines the content of the
cited paper relevant to the citation location. Thus, our approach extracts and
models the citances of a paper, retrieves relevant passages from cited papers,
and generates abstractive summaries tailored to each citance. We evaluate our
approach using $\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing
540K~computer science papers and 4.6M~citances therein.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03663">Principles from Clinical Research for NLP Model Generalization. (arXiv:2311.03663v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1">Aparna Elangovan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jiayuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1">Karin Verspoor</a></p>
<p>The NLP community typically relies on performance of a model on a held-out
test set to assess generalization. Performance drops observed in datasets
outside of official test sets are generally attributed to
"out-of-distribution'' effects. Here, we explore the foundations of
generalizability and study the various factors that affect it, articulating
generalizability lessons from clinical studies. In clinical research
generalizability depends on (a) internal validity of experiments to ensure
controlled measurement of cause and effect, and (b) external validity or
transportability of the results to the wider population. We present the need to
ensure internal validity when building machine learning models in natural
language processing, especially where results may be impacted by spurious
correlations in the data. We demonstrate how spurious factors, such as the
distance between entities in relation extraction tasks, can affect model
internal validity and in turn adversely impact generalization. We also offer
guidance on how to analyze generalization failures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03755">Multilingual Mathematical Autoformalization. (arXiv:2311.03755v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1">Albert Q. Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1">Mateja Jamnik</a></p>
<p>Autoformalization is the task of translating natural language materials into
machine-verifiable formalisations. Progress in autoformalization research is
hindered by the lack of a sizeable dataset consisting of informal-formal pairs
expressing the same essence. Existing methods tend to circumvent this challenge
by manually curating small corpora or using few-shot learning with large
language models. But these methods suffer from data scarcity and formal
language acquisition difficulty. In this work, we create $\texttt{MMA}$, a
large, flexible, multilingual, and multi-domain dataset of informal-formal
pairs, by using a language model to translate in the reverse direction, that
is, from formal mathematical statements into corresponding informal ones.
Experiments show that language models fine-tuned on $\texttt{MMA}$ produce
$16-18\%$ of statements acceptable with minimal corrections on the
$\texttt{miniF2F}$ and $\texttt{ProofNet}$ benchmarks, up from $0\%$ with the
base model. We demonstrate that fine-tuning on multilingual formal data results
in more capable autoformalization models even when deployed on monolingual
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03839">Aspects of human memory and Large Language Models. (arXiv:2311.03839v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1">Romuald A. Janik</a></p>
<p>Large Language Models (LLMs) are huge artificial neural networks which
primarily serve to generate text, but also provide a very sophisticated
probabilistic model of language use. Since generating a semantically consistent
text requires a form of effective memory, we investigate the memory properties
of LLMs and find surprising similarities with key characteristics of human
memory. We argue that the human-like memory properties of the Large Language
Model do not follow automatically from the LLM architecture but are rather
learned from the statistics of the training textual data. These results
strongly suggest that the biological features of human memory leave an imprint
on the way that we structure our textual narratives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04661">Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chenmien Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a></p>
<p>While large language models (LLMs) have enabled learning knowledge from the
pre-training corpora, the acquired knowledge may be fundamentally incorrect or
outdated over time, which necessitates rectifying the knowledge of the language
model (LM) after the training. A promising approach involves employing a
hyper-network to generate parameter shift, whereas existing hyper-networks
suffer from inferior scalability in synchronous editing operation amount. To
mitigate the problem, we propose the MAssive Language Model Editing Network
(MALMEN), which formulates the parameter shift aggregation as the least square
problem, subsequently updating the LM parameters using the normal equation. To
accommodate editing multiple facts simultaneously with limited memory budgets,
we separate the computation on the hyper-network and LM, enabling arbitrary
batch size on both neural networks. Our method is evaluated by editing up to
thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,
T5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,
i.e., closed book fact-checking and question answering. Remarkably, MALMEN is
capable of editing hundreds of times more facts than strong baselines with the
identical hyper-network architecture and outperforms editor specifically
designed for GPT. Our code is available at
https://github.com/ChenmienTan/malmen.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04879">LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. (arXiv:2311.04879v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jianxin Yang</a></p>
<p>We present LongQLoRA, an efficient and effective method to extend context
length of large language models with less training resources. LongQLoRA
combines the advantages of Position Interpolation, QLoRA and Shift Short
Attention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the
context length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within
1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on
PG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close
to MPT-7B-8K within the evaluation context length of 8192. We collect and build
39k long instruction data to extend context length of Vicuna-13B from 4096 to
8192 and achieve good performance both in long and short context generation
task. We also do some ablation experiments to study the effect of LoRA rank,
finetuning steps and attention patterns in inference.The model weights,
training data and code are avaliable at
https://github.com/yangjianxin1/LongQLoRA.
</p>
</p>
</div>

    </div>
    </body>
    