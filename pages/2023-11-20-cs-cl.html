<!DOCTYPE html>
<html>
<head>
<title>2023-11-20-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.09224">The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks. (arXiv:2311.09224v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsamados_A/0/1/0/all/0/1">Andreas Tsamados</a>, <a href="http://arxiv.org/find/cs/1/au:+Floridi_L/0/1/0/all/0/1">Luciano Floridi</a>, <a href="http://arxiv.org/find/cs/1/au:+Taddeo_M/0/1/0/all/0/1">Mariarosaria Taddeo</a></p>
<p>The widespread integration of autoregressive-large language models (AR-LLMs),
such as ChatGPT, across established applications, like search engines, has
introduced critical vulnerabilities with uniquely scalable characteristics. In
this commentary, we analyse these vulnerabilities, their dependence on natural
language as a vector of attack, and their challenges to cybersecurity best
practices. We offer recommendations designed to mitigate these challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09252">In the Red(dit): Social Media and Stock Prices. (arXiv:2311.09252v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baker_J/0/1/0/all/0/1">James Baker</a></p>
<p>Spearheaded by retail traders on the website reddit, the GameStop short
squeeze of early 2021 shows that social media embeds information that
correlates with market movements. This paper seeks to examine this relationship
by using daily frequencies of classified comments and buzzwords as additional
factors in a Fama-French three factor model. Comments are classified using an
unsupervised clustering method, while past studies have used pretrained models
that are not specific to the domains being studied.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09263">Auto-ICL: In-Context Learning without Human Supervision. (arXiv:2311.09263v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinghan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>In the era of Large Language Models (LLMs), human-computer interaction has
evolved towards natural language, offering unprecedented flexibility. Despite
this, LLMs are heavily reliant on well-structured prompts to function
efficiently within the realm of In-Context Learning. Vanilla In-Context
Learning relies on human-provided contexts, such as labeled examples, explicit
instructions, or other guiding mechanisms that shape the model's outputs. To
address this challenge, our study presents a universal framework named
Automatic In-Context Learning. Upon receiving a user's request, we ask the
model to independently generate examples, including labels, instructions, or
reasoning pathways. The model then leverages this self-produced context to
tackle the given problem. Our approach is universally adaptable and can be
implemented in any setting where vanilla In-Context Learning is applicable. We
demonstrate that our method yields strong performance across a range of tasks,
standing up well when compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09277">Contrastive Chain-of-Thought Prompting. (arXiv:2311.09277v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1">Yew Ken Chia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guizhen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1">Luu Anh Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1">Soujanya Poria</a>, <a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1">Lidong Bing</a></p>
<p>Despite the success of chain of thought in enhancing language model
reasoning, the underlying process remains less well understood. Although
logically sound reasoning appears inherently crucial for chain of thought,
prior studies surprisingly reveal minimal impact when using invalid
demonstrations instead. Furthermore, the conventional chain of thought does not
inform language models on what mistakes to avoid, which potentially leads to
more errors. Hence, inspired by how humans can learn from both positive and
negative examples, we propose contrastive chain of thought to enhance language
model reasoning. Compared to the conventional chain of thought, our approach
provides both valid and invalid reasoning demonstrations, to guide the model to
reason step-by-step while reducing reasoning mistakes. To improve
generalization, we introduce an automatic method to construct contrastive
demonstrations. Our experiments on reasoning benchmarks demonstrate that
contrastive chain of thought can serve as a general enhancement of
chain-of-thought prompting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09278">Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models. (arXiv:2311.09278v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Fangzhi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qiushi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Siyu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1">Fei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1">Shuai Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qika Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a></p>
<p>Large Language Models (LLMs) have greatly propelled the progress in natural
language(NL)-centric tasks based on NL interface. However, the NL form is not
enough for world knowledge. Current works focus on this question by injecting
specific symbolic knowledge into LLM, which ignore two critical challenges: the
interrelations between various symbols and the balance between symbolic-centric
and NL-centric capabilities. In this work, we tackle these challenges from both
a data and framework perspective and introduce Symbol-LLM series models. First,
we collect 34 symbolic tasks, covering ~20 different forms, which are unified
to capture symbol interrelations. Then, a two-stage tuning framework succeeds
in injecting symbolic knowledge without loss of the generality ability.
Extensive experiments on both symbol- and NL-centric tasks demonstrate the
balanced and superior performances of Symbol-LLM series models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09308">Divergences between Language Models and Human Brains. (arXiv:2311.09308v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuchen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1">Emmy Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1">Graham Neubig</a>, <a href="http://arxiv.org/find/cs/1/au:+Wehbe_L/0/1/0/all/0/1">Leila Wehbe</a></p>
<p>Do machines and humans process language in similar ways? A recent line of
research has hinted in the affirmative, demonstrating that human brain signals
can be effectively predicted using the internal representations of language
models (LMs). This is thought to reflect shared computational principles
between LMs and human language processing. However, there are also clear
differences in how LMs and humans acquire and use language, even if the final
task they are performing is the same. Despite this, there is little work
exploring systematic differences between human and machine language processing
using brain data. To address this question, we examine the differences between
LM representations and the human brain's responses to language, specifically by
examining a dataset of Magnetoencephalography (MEG) responses to a written
narrative. In doing so we identify three phenomena that, in prior work, LMs
have been found to not capture well: emotional understanding, figurative
language processing, and physical commonsense. By fine-tuning LMs on datasets
related to these phenomena, we observe that fine-tuned LMs show improved
alignment with human brain responses across these tasks. Our study implies that
the observed divergences between LMs and human brains may stem from LMs'
inadequate representation of these specific types of knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09319">Spoken Word2Vec: A Perspective And Some Techniques. (arXiv:2311.09319v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sayeed_M/0/1/0/all/0/1">Mohammad Amaan Sayeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1">Hanan Aldarmaki</a></p>
<p>Text word embeddings that encode distributional semantic features work by
modeling contextual similarities of frequently occurring words. Acoustic word
embeddings, on the other hand, typically encode low-level phonetic
similarities. Semantic embeddings for spoken words have been previously
explored using similar algorithms to Word2Vec, but the resulting vectors still
mainly encoded phonetic rather than semantic features. In this paper, we
examine the assumptions and architectures used in previous works and show
experimentally how Word2Vec algorithms fail to encode distributional semantics
when the input units are acoustically correlated. In addition, previous works
relied on the simplifying assumptions of perfect word segmentation and
clustering by word type. Given these conditions, a trivial solution identical
to text-based embeddings has been overlooked. We follow this simpler path using
automatic word type clustering and examine the effects on the resulting
embeddings, highlighting the true challenges in this task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09325">Improving fit to human reading times via temperature-scaled surprisal. (arXiv:2311.09325v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Skrjanec_I/0/1/0/all/0/1">Iza &#x160;krjanec</a>, <a href="http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1">Vera Demberg</a></p>
<p>Past studies have provided broad support for that words with lower
predictability (i.e., higher surprisal) require more time for comprehension by
using large language models (LLMs) to simulate humans' cognitive load. In
general, these studies have implicitly assumed that the probability scores from
LLMs are accurate, ignoring the discrepancies between human cognition and LLMs
from this standpoint. Inspired by the concept of probability calibration, we
are the first work to focus on the probability distribution for human reading
simulation. We propose to use temperature-scaled surprisal, a surprisal
calculated by shaped probability, to be the predictor of human reading times.
Our results across three corpora consistently revealed that such a surprisal
can drastically improve the prediction of reading times. Setting the
temperature to be approximately 2.5 across all models and datasets can yield up
to an 89% of increase in delta log-likelihood in our setting. We also propose a
calibration metric to quantify the possible human-likeness bias. Further
analysis was done and provided insights into this phenomenon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09335">Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1">George Chrysostomou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhixue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Miles Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a></p>
<p>Despite their remarkable performance on abstractive summarization, large
language models (LLMs) face two significant challenges: their considerable size
and tendency to hallucinate. Hallucinations are concerning because they erode
the reliability of LLMs and raise safety issues. Pruning is a technique that
reduces model size by removing redundant weights to create sparse models that
enable more efficient inference. Pruned models yield comparable performance to
their counterpart full-sized models, making them ideal alternatives when
operating on a limited budget. However, the effect that pruning has upon
hallucinations in abstractive summarization with LLMs has yet to be explored.
In this paper, we provide an extensive empirical study on the hallucinations
produced by pruned models across three standard summarization tasks, two
pruning approaches, three instruction-tuned LLMs, and three hallucination
evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less
compared to their full-sized counterparts. Our follow-up analysis suggests that
pruned models tend to depend more on the source input and less on their
parametric knowledge from pre-training for generation. This greater dependency
on the source input leads to a higher lexical overlap between generated content
and the source input, which can be a reason for the reduction in
hallucinations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09336">Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback. (arXiv:2311.09336v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenda Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1">Daniel Deutsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Finkelstein_M/0/1/0/all/0/1">Mara Finkelstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1">Juraj Juraska</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Biao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhongtao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1">Markus Freitag</a></p>
<p>Recent improvements in text generation have leveraged human feedback to
improve the quality of the generated output. However, human feedback is not
always available, especially during inference. In this work, we propose an
inference time optimization method FITO to use fine-grained actionable feedback
in the form of error type, error location and severity level that are predicted
by a learned error pinpoint model for iterative refinement. FITO starts with an
initial output, then iteratively incorporates the feedback via a refinement
model that generates an improved output conditioned on the feedback. Given the
uncertainty of consistent refined samples at iterative steps, we formulate
iterative refinement into a local search problem and develop a simulated
annealing based algorithm that balances exploration of the search space and
optimization for output quality. We conduct experiments on three text
generation tasks, including machine translation, long-form question answering
(QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on
Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at
long form QA and topic summarization respectively, with a single iteration of
refinement. With our simulated annealing algorithm, we see further quality
improvements, including up to 1.7 MetricX improvements over the baseline
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09344">Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization. (arXiv:2311.09344v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1">Alexandra Chronopoulou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1">Jonas Pfeiffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1">Joshua Maynez</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1">Sebastian Ruder</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Priyanka Agrawal</a></p>
<p>Parameter-efficient fine-tuning (PEFT) using labeled task data can
significantly improve the performance of large language models (LLMs) on the
downstream task. However, there are 7000 languages in the world and many of
these languages lack labeled data for real-world language generation tasks. In
this paper, we propose to improve zero-shot cross-lingual transfer by composing
language or task specialized parameters. Our method composes language and task
PEFT modules via element-wise arithmetic operations to leverage unlabeled data
and English labeled data. We extend our approach to cases where labeled data
from more languages is available and propose to arithmetically compose PEFT
modules trained on languages related to the target. Empirical results on
summarization demonstrate that our method is an effective strategy that obtains
consistent gains using minimal training of PEFT modules.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09356">LePaRD: A Large-Scale Dataset of Judges Citing Precedents. (arXiv:2311.09356v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahari_R/0/1/0/all/0/1">Robert Mahari</a>, <a href="http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1">Dominik Stammbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1">Elliott Ash</a>, <a href="http://arxiv.org/find/cs/1/au:+Pentland_A/0/1/0/all/0/1">Alex `Sandy&#x27; Pentland</a></p>
<p>We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive
collection of U.S. federal judicial citations to precedent in context. The
dataset aims to facilitate work on legal passage prediction, a challenging
practice-oriented legal retrieval and reasoning task. Legal passage prediction
seeks to predict relevant passages from precedential court decisions given the
context of a legal argument. We extensively evaluate various retrieval
approaches on LePaRD, and find that classification appears to work best.
However, we note that legal precedent prediction is a difficult task, and there
remains significant room for improvement. We hope that by publishing LePaRD, we
will encourage others to engage with a legal NLP task that promises to help
expand access to justice by reducing the burden associated with legal research.
A subset of the LePaRD dataset is freely available and the whole dataset will
be released upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09358">Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science. (arXiv:2311.09358v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1">Sridevi Wagle</a>, <a href="http://arxiv.org/find/cs/1/au:+Munikoti_S/0/1/0/all/0/1">Sai Munikoti</a>, <a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1">Anurag Acharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1">Sara Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Horawalavithana_S/0/1/0/all/0/1">Sameera Horawalavithana</a></p>
<p>Large language models (LLMs) have shown remarkable achievements in natural
language processing tasks, producing high-quality outputs. However, LLMs still
exhibit limitations, including the generation of factually incorrect
information. In safety-critical applications, it is important to assess the
confidence of LLM-generated content to make informed decisions. Retrieval
Augmented Language Models (RALMs) is relatively a new area of research in NLP.
RALMs offer potential benefits for scientific NLP tasks, as retrieved
documents, can serve as evidence to support model-generated content. This
inclusion of evidence enhances trustworthiness, as users can verify and explore
the retrieved documents to validate model outputs. Quantifying uncertainty in
RALM generations further improves trustworthiness, with retrieved text and
confidence scores contributing to a comprehensive and reliable model for
scientific applications. However, there is limited to no research on UQ for
RALMs, particularly in scientific contexts. This study aims to address this gap
by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific
tasks. This research investigates how uncertainty scores vary when scientific
knowledge is incorporated as pretraining and retrieval data and explores the
relationship between uncertainty scores and the accuracy of model-generated
outputs. We observe that an existing RALM finetuned with scientific knowledge
as the retrieval data tends to be more confident in generating predictions
compared to the model pretrained only with scientific knowledge. We also found
that RALMs are overconfident in their predictions, making inaccurate
predictions more confidently than accurate ones. Scientific knowledge provided
either as pretraining or retrieval corpus does not help alleviate this issue.
We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09363">Investigating the Emergent Audio Classification Ability of ASR Foundation Models. (arXiv:2311.09363v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Rao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1">Adian Liusie</a>, <a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1">Mark J. F. Gales</a>, <a href="http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1">Kate M. Knill</a></p>
<p>Text and vision foundation models can perform many tasks in a zero-shot
setting, a desirable property that enables these systems to be applied in
general and low-resource settings. However, there has been significantly less
work on the zero-shot abilities of ASR foundation models, with these systems
typically fine-tuned to specific tasks or constrained to applications that
match their training criterion and data annotation. In this work we investigate
the ability of Whisper and MMS, ASR foundation models trained primarily for
speech recognition, to perform zero-shot audio classification. We use simple
template-based text prompts at the decoder and use the resulting decoding
probabilities to generate zero-shot predictions. Without training the model on
extra data or adding any new parameters, we demonstrate that Whisper shows
promising zero-shot classification performance on a range of 8
audio-classification datasets, outperforming existing state-of-the-art
zero-shot baseline's accuracy by an average of 9%. One important step to unlock
the emergent ability is debiasing, where a simple unsupervised reweighting
method of the class probabilities yields consistent significant performance
gains. We further show that performance increases with model size, implying
that as ASR foundation models scale up, they may exhibit improved zero-shot
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09366">LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction. (arXiv:2311.09366v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McCusker_J/0/1/0/all/0/1">Jamie McCusker</a></p>
<p>While the potential of Open Information Extraction (Open IE) for Knowledge
Graph Construction (KGC) may seem promising, we find that the alignment of Open
IE extraction results with existing knowledge graphs to be inadequate. The
advent of Large Language Models (LLMs), especially the commercially available
OpenAI models, have reset expectations for what is possible with deep learning
models and have created a new field called prompt engineering. We investigate
the use of GPT models and prompt engineering for knowledge graph construction
with the Wikidata knowledge graph to address a similar problem to Open IE,
which we call Open Knowledge Extraction (OKE) using an approach we call the
Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the
entity linking task essential to construction of real world knowledge graphs.
We merge the CaRB benchmark scoring approach with data from the TekGen dataset
for the LOKE task. We then show that a well engineered prompt, paired with a
naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's
OpenIE 4 implementation on the OKE task, although it over-generates triples
compared to the reference set due to overall triple scarcity in the TekGen set.
Through an analysis of entity linkability in the CaRB dataset, as well as
outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver"
TekGen triples show that the task is significantly different in content from
OIE, if not structure. Through this analysis and a qualitative analysis of
sentence extractions via all methods, we found that LOKE-GPT extractions are of
high utility for the KGC task and suitable for use in semi-automated extraction
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09367">A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms. (arXiv:2311.09367v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mane_S/0/1/0/all/0/1">Swapnil Mane</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1">Suman Kundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Rajesh Sharma</a></p>
<p>The rise of social media platforms has led to an increase in cyber-aggressive
behavior, encompassing a broad spectrum of hostile behavior, including
cyberbullying, online harassment, and the dissemination of offensive and hate
speech. These behaviors have been associated with significant societal
consequences, ranging from online anonymity to real-world outcomes such as
depression, suicidal tendencies, and, in some instances, offline violence.
Recognizing the societal risks associated with unchecked aggressive content,
this paper delves into the field of Aggression Content Detection and Behavioral
Analysis of Aggressive Users, aiming to bridge the gap between disparate
studies. In this paper, we analyzed the diversity of definitions and proposed a
unified cyber-aggression definition. We examine the comprehensive process of
Aggression Content Detection, spanning from dataset creation, feature selection
and extraction, and detection algorithm development. Further, we review studies
on Behavioral Analysis of Aggression that explore the influencing factors,
consequences, and patterns associated with cyber-aggressive behavior. This
systematic literature review is a cross-examination of content detection and
behavioral analysis in the realm of cyber-aggression. The integrated
investigation reveals the effectiveness of incorporating sociological insights
into computational techniques for preventing cyber-aggressive behavior.
Finally, the paper concludes by identifying research gaps and encouraging
further progress in the unified domain of socio-computational aggressive
behavior analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09383">Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach. (arXiv:2311.09383v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akash_P/0/1/0/all/0/1">Pritom Saha Akash</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kashob Kumar Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1">Lucian Popa</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kevin Chen-Chuan Chang</a></p>
<p>Long-form question answering (LFQA) poses a challenge as it involves
generating detailed answers in the form of paragraphs, which go beyond simple
yes/no responses or short factual answers. While existing QA models excel in
questions with concise answers, LFQA requires handling multiple topics and
their intricate relationships, demanding comprehensive explanations. Previous
attempts at LFQA focused on generating long-form answers by utilizing relevant
contexts from a corpus, relying solely on the question itself. However, they
overlooked the possibility that the question alone might not provide sufficient
information to identify the relevant contexts. Additionally, generating
detailed long-form answers often entails aggregating knowledge from diverse
sources. To address these limitations, we propose an LFQA model with iterative
Planning, Retrieval, and Generation. This iterative process continues until a
complete answer is generated for the given question. From an extensive
experiment on both an open domain and a technical domain QA dataset, we find
that our model outperforms the state-of-the-art models on various textual and
factual metrics for the LFQA task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09387">Banach-Tarski Embeddings and Transformers. (arXiv:2311.09387v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maher_J/0/1/0/all/0/1">Joshua Maher</a></p>
<p>We introduce a new construction of embeddings of arbitrary recursive data
structures into high dimensional vectors. These embeddings provide an
interpretable model for the latent state vectors of transformers. We
demonstrate that these embeddings can be decoded to the original data structure
when the embedding dimension is sufficiently large. This decoding algorithm has
a natural implementation as a transformer. We also show that these embedding
vectors can be manipulated directly to perform computations on the underlying
data without decoding. As an example we present an algorithm that constructs
the embedded parse tree of an embedded token sequence using only vector
operations in embedding space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09389">Neural machine translation for automated feedback on children&#x27;s early-stage writing. (arXiv:2311.09389v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jensen_J/0/1/0/all/0/1">Jonas Vestergaard Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordahn_M/0/1/0/all/0/1">Mikkel Jordahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Andersen_M/0/1/0/all/0/1">Michael Riis Andersen</a></p>
<p>In this work, we address the problem of assessing and constructing feedback
for early-stage writing automatically using machine learning. Early-stage
writing is typically vastly different from conventional writing due to phonetic
spelling and lack of proper grammar, punctuation, spacing etc. Consequently,
early-stage writing is highly non-trivial to analyze using common linguistic
metrics. We propose to use sequence-to-sequence models for "translating"
early-stage writing by students into "conventional" writing, which allows the
translated text to be analyzed using linguistic metrics. Furthermore, we
propose a novel robust likelihood to mitigate the effect of noise in the
dataset. We investigate the proposed methods using a set of numerical
experiments and demonstrate that the conventional text can be predicted with
high accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09390">LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems. (arXiv:2311.09390v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1">Nalin Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1">Ond&#x159;ej Du&#x161;ek</a></p>
<p>Linguistic entrainment, or alignment, represents a phenomenon where
linguistic patterns employed by conversational participants converge to one
another. While alignment has been shown to produce a more natural user
experience, most dialogue systems do not have any provisions for it. In this
work, we introduce methods for achieving dialogue alignment in a GPT-2-based
end-to-end dialogue system through the utilization of shared vocabulary. We
experiment with training instance weighting, alignment-specific loss, and
additional conditioning to generate responses that align with the user. By
comparing different entrainment techniques on the MultiWOZ dataset, we
demonstrate that all three approaches produce significantly better-aligned
results than the baseline, as confirmed by both automated and manual evaluation
metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09404">To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages. (arXiv:2311.09404v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ebing_B/0/1/0/all/0/1">Benedikt Ebing</a>, <a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1">Goran Glava&#x161;</a></p>
<p>Perfect machine translation (MT) would render cross-lingual transfer (XLT) by
means of multilingual language models (LMs) superfluous. Given, on one hand,
the large body of work on improving XLT with multilingual LMs and, on the other
hand, recent advances in massively multilingual MT, in this work, we
systematically evaluate existing and propose new translation-based XLT
approaches for transfer to low-resource languages. We show that all
translation-based approaches dramatically outperform zero-shot XLT with
multilingual LMs, rendering the approach that combines the round-trip
translation of the source-language training data with the translation of the
target-language test instances the most effective. We next show that one can
obtain further empirical gains by adding reliable translations to other
high-resource languages to the training data. Moreover, we propose an effective
translation-based XLT strategy even for languages not supported by the MT
system. Finally, we show that model selection for XLT based on target-language
validation data obtained with MT outperforms model selection based on the
source-language data. We hope that our findings encourage adoption of more
robust translation-based baselines in XLT research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09406">Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture. (arXiv:2311.09406v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bernhard_J/0/1/0/all/0/1">James Bernhard</a></p>
<p>The transformer neural network architecture uses a form of attention in which
the dot product of query and key is divided by the square root of the key
dimension before applying softmax. This scaling of the dot product is designed
to avoid the absolute value of the dot products becoming so large that applying
softmax leads to vanishing gradients. In this paper, we propose some
alternative scalings, including dividing the dot product instead by the sum of
the key lengths before applying softmax. We use simulated keys and queries to
show that in many situations this appears to be more effective at avoiding
regions where applying softmax leads to vanishing gradients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09410">When Large Language Models contradict humans? Large Language Models&#x27; Sycophantic Behaviour. (arXiv:2311.09410v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1">Leonardo Ranaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pucci_G/0/1/0/all/0/1">Giulia Pucci</a></p>
<p>Large Language Models (LLMs) have been demonstrating the ability to solve
complex tasks by delivering answers that are positively evaluated by humans due
in part to the intensive use of human feedback that refines responses. However,
the suggestibility transmitted through human feedback increases the inclination
to produce responses that correspond to the user's beliefs or misleading
prompts as opposed to true facts, a behaviour known as sycophancy. This
phenomenon decreases the bias, robustness, and, consequently, their
reliability.
</p>
<p>In this paper, we shed light on the suggestibility of LLMs to sycophantic
behaviour, demonstrating these tendencies via human-influenced prompts over
different tasks. Our investigation reveals that LLMs show sycophantic
tendencies when responding to queries involving subjective opinions and
statements that should elicit a contrary response based on facts, demonstrating
a lack of robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09422">Predicting generalization performance with correctness discriminators. (arXiv:2311.09422v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuekun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1">Alexander Koller</a></p>
<p>The ability to predict an NLP model's accuracy on unseen, potentially
out-of-distribution data is a prerequisite for trustworthiness. We present a
novel model that establishes upper and lower bounds on the accuracy, without
requiring gold labels for the unseen data. We achieve this by training a
discriminator which predicts whether the output of a given sequence-to-sequence
model is correct or not. We show across a variety of tagging, parsing, and
semantic parsing tasks that the gold accuracy is reliably between the predicted
upper and lower bounds, and that these bounds are remarkably close together.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09428">Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models. (arXiv:2311.09428v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yueqing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1">Lu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1">Ali Payani</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a></p>
<p>This work investigates the potential of undermining both fairness and
detection performance in abusive language detection. In a dynamic and complex
digital world, it is crucial to investigate the vulnerabilities of these
detection models to adversarial fairness attacks to improve their fairness
robustness. We propose a simple yet effective framework FABLE that leverages
backdoor attacks as they allow targeted control over the fairness and detection
performance. FABLE explores three types of trigger designs (i.e., rare,
artificial, and natural triggers) and novel sampling strategies. Specifically,
the adversary can inject triggers into samples in the minority group with the
favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored
outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the
effectiveness of FABLE attacking fairness and utility in abusive language
detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09431">Striped Attention: Faster Ring Attention for Causal Transformers. (arXiv:2311.09431v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brandon_W/0/1/0/all/0/1">William Brandon</a>, <a href="http://arxiv.org/find/cs/1/au:+Nrusimha_A/0/1/0/all/0/1">Aniruddha Nrusimha</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1">Kevin Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ankner_Z/0/1/0/all/0/1">Zachary Ankner</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1">Tian Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhiye Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragan_Kelley_J/0/1/0/all/0/1">Jonathan Ragan-Kelley</a></p>
<p>To help address the growing demand for ever-longer sequence lengths in
transformer models, Liu et al. recently proposed Ring Attention, an exact
attention algorithm capable of overcoming per-device memory bottle- necks by
distributing self-attention across multiple devices. In this paper, we study
the performance characteristics of Ring Attention in the important special case
of causal transformer models, and identify a key workload imbal- ance due to
triangular structure of causal attention computations. We propose a simple
extension to Ring Attention, which we call Striped Attention to fix this
imbalance. Instead of devices having contiguous subsequences, each device has a
subset of tokens distributed uniformly throughout the sequence, which we
demonstrate leads to more even workloads. In experiments running Striped
Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x
end-to-end throughput improvements over the original Ring Attention algorithm
on causal transformer training at a sequence length of 256k. Furthermore, on 16
TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of
786k. We release the code for our experiments as open source
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09433">Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment. (arXiv:2311.09433v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a></p>
<p>To ensure AI safety, instruction-tuned Large Language Models (LLMs) are
specifically trained to ensure alignment, which refers to making models behave
in accordance with human intentions. While these models have demonstrated
commendable results on various safety benchmarks, the vulnerability of their
safety alignment has not been extensively studied. This is particularly
troubling given the potential harm that LLMs can inflict. Existing attack
methods on LLMs often rely on poisoned training data or the injection of
malicious prompts. These approaches compromise the stealthiness and
generalizability of the attacks, making them susceptible to detection.
Additionally, these models often demand substantial computational resources for
implementation, making them less practical for real-world applications. In this
work, we introduce a novel attack framework, called Backdoor Activation Attack,
which injects trojan steering vectors into the activation layers of LLMs. These
malicious steering vectors can be triggered at inference time to steer the
models toward attacker-desired behaviors by manipulating their activations. In
particular, the steering vectors are generated by taking the difference between
benign and malicious activations. Then, the most effective steering vector is
selected and added to the forward passes of the LLMs. Our experiment results on
four primary alignment tasks show that our proposed method is highly effective
and adds little or no overhead to attack efficiency. Additionally, we discuss
potential countermeasures against such activation attacks. Our code and data
are available at https://email-haoran-for-link. Warning: this paper contains
content that can be offensive or upsetting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09438">Labeled Interactive Topic Models. (arXiv:2311.09438v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seelman_K/0/1/0/all/0/1">Kyle Seelman</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mozhi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1">Jordan Boyd-Graber</a></p>
<p>Topic models help users understand large document collections; however, topic
models do not always find the ``right'' topics. While classical probabilistic
and anchor-based topic models have interactive variants to guide models toward
better topics, such interactions are not available for neural topic models such
as the embedded topic model (\abr{etm}). We correct this lacuna by adding an
intuitive interaction to neural topic models: users can label a topic with a
word, and topics are updated so that the topic words are close to the label.
This allows a user to refine topics based on their information need. While,
interactivity is intuitive for \abr{etm}, we extend this framework to work with
other neural topic models as well. We develop an interactive interface which
allows users to interact and relabel topic models as they see fit. We evaluate
our method through a human study, where users can relabel topics to find
relevant documents. Using our method, user labeling improves document rank
scores, helping to find more relevant documents to a given query when compared
to no user labeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09443">Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset. (arXiv:2311.09443v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheppard_B/0/1/0/all/0/1">Brooklyn Sheppard</a>, <a href="http://arxiv.org/find/cs/1/au:+Richter_A/0/1/0/all/0/1">Anna Richter</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1">Allison Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1">Elizabeth Allyn Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Kneese_T/0/1/0/all/0/1">Tamara Kneese</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1">Carolyne Pelletier</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1">Ioana Baldini</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yue Dong</a></p>
<p>Using novel approaches to dataset development, the Biasly dataset captures
the nuance and subtlety of misogyny in ways that are unique within the
literature. Built in collaboration with multi-disciplinary experts and
annotators themselves, the dataset contains annotations of movie subtitles,
capturing colloquial expressions of misogyny in North American film. The
dataset can be used for a range of NLP tasks, including classification,
severity score regression, and text generation for rewrites. In this paper, we
discuss the methodology used, analyze the annotations obtained, and provide
baselines using common NLP algorithms in the context of misogyny detection and
mitigation. We hope this work will promote AI for social good in NLP for bias
detection, explanation, and removal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09447">How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities. (arXiv:2311.09447v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1">Lingbo Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boshi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huan Sun</a></p>
<p>The rapid progress in open-source Large Language Models (LLMs) is
significantly driving AI development forward. However, there is still a limited
understanding of their trustworthiness. Deploying these models at scale without
sufficient trustworthiness can pose significant risks, highlighting the need to
uncover these issues promptly. In this work, we conduct an assessment of
open-source LLMs on trustworthiness, scrutinizing them across eight different
aspects including toxicity, stereotypes, ethics, hallucination, fairness,
sycophancy, privacy, and robustness against adversarial demonstrations. We
propose an enhanced Chain of Utterances-based (CoU) prompting strategy by
incorporating meticulously crafted malicious demonstrations for trustworthiness
attack. Our extensive experiments encompass recent and representative series of
open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The
empirical outcomes underscore the efficacy of our attack strategy across
diverse aspects. More interestingly, our result analysis reveals that models
with superior performance in general NLP tasks do not always have greater
trustworthiness; in fact, larger models can be more vulnerable to attacks.
Additionally, models that have undergone instruction tuning, focusing on
instruction following, tend to be more susceptible, although fine-tuning LLMs
for safety alignment proves effective in mitigating adversarial trustworthiness
attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09458">Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries. (arXiv:2311.09458v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1">Prafulla Kumar Choubey</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1">Alexander R. Fabbri</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chien-Sheng Wu</a></p>
<p>Ideal summarization models should generalize to novel summary-worthy content
without remembering reference training summaries by rote. However, a single
average performance score on the entire test set is inadequate in determining
such model competencies. We propose a fine-grained evaluation protocol by
partitioning a test set based on the lexical similarity of reference test
summaries with training summaries. We observe up to a 5x (1.2x) difference in
ROUGE-2 (entity recall) scores between the subsets with the lowest and highest
similarity. Next, we show that such training repetitions also make a model
vulnerable to rote learning, reproducing data artifacts such as factual errors,
especially when reference test summaries are lexically close to training
summaries. Consequently, we propose to limit lexical repetitions in training
summaries during both supervised fine-tuning and likelihood calibration stages
to improve the performance on novel test cases while retaining average
performance. Our automatic and human evaluations on novel test subsets and
recent news articles show that limiting lexical repetitions in training
summaries can prevent rote learning and improve generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09467">Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation. (arXiv:2311.09467v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yifu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Embar_V/0/1/0/all/0/1">Varun Embar</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Benjamin Han</a></p>
<p>Neural knowledge-to-text generation models often struggle to faithfully
generate descriptions for the input facts: they may produce hallucinations that
contradict the given facts, or describe facts not present in the input. To
reduce hallucinations, we propose a novel decoding method, TWEAK (Think While
Effectively Articulating Knowledge). TWEAK treats the generated sequences at
each decoding step and its future sequences as hypotheses, and ranks each
generation candidate based on how well their corresponding hypotheses support
the input facts using a Hypothesis Verification Model (HVM). We first
demonstrate the effectiveness of TWEAK by using a Natural Language Inference
(NLI) model as the HVM and report improved faithfulness with minimal impact on
the quality. We then replace the NLI model with our task-specific HVM trained
with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which
pairs input facts with their faithful and hallucinated descriptions with the
hallucinated spans marked. The new HVM improves the faithfulness and the
quality further and runs faster. Overall the best TWEAK variants improve on
average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and
TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality
measured by BERTScore over the same datasets. Since TWEAK is a decoding-only
approach, it can be integrated with any neural generative model without
retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09469">Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs. (arXiv:2311.09469v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Michael J.Q. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1">Eunsol Choi</a></p>
<p>Resolving ambiguities through interaction is a hallmark of natural language,
and modeling this behavior is a core challenge in crafting AI assistants. In
this work, we study such behavior in LMs by proposing a task-agnostic framework
for resolving ambiguity by asking users clarifying questions. Our framework
breaks down this objective into three subtasks: (1) determining when
clarification is needed, (2) determining what clarifying question to ask, and
(3) responding accurately with the new information gathered through
clarification. We evaluate systems across three NLP applications: question
answering, machine translation and natural language inference. For the first
subtask, we present a novel uncertainty estimation approach, intent-sim, that
determines the utility of querying for clarification by estimating the entropy
over user intents. Our method consistently outperforms existing uncertainty
estimation approaches at identifying predictions that will benefit from
clarification. When only allowed to ask for clarification on 10% of examples,
our system is able to double the performance gains over randomly selecting
examples to clarify. Furthermore, we find that intent-sim is robust,
demonstrating improvements across a wide range of NLP tasks and LMs. Together,
our work lays foundation for studying clarifying interactions with LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09473">JAB: Joint Adversarial Prompting and Belief Augmentation. (arXiv:2311.09473v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1">Ninareh Mehrabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Palash Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishna_A/0/1/0/all/0/1">Anil Ramakrishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1">Jwala Dhamala</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shalini Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1">Aram Galstyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Rahul Gupta</a></p>
<p>With the recent surge of language models in different applications, attention
to safety and robustness of these models has gained significant importance.
Here we introduce a joint framework in which we simultaneously probe and
improve the robustness of a black-box target model via adversarial prompting
and belief augmentation using iterative feedback loops. This framework utilizes
an automated red teaming approach to probe the target model, along with a
belief augmenter to generate instructions for the target model to improve its
robustness to those adversarial probes. Importantly, the adversarial model and
the belief generator leverage the feedback from past interactions to improve
the effectiveness of the adversarial prompts and beliefs, respectively. In our
experiments, we demonstrate that such a framework can reduce toxic content
generation both in dynamic cases where an adversary directly interacts with a
target model and static cases where we use a static benchmark dataset to
evaluate our model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09476">ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems. (arXiv:2311.09476v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1">Jon Saad-Falcon</a>, <a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1">Omar Khattab</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1">Matei Zaharia</a></p>
<p>Evaluating retrieval-augmented generation (RAG) systems traditionally relies
on hand annotations for input queries, passages to retrieve, and responses to
generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
RAG systems along the dimensions of context relevance, answer faithfulness, and
answer relevance. Using synthetic training data, ARES finetunes lightweight LM
judges to assess the quality of individual RAG components. To mitigate
potential prediction errors, ARES utilizes a small set of human-annotated
datapoints for prediction-powered inference (PPI). Across six different
knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG
systems while using a few hundred human annotations during evaluation.
Furthermore, ARES judges remain effective across domain shifts, proving
accurate even after changing the type of queries and/or documents used in the
evaluated RAG systems. We make our datasets and code for replication and
deployment available at https://github.com/stanford-futuredata/ARES.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09480">Show Your Work with Confidence: Confidence Bands for Tuning Curves. (arXiv:2311.09480v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lourie_N/0/1/0/all/0/1">Nicholas Lourie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">He He</a></p>
<p>The choice of hyperparameters greatly impacts performance in natural language
processing. Often, it is hard to tell if a method is better than another or
just better tuned. Tuning curves fix this ambiguity by accounting for tuning
effort. Specifically, they plot validation performance as a function of the
number of hyperparameter choices tried so far. While several estimators exist
for these curves, it is common to use point estimates, which we show fail
silently and give contradictory results when given too little data.
</p>
<p>Beyond point estimates, confidence bands are necessary to rigorously
establish the relationship between different approaches. We present the first
method to construct valid confidence bands for tuning curves. The bands are
exact, simultaneous, and distribution-free, thus they provide a robust basis
for comparing methods.
</p>
<p>Empirical analysis shows that while bootstrap confidence bands, which serve
as a baseline, fail to approximate their target confidence, ours achieve it
exactly. We validate our design with ablations, analyze the effect of sample
size, and provide guidance on comparing models with our method. To promote
confident comparisons in future work, we release a library implementing the
method at https://github.com/nalourie/opda .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09481">Personalized Jargon Identification for Enhanced Interdisciplinary Communication. (arXiv:2311.09481v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yue Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Joseph Chee Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1">Maria Antoniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Bransom_E/0/1/0/all/0/1">Erin Bransom</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1">Trevor Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lucy Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+August_T/0/1/0/all/0/1">Tal August</a></p>
<p>Scientific jargon can impede researchers when they read materials from other
domains. Current methods of jargon identification mainly use corpus-level
familiarity indicators (e.g., Simple Wikipedia represents plain language).
However, researchers' familiarity of a term can vary greatly based on their own
background. We collect a dataset of over 10K term familiarity annotations from
11 computer science researchers for terms drawn from 100 paper abstracts.
Analysis of this data reveals that jargon familiarity and information needs
vary widely across annotators, even within the same sub-domain (e.g., NLP). We
investigate features representing individual, sub-domain, and domain knowledge
to predict individual jargon familiarity. We compare supervised and
prompt-based approaches, finding that prompt-based methods including personal
publications yields the highest accuracy, though zero-shot prompting provides a
strong baseline. This research offers insight into features and methods to
integrate personal data into scientific jargon identification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09502">SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU. (arXiv:2311.09502v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1">Evgeniia Razumovskaia</a>, <a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1">Goran Glava&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a></p>
<p>Task-oriented dialogue (ToD) systems help users execute well-defined tasks
across a variety of domains (e.g., $\textit{flight booking}$ or $\textit{food
ordering}$), with their Natural Language Understanding (NLU) components being
dedicated to the analysis of user utterances, predicting users' intents
($\textit{Intent Detection}$, ID) and extracting values for informational slots
($\textit{Value Extraction}$, VE). In most domains, labelled NLU data is
scarce, making sample-efficient learning -- enabled with effective transfer
paradigms -- paramount. In this work, we introduce SQATIN, a new framework for
dialog NLU based on (i) instruction tuning and (ii) question-answering-based
formulation of ID and VE tasks. According to the evaluation on established NLU
benchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially
surpassing the performance of current models based on standard fine-tuning
objectives in both in-domain training and cross-domain transfer. SQATIN yields
particularly large performance gains in cross-domain transfer, owing to the
fact that our QA-based instruction tuning leverages similarities between
natural language descriptions of classes (i.e., slots and intents) across
domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09505">SegMix: A Simple Structure-Aware Data Augmentation Method. (arXiv:2311.09505v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1">Yuxin Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhuse_P/0/1/0/all/0/1">Pushkar Bhuse</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengzhong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric Xing</a></p>
<p>Interpolation-based Data Augmentation (DA) methods (Mixup) linearly
interpolate the inputs and labels of two or more training examples. Mixup has
more recently been adapted to the field of Natural Language Processing (NLP),
mainly for sequence labeling tasks. However, such a simple adoption yields
mixed or unstable improvements over the baseline models. We argue that the
direct-adoption methods do not account for structures in NLP tasks. To this
end, we propose SegMix, a collection of interpolation-based DA algorithms that
can adapt to task-specific structures. SegMix poses fewer constraints on data
structures, is robust to various hyperparameter settings, applies to more task
settings, and adds little computational overhead. In the algorithm's core, we
apply interpolation methods on task-specific meaningful segments, in contrast
to applying them on sequences as in prior work. We find SegMix to be a flexible
framework that combines rule-based DA methods with interpolation-based methods,
creating interesting mixtures of DA techniques. We show that SegMix
consistently improves performance over strong baseline models in Named Entity
Recognition (NER) and Relation Extraction (RE) tasks, especially under
data-scarce settings. Furthermore, this method is easy to implement and adds
negligible training overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09510">One Size Does Not Fit All: Customizing Open-Domain Procedures. (arXiv:2311.09510v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1">Yash Kumar Lal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1">Bodhisattwa Prasad Majumder</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1">Niket Tandon</a></p>
<p>How-to procedures, such as how to plant a garden, are ubiquitous. But one
size does not fit all - humans often need to customize these procedural plans
according to their specific needs, e.g., planting a garden without pesticides.
While LLMs can fluently generate generic procedures, we present the first study
on how well LLMs can customize open-domain procedures. We introduce
CustomPlans, a probe dataset of customization hints that encodes diverse user
needs for open-domain How-to procedures. Using LLMs as CustomizationAgent and
ExecutionAgent in different settings, we establish their abilities to perform
open-domain procedure customization. Human evaluation shows that using these
agents in a Sequential setting is the best, but they are good enough only ~51%
of the time. Error analysis shows that LLMs do not sufficiently address user
customization needs in their generated procedures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09513">Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents. (arXiv:2311.09513v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patwardhan_Q/0/1/0/all/0/1">Quinn Patwardhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Grace Hui Yang</a></p>
<p>This paper contains what the Georgetown InfoSense group has done in regard to
solving the challenges presented by TREC iKAT 2023. Our submitted runs
outperform the median runs by a significant margin, exhibiting superior
performance in nDCG across various cut numbers and in overall success rate. Our
approach uses a Generate-Retrieve-Generate method, which we've found to greatly
outpace Retrieve-Then-Generate approaches for the purposes of iKAT. Our
solution involves the use of Large Language Models (LLMs) for initial answers,
answer grounding by BM25, passage quality filtering by logistic regression, and
answer generation by LLMs again. We leverage several purpose-built Language
Models, including BERT, Chat-based, and text-to-transfer-based models, for text
understanding, classification, generation, and summarization. The official
results of the TREC evaluation contradict our initial self-evaluation, which
may suggest that a decrease in the reliance on our retrieval and classification
methods is better. Nonetheless, our findings suggest that the sequence of
involving these different components matters, where we see an essentiality of
using LLMs before using search engines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09517">GEE! Grammar Error Explanation with Large Language Models. (arXiv:2311.09517v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yixiao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1">Kalpesh Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_R/0/1/0/all/0/1">Rajesh Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1">Kevin Gimpel</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1">Mohit Iyyer</a></p>
<p>Grammatical error correction tools are effective at correcting grammatical
errors in users' input sentences but do not provide users with \textit{natural
language} explanations about their errors. Such explanations are essential for
helping users learn the language by gaining a deeper understanding of its
grammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we
propose the task of grammar error explanation, where a system needs to provide
one-sentence explanations for each grammatical error in a pair of erroneous and
corrected sentences. We analyze the capability of GPT-4 in grammar error
explanation, and find that it only produces explanations for 60.2% of the
errors using one-shot prompting. To improve upon this performance, we develop a
two-step pipeline that leverages fine-tuned and prompted large language models
to perform structured atomic token edit extraction, followed by prompting GPT-4
to generate explanations. We evaluate our pipeline on German and Chinese
grammar error correction data sampled from language learners with a wide range
of proficiency levels. Human evaluation reveals that our pipeline produces
93.9% and 98.0% correct explanations for German and Chinese data, respectively.
To encourage further research in this area, we will open-source our data and
code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09519">Leveraging Code to Improve In-context Learning for Semantic Parsing. (arXiv:2311.09519v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1">Ben Bogin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shivanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1">Ashish Sabharwal</a></p>
<p>In-context learning (ICL) is an appealing approach for semantic parsing due
to its few-shot nature and improved generalization. However, learning to parse
to rare domain-specific languages (DSLs) from just a few demonstrations is
challenging, limiting the performance of even the most capable LLMs. In this
work, we improve the effectiveness of ICL for semantic parsing by (1) using
general-purpose programming languages such as Python instead of DSLs, and (2)
augmenting prompts with a structured domain description that includes, e.g.,
the available classes and functions. We show that both these changes
significantly improve accuracy across three popular datasets. Combined, they
lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional
split), nearly closing the performance gap between easier i.i.d.\ and harder
compositional splits when used with a strong model, and reducing the need for a
large number of demonstrations. We find that the resemblance of the target
parse language to general-purpose code is a more important factor than the
language's popularity in pre-training corpora. Our findings provide an improved
methodology for building semantic parsers in the modern context of ICL with
LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09521">AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven Training Data Generation. (arXiv:2311.09521v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Haoyi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kung-Hsiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1">Jingnong Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a></p>
<p>Ensuring factual consistency is crucial in various natural language
processing tasks, particularly in abstractive summarization, where preserving
the integrity of information is paramount. Prior entailment-based approaches
often generate factually inconsistent summaries and then train a classifier on
the generated data. However, summaries produced by these approaches are either
of low coherence or lack error-type coverage. To address these issues, we
propose AMRFact, a novel framework that generates factually inconsistent
summaries using Abstract Meaning Representation (AMR). Our approach parses
factually correct summaries into AMR graphs and injects controlled factual
inconsistencies to create negative examples, allowing for coherent factually
inconsistent summaries to be generated with high error-type coverage.
Additionally, we present a data selection module NegFilter based on natural
language inference and BARTScore to ensure the quality of the generated
negative samples. Experimental results demonstrate that our approach
significantly outperforms previous systems on the AggreFact-SOTA dataset,
showcasing its efficacy in assessing factuality in abstractive summarization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09528">HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM. (arXiv:2311.09528v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yi Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jiaqi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Adams_V/0/1/0/all/0/1">Virginia Adams</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1">Makesh Narsimhan Sreedhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Egert_D/0/1/0/all/0/1">Daniel Egert</a>, <a href="http://arxiv.org/find/cs/1/au:+Delalleau_O/0/1/0/all/0/1">Olivier Delalleau</a>, <a href="http://arxiv.org/find/cs/1/au:+Scowcroft_J/0/1/0/all/0/1">Jane Polak Scowcroft</a>, <a href="http://arxiv.org/find/cs/1/au:+Kant_N/0/1/0/all/0/1">Neel Kant</a>, <a href="http://arxiv.org/find/cs/1/au:+Swope_A/0/1/0/all/0/1">Aidan Swope</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1">Oleksii Kuchaiev</a></p>
<p>Existing open-source helpfulness preference datasets do not specify what
makes some responses more helpful and others less so. Models trained on these
datasets can incidentally learn to model dataset artifacts (e.g. preferring
longer but unhelpful responses only due to their length). To alleviate this
problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated
for the various aspects that make responses helpful. Specifically, our
37k-sample dataset has annotations for correctness, coherence, complexity, and
verbosity in addition to overall helpfulness of responses. Training Llama 2 70B
using the HelpSteer dataset with SteerLM technique produces a model that scores
7.54 on MT Bench, which is currently the highest score for open models that do
not require training data from more powerful models (e.g. GPT4). We release
this dataset with CC-BY-4.0 license at
https://huggingface.co/datasets/nvidia/HelpSteer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09533">Effective Large Language Model Adaptation for Improved Grounding. (arXiv:2311.09533v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xi Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Ruoxi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1">Sercan &#xd6;. Arik</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a></p>
<p>Large language models (LLMs) have achieved remarkable advancements in natural
language understanding, generation, and manipulation of text-based data.
However, one major issue towards their widespread deployment in the real world
is that they can generate "hallucinated" answers that are not factual. Towards
this end, this paper focuses on improving grounding from a holistic perspective
with a novel framework, AGREE, Adaptation of LLMs for GRounding EnhancEment. We
start with the design of an iterative test-time adaptation (TTA) capability
that takes into account the support information generated in self-grounded
responses. To effectively enable this capability, we tune LLMs to ground the
claims in their responses to retrieved documents by providing citations. This
tuning on top of the pre-trained LLMs requires a small amount of data that
needs to be constructed in a particular way to learn the grounding information,
for which we introduce a data construction method. Our results show that the
tuning-based AGREE framework generates better grounded responses with more
accurate citations compared to prompting-based approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09538">Reducing Privacy Risks in Online Self-Disclosures with Language Models. (arXiv:2311.09538v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1">Yao Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Krsek_I/0/1/0/all/0/1">Isadora Krsek</a>, <a href="http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1">Tarek Naous</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabra_A/0/1/0/all/0/1">Anubha Kabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sauvik Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1">Alan Ritter</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a></p>
<p>Self-disclosure, while being common and rewarding in social media
interaction, also poses privacy risks. In this paper, we take the initiative to
protect the user-side privacy associated with online self-disclosure through
identification and abstraction. We develop a taxonomy of 19 self-disclosure
categories, and curate a large corpus consisting of 4.8K annotated disclosure
spans. We then fine-tune a language model for identification, achieving over
75% in Token F$_1$. We further conduct a HCI user study, with 82\% of
participants viewing the model positively, highlighting its real world
applicability. Motivated by the user feedback, we introduce the task of
self-disclosure abstraction. We experiment with both one-span abstraction and
three-span abstraction settings, and explore multiple fine-tuning strategies.
Our best model can generate diverse abstractions that moderately reduce privacy
risks while maintaining high utility according to human evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09542">Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health. (arXiv:2311.09542v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srikanth_N/0/1/0/all/0/1">Neha Srikanth</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1">Rupak Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1">Rachel Rudinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1">Jordan Boyd-Graber</a></p>
<p>Questions posed by information-seeking users often contain implicit false or
potentially harmful assumptions. In a high-risk domain such as maternal and
infant health, a question-answering system must recognize these pragmatic
constraints and go beyond simply answering user questions, examining them in
context to respond helpfully. To achieve this, we study pragmatic inferences
made when mothers ask questions about pregnancy and infant care. Some of the
inferences in these questions evade detection by existing methods, risking the
possibility of QA systems failing to address them which can have dangerous
health and policy implications. We explore the viability of detecting
inferences from questions using large language models and illustrate that
informing existing QA pipelines with pragmatic inferences produces responses
that can mitigate the propagation of harmful beliefs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09550">A Speed Odyssey for Deployable Quantization of LLMs. (arXiv:2311.09550v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qingyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1">Ran Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiduo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yifan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yerui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yuchen Xie</a></p>
<p>The large language model era urges faster and less costly inference. Prior
model compression works on LLMs tend to undertake a software-centric approach
primarily focused on the simulated quantization performance. By neglecting the
feasibility of deployment, these approaches are typically disabled in real
practice. They used to drastically push down the quantization bit range for a
reduced computation which might not be supported by the mainstream hardware, or
involve sophisticated algorithms that introduce extra computation or memory
access overhead. We argue that pursuing a hardware-centric approach in the
construction of quantization algorithms is crucial. In this regard, we are
driven to build our compression method on top of hardware awareness,
eliminating impractical algorithm choices while maximizing the benefit of
hardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel
implementation called FastGEMM and a combined recipe of quantization
strategies. Extensive experiments manifest the superiority of our W4A8 method
which brings the actual speed boosting up to \textbf{4$\times$} compared to
Hugging Face FP16 inference and \textbf{2.23$\times$} vs. the state-of-the-art
inference engine TensorRT-LLM in FP16, and \textbf{1.45$\times$} vs.
TensorRT-LLM in INT8, yet without substantially harming the performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09552">Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition. (arXiv:2311.09552v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alhindi_T/0/1/0/all/0/1">Tariq Alhindi</a>, <a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1">Smaranda Muresan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1">Preslav Nakov</a></p>
<p>Recognizing fallacies is crucial for ensuring the quality and validity of
arguments across various domains. However, computational fallacy recognition
faces challenges due to the diverse genres, domains, and types of fallacies
found in datasets. This leads to a highly multiclass, and even multi-label,
setup with substantial class imbalance. In this study, we aim to enhance
existing models for fallacy recognition by incorporating additional context and
by leveraging large language models to generate synthetic data, thus increasing
the representation of the infrequent classes. We experiment with GPT3.5 to
generate synthetic examples and we examine the impact of prompt settings for
this. Moreover, we explore zero-shot and few-shot scenarios to evaluate the
effectiveness of using the generated examples for training smaller models
within a unified fallacy recognition framework. Furthermore, we analyze the
overlap between the synthetic data and existing fallacy datasets. Finally, we
investigate the usefulness of providing supplementary context for detecting
fallacy types that need such context, e.g., diversion fallacies. Our evaluation
results demonstrate consistent improvements across fallacy types, datasets, and
generators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09558">Pachinko: Patching Interpretable QA Models through Natural Language Feedback. (arXiv:2311.09558v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malaviya_C/0/1/0/all/0/1">Chaitanya Malaviya</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Subin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1">Mark Yatskar</a></p>
<p>Eliciting feedback from end users of NLP models can be beneficial for
improving models. However, how should we present model responses to users so
they are most amenable to be corrected from user feedback? Further, what
properties do users value to understand and trust responses? We answer these
questions by analyzing the effect of rationales generated by QA models to
support their answers. We specifically consider decomposed question-answering
models that first extract an intermediate rationale based on a context and a
question and then use solely this rationale to answer the question. A rationale
outlines the approach followed by the model to answer the question. Our work
considers various formats of these rationales that vary according to
well-defined properties of interest. We sample these rationales from large
language models using few-shot prompting for two reading comprehension
datasets, and then perform two user studies. In the first one, we present users
with incorrect answers and corresponding rationales of various formats and ask
them to provide natural language feedback to revise the rationale. We then
measure the effectiveness of this feedback in patching these rationales through
in-context learning. The second study evaluates how well different rationale
formats enable users to understand and trust model answers, when they are
correct. We find that rationale formats significantly affect how easy it is (1)
for users to give feedback for rationales, and (2) for models to subsequently
execute this feedback. In addition to influencing critiquablity, certain
formats significantly enhance user reported understanding and trust of model
outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09559">Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler. (arXiv:2311.09559v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1">Gaurav Sahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1">Olga Vechtomova</a>, <a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1">Issam H. Laradji</a></p>
<p>This work tackles the task of extractive text summarization in a limited
labeled data scenario using a semi-supervised approach. Specifically, we
propose a prompt-based pseudolabel selection strategy using GPT-4. We evaluate
our method on three text summarization datasets: TweetSumm, WikiHow, and
ArXiv/PubMed. Our experiments show that by using an LLM to evaluate and
generate pseudolabels, we can improve the ROUGE-1 by 10-20\% on the different
datasets, which is akin to enhancing pretrained models. We also show that such
a method needs a smaller pool of unlabeled examples to perform better.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09562">A Reevaluation of Event Extraction: Past, Present, and Future Challenges. (arXiv:2311.09562v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kuan-Hao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1">I-Hung Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Parekh_T/0/1/0/all/0/1">Tanmay Parekh</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhiyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1">Premkumar Natarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a></p>
<p>Event extraction has attracted much attention in recent years due to its
potential for many applications. However, recent studies observe some
evaluation challenges, suggesting that reported scores might not reflect the
true performance. In this work, we first identify and discuss these evaluation
challenges, including the unfair comparisons resulting from different
assumptions about data or different data preprocessing steps, the
incompleteness of the current evaluation framework leading to potential dataset
bias or data split bias, and low reproducibility of prior studies. To address
these challenges, we propose TextEE, a standardized, fair, and reproducible
benchmark for event extraction. TextEE contains standardized data preprocessing
scripts and splits for more than ten datasets across different domains. In
addition, we aggregate and re-implement over ten event extraction approaches
published in recent years and conduct a comprehensive reevaluation. Finally, we
explore the capability of large language models in event extraction and discuss
some future challenges. We expect TextEE will serve as a reliable benchmark for
event extraction, facilitating future research in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09564">LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks. (arXiv:2311.09564v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1">Mihir Parmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1">Aakanksha Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Himanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_D/0/1/0/all/0/1">Disha Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1">Chitta Baral</a></p>
<p>Many large language models (LLMs) for medicine have largely been evaluated on
short texts, and their ability to handle longer sequences such as a complete
electronic health record (EHR) has not been systematically explored. Assessing
these models on long sequences is crucial since prior work in the general
domain has demonstrated performance degradation of LLMs on longer texts.
Motivated by this, we introduce LongBoX, a collection of seven medical datasets
in text-to-text format, designed to investigate model performance on long
sequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)
and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We
further evaluate two techniques designed for long-sequence handling: (i)
local-global attention, and (ii) Fusion-in-Decoder (FiD). Our results
demonstrate mixed results with long-sequence handling - while scores on some
datasets increase, there is substantial room for improvement. We hope that
LongBoX facilitates the development of more effective long-sequence techniques
for the medical domain. Data and source code are available at
https://github.com/Mihir3009/LongBoX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09569">Prompt Optimisation with Random Sampling. (arXiv:2311.09569v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiayi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1">Sebastian Riedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1">Pontus Stenetorp</a></p>
<p>Using the generative nature of a language model to generate task-relevant
separators has shown competitive results compared to human-curated prompts like
"TL;DR". We demonstrate that even randomly chosen tokens from the vocabulary as
separators can achieve near-state-of-the-art performance. We analyse this
phenomenon in detail using three different random generation strategies,
establishing that the language space is rich with potential good separators,
regardless of the underlying language model size. These observations challenge
the common assumption that an effective prompt should be human-readable or
task-relevant. Experimental results show that using random separators leads to
an average 16% relative improvement across nine text classification tasks on
seven language models, compared to human-curated separators, and is on par with
automatic prompt searching methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09576">Work State-Centric AI Agents: Design, Implementation, and Management of Cognitive Work Threads. (arXiv:2311.09576v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a></p>
<p>AI agents excel in executing predefined tasks, but the dynamic management of
work state information during task execution remains an underexplored area. We
propose a work state-centric AI agent model employing "work notes" to record
and reflect the state throughout task execution. This paper details the model's
architecture, featuring worker threads for task oversight, planner modules for
task decomposition and planning, and executor modules for performing subtasks
using a ReAct-inspired thought-action loop. We provide an exhaustive work state
record incorporating plans and outcomes, constituting a comprehensive work
journal. Our results show that this model not only improves task execution
efficiency but also lays a solid foundation for subsequent task analysis and
auditing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09578">Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying. (arXiv:2311.09578v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1">Adithya Renduchintala</a>, <a href="http://arxiv.org/find/cs/1/au:+Konuk_T/0/1/0/all/0/1">Tugrul Konuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1">Oleksii Kuchaiev</a></p>
<p>We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective
training to further increase parameter efficiency of the Low-rank adaptation
(LoRA) method. Our investigations include all feasible combinations parameter
training/freezing in conjunction with weight tying to identify the optimal
balance between performance and the number of trainable parameters. Through
experiments covering a variety of tasks and two base language models, we
provide analysis revealing trade-offs between efficiency and performance. Our
experiments uncovered a particular Tied-LoRA configuration that stands out by
demonstrating comparable performance across several tasks while employing only
13~\% percent of parameters utilized by the standard LoRA method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09579">Crafting In-context Examples according to LMs&#x27; Parametric Knowledge. (arXiv:2311.09579v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yoonsang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Atreya_P/0/1/0/all/0/1">Pranav Atreya</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xi Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1">Eunsol Choi</a></p>
<p>In-context learning has been applied to knowledge-rich tasks such as question
answering. In such scenarios, in-context examples are used to trigger a
behaviour in the language model: namely, it should surface information stored
in its parametric knowledge. We study the construction of in-context example
sets, with a focus on the parametric knowledge of the model regarding
in-context examples. We identify 'known' examples, where models can correctly
answer from its parametric knowledge, and 'unknown' ones. Our experiments show
that prompting with 'unknown' examples decreases the performance, potentially
as it encourages hallucination rather than searching its parametric knowledge.
Constructing an in-context example set that presents both known and unknown
information performs the best across diverse settings. We perform analysis on
three multi-answer question answering datasets, which allows us to further
study answer set ordering strategies based on the LM's knowledge about each
answer. Together, our study sheds lights on how to best construct in-context
example sets for knowledge-rich tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09580">MMOE: Mixture of Multimodal Interaction Experts. (arXiv:2311.09580v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haofei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>Multimodal machine learning, which studies the information and interactions
across various input modalities, has made significant advancements in
understanding the relationship between images and descriptive text. However,
this is just a portion of the potential multimodal interactions seen in the
real world and does not include new interactions between conflicting utterances
and gestures in predicting sarcasm, for example. Notably, the current methods
for capturing shared information often do not extend well to these more nuanced
interactions, sometimes performing as low as 50% in binary classification. In
this paper, we address this problem via a new approach called MMOE, which
stands for a mixture of multimodal interaction experts. Our method
automatically classifies data points from unlabeled multimodal datasets by
their interaction type and employs specialized models for each specific
interaction. Based on our experiments, this approach improves performance on
these challenging interactions by more than 10%, leading to an overall increase
of 2% for tasks like sarcasm prediction. As a result, interaction
quantification provides new insights for dataset analysis and yields simple
approaches that obtain state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09581">Enhancing Medical Text Evaluation with GPT-4. (arXiv:2311.09581v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yiqing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gero_Z/0/1/0/all/0/1">Zelalem Gero</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1">Cliff Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1">Tristan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1">Hoifung Poon</a></p>
<p>In the evaluation of medical text generation, it is essential to scrutinize
each piece of information and ensure the utmost accuracy of the evaluation.
Existing evaluation metrics either focus on coarse-level evaluation that
assigns one score for the whole generated output or rely on evaluation models
trained on general domain, resulting in inaccuracies when adapted to the
medical domain. To address these issues, we propose a set of factuality-centric
evaluation aspects and design corresponding GPT-4-based metrics for medical
text generation. We systematically compare these metrics with existing ones on
clinical note generation and medical report summarization tasks, revealing low
inter-metric correlation. A comprehensive human evaluation confirms that the
proposed GPT-4-based metrics exhibit substantially higher agreement with human
judgments than existing evaluation metrics. Our study contributes to the
understanding of medical text generation evaluation and offers a more reliable
alternative to existing metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09585">LifeTox: Unveiling Implicit Toxicity in Life Advice. (arXiv:2311.09585v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minbeom Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1">Jahyun Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hwanhee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Joonsuk Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hwaran Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kyomin Jung</a></p>
<p>As large language models become increasingly integrated into daily life,
detecting implicit toxicity across diverse contexts is crucial. To this end, we
introduce LifeTox, a dataset designed for identifying implicit toxicity within
a broad range of advice-seeking scenarios. Unlike existing safety datasets,
LifeTox comprises diverse contexts derived from personal experiences through
open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on
LifeTox matches or surpasses the zero-shot performance of large language models
in toxicity classification tasks. These results underscore the efficacy of
LifeTox in addressing the complex challenges inherent in implicit toxicity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09593">Multi-Step Dialogue Workflow Action Prediction. (arXiv:2311.09593v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_R/0/1/0/all/0/1">Ramya Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Elenberg_E/0/1/0/all/0/1">Ethan Elenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Narangodage_H/0/1/0/all/0/1">Hashan Narangodage</a>, <a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1">Ryan McDonald</a></p>
<p>In task-oriented dialogue, a system often needs to follow a sequence of
actions, called a workflow, that complies with a set of guidelines in order to
complete a task. In this paper, we propose the novel problem of multi-step
workflow action prediction, in which the system predicts multiple future
workflow actions. Accurate prediction of multiple steps allows for multi-turn
automation, which can free up time to focus on more complex tasks. We propose
three modeling approaches that are simple to implement yet lead to more action
automation: 1) fine-tuning on a training dataset, 2) few-shot in-context
learning leveraging retrieval and large language model prompting, and 3)
zero-shot graph traversal, which aggregates historical action sequences into a
graph for prediction. We show that multi-step action prediction produces
features that improve accuracy on downstream dialogue tasks like predicting
task success, and can increase automation of steps by 20% without requiring as
much feedback from a human overseeing the system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09602">Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion. (arXiv:2311.09602v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Smriti Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1">Cornelia Caragea</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyi Jessy Li</a></p>
<p>Situations and events evoke emotions in humans, but to what extent do they
inform the prediction of emotion detection models? Prior work in emotion
trigger or cause identification focused on training models to recognize events
that trigger an emotion. Instead, this work investigates how well
human-annotated emotion triggers correlate with features that models deemed
salient in their prediction of emotions. First, we introduce a novel dataset
EmoTrigger, consisting of 900 social media posts sourced from three different
datasets; these were annotated by experts for emotion triggers with high
agreement. Using EmoTrigger, we evaluate the ability of large language models
(LLMs) to identify emotion triggers, and conduct a comparative analysis of the
features considered important for these tasks between LLMs and fine-tuned
models. Our analysis reveals that emotion triggers are largely not considered
salient features for emotion prediction models, instead there is intricate
interplay between various features and the task of emotion detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09603">SCORE: A framework for Self-Contradictory Reasoning Evaluation. (arXiv:2311.09603v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Isabelle Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yongkang Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Soumya Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jieyu Zhao</a></p>
<p>Large language models (LLMs) have demonstrated impressive reasoning ability
in various language-based tasks. Despite many proposed reasoning methods aimed
at enhancing performance in downstream tasks, two fundamental questions
persist: Does reasoning genuinely support predictions, and how reliable is the
quality of reasoning? In this paper, we propose a framework \textsc{SCORE} to
analyze how well LLMs can reason. Specifically, we focus on self-contradictory
reasoning, where reasoning does not support the prediction. We find that LLMs
often contradict themselves when performing reasoning tasks that involve
contextual information and commonsense. The model may miss evidence or use
shortcuts, thereby exhibiting self-contradictory behaviors. We also employ the
Point-of-View (POV) method, which probes models to generate reasoning from
multiple perspectives, as a diagnostic tool for further analysis. We find that
though LLMs may appear to perform well in one-perspective settings, they fail
to stabilize such behavior in multi-perspectives settings. Even for correct
predictions, the reasoning may be messy and incomplete, and LLMs can easily be
led astray from good reasoning. \textsc{SCORE}'s results underscore the lack of
robustness required for trustworthy reasoning and the urgency for further
research to establish best practices for a comprehensive evaluation of
reasoning beyond accuracy-based metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09605">Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals. (arXiv:2311.09605v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1">Yanai Elazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1">Bhargavi Paranjape</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1">Sarah Wiegreffe</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghavi_K/0/1/0/all/0/1">Khyathi Raghavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1">Vivek Srikumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1">Noah A. Smith</a></p>
<p>The inevitable appearance of spurious correlations in training datasets hurts
the generalization of NLP models on unseen data. Previous work has found that
datasets with paired inputs are prone to correlations between a specific part
of the input (e.g., the hypothesis in NLI) and the label; consequently, models
trained only on those outperform chance. Are these correlations picked up by
models trained on the full input data? To address this question, we propose a
new evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses
counterfactuals by replacing part of the input with its counterpart from a
different example (subject to some restrictions), expecting an attentive model
to change its prediction. Using CAT, we systematically investigate established
supervised and in-context learning models on ten datasets spanning four tasks:
natural language inference, reading comprehension, paraphrase detection, and
visual &amp; language reasoning. CAT reveals that reliance on such correlations is
mainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive
with an increased number of demonstrations, while its accuracy on the test data
improves. Our results demonstrate that augmenting training or demonstration
data with counterfactuals is effective in improving models' attentiveness. We
show that models' attentiveness measured by CAT reveals different conclusions
from solely measuring correlations in data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09606">GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks. (arXiv:2311.09606v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1">Shivanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenbaum_C/0/1/0/all/0/1">Clemens Rosenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Elenberg_E/0/1/0/all/0/1">Ethan R. Elenberg</a></p>
<p>Large language models (LLMs) have the ability to perform in-context learning
(ICL) of new tasks by conditioning on prompts comprising a few task examples.
This work studies the problem of selecting the best examples given a candidate
pool to improve ICL performance on given a test input. Existing approaches
either require training with feedback from a much larger LLM or are
computationally expensive. We propose a novel metric, GistScore, based on
Example Gisting, a novel approach for training example retrievers for ICL using
an attention bottleneck via Gisting, a recent technique for compressing task
instructions. To tradeoff performance with ease of use, we experiment with both
fine-tuning gist models on each dataset and multi-task training a single model
on a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we
show that our fine-tuned models get state-of-the-art ICL performance with 20%
absolute average gain over off-the-shelf retrievers and 7% over the best prior
methods. Our multi-task model generalizes well out-of-the-box to new task
categories, datasets, and prompt templates with retrieval speeds that are
consistently thousands of times faster than the best prior training-free
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09612">Efficient End-to-End Visual Document Understanding with Rationale Distillation. (arXiv:2311.09612v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Alekh Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1">Mandar Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a>, <a href="http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1">Kristina Toutanova</a></p>
<p>Understanding visually situated language requires recognizing text and visual
elements, and interpreting complex layouts. State-of-the-art methods commonly
use specialized pre-processing tools, such as optical character recognition
(OCR) systems, that map document image inputs to extracted information in the
space of textual tokens, and sometimes also employ large language models (LLMs)
to reason in text token space. However, the gains from external tools and LLMs
come at the cost of increased computational and engineering complexity. In this
paper, we ask whether small pretrained image-to-text models can learn selective
text or layout recognition and reasoning as an intermediate inference step in
an end-to-end model for pixel-level visual language understanding. We
incorporate the outputs of such OCR tools, LLMs, and larger multimodal models
as intermediate ``rationales'' on training data, and train a small student
model to predict both rationales and answers for input questions based on those
training examples. A student model based on Pix2Struct (282M parameters)
achieves consistent improvements on three visual document understanding
benchmarks representing infographics, scanned documents, and figures, with
improvements of more than 4\% absolute over a comparable Pix2Struct model that
predicts answers directly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09613">Digital Socrates: Evaluating LLMs through explanation critiques. (arXiv:2311.09613v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yuling Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1">Oyvind Tafjord</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a></p>
<p>While LLMs can provide reasoned explanations along with their answers, the
nature and quality of those explanations are still poorly understood. In
response, our goal is to define a detailed way of characterizing the
explanation capabilities of modern models and to create a nuanced,
interpretable explanation evaluation tool that can generate such
characterizations automatically, without relying on expensive API calls or
human annotations. Our approach is to (a) define the new task of explanation
critiquing - identifying and categorizing any main flaw in an explanation and
providing suggestions to address the flaw, (b) create a sizeable,
human-verified dataset for this task, and (c) train an open-source, automatic
critiquing model (called Digital Socrates) using this data. Through
quantitative and qualitative analysis, we demonstrate how Digital Socrates is
useful for revealing insights about student models by examining their reasoning
chains, and how it can provide high-quality, nuanced, automatic evaluation of
those model explanations for the first time. Digital Socrates thus fills an
important gap in evaluation tools for understanding and improving the
explanation behavior of models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09615">On Retrieval Augmentation and the Limitations of Language Model Training. (arXiv:2311.09615v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1">Ting-Rui Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xinyan Velocity Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1">Joshua Robinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1">Ollie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Isabelle Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1">Dani Yogatama</a></p>
<p>Augmenting a language model (LM) with $k$-nearest neighbors (kNN) retrieval
on its training data alone can decrease its perplexity, though the underlying
reasons for this remains elusive. In this work, we first rule out one
previously posited possibility -- the "softmax bottleneck." We further identify
the MLP hurdle phenomenon, where the final MLP layer in LMs may impede LM
optimization early on. We explore memorization and generalization in language
models with two new datasets, where advanced model like GPT-3.5-turbo find
generalizing to irrelevant information in the training data challenging.
However, incorporating kNN retrieval to vanilla GPT-2 117M can consistently
improve performance in this setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09618">Simulating Opinion Dynamics with Networks of LLM-based Agents. (arXiv:2311.09618v1 [physics.soc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Chuang_Y/0/1/0/all/0/1">Yun-Shiuan Chuang</a>, <a href="http://arxiv.org/find/physics/1/au:+Goyal_A/0/1/0/all/0/1">Agam Goyal</a>, <a href="http://arxiv.org/find/physics/1/au:+Harlalka_N/0/1/0/all/0/1">Nikunj Harlalka</a>, <a href="http://arxiv.org/find/physics/1/au:+Suresh_S/0/1/0/all/0/1">Siddharth Suresh</a>, <a href="http://arxiv.org/find/physics/1/au:+Hawkins_R/0/1/0/all/0/1">Robert Hawkins</a>, <a href="http://arxiv.org/find/physics/1/au:+Yang_S/0/1/0/all/0/1">Sijia Yang</a>, <a href="http://arxiv.org/find/physics/1/au:+Shah_D/0/1/0/all/0/1">Dhavan Shah</a>, <a href="http://arxiv.org/find/physics/1/au:+Hu_J/0/1/0/all/0/1">Junjie Hu</a>, <a href="http://arxiv.org/find/physics/1/au:+Rogers_T/0/1/0/all/0/1">Timothy T. Rogers</a></p>
<p>Accurately simulating human opinion dynamics is crucial for understanding a
variety of societal phenomena, including polarization and the spread of
misinformation. However, the agent-based models (ABMs) commonly used for such
simulations lack fidelity to human behavior. We propose a new approach to
simulating opinion dynamics based on populations of Large Language Models
(LLMs). Our findings reveal a strong inherent bias in LLM agents towards
accurate information, leading to consensus in line with scientific reality.
However, this bias limits the simulation of individuals with resistant views on
issues like climate change. After inducing confirmation bias through prompt
engineering, we observed opinion fragmentation in line with existing
agent-based research. These insights highlight the promise and limitations of
LLM agents in this domain and suggest a path forward: refining LLMs with
real-world discourse to better simulate the evolution of human beliefs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09619">Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning. (arXiv:2311.09619v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1">Kazuma Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1">Karthik Raman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1">Michael Bendersky</a></p>
<p>In-Context Learning (ICL) is an emergent capability of Large Language Models
(LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new
tasks. Previous studies have shown that using LLMs' outputs as labels is
effective in training models to select demonstrations. Such a label is expected
to estimate utility of a demonstration in ICL; however, it has not been well
understood how different labeling strategies affect results on target tasks.
This paper presents an analysis on different utility functions by focusing on
LLMs' output probability given ground-truth output, and task-specific reward
given LLMs' prediction. Unlike the previous work, we introduce a novel labeling
method, incremental utility, which estimates how much incremental knowledge is
brought into the LLMs by a demonstration. We conduct experiments with
instruction-tuned LLMs on binary/multi-class classification, segmentation, and
translation across Arabic, English, Finnish, Japanese, and Spanish. Our results
show that (1) the probability is effective when the probability values are
distributed across the whole value range (on the classification tasks), and (2)
the downstream metric is more robust when nuanced reward values are provided
with long outputs (on the segmentation and translation tasks). We then show
that the proposed incremental utility further helps ICL by contrasting how the
LLMs perform with and without the demonstrations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09627">CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model. (arXiv:2311.09627v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1">Nakyeong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1">Taegwan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kyomin Jung</a></p>
<p>Large language models (LLMs) executing tasks through instruction-based
prompts often face challenges stemming from distribution differences between
user instructions and training instructions. This leads to distractions and
biases, especially when dealing with inconsistent dynamic labels. In this
paper, we introduces a novel bias mitigation method, CRISPR, designed to
alleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods
to identify bias neurons influencing biased outputs and employs pruning to
eliminate the bias neurons. Experimental results demonstrate the method's
effectiveness in mitigating biases in instruction-based prompting, enhancing
language model performance on social bias benchmarks without compromising
pre-existing knowledge. CRISPR proves highly practical, model-agnostic,
offering flexibility in adapting to evolving social biases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09630">From Scroll to Misbelief: Modeling the Unobservable Susceptibility to Misinformation on Social Media. (arXiv:2311.09630v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yanchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Mingyu Derek Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1">Wenna Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Azure Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weiyan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a></p>
<p>Susceptibility to misinformation describes the extent to believe unverifiable
claims, which is hidden in people's mental process and infeasible to observe.
Existing susceptibility studies heavily rely on the self-reported beliefs,
making any downstream applications on susceptability hard to scale. To address
these limitations, in this work, we propose a computational model to infer
users' susceptibility levels given their activities. Since user's
susceptibility is a key indicator for their reposting behavior, we utilize the
supervision from the observable sharing behavior to infer the underlying
susceptibility tendency. The evaluation shows that our model yields estimations
that are highly aligned with human judgment on users' susceptibility level
comparisons. Building upon such large-scale susceptibility labeling, we further
conduct a comprehensive analysis of how different social factors relate to
susceptibility. We find that political leanings and psychological factors are
associated with susceptibility in varying degrees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09632">Online Continual Knowledge Learning for Language Models. (arXiv:2311.09632v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1">Tongjun Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1">Karthick Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Seah_C/0/1/0/all/0/1">Chun Wei Seah</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuhao Zhang</a></p>
<p>Large Language Models (LLMs) serve as repositories of extensive world
knowledge, enabling them to perform tasks such as question-answering and
fact-checking. However, this knowledge can become obsolete as global contexts
change. In this paper, we introduce a novel problem in the realm of continual
learning: Online Continual Knowledge Learning (OCKL). This problem formulation
aims to manage the dynamic nature of world knowledge in LMs under real-time
constraints. We propose a new benchmark and evaluation metric designed to
measure both the rate of new knowledge acquisition and the retention of
previously learned knowledge. Our empirical evaluation, conducted using a
variety of state-of-the-art methods, establishes robust base-lines for OCKL.
Our results reveal that existing continual learning approaches are
unfortunately insufficient for tackling the unique challenges posed by OCKL. We
identify key factors that influence the trade-off between knowledge acquisition
and retention, thereby advancing our understanding of how to train LMs in a
continually evolving environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09635">Evaluating In-Context Learning of Libraries for Code Generation. (arXiv:2311.09635v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Arkil Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1">Dzmitry Bahdanau</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1">Pradeep Dasigi</a></p>
<p>Contemporary Large Language Models (LLMs) exhibit a high degree of code
generation and comprehension capability. A particularly promising area is their
ability to interpret code modules from unfamiliar libraries for solving
user-instructed tasks. Recent work has shown that large proprietary LLMs can
learn novel library usage in-context from demonstrations. These results raise
several open questions: whether demonstrations of library usage is required,
whether smaller (and more open) models also possess such capabilities, etc. In
this work, we take a broader approach by systematically evaluating a diverse
array of LLMs across three scenarios reflecting varying levels of domain
specialization to understand their abilities and limitations in generating code
based on libraries defined in-context. Our results show that even smaller
open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding
of novel code libraries based on specification presented in-context. Our
findings further reveal that LLMs exhibit a surprisingly high proficiency in
learning novel library modules even when provided with just natural language
descriptions or raw code implementations of the functions, which are often
cheaper to obtain than demonstrations. Overall, our results pave the way for
harnessing LLMs in more adaptable and dynamic coding environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09641">On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models. (arXiv:2311.09641v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiongxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Junlin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1">Yevgeniy Vorobeychik</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a></p>
<p>Reinforcement Learning with Human Feedback (RLHF) is a methodology designed
to align Large Language Models (LLMs) with human preferences, playing an
important role in LLMs alignment. Despite its advantages, RLHF relies on human
annotators to rank the text, which can introduce potential security
vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the
ranking score by up-ranking any malicious text to steer the LLM adversarially.
To assess the red-teaming of RLHF against human preference data poisoning, we
propose RankPoison, a poisoning attack method on candidates' selection of
preference rank flipping to reach certain malicious behaviors (e.g., generating
longer sequences, which can increase the computational cost). With poisoned
dataset generated by RankPoison, we can perform poisoning attacks on LLMs to
generate longer tokens without hurting the original safety alignment
performance. Moreover, applying RankPoison, we also successfully implement a
backdoor attack where LLMs can generate longer answers under questions with the
trigger word. Our findings highlight critical security challenges in RLHF,
underscoring the necessity for more robust alignment methods for LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09648">Event Causality Is Key to Computational Story Understanding. (arXiv:2311.09648v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yidan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_Q/0/1/0/all/0/1">Qin Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a></p>
<p>Psychological research suggests the central role of event causality in human
story understanding. Further, event causality has been heavily utilized in
symbolic story generation. However, few machine learning systems for story
understanding employ event causality, partially due to the lack of reliable
methods for identifying open-world causal event relations. Leveraging recent
progress in large language models (LLMs), we present the first method for event
causality identification that leads to material improvements in computational
story understanding. We design specific prompts for extracting event causal
relations from GPT. Against human-annotated event causal relations in the
GLUCOSE dataset, our technique performs on par with supervised models, while
being easily generalizable to stories of different types and lengths. The
extracted causal relations lead to 5.7\% improvements on story quality
evaluation and 8.7\% on story video-text alignment. Our findings indicate
enormous untapped potential for event causality in computational story
understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09649">ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification. (arXiv:2311.09649v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yaxin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1">Hamed Zamani</a></p>
<p>This paper focuses on the task of Extreme Multi-Label Classification (XMC)
whose goal is to predict multiple labels for each instance from an extremely
large label space. While existing research has primarily focused on fully
supervised XMC, real-world scenarios often lack complete supervision signals,
highlighting the importance of zero-shot settings. Given the large label space,
utilizing in-context learning approaches is not trivial. We address this issue
by introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage
framework that cuts down the search space by generating a set of candidate
labels through incontext learning and then reranks them. Extensive experiments
suggest that ICXML advances the state of the art on two diverse public
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09656">Structured Chemistry Reasoning with Large Language Models. (arXiv:2311.09656v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1">Siru Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhuosheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bing Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">Lianhui Qin</a></p>
<p>This paper studies the problem of solving complex chemistry problems with
large language models (LLMs). Despite the extensive general knowledge in LLMs
(such as GPT-4), they struggle with chemistry reasoning that requires faithful
grounded reasoning with diverse chemical knowledge and an integrative
understanding of chemical interactions. We propose InstructChem, a new
structured reasoning approach that substantially boosts the LLMs' chemical
reasoning capabilities. InstructChem explicitly decomposes the reasoning into
three critical phrases, including chemical formulae generation by LLMs that
offers the basis for subsequent grounded reasoning, step-by-step reasoning that
makes multi-step derivations with the identified formulae for a preliminary
answer, and iterative review-and-refinement that steers LLMs to progressively
revise the previous phases for increasing confidence, leading to the final
high-confidence answer. We conduct extensive experiments on four different
chemistry challenges, including quantum chemistry, quantum mechanics, physical
chemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on
chemistry reasoning, yielding an 8% average absolute improvement and a 30% peak
improvement. We further use the generated reasoning by GPT-4 to fine-tune
smaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs.
This validates our approach and enables LLMs to generate high-quality
reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09661">Evolving Domain Adaptation of Pretrained Language Models for Text Classification. (arXiv:2311.09661v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1">Yun-Shiuan Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1">Dhruv Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Uppaal_R/0/1/0/all/0/1">Rheeya Uppaal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Ananya Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Luhang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1">Makesh Narsimhan Sreedhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sijia Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1">Timothy T. Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Junjie Hu</a></p>
<p>Adapting pre-trained language models (PLMs) for time-series text
classification amidst evolving domain shifts (EDS) is critical for maintaining
accuracy in applications like stance detection. This study benchmarks the
effectiveness of evolving domain adaptation (EDA) strategies, notably
self-training, domain-adversarial training, and domain-adaptive pretraining,
with a focus on an incremental self-training method. Our analysis across
various datasets reveals that this incremental method excels at adapting PLMs
to EDS, outperforming traditional domain adaptation techniques. These findings
highlight the importance of continually updating PLMs to ensure their
effectiveness in real-world applications, paving the way for future research
into PLM robustness against the natural temporal evolution of language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09665">Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds. (arXiv:2311.09665v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1">Yun-Shiuan Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1">Siddharth Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Harlalka_N/0/1/0/all/0/1">Nikunj Harlalka</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Agam Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1">Robert Hawkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sijia Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Dhavan Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Junjie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1">Timothy T. Rogers</a></p>
<p>This study investigates the potential of Large Language Models (LLMs) to
simulate human group dynamics, particularly within politically charged
contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to
role-play as Democrat and Republican personas, engaging in a structured
interaction akin to human group study. Our approach evaluates how agents'
responses evolve through social influence. Our key findings indicate that LLM
agents role-playing detailed personas and without Chain-of-Thought (CoT)
reasoning closely align with human behaviors, while having CoT reasoning hurts
the alignment. However, incorporating explicit biases into agent prompts does
not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning
LLMs with human data shows promise in achieving human-like behavior but poses a
risk of overfitting certain behaviors. These findings show the potential and
limitations of using LLM agents in modeling human group phenomena.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09668">Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring. (arXiv:2311.09668v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhouxing Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a></p>
<p>The strong general capabilities of Large Language Models (LLMs) bring
potential ethical risks if they are unrestrictedly accessible to malicious
users. Token-level watermarking inserts watermarks in the generated texts by
altering the token probability distributions with a private random number
generator seeded by its prefix tokens. However, this watermarking algorithm
alters the logits during generation, which can lead to a downgraded text
quality if it chooses to promote tokens that are less relevant given the input.
In this work, we propose to improve the quality of texts generated by a
watermarked language model by Watermarking with Importance Scoring (WIS). At
each generation step, we estimate the importance of the token to generate, and
prevent it from being impacted by watermarking if it is important for the
semantic correctness of the output. We further propose three methods to predict
importance scoring, including a perturbation-based method and two model-based
methods. Empirical experiments show that our method can generate texts with
better quality with comparable level of detection rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09675">Where Do People Tell Stories Online? Story Detection Across Online Communities. (arXiv:2311.09675v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1">Maria Antoniak</a>, <a href="http://arxiv.org/find/cs/1/au:+Mire_J/0/1/0/all/0/1">Joel Mire</a>, <a href="http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1">Maarten Sap</a>, <a href="http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1">Elliott Ash</a>, <a href="http://arxiv.org/find/cs/1/au:+Piper_A/0/1/0/all/0/1">Andrew Piper</a></p>
<p>People share stories online for a myriad of purposes, whether as a means of
self-disclosure, processing difficult personal experiences, providing needed
information or entertainment, or persuading others to share their beliefs.
Better understanding of online storytelling can illuminate the dynamics of
social movements, sensemaking practices, persuasion strategies, and more.
However, unlike other media such as books and visual content where the
narrative nature of the content is often overtly signaled at the document
level, studying storytelling in online communities is challenging due to the
mixture of storytelling and non-storytelling behavior, which can be
interspersed within documents and across diverse topics and settings. We
introduce a codebook and create the Storytelling in Online Communities Corpus,
an expert-annotated dataset of 502 English-language posts and comments with
labeled story and event spans. Using our corpus, we train and evaluate an
online story detection model, which we use to investigate the role storytelling
of in different social contexts. We identify distinctive features of online
storytelling, the prevalence of storytelling among different communities, and
the conversational patterns of storytelling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09677">R-Tuning: Teaching Large Language Models to Refuse Unknown Questions. (arXiv:2311.09677v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1">Shizhe Diao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1">Yi R. Fung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1">Qing Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>Large language models (LLMs) have revolutionized numerous domains with their
impressive performance but still face their challenges. A predominant issue is
the propensity for these models to generate non-existent facts, a concern
termed hallucination. Our research is motivated by the observation that
previous instruction tuning methods force the model to complete a sentence no
matter whether the model knows the knowledge or not. When the question is out
of the parametric knowledge, it will try to make up something and fail to
indicate when it lacks knowledge. In this paper, we present a new approach
called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized
by first identifying the knowledge gap between parametric knowledge and the
instruction tuning data. Then, we construct the refusal-aware data based on the
knowledge intersection, to tune LLMs to refrain from responding to questions
beyond its parametric knowledge. Experimental results demonstrate this new
instruction tuning approach effectively improves a model's ability to answer
known questions and refrain from answering unknown questions. Furthermore, when
tested on out-of-domain datasets, the refusal ability was found to be a
meta-skill that could be generalized to other tasks. Further analysis
surprisingly finds that learning the uncertainty during training displays a
better ability to estimate uncertainty than uncertainty-based testing. Our code
will be released at https://github.com/shizhediao/R-Tuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09682">MacGyver: Are Large Language Models Creative Problem Solvers?. (arXiv:2311.09682v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yufei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1">Abhilasha Ravichander</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1">Lianhui Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1">Ronan Le Bras</a>, <a href="http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1">Raja Marjieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1">Thomas L. Griffiths</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a></p>
<p>We explore the creative problem-solving capabilities of modern large language
models (LLMs) in a constrained setting. The setting requires circumventing a
cognitive bias known in psychology as ''functional fixedness'' to use familiar
objects in innovative or unconventional ways. To this end, we create MacGyver,
an automatically generated dataset consisting of 1,600 real-world problems that
deliberately trigger functional fixedness and require thinking
'out-of-the-box'. We then present our collection of problems to both LLMs and
humans to compare and contrast their problem-solving abilities. We show that
MacGyver is challenging for both groups, but in unique and complementary ways.
For example, humans typically excel in solving problems that they are familiar
with but may struggle with tasks requiring domain-specific knowledge, leading
to a higher variance. On the other hand, LLMs, being exposed to a variety of
highly specialized knowledge, attempt broader problems but are prone to
overconfidence and propose actions that are physically infeasible or
inefficient. We also provide a detailed error analysis of LLMs, and demonstrate
the potential of enhancing their problem-solving ability with novel prompting
techniques such as iterative step-wise reflection and divergent-convergent
thinking. This work provides insight into the creative problem-solving
capabilities of humans and AI and illustrates how psychological paradigms can
be extended into large-scale tasks for comparing humans and machines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09684">Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation. (arXiv:2311.09684v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zonghai Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaafar_A/0/1/0/all/0/1">Ahmed Jaafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Beining Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yue Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhichao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hong Yu</a></p>
<p>This study examines the effect of prompt engineering on the performance of
Large Language Models (LLMs) in clinical note generation. We introduce an
Automatic Prompt Optimization (APO) framework to refine initial prompts and
compare the outputs of medical experts, non-medical experts, and APO-enhanced
GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in
standardizing prompt quality across clinical note sections. A human-in-the-loop
approach shows that experts maintain content quality post-APO, with a
preference for their own modifications, suggesting the value of expert
customization. We recommend a two-phase optimization process, leveraging
APO-GPT4 for consistency and expert input for personalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09687">Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies. (arXiv:2311.09687v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zihao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Siyi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Ashwin Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1">Kristina Lerman</a></p>
<p>Social media platforms are rife with politically charged discussions.
Therefore, accurately deciphering and predicting partisan biases using Large
Language Models (LLMs) is increasingly critical. In this study, we address the
challenge of understanding political bias in digitized discourse using LLMs.
While traditional approaches often rely on finetuning separate models for each
political faction, our work innovates by employing a singular,
instruction-tuned LLM to reflect a spectrum of political ideologies. We present
a comprehensive analytical framework, consisting of Partisan Bias Divergence
Assessment and Partisan Class Tendency Prediction, to evaluate the model's
alignment with real-world political ideologies in terms of stances, emotions,
and moral foundations. Our findings reveal the model's effectiveness in
capturing emotional and moral nuances, albeit with some challenges in stance
detection, highlighting the intricacies and potential for refinement in NLP
tools for politically sensitive contexts. This research contributes
significantly to the field by demonstrating the feasibility and importance of
nuanced political understanding in LLMs, particularly for applications
requiring acute awareness of political bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09693">BLT: Can Large Language Models Handle Basic Legal Text?. (arXiv:2311.09693v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blair_Stanek_A/0/1/0/all/0/1">Andrew Blair-Stanek</a>, <a href="http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1">Nils Holzenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a></p>
<p>We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently
perform poorly at basic text handling required of lawyers or paralegals, such
as looking up the text at a line of a witness deposition or at a subsection of
a contract. We introduce a benchmark to quantify this poor performance, which
casts into doubt LLMs' current reliability as-is for legal practice. Finetuning
for these tasks brings an older LLM to near-perfect performance on our test set
and also raises performance on a related legal task. This stark result
highlights the need for more domain expertise in LLM training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09694">Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness. (arXiv:2311.09694v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ashim Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendhran_R/0/1/0/all/0/1">Rishanth Rajendhran</a>, <a href="http://arxiv.org/find/cs/1/au:+Stringham_N/0/1/0/all/0/1">Nathan Stringham</a>, <a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1">Vivek Srikumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1">Ana Marasovi&#x107;</a></p>
<p>Are the longstanding robustness issues in NLP resolved by today's larger and
more performant models? To address this question, we conduct a thorough
investigation using 19 models of different sizes spanning different
architectural choices and pretraining objectives. We conduct evaluations using
(a) OOD and challenge test sets, (b) CheckLists, (c) contrast sets, and (d)
adversarial inputs. Our analysis reveals that not all OOD tests provide further
insight into robustness. Evaluating with CheckLists and contrast sets shows
significant gaps in model performance; merely scaling models does not make them
sufficiently robust. Finally, we point out that current approaches for
adversarial evaluations of models are themselves problematic: they can be
easily thwarted, and in their current forms, do not represent a sufficiently
deep probe of model robustness. We conclude that not only is the question of
robustness in NLP as yet unresolved, but even some of the approaches to measure
robustness need to be reassessed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09696">Fumbling in Babel: An Investigation into ChatGPT&#x27;s Language Identification Ability. (arXiv:2311.09696v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei-Rui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1">Ife Adebara</a>, <a href="http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1">Khai Duy Doan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1">Qisheng Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1">Muhammad Abdul-Mageed</a></p>
<p>Recently, ChatGPT has emerged as a powerful NLP tool that can carry out
several tasks. However, the range of languages ChatGPT can handle remains
largely a mystery. In this work, we investigate ChatGPT's language
identification abilities. For this purpose, we compile Babel-670, a benchmark
comprising $670$ languages representing $23$ language families. Languages in
Babel-670 run the gamut between the very high-resource to the very low-resource
and are spoken in five continents. We then study ChatGPT's (both GPT-3.5 and
GPT-4) ability to (i) identify both language names and language codes (ii)
under both zero- and few-shot conditions (iii) with and without provision of
label set. When compared to smaller finetuned language identification tools, we
find that ChatGPT lags behind. Our empirical analysis shows the reality that
ChatGPT still resides in a state of potential enhancement before it can
sufficiently serve diverse communities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09702">Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?. (arXiv:2311.09702v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bangzheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Ben Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1">Xingyu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>Despite the recent advancement in large language models (LLMs) and their high
performances across numerous benchmarks, recent research has unveiled that LLMs
suffer from hallucinations and unfaithful reasoning. This work studies a
specific type of hallucination induced by semantic associations. Specifically,
we investigate to what extent LLMs take shortcuts from certain keyword/entity
biases in the prompt instead of following the correct reasoning path. To
quantify this phenomenon, we propose a novel probing method and benchmark
called EureQA. We start from questions that LLMs will answer correctly with
utmost certainty, and mask the important entity with evidence sentence
recursively, asking models to find masked entities according to a chain of
evidence before answering the question.
</p>
<p>During the construction of the evidence, we purposefully replace semantic
clues (entities) that may lead to the correct answer with distractor clues
(evidence) that will not directly lead to the correct answer but require a
chain-like reasoning process. We evaluate if models can follow the correct
reasoning chain instead of short-cutting through distractor clues. We find that
existing LLMs lack the necessary capabilities to follow correct reasoning paths
and resist the attempt of greedy shortcuts. We show that the distractor
semantic associations often lead to model hallucination, which is strong
evidence that questions the validity of current LLM reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09707">GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding. (arXiv:2311.09707v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1">Andor Diera</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahou_A/0/1/0/all/0/1">Abdelhalim Dahou</a>, <a href="http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1">Lukas Galke</a>, <a href="http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1">Fabian Karl</a>, <a href="http://arxiv.org/find/cs/1/au:+Sihler_F/0/1/0/all/0/1">Florian Sihler</a>, <a href="http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1">Ansgar Scherp</a></p>
<p>Language models can serve as a valuable tool for software developers to
increase productivity. Large generative models can be used for code generation
and code completion, while smaller encoder-only models are capable of
performing code search tasks using natural language queries.These capabilities
are heavily influenced by the quality and diversity of the available training
data. Source code datasets used for training usually focus on the most popular
languages and testing is mostly conducted on the same distributions, often
overlooking low-resource programming languages. Motivated by the NLP
generalization taxonomy proposed by Hupkes et.\,al., we propose a new benchmark
dataset called GenCodeSearchNet (GeCS) which builds upon existing natural
language code search datasets to systemically evaluate the programming language
understanding generalization capabilities of language models. As part of the
full dataset, we introduce a new, manually curated subset StatCodeSearch that
focuses on R, a popular but so far underrepresented programming language that
is often used by researchers outside the field of computer science. For
evaluation and comparison, we collect several baseline results using fine-tuned
BERT-style models and GPT-style large language models in a zero-shot setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09708">A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection. (arXiv:2311.09708v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thi-Nhung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1">Hoang Ngo</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Kiem-Hieu Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1">Tuan-Dung Cao</a></p>
<p>Our work addresses the problem of unsupervised Aspect Category Detection
using a small set of seed words. Recent works have focused on learning
embedding spaces for seed words and sentences to establish similarities between
sentences and aspects. However, aspect representations are limited by the
quality of initial seed words, and model performances are compromised by noise.
To mitigate this limitation, we propose a simple framework that automatically
enhances the quality of initial seed words and selects high-quality sentences
for training instead of using the entire dataset. Our main concepts are to add
a number of seed words to the initial set and to treat the task of noise
resolution as a task of augmenting data for a low-resource task. In addition,
we jointly train Aspect Category Detection with Aspect Term Extraction and
Aspect Term Polarity to further enhance performance. This approach facilitates
shared representation learning, allowing Aspect Category Detection to benefit
from the additional guidance offered by other tasks. Extensive experiments
demonstrate that our framework surpasses strong baselines on standard datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09709">Large Language Model Inference with Lexical Shortlisting. (arXiv:2311.09709v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1">Nikolay Bogoychev</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pinzhen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1">Barry Haddow</a>, <a href="http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1">Alexandra Birch</a></p>
<p>Large language model (LLM) inference is computation and memory intensive, so
we adapt lexical shortlisting to it hoping to improve both. While lexical
shortlisting is well-explored in tasks like machine translation, it requires
modifications before being suitable for LLMs as the intended applications vary
significantly. Our work studies two heuristics to shortlist sub-vocabulary at
LLM inference time: Unicode-based script filtering and corpus-based selection.
We explore different LLM families and sizes, and we find that lexical
shortlisting can reduce the memory usage of some models by nearly 50\% and has
an upper bound of 25\% improvement in generation speed. In this pilot study, we
also identify the drawbacks of such vocabulary selection methods and propose
avenues for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09712">Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning. (arXiv:2311.09712v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacob_A/0/1/0/all/0/1">Athul Paul Jacob</a>, <a href="http://arxiv.org/find/cs/1/au:+Farina_G/0/1/0/all/0/1">Gabriele Farina</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a></p>
<p>We present a model of pragmatic language understanding, where utterances are
produced and understood by searching for regularized equilibria of signaling
games. In this model (which we call ReCo, for Regularized Conventions),
speakers and listeners search for contextually appropriate utterance--meaning
mappings that are both close to game-theoretically optimal conventions and
close to a shared, ''default'' semantics. By characterizing pragmatic
communication as equilibrium search, we obtain principled sampling algorithms
and formal guarantees about the trade-off between communicative success and
naturalness. Across several datasets capturing real and idealized human
judgments about pragmatic implicatures, ReCo matches or improves upon
predictions made by best response and rational speech act models of language
understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09718">You don&#x27;t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments. (arXiv:2311.09718v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shu_B/0/1/0/all/0/1">Bangzhao Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lechen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minje Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dunagan_L/0/1/0/all/0/1">Lavinia Dunagan</a>, <a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1">Dallas Card</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1">David Jurgens</a></p>
<p>The versatility of Large Language Models (LLMs) on natural language
understanding tasks has made them popular for research in social sciences. In
particular, to properly understand the properties and innate personas of LLMs,
researchers have performed studies that involve using prompts in the form of
questions that ask LLMs of particular opinions. In this study, we take a
cautionary step back and examine whether the current format of prompting
enables LLMs to provide responses in a consistent and robust manner. We first
construct a dataset that contains 693 questions encompassing 39 different
instruments of persona measurement on 115 persona axes. Additionally, we design
a set of prompts containing minor variations and examine LLM's capabilities to
generate accurate answers, as well as consistency variations to examine their
consistency towards simple perturbations such as switching the option order.
Our experiments on 15 different open-source LLMs reveal that even simple
perturbations are sufficient to significantly downgrade a model's
question-answering ability, and that most LLMs have low negation consistency.
Our results suggest that the currently widespread practice of prompting is
insufficient to accurately capture model perceptions, and we discuss potential
alternatives to improve such issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09721">On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering. (arXiv:2311.09721v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1">Linyong Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Ellen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1">Weijin Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wenfei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>This study introduces a new long-form database question answering dataset
designed to evaluate how Large Language Models (LLMs) interact with a SQL
interpreter. The task necessitates LLMs to strategically generate multiple SQL
queries to retrieve sufficient data from a database, to reason with the
acquired context, and to synthesize them into a comprehensive analytical
narrative. Our findings highlight that this task poses great challenges even
for the state-of-the-art GPT-4 model. We propose and evaluate two interaction
strategies, and provide a fine-grained analysis of the individual stages within
the interaction. A key discovery is the identification of two primary
bottlenecks hindering effective interaction: the capacity for planning and the
ability to generate multiple SQL queries. To address the challenge of
accurately assessing answer quality, we introduce a multi-agent evaluation
framework that simulates the academic peer-review process, enhancing the
precision and reliability of our evaluations. This framework allows for a more
nuanced understanding of the strengths and limitations of current LLMs in
complex retrieval and reasoning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09724">Outcome-supervised Verifiers for Planning in Mathematical Reasoning. (arXiv:2311.09724v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1">Anningzhe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a></p>
<p>Large language models (LLMs) often struggle with maintaining accuracy across
a sequence of intermediate reasoning steps in mathematical reasoning, leading
to error propagation that undermines the final result. The current methodology
to mitigate this issue primarily involves using a verifier model to assess the
correctness of generated solution candidates, focusing either on the overall
reasoning path or on an incomplete reasoning path. By rethinking this approach,
we argue that assessing potentials of incomplete reasoning paths could be more
advantageous as it guides towards correct final answers, transforming the task
into a \textit{planning} problem. Our proposed verifier, the
Outcome-supervision Value Model (OVM), employs outcome supervision for
training, offering an efficient and intuitive method for \textit{planning} by
prioritizing steps that lead to accurate conclusions over mere per-step
correctness. Furthermore, the OVM eschews the need for labor-intensive
annotations on step-level correctness, enhancing its scalability. Our
experiments on two multi-step mathematical reasoning datasets, GSM8K and Game
of 24, demonstrate the superior performance of the OVM model. Notably, in
GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up
to 13B parameters}; especially it does not utilize GPT-4 or code execution.
These findings offer a novel perspective on the role of outcome supervision in
training verifiers for multi-step reasoning tasks and provide theoretical
justification for its advantage in value estimation for planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09730">Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks. (arXiv:2311.09730v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Huaman Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jiaxin Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minje Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1">David Jurgens</a></p>
<p>Human perception of language depends on personal backgrounds like gender and
ethnicity. While existing studies have shown that large language models (LLMs)
hold values that are closer to certain societal groups, it is unclear whether
their prediction behaviors on subjective NLP tasks also exhibit a similar bias.
In this study, leveraging the POPQUORN dataset which contains annotations of
diverse demographic backgrounds, we conduct a series of experiments on four
popular LLMs to investigate their capability to understand group differences
and potential biases in their predictions for politeness and offensiveness. We
find that for both tasks, model predictions are closer to the labels from White
and female participants. We further explore prompting with the target
demographic labels and show that including the target demographic in the prompt
actually worsens the model's performance. More specifically, when being
prompted to respond from the perspective of "Black" and "Asian" individuals,
models show lower performance in predicting both overall scores as well as the
scores from corresponding groups. Our results suggest that LLMs hold gender and
racial biases for subjective NLP tasks and that demographic-infused prompts
alone may be insufficient to mitigate such effects. Code and data are available
at https://github.com/Jiaxin-Pei/LLM-Group-Bias.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09731">Prudent Silence or Foolish Babble? Examining Large Language Models&#x27; Responses to the Unknown. (arXiv:2311.09731v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Genglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lifan Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hao Peng</a></p>
<p>Large Language Models (LLMs) often struggle when faced with situations where
they lack the prerequisite knowledge to generate a sensical response. In these
cases, models tend to fabricate and hallucinate, rather than appropriately
signaling uncertainty as humans would. This behavior misaligns with human
conversational norms and presents challenges surrounding responsible and
ethical AI development. This work aims to systematically investigate LLMs'
behaviors in such situations. We curate an adversarial question-answering
benchmark containing unanswerable questions targeting information absent from
the LLM's training data. Concretely, these unanswerable questions contain
non-existent concepts or false premises. When presented with such unanswerable
questions, an LLM should appropriately convey uncertainty, and be able to
challenge the premise and refuse to generate a response. While facing
answerable valid questions, a model should demonstrate a positive correlation
between accuracy and confidence. Using a model-agnostic unified confidence
elicitation approach, we observe that LLMs that have gone through instruction
finetuning and reinforcement learning from human feedback (RLHF) perform
significantly better than their counterparts that do not. Moreover, uncertainty
expression 1 through our elicitation method does not always stay consistent
with the perceived confidence of the direct response of an LLM. Our findings
call for further research into teaching LLMs to proactively and reliably
express uncertainty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09732">Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources. (arXiv:2311.09732v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yipei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1">Dakuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jiaqing Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1">Yipeng Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1">Yingsi Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hengkui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Ken Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+zhang_r/0/1/0/all/0/1">ruiji zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yanghua Xiao</a></p>
<p>Pre-trained language models (PLMs) have established the new paradigm in the
field of NLP. For more powerful PLMs, one of the most popular and successful
way is to continuously scale up sizes of the models and the pre-training
corpora. These large corpora are generally obtained by converging smaller ones
from multiple sources, they are thus growing increasingly diverse. However, the
side-effects of these colossal converged corpora remain understudied. In this
paper, we identify the disadvantage of heterogeneous corpora from multiple
sources for pre-training PLMs. Towards coordinated pre-training on diverse
corpora, we further propose source prompts (SP), which explicitly prompt the
model of the data source at the pre-training and fine-tuning stages. Results of
extensive experiments demonstrate that PLMs pre-trained with SP on diverse
corpora gain significant improvement in various downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09733">MOKA: Moral Knowledge Augmentation for Moral Event Extraction. (arXiv:2311.09733v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinliang Frederick Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Winston Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Beauchamp_N/0/1/0/all/0/1">Nick Beauchamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a></p>
<p>News media employ moral language to create memorable stories, and readers
often engage with the content that align with their values. Moral theories have
been applied to news analysis studying moral values in isolation, while the
intricate dynamics among participating entities in shaping moral events have
been overlooked. This is mainly due to the use of obscure language to conceal
evident ideology and values, coupled with the insufficient moral reasoning
capability in most existing NLP systems, where LLMs are no exception. To study
this phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of
5,494 structured annotations on 474 news articles by diverse US media across
the political spectrum. We further propose MOKA, a moral event extraction
framework with MOral Knowledge Augmentation, that leverages knowledge derived
from moral words and moral scenarios. Experimental results show that MOKA
outperforms competitive baselines across three moral event understanding tasks.
Further analyses illuminate the selective reporting of moral events by media
outlets of different ideological leanings, suggesting the significance of
event-level morality analysis in news. Our datasets and codebase are available
at https://github.com/launchnlp/MOKA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09734">Tracking the Newsworthiness of Public Documents. (arXiv:2311.09734v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1">Alexander Spangher</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1">Emilio Ferrara</a>, <a href="http://arxiv.org/find/cs/1/au:+Welsh_B/0/1/0/all/0/1">Ben Welsh</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tumgoren_S/0/1/0/all/0/1">Serdar Tumgoren</a>, <a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1">Jonathan May</a></p>
<p>Journalists must find stories in huge amounts of textual data (e.g. leaks,
bills, press releases) as part of their jobs: determining when and why text
becomes news can help us understand coverage patterns and help us build
assistive tools. Yet, this is challenging because very few labelled links
exist, language use between corpora is very different, and text may be covered
for a variety of reasons. In this work we focus on news coverage of local
public policy in the San Francisco Bay Area by the San Francisco Chronicle.
First, we gather news articles, public policy documents and meeting recordings
and link them using probabilistic relational modeling, which we show is a
low-annotation linking methodology that outperforms other retrieval-based
baselines. Second, we define a new task: newsworthiness prediction, to predict
if a policy item will get covered. We show that different aspects of public
policy discussion yield different newsworthiness signals. Finally we perform
human evaluation with expert journalists and show our systems identify policies
they consider newsworthy with 68% F1 and our coverage recommendations are
helpful with an 84% win-rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09736">CARE: Extracting Experimental Findings From Clinical Literature. (arXiv:2311.09736v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1">Aakanksha Naik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1">Bailey Kuehl</a>, <a href="http://arxiv.org/find/cs/1/au:+Bransom_E/0/1/0/all/0/1">Erin Bransom</a>, <a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1">Doug Downey</a>, <a href="http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1">Tom Hope</a></p>
<p>Extracting fine-grained experimental findings from literature can provide
massive utility for scientific applications. Prior work has focused on
developing annotation schemas and datasets for limited aspects of this problem,
leading to simpler information extraction datasets which do not capture the
real-world complexity and nuance required for this task. Focusing on
biomedicine, this work presents CARE (Clinical Aggregation-oriented Result
Extraction) -- a new IE dataset for the task of extracting clinical findings.
We develop a new annotation schema capturing fine-grained findings as n-ary
relations between entities and attributes, which includes phenomena challenging
for current IE systems such as discontinuous entity spans, nested relations,
and variable arity n-ary relations. Using this schema, we collect extensive
annotations for 700 abstracts from two sources: clinical trials and case
reports. We also benchmark the performance of various state-of-the-art IE
systems on our dataset, including extractive models and generative LLMs in
fully supervised and limited data settings. Our results demonstrate the
difficulty of our dataset -- even SOTA models such as GPT4 struggle,
particularly on relation extraction. We release our annotation schema and CARE
to encourage further research on extracting and aggregating scientific findings
from literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09741">What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization. (arXiv:2311.09741v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuhan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shangbin Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xiaochuang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1">Vidhisha Balachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1">Chan Young Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sachin Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1">Yulia Tsvetkov</a></p>
<p>In this work, we take a first step towards designing summarization systems
that are faithful to the author's opinions and perspectives. Focusing on a case
study of preserving political perspectives in news summarization, we find that
existing approaches alter the political opinions and stances of news articles
in more than 50% of summaries, misrepresenting the intent and perspectives of
the news authors. We thus propose P^3Sum, a diffusion model-based summarization
approach controlled by political perspective classifiers. In P^3Sum, the
political leaning of a generated summary is iteratively evaluated at each
decoding step, and any drift from the article's original stance incurs a loss
back-propagated to the embedding layers, steering the political stance of the
summary at inference time. Extensive experiments on three news summarization
datasets demonstrate that P^3Sum outperforms state-of-the-art summarization
systems and large language models by up to 11.4% in terms of the success rate
of stance preservation, with on-par performance on standard summarization
utility metrics. These findings highlight the lacunae that even for
state-of-the-art models it is still challenging to preserve author perspectives
in news summarization, while P^3Sum presents an important first step towards
evaluating and developing summarization systems that are faithful to author
intent and perspectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09743">Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks. (arXiv:2311.09743v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1">Negar Mokhberian</a>, <a href="http://arxiv.org/find/cs/1/au:+Marmarelis_M/0/1/0/all/0/1">Myrl G. Marmarelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Hopp_F/0/1/0/all/0/1">Frederic R. Hopp</a>, <a href="http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1">Valerio Basile</a>, <a href="http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1">Fred Morstatter</a>, <a href="http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1">Kristina Lerman</a></p>
<p>In most classification models, it has been assumed to have a single ground
truth label for each data point. However, subjective tasks like toxicity
classification can lead to genuine disagreement among annotators. In these
cases aggregating labels will result in biased labeling and, consequently,
biased models that can overlook minority opinions. Previous studies have shed
light on the pitfalls of label aggregation and have introduced a handful of
practical approaches to tackle this issue. Recently proposed multi-annotator
models, which predict labels individually per annotator, are vulnerable to
under-determination for annotators with small samples. This problem is
especially the case in crowd-sourced datasets. In this work, we propose
Annotator Aware Representations for Texts (AART) for subjective classification
tasks. We will show the improvement of our method on metrics that assess the
performance on capturing annotators' perspectives. Additionally, our approach
involves learning representations for annotators, allowing for an exploration
of the captured annotation behaviors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09748">Translation Aligned Sentence Embeddings for Turkish Language. (arXiv:2311.09748v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Unlu_E/0/1/0/all/0/1">Eren Unlu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciftci_U/0/1/0/all/0/1">Unver Ciftci</a></p>
<p>Due to the limited availability of high quality datasets for training
sentence embeddings in Turkish, we propose a training methodology and a regimen
to develop a sentence embedding model. The central idea is simple but effective
: is to fine-tune a pretrained encoder-decoder model in two consecutive stages,
where the first stage involves aligning the embedding space with translation
pairs. Thanks to this alignment, the prowess of the main model can be better
projected onto the target language in a sentence embedding setting where it can
be fine-tuned with high accuracy in short duration with limited target language
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09755">How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?. (arXiv:2311.09755v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Miles Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1">Nikolaos Aletras</a></p>
<p>Pruning and quantization form the foundation of model compression for neural
networks, enabling efficient inference for large language models (LLMs).
Recently, various quantization and pruning techniques have demonstrated
state-of-the-art performance in a post-training setting. They rely upon
calibration data, a small set of unlabeled examples, to generate layer
activations. However, no prior work has systematically investigated how the
calibration data impacts the effectiveness of model compression methods. In
this paper, we present the first extensive empirical study on the effect of
calibration data upon LLM performance. We trial a variety of pruning and
quantization methods, tasks, models, and datasets. Surprisingly, we find
substantial variations in downstream task performance, contrasting existing
work that suggests a greater level of robustness to the calibration data.
Finally, we make a series of recommendations for the effective use of
calibration data in LLM quantization and pruning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09756">FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children&#x27;s Storybook Narratives. (arXiv:2311.09756v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaju Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuxuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bingsheng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuanzhe Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Ying Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuling Sun</a></p>
<p>AI models (including LLM) often rely on narrative question-answering (QA)
datasets to provide customized QA functionalities to support downstream
children education applications; however, existing datasets only include QA
pairs that are grounded within the given storybook content, but children can
learn more when teachers refer the storybook content to real-world knowledge
(e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is
annotated by children education experts, to supplement 278 storybook narratives
with educationally appropriate commonsense knowledge. The dataset has 5,868 QA
pairs that not only originate from the storybook narrative but also contain the
commonsense knowledge grounded by an external knowledge graph (i.e.,
ConceptNet). A follow-up experiment shows that a smaller model (T5-large)
fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered
LLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result
suggests that: 1) our dataset brings novel challenges to existing LLMs, and 2)
human experts' data annotation are still critical as they have much nuanced
knowledge that LLMs do not know in the children educational domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09758">OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking. (arXiv:2311.09758v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chia-Hsuan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1">Mari Ostendorf</a></p>
<p>Large language models (LLMs) have revolutionized the landscape of Natural
Language Processing systems, but are computationally expensive. To reduce the
cost without sacrificing performance, previous studies have explored various
approaches to harness the potential of Small Language Models (SLMs) as
cost-effective alternatives to their larger counterparts. Driven by findings
that SLMs and LLMs exhibit complementary strengths in a structured knowledge
extraction task, this work presents a novel SLM/LLM routing framework designed
to improve computational efficiency and enhance task performance. First,
exemplar pools are created to represent the types of contexts where each LM
provides a more reliable answer, leveraging a sentence embedding fine-tuned so
that context similarity is close to dialogue state similarity. Then, during
inference, the k-nearest exemplars to the testing instance are retrieved, and
the instance is routed according to majority vote. In dialogue state tracking
tasks, the proposed routing framework enhances performance substantially
compared to relying solely on LLMs, while reducing the computational costs by
over 50%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09761">MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification. (arXiv:2311.09761v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Helwe_C/0/1/0/all/0/1">Chadi Helwe</a>, <a href="http://arxiv.org/find/cs/1/au:+Calamai_T/0/1/0/all/0/1">Tom Calamai</a>, <a href="http://arxiv.org/find/cs/1/au:+Paris_P/0/1/0/all/0/1">Pierre-Henri Paris</a>, <a href="http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1">Chlo&#xe9; Clavel</a>, <a href="http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1">Fabian Suchanek</a></p>
<p>Fallacies can be used to spread disinformation, fake news, and propaganda,
underlining the importance of their detection. Automated detection and
classification of fallacies, however, remain challenging, mainly because of the
innate subjectivity of the task and the need for a comprehensive, unified
approach in existing research. Addressing these limitations, our study
introduces a novel taxonomy of fallacies that aligns and refines previous
classifications, a new annotation scheme tailored for subjective NLP tasks, and
a new evaluation method designed to handle subjectivity, adapted to precision,
recall, and F1-Score metrics. Using our annotation scheme, the paper introduces
MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset.
MAFALDA is based on examples from various previously existing fallacy datasets
under our unified taxonomy across three levels of granularity. We then evaluate
several language models under a zero-shot learning setting using MAFALDA to
assess their fallacy detection and classification capability. Our comprehensive
evaluation not only benchmarks the performance of these models but also
provides valuable insights into their strengths and limitations in addressing
fallacious reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09762">Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models. (arXiv:2311.09762v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Ameen Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_O/0/1/0/all/0/1">Omar Zia Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunwoo J. Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joo-Kyung Kim</a></p>
<p>Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning
capabilities of Large Language Models (LLMs) by generating a series of
rationales before the final answer. We analyze the reasoning paths generated by
CoT and find two issues in multi-step reasoning: (i) Generating rationales
irrelevant to the question, (ii) Unable to compose subquestions or queries for
generating/retrieving all the relevant information. To address them, we propose
a graph-guided CoT prompting method, which guides the LLMs to reach the correct
answer with graph representation/verification steps. Specifically, we first
leverage LLMs to construct a "question/rationale graph" by using knowledge
extraction prompting given the initial question and the rationales generated in
the previous steps. Then, the graph verification step diagnoses the current
rationale triplet by comparing it with the existing question/rationale graph to
filter out irrelevant rationales and generate follow-up questions to obtain
relevant information. Additionally, we generate CoT paths that exclude the
extracted graph information to represent the context information missed from
the graph extraction. Our graph-guided reasoning method shows superior
performance compared to previous CoT prompting and the variants on multi-hop
question answering benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09763">Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations. (arXiv:2311.09763v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_W/0/1/0/all/0/1">Wenjie Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiashu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiongxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>Existing studies in backdoor defense have predominantly focused on the
training phase, overlooking the critical aspect of testing time defense. This
gap becomes particularly pronounced in the context of Large Language Models
(LLMs) deployed as Web Services, which typically offer only black-box access,
rendering training-time defenses impractical. To bridge this gap, our work
introduces defensive demonstrations, an innovative backdoor defense strategy
for blackbox large language models. Our method involves identifying the task
and retrieving task-relevant demonstrations from an uncontaminated pool. These
demonstrations are then combined with user queries and presented to the model
during testing, without requiring any modifications/tuning to the black-box
model or insights into its internal mechanisms. Defensive demonstrations are
designed to counteract the adverse effects of triggers, aiming to recalibrate
and correct the behavior of poisoned models during test-time evaluations.
Extensive experiments show that defensive demonstrations are effective in
defending both instance-level and instruction-level backdoor attacks, not only
rectifying the behavior of poisoned models but also surpassing existing
baselines in most scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09766">LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores. (arXiv:2311.09766v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1">Nafise Sadat Moosavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a></p>
<p>Automatic evaluation of generated textual content presents an ongoing
challenge within the field of NLP. Given the impressive capabilities of modern
language models (LMs) across diverse NLP tasks, there is a growing trend to
employ these models in creating innovative evaluation metrics for automated
assessment of generation tasks. This paper investigates a pivotal question: Do
language model-driven evaluation metrics inherently exhibit bias favoring texts
generated by the same underlying language model? Specifically, we assess
whether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and
GPTScore--demonstrate a favorable bias toward their respective underlying LMs
in the context of summarization tasks. Our findings unveil a latent bias,
particularly pronounced when such evaluation metrics are used in an
reference-free manner without leveraging gold summaries. These results
underscore that assessments provided by generative evaluation models can be
influenced by factors beyond the inherent text quality, highlighting the
necessity of developing more dependable evaluation protocols in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09773">To be or not to be? an exploration of continuously controllable prompt engineering. (arXiv:2311.09773v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuhan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mukai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yixin Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xingyu Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a></p>
<p>As the use of large language models becomes more widespread, techniques like
parameter-efficient fine-tuning and other methods for controlled generation are
gaining traction for customizing models and managing their outputs. However,
the challenge of precisely controlling how prompts influence these models is an
area ripe for further investigation. In response, we introduce ControlPE
(Continuously Controllable Prompt Engineering). ControlPE enables finer
adjustments to prompt effects, complementing existing prompt engineering, and
effectively controls continuous targets. This approach harnesses the power of
LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,
enabling fine-tuned adjustments to the impact of prompts. Our methodology
involves generating specialized datasets for prompt distillation, incorporating
these prompts into the LoRA model, and carefully adjusting LoRA merging weight
to regulate the influence of prompts. This provides a dynamic and adaptable
tool for prompt control. Through our experiments, we have validated the
practicality and efficacy of ControlPE. It proves to be a promising solution
for control a variety of prompts, ranging from generating short responses
prompts, refusal prompts to chain-of-thought prompts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09774">HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs. (arXiv:2311.09774v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junying Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1">Anningzhe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Feng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shunian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dingjie Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Wenya Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1">Chuyi Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianquan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiang Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a></p>
<p>Adapting a language model into a specific domain, a.k.a `domain adaption', is
a common practice when specialized knowledge, e.g. medicine, is not
encapsulated in a general language model like Llama2. The challenge lies in the
heterogeneity of data across the two training stages, as it varies in
languages, genres, or formats. To tackle this and simplify the learning
protocol, we propose to transform heterogeneous data, from the both
pre-training and supervised stages, into a unified, simple input-output pair
format. We validate the new protocol in the domains where proprietary LLMs like
ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The
developed model, HuatuoGPT-II, has shown state-of-the-art performance in
Chinese medicine domain on a number of benchmarks, e.g. medical licensing
exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some
aspects, especially in Traditional Chinese Medicine. Expert manual evaluations
further validate HuatuoGPT-II's advantages over existing LLMs. Notably,
HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing
Examination where it achieved the best performance, showcasing not only its
effectiveness but also its generalization capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09782">More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering. (arXiv:2311.09782v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bingsheng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guiming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1">Ruishi Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuxuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1">James Hendler</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a></p>
<p>While most existing works on LLM prompt-engineering focus only on how to
select a better set of data samples inside one single prompt input (In-Context
Learning or ICL), why can't we design and leverage multiple prompt inputs
together to further improve the LLM performance? In this work, we propose
In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to
produce the most confident prediction results by optimizing the construction of
multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL
and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate
that ICS can consistently enhance LLM's prediction performance and confidence.
An ablation study suggests that a diversity-based ICS strategy may further
improve LLM's performance, which sheds light on a new yet promising future
research direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09783">Investigating Data Contamination in Modern Benchmarks for Large Language Models. (arXiv:2311.09783v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chunyuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>Recent observations have underscored a disparity between the inflated
benchmark scores and the actual performance of LLMs, raising concerns about
potential contamination of evaluation benchmarks. This issue is especially
critical for closed-source models and certain open-source models where training
data transparency is lacking. In this paper we study data contamination by
proposing two methods tailored for both open-source and proprietary LLMs. We
first introduce a retrieval-based system to explore potential overlaps between
evaluation benchmarks and pretraining corpora. We further present a novel
investigation protocol named \textbf{T}estset \textbf{S}lot Guessing
(\textit{TS-Guessing}), applicable to both open and proprietary models. This
approach entails masking a wrong answer in a multiple-choice question and
prompting the model to fill in the gap. Additionally, it involves obscuring an
unlikely word in an evaluation example and asking the model to produce it. We
find that certain commercial LLMs could surprisingly guess the missing option
in various test sets. Specifically, in the TruthfulQA benchmark, we find that
LLMs exhibit notable performance improvement when provided with additional
metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4
demonstrated an exact match rate of 52\% and 57\%, respectively, in guessing
the missing options in benchmark test data. We hope these results underscore
the need for more robust evaluation methodologies and benchmarks in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09796">Interpreting User Requests in the Context of Natural Language Standing Instructions. (arXiv:2311.09796v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1">Nikita Moghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1">Patrick Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1">Jason Eisner</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a>, <a href="http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1">Harsh Jhamtani</a></p>
<p>Users of natural language interfaces, generally powered by Large Language
Models (LLMs),often must repeat their preferences each time they make a similar
request. To alleviate this, we propose including some of a user's preferences
and instructions in natural language -- collectively termed standing
instructions -- as additional context for such interfaces. For example, when a
user states I'm hungry, their previously expressed preference for Persian food
will be automatically added to the LLM prompt, so as to influence the search
for relevant restaurants. We develop NLSI, a language-to-program dataset
consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is
paired with a user profile (a set of users specific standing instructions) and
corresponding structured representations (API calls). A key challenge in NLSI
is to identify which subset of the standing instructions is applicable to a
given dialogue. NLSI contains diverse phenomena, from simple preferences to
interdependent instructions such as triggering a hotel search whenever the user
is booking tickets to an event. We conduct experiments on NLSI using prompting
with large language models and various retrieval approaches, achieving a
maximum of 44.7% exact match on API prediction. Our results demonstrate the
challenges in identifying the relevant standing instructions and their
interpretation into API calls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09797">KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains. (arXiv:2311.09797v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongjun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1">Yitao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs'
capabilities in applying financial knowledge to solve complex math word
problems. Compared to prior works, this study features three core advancements.
First, KnowledgeMath includes 1,259 problems with a hybrid of textual and
tabular content and require college-level knowledge in the finance domain for
effective resolution. Second, we provide expert-annotated, detailed solution
references in Python program format, ensuring a high-quality benchmark for LLM
assessment. Finally, we evaluate a wide spectrum of 14 LLMs with different
prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The
current best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves
only 45.4% accuracy, leaving substantial room for improvement. While
knowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0%
for GPT-3.5), it is still significantly lower the estimated human expert
performance of 94%. We believe that KnowledgeMath can facilitate future
research on domain-specific knowledge retrieval and augmentation into the math
word problem-solving process. We will release the benchmark and code at
https://github.com/yale-nlp/KnowledgeMath.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09799">How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!. (arXiv:2311.09799v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1">Shirley Anugrah Hayati</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Minhwa Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1">Dheeraj Rajagopal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Dongyeop Kang</a></p>
<p>Collecting diverse human data on subjective NLP topics is costly and
challenging. As Large Language Models (LLMs) have developed human-like
capabilities, there is a recent trend in collaborative efforts between humans
and LLMs for generating diverse data, offering potential scalable and efficient
solutions. However, the extent of LLMs' capability to generate diverse
perspectives on subjective topics remains an unexplored question. In this
study, we investigate LLMs' capacity for generating diverse perspectives and
rationales on subjective topics, such as social norms and argumentative texts.
We formulate this problem as diversity extraction in LLMs and propose a
criteria-based prompting technique to ground diverse opinions and measure
perspective diversity from the generated criteria words. Our results show that
measuring semantic diversity through sentence embeddings and distance metrics
is not enough to measure perspective diversity. To see how far we can extract
diverse perspectives from LLMs, or called diversity coverage, we employ a
step-by-step recall prompting for generating more outputs from the model in an
iterative manner. As we apply our prompting method to other tasks (hate speech
labeling and story continuation), indeed we find that LLMs are able to generate
diverse opinions according to the degree of task subjectivity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09800">$\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning. (arXiv:2311.09800v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1">Evgeniia Razumovskaia</a>, <a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Markovic_P/0/1/0/all/0/1">Pavle Markovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Cichy_T/0/1/0/all/0/1">Tomasz Cichy</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qian Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_T/0/1/0/all/0/1">Tsung-Hsien Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1">Pawe&#x142; Budzianowski</a></p>
<p>Factuality is a crucial requirement in information seeking dialogue: the
system should respond to the user's queries so that the responses are
meaningful and aligned with the knowledge provided to the system. However, most
modern large language models suffer from hallucinations, that is, they generate
responses not supported by or contradicting the knowledge source. To mitigate
the issue and increase faithfulness of information-seeking dialogue systems, we
introduce BeInfo, a simple yet effective method that applies behavioural tuning
to aid information-seeking dialogue. Relying on three standard datasets, we
show that models tuned with BeInfo} become considerably more faithful to the
knowledge source both for datasets and domains seen during BeInfo-tuning, as
well as on unseen domains, when applied in a zero-shot manner. In addition, we
show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo
demonstrate strong performance on data from real `production' conversations and
outperform GPT4 when tuned on a limited amount of such realistic in-domain
dialogues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09802">Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs. (arXiv:2311.09802v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Leyang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1">Lidong Bing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1">Wai Lam</a></p>
<p>Though prompting LLMs with various reasoning structures produces reasoning
proofs along with answers, these proofs are not ensured to be causal and
reliable due to the inherent defects of LLMs. Tracking such deficiencies, we
present a neuro-symbolic integration method, in which a neural LLM is used to
represent the knowledge of the problem while an LLM-free symbolic solver is
adopted to do deliberative reasoning using the knowledge. Specifically, our
customized meta-interpreters allow the production of reasoning proofs and
support flexible search strategies. These reasoning proofs are ensured to be
causal and reliable because of the deterministic executing nature of the
symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT
baseline by nearly double in accuracy and more than triple in proof similarity.
On GSM8K, our method also shows accuracy improvements and nearly doubled proof
similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09805">DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data. (arXiv:2311.09805v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1">Yitao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongjun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1">Linyong Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lyuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamoi_R/0/1/0/all/0/1">Ryo Kamoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>Recent LLMs have demonstrated remarkable performance in solving exam-like
math word problems. However, the degree to which these numerical reasoning
skills are effective in real-world scenarios, particularly in expert domains,
is still largely unexplored. This paper introduces DocMath-Eval, a
comprehensive benchmark specifically designed to evaluate the numerical
reasoning and problem-solving capabilities of LLMs in the context of
understanding and analyzing financial documents containing both text and
tables. We evaluate a wide spectrum of 19 LLMs, including those specialized in
coding and finance. We also incorporate different prompting strategies (i.e.,
Chain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the
capabilities and limitations of existing LLMs in DocMath-Eval. We found that,
although the current best-performing system (i.e., GPT-4), can perform well on
simple problems such as calculating the rate of increase in a financial metric
within a short document context, it significantly lags behind human experts in
more complex problems grounded in longer contexts. We believe DocMath-Eval can
be used as a valuable benchmark to evaluate LLMs' capabilities to solve
challenging numerical reasoning problems in expert domains. We will release the
benchmark and code at https://github.com/yale-nlp/DocMath-Eval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09807">The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text. (arXiv:2311.09807v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yanzhu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_G/0/1/0/all/0/1">Guokan Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a>, <a href="http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1">Chlo&#xe9; Clavel</a></p>
<p>This study investigates the consequences of training large language models
(LLMs) on synthetic data generated by their predecessors, an increasingly
prevalent practice aimed at addressing the limited supply of human-generated
training data. Diverging from the usual emphasis on performance metrics, we
focus on the impact of this training methodology on linguistic diversity,
especially when conducted recursively over time. To assess this, we developed a
set of novel metrics targeting lexical, syntactic, and semantic diversity,
applying them in recursive fine-tuning experiments across various natural
language generation tasks. Our findings reveal a marked decrease in the
diversity of the models' outputs through successive iterations. This trend
underscores the potential risks of training LLMs on predecessor-generated text,
particularly concerning the preservation of linguistic richness. Our study
highlights the need for careful consideration of the long-term effects of such
training approaches on the linguistic capabilities of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09808">PixT3: Pixel-based Table To Text generation. (arXiv:2311.09808v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1">I&#xf1;igo Alonso</a>, <a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1">Eneko Agirre</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1">Mirella Lapata</a></p>
<p>Table-to-Text has been traditionally approached as a linear language to text
problem. However, visually represented tables are rich in visual information
and serve as a concise, effective form of representing data and its
relationships. When using text-based approaches, after the linearization
process, this information is either lost or represented in a space inefficient
manner. This inefficiency has remained a constant challenge for text-based
approaches making them struggle with large tables. In this paper, we
demonstrate that image representation of tables are more space-efficient than
the typical textual linearizations, and multi-modal approaches are competitive
in Table-to-Text tasks. We present PixT3, a multimodal table-to-text model that
outperforms the state-of-the-art (SotA) in the ToTTo benchmark in a pure
Table-to-Text setting while remaining competitive in controlled Table-to-Text
scenarios. It also generalizes better in unseen datasets, outperforming ToTTo
SotA in all generation settings. Additionally, we introduce a new intermediate
training curriculum to reinforce table structural awareness, leading to
improved generation and overall faithfulness of the models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09812">Large Language Models for Propaganda Span Annotation. (arXiv:2311.09812v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1">Maram Hasanain</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1">Fatema Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1">Firoj Alam</a></p>
<p>The use of propagandistic techniques in online communication has increased in
recent years, aiming to manipulate online audiences. Efforts to automatically
detect and debunk such content have been made, addressing various modeling
scenarios. These include determining whether the content (text, image, or
multimodal) (i) is propagandistic, (ii) employs one or more techniques, and
(iii) includes techniques with identifiable spans. Significant research efforts
have been devoted to the first two scenarios compared to the latter. Therefore,
in this study, we focus on the task of detecting propagandistic textual spans.
We investigate whether large language models such as GPT-4 can be utilized to
perform the task of an annotator. For the experiments, we used an in-house
developed dataset consisting of annotations from multiple annotators. Our
results suggest that providing more information to the model as prompts
improves the annotation agreement and performance compared to human
annotations. We plan to make the annotated labels from multiple annotators,
including GPT-4, available for the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09816">Performance Trade-offs of Watermarking Large Language Models. (arXiv:2311.09816v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1">Anirudh Ajith</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1">Danish Pruthi</a></p>
<p>Amidst growing concerns of large language models (LLMs) being misused for
generating misinformation or completing homework assignments, watermarking has
emerged as an effective solution for distinguishing human-written and
LLM-generated text. A prominent watermarking strategy is to embed a signal into
generated text by upsampling a (pseudorandomly-chosen) subset of tokens at
every generation step. Although this signal is imperceptible to a human reader,
it is detectable through statistical testing. However, implanting such signals
alters the model's output distribution and can have unintended effects when
watermarked LLMs are used for downstream applications. In this work, we
evaluate the performance of watermarked LLMs on a diverse suite of tasks,
including text classification, textual entailment, reasoning, question
answering, translation, summarization, and language modeling. We find that
watermarking has negligible impact on the performance of tasks posed as k-class
classification problems in the average case. However, the accuracy can plummet
to that of a random classifier for some scenarios (that occur with
non-negligible probability). Tasks that are cast as multiple-choice questions
and short-form generation are surprisingly unaffected by watermarking. For
long-form generation tasks, including summarization and translation, we see a
drop of 15-20% in the performance due to watermarking. Our findings highlight
the trade-offs that users should be cognizant of when using watermarked models,
and point to cases where future research could improve existing trade-offs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09818">SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models. (arXiv:2311.09818v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jialiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tjangnaka_W/0/1/0/all/0/1">Wesley Tjangnaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1">Sina J. Semnani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chen Jie Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+David_G/0/1/0/all/0/1">Gui D&#xe1;vid</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1">Monica S. Lam</a></p>
<p>Many knowledge sources consist of both structured information such as
relational databases as well as unstructured free text. Building a
conversational interface to such data sources is challenging.
</p>
<p>This paper introduces SUQL, Structured and Unstructured Query Language, the
first formal executable representation that naturally covers compositions of
structured and unstructured data queries. Specifically, it augments SQL with
several free-text primitives to form a precise, succinct, and expressive
representation. This paper also presents a conversational search agent based on
large language models, including a few-shot contextual semantic parser for
SUQL.
</p>
<p>To validate our approach, we introduce a dataset consisting of crowdsourced
questions and conversations about real restaurants. Over 51% of the questions
in the dataset require both structured and unstructured data, suggesting that
it is a common phenomenon. We show that our few-shot conversational agent based
on SUQL finds an entity satisfying all user requirements 89.3% of the time,
compared to just 65.0% for a strong and commonly used baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09821">Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning. (arXiv:2311.09821v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1">Qingyu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1">Hwee Tou Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1">Lidong Bing</a></p>
<p>Knowledge in the real world is being updated constantly. However, it is
costly to frequently update large language models (LLMs). Therefore, it is
crucial for LLMs to understand the concept of temporal knowledge. However,
prior works on temporal question answering did not emphasize multi-answer and
multi-hop types of temporal reasoning. In this paper, we propose a complex
temporal question-answering (QA) dataset Complex-TR that focuses on
multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel
data augmentation strategy to improve the complex temporal reasoning capability
and robustness of LLMs. We conducted experiments on multiple temporal QA
datasets. Experimental results show that our method is able to improve LLMs'
performance on temporal QA benchmarks by significant margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09825">Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks. (arXiv:2311.09825v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuxuan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1">Bingsheng Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tun Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Toby Jia-Jun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a></p>
<p>Large Language Models (LLMs) have demonstrated considerable advances, and
several claims have been made about their exceeding human performance. However,
in real-world tasks, domain knowledge is often required. Low-resource learning
methods like Active Learning (AL) have been proposed to tackle the cost of
domain expert annotation, raising this question: Can LLMs surpass compact
models trained with expert annotations in domain-specific tasks? In this work,
we conduct an empirical experiment on four datasets from three different
domains comparing SOTA LLMs with small models trained on expert annotations
with AL. We found that small models can outperform GPT-3.5 with a few hundreds
of labeled data, and they achieve higher or similar performance with GPT-4
despite that they are hundreds time smaller. Based on these findings, we posit
that LLM predictions can be used as a warmup method in real-world applications
and human experts remain indispensable in tasks involving data annotation
driven by domain-specific knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09827">Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking. (arXiv:2311.09827v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Ben Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bang Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>While large language models (LLMs) have demonstrated increasing power, they
have also given rise to a wide range of harmful behaviors. As representatives,
jailbreak attacks can provoke harmful or unethical responses from LLMs, even
after safety alignment. In this paper, we investigate a novel category of
jailbreak attacks specifically designed to target the cognitive structure and
processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in
the face of (1) multilingual cognitive overload, (2) veiled expression, and (3)
effect-to-cause reasoning. Different from previous jailbreak attacks, our
proposed cognitive overload is a black-box attack with no need for knowledge of
model architecture or access to model weights. Experiments conducted on
AdvBench and MasterKey reveal that various LLMs, including both popular
open-source model Llama 2 and the proprietary model ChatGPT, can be compromised
through cognitive overload. Motivated by cognitive psychology work on managing
cognitive load, we further investigate defending cognitive overload attack from
two perspectives. Empirical studies show that our cognitive overload from three
perspectives can jailbreak all studied LLMs successfully, while existing
defense strategies can hardly mitigate the caused malicious uses effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09828">AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African Languages. (arXiv:2311.09828v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiayi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1">David Ifeoluwa Adelani</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Sweta Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1">Ricardo Rei</a>, <a href="http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1">Eleftheria Briakou</a>, <a href="http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1">Marine Carpuat</a>, <a href="http://arxiv.org/find/cs/1/au:+Masiak_M/0/1/0/all/0/1">Marek Masiak</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuanli He</a>, <a href="http://arxiv.org/find/cs/1/au:+Bourhim_S/0/1/0/all/0/1">Sofia Bourhim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bukula_A/0/1/0/all/0/1">Andiswa Bukula</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1">Muhidin Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Olatoye_T/0/1/0/all/0/1">Temitayo Olatoye</a>, <a href="http://arxiv.org/find/cs/1/au:+Mokayede_H/0/1/0/all/0/1">Hamam Mokayede</a>, <a href="http://arxiv.org/find/cs/1/au:+Mwase_C/0/1/0/all/0/1">Christine Mwase</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimotho_W/0/1/0/all/0/1">Wangui Kimotho</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuehgoh_F/0/1/0/all/0/1">Foutse Yuehgoh</a>, <a href="http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1">Anuoluwapo Aremu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ojo_J/0/1/0/all/0/1">Jessica Ojo</a>, <a href="http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1">Shamsuddeen Hassan Muhammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1">Salomey Osei</a>, <a href="http://arxiv.org/find/cs/1/au:+Omotayo_A/0/1/0/all/0/1">Abdul-Hakeem Omotayo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1">Chiamaka Chukwuneke</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1">Perez Ogayo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hourrane_O/0/1/0/all/0/1">Oumaima Hourrane</a>, <a href="http://arxiv.org/find/cs/1/au:+Anigri_S/0/1/0/all/0/1">Salma El Anigri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndolela_L/0/1/0/all/0/1">Lolwethu Ndolela</a>, <a href="http://arxiv.org/find/cs/1/au:+Mangwana_T/0/1/0/all/0/1">Thabiso Mangwana</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1">Shafie Abdi Mohamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1">Ayinde Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Awoyomi_O/0/1/0/all/0/1">Oluwabusayo Olufunke Awoyomi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alkhaled_L/0/1/0/all/0/1">Lama Alkhaled</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Azzawi_S/0/1/0/all/0/1">Sana Al-Azzawi</a>, <a href="http://arxiv.org/find/cs/1/au:+Etori_N/0/1/0/all/0/1">Naome A. Etori</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochieng_M/0/1/0/all/0/1">Millicent Ochieng</a>, <a href="http://arxiv.org/find/cs/1/au:+Siro_C/0/1/0/all/0/1">Clemencia Siro</a>, <a href="http://arxiv.org/find/cs/1/au:+Njoroge_S/0/1/0/all/0/1">Samuel Njoroge</a>, <a href="http://arxiv.org/find/cs/1/au:+Muchiri_E/0/1/0/all/0/1">Eric Muchiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimotho_W/0/1/0/all/0/1">Wangari Kimotho</a>, <a href="http://arxiv.org/find/cs/1/au:+Momo_L/0/1/0/all/0/1">Lyse Naomi Wamba Momo</a>, <a href="http://arxiv.org/find/cs/1/au:+Abolade_D/0/1/0/all/0/1">Daud Abolade</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajao_S/0/1/0/all/0/1">Simbiat Ajao</a>, <a href="http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1">Tosin Adewumi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1">Iyanuoluwa Shode</a>, <a href="http://arxiv.org/find/cs/1/au:+Macharm_R/0/1/0/all/0/1">Ricky Macharm</a>, <a href="http://arxiv.org/find/cs/1/au:+Iro_R/0/1/0/all/0/1">Ruqayya Nasir Iro</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1">Saheed S. Abdullahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_S/0/1/0/all/0/1">Stephen E. Moore</a>, et al. (10 additional authors not shown)</p>
<p>Despite the progress we have recorded in scaling multilingual machine
translation (MT) models and evaluation data to several under-resourced African
languages, it is difficult to measure accurately the progress we have made on
these languages because evaluation is often performed on n-gram matching
metrics like BLEU that often have worse correlation with human judgments.
Embedding-based metrics such as COMET correlate better; however, lack of
evaluation data with human ratings for under-resourced languages, complexity of
annotation guidelines like Multidimensional Quality Metrics (MQM), and limited
language coverage of multilingual encoders have hampered their applicability to
African languages. In this paper, we address these challenges by creating
high-quality human evaluation data with a simplified MQM guideline for
error-span annotation and direct assessment (DA) scoring for 13 typologically
diverse African languages. Furthermore, we develop AfriCOMET, a COMET
evaluation metric for African languages by leveraging DA training data from
high-resource languages and African-centric multilingual encoder
(AfroXLM-Roberta) to create the state-of-the-art evaluation metric for African
languages MT with respect to Spearman-rank correlation with human judgments
(+0.406).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09829">FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models. (arXiv:2311.09829v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1">Yimin Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Renren Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jiahao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Huishi Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaohua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1">Deyi Xiong</a></p>
<p>The effective assessment of the instruction-following ability of large
language models (LLMs) is of paramount importance. A model that cannot adhere
to human instructions might be not able to provide reliable and helpful
responses. In pursuit of this goal, various benchmarks have been constructed to
evaluate the instruction-following capacity of these models. However, these
benchmarks are limited to a single language and are constructed using automated
approaches, which restricts their applicability and the quality of the test
examples they contain. To bridge this gap, we introduce the FollowEval
benchmark in this paper. This benchmark is composed of instances in both
English and Chinese, and all test examples are crafted by human experts.
Furthermore, the FollowEval benchmark is designed to assess LLMs across five
critical dimensions of instruction following: string manipulation, commonsense
reasoning, logical reasoning, spatial reasoning, and response constraints. To
enhance the complexity and present a sufficient challenge, each test example is
designed to evaluate more than one dimension. We have evaluated various LLMs
using the FollowEval benchmark and found that their performance significantly
lags behind that of humans. This highlights the considerable room for
improvement in the instruction-following ability of these models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09830">AutoPlanBench: : Automatically generating benchmarks for LLM planners from PDDL. (arXiv:2311.09830v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stein_K/0/1/0/all/0/1">Katharina Stein</a>, <a href="http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1">Alexander Koller</a></p>
<p>LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present a
novel method for automatically converting planning benchmarks written in PDDL
into textual descriptions and offer a benchmark dataset created with our
method. We show that while the best LLM planners do well on many planning
tasks, others remain out of reach of current methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09832">X-Mark: Towards Lossless Watermarking Through Lexical Redundancy. (arXiv:2311.09832v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1">Yatao Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuaiyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Bingzhe Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peilin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1">Kam-fai Wong</a></p>
<p>Text watermarking has emerged as an important technique for detecting
machine-generated text. However, existing methods can severely degrade text
quality due to arbitrary vocabulary partitioning, which disrupts the language
model's expressiveness and impedes textual coherence. To mitigate this, we
introduce XMark, a novel approach that capitalizes on text redundancy within
the lexical space. Specifically, XMark incorporates a mutually exclusive rule
for synonyms during the language model decoding process, thereby integrating
prior knowledge into vocabulary partitioning and preserving the capabilities of
language generation. We present theoretical analyses and empirical evidence
demonstrating that XMark substantially enhances text generation fluency while
maintaining watermark detectability. Furthermore, we investigate watermarking's
impact on the emergent abilities of large language models, including zero-shot
and few-shot knowledge recall, logical reasoning, and instruction following.
Our comprehensive experiments confirm that XMark consistently outperforms
existing methods in retaining these crucial capabilities of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09834">Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens Contributing to Explicit Hate in English by Span Detection. (arXiv:2311.09834v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masud_S/0/1/0/all/0/1">Sarah Masud</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Mohammad Aflah Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1">Md. Shad Akhtar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Tanmoy Chakraborty</a></p>
<p>As hate speech continues to proliferate on the web, it is becoming
increasingly important to develop computational methods to mitigate it.
Reactively, using black-box models to identify hateful content can perplex
users as to why their posts were automatically flagged as hateful. On the other
hand, proactive mitigation can be achieved by suggesting rephrasing before a
post is made public. However, both mitigation techniques require information
about which part of a post contains the hateful aspect, i.e., what spans within
a text are responsible for conveying hate. Better detection of such spans can
significantly reduce explicitly hateful content on the web. To further
contribute to this research area, we organized HateNorm at HASOC-FIRE 2023,
focusing on explicit span detection in English Tweets. A total of 12 teams
participated in the competition, with the highest macro-F1 observed at 0.58.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09835">ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks. (arXiv:2311.09835v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zefan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Junjie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1">Yanjun Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zexuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Helan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zengxian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1">Kaikai An</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Ruijun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Shuzheng Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haozhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1">Yiming Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhiwei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1">Baobao Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yujia Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wangchunshu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a></p>
<p>Large language models have shown promising performance in code generation
benchmarks. However, a considerable divide exists between these benchmark
achievements and their practical applicability, primarily attributed to
real-world programming's reliance on pre-existing libraries. Instead of
evaluating LLMs to code from scratch, this work aims to propose a new
evaluation setup where LLMs use open-source libraries to finish machine
learning tasks. Therefore, we propose ML-Bench, an expansive benchmark
developed to assess the effectiveness of LLMs in leveraging existing functions
in open-source libraries. Consisting of 10044 samples spanning 130 tasks over
14 notable machine learning GitHub repositories. In this setting, given a
specific machine learning task instruction and the accompanying README in a
codebase, an LLM is tasked to generate code to accomplish the task. This
necessitates the comprehension of long and language-code interleaved documents,
as well as the understanding of complex cross-file code structures, introducing
new challenges. Notably, while GPT-4 exhibits remarkable improvement over other
LLMs, it manages to accomplish only 39.73\% of the tasks, leaving a huge space
for improvement. We address these challenges by proposing ML-Agent, designed to
effectively navigate the codebase, locate documentation, retrieve code, and
generate executable code. Empirical results demonstrate that ML-Agent, built
upon GPT-4, results in further improvements. Code, data, and models are
available at \url{https://ml-bench.github.io/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09836">PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization. (arXiv:2311.09836v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peper_J/0/1/0/all/0/1">Joseph J. Peper</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Wenzhao Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a></p>
<p>We investigate pre-training techniques for abstractive multi-document
summarization (MDS), which is much less studied than summarizing single
documents. Though recent work has demonstrated the effectiveness of
highlighting information salience for pre-training strategy design, it
struggles to generate abstractive and reflective summaries, which are critical
properties for MDS. To this end, we present PELMS, a pre-trained model that
uses objectives based on semantic coherence heuristics and faithfulness
constraints with un-labeled multi-document inputs, to promote the generation of
concise, fluent, and faithful summaries. To support the training of PELMS, we
compile MultiPT, a multi-document pre-training corpus containing over 93
million documents to form more than 3 million unlabeled topic-centric document
clusters, covering diverse genres such as product reviews, news, and general
knowledge. We perform extensive evaluation of PELMS in low-shot settings on a
wide range of MDS datasets. Our approach consistently outperforms competitive
comparisons with respect to overall informativeness, abstractiveness,
coherence, and faithfulness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09841">Leveraging LLMs in Scholarly Knowledge Graph Question Answering. (arXiv:2311.09841v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taffa_T/0/1/0/all/0/1">Tilahun Abedissa Taffa</a>, <a href="http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1">Ricardo Usbeck</a></p>
<p>This paper presents a scholarly Knowledge Graph Question Answering (KGQA)
that answers bibliographic natural language questions by leveraging a large
language model (LLM) in a few-shot manner. The model initially identifies the
top-n similar training questions related to a given test question via a
BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the
top-n similar question-SPARQL pairs as an example and the test question creates
a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs
the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and
returns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of
the Scholarly-QALD-23 challenge benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09860">GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets. (arXiv:2311.09860v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Otto_W/0/1/0/all/0/1">Wolfgang Otto</a>, <a href="http://arxiv.org/find/cs/1/au:+Zloch_M/0/1/0/all/0/1">Matth&#xe4;us Zloch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1">Lu Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1">Saurav Karmakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1">Stefan Dietze</a></p>
<p>Named Entity Recognition (NER) models play a crucial role in various NLP
tasks, including information extraction (IE) and text understanding. In
academic writing, references to machine learning models and datasets are
fundamental components of various computer science publications and necessitate
accurate models for identification. Despite the advancements in NER, existing
ground truth datasets do not treat fine-grained types like ML model and model
architecture as separate entity types, and consequently, baseline models cannot
recognize them as such. In this paper, we release a corpus of 100 manually
annotated full-text scientific publications and a first baseline model for 10
entity types centered around ML models and datasets. In order to provide a
nuanced understanding of how ML models and datasets are mentioned and utilized,
our dataset also contains annotations for informal mentions like "our
BERT-based model" or "an image CNN". You can find the ground truth dataset and
code to replicate model training at https://data.gesis.org/gsap/gsap-ner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09861">PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models. (arXiv:2311.09861v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junlei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hongliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_N/0/1/0/all/0/1">Nirui Song</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shuyuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_%5C/0/1/0/all/0/1">\\Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Huachuan Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Anqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lizhi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a></p>
<p>As Large Language Models (LLMs) are becoming prevalent in various fields,
there is an urgent need for improved NLP benchmarks that encompass all the
necessary knowledge of individual discipline. Many contemporary benchmarks for
foundational models emphasize a broad range of subjects but often fall short in
presenting all the critical subjects and encompassing necessary professional
knowledge of them. This shortfall has led to skewed results, given that LLMs
exhibit varying performance across different subjects and knowledge areas. To
address this issue, we present psybench, the first comprehensive Chinese
evaluation suite that covers all the necessary knowledge required for graduate
entrance exams. psybench offers a deep evaluation of a model's strengths and
weaknesses in psychology through multiple-choice questions. Our findings show
significant differences in performance across different sections of a subject,
highlighting the risk of skewed results when the knowledge in test sets is not
balanced. Notably, only the ChatGPT model reaches an average accuracy above
$70\%$, indicating that there is still plenty of room for improvement. We
expect that psybench will help to conduct thorough evaluations of base models'
strengths and weaknesses and assist in practical application in the field of
psychology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09862">Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models. (arXiv:2311.09862v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Debarati Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_I/0/1/0/all/0/1">Ishaan Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_J/0/1/0/all/0/1">Jaideep Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1">Dongyeop Kang</a></p>
<p>Large language models (LLMs) are revolutionizing various fields by leveraging
large text corpora for context-aware intelligence. Due to the context size,
however, encoding an entire graph with LLMs is fundamentally limited. This
paper explores how to better integrate graph data with LLMs and presents a
novel approach using various encoding modalities (e.g., text, image, and motif)
and approximation of global connectivity of a graph using different prompting
methods to enhance LLMs' effectiveness in handling complex graph structures.
The study also introduces GraphTMI, a new benchmark for evaluating LLMs in
graph structure analysis, focusing on factors such as homophily, motif
presence, and graph difficulty. Key findings reveal that image modality,
supported by advanced vision-language models like GPT-4V, is more effective
than text in managing token limits while retaining critical information. The
research also examines the influence of different factors on each encoding
modality's performance. This study highlights the current limitations and
charts future directions for LLMs in graph understanding and reasoning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09889">Language Generation from Human Brain Activities. (arXiv:2311.09889v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Ziyi Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1">Qingyao Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiqun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lioma_C/0/1/0/all/0/1">Christina Lioma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruotsalo_T/0/1/0/all/0/1">Tuukka Ruotsalo</a></p>
<p>Generating human language through non-invasive brain-computer interfaces
(BCIs) has the potential to unlock many applications, such as serving disabled
patients and improving communication. Currently, however, generating language
via BCIs has been previously successful only within a classification setup for
selecting pre-generated sentence continuation candidates with the most likely
cortical semantic representation. Inspired by recent research that revealed
associations between the brain and the large computational language models, we
propose a generative language BCI that utilizes the capacity of a large
language model (LLM) jointly with a semantic brain decoder to directly generate
language from functional magnetic resonance imaging (fMRI) input. The proposed
model can generate coherent language sequences aligned with the semantic
content of visual or auditory language stimuli perceived, without prior
knowledge of any pre-generated candidates. We compare the language generated
from the presented model with a random control, pre-generated language
selection approach, and a standard LLM, which generates common coherent text
solely based on the next word likelihood according to statistical language
training data. The proposed model is found to generate language that is more
aligned with semantic stimulus in response to which brain input is sampled. Our
findings demonstrate the potential and feasibility of employing BCIs in direct
language generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09945">An Attention-Based Denoising Framework for Personality Detection in Social Media Texts. (arXiv:2311.09945v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1">Qirui Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wenkang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yihua Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lei Lin</a></p>
<p>In social media networks, users produce a large amount of text content
anytime, providing researchers with a valuable approach to digging for
personality-related information. Personality detection based on user-generated
texts is a universal method that can be used to build user portraits. The
presence of noise in social media texts hinders personality detection. However,
previous studies have not fully addressed this challenge. Inspired by the
scanning reading technique, we propose an attention-based information
extraction mechanism (AIEM) for long texts, which is applied to quickly locate
valuable pieces of information, and focus more attention on the deep semantics
of key pieces. Then, we provide a novel attention-based denoising framework
(ADF) for personality detection tasks and achieve state-of-the-art performance
on two commonly used datasets. Notably, we obtain an average accuracy
improvement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator
(Twitter-MBTI) dataset. We made our code publicly available on GitHub. We shed
light on how AIEM works to magnify personality-related signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09948">Hijacking Large Language Models via Adversarial In-Context Learning. (arXiv:2311.09948v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiang_Y/0/1/0/all/0/1">Yao Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xiangyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1">Dongxiao Zhu</a></p>
<p>In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs
for specific tasks by utilizing labeled examples as demonstrations in the
precondition prompts. Despite its promising performance, ICL suffers from
instability with the choice and arrangement of examples. Additionally, crafted
adversarial attacks pose a notable threat to the robustness of ICL. However,
existing attacks are either easy to detect, rely on external models, or lack
specificity towards ICL. To address these issues, this work introduces a novel
transferable attack for ICL, aiming to hijack LLMs to generate the targeted
response. The proposed LLM hijacking attack leverages a gradient-based prompt
search method to learn and append imperceptible adversarial suffixes to the
in-context demonstrations. Extensive experimental results on various tasks and
datasets demonstrate the effectiveness of our LLM hijacking attack, resulting
in a distracted attention towards adversarial tokens, consequently leading to
the targeted unwanted outputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09979">Unambiguity and Fewness for Nonuniform Families of Polynomial-Size Nondeterministic Finite Automata. (arXiv:2311.09979v1 [cs.FL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamakami_T/0/1/0/all/0/1">Tomoyuki Yamakami</a></p>
<p>Nonuniform families of polynomial-size finite automata, which are series of
indexed finite automata having polynomially many inner states, are used in the
past literature to solve nonuniform families of promise decision problems.
Among such nonuniform families of finite automata, we focus our attention, in
particular, on the variants of nondeterministic finite automata, which have at
most "one" (unambiguous), "polynomially many" (few) accepting computation
paths, or unambiguous/few computation paths leading to each fixed
configuration. When such machines are limited to make only one-way head moves,
we can prove with no unproven hardness assumptions that some of these variants
are different in computational power from each other. As for two-way machines
restricted to instances of polynomially-bounded length, families of two-way
polynomial-size nondeterministic finite automata are equivalent in power to
families of polynomial-size unambiguous finite automata.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09993">Generative AI for Hate Speech Detection: Evaluation and Findings. (arXiv:2311.09993v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pendzel_S/0/1/0/all/0/1">Sagi Pendzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1">Tomer Wullach</a>, <a href="http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1">Amir Adler</a>, <a href="http://arxiv.org/find/cs/1/au:+Minkov_E/0/1/0/all/0/1">Einat Minkov</a></p>
<p>Automatic hate speech detection using deep neural models is hampered by the
scarcity of labeled datasets, leading to poor generalization. To mitigate this
problem, generative AI has been utilized to generate large amounts of synthetic
hate speech sequences from available labeled examples, leveraging the generated
data in finetuning large pre-trained language models (LLMs). In this chapter,
we provide a review of relevant methods, experimental setups and evaluation of
this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,
we apply and evaluate the impact of train set augmentation with generated data
using LLMs that have been already adapted for hate detection, including
RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical
study corroborates our previous findings, showing that this approach improves
hate speech generalization, boosting recall performance across data
distributions. In addition, we explore and compare the performance of the
finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results
demonstrate that while better generalization is achieved using the GPT-3.5
model, it achieves mediocre recall and low precision on most datasets. It is an
open question whether the sensitivity of models such as GPT-3.5, and onward,
can be improved using similar techniques of text generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.08955">Making first order linear logic a generating grammar. (arXiv:2206.08955v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1">Sergey Slavnov</a></p>
<p>It is known that different categorial grammars have surface representation in
a fragment of first order multiplicative linear logic (MLL1). We show that the
fragment of interest is equivalent to the recently introduced extended tensor
type calculus (ETTC). ETTC is a calculus of specific typed terms, which
represent tuples of strings, more precisely bipartite graphs decorated with
strings. Types are derived from linear logic formulas, and rules correspond to
concrete operations on these string-labeled graphs, so that they can be
conveniently visualized. This provides the above mentioned fragment of MLL1
that is relevant for language modeling not only with some alternative syntax
and intuitive geometric representation, but also with an intrinsic deductive
system, which has been absent.
</p>
<p>In this work we consider a non-trivial notationally enriched variation of the
previously introduced ETTC, which allows more concise and transparent
computations. We present both a cut-free sequent calculus and a natural
deduction formalism.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14838">EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v3 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Angelica Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1">David M. Dohan</a>, <a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1">David R. So</a></p>
<p>Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03290">AmQA: Amharic Question Answering Dataset. (arXiv:2303.03290v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abedissa_T/0/1/0/all/0/1">Tilahun Abedissa</a>, <a href="http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1">Ricardo Usbeck</a>, <a href="http://arxiv.org/find/cs/1/au:+Assabie_Y/0/1/0/all/0/1">Yaregal Assabie</a></p>
<p>Question Answering (QA) returns concise answers or answer lists from natural
language text given a context document. Many resources go into curating QA
datasets to advance robust models' development. There is a surge of QA datasets
for languages like English, however, this is not true for Amharic. Amharic, the
official language of Ethiopia, is the second most spoken Semitic language in
the world. There is no published or publicly available Amharic QA dataset.
Hence, to foster the research in Amharic QA, we present the first Amharic QA
(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia
articles. Additionally, we run an XLMR Large-based baseline model to spark
open-domain QA research interest. The best-performing baseline achieves an
F-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension
settings respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03608">Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation. (arXiv:2303.03608v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1">Alexander R. Fabbri</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yilun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chien-Sheng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1">Dragomir Radev</a></p>
<p>Interpretability and efficiency are two important considerations for the
adoption of neural automatic metrics. In this work, we develop
strong-performing automatic metrics for reference-based summarization
evaluation, based on a two-stage evaluation pipeline that first extracts basic
information units from one text sequence and then checks the extracted units in
another sequence. The metrics we developed include two-stage metrics that can
provide high interpretability at both the fine-grained unit level and summary
level, and one-stage metrics that achieve a balance between efficiency and
interpretability. We make the developed tools publicly available at
https://github.com/Yale-LILY/AutoACU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.07438">Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Honghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1">Meihua Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1">Nanyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a></p>
<p>Despite the success of autoregressive large language models in text
generation, it remains a major challenge to generate text that satisfies
complex constraints: sampling from the conditional distribution
${\Pr}(\text{text} | \alpha)$ is intractable for even the simplest lexical
constraints $\alpha$. To overcome this challenge, we propose to use tractable
probabilistic models (TPMs) to impose lexical constraints in autoregressive
text generation models, which we refer to as GeLaTo (Generating Language with
Tractable Constraints). To demonstrate the effectiveness of this framework, we
use distilled hidden Markov models, where we can efficiently compute
${\Pr}(\text{text} | \alpha)$, to guide autoregressive generation from GPT2.
GeLaTo achieves state-of-the-art performance on challenging benchmarks for
constrained text generation (e.g., CommonGen), beating various strong baselines
by a large margin. Our work not only opens up new avenues for controlling large
language models but also motivates the development of more expressive TPMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02547">PersonaLLM: Investigating the Ability of Large Language Models to Express Big Five Personality Traits. (arXiv:2305.02547v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiajie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xubo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabbara_J/0/1/0/all/0/1">Jad Kabbara</a></p>
<p>Despite the many use cases for large language models (LLMs) in creating
personalized chatbots, there has been limited research on evaluating the extent
to which the behaviors of personalized LLMs accurately and consistently reflect
specific personality traits. We consider studying the behavior of LLM-based
agents, referred to as LLM personas, and present a case study with ChatGPT and
GPT-4. The study investigates whether LLMs can generate content that aligns
with their assigned personality profiles. To this end, we create distinct LLM
personas based on the Big Five personality model, have them complete the
44-item Big Five Inventory (BFI) personality test and a story writing task, and
then assess their essays with automatic and human evaluations. Results show
that LLM personas' self-reported BFI scores are consistent with their
designated personality types, with large effect sizes observed across five
traits. Additionally, there are significant correlations between the assigned
personality types and certain psycholinguistic features of their writings, as
measured by the Linguistic Inquiry and Word Count (LIWC) tool. Interestingly,
human evaluators perceive the stories as less personal when told that the
stories are authored by AI. However, their judgments on other aspects of the
writing such as readability, cohesiveness, redundancy, likeability, and
believability remain largely unaffected. Notably, when evaluators were informed
about the AI authorship, their accuracy in identifying the intended personality
traits from the stories decreased by more than 10% for some traits. This
research marks a significant step forward in understanding the capabilities of
LLMs to express personality traits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06626">When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Abebe_R/0/1/0/all/0/1">Rediet Abebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>Though majority vote among annotators is typically used for ground truth
labels in natural language processing, annotator disagreement in tasks such as
hate speech detection may reflect differences in opinion across groups, not
noise. Thus, a crucial problem in hate speech detection is determining whether
a statement is offensive to the demographic group that it targets, when that
group may constitute a small fraction of the annotator pool. We construct a
model that predicts individual annotator ratings on potentially offensive text
and combines this information with the predicted target group of the text to
model the opinions of target group members. We show gains across a range of
metrics, including raising performance over the baseline by 22% at predicting
individual annotators' ratings and by 33% at predicting variance among
annotators, which provides a metric for model uncertainty downstream. We find
that annotator ratings can be predicted using their demographic information and
opinions on online content, without the need to track identifying annotator IDs
that link each annotator to their ratings. We also find that use of
non-invasive survey questions on annotators' online experiences helps to
maximize privacy and minimize unnecessary collection of demographic information
when predicting annotators' opinions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11860">Let&#x27;s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs. (arXiv:2305.11860v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1">Pranjal Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1">Aman Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yiming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a></p>
<p>A popular approach for improving the correctness of output from large
language models (LLMs) is Self-Consistency - poll the LLM multiple times and
output the most frequent solution. Existing Self-Consistency techniques always
generate a constant number of samples per question, where a better approach
will be to non-uniformly distribute the available budget based on the amount of
agreement in the samples generated so far. In response, we introduce
Adaptive-Consistency, a cost-efficient, model-agnostic technique that
dynamically adjusts the number of samples per question using a lightweight
stopping criterion. Our experiments over 17 reasoning and code generation
datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample
budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our
code and data are available at https://www.sample-step-by-step.info
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14239">On Learning to Summarize with Large Language Models as References. (arXiv:2305.14239v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1">Kejian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1">Katherine S He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1">Longtian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1">Alexander R. Fabbri</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1">Dragomir Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>Recent studies have found that summaries generated by large language models
(LLMs) are favored by human annotators over the original reference summaries in
commonly used summarization datasets. Therefore, we investigate a new learning
setting of text summarization models that considers the LLMs as the reference
or the gold-standard oracle on these datasets. To examine the standard
practices that are aligned with this new learning setting, we investigate two
LLM-based summary quality evaluation methods for model training and adopt a
contrastive learning training method to leverage the LLM-guided learning
signals. Our experiments on the CNN/DailyMail and XSum datasets demonstrate
that smaller summarization models can achieve similar performance as LLMs under
LLM-based evaluation. However, we found that the smaller models can not yet
reach LLM-level performance under human evaluation despite promising
improvements brought by our proposed training methods. Meanwhile, we perform a
meta-analysis on this new learning setting that reveals a discrepancy between
human and LLM-based evaluation, highlighting the benefits and risks of this
LLM-as-reference setting we investigated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14456">Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1">Tarek Naous</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryan_M/0/1/0/all/0/1">Michael J. Ryan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1">Alan Ritter</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a></p>
<p>It is important that language models appropriately adapt to specific cultural
contexts. However, as we show in this paper, multilingual and Arabic
monolingual language models default to Western culture even when prompted in
Arabic and contextualized by an Arab cultural setting. To measure this Western
bias, we introduce CAMeL, a dataset of naturally occurring Arabic prompts
spanning eight diverse cultural aspects and an extensive list of 20,504
cultural targets corresponding to Arab or Western culture. Using CAMeL, we show
that models favor Western targets and demonstrate cultural unfairness on
downstream tasks such as named entity recognition and sentiment analysis. Our
analyses of pretraining corpora also reveal that commonly used sources such as
Wikipedia may not be suited to build culturally aware models, underscoring the
importance of carefully curating pretraining data in constructing language
models to serve a global population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14806">AWESOME: GPU Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content. (arXiv:2305.14806v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Shuyang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a></p>
<p>Long document summarization systems are critical for domains with lengthy and
jargonladen text, yet they present significant challenges to researchers and
developers with limited computing resources. Existing solutions mainly focus on
efficient attentions or divide-and-conquer strategies. The former reduces
theoretical time complexity, but is still memory-heavy. The latter methods
sacrifice global context, leading to uninformative and incoherent summaries.
This work aims to leverage the memory-efficient nature of divide-and-conquer
methods while preserving global context. Concretely, our framework AWESOME uses
two novel mechanisms: (1) External memory mechanisms track previously encoded
document segments and their corresponding summaries, to enhance global document
understanding and summary coherence. (2) Global salient content is further
identified beforehand to augment each document segment to support its
summarization. Extensive experiments on diverse genres of text, including
government reports, transcripts, scientific papers, and novels, show that
AWESOME produces summaries with improved informativeness, faithfulness, and
coherence than competitive baselines on longer documents, while having a
smaller GPU memory footprint.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14824">Mitigating Temporal Misalignment by Discarding Outdated Facts. (arXiv:2305.14824v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Michael J.Q. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1">Eunsol Choi</a></p>
<p>While large language models are able to retain vast amounts of world
knowledge seen during pretraining, such knowledge is prone to going out of date
and is nontrivial to update. Furthermore, these models are often used under
temporal misalignment, tasked with answering questions about the present,
despite having only been trained on data collected in the past. To mitigate the
effects of temporal misalignment, we propose fact duration prediction: the task
of predicting how long a given fact will remain true. In our experiments, we
demonstrate that identifying which facts are prone to rapid change can help
models avoid reciting outdated information and determine which predictions
require seeking out up-to-date knowledge sources. We also show how modeling
fact duration improves calibration for knowledge-intensive tasks, such as
open-retrieval question answering, under temporal misalignment, by discarding
volatile facts. Our data and code are released publicly at
https://github.com/mikejqzhang/mitigating_misalignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17718">FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions. (arXiv:2305.17718v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rotstein_N/0/1/0/all/0/1">Noam Rotstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Bensaid_D/0/1/0/all/0/1">David Bensaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Brody_S/0/1/0/all/0/1">Shaked Brody</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1">Roy Ganz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1">Ron Kimmel</a></p>
<p>The advent of vision-language pre-training techniques enhanced substantial
progress in the development of models for image captioning. However, these
models frequently produce generic captions and may omit semantically important
image details. This limitation can be traced back to the image-text datasets;
while their captions typically offer a general description of image content,
they frequently omit salient details. Considering the magnitude of these
datasets, manual reannotation is impractical, emphasizing the need for an
automated approach. To address this challenge, we leverage existing captions
and explore augmenting them with visual details using "frozen" vision experts
including an object detector, an attribute recognizer, and an Optical Character
Recognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such
vision experts with the original captions using a large language model (LLM),
yielding comprehensive image descriptions. We automatically curate a training
set of 12M image-enriched caption pairs. These pairs undergo extensive
evaluation through both quantitative and qualitative analyses. Subsequently,
this data is utilized to train a captioning generation BLIP-based model. This
model outperforms current state-of-the-art approaches, producing more precise
and detailed descriptions, demonstrating the effectiveness of the proposed
data-centric approach. We release this large-scale dataset of enriched
image-caption pairs for the community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18787">Universality and Limitations of Prompt Tuning. (arXiv:2305.18787v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1">Jatin Chauhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a></p>
<p>Despite the demonstrated empirical efficacy of prompt tuning to adapt a
pretrained language model for a new task, the theoretical underpinnings of the
difference between "tuning parameters before the input" against "the tuning of
model weights" are limited. We thus take one of the first steps to understand
the role of soft-prompt tuning for transformer-based architectures. By
considering a general purpose architecture, we analyze prompt tuning from the
lens of both: universal approximation and limitations with finite-depth
fixed-weight pretrained transformers for continuous-valued functions. Our
universality result guarantees the existence of a strong transformer with a
prompt to approximate any sequence-to-sequence function in the set of Lipschitz
functions. The limitations of prompt tuning for limited-depth transformers are
first proved by constructing a set of datasets, that cannot be memorized by a
prompt of any length for a given single encoder layer. We also provide a lower
bound on the required number of tunable prompt parameters and compare the
result with the number of parameters required for a low-rank update (based on
LoRA) for a single-layer setting. We finally extend our analysis to multi-layer
settings by providing sufficient conditions under which the transformer can at
best learn datasets from invertible functions only. Our theoretical claims are
also corroborated by empirical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15518">Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool. (arXiv:2306.15518v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1">Jingwei Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1">Julia Bingler</a>, <a href="http://arxiv.org/find/cs/1/au:+Colesanti_Senni_C/0/1/0/all/0/1">Chiara Colesanti-Senni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1">Mathias Kraus</a>, <a href="http://arxiv.org/find/cs/1/au:+Gostlow_G/0/1/0/all/0/1">Glen Gostlow</a>, <a href="http://arxiv.org/find/cs/1/au:+Schimanski_T/0/1/0/all/0/1">Tobias Schimanski</a>, <a href="http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1">Dominik Stammbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaghefi_S/0/1/0/all/0/1">Saeid Ashraf Vaghefi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1">Nicolas Webersinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Wekhof_T/0/1/0/all/0/1">Tobias Wekhof</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tingyu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1">Markus Leippold</a></p>
<p>This paper introduces a novel approach to enhance Large Language Models
(LLMs) with expert knowledge to automate the analysis of corporate
sustainability reports by benchmarking them against the Task Force for
Climate-Related Financial Disclosures (TCFD) recommendations. Corporate
sustainability reports are crucial in assessing organizations' environmental
and social risks and impacts. However, analyzing these reports' vast amounts of
information makes human analysis often too costly. As a result, only a few
entities worldwide have the resources to analyze these reports, which could
lead to a lack of transparency. While AI-powered tools can automatically
analyze the data, they are prone to inaccuracies as they lack domain-specific
expertise. This paper introduces a novel approach to enhance LLMs with expert
knowledge to automate the analysis of corporate sustainability reports. We
christen our tool CHATREPORT, and apply it in a first use case to assess
corporate climate risk disclosures following the TCFD recommendations.
CHATREPORT results from collaborating with experts in climate science, finance,
economic policy, and computer science, demonstrating how domain experts can be
involved in developing AI tools. We make our prompt templates, generated data,
and scores available to the public to encourage transparency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00002">An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wenzel_G/0/1/0/all/0/1">Georg Wenzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1">Adam Jatowt</a></p>
<p>Temporal commonsense reasoning refers to the ability to understand the
typical temporal context of phrases, actions, and events, and use it to reason
over problems requiring such knowledge. This trait is essential in temporal
natural language processing tasks, with possible applications such as timeline
summarization, temporal question answering, and temporal natural language
inference. Recent research on the performance of large language models suggests
that, although they are adept at generating syntactically correct sentences and
solving classification tasks, they often take shortcuts in their reasoning and
fall prey to simple linguistic traps. This article provides an overview of
research in the domain of temporal commonsense reasoning, particularly focusing
on enhancing language model performance through a variety of augmentations and
their evaluation across a growing number of datasets. However, these augmented
models still struggle to approach human performance on reasoning tasks over
temporal common sense properties, such as the typical occurrence times,
orderings, or durations of events. We further emphasize the need for careful
interpretation of research to guard against overpromising evaluation results in
light of the shallow reasoning present in transformers. This can be achieved by
appropriately preparing datasets and suitable evaluation metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02463">Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&amp;3D Medical Data. (arXiv:2308.02463v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chaoyi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoman Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weidi Xie</a></p>
<p>In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM. We consider the construction of foundational models
from three perspectives, namely, dataset construction, model design, and
thorough evaluation. Our contribution can be concluded as follows: (i), we
construct a large-scale Medical Multi-modal Dataset, MedMD, which consists of
16M 2D and 3D medical scans with high-quality text descriptions or reports
across various data formats, modalities, and tasks, covering over 5000 distinct
diseases. To the best of our knowledge, this is the first large-scale,
high-quality, medical visual-language dataset, with both 2D and 3D scans; (ii),
we propose an architecture that enables visually conditioned generative
pre-training, i.e., allowing for integration of text input with 2D or 3D
medical scans, and generate responses for diverse radiologic tasks. The model
was initially pre-trained on MedMD and subsequently fine-tuned on the
domain-specific dataset, which is a radiologic cleaned version of MedMD,
containing 3M radiologic visual-language pairs, termed as RadMD; (iii), we
propose a new evaluation benchmark, RadBench, that comprises five tasks,
including modality recognition, disease diagnosis, visual question answering,
report generation and rationale diagnosis, aiming to comprehensively assess the
capability of foundation models in handling practical clinical problems. We
conduct both automatic and human evaluation on RadBench, in both cases, RadFM
outperforms existing multi-modal foundation models, that are publicaly
accessible, including Openflamingo, MedFlamingo, MedVInT and GPT-4V.
Additionally, we also adapt RadFM for different public benchmarks, surpassing
existing SOTAs on diverse datasets. All codes, data, and model checkpoint will
all be made publicly available to promote further research and development in
the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03656">Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench. (arXiv:2308.03656v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jen-tse Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1">Man Ho Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1">Eric John Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1">Shujie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1">Wenxiang Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1">Michael R. Lyu</a></p>
<p>Recently, the community has witnessed the advancement of Large Language
Models (LLMs), which have shown remarkable performance on various downstream
tasks. Led by powerful models like ChatGPT and Claude, LLMs are revolutionizing
how users engage with software, assuming more than mere tools but intelligent
assistants. Consequently, evaluating LLMs' anthropomorphic capabilities becomes
increasingly important in contemporary discourse. Utilizing the emotion
appraisal theory from psychology, we propose to evaluate the empathy ability of
LLMs, i.e., how their feelings change when presented with specific situations.
After a careful and comprehensive survey, we collect a dataset containing over
400 situations that have proven effective in eliciting the eight emotions
central to our study. Categorizing the situations into 36 factors, we conduct a
human evaluation involving more than 1,200 subjects worldwide. With the human
evaluation results as references, our evaluation includes five LLMs, covering
both commercial and open-source models, including variations in model sizes,
featuring the latest iterations, such as GPT-4 and LLaMA 2. A conclusion can be
drawn from the results that, despite several misalignments, LLMs can generally
respond appropriately to certain situations. Nevertheless, they fall short in
alignment with the emotional behaviors of human beings and cannot establish
connections between similar situations. Our collected dataset of situations,
the human evaluation results, and the code of our testing framework, dubbed
EmotionBench, is made publicly in https://github.com/CUHK-ARISE/EmotionBench.
We aspire to contribute to the advancement of LLMs regarding better alignment
with the emotional behaviors of human beings, thereby enhancing their utility
and applicability as intelligent assistants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12770">WavMark: Watermarking for Audio Generation. (arXiv:2308.12770v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guangyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xiaoyong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>Recent breakthroughs in zero-shot voice synthesis have enabled imitating a
speaker's voice using just a few seconds of recording while maintaining a high
level of realism. Alongside its potential benefits, this powerful technology
introduces notable risks, including voice fraud and speaker impersonation.
Unlike the conventional approach of solely relying on passive methods for
detecting synthetic data, watermarking presents a proactive and robust defence
mechanism against these looming risks. This paper introduces an innovative
audio watermarking framework that encodes up to 32 bits of watermark within a
mere 1-second audio snippet. The watermark is imperceptible to human senses and
exhibits strong resilience against various attacks. It can serve as an
effective identifier for synthesized voices and holds potential for broader
applications in audio copyright protection. Moreover, this framework boasts
high flexibility, allowing for the combination of multiple watermark segments
to achieve heightened robustness and expanded capacity. Utilizing 10 to
20-second audio as the host, our approach demonstrates an average Bit Error
Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over
2800\% in BER compared to the state-of-the-art watermarking tool. See
https://aka.ms/wavmark for demos of our work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16137">LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chi Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Wenhan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sinong Wang</a></p>
<p>In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex domains, they often face the
need to follow longer user prompts or generate longer texts. In these
situations, the $\textit{length generalization failure}$ of LLMs on long
sequences becomes more prominent. Most pre-training schemes truncate training
sequences to a fixed length. LLMs often struggle to generate fluent and
coherent texts after longer contexts, even with relative positional encoding
specifically designed to cope with this problem. Common solutions such as
finetuning on longer corpora often involve daunting hardware and time costs and
require careful training process design. To more efficiently extrapolate
existing LLMs' generation quality to longer texts, we theoretically and
empirically investigate the main out-of-distribution (OOD) factors contributing
to this problem. Inspired by this diagnosis, we propose a simple yet effective
solution for on-the-fly length generalization, LM-Infinite. It involves only a
$\mathbf{\Lambda}$-shaped attention mask (to avoid excessive attended tokens)
and a distance limit (to avoid unseen distances) while requiring no parameter
updates or learning. We find it applicable to a variety of LLMs using
relative-position encoding methods. LM-Infinite is computationally efficient
with $O(n)$ time and space, and demonstrates consistent text generation fluency
and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with
2.72x decoding speedup. We will make the codes publicly available following
publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10923">Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Foppiano_L/0/1/0/all/0/1">Luca Foppiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Mato_T/0/1/0/all/0/1">Tomoya Mato</a>, <a href="http://arxiv.org/find/cs/1/au:+Terashima_K/0/1/0/all/0/1">Kensei Terashima</a>, <a href="http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1">Pedro Ortiz Suarez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tou_T/0/1/0/all/0/1">Taku Tou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakai_C/0/1/0/all/0/1">Chikako Sakai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei-Sheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Amagasa_T/0/1/0/all/0/1">Toshiyuki Amagasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Takano_Y/0/1/0/all/0/1">Yoshihiko Takano</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1">Masashi Ishii</a></p>
<p>We propose a semi-automatic staging area for efficiently building an accurate
database of experimental physical properties of superconductors from
literature, called SuperCon2, to enrich the existing manually-built
superconductor database SuperCon. Here we report our curation interface
(SuperCon2 Interface) and a workflow managing the state transitions of each
examined record, to validate the dataset of superconductors from PDF documents
collected using Grobid-superconductors in a previous work. This curation
workflow allows both automatic and manual operations, the former contains
``anomaly detection'' that scans new data identifying outliers, and a
``training data collector'' mechanism that collects training data examples
based on manual corrections. Such training data collection policy is effective
in improving the machine-learning models with a reduced number of examples. For
manual operations, the interface (SuperCon2 interface) is developed to increase
efficiency during manual correction by providing a smart interface and an
enhanced PDF document viewer. We show that our interface significantly improves
the curation quality by boosting precision and recall as compared with the
traditional ``manual correction''. Our semi-automatic approach would provide a
solution for achieving a reliable database with text-data mining of scientific
documents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09917">Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation. (arXiv:2310.09917v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1">Nadezhda Chirkova</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Sheng Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1">Vassilina Nikoulina</a></p>
<p>Zero-shot cross-lingual generation assumes finetuning the multilingual
pretrained language model (mPLM) on a generation task in one language and then
using it to make predictions for this task in other languages. Previous works
notice a frequent problem of generation in a wrong language and propose
approaches to address it, usually using mT5 as a backbone model. In this work,
we test alternative mPLMs, such as mBART and NLLB-200, and compare various
approaches proposed in the literature in a unified setting. We first underline
the importance of tuning learning rate used for finetuning, which helps to
substantially alleviate the problem of generation in the wrong language. Then,
we show that with careful learning rate tuning, the simple full finetuning of
the model acts as a very strong baseline; other competitive approaches include
parameter-efficient tuning with adapters and training on several source
languages. Finally, we find that mBART performs similarly to mT5 of the same
size, and NLLB-200 can be competitive in some cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10865">Will the Prince Get True Love&#x27;s Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts. (arXiv:2310.10865v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chance_C/0/1/0/all/0/1">Christina Chance</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Da Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a></p>
<p>Recent studies show that traditional fairytales are rife with harmful gender
biases. To help mitigate these gender biases in fairytales, this work aims to
assess learned biases of language models by evaluating their robustness against
gender perturbations. Specifically, we focus on Question Answering (QA) tasks
in fairytales. Using counterfactual data augmentation to the FairytaleQA
dataset, we evaluate model robustness against swapped gender character
information, and then mitigate learned biases by introducing counterfactual
gender stereotypes during training time. We additionally introduce a novel
approach that utilizes the massive vocabulary of language models to support
text genres beyond fairytales. Our experimental results suggest that models are
sensitive to gender perturbations, with significant performance drops compared
to the original testing set. However, when first fine-tuned on a counterfactual
training dataset, models are less sensitive to the later introduced anti-gender
stereotyped text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11722">Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical Foundation Model: A Computational Analysis. (arXiv:2310.11722v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yaxin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Feng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peifeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a></p>
<p>Foundation Models (FMs) have the potential to revolutionize the way users
self-diagnose through search engines by offering direct and efficient
suggestions. Recent studies primarily focused on the quality of FMs evaluated
by GPT-4 or their ability to pass medical exams, no studies have quantified the
extent of self-diagnostic atomic knowledge stored in FMs' memory, which is the
basis of foundation models to provide factual and reliable suggestions. In this
paper, we first constructed a benchmark of Self-diagnostic Atomic Knowledge
(SdAK), including the most common types of atomic knowledge involved in
self-diagnostic queries, with 17 atomic types and a total of 14, 048 pieces of
atomic knowledge. Then, we evaluated both generic and open-source Chinese
medical FMs on the benchmark. The experimental results showcase that generic
FMs perform better than medical FMs in terms of self-diagnostic atomic
knowledge. Error analysis revealed that both generic and medical FMs are
sycophantic, e.g., always catering to users' claims when it comes to unknown
knowledge. We further explored different types of data commonly adopted for
fine-tuning medical FMs, i.e., real-world, semi-distilled, and distilled data,
and found that distilled data can benefit FMs most. The code and data are
available at \url{https://github.com/FreedomIntelligence/SDAK}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12059">Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duc-Vu Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quoc-Nam Nguyen</a></p>
<p>In this paper, we evaluate the ability of large language models (LLMs) to
perform multiple choice symbol binding (MCSB) for multiple choice question
answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus
on Vietnamese, with fewer challenging MCQA datasets than in English. The two
existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent
research in Vietnamese natural language processing (NLP) has focused on the
Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to
2023 to evaluate ChatGPT. However, these studies have mainly focused on how
ChatGPT solves the VNHSGE step by step. We aim to create a novel and
high-quality dataset by providing structured guidelines for typing LaTeX
formulas for mathematics, physics, chemistry, and biology. This dataset can be
used to evaluate the MCSB ability of LLMs and smaller language models (LMs)
because it is typed in a strict LaTeX style. We focus on predicting the
character (A, B, C, or D) that is the most likely answer to a question, given
the context of the question. Our evaluation of six well-known LLMs, namely
BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the
ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising
results on the MCSB ability of LLMs for Vietnamese. The dataset is available
for research purposes only.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16776">DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Devleena Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1">Vivek Khetan</a></p>
<p>Recent advances have led to the availability of many pre-trained language
models (PLMs); however, a question that remains is how much data is truly
needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,
a data-efficient fine-tuning framework that leverages unsupervised core-set
selection to minimize the amount of data needed to fine-tune PLMs for
downstream tasks. We demonstrate the efficacy of our DEFT framework in the
context of text-editing LMs, and compare to the state-of-the art text-editing
model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT
models are just as accurate as CoEDIT while being finetuned on ~70% less data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00790">Construction Artifacts in Metaphor Identification Datasets. (arXiv:2311.00790v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boisson_J/0/1/0/all/0/1">Joanne Boisson</a>, <a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1">Luis Espinosa-Anke</a>, <a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1">Jose Camacho-Collados</a></p>
<p>Metaphor identification aims at understanding whether a given expression is
used figuratively in context. However, in this paper we show how existing
metaphor identification datasets can be gamed by fully ignoring the potential
metaphorical expression or the context in which it occurs. We test this
hypothesis in a variety of datasets and settings, and show that metaphor
identification systems based on language models without complete information
can be competitive with those using the full context. This is due to the
construction procedures to build such datasets, which introduce unwanted biases
for positive and negative classes. Finally, we test the same hypothesis on
datasets that are carefully sampled from natural corpora and where this bias is
not present, making these datasets more challenging and reliable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01766">Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Weidong Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shujun Li</a></p>
<p>Mis- and disinformation online have become a major societal problem as major
sources of online harms of different kinds. One common form of mis- and
disinformation is out-of-context (OOC) information, where different pieces of
information are falsely associated, e.g., a real image combined with a false
textual caption or a misleading textual description. Although some past studies
have attempted to defend against OOC mis- and disinformation through external
evidence, they tend to disregard the role of different pieces of evidence with
different stances. Motivated by the intuition that the stance of evidence
represents a bias towards different detection results, we propose a stance
extraction network (SEN) that can extract the stances of different pieces of
multi-modal evidence in a unified framework. Moreover, we introduce a
support-refutation score calculated based on the co-occurrence relations of
named entities into the textual SEN. Extensive experiments on a public
large-scale dataset demonstrated that our proposed method outperformed the
state-of-the-art baselines, with the best model achieving a performance gain of
3.2% in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02802">Incorporating Worker Perspectives into MTurk Annotation Practices for NLP. (arXiv:2311.02802v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_O/0/1/0/all/0/1">Olivia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1">Eve Fleisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a></p>
<p>Current practices regarding data collection for natural language processing
on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on
data quality and heuristics shared among NLP researchers. However, without
considering the perspectives of MTurk workers, these approaches are susceptible
to issues regarding workers' rights and poor response quality. We conducted a
critical literature review and a survey of MTurk workers aimed at addressing
open questions regarding best practices for fair payment, worker privacy, data
quality, and considering worker incentives. We found that worker preferences
are often at odds with received wisdom among NLP researchers. Surveyed workers
preferred reliable, reasonable payments over uncertain, very high payments;
reported frequently lying on demographic questions; and expressed frustration
at having work rejected with no explanation. We also found that workers view
some quality control methods, such as requiring minimum response times or
Master's qualifications, as biased and largely ineffective. Based on the survey
results, we provide recommendations on how future NLP studies may better
account for MTurk workers' experiences in order to respect workers' rights and
improve data quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03220">ALYMPICS: Language Agents Meet Game Theory. (arXiv:2311.03220v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1">Shaoguang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yuzhe Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wenshan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>This paper introduces Alympics, a platform that leverages Large Language
Model (LLM) agents to facilitate investigations in game theory. By employing
LLMs and autonomous agents to simulate human behavior and enable multi-agent
collaborations, we can construct realistic and dynamic models of human
interactions for game theory hypothesis formulating and testing. To demonstrate
this, we present and implement a survival game involving unequal competition
for limited resources. Through manipulation of resource availability and agent
personalities, we observe how different agents engage in the competition and
adapt their strategies. The use of LLM agents in game theory research offers
significant advantages, including simulating realistic behavior, providing a
controlled, scalable, and reproducible environment. Our work highlights the
potential of LLM agents in enhancing the understanding of strategic
decision-making within complex socioeconomic contexts. All codes are available
at https://github.com/microsoft/Alympics
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06694">Comparative Multi-View Language Grounding. (arXiv:2311.06694v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1">Chancharik Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1">Abrar Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Corona_R/0/1/0/all/0/1">Rodolfo Corona</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>In this work, we consider the task of resolving object referents when given a
comparative language description. We present a Multi-view Approach to Grounding
in Context (MAGiC) that leverages transformers to pragmatically reason over
both objects given multiple image views and a language description. In contrast
to past efforts that attempt to connect vision and language for this task
without fully considering the resulting referential context, MAGiC makes use of
the comparative information by jointly reasoning over multiple views of both
object referent candidates and the referring language expression. We present an
analysis demonstrating that comparative reasoning contributes to SOTA
performance on the SNARE object reference task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07194">Exploring the Dialogue Comprehension Ability of Large Language Models. (arXiv:2311.07194v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+She_S/0/1/0/all/0/1">Shuaijie She</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shujian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingyun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yanke Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiajun Chen</a></p>
<p>LLMs may interact with users in the form of dialogue and generate responses
following their instructions, which naturally require dialogue comprehension
abilities. However, dialogue comprehension is a general language ability which
is hard to be evaluated directly. In this work, we propose to perform the
evaluation with the help of the dialogue summarization task. Beside evaluating
and analyzing the dialogue summarization performance (DIAC-Sum) of different
LLMs, we also derive factual questions from the generated summaries and use
them as a more flexible measurement of dialogue comprehension (DIAC-FactQA).
Our evaluation shows that, on average, 27% of the summaries generated by LLMs
contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has
such errors in 16% of its summaries. For answering the factual questions, which
is more challenging, the average error rate of all evaluated LLMs is 37.2%.
Both results indicate serious deficiencies. Detailed analysis shows that the
understanding of subject/object of the conversation is still the most
challenging problem for LLMs. Furthermore, to stimulate and enhance the
dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with
auto-constructed multi-task data. The experimental results demonstrate that our
method achieved an error rate improvement of 10.9% on DIAC-FactQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07468">Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse. (arXiv:2311.07468v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_A/0/1/0/all/0/1">Ang Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Shufang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Q/0/1/0/all/0/1">Quan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a></p>
<p>Recent studies have highlighted a phenomenon in large language models (LLMs)
known as "the reversal curse," in which the order of knowledge entities in the
training data biases the models' comprehension. For example, if a model is
trained on sentences where entity A consistently appears before entity B, it
can respond to queries about A by providing B as the answer. However, it may
encounter confusion when presented with questions concerning B. We contend that
the reversal curse is partially a result of specific model training objectives,
particularly evident in the prevalent use of the next-token prediction within
most causal language models. For the next-token prediction, models solely focus
on a token's preceding context, resulting in a restricted comprehension of the
input. In contrast, we illustrate that the GLM, trained using the
autoregressive blank infilling objective where tokens to be predicted have
access to the entire context, exhibits better resilience against the reversal
curse. We propose a novel training method, BIdirectional Casual language
modeling Optimization (BICO), designed to mitigate the reversal curse when
fine-tuning pretrained causal language models on new data. BICO modifies the
causal attention mechanism to function bidirectionally and employs a mask
denoising optimization. In the task designed to assess the reversal curse, our
approach improves Llama's accuracy from the original 0% to around 70%. We hope
that more attention can be focused on exploring and addressing these inherent
weaknesses of the current LLMs, in order to achieve a higher level of
intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07469">InCA: Rethinking In-Car Conversational System Assessment Leveraging Large Language Models. (arXiv:2311.07469v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Friedl_K/0/1/0/all/0/1">Ken E. Friedl</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Abbas Goher Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1">Soumya Ranjan Sahoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rony_M/0/1/0/all/0/1">Md Rashad Al Hasan Rony</a>, <a href="http://arxiv.org/find/cs/1/au:+Germies_J/0/1/0/all/0/1">Jana Germies</a>, <a href="http://arxiv.org/find/cs/1/au:+Suss_C/0/1/0/all/0/1">Christian S&#xfc;&#xdf;</a></p>
<p>The assessment of advanced generative large language models (LLMs) poses a
significant challenge, given their heightened complexity in recent
developments. Furthermore, evaluating the performance of LLM-based applications
in various industries, as indicated by Key Performance Indicators (KPIs), is a
complex undertaking. This task necessitates a profound understanding of
industry use cases and the anticipated system behavior. Within the context of
the automotive industry, existing evaluation metrics prove inadequate for
assessing in-car conversational question answering (ConvQA) systems. The unique
demands of these systems, where answers may relate to driver or car safety and
are confined within the car domain, highlight the limitations of current
metrics. To address these challenges, this paper introduces a set of KPIs
tailored for evaluating the performance of in-car ConvQA systems, along with
datasets specifically designed for these KPIs. A preliminary and comprehensive
empirical evaluation substantiates the efficacy of our proposed approach.
Furthermore, we investigate the impact of employing varied personas in prompts
and found that it enhances the model's capacity to simulate diverse viewpoints
in assessments, mirroring how individuals with different backgrounds perceive a
topic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07587">Frontier Language Models are not Robust to Adversarial Arithmetic, or &quot;What do I need to say so you agree 2+2=5?. (arXiv:2311.07587v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freeman_C/0/1/0/all/0/1">C. Daniel Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1">Laura Culp</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1">Aaron Parisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bileschi_M/0/1/0/all/0/1">Maxwell L Bileschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1">Gamaleldin F Elsayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizkowsky_A/0/1/0/all/0/1">Alex Rizkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1">Isabelle Simpson</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1">Alex Alemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1">Azade Nova</a>, <a href="http://arxiv.org/find/cs/1/au:+Adlam_B/0/1/0/all/0/1">Ben Adlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1">Bernd Bohnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1">Gaurav Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1">Izzeddin Gur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaehoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1">JD Co-Reyes</a>, <a href="http://arxiv.org/find/cs/1/au:+Pennington_J/0/1/0/all/0/1">Jeffrey Pennington</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kelvin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1">Kevin Swersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1">Kshiteej Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lechao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rosanne Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1">Noah Constant</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1">Roman Novak</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yundi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1">Noah Fiedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1">Jascha Sohl-Dickstein</a></p>
<p>We introduce and study the problem of adversarial arithmetic, which provides
a simple yet challenging testbed for language model alignment. This problem is
comprised of arithmetic questions posed in natural language, with an arbitrary
adversarial string inserted before the question is complete. Even in the simple
setting of 1-digit addition problems, it is easy to find adversarial prompts
that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and
even to steer models to a particular wrong answer. We additionally provide a
simple algorithm for finding successful attacks by querying those same models,
which we name "prompt inversion rejection sampling" (PIRS). We finally show
that models can be partially hardened against these attacks via reinforcement
learning and via agentic constitutional loops. However, we were not able to
make a language model fully robust against adversarial arithmetic attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08329">KTRL+F: Knowledge-Augmented In-Document Search. (arXiv:2311.08329v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1">Hanseok Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1">Haebin Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_M/0/1/0/all/0/1">Miyoung Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyunji Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1">Minjoon Seo</a></p>
<p>We introduce a new problem KTRL+F, a knowledge-augmented in-document search
task that necessitates real-time identification of all semantic targets within
a document with the awareness of external sources through a single natural
query. This task addresses following unique challenges for in-document search:
1) utilizing knowledge outside the document for extended use of additional
information about targets to bridge the semantic gap between the query and the
targets, and 2) balancing between real-time applicability with the performance.
We analyze various baselines in KTRL+F and find there are limitations of
existing models, such as hallucinations, low latency, or difficulties in
leveraging external knowledge. Therefore we propose a Knowledge-Augmented
Phrase Retrieval model that shows a promising balance between speed and
performance by simply augmenting external knowledge embedding in phrase
embedding. Additionally, we conduct a user study to verify whether solving
KTRL+F can enhance search experience of users. It demonstrates that even with
our simple model users can reduce the time for searching with less queries and
reduced extra visits to other sources for collecting evidence. We encourage the
research community to work on KTRL+F to enhance more efficient in-document
information access.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08398">Are Large Language Models Temporally Grounded?. (arXiv:2311.08398v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yifu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1">Yftah Ziser</a>, <a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1">Anna Korhonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1">Edoardo M. Ponti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a></p>
<p>Are Large language models (LLMs) temporally grounded? Since LLMs cannot
perceive and interact with the environment, it is impossible to answer this
question directly. Instead, we provide LLMs with textual narratives and probe
them with respect to their common-sense knowledge of the structure and duration
of events, their ability to order events along a timeline, and self-consistency
within their temporal model (e.g., temporal relations such as after and before
are mutually exclusive for any pair of events). We evaluate state-of-the-art
LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.
Generally, we find that LLMs lag significantly behind both human performance as
well as small-scale, specialised LMs. In-context learning, instruction tuning,
and chain-of-thought prompting reduce this gap only to a limited degree.
Crucially, LLMs struggle the most with self-consistency, displaying incoherent
behaviour in at least 27.23% of their predictions. Contrary to expectations, we
also find that scaling the model size does not guarantee positive gains in
performance. To explain these results, we study the sources from which LLMs may
gather temporal information: we find that sentence ordering in unlabelled
texts, available during pre-training, is only weakly correlated with event
ordering. Moreover, public instruction tuning mixtures contain few temporal
tasks. Hence, we conclude that current LLMs lack a consistent temporal model of
textual narratives. Code, datasets, and LLM outputs are available at
https://github.com/yfqiu-nlp/temporal-llms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08562">MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration. (arXiv:2311.08562v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhiyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Daquan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Hongyu Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1">Kurt Keutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1">See Kiong Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a></p>
<p>Large Language Models (LLMs) have marked a significant advancement in the
field of natural language processing, demonstrating exceptional capabilities in
reasoning, tool usage, and memory. As their applications extend into
multi-agent environments, a need has arisen for a comprehensive evaluation
framework that captures their abilities in reasoning, planning, collaboration,
and more. This work introduces a novel benchmarking framework specifically
tailored to assess LLMs within multi-agent settings, providing quantitative
metrics to evaluate their judgment, reasoning, deception, self-awareness,
cooperation, coordination, and rationality. We utilize games such as Chameleon
and Undercover, alongside game theory scenarios like Cost Sharing, Multi-player
Prisoner's Dilemma, and Public Good, to create diverse testing environments.
Our framework is fortified with the Probabilistic Graphical Modeling (PGM)
method, enhancing the LLMs' capabilities in navigating complex social and
cognitive dimensions. The benchmark evaluates seven multi-agent systems powered
by different LLMs, quantitatively highlighting a significant capability gap
over threefold between the strongest, GPT-4, and the weakest, Llama-2-70B. It
also confirms that our PGM enhancement boosts the inherent abilities of all
selected models by 50% on average. Our codes are released here
https://github.com/cathyxl/MAgIC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09000">Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output. (arXiv:2311.09000v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1">Revanth Gangi Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mujahid_Z/0/1/0/all/0/1">Zain Muhammad Mujahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1">Arnav Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubashevskii_A/0/1/0/all/0/1">Aleksandr Rubashevskii</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1">Jiahui Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Afzal_O/0/1/0/all/0/1">Osama Mohammed Afzal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Liangming Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Borenstein_N/0/1/0/all/0/1">Nadav Borenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillai_A/0/1/0/all/0/1">Aditya Pillai</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1">Preslav Nakov</a></p>
<p>The increased use of large language models (LLMs) across a variety of
real-world applications calls for mechanisms to verify the factual accuracy of
their outputs. In this work, we present a holistic end-to-end solution for
annotating the factuality of LLM-generated responses, which encompasses a
multi-stage annotation scheme designed to yield detailed labels concerning the
verifiability and factual inconsistencies found in LLM outputs. We design and
build an annotation tool to speed up the labelling procedure and ease the
workload of raters. It allows flexible incorporation of automatic results in
any stage, e.g. automatically-retrieved evidence. We further construct an
open-domain document-level factuality benchmark in three-level granularity:
claim, sentence and document. Preliminary experiments show that FacTool,
FactScore and Perplexity.ai are struggling to identify false claims with the
best F1=0.53. Annotation tool, benchmark and code are available at
https://github.com/yuxiaw/Factcheck-GPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09174">AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph. (arXiv:2311.09174v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Haochen Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1">Tianqing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Sehyun Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>Cognitive research indicates that abstraction ability is essential in human
intelligence, which remains under-explored in language models. In this paper,
we present AbsPyramid, a unified entailment graph of 221K textual descriptions
of abstraction knowledge. While existing resources only touch nouns or verbs
within simplified events or specific domains, AbsPyramid collects abstract
knowledge for three components of diverse events to comprehensively evaluate
the abstraction ability of language models in the open domain. Experimental
results demonstrate that current LLMs face challenges comprehending abstraction
knowledge in zero-shot and few-shot settings. By training on our rich
abstraction knowledge, we find LLMs can acquire basic abstraction abilities and
generalize to unseen events. In the meantime, we empirically show that our
benchmark is comprehensive to enhance LLMs across two previous abstraction
tasks.
</p>
</p>
</div>

    </div>
    </body>
    