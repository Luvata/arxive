<!DOCTYPE html>
<html>
<head>
<title>2023-07-20-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.09485">Enhancing Evacuation Planning through Multi-Agent Simulation and Artificial Intelligence: Understanding Human Behavior in Hazardous Environments. (arXiv:2307.09485v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alazbah_A/0/1/0/all/0/1">Afnan Alazbah</a>, <a href="http://arxiv.org/find/cs/1/au:+Fakeeh_K/0/1/0/all/0/1">Khalid Fakeeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabie_O/0/1/0/all/0/1">Osama Rabie</a></p>
<p>This paper focuses on the crucial task of addressing the evacuation of
hazardous places, which holds great importance for coordinators, event hosts,
and authorities. To facilitate the development of effective solutions, the
paper employs Artificial Intelligence (AI) techniques, specifically Multi-Agent
Systems (MAS), to construct a simulation model for evacuation. NetLogo is
selected as the simulation tool of choice due to its ability to provide a
comprehensive understanding of human behaviour in distressing situations within
hazardous environments. The primary objective of this paper is to enhance our
comprehension of how individuals react and respond during such distressing
situations. By leveraging AI and MAS, the simulation model aims to capture the
complex dynamics of evacuation scenarios, enabling policymakers and emergency
planners to make informed decisions and implement more efficient and effective
evacuation strategies. This paper endeavours to contribute to the advancement
of evacuation planning and ultimately improve the safety and well-being of
individuals in hazardous places
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09564">Reinforcement Learning for Syntax-Guided Synthesis. (arXiv:2307.09564v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parsert_J/0/1/0/all/0/1">Julian Parsert</a>, <a href="http://arxiv.org/find/cs/1/au:+Polgreen_E/0/1/0/all/0/1">Elizabeth Polgreen</a></p>
<p>Program synthesis is the task of automatically generating code based on a
specification. In Syntax-Guided Synthesis(SyGuS) this specification is a
combination of a syntactic template and a logical formula, and any generated
code is proven to satisfy both. Techniques like SyGuS are critical to
guaranteeing correct synthesis results. Despite the proliferation of machine
learning in other types of program synthesis, state-of-the-art techniques in
SyGuS are still driven by automated reasoning tools and simple enumeration. We
hypothesize this is for two reasons: first the complexity of the search
problem, and second the relatively small data sets available. In this work, we
tackle these challenges by framing general SyGuS problems as a tree-search, and
present a reinforcement learning guided synthesis algorithm for SyGuS based on
Monte-Carlo Tree Search (MCTS). Our algorithm incorporates learned policy and
value functions combined with the upper confidence bound for trees to balance
exploration and exploitation. We incorporate this search procedure in a
reinforcement learning setup in order to iteratively improve our policy and
value estimators which are based on boosted tree models. To address the
scarcity of training data, we present a method for automatically generating
training data for SyGuS based on \emph{anti-unification} of existing
first-order satisfiability problems, which we use to train our MCTS policy. We
implement and evaluate this setup and demonstrate that learned policy and value
improve the synthesis performance over a baseline enumerator by over $26$
percentage points in the training and testing sets. With these results our tool
outperforms state-of-the-art-tools such as CVC5 on the training set and
performs comparably on the testing set. We make our data set publicly
available, enabling further application of machine learning methods to the
SyGuS problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09579">Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots. (arXiv:2307.09579v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bocheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangjing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hanqing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuanda Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qiben Yan</a></p>
<p>Recent advances in natural language processing and machine learning have led
to the development of chatbot models, such as ChatGPT, that can engage in
conversational dialogue with human users. However, the ability of these models
to generate toxic or harmful responses during a non-toxic multi-turn
conversation remains an open research question. Existing research focuses on
single-turn sentence testing, while we find that 82\% of the individual
non-toxic sentences that elicit toxic behaviors in a conversation are
considered safe by existing tools. In this paper, we design a new attack,
\toxicbot, by fine-tuning a chatbot to engage in conversation with a target
open-domain chatbot. The chatbot is fine-tuned with a collection of crafted
conversation sequences. Particularly, each conversation begins with a sentence
from a crafted prompt sentences dataset. Our extensive evaluation shows that
open-domain chatbot models can be triggered to generate toxic responses in a
multi-turn conversation. In the best scenario, \toxicbot achieves a 67\%
activation rate. The conversation sequences in the fine-tuning stage help
trigger the toxicity in a conversation, which allows the attack to bypass two
defense methods. Our findings suggest that further research is needed to
address chatbot toxicity in a dynamic interactive environment. The proposed
\toxicbot can be used by both industry and researchers to develop methods for
detecting and mitigating toxic responses in conversational dialogue and improve
the robustness of chatbots for end users.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09588">Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning. (arXiv:2307.09588v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nieradzik_L/0/1/0/all/0/1">Lars Nieradzik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sieburg_Rockel_J/0/1/0/all/0/1">J&#xf6;rdis Sieburg-Rockel</a>, <a href="http://arxiv.org/find/cs/1/au:+Helmling_S/0/1/0/all/0/1">Stephanie Helmling</a>, <a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1">Janis Keuper</a>, <a href="http://arxiv.org/find/cs/1/au:+Weibel_T/0/1/0/all/0/1">Thomas Weibel</a>, <a href="http://arxiv.org/find/cs/1/au:+Olbrich_A/0/1/0/all/0/1">Andrea Olbrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Stephani_H/0/1/0/all/0/1">Henrike Stephani</a></p>
<p>We have developed a methodology for the systematic generation of a large
image dataset of macerated wood references, which we used to generate image
data for nine hardwood genera. This is the basis for a substantial approach to
automate, for the first time, the identification of hardwood species in
microscopic images of fibrous materials by deep learning. Our methodology
includes a flexible pipeline for easy annotation of vessel elements. We compare
the performance of different neural network architectures and hyperparameters.
Our proposed method performs similarly well to human experts. In the future,
this will improve controls on global wood fiber product flows to protect
forests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09591">Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muzellec_S/0/1/0/all/0/1">Sabine Muzellec</a>, <a href="http://arxiv.org/find/cs/1/au:+Andeol_L/0/1/0/all/0/1">Leo Andeol</a>, <a href="http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1">Thomas Fel</a>, <a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1">Rufin VanRullen</a>, <a href="http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1">Thomas Serre</a></p>
<p>Recent years have witnessed an explosion in the development of novel
prediction-based attribution methods, which have slowly been supplanting older
gradient-based methods to explain the decisions of deep neural networks.
However, it is still not clear why prediction-based methods outperform
gradient-based ones. Here, we start with an empirical observation: these two
approaches yield attribution maps with very different power spectra, with
gradient-based methods revealing more high-frequency content than
prediction-based methods. This observation raises multiple questions: What is
the source of this high-frequency information, and does it truly reflect
decisions made by the system? Lastly, why would the absence of high-frequency
information in prediction-based methods yield better explainability scores
along multiple metrics? We analyze the gradient of three representative visual
classification models and observe that it contains noisy information emanating
from high-frequencies. Furthermore, our analysis reveals that the operations
used in Convolutional Neural Networks (CNNs) for downsampling appear to be a
significant source of this high-frequency content -- suggesting aliasing as a
possible underlying basis. We then apply an optimal low-pass filter for
attribution maps and demonstrate that it improves gradient-based attribution
methods. We show that (i) removing high-frequency noise yields significant
improvements in the explainability scores obtained with gradient-based methods
across multiple models -- leading to (ii) a novel ranking of state-of-the-art
methods with gradient-based methods at the top. We believe that our results
will spur renewed interest in simpler and computationally more efficient
gradient-based methods for explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09602">A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation. (arXiv:2307.09602v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perrett_A/0/1/0/all/0/1">Adam Perrett</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_D/0/1/0/all/0/1">Danny Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1">Gavin Brown</a></p>
<p>This work presents a novel algorithm for transforming a neural network into a
spline representation. Unlike previous work that required convex and
piecewise-affine network operators to create a max-affine spline alternate
form, this work relaxes this constraint. The only constraint is that the
function be bounded and possess a well-define second derivative, although this
was shown experimentally to not be strictly necessary. It can also be performed
over the whole network rather than on each layer independently. As in previous
work, this bridges the gap between neural networks and approximation theory but
also enables the visualisation of network feature maps. Mathematical proof and
experimental investigation of the technique is performed with approximation
error and feature maps being extracted from a range of architectures, including
convolutional neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09607">Sequential Monte Carlo Learning for Time Series Structure Discovery. (arXiv:2307.09607v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saad_F/0/1/0/all/0/1">Feras A. Saad</a>, <a href="http://arxiv.org/find/cs/1/au:+Patton_B/0/1/0/all/0/1">Brian J. Patton</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1">Matthew D. Hoffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1">Rif A. Saurous</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1">Vikash K. Mansinghka</a></p>
<p>This paper presents a new approach to automatically discovering accurate
models of complex time series data. Working within a Bayesian nonparametric
prior over a symbolic space of Gaussian process time series models, we present
a novel structure learning algorithm that integrates sequential Monte Carlo
(SMC) and involutive MCMC for highly effective posterior inference. Our method
can be used both in "online" settings, where new data is incorporated
sequentially in time, and in "offline" settings, by using nested subsets of
historical data to anneal the posterior. Empirical measurements on real-world
time series show that our method can deliver 10x--100x runtime speedups over
previous MCMC and greedy-search structure learning algorithms targeting the
same model family. We use our method to perform the first large-scale
evaluation of Gaussian process time series structure learning on a prominent
benchmark of 1,428 econometric datasets. The results show that our method
discovers sensible models that deliver more accurate point forecasts and
interval forecasts over multiple horizons as compared to widely used
statistical and neural baselines that struggle on this challenging data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09615">Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Md. Mahfuzur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1">Vince D. Calhoun</a>, <a href="http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1">Sergey M. Plis</a></p>
<p>Deep learning (DL) models have been popular due to their ability to learn
directly from the raw data in an end-to-end paradigm, alleviating the concern
of a separate error-prone feature extraction phase. Recent DL-based
neuroimaging studies have also witnessed a noticeable performance advancement
over traditional machine learning algorithms. But the challenges of deep
learning models still exist because of the lack of transparency in these models
for their successful deployment in real-world applications. In recent years,
Explainable AI (XAI) has undergone a surge of developments mainly to get
intuitions of how the models reached the decisions, which is essential for
safety-critical domains such as healthcare, finance, and law enforcement
agencies. While the interpretability domain is advancing noticeably,
researchers are still unclear about what aspect of model learning a post hoc
method reveals and how to validate its reliability. This paper comprehensively
reviews interpretable deep learning models in the neuroimaging domain. Firstly,
we summarize the current status of interpretability resources in general,
focusing on the progression of methods, associated challenges, and opinions.
Secondly, we discuss how multiple recent neuroimaging studies leveraged model
interpretability to capture anatomical and functional brain alterations most
relevant to model predictions. Finally, we discuss the limitations of the
current practices and offer some valuable insights and guidance on how we can
steer our future research directions to make deep learning models substantially
interpretable and thus advance scientific understanding of brain disorders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09624">Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions. (arXiv:2307.09624v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1">Huidong Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiongchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1">Xueqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Thorn_S/0/1/0/all/0/1">Stephanie Thorn</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yi-Hwa Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1">Ge Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1">Albert Sinusas</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a></p>
<p>Cardiovascular disease (CVD) is the leading cause of death worldwide, and
myocardial perfusion imaging using SPECT has been widely used in the diagnosis
of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary
geometry to simultaneously acquire 19 projections to increase sensitivity and
achieve dynamic imaging. However, the limited amount of angular sampling
negatively affects image quality. Deep learning methods can be implemented to
produce higher-quality images from stationary data. This is essentially a
few-view imaging problem. In this work, we propose a novel 3D transformer-based
dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image
reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images
directly from projection data without the iterative reconstruction process by
proposing a customized projection-to-image domain transformer. Then, given its
reconstruction output and the original few-view reconstruction, we further
refine the reconstruction using an image-domain reconstruction network.
Validated by cardiac catheterization images, diagnostic interpretations from
nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared
clinical software, our method produced images with higher cardiac defect
contrast on human studies compared with previous baseline methods, potentially
enabling high-quality defect visualization using stationary few-view dedicated
cardiac SPECT scanners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09636">Traffic-Domain Video Question Answering with Automatic Captioning. (arXiv:2307.09636v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qasemi_E/0/1/0/all/0/1">Ehsan Qasemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1">Jonathan M. Francis</a>, <a href="http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1">Alessandro Oltramari</a></p>
<p>Video Question Answering (VidQA) exhibits remarkable potential in
facilitating advanced machine reasoning capabilities within the domains of
Intelligent Traffic Monitoring and Intelligent Transportation Systems.
Nevertheless, the integration of urban traffic scene knowledge into VidQA
systems has received limited attention in previous research endeavors. In this
work, we present a novel approach termed Traffic-domain Video Question
Answering with Automatic Captioning (TRIVIA), which serves as a
weak-supervision technique for infusing traffic-domain knowledge into large
video-language models. Empirical findings obtained from the SUTD-TrafficQA task
highlight the substantial enhancements achieved by TRIVIA, elevating the
accuracy of representative video-language models by a remarkable 6.5 points
(19.88%) compared to baseline settings. This pioneering methodology holds great
promise for driving advancements in the field, inspiring researchers and
practitioners alike to unlock the full potential of emerging video-language
models in traffic-related applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09638">Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malviya_P/0/1/0/all/0/1">Pranshu Malviya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordido_G/0/1/0/all/0/1">Gon&#xe7;alo Mordido</a>, <a href="http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1">Aristide Baratin</a>, <a href="http://arxiv.org/find/cs/1/au:+Harikandeh_R/0/1/0/all/0/1">Reza Babanezhad Harikandeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jerry Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1">Simon Lacoste-Julien</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1">Razvan Pascanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a></p>
<p>Adaptive gradient-based optimizers, particularly Adam, have left their mark
in training large-scale deep learning models. The strength of such optimizers
is that they exhibit fast convergence while being more robust to hyperparameter
choice. However, they often generalize worse than non-adaptive methods. Recent
studies have tied this performance gap to flat minima selection: adaptive
methods tend to find solutions in sharper basins of the loss landscape, which
in turn hurts generalization. To overcome this issue, we propose a new
memory-augmented version of Adam that promotes exploration towards flatter
minima by using a buffer of critical momentum terms during training.
Intuitively, the use of the buffer makes the optimizer overshoot outside the
basin of attraction if it is not wide enough. We empirically show that our
method improves the performance of several variants of Adam on standard
supervised language modelling and image classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09650">With Flying Colors: Predicting Community Success in Large-scale Collaborative Campaigns. (arXiv:2307.09650v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Israeli_A/0/1/0/all/0/1">Abraham Israeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1">Oren Tsur</a></p>
<p>Online communities develop unique characteristics, establish social norms,
and exhibit distinct dynamics among their members. Activity in online
communities often results in concrete ``off-line'' actions with a broad
societal impact (e.g., political street protests and norms related to sexual
misconduct). While community dynamics, information diffusion, and online
collaborations have been widely studied in the past two decades, quantitative
studies that measure the effectiveness of online communities in promoting their
agenda are scarce. In this work, we study the correspondence between the
effectiveness of a community, measured by its success level in a competitive
online campaign, and the underlying dynamics between its members. To this end,
we define a novel task: predicting the success level of online communities in
Reddit's r/place - a large-scale distributed experiment that required
collaboration between community members. We consider an array of definitions
for success level; each is geared toward different aspects of collaborative
achievement. We experiment with several hybrid models, combining various types
of features. Our models significantly outperform all baseline models over all
definitions of `success level'. Analysis of the results and the factors that
contribute to the success of coordinated campaigns can provide a better
understanding of the resilience or the vulnerability of communities to online
social threats such as election interference or anti-science trends. We make
all data used for this study publicly available for further research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09652">VISER: A Tractable Solution Concept for Games with Information Asymmetry. (arXiv:2307.09652v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McMahan_J/0/1/0/all/0/1">Jeremy McMahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Young Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yudong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaojin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qiaomin Xie</a></p>
<p>Many real-world games suffer from information asymmetry: one player is only
aware of their own payoffs while the other player has the full game
information. Examples include the critical domain of security games and
adversarial multi-agent reinforcement learning. Information asymmetry renders
traditional solution concepts such as Strong Stackelberg Equilibrium (SSE) and
Robust-Optimization Equilibrium (ROE) inoperative. We propose a novel solution
concept called VISER (Victim Is Secure, Exploiter best-Responds). VISER enables
an external observer to predict the outcome of such games. In particular, for
security applications, VISER allows the victim to better defend itself while
characterizing the most damaging attacks available to the attacker. We show
that each player's VISER strategy can be computed independently in polynomial
time using linear programming (LP). We also extend VISER to its Markov-perfect
counterpart for Markov games, which can be solved efficiently using a series of
LPs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09653">HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1">Xiaotian Duan</a></p>
<p>Catastrophic forgetting, the phenomenon in which a neural network loses
previously obtained knowledge during the learning of new tasks, poses a
significant challenge in continual learning. The Hard-Attention-to-the-Task
(HAT) mechanism has shown potential in mitigating this problem, but its
practical implementation has been complicated by issues of usability and
compatibility, and a lack of support for existing network reuse. In this paper,
we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT
mechanism. HAT-CL not only automates gradient manipulation but also streamlines
the transformation of PyTorch modules into HAT modules. It achieves this by
providing a comprehensive suite of modules that can be seamlessly integrated
into existing architectures. Additionally, HAT-CL offers ready-to-use HAT
networks that are smoothly integrated with the TIMM library. Beyond the
redesign and reimplementation of HAT, we also introduce novel mask manipulation
techniques for HAT, which have consistently shown improvements across various
experiments. Our work paves the way for a broader application of the HAT
mechanism, opening up new possibilities in continual learning across diverse
models and applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09665">Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers. (arXiv:2307.09665v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Horawalavithana_S/0/1/0/all/0/1">Sameera Horawalavithana</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayton_E/0/1/0/all/0/1">Ellyn Ayton</a>, <a href="http://arxiv.org/find/cs/1/au:+Usenko_A/0/1/0/all/0/1">Anastasiya Usenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Cosbey_R/0/1/0/all/0/1">Robin Cosbey</a>, <a href="http://arxiv.org/find/cs/1/au:+Volkova_S/0/1/0/all/0/1">Svitlana Volkova</a></p>
<p>The ability to anticipate technical expertise and capability evolution trends
globally is essential for national and global security, especially in
safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging
fields like artificial intelligence (AI). In this work, we extend traditional
statistical relational learning approaches (e.g., link prediction in
collaboration networks) and formulate a problem of anticipating technical
expertise and capability evolution using dynamic heterogeneous graph
representations. We develop novel capabilities to forecast collaboration
patterns, authorship behavior, and technical capability evolution at different
granularities (e.g., scientist and institution levels) in two distinct research
fields. We implement a dynamic graph transformer (DGT) neural architecture,
which pushes the state-of-the-art graph neural network models by (a)
forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b)
relying on both discrete -- and continuous -- time inputs. We demonstrate that
our DGT models predict collaboration, partnership, and expertise patterns with
0.26, 0.73, and 0.53 mean reciprocal rank values for AI and 0.48, 0.93, and
0.22 for NN domains. DGT model performance exceeds the best-performing static
graph baseline models by 30-80% across AI and NN domains. Our findings
demonstrate that DGT models boost inductive task performance, when previously
unseen nodes appear in the test data, for the domains with emerging
collaboration patterns (e.g., AI). Specifically, models accurately predict
which established scientists will collaborate with early career scientists and
vice-versa in the AI domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09668">Towards A Unified Agent with Foundation Models. (arXiv:2307.09668v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1">Norman Di Palo</a>, <a href="http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1">Arunkumar Byravan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1">Leonard Hasenclever</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1">Markus Wulfmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1">Nicolas Heess</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1">Martin Riedmiller</a></p>
<p>Language Models and Vision Language Models have recently demonstrated
unprecedented capabilities in terms of understanding human intentions,
reasoning, scene understanding, and planning-like behaviour, in text form,
among many others. In this work, we investigate how to embed and leverage such
abilities in Reinforcement Learning (RL) agents. We design a framework that
uses language as the core reasoning tool, exploring how this enables an agent
to tackle a series of fundamental RL challenges, such as efficient exploration,
reusing experience data, scheduling skills, and learning from observations,
which traditionally require separate, vertically designed algorithms. We test
our method on a sparse-reward simulated robotic manipulation environment, where
a robot needs to stack a set of objects. We demonstrate substantial performance
improvements over baselines in exploration efficiency and ability to reuse data
from offline datasets, and illustrate how to reuse learned skills to solve
novel tasks or imitate videos of human experts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09673">What&#x27;s meant by explainable model: A Scoping Review. (arXiv:2307.09673v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mainali_M/0/1/0/all/0/1">Mallika Mainali</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_R/0/1/0/all/0/1">Rosina O Weber</a></p>
<p>We often see the term explainable in the titles of papers that describe
applications based on artificial intelligence (AI). However, the literature in
explainable artificial intelligence (XAI) indicates that explanations in XAI
are application- and domain-specific, hence requiring evaluation whenever they
are employed to explain a model that makes decisions for a specific application
problem. Additionally, the literature reveals that the performance of post-hoc
methods, particularly feature attribution methods, varies substantially hinting
that they do not represent a solution to AI explainability. Therefore, when
using XAI methods, the quality and suitability of their information outputs
should be evaluated within the specific application. For these reasons, we used
a scoping review methodology to investigate papers that apply AI models and
adopt methods to generate post-hoc explanations while referring to said models
as explainable. This paper investigates whether the term explainable model is
adopted by authors under the assumption that incorporating a post-hoc XAI
method suffices to characterize a model as explainable. To inspect this
problem, our review analyzes whether these papers conducted evaluations. We
found that 81% of the application papers that refer to their approaches as an
explainable model do not conduct any form of evaluation on the XAI method they
used.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09683">PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search. (arXiv:2307.09683v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1">Robert Leaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a></p>
<p>Biomedical research yields a wealth of information, much of which is only
accessible through the literature. Consequently, literature search is an
essential tool for building on prior knowledge in clinical and biomedical
research. Although recent improvements in artificial intelligence have expanded
functionality beyond keyword-based search, these advances may be unfamiliar to
clinicians and researchers. In response, we present a survey of literature
search tools tailored to both general and specific information needs in
biomedicine, with the objective of helping readers efficiently fulfill their
information needs. We first examine the widely used PubMed search engine,
discussing recent improvements and continued challenges. We then describe
literature search tools catering to five specific information needs: 1.
Identifying high-quality clinical research for evidence-based medicine. 2.
Retrieving gene-related information for precision medicine and genomics. 3.
Searching by meaning, including natural language questions. 4. Locating related
articles with literature recommendation. 5. Mining literature to discover
associations between concepts such as diseases and genetic variants.
Additionally, we cover practical considerations and best practices for choosing
and using these tools. Finally, we provide a perspective on the future of
literature search engines, considering recent breakthroughs in large language
models such as ChatGPT. In summary, our survey provides a comprehensive view of
biomedical literature search functionalities with 36 publicly available tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09688">Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Wei Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Haitao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haoming Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hongzhi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Haoyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hanqing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruirui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Monica Xiao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Goutam_R/0/1/0/all/0/1">Rahul Goutam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Subbian_K/0/1/0/all/0/1">Karthik Subbian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Suhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yizhou Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Bing Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xianfeng Tang</a></p>
<p>Modeling customer shopping intentions is a crucial task for e-commerce, as it
directly impacts user experience and engagement. Thus, accurately understanding
customer preferences is essential for providing personalized recommendations.
Session-based recommendation, which utilizes customer session data to predict
their next interaction, has become increasingly popular. However, existing
session datasets have limitations in terms of item attributes, user diversity,
and dataset scale. As a result, they cannot comprehensively capture the
spectrum of user behaviors and preferences. To bridge this gap, we present the
Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It
is the first multilingual dataset consisting of millions of user sessions from
six different locales, where the major languages of products are English,
German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can
help us enhance personalization and understanding of user preferences, which
can benefit various existing tasks as well as enable new tasks. To test the
potential of the dataset, we introduce three tasks in this work: (1)
next-product recommendation, (2) next-product recommendation with domain
shifts, and (3) next-product title generation. With the above tasks, we
benchmark a range of algorithms on our proposed dataset, drawing new insights
for further research and practice. In addition, based on the proposed dataset
and tasks, we hosted a competition in the KDD CUP 2023 and have attracted
thousands of users and submissions. The winning solutions and the associated
workshop can be accessed at our website https://kddcup23.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09692">STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization. (arXiv:2307.09692v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yachen Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Li He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zifeng Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>Preference-based reinforcement learning (PbRL) promises to learn a complex
reward function with binary human preference. However, such human-in-the-loop
formulation requires considerable human effort to assign preference labels to
segment pairs, hindering its large-scale applications. Recent approache has
tried to reuse unlabeled segments, which implicitly elucidates the distribution
of segments and thereby alleviates the human effort. And consistency
regularization is further considered to improve the performance of
semi-supervised learning. However, we notice that, unlike general
classification tasks, in PbRL there exits a unique phenomenon that we defined
as similarity trap in this paper. Intuitively, human can have diametrically
opposite preferredness for similar segment pairs, but such similarity may trap
consistency regularization fail in PbRL. Due to the existence of similarity
trap, such consistency regularization improperly enhances the consistency
possiblity of the model's predictions between segment pairs, and thus reduces
the confidence in reward learning, since the augmented distribution does not
match with the original one in PbRL. To overcome such issue, we present a
self-training method along with our proposed peer regularization, which
penalizes the reward model memorizing uninformative labels and acquires
confident predictions. Empirically, we demonstrate that our approach is capable
of learning well a variety of locomotion and robotic manipulation behaviors
using different semi-supervised alternatives and peer regularization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09706">RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap. (arXiv:2307.09706v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tianjian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1">Phillipe Langlais</a></p>
<p>Taxonomies are an essential knowledge representation, yet most studies on
automatic taxonomy construction (ATC) resort to manual evaluation to score
proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just
as important as taxonomy construction. We propose RaTE, an automatic label-free
taxonomy scoring procedure, which relies on a large pre-trained language model.
We apply our evaluation procedure to three state-of-the-art ATC algorithms with
which we built seven taxonomies from the Yelp domain, and show that 1) RaTE
correlates well with human judgments and 2) artificially degrading a taxonomy
leads to decreasing RaTE score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09711">Two Tales of Platoon Intelligence for Autonomous Mobility Control: Enabling Deep Learning Recipes. (arXiv:2307.09711v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Soohyun Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Haemin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1">Chanyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1">Soyi Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1">Minseok Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Joongheon Kim</a></p>
<p>This paper presents the deep learning-based recent achievements to resolve
the problem of autonomous mobility control and efficient resource management of
autonomous vehicles and UAVs, i.e., (i) multi-agent reinforcement learning
(MARL), and (ii) neural Myerson auction. Representatively, communication
network (CommNet), which is one of the most popular MARL algorithms, is
introduced to enable multiple agents to take actions in a distributed manner
for their shared goals by training all agents' states and actions in a single
neural network. Moreover, the neural Myerson auction guarantees trustfulness
among multiple agents as well as achieves the optimal revenue of highly dynamic
systems. Therefore, we survey the recent studies on autonomous mobility control
based on MARL and neural Myerson auction. Furthermore, we emphasize that
integration of MARL and neural Myerson auction is expected to be critical for
efficient and trustful autonomous mobility services.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09721">Multi-Grained Multimodal Interaction Network for Entity Linking. (arXiv:2307.09721v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Pengfei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shiwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Chen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Linli Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Enhong Chen</a></p>
<p>Multimodal entity linking (MEL) task, which aims at resolving ambiguous
mentions to a multimodal knowledge graph, has attracted wide attention in
recent years. Though large efforts have been made to explore the complementary
effect among multiple modalities, however, they may fail to fully absorb the
comprehensive expression of abbreviated textual context and implicit visual
indication. Even worse, the inevitable noisy data may cause inconsistency of
different modalities during the learning process, which severely degenerates
the performance. To address the above issues, in this paper, we propose a novel
Multi-GraIned Multimodal InteraCtion Network $\textbf{(MIMIC)}$ framework for
solving the MEL task. Specifically, the unified inputs of mentions and entities
are first encoded by textual/visual encoders separately, to extract global
descriptive features and local detailed features. Then, to derive the
similarity matching score for each mention-entity pair, we device three
interaction units to comprehensively explore the intra-modal interaction and
inter-modal fusion among features of entities and mentions. In particular,
three modules, namely the Text-based Global-Local interaction Unit (TGLU),
Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based
interaction Unit (CMFU) are designed to capture and integrate the fine-grained
representation lying in abbreviated text and implicit visual cues. Afterwards,
we introduce a unit-consistency objective function via contrastive learning to
avoid inconsistency and model degradation. Experimental results on three public
benchmark datasets demonstrate that our solution outperforms various
state-of-the-art baselines, and ablation studies verify the effectiveness of
designed modules.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09744">Enhancing conversational quality in language learning chatbots: An evaluation of GPT4 for ASR error correction. (arXiv:2307.09744v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1">Long Mai</a>, <a href="http://arxiv.org/find/cs/1/au:+Carson_Berndsen_J/0/1/0/all/0/1">Julie Carson-Berndsen</a></p>
<p>The integration of natural language processing (NLP) technologies into
educational applications has shown promising results, particularly in the
language learning domain. Recently, many spoken open-domain chatbots have been
used as speaking partners, helping language learners improve their language
skills. However, one of the significant challenges is the high word-error-rate
(WER) when recognizing non-native/non-fluent speech, which interrupts
conversation flow and leads to disappointment for learners. This paper explores
the use of GPT4 for ASR error correction in conversational settings. In
addition to WER, we propose to use semantic textual similarity (STS) and next
response sensibility (NRS) metrics to evaluate the impact of error correction
models on the quality of the conversation. We find that transcriptions
corrected by GPT4 lead to higher conversation quality, despite an increase in
WER. GPT4 also outperforms standard error correction methods without the need
for in-domain training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09751">Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. (arXiv:2307.09751v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1">Qingyao Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1">Ting Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1">Zhao Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhumin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhiyong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1">Shoubin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1">Zhicheng Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1">Fuli Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shen Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiafeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiangnan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1">Yanyan Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenliang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiqun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1">Ziyu Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Weizhi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhaochun Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1">Pengjie Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingwen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Jirong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Le Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1">Xin Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Dawei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weinan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofei Zhu</a></p>
<p>The research field of Information Retrieval (IR) has evolved significantly,
expanding beyond traditional search to meet diverse user information needs.
Recently, Large Language Models (LLMs) have demonstrated exceptional
capabilities in text understanding, generation, and knowledge inference,
opening up exciting avenues for IR research. LLMs not only facilitate
generative retrieval but also offer improved solutions for user understanding,
model evaluation, and user-system interactions. More importantly, the
synergistic relationship among IR models, LLMs, and humans forms a new
technical paradigm that is more powerful for information seeking. IR models
provide real-time and relevant information, LLMs contribute internal knowledge,
and humans play a central role of demanders and evaluators to the reliability
of information services. Nevertheless, significant challenges exist, including
computational costs, credibility concerns, domain-specific limitations, and
ethical considerations. To thoroughly discuss the transformative impact of LLMs
on IR research, the Chinese IR community conducted a strategic workshop in
April 2023, yielding valuable insights. This paper provides a summary of the
workshop's outcomes, including the rethinking of IR's core values, the mutual
enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and
open challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09755">Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation. (arXiv:2307.09755v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Haoyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuhui Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chong Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiangyu Yue</a></p>
<p>Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model
with limited labeled images and a substantial volume of unlabeled images. To
improve the robustness of representations, powerful methods introduce a
pixel-wise contrastive learning approach in latent space (i.e., representation
space) that aggregates the representations to their prototypes in a fully
supervised manner. However, previous contrastive-based S4 methods merely rely
on the supervision from the model's output (logits) in logit space during
unlabeled training. In contrast, we utilize the outputs in both logit space and
representation space to obtain supervision in a collaborative way. The
supervision from two spaces plays two roles: 1) reduces the risk of
over-fitting to incorrect semantic information in logits with the help of
representations; 2) enhances the knowledge exchange between the two spaces.
Furthermore, unlike previous approaches, we use the similarity between
representations and prototypes as a new indicator to tilt training those
under-performing representations and achieve a more efficient contrastive
learning process. Results on two public benchmarks demonstrate the competitive
performance of our method compared with state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09762">Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ajayakumar_A/0/1/0/all/0/1">Abhishek Ajayakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Raha_S/0/1/0/all/0/1">Soumyendu Raha</a></p>
<p>Complex networks are used to model many real-world systems. However, the
dimensionality of these systems can make them challenging to analyze.
Dimensionality reduction techniques like POD can be used in such cases.
However, these models are susceptible to perturbations in the input data. We
propose an algorithmic framework that combines techniques from pattern
recognition (PR) and stochastic filtering theory to enhance the output of such
models. The results of our study show that our method can improve the accuracy
of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are
susceptible to adversarial attacks. However, recent research has revealed that
neural Ordinary Differential Equations (ODEs) exhibit robustness in specific
applications. We benchmark our algorithmic framework with a Neural ODE-based
approach as a reference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09763">Towards Building More Robust Models with Frequency Bias. (arXiv:2307.09763v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1">Qingwen Bu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1">Heming Cui</a></p>
<p>The vulnerability of deep neural networks to adversarial samples has been a
major impediment to their broad applications, despite their success in various
fields. Recently, some works suggested that adversarially-trained models
emphasize the importance of low-frequency information to achieve higher
robustness. While several attempts have been made to leverage this frequency
characteristic, they have all faced the issue that applying low-pass filters
directly to input images leads to irreversible loss of discriminative
information and poor generalizability to datasets with distinct frequency
features. This paper presents a plug-and-play module called the Frequency
Preference Control Module that adaptively reconfigures the low- and
high-frequency components of intermediate feature representations, providing
better utilization of frequency in robust learning. Empirical studies show that
our proposed module can be easily incorporated into any adversarial training
framework, further improving model robustness across different architectures
and datasets. Additionally, experiments were conducted to examine how the
frequency bias of robust models impacts the adversarial training process and
its final robustness, revealing interesting insights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09767">Sig-Splines: universal approximation and convex calibration of time series generative models. (arXiv:2307.09767v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wiese_M/0/1/0/all/0/1">Magnus Wiese</a>, <a href="http://arxiv.org/find/cs/1/au:+Murray_P/0/1/0/all/0/1">Phillip Murray</a>, <a href="http://arxiv.org/find/cs/1/au:+Korn_R/0/1/0/all/0/1">Ralf Korn</a></p>
<p>We propose a novel generative model for multivariate discrete-time time
series data. Drawing inspiration from the construction of neural spline flows,
our algorithm incorporates linear transformations and the signature transform
as a seamless substitution for traditional neural networks. This approach
enables us to achieve not only the universality property inherent in neural
networks but also introduces convexity in the model's parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09770">Perturbing a Neural Network to Infer Effective Connectivity: Evidence from Synthetic EEG Data. (arXiv:2307.09770v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_P/0/1/0/all/0/1">Peizhen Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1">Xinke Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zongsheng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1">Zixiang Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Lou_K/0/1/0/all/0/1">Kexin Lou</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Quanying Liu</a></p>
<p>Identifying causal relationships among distinct brain areas, known as
effective connectivity, holds key insights into the brain's information
processing and cognitive functions. Electroencephalogram (EEG) signals exhibit
intricate dynamics and inter-areal interactions within the brain. However,
methods for characterizing nonlinear causal interactions among multiple brain
regions remain relatively underdeveloped. In this study, we proposed a
data-driven framework to infer effective connectivity by perturbing the trained
neural networks. Specifically, we trained neural networks (i.e., CNN, vanilla
RNN, GRU, LSTM, and Transformer) to predict future EEG signals according to
historical data and perturbed the networks' input to obtain effective
connectivity (EC) between the perturbed EEG channel and the rest of the
channels. The EC reflects the causal impact of perturbing one node on others.
The performance was tested on the synthetic EEG generated by a
biological-plausible Jansen-Rit model. CNN and Transformer obtained the best
performance on both 3-channel and 90-channel synthetic EEG data, outperforming
the classical Granger causality method. Our work demonstrated the potential of
perturbing an artificial neural network, learned to predict future system
dynamics, to uncover the underlying causal structure.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09777">Generating Redstone Style Cities in Minecraft. (arXiv:2307.09777v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shuo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chengpeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1">Julian Togelius</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jialin Liu</a></p>
<p>Procedurally generating cities in Minecraft provides players more diverse
scenarios and could help understand and improve the design of cities in other
digital worlds and the real world. This paper presents a city generator that
was submitted as an entry to the 2023 Edition of Minecraft Settlement
Generation Competition for Minecraft. The generation procedure is composed of
six main steps, namely vegetation clearing, terrain reshaping, building layout
generation, route planning, streetlight placement, and wall construction. Three
algorithms, including a heuristic-based algorithm, an evolving layout
algorithm, and a random one are applied to generate the building layout, thus
determining where to place different redstone style buildings, and tested by
generating cities on random maps in limited time. Experimental results show
that the heuristic-based algorithm is capable of finding an acceptable building
layout faster for flat maps, while the evolving layout algorithm performs
better in evolving layout for rugged maps. A user study is conducted to compare
our generator with outstanding entries of the competition's 2022 edition using
the competition's evaluation criteria and shows that our generator performs
well in the adaptation and functionality criteria
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09779">Beyond Single-Feature Importance with ICECREAM. (arXiv:2307.09779v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oesterle_M/0/1/0/all/0/1">Michael Oesterle</a>, <a href="http://arxiv.org/find/cs/1/au:+Blobaum_P/0/1/0/all/0/1">Patrick Bl&#xf6;baum</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastakouri_A/0/1/0/all/0/1">Atalanti A. Mastakouri</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirschbaum_E/0/1/0/all/0/1">Elke Kirschbaum</a></p>
<p>Which set of features was responsible for a certain output of a machine
learning model? Which components caused the failure of a cloud computing
application? These are just two examples of questions we are addressing in this
work by Identifying Coalition-based Explanations for Common and Rare Events in
Any Model (ICECREAM). Specifically, we propose an information-theoretic
quantitative measure for the influence of a coalition of variables on the
distribution of a target variable. This allows us to identify which set of
factors is essential to obtain a certain outcome, as opposed to
well-established explainability and causal contribution analysis methods which
can assign contributions only to individual factors and rank them by their
importance. In experiments with synthetic and real-world data, we show that
ICECREAM outperforms state-of-the-art methods for explainability and root cause
analysis, and achieves impressive accuracy in both tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09781">Text2Layer: Layered Image Generation using Latent Diffusion Model. (arXiv:2307.09781v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wentian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chien_J/0/1/0/all/0/1">Jeff Chien</a></p>
<p>Layer compositing is one of the most popular image editing workflows among
both amateurs and professionals. Motivated by the success of diffusion models,
we explore layer compositing from a layered image generation perspective.
Instead of generating an image, we propose to generate background, foreground,
layer mask, and the composed image simultaneously. To achieve layered image
generation, we train an autoencoder that is able to reconstruct layered images
and train diffusion models on the latent representation. One benefit of the
proposed problem is to enable better compositing workflows in addition to the
high-quality image output. Another benefit is producing higher-quality layer
masks compared to masks produced by a separate step of image segmentation.
Experimental results show that the proposed method is able to generate
high-quality layered images and initiates a benchmark for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09782">ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoxia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhewei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuxiong He</a></p>
<p>In the complex domain of large language models (LLMs), striking a balance
between computational efficiency and maintaining model quality is a formidable
challenge. Navigating the inherent limitations of uniform quantization,
particularly when dealing with outliers, and motivated by the launch of
NVIDIA's H100 hardware, this study delves into the viability of floating-point
(FP) quantization, particularly focusing on FP8 and FP4, as a potential
solution. Our comprehensive investigation reveals that for LLMs, FP8 activation
consistently outshines its integer (INT8) equivalent, with the performance edge
becoming more noticeable in models possessing parameters beyond one billion.
For weight quantization, our findings indicate that FP4 exhibits comparable, if
not superior, performance to INT4, simplifying deployment on FP-supported
hardware like H100. To mitigate the overhead from precision alignment caused by
the disparity between weights and activations, we propose two scaling
constraints for weight quantization that negligibly impact the performance
compared to the standard W4A8 model. We additionally enhance our quantization
methods by integrating the Low Rank Compensation (LoRC) strategy, yielding
improvements especially in smaller models. The results of our investigation
emphasize the immense potential of FP quantization for LLMs, paving the way for
high-efficiency deployment in resource-limited settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09797">Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Negiar_G/0/1/0/all/0/1">Geoffrey N&#xe9;giar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruijun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Meetei_O/0/1/0/all/0/1">O. Nangba Meetei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1">Mengfei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1">Michael W. Mahoney</a></p>
<p>Obtaining accurate probabilistic forecasts while respecting hierarchical
information is an important operational challenge in many applications, perhaps
most obviously in energy management, supply chain planning, and resource
allocation. The basic challenge, especially for multivariate forecasting, is
that forecasts are often required to be coherent with respect to the
hierarchical structure. In this paper, we propose a new model which leverages a
factor model structure to produce coherent forecasts by construction. This is a
consequence of a simple (exchangeability) observation: permuting
\textit{}base-level series in the hierarchy does not change their aggregates.
Our model uses a convolutional neural network to produce parameters for the
factors, their loadings and base-level distributions; it produces samples which
can be differentiated with respect to the model's parameters; and it can
therefore optimize for any sample-based loss function, including the Continuous
Ranked Probability Score and quantile losses. We can choose arbitrary
continuous distributions for the factor and the base-level distributions. We
compare our method to two previous methods which can be optimized end-to-end,
while enforcing coherent aggregation. Our model achieves significant
improvements: between $11.8-41.4\%$ on three hierarchical forecasting datasets.
We also analyze the influence of parameters in our model with respect to
base-level distribution and number of factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09827">Online Continual Learning for Robust Indoor Object Recognition. (arXiv:2307.09827v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1">Umberto Michieli</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1">Mete Ozay</a></p>
<p>Vision systems mounted on home robots need to interact with unseen classes in
changing environments. Robots have limited computational resources, labelled
data and storage capability. These requirements pose some unique challenges:
models should adapt without forgetting past knowledge in a data- and
parameter-efficient way. We characterize the problem as few-shot (FS) online
continual learning (OCL), where robotic agents learn from a non-repeated stream
of few-shot data updating only a few model parameters. Additionally, such
models experience variable conditions at test time, where objects may appear in
different poses (e.g., horizontal or vertical) and environments (e.g., day or
night). To improve robustness of CL agents, we propose RobOCLe, which; 1)
constructs an enriched feature space computing high order statistical moments
from the embedded features of samples; and 2) computes similarity between high
order statistics of the samples on the enriched feature space, and predicts
their class labels. We evaluate robustness of CL models to train/test
augmentations in various cases. We show that different moments allow RobOCLe to
capture different properties of deformations, providing higher robustness with
no decrease of inference speed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09831">A Fast and Map-Free Model for Trajectory Prediction in Traffics. (arXiv:2307.09831v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1">Junhong Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingmin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1">Zhixiong Nan</a></p>
<p>To handle the two shortcomings of existing methods, (i)nearly all models rely
on high-definition (HD) maps, yet the map information is not always available
in real traffic scenes and HD map-building is expensive and time-consuming and
(ii) existing models usually focus on improving prediction accuracy at the
expense of reducing computing efficiency, yet the efficiency is crucial for
various real applications, this paper proposes an efficient trajectory
prediction model that is not dependent on traffic maps. The core idea of our
model is encoding single-agent's spatial-temporal information in the first
stage and exploring multi-agents' spatial-temporal interactions in the second
stage. By comprehensively utilizing attention mechanism, LSTM, graph
convolution network and temporal transformer in the two stages, our model is
able to learn rich dynamic and interaction information of all agents. Our model
achieves the highest performance when comparing with existing map-free methods
and also exceeds most map-based state-of-the-art methods on the Argoverse
dataset. In addition, our model also exhibits a faster inference speed than the
baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09858">Towards Reliable Rare Category Analysis on Graphs via Individual Calibration. (arXiv:2307.09858v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Longfeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Bowen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongkuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dawei Zhou</a></p>
<p>Rare categories abound in a number of real-world networks and play a pivotal
role in a variety of high-stakes applications, including financial fraud
detection, network intrusion detection, and rare disease diagnosis. Rare
category analysis (RCA) refers to the task of detecting, characterizing, and
comprehending the behaviors of minority classes in a highly-imbalanced data
distribution. While the vast majority of existing work on RCA has focused on
improving the prediction performance, a few fundamental research questions
heretofore have received little attention and are less explored: How confident
or uncertain is a prediction model in rare category analysis? How can we
quantify the uncertainty in the learning process and enable reliable rare
category analysis?
</p>
<p>To answer these questions, we start by investigating miscalibration in
existing RCA methods. Empirical results reveal that state-of-the-art RCA
methods are mainly over-confident in predicting minority classes and
under-confident in predicting majority classes. Motivated by the observation,
we propose a novel individual calibration framework, named CALIRARE, for
alleviating the unique challenges of RCA, thus enabling reliable rare category
analysis. In particular, to quantify the uncertainties in RCA, we develop a
node-level uncertainty quantification algorithm to model the overlapping
support regions with high uncertainty; to handle the rarity of minority classes
in miscalibration calculation, we generalize the distribution-based calibration
metric to the instance level and propose the first individual calibration
measurement on graphs named Expected Individual Calibration Error (EICE). We
perform extensive experimental evaluations on real-world datasets, including
rare category characterization and model calibration tasks, which demonstrate
the significance of our proposed framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09862">Towards a population-informed approach to the definition of data-driven models for structural dynamics. (arXiv:2307.09862v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsialiamanis_G/0/1/0/all/0/1">G. Tsialiamanis</a>, <a href="http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1">N. Dervilis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagg_D/0/1/0/all/0/1">D.J. Wagg</a>, <a href="http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1">K. Worden</a></p>
<p>Machine learning has affected the way in which many phenomena for various
domains are modelled, one of these domains being that of structural dynamics.
However, because machine-learning algorithms are problem-specific, they often
fail to perform efficiently in cases of data scarcity. To deal with such
issues, combination of physics-based approaches and machine learning algorithms
have been developed. Although such methods are effective, they also require the
analyser's understanding of the underlying physics of the problem. The current
work is aimed at motivating the use of models which learn such relationships
from a population of phenomena, whose underlying physics are similar. The
development of such models is motivated by the way that physics-based models,
and more specifically finite element models, work. Such models are considered
transferrable, explainable and trustworthy, attributes which are not trivially
imposed or achieved for machine-learning models. For this reason,
machine-learning approaches are less trusted by industry and often considered
more difficult to form validated models. To achieve such data-driven models, a
population-based scheme is followed here and two different machine-learning
algorithms from the meta-learning domain are used. The two algorithms are the
model-agnostic meta-learning (MAML) algorithm and the conditional neural
processes (CNP) model. The algorithms seem to perform as intended and
outperform a traditional machine-learning algorithm at approximating the
quantities of interest. Moreover, they exhibit behaviour similar to traditional
machine learning algorithms (e.g. neural networks or Gaussian processes),
concerning their performance as a function of the available structures in the
training population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09866">Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jinzhu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Liu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chen Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huandong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Hangyu Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1">Depeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a></p>
<p>Understanding and characterizing the vulnerability of urban infrastructures,
which refers to the engineering facilities essential for the regular running of
cities and that exist naturally in the form of networks, is of great value to
us. Potential applications include protecting fragile facilities and designing
robust topologies, etc. Due to the strong correlation between different
topological characteristics and infrastructure vulnerability and their
complicated evolution mechanisms, some heuristic and machine-assisted analysis
fall short in addressing such a scenario. In this paper, we model the
interdependent network as a heterogeneous graph and propose a system based on
graph neural network with reinforcement learning, which can be trained on
real-world data, to characterize the vulnerability of the city system
accurately. The presented system leverages deep learning techniques to
understand and analyze the heterogeneous graph, which enables us to capture the
risk of cascade failure and discover vulnerable infrastructures of cities.
Extensive experiments with various requests demonstrate not only the expressive
power of our system but also transferring ability and necessity of the specific
components.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09878">Amortised Experimental Design and Parameter Estimation for User Models of Pointing. (arXiv:2307.09878v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keurulainen_A/0/1/0/all/0/1">Antti Keurulainen</a>, <a href="http://arxiv.org/find/cs/1/au:+Westerlund_I/0/1/0/all/0/1">Isak Westerlund</a>, <a href="http://arxiv.org/find/cs/1/au:+Keurulainen_O/0/1/0/all/0/1">Oskar Keurulainen</a>, <a href="http://arxiv.org/find/cs/1/au:+Howes_A/0/1/0/all/0/1">Andrew Howes</a></p>
<p>User models play an important role in interaction design, supporting
automation of interaction design choices. In order to do so, model parameters
must be estimated from user data. While very large amounts of user data are
sometimes required, recent research has shown how experiments can be designed
so as to gather data and infer parameters as efficiently as possible, thereby
minimising the data requirement. In the current article, we investigate a
variant of these methods that amortises the computational cost of designing
experiments by training a policy for choosing experimental designs with
simulated participants. Our solution learns which experiments provide the most
useful data for parameter estimation by interacting with in-silico agents
sampled from the model space thereby using synthetic data rather than vast
amounts of human data. The approach is demonstrated for three progressively
complex models of pointing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09882">Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ben_Dov_O/0/1/0/all/0/1">Omri Ben-Dov</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Pravir Singh Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1">Victoria Abrevaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1">Partha Ghosh</a></p>
<p>Generative Adversarial Networks (GANs) can produce high-quality samples, but
do not provide an estimate of the probability density around the samples.
However, it has been noted that maximizing the log-likelihood within an
energy-based setting can lead to an adversarial framework where the
discriminator provides unnormalized density (often called energy). We further
develop this perspective, incorporate importance sampling, and show that 1)
Wasserstein GAN performs a biased estimate of the partition function, and we
propose instead to use an unbiased estimator; 2) when optimizing for
likelihood, one must maximize generator entropy. This is hypothesized to
provide a better mode coverage. Different from previous works, we explicitly
compute the density of the generated samples. This is the key enabler to
designing an unbiased estimator of the partition function and computation of
the generator entropy term. The generator density is obtained via a new type of
flow network, called one-way flow network, that is less constrained in terms of
architecture, as it does not require to have a tractable inverse function. Our
experimental results show that we converge faster, produce comparable sample
quality to GANs with similar architecture, successfully avoid over-fitting to
commonly used datasets and produce smooth low-dimensional latent
representations of the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09885">Test-takers have a say: understanding the implications of the use of AI in language tests. (arXiv:2307.09885v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thong Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shidong Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yongquan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1">Mark Staples</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Quigley_A/0/1/0/all/0/1">Aaron Quigley</a></p>
<p>Language tests measure a person's ability to use a language in terms of
listening, speaking, reading, or writing. Such tests play an integral role in
academic, professional, and immigration domains, with entities such as
educational institutions, professional accreditation bodies, and governments
using them to assess candidate language proficiency. Recent advances in
Artificial Intelligence (AI) and the discipline of Natural Language Processing
have prompted language test providers to explore AI's potential applicability
within language testing, leading to transformative activity patterns
surrounding language instruction and learning. However, with concerns over AI's
trustworthiness, it is imperative to understand the implications of integrating
AI into language testing. This knowledge will enable stakeholders to make
well-informed decisions, thus safeguarding community well-being and testing
integrity. To understand the concerns and effects of AI usage in language
tests, we conducted interviews and surveys with English test-takers. To the
best of our knowledge, this is the first empirical study aimed at identifying
the implications of AI adoption in language tests from a test-taker
perspective. Our study reveals test-taker perceptions and behavioral patterns.
Specifically, we identify that AI integration may enhance perceptions of
fairness, consistency, and availability. Conversely, it might incite mistrust
regarding reliability and interactivity aspects, subsequently influencing the
behaviors and well-being of test-takers. These insights provide a better
understanding of potential societal implications and assist stakeholders in
making informed decisions concerning AI usage in language testing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09886">A reinforcement learning approach for VQA validation: an application to diabetic macular edema grading. (arXiv:2307.09886v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fountoukidou_T/0/1/0/all/0/1">Tatiana Fountoukidou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1">Raphael Sznitman</a></p>
<p>Recent advances in machine learning models have greatly increased the
performance of automated methods in medical image analysis. However, the
internal functioning of such models is largely hidden, which hinders their
integration in clinical practice. Explainability and trust are viewed as
important aspects of modern methods, for the latter's widespread use in
clinical communities. As such, validation of machine learning models represents
an important aspect and yet, most methods are only validated in a limited way.
In this work, we focus on providing a richer and more appropriate validation
approach for highly powerful Visual Question Answering (VQA) algorithms. To
better understand the performance of these methods, which answer arbitrary
questions related to images, this work focuses on an automatic visual Turing
test (VTT). That is, we propose an automatic adaptive questioning method, that
aims to expose the reasoning behavior of a VQA algorithm. Specifically, we
introduce a reinforcement learning (RL) agent that observes the history of
previously asked questions, and uses it to select the next question to pose. We
demonstrate our approach in the context of evaluating algorithms that
automatically answer questions related to diabetic macular edema (DME) grading.
The experiments show that such an agent has similar behavior to a clinician,
whereby asking questions that are relevant to key clinical concepts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09891">Amortised Design Optimization for Item Response Theory. (arXiv:2307.09891v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Keurulainen_A/0/1/0/all/0/1">Antti Keurulainen</a>, <a href="http://arxiv.org/find/cs/1/au:+Westerlund_I/0/1/0/all/0/1">Isak Westerlund</a>, <a href="http://arxiv.org/find/cs/1/au:+Keurulainen_O/0/1/0/all/0/1">Oskar Keurulainen</a>, <a href="http://arxiv.org/find/cs/1/au:+Howes_A/0/1/0/all/0/1">Andrew Howes</a></p>
<p>Item Response Theory (IRT) is a well known method for assessing responses
from humans in education and psychology. In education, IRT is used to infer
student abilities and characteristics of test items from student responses.
Interactions with students are expensive, calling for methods that efficiently
gather information for inferring student abilities. Methods based on Optimal
Experimental Design (OED) are computationally costly, making them inapplicable
for interactive applications. In response, we propose incorporating amortised
experimental design into IRT. Here, the computational cost is shifted to a
precomputing phase by training a Deep Reinforcement Learning (DRL) agent with
synthetic data. The agent is trained to select optimally informative test items
for the distribution of students, and to conduct amortised inference
conditioned on the experiment outcomes. During deployment the agent estimates
parameters from data, and suggests the next test item for the student, in close
to real-time, by taking into account the history of experiments and outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09898">An analysis on the effects of speaker embedding choice in non auto-regressive TTS. (arXiv:2307.09898v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Stan_A/0/1/0/all/0/1">Adriana Stan</a>, <a href="http://arxiv.org/find/eess/1/au:+OMahony_J/0/1/0/all/0/1">Johannah O&#x27;Mahony</a></p>
<p>In this paper we introduce a first attempt on understanding how a
non-autoregressive factorised multi-speaker speech synthesis architecture
exploits the information present in different speaker embedding sets. We
analyse if jointly learning the representations, and initialising them from
pretrained models determine any quality improvements for target speaker
identities. In a separate analysis, we investigate how the different sets of
embeddings impact the network's core speech abstraction (i.e. zero conditioned)
in terms of speaker identity and representation learning. We show that,
regardless of the used set of embeddings and learning strategy, the network can
handle various speaker identities equally well, with barely noticeable
variations in speech output quality, and that speaker leakage within the core
structure of the synthesis system is inevitable in the standard training
procedures adopted thus far.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09905">PyTAG: Challenges and Opportunities for Reinforcement Learning in Tabletop Games. (arXiv:2307.09905v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balla_M/0/1/0/all/0/1">Martin Balla</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1">George E.M. Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeurissen_D/0/1/0/all/0/1">Dominik Jeurissen</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_J/0/1/0/all/0/1">James Goodman</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaina_R/0/1/0/all/0/1">Raluca D. Gaina</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1">Diego Perez-Liebana</a></p>
<p>In recent years, Game AI research has made important breakthroughs using
Reinforcement Learning (RL). Despite this, RL for modern tabletop games has
gained little to no attention, even when they offer a range of unique
challenges compared to video games. To bridge this gap, we introduce PyTAG, a
Python API for interacting with the Tabletop Games framework (TAG). TAG
contains a growing set of more than 20 modern tabletop games, with a common API
for AI agents. We present techniques for training RL agents in these games and
introduce baseline results after training Proximal Policy Optimisation
algorithms on a subset of games. Finally, we discuss the unique challenges
complex modern tabletop games provide, now open to RL research through PyTAG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09906">Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation. (arXiv:2307.09906v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1">Fa-Ting Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dan Xu</a></p>
<p>Talking head video generation aims to animate a human face in a still image
with dynamic poses and expressions using motion information derived from a
target-driving video, while maintaining the person's identity in the source
image. However, dramatic and complex motions in the driving video cause
ambiguous generation, because the still source image cannot provide sufficient
appearance information for occluded regions or delicate expression variations,
which produces severe artifacts and significantly degrades the generation
quality. To tackle this problem, we propose to learn a global facial
representation space, and design a novel implicit identity representation
conditioned memory compensation network, coined as MCNet, for high-fidelity
talking head generation.~Specifically, we devise a network module to learn a
unified spatial facial meta-memory bank from all training samples, which can
provide rich facial structure and appearance priors to compensate warped source
facial features for the generation. Furthermore, we propose an effective query
mechanism based on implicit identity representations learned from the discrete
keypoints of the source image. It can greatly facilitate the retrieval of more
correlated information from the memory bank for the compensation. Extensive
experiments demonstrate that MCNet can learn representative and complementary
facial memory, and can clearly outperform previous state-of-the-art talking
head generation methods on VoxCeleb1 and CelebV datasets. Please check our
\href{https://github.com/harlanhong/ICCV2023-MCNET}{Project}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09909">Chit-Chat or Deep Talk: Prompt Engineering for Process Mining. (arXiv:2307.09909v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jessen_U/0/1/0/all/0/1">Urszula Jessen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sroka_M/0/1/0/all/0/1">Michal Sroka</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahland_D/0/1/0/all/0/1">Dirk Fahland</a></p>
<p>This research investigates the application of Large Language Models (LLMs) to
augment conversational agents in process mining, aiming to tackle its inherent
complexity and diverse skill requirements. While LLM advancements present novel
opportunities for conversational process mining, generating efficient outputs
is still a hurdle. We propose an innovative approach that amend many issues in
existing solutions, informed by prior research on Natural Language Processing
(NLP) for conversational agents. Leveraging LLMs, our framework improves both
accessibility and agent performance, as demonstrated by experiments on public
question and data sets. Our research sets the stage for future explorations
into LLMs' role in process mining and concludes with propositions for enhancing
LLM memory, implementing real-time user testing, and examining diverse data
sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09913">Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features. (arXiv:2307.09913v1 [cs.LO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bednarczyk_B/0/1/0/all/0/1">Bartosz Bednarczyk</a></p>
<p>We investigate the impact of non-regular path expressions on the decidability
of satisfiability checking and querying in description logics extending ALC.
Our primary objects of interest are ALCreg and ALCvpl, the extensions of with
path expressions employing, respectively, regular and visibly-pushdown
languages. The first one, ALCreg, is a notational variant of the well-known
Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was
introduced and investigated by Loding and Serre in 2007. The logic ALCvpl
generalises many known decidable non-regular extensions of ALCreg.
</p>
<p>We provide a series of undecidability results. First, we show that
decidability of the concept satisfiability problem for ALCvpl is lost upon
adding the seemingly innocent Self operator. Second, we establish
undecidability for the concept satisfiability problem for ALCvpl extended with
nominals. Interestingly, our undecidability proof relies only on one single
non-regular (visibly-pushdown) language, namely on r#s# := { r^n s^n | n in N }
for fixed role names r and s. Finally, in contrast to the classical database
setting, we establish undecidability of query entailment for queries involving
non-regular atoms from r#s#, already in the case of ALC-TBoxes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09933">Spuriosity Didn&#x27;t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1">Cian Eastwood</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shashank Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1">Andrei Liviu Nicolicioiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1">Marin Vlastelica</a>, <a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1">Julius von K&#xfc;gelgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>To avoid failures on out-of-distribution data, recent works have sought to
extract features that have a stable or invariant relationship with the label
across domains, discarding the "spurious" or unstable features whose
relationship with the label changes across domains. However, unstable features
often carry complementary information about the label that could boost
performance if used correctly in the test domain. Our main contribution is to
show that it is possible to learn how to use these unstable features in the
test domain without labels. In particular, we prove that pseudo-labels based on
stable features provide sufficient guidance for doing so, provided that stable
and unstable features are conditionally independent given the label. Based on
this theoretical insight, we propose Stable Feature Boosting (SFB), an
algorithm for: (i) learning a predictor that separates stable and
conditionally-independent unstable features; and (ii) using the stable-feature
predictions to adapt the unstable-feature predictions in the test domain.
Theoretically, we prove that SFB can learn an asymptotically-optimal predictor
without test-domain labels. Empirically, we demonstrate the effectiveness of
SFB on real and synthetic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09942">TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network. (arXiv:2307.09942v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1">Brandon Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Clinical trials are critical for drug development but often suffer from
expensive and inefficient patient recruitment. In recent years, machine
learning models have been proposed for speeding up patient recruitment via
automatically matching patients with clinical trials based on longitudinal
patient electronic health records (EHR) data and eligibility criteria of
clinical trials. However, they either depend on trial-specific expert rules
that cannot expand to other trials or perform matching at a very general level
with a black-box model where the lack of interpretability makes the model
results difficult to be adopted.
</p>
<p>To provide accurate and interpretable patient trial matching, we introduce a
personalized dynamic tree-based memory network model named TREEMENT. It
utilizes hierarchical clinical ontologies to expand the personalized patient
representation learned from sequential EHR data, and then uses an attentional
beam-search query learned from eligibility criteria embedding to offer a
granular level of alignment for improved performance and interpretability. We
evaluated TREEMENT against existing models on real-world datasets and
demonstrated that TREEMENT outperforms the best baseline by 7% in terms of
error reduction in criteria-level matching and achieves state-of-the-art
results in its trial-level matching ability. Furthermore, we also show TREEMENT
can offer good interpretability to make the model results easier for adoption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09947">U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation. (arXiv:2307.09947v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Landgraf_S/0/1/0/all/0/1">Steven Landgraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Hillemann_M/0/1/0/all/0/1">Markus Hillemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Wursthorn_K/0/1/0/all/0/1">Kira Wursthorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Ulrich_M/0/1/0/all/0/1">Markus Ulrich</a></p>
<p>Deep neural networks have shown exceptional performance in various tasks, but
their lack of robustness, reliability, and tendency to be overconfident pose
challenges for their deployment in safety-critical applications like autonomous
driving. In this regard, quantifying the uncertainty inherent to a model's
prediction is a promising endeavour to address these shortcomings. In this
work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that
incorporates dynamic predictive uncertainties into the training process by
pixel-wise weighting of the well-known cross-entropy loss (CE). Through
extensive experimentation, we demonstrate the superiority of U-CE over regular
CE training on two benchmark datasets, Cityscapes and ACDC, using two common
backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train
models that not only improve their segmentation performance but also provide
meaningful uncertainties after training. Consequently, we contribute to the
development of more robust and reliable segmentation models, ultimately
advancing the state-of-the-art in safety-critical applications and beyond.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09955">XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mengda Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenjia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1">Cheng Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1">Manuela Veloso</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shuran Song</a></p>
<p>Human demonstration videos are a widely available data source for robot
learning and an intuitive user interface for expressing desired behavior.
However, directly extracting reusable robot manipulation skills from
unstructured human videos is challenging due to the big embodiment difference
and unobserved action parameters. To bridge this embodiment gap, this paper
introduces XSkill, an imitation learning framework that 1) discovers a
cross-embodiment representation called skill prototypes purely from unlabeled
human and robot manipulation videos, 2) transfers the skill representation to
robot actions using conditional diffusion policy, and finally, 3) composes the
learned skill to accomplish unseen tasks specified by a human prompt video. Our
experiments in simulation and real-world environments show that the discovered
skill prototypes facilitate both skill transfer and composition for unseen
tasks, resulting in a more general and scalable imitation learning framework.
The performance of XSkill is best understood from the anonymous website:
https://xskillcorl.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09985">Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yu-chen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yitong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1">Aixin Sun</a></p>
<p>A typical benchmark dataset for recommender system (RecSys) evaluation
consists of user-item interactions generated on a platform within a time
period. The interaction generation mechanism partially explains why a user
interacts with (e.g.,like, purchase, rate) an item, and the context of when a
particular interaction happened. In this study, we conduct a meticulous
analysis on the MovieLens dataset and explain the potential impact on using the
dataset for evaluating recommendation algorithms. We make a few main findings
from our analysis. First, there are significant differences in user
interactions at the different stages when a user interacts with the MovieLens
platform. The early interactions largely define the user portrait which affect
the subsequent interactions. Second, user interactions are highly affected by
the candidate movies that are recommended by the platform's internal
recommendation algorithm(s). Removal of interactions that happen nearer to the
last few interactions of a user leads to increasing difficulty in learning user
preference, thus deteriorating recommendation accuracy. Third, changing the
order of user interactions makes it more difficult for sequential algorithms to
capture the progressive interaction process. Based on these findings, we
further discuss the discrepancy between the interaction generation mechanism
that is employed by the MovieLens system and that of typical real world
recommendation scenarios. In summary, models that achieve excellent
recommendation accuracy on the MovieLens dataset may not demonstrate superior
performance in practice for at least two kinds of differences: (i) the
differences in the contexts of user-item interaction generation, and (ii) the
differences in user knowledge about the item collections.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10003">TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aminimehr_A/0/1/0/all/0/1">Amirhossein Aminimehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Khani_P/0/1/0/all/0/1">Pouya Khani</a>, <a href="http://arxiv.org/find/cs/1/au:+Molaei_A/0/1/0/all/0/1">Amirali Molaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazemeini_A/0/1/0/all/0/1">Amirmohammad Kazemeini</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a></p>
<p>The field of Explainable Artificial Intelligence (XAI) aims to improve the
interpretability of black-box machine learning models. Building a heatmap based
on the importance value of input features is a popular method for explaining
the underlying functions of such models in producing their predictions.
Heatmaps are almost understandable to humans, yet they are not without flaws.
Non-expert users, for example, may not fully understand the logic of heatmaps
(the logic in which relevant pixels to the model's prediction are highlighted
with different intensities or colors). Additionally, objects and regions of the
input image that are relevant to the model prediction are frequently not
entirely differentiated by heatmaps. In this paper, we propose a framework
called TbExplain that employs XAI techniques and a pre-trained object detector
to present text-based explanations of scene classification models. Moreover,
TbExplain incorporates a novel method to correct predictions and textually
explain them based on the statistics of objects in the input image when the
initial prediction is unreliable. To assess the trustworthiness and validity of
the text-based explanations, we conducted a qualitative experiment, and the
findings indicated that these explanations are sufficiently reliable.
Furthermore, our quantitative and qualitative experiments on TbExplain with
scene classification datasets reveal an improvement in classification accuracy
over ResNet variants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10004">6G Network Business Support System. (arXiv:2307.10004v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1">Ye Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yaqin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_W/0/1/0/all/0/1">Wen Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Feng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xidong Wang</a></p>
<p>6G is the next-generation intelligent and integrated digital information
infrastructure, characterized by ubiquitous interconnection, native
intelligence, multi-dimensional perception, global coverage, green and
low-carbon, native network security, etc. 6G will realize the transition from
serving people and people-things communication to supporting the efficient
connection of intelligent agents, and comprehensively leading the digital,
intelligent and green transformation of the economy and the society. As the
core support system for mobile communication network, 6 6G BSS need to
integrate with new business models brought about by the development of the
next-generation Internet and IT, upgrade from "network-centric" to "business
and service centric" and "customer-centric". 6G OSS and BSS systems need to
strengthen their integration to improve the operational efficiency and benefits
of customers by connecting the digital intelligence support capabilities on
both sides of supply and demand. This paper provides a detailed introduction to
the overall vision, potential key technologies, and functional architecture of
6G BSS systems. It also presents an evolutionary roadmap and technological
prospects for the BSS systems from 5G to 6G.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10018">Rob\^oCIn Small Size League Extended Team Description Paper for RoboCup 2023. (arXiv:2307.10018v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1">Aline Lima de Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1">Cau&#xea; Addae da Silva Gomes</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1">Cec&#xed;lia Virginia Santos da Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Alves_C/0/1/0/all/0/1">Charles Matheus de Sousa Alves</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_D/0/1/0/all/0/1">Danilo Andrade Martins de Souza</a>, <a href="http://arxiv.org/find/cs/1/au:+Xavier_D/0/1/0/all/0/1">Driele Pires Ferreira Ara&#xfa;jo Xavier</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_E/0/1/0/all/0/1">Edgleyson Pereira da Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_F/0/1/0/all/0/1">Felipe Bezerra Martins</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1">Lucas Henrique Cavalcanti Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Maciel_L/0/1/0/all/0/1">Lucas Dias Maciel</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1">Matheus Paix&#xe3;o Gumercindo dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_M/0/1/0/all/0/1">Matheus Lafayette Vasconcelos</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrade_M/0/1/0/all/0/1">Matheus Vin&#xed;cius Teotonio do Nascimento Andrade</a>, <a href="http://arxiv.org/find/cs/1/au:+Melo_J/0/1/0/all/0/1">Jo&#xe3;o Guilherme Oliveira Carvalho de Melo</a>, <a href="http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1">Jo&#xe3;o Pedro Souza Pereira de Moura</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1">Jos&#xe9; Ronald da Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1">Jos&#xe9; Victor Silva Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Morais_P/0/1/0/all/0/1">Pedro Henrique Santana de Morais</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_P/0/1/0/all/0/1">Pedro Paulo Salman de Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodrigues_R/0/1/0/all/0/1">Riei Joaquim Matos Rodrigues</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandes_R/0/1/0/all/0/1">Roberto Costa Fernandes</a>, <a href="http://arxiv.org/find/cs/1/au:+Morais_R/0/1/0/all/0/1">Ryan Vinicius Santos Morais</a>, <a href="http://arxiv.org/find/cs/1/au:+Teobaldo_T/0/1/0/all/0/1">Tamara Mayara Ramos Teobaldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_W/0/1/0/all/0/1">Washington Igor dos Santos Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Barros_E/0/1/0/all/0/1">Edna Natividade Silva Barros</a></p>
<p>Rob\^oCIn has participated in RoboCup Small Size League since 2019, won its
first world title in 2022 (Division B), and is currently a three-times
Latin-American champion. This paper presents our improvements to defend the
Small Size League (SSL) division B title in RoboCup 2023 in Bordeaux, France.
This paper aims to share some of the academic research that our team developed
over the past year. Our team has successfully published 2 articles related to
SSL at two high-impact conferences: the 25th RoboCup International Symposium
and the 19th IEEE Latin American Robotics Symposium (LARS 2022). Over the last
year, we have been continuously migrating from our past codebase to
Unification. We will describe the new architecture implemented and some points
of software and AI refactoring. In addition, we discuss the process of
integrating machined components into the mechanical system, our development for
participating in the vision blackout challenge last year and what we are
preparing for this year.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10025">An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yulin Zhou</a></p>
<p>Fertility issues are closely related to population security, in 60 years
China's population for the first time in a negative growth trend, the change of
fertility policy is of great concern to the community. 2023 ``two sessions"
proposal ``suggests that the country in the form of legislation, the birth of
the registration of the cancellation of the marriage restriction" This topic
was once a hot topic on the Internet, and ``unbundling" the relationship
between birth registration and marriage has become the focus of social debate.
In this paper, we adopt co-occurrence semantic analysis, topic analysis and
sentiment analysis to conduct multi-granularity semantic analysis of microblog
comments. It is found that the discussion on the proposal of ``removing
marriage restrictions from birth registration" involves the individual, society
and the state at three dimensions, and is detailed into social issues such as
personal behaviour, social ethics and law, and national policy, with people's
sentiment inclined to be negative in most of the topics. Based on this, eight
proposals were made to provide a reference for governmental decision making and
to form a reference method for researching public opinion on political issues.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10032">Automatic Conversion of MiniZinc Programs to QUBO. (arXiv:2307.10032v1 [cs.MS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolf_A/0/1/0/all/0/1">Armin Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Grozea_C/0/1/0/all/0/1">Cristian Grozea</a></p>
<p>Obtaining Quadratic Unconstrained Binary Optimisation models for various
optimisation problems, in order to solve those on physical quantum computers
(such as the the DWave annealers) is nowadays a lengthy and tedious process
that requires one to remodel all problem variables as binary variables and
squeeze the target function and the constraints into a single quadratic
polynomial into these new variables.
</p>
<p>We report here on the basis of our automatic converter from MiniZinc to QUBO,
which is able to process a large set of constraint optimisation and constraint
satisfaction problems and turn them into equivalent QUBOs, effectively
optimising the whole process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10053">Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Xiao_N/0/1/0/all/0/1">Nachuan Xiao</a>, <a href="http://arxiv.org/find/math/1/au:+Hu_X/0/1/0/all/0/1">Xiaoyin Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Toh_K/0/1/0/all/0/1">Kim-Chuan Toh</a></p>
<p>In this paper, we investigate the convergence properties of the stochastic
gradient descent (SGD) method and its variants, especially in training neural
networks built from nonsmooth activation functions. We develop a novel
framework that assigns different timescales to stepsizes for updating the
momentum terms and variables, respectively. Under mild conditions, we prove the
global convergence of our proposed framework in both single-timescale and
two-timescale cases. We show that our proposed framework encompasses a wide
range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion,
normalized SGD and clipped SGD. Furthermore, when the objective function adopts
a finite-sum formulation, we prove the convergence properties for these
SGD-type methods based on our proposed framework. In particular, we prove that
these SGD-type methods find the Clarke stationary points of the objective
function with randomly chosen stepsizes and initial points under mild
assumptions. Preliminary numerical experiments demonstrate the high efficiency
of our analyzed SGD-type methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10057">Ethics in the Age of AI: An Analysis of AI Practitioners&#x27; Awareness and Challenges. (arXiv:2307.10057v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pant_A/0/1/0/all/0/1">Aastha Pant</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoda_R/0/1/0/all/0/1">Rashina Hoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Spiegler_S/0/1/0/all/0/1">Simone V. Spiegler</a>, <a href="http://arxiv.org/find/cs/1/au:+Tantithamthavorn_C/0/1/0/all/0/1">Chakkrit Tantithamthavorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Turhan_B/0/1/0/all/0/1">Burak Turhan</a></p>
<p>Ethics in AI has become a debated topic of public and expert discourse in
recent years. But what do people who build AI - AI practitioners - have to say
about their understanding of AI ethics and the challenges associated with
incorporating it in the AI-based systems they develop? Understanding AI
practitioners' views on AI ethics is important as they are the ones closest to
the AI systems and can bring about changes and improvements. We conducted a
survey aimed at understanding AI practitioners' awareness of AI ethics and
their challenges in incorporating ethics. Based on 100 AI practitioners'
responses, our findings indicate that majority of AI practitioners had a
reasonable familiarity with the concept of AI ethics, primarily due to
workplace rules and policies. Privacy protection and security was the ethical
principle that majority of them were aware of. Formal education/training was
considered somewhat helpful in preparing practitioners to incorporate AI
ethics. The challenges that AI practitioners faced in the development of
ethical AI-based systems included (i) general challenges, (ii)
technology-related challenges and (iii) human-related challenges. We also
identified areas needing further investigation and provided recommendations to
assist AI practitioners and companies in incorporating ethics into AI
development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10060">Accurate deep learning sub-grid scale models for large eddy simulations. (arXiv:2307.10060v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Bose_R/0/1/0/all/0/1">Rikhi Bose</a>, <a href="http://arxiv.org/find/physics/1/au:+Roy_A/0/1/0/all/0/1">Arunabha M. Roy</a></p>
<p>We present two families of sub-grid scale (SGS) turbulence models developed
for large-eddy simulation (LES) purposes. Their development required the
formulation of physics-informed robust and efficient Deep Learning (DL)
algorithms which, unlike state-of-the-art analytical modeling techniques can
produce high-order complex non-linear relations between inputs and outputs.
Explicit filtering of data from direct simulations of the canonical channel
flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided
accurate data for training and testing. The two sets of models use different
network architectures. One of the architectures uses tensor basis neural
networks (TBNN) and embeds the simplified analytical model form of the general
effective-viscosity hypothesis, thus incorporating the Galilean, rotational and
reflectional invariances. The other architecture is that of a relatively simple
network, that is able to incorporate the Galilean invariance only. However,
this simpler architecture has better feature extraction capacity owing to its
ability to establish relations between and extract information from
cross-components of the integrity basis tensors and the SGS stresses. Both sets
of models are used to predict the SGS stresses for feature datasets generated
with different filter widths, and at different Reynolds numbers. It is shown
that due to the simpler model's better feature learning capabilities, it
outperforms the invariance embedded model in statistical performance metrics.
In a priori tests, both sets of models provide similar levels of dissipation
and backscatter. Based on the test results, both sets of models should be
usable in a posteriori actual LESs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10085">A decision making framework for recommended maintenance of road segments. (arXiv:2307.10085v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haoyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yan Yan</a></p>
<p>With the rapid development of global road transportation, countries worldwide
have completed the construction of road networks. However, the ensuing
challenge lies in the maintenance of existing roads. It is well-known that
countries allocate limited budgets to road maintenance projects, and road
management departments face difficulties in making scientifically informed
maintenance decisions. Therefore, integrating various artificial intelligence
decision-making techniques to thoroughly explore historical maintenance data
and adapt them to the context of road maintenance scientific decision-making
has become an urgent issue. This integration aims to provide road management
departments with more scientific tools and evidence for decision-making. The
framework proposed in this paper primarily addresses the following four issues:
1) predicting the pavement performance of various routes, 2) determining the
prioritization of maintenance routes, 3) making maintenance decisions based on
the evaluation of the effects of past maintenance, and considering
comprehensive technical and management indicators, and 4) determining the
prioritization of maintenance sections based on the maintenance effectiveness
and recommended maintenance effectiveness. By tackling these four problems, the
framework enables intelligent decision-making for the optimal maintenance plan
and maintenance sections, taking into account limited funding and historical
maintenance management experience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10112">Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mueller_T/0/1/0/all/0/1">Tamara T. Mueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Starck_S/0/1/0/all/0/1">Sophie Starck</a>, <a href="http://arxiv.org/find/cs/1/au:+Feiner_L/0/1/0/all/0/1">Leonhard F. Feiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1">Kyriaki-Margarita Bintsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1">Georgios Kaissis</a></p>
<p>When re-structuring patient cohorts into so-called population graphs,
initially independent data points can be incorporated into one interconnected
graph structure. This population graph can then be used for medical downstream
tasks using graph neural networks (GNNs). The construction of a suitable graph
structure is a challenging step in the learning pipeline that can have severe
impact on model performance. To this end, different graph assessment metrics
have been introduced to evaluate graph structures. However, these metrics are
limited to classification tasks and discrete adjacency matrices, only covering
a small subset of real-world applications. In this work, we introduce extended
graph assessment metrics (GAMs) for regression tasks and continuous adjacency
matrices. We focus on two GAMs in specific: \textit{homophily} and
\textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of
GAMs to more than one hop, define homophily for regression tasks, as well as
continuous adjacency matrices, and propose a light-weight CCNS distance for
discrete and continuous adjacency matrices. We show the correlation of these
metrics with model performance on different medical population graphs and under
different learning settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10142">Benchmarking Potential Based Rewards for Learning Humanoid Locomotion. (arXiv:2307.10142v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1">Se Hwan Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Heim_S/0/1/0/all/0/1">Steve Heim</a>, <a href="http://arxiv.org/find/cs/1/au:+Khazoom_C/0/1/0/all/0/1">Charles Khazoom</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangbae Kim</a></p>
<p>The main challenge in developing effective reinforcement learning (RL)
pipelines is often the design and tuning the reward functions. Well-designed
shaping reward can lead to significantly faster learning. Naively formulated
rewards, however, can conflict with the desired behavior and result in
overfitting or even erratic performance if not properly tuned. In theory, the
broad class of potential based reward shaping (PBRS) can help guide the
learning process without affecting the optimal policy. Although several studies
have explored the use of potential based reward shaping to accelerate learning
convergence, most have been limited to grid-worlds and low-dimensional systems,
and RL in robotics has predominantly relied on standard forms of reward
shaping. In this paper, we benchmark standard forms of shaping with PBRS for a
humanoid robot. We find that in this high-dimensional system, PBRS has only
marginal benefits in convergence speed. However, the PBRS reward terms are
significantly more robust to scaling than typical reward shaping approaches,
and thus easier to tune.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10160">Robust Driving Policy Learning with Guided Meta Reinforcement Learning. (arXiv:2307.10160v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kanghoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Isele_D/0/1/0/all/0/1">David Isele</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinkyoo Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujimura_K/0/1/0/all/0/1">Kikuo Fujimura</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a></p>
<p>Although deep reinforcement learning (DRL) has shown promising results for
autonomous navigation in interactive traffic scenarios, existing work typically
adopts a fixed behavior policy to control social vehicles in the training
environment. This may cause the learned driving policy to overfit the
environment, making it difficult to interact well with vehicles with different,
unseen behaviors. In this work, we introduce an efficient method to train
diverse driving policies for social vehicles as a single meta-policy. By
randomizing the interaction-based reward functions of social vehicles, we can
generate diverse objectives and efficiently train the meta-policy through
guiding policies that achieve specific objectives. We further propose a
training strategy to enhance the robustness of the ego vehicle's driving policy
using the environment where social vehicles are controlled by the learned
meta-policy. Our method successfully learns an ego driving policy that
generalizes well to unseen situations with out-of-distribution (OOD) social
agents' behaviors in a challenging uncontrolled T-intersection scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10169">Challenges and Applications of Large Language Models. (arXiv:2307.10169v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_J/0/1/0/all/0/1">Joshua Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1">Maximilian Mozes</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1">Herbie Bradley</a>, <a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1">Roberta Raileanu</a>, <a href="http://arxiv.org/find/cs/1/au:+McHardy_R/0/1/0/all/0/1">Robert McHardy</a></p>
<p>Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10171">LightPath: Lightweight and Scalable Path Representation Learning. (arXiv:2307.10171v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sean Bin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jilin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chenjuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1">Christian S. Jensen</a></p>
<p>Movement paths are used widely in intelligent transportation and smart city
applications. To serve such applications, path representation learning aims to
provide compact representations of paths that enable efficient and accurate
operations when used for different downstream tasks such as path ranking and
travel cost estimation. In many cases, it is attractive that the path
representation learning is lightweight and scalable; in resource-limited
environments and under green computing limitations, it is essential. Yet,
existing path representation learning studies focus on accuracy and pay at most
secondary attention to resource consumption and scalability.
</p>
<p>We propose a lightweight and scalable path representation learning framework,
termed LightPath, that aims to reduce resource consumption and achieve
scalability without affecting accuracy, thus enabling broader applicability.
More specifically, we first propose a sparse auto-encoder that ensures that the
framework achieves good scalability with respect to path length. Next, we
propose a relational reasoning framework to enable faster training of more
robust sparse path encoders. We also propose global-local knowledge
distillation to further reduce the size and improve the performance of sparse
path encoders. Finally, we report extensive experiments on two real-world
datasets to offer insight into the efficiency, scalability, and effectiveness
of the proposed framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10172">DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianguo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1">Kun Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1">Shelby Heinecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1">Rui Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Ye Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhou Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a></p>
<p>Despite advancements in conversational AI, language models encounter
challenges to handle diverse conversational tasks, and existing dialogue
dataset collections often lack diversity and comprehensiveness. To tackle these
issues, we introduce DialogStudio: the largest and most diverse collection of
dialogue datasets, unified under a consistent format while preserving their
original information. Our collection encompasses data from open-domain
dialogues, task-oriented dialogues, natural language understanding,
conversational recommendation, dialogue summarization, and knowledge-grounded
dialogues, making it an incredibly rich and diverse resource for dialogue
research and model training. To further enhance the utility of DialogStudio, we
identify the licenses for each dataset and design domain-aware prompts for
selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we
develop conversational AI models using the dataset collection, and our
experiments in both zero-shot and few-shot learning scenarios demonstrate the
superiority of DialogStudio. To improve transparency and support dataset and
task-based research, as well as language model pre-training, all datasets,
licenses, codes, and models associated with DialogStudio are made publicly
accessible at https://github.com/salesforce/DialogStudio
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1912.13122">Declarative Mechanism Design. (arXiv:1912.13122v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcia_Camino_A/0/1/0/all/0/1">Andr&#xe9;s Garc&#xed;a-Camino</a></p>
<p>Regulation of Multi-Agent Systems (MAS) and Declarative Electronic
Institutions (DEIs) was a multidisciplinary research topic of the past decade
involving (Physical and Software) Agents and Law since the beginning, but
recently evolved towards News-claimed Robot Lawyer since 2016. One of these
first proposals of restricting the behaviour of Software Agentswas Electronic
Institutions.However, with the recent reformulation of Artificial Neural
Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal
issues regarding the use of DL has raised concerns in the Artificial
Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly
addressed, we propose the Regulation of Artificial Neural Networks as
Agent-based Training of a special type of regulated Artificial Neural Network
that we call Institutional Neural Network (INN).The main purpose of this paper
is to bring attention to Artificial Teaching (AT) and to give a tentative
answer showing a proof-of-concept implementation of Regulated Deep Learning
(RDL). This paper introduces the former concept and provide sI, a language
previously used to model declaratively and extend Electronic Institutions, as a
means to regulate the execution of Artificial Neural Networks and their
interactions with Artificial Teachers (ATs)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.07677">Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1">Christine Herlihy</a>, <a href="http://arxiv.org/find/cs/1/au:+Prins_A/0/1/0/all/0/1">Aviva Prins</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1">Aravind Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P. Dickerson</a></p>
<p>Restless and collapsing bandits are often used to model budget-constrained
resource allocation in settings where arms have action-dependent transition
probabilities, such as the allocation of health interventions among patients.
However, state-of-the-art Whittle-index-based approaches to this planning
problem either do not consider fairness among arms, or incentivize fairness
without guaranteeing it. We thus introduce ProbFair, a probabilistically fair
policy that maximizes total expected reward and satisfies the budget constraint
while ensuring a strictly positive lower bound on the probability of being
pulled at each timestep. We evaluate our algorithm on a real-world application,
where interventions support continuous positive airway pressure (CPAP) therapy
adherence among patients, as well as on a broader class of synthetic transition
matrices. We find that ProbFair preserves utility while providing fairness
guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03597">Meta-Learning Parameterized Skills. (arXiv:2206.03597v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Haotian Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangqun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1">Saket Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Littman_M/0/1/0/all/0/1">Michael Littman</a>, <a href="http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1">George Konidaris</a></p>
<p>We propose a novel parameterized skill-learning algorithm that aims to learn
transferable parameterized skills and synthesize them into a new action space
that supports efficient learning in long-horizon tasks. We propose to leverage
off-policy Meta-RL combined with a trajectory-centric smoothness term to learn
a set of parameterized skills. Our agent can use these learned skills to
construct a three-level hierarchical framework that models a
Temporally-extended Parameterized Action Markov Decision Process. We
empirically demonstrate that the proposed algorithms enable an agent to solve a
set of difficult long-horizon (obstacle-course and robot manipulation) tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04166">Planning with Dynamically Estimated Action Costs. (arXiv:2206.04166v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1">Eyal Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaminka_G/0/1/0/all/0/1">Gal A. Kaminka</a></p>
<p>Information about action costs is critical for real-world AI planning
applications. Rather than rely solely on declarative action models, recent
approaches also use black-box external action cost estimators, often learned
from data, that are applied during the planning phase. These, however, can be
computationally expensive, and produce uncertain values. In this paper we
suggest a generalization of deterministic planning with action costs that
allows selecting between multiple estimators for action cost, to balance
computation time against bounded estimation uncertainty. This enables a much
richer -- and correspondingly more realistic -- problem representation.
Importantly, it allows planners to bound plan accuracy, thereby increasing
reliability, while reducing unnecessary computational burden, which is critical
for scaling to large problems. We introduce a search algorithm, generalizing
$A^*$, that solves such planning problems, and additional algorithmic
extensions. In addition to theoretical guarantees, extensive experiments show
considerable savings in runtime compared to alternatives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.07734">Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1">Jaemin Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tiancheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1">Leman Akoglu</a></p>
<p>Self-supervised learning (SSL) has emerged as a promising alternative to
create supervisory signals to real-world problems, avoiding the extensive cost
of manual labeling. SSL is particularly attractive for unsupervised tasks such
as anomaly detection (AD), where labeled anomalies are rare or often
nonexistent. A large catalog of augmentation functions has been used for
SSL-based AD (SSAD) on image data, and recent works have reported that the type
of augmentation has a significant impact on accuracy. Motivated by those, this
work sets out to put image-based SSAD under a larger lens and investigate the
role of data augmentation in SSAD. Through extensive experiments on 3 different
detector models and across 420 AD tasks, we provide comprehensive numerical and
visual evidences that the alignment between data augmentation and
anomaly-generating mechanism is the key to the success of SSAD, and in the lack
thereof, SSL may even impair accuracy. To the best of our knowledge, this is
the first meta-analysis on the role of data augmentation in SSAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10967">The Value of Out-of-Distribution Data. (arXiv:2208.10967v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1">Ashwin De Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1">Rahul Ramesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1">Carey E. Priebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1">Pratik Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1">Joshua T. Vogelstein</a></p>
<p>We expect the generalization error to improve with more samples from a
similar task, and to deteriorate with more samples from an out-of-distribution
(OOD) task. In this work, we show a counter-intuitive phenomenon: the
generalization error of a task can be a non-monotonic function of the number of
OOD samples. As the number of OOD samples increases, the generalization error
on the target task improves before deteriorating beyond a threshold. In other
words, there is value in training on small amounts of OOD data. We use Fisher's
Linear Discriminant on synthetic datasets and deep networks on computer vision
benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate
and analyze this phenomenon. In the idealistic setting where we know which
samples are OOD, we show that these non-monotonic trends can be exploited using
an appropriately weighted objective of the target and OOD empirical risk. While
its practical utility is limited, this does suggest that if we can detect OOD
samples, then there may be ways to benefit from them. When we do not know which
samples are OOD, we show how a number of go-to strategies such as
data-augmentation, hyper-parameter optimization, and pre-training are not
enough to ensure that the target generalization error does not deteriorate with
the number of OOD samples in the dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12547">SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems. (arXiv:2210.12547v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1">Aaron Ferber</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Taoan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1">Martin Schubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1">Benoit Steiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1">Bistra Dilkina</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a></p>
<p>Optimization problems with nonlinear cost functions and combinatorial
constraints appear in many real-world applications but remain challenging to
solve efficiently compared to their linear counterparts. To bridge this gap, we
propose $\textbf{SurCo}$ that learns linear $\underline{\text{Sur}}$rogate
costs which can be used in existing $\underline{\text{Co}}$mbinatorial solvers
to output good solutions to the original nonlinear combinatorial optimization
problem. The surrogate costs are learned end-to-end with nonlinear loss by
differentiating through the linear surrogate solver, combining the flexibility
of gradient-based methods with the structure of linear combinatorial
optimization. We propose three $\texttt{SurCo}$ variants:
$\texttt{SurCo}-\texttt{zero}$ for individual nonlinear problems,
$\texttt{SurCo}-\texttt{prior}$ for problem distributions, and
$\texttt{SurCo}-\texttt{hybrid}$ to combine both distribution and
problem-specific information. We give theoretical intuition motivating
$\texttt{SurCo}$, and evaluate it empirically. Experiments show that
$\texttt{SurCo}$ finds better solutions faster than state-of-the-art and domain
expert approaches in real-world optimization problems such as embedding table
sharding, inverse photonic design, and nonlinear route planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01692">Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1">Michal &#x160;tef&#xe1;nik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1">Marek Kadl&#x10d;&#xed;k</a></p>
<p>Language models exhibit an emergent ability to learn a new task from a small
number of input-output demonstrations. However, recent work shows that
in-context learners largely rely on their pre-trained knowledge, such as the
sentiment of the labels, instead of learning new associations from the input.
We argue that the commonly-used few-shot evaluation using a random selection of
in-context demonstrations can not disentangle models' reliance on such biases,
as most of the randomly-selected demonstrations do not present relations
informative for prediction beyond exposing the task's input-output
distribution.
</p>
<p>Therefore, to evaluate models' in-context learning ability independent of
models' memory, we introduce a Concept-sharing few-shot learning method
choosing the demonstrations that share an underlying concept with the predicted
sample. We extract a set of such concepts from available human explanations and
measure how much models can benefit from presenting these concepts in few-shot
demonstrations.
</p>
<p>We find that most of the recent in-context learners can not consistently
benefit from the demonstrated concepts, irrespective of the model size.
However, we note that T0 models are more sensitive to exhibited concepts,
benefiting from concept-sharing demonstrations in 7 out of 8 evaluation
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.07253">API-Miner: an API-to-API Specification Recommendation Engine. (arXiv:2212.07253v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Sae Young Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerr_G/0/1/0/all/0/1">Gregor Kerr</a>, <a href="http://arxiv.org/find/cs/1/au:+Silavong_F/0/1/0/all/0/1">Fran Silavong</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1">Sean Moran</a></p>
<p>When designing a new API for a large project, developers need to make smart
design choices so that their code base can grow sustainably. To ensure that new
API components are well designed, developers can learn from existing API
components. However, the lack of standardized methods for comparing API designs
makes this learning process time-consuming and difficult. To address this gap
we developed API-Miner, to the best of our knowledge, one of the first
API-to-API specification recommendation engines. API-Miner retrieves relevant
specification components written in OpenAPI (a widely adopted language used to
describe web APIs). API-miner presents several significant contributions,
including: (1) novel methods of processing and extracting key information from
OpenAPI specifications, (2) innovative feature extraction techniques that are
optimized for the highly technical API specification domain, and (3) a novel
log-linear probabilistic model that combines multiple signals to retrieve
relevant and high quality OpenAPI specification components given a query
specification. We evaluate API-Miner in both quantitative and qualitative tasks
and achieve an overall of 91.7% recall@1 and 56.2% F1, which surpasses baseline
performance by 15.4% in recall@1 and 3.2% in F1. Overall, API-Miner will allow
developers to retrieve relevant OpenAPI specification components from a public
or internal database in the early stages of the API development cycle, so that
they can learn from existing established examples and potentially identify
redundancies in their work. It provides the guidance developers need to
accelerate development process and contribute thoughtfully designed APIs that
promote code maintainability and quality. Code is available on GitHub at
https://github.com/jpmorganchase/api-miner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.10551">Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation. (arXiv:2212.10551v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1">Fei Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yinquan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">WenHao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Lingpeng Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingjing Xu</a></p>
<p>Multilingual neural machine translation (MNMT) aims to build a unified model
for many language directions. Existing monolithic models for MNMT encounter two
challenges: parameter interference among languages and inefficient inference
for large models. In this paper, we revisit the classic multi-way structures
and develop a detachable model by assigning each language (or group of
languages) to an individual branch that supports plug-and-play training and
inference. To address the needs of learning representations for all languages
in a unified space, we propose a novel efficient training recipe, upon which we
build an effective detachable model, Lego-MT. For a fair comparison, we collect
data from OPUS and build a translation benchmark covering 433 languages and
1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings
an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters.
The proposed training recipe brings a 28.2$\times$ speedup over the
conventional multi-way training method.\footnote{
\url{https://github.com/CONE-MT/Lego-MT}.}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11596">ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1">Simon Ott</a>, <a href="http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1">Konstantin Hebenstreit</a>, <a href="http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1">Valentin Li&#xe9;vin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1">Christoffer Egeberg Hother</a>, <a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1">Milad Moradi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayrhauser_M/0/1/0/all/0/1">Maximilian Mayrhauser</a>, <a href="http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1">Robert Praas</a>, <a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1">Ole Winther</a>, <a href="http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1">Matthias Samwald</a></p>
<p>Large language models (LLMs) such as GPT-4 have recently demonstrated
impressive results across a wide range of tasks. LLMs are still limited,
however, in that they frequently fail at complex reasoning, their reasoning
processes are opaque, they are prone to 'hallucinate' facts, and there are
concerns about their underlying biases. Letting models verbalize reasoning
steps as natural language, a technique known as chain-of-thought prompting, has
recently been proposed as a way to address some of these issues. Here we
present ThoughtSource, a meta-dataset and software library for chain-of-thought
(CoT) reasoning. The goal of ThoughtSource is to improve future artificial
intelligence systems by facilitating qualitative understanding of CoTs,
enabling empirical evaluations, and providing training data. This first release
of ThoughtSource integrates six scientific/medical, three general-domain and
five math word question answering datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01561">Hierarchically Composing Level Generators for the Creation of Complex Structures. (arXiv:2302.01561v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1">Michael Beukman</a>, <a href="http://arxiv.org/find/cs/1/au:+Fokam_M/0/1/0/all/0/1">Manuel Fokam</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruger_M/0/1/0/all/0/1">Marcel Kruger</a>, <a href="http://arxiv.org/find/cs/1/au:+Axelrod_G/0/1/0/all/0/1">Guy Axelrod</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1">Muhammad Nasir</a>, <a href="http://arxiv.org/find/cs/1/au:+Ingram_B/0/1/0/all/0/1">Branden Ingram</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1">Benjamin Rosman</a>, <a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1">Steven James</a></p>
<p>Procedural content generation (PCG) is a growing field, with numerous
applications in the video game industry and great potential to help create
better games at a fraction of the cost of manual creation. However, much of the
work in PCG is focused on generating relatively straightforward levels in
simple games, as it is challenging to design an optimisable objective function
for complex settings. This limits the applicability of PCG to more complex and
modern titles, hindering its adoption in industry. Our work aims to address
this limitation by introducing a compositional level generation method that
recursively composes simple low-level generators to construct large and complex
creations. This approach allows for easily-optimisable objectives and the
ability to design a complex structure in an interpretable way by referencing
lower-level components. We empirically demonstrate that our method outperforms
a non-compositional baseline by more accurately satisfying a designer's
functional requirements in several tasks. Finally, we provide a qualitative
showcase (in Minecraft) illustrating the large and complex, but still coherent,
structures that were generated using simple base generators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02330">CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning. (arXiv:2302.02330v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1">Jochen Triesch</a></p>
<p>Self-supervised representation learning (SSRL) methods have shown great
success in computer vision. In recent studies, augmentation-based contrastive
learning methods have been proposed for learning representations that are
invariant or equivariant to pre-defined data augmentation operations. However,
invariant or equivariant features favor only specific downstream tasks
depending on the augmentations chosen. They may result in poor performance when
the learned representation does not match task requirements. Here, we consider
an active observer that can manipulate views of an object and has knowledge of
the action(s) that generated each view. We introduce Contrastive Invariant and
Predictive Equivariant Representation learning (CIPER). CIPER comprises both
invariant and equivariant learning objectives using one shared encoder and two
different output heads on top of the encoder. One output head is a projection
head with a state-of-the-art contrastive objective to encourage invariance to
augmentations. The other is a prediction head estimating the augmentation
parameters, capturing equivariant features. Both heads are discarded after
training and only the encoder is used for downstream tasks. We evaluate our
method on static image tasks and time-augmented image datasets. Our results
show that CIPER outperforms a baseline contrastive method on various tasks.
Interestingly, CIPER encourages the formation of hierarchically structured
representations where different views of an object become systematically
organized in the latent representation space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07265">The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus. (arXiv:2302.07265v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1">Anna Hedstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Bommer_P/0/1/0/all/0/1">Philine Bommer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wickstrom_K/0/1/0/all/0/1">Kristoffer K. Wickstr&#xf8;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1">Sebastian Lapuschkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a></p>
<p>One of the unsolved challenges in the field of Explainable AI (XAI) is
determining how to most reliably estimate the quality of an explanation method
in the absence of ground truth explanation labels. Resolving this issue is of
utmost importance as the evaluation outcomes generated by competing evaluation
methods (or ''quality estimators''), which aim at measuring the same property
of an explanation method, frequently present conflicting rankings. Such
disagreements can be challenging for practitioners to interpret, thereby
complicating their ability to select the best-performing explanation method. We
address this problem through a meta-evaluation of different quality estimators
in XAI, which we define as ''the process of evaluating the evaluation method''.
Our novel framework, MetaQuantus, analyses two complementary performance
characteristics of a quality estimator: its resilience to noise and reactivity
to randomness, thus circumventing the need for ground truth labels. We
demonstrate the effectiveness of our framework through a series of experiments,
targeting various open questions in XAI such as the selection and
hyperparameter optimisation of quality estimators. Our work is released under
an open-source license (https://github.com/annahedstroem/MetaQuantus) to serve
as a development tool for XAI- and Machine Learning (ML) practitioners to
verify and benchmark newly constructed quality estimators in a given
explainability context. With this work, we provide the community with clear and
theoretically-grounded guidance for identifying reliable evaluation methods,
thus facilitating reproducibility in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14015">CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design. (arXiv:2302.14015v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ivanova_D/0/1/0/all/0/1">Desi R. Ivanova</a>, <a href="http://arxiv.org/find/stat/1/au:+Jennings_J/0/1/0/all/0/1">Joel Jennings</a>, <a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1">Adam Foster</a></p>
<p>We formalize the problem of contextual optimization through the lens of
Bayesian experimental design and propose CO-BED -- a general, model-agnostic
framework for designing contextual experiments using information-theoretic
principles. After formulating a suitable information-based objective, we employ
black-box variational methods to simultaneously estimate it and optimize the
designs in a single stochastic gradient scheme. In addition, to accommodate
discrete actions within our framework, we propose leveraging continuous
relaxation schemes, which can naturally be integrated into our variational
objective. As a result, CO-BED provides a general and automated solution to a
wide range of contextual optimization problems. We illustrate its effectiveness
in a number of experiments, where CO-BED demonstrates competitive performance
even when compared to bespoke, model-specific alternatives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06635">Schema Inference for Interpretable Image Classification. (arXiv:2303.06635v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haofei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1">Mengqi Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaokang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kaixuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jie Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Mingli Song</a></p>
<p>In this paper, we study a novel inference paradigm, termed as schema
inference, that learns to deductively infer the explainable predictions by
rebuilding the prior deep neural network (DNN) forwarding scheme, guided by the
prevalent philosophical cognitive concept of schema. We strive to reformulate
the conventional model inference pipeline into a graph matching policy that
associates the extracted visual concepts of an image with the pre-computed
scene impression, by analogy with human reasoning mechanism via impression
matching. To this end, we devise an elaborated architecture, termed as
SchemaNet, as a dedicated instantiation of the proposed schema inference
concept, that models both the visual semantics of input instances and the
learned abstract imaginations of target categories as topological relational
graphs. Meanwhile, to capture and leverage the compositional contributions of
visual semantics in a global view, we also introduce a universal Feat2Graph
scheme in SchemaNet to establish the relational graphs that contain abundant
interaction information. Both the theoretical analysis and the experimental
results on several benchmarks demonstrate that the proposed schema inference
achieves encouraging performance and meanwhile yields a clear picture of the
deductive process leading to the predictions. Our code is available at
https://github.com/zhfeing/SchemaNet-PyTorch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11103">Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling. (arXiv:2303.11103v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1">Jakob Hoydis</a>, <a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1">Fay&#xe7;al A&#xef;t Aoudia</a>, <a href="http://arxiv.org/find/cs/1/au:+Cammerer_S/0/1/0/all/0/1">Sebastian Cammerer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nimier_David_M/0/1/0/all/0/1">Merlin Nimier-David</a>, <a href="http://arxiv.org/find/cs/1/au:+Binder_N/0/1/0/all/0/1">Nikolaus Binder</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1">Guillermo Marcus</a>, <a href="http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1">Alexander Keller</a></p>
<p>Sionna is a GPU-accelerated open-source library for link-level simulations
based on TensorFlow. Since release v0.14 it integrates a differentiable ray
tracer (RT) for the simulation of radio wave propagation. This unique feature
allows for the computation of gradients of the channel impulse response and
other related quantities with respect to many system and environment
parameters, such as material properties, antenna patterns, array geometries, as
well as transmitter and receiver orientations and positions. In this paper, we
outline the key components of Sionna RT and showcase example applications such
as learning radio materials and optimizing transmitter orientations by gradient
descent. While classic ray tracing is a crucial tool for 6G research topics
like reconfigurable intelligent surfaces, integrated sensing and
communications, as well as user localization, differentiable ray tracing is a
key enabler for many novel and exciting research directions, for example,
digital twins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.13126">MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models. (arXiv:2303.13126v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Heliang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1">Long Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenjing Yang</a></p>
<p>The advent of open-source AI communities has produced a cornucopia of
powerful text-guided diffusion models that are trained on various datasets.
While few explorations have been conducted on ensembling such models to combine
their strengths. In this work, we propose a simple yet effective method called
Saliency-aware Noise Blending (SNB) that can empower the fused text-guided
diffusion models to achieve more controllable generation. Specifically, we
experimentally find that the responses of classifier-free guidance are highly
related to the saliency of generated images. Thus we propose to trust different
models in their areas of expertise by blending the predicted noises of two
diffusion models in a saliency-aware manner. SNB is training-free and can be
completed within a DDIM sampling process. Additionally, it can automatically
align the semantics of two noise spaces without requiring additional
annotations such as masks. Extensive experiments show the impressive
effectiveness of SNB in various applications. Project page is available at
https://magicfusion.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06403">Leveraging triplet loss for unsupervised action segmentation. (arXiv:2304.06403v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bueno_Benito_E/0/1/0/all/0/1">E. Bueno-Benito</a>, <a href="http://arxiv.org/find/cs/1/au:+Tura_B/0/1/0/all/0/1">B. Tura</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1">M. Dimiccoli</a></p>
<p>In this paper, we propose a novel fully unsupervised framework that learns
action representations suitable for the action segmentation task from the
single input video itself, without requiring any training data. Our method is a
deep metric learning approach rooted in a shallow network with a triplet loss
operating on similarity distributions and a novel triplet selection strategy
that effectively models temporal and semantic priors to discover actions in the
new representational space. Under these circumstances, we successfully recover
temporal boundaries in the learned action representations with higher quality
compared with existing unsupervised approaches. The proposed method is
evaluated on two widely used benchmark datasets for the action segmentation
task and it achieves competitive performance by applying a generic clustering
algorithm on the learned representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.10727">RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models. (arXiv:2304.10727v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Seulki Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Um_D/0/1/0/all/0/1">Daeho Um</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1">Hajung Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1">Sanghyuk Chun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Sangdoo Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jin Young Choi</a></p>
<p>In this paper, we propose a robustness benchmark for image-text matching
models to assess their vulnerabilities. To this end, we insert adversarial
texts and images into the search pool (i.e., gallery set) and evaluate models
with the adversarial data. Specifically, we replace a word in the text to
change the meaning of the text and mix images with different images to create
perceptible changes in pixels. We assume that such explicit alterations would
not deceive a robust model, as they should understand the holistic meaning of
texts and images simultaneously. However, in our evaluations on the proposed
benchmark, many state-of-the-art models show significant performance
degradation, e.g., Recall@1: 81.9% $\rightarrow$ 64.5% in BLIP, 66.1%
$\rightarrow$ 37.5% in VSE$\infty$, where the models favor adversarial
texts/images over the original ones. This reveals the current vision-language
models may not account for subtle changes or understand the overall context of
texts and images. Our findings can provide insights for improving the
robustness of the vision-language models and devising more diverse stress-test
methods in cross-modal retrieval task. Source code and dataset will be
available at https://github.com/pseulki/rococo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12562">Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting. (arXiv:2304.12562v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianzhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_N/0/1/0/all/0/1">Nan Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yinglin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chuang Liu</a></p>
<p>Recently, various illustrative examples have shown the impressive ability of
generative large language models (LLMs) to perform NLP related tasks. ChatGPT
undoubtedly is the most representative model. We empirically evaluate ChatGPT's
performance on requirements information retrieval (IR) tasks to derive insights
into designing or developing more effective requirements retrieval methods or
tools based on generative LLMs. We design an evaluation framework considering
four different combinations of two popular IR tasks and two common artifact
types. Under zero-shot setting, evaluation results reveal ChatGPT's promising
ability to retrieve requirements relevant information (high recall) and limited
ability to retrieve more specific requirements information (low precision). Our
evaluation of ChatGPT on requirements IR under zero-shot setting provides
preliminary evidence for designing or developing more effective requirements IR
methods or tools based on LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00909">Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v4 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wenqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharan_S/0/1/0/all/0/1">S P Sharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1">Ajay Kumar Jaiswal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kevin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1">Yihan Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>For a complicated algorithm, its implementation by a human programmer usually
starts with outlining a rough control flow followed by iterative enrichments,
eventually yielding carefully generated syntactic structures and variables in a
hierarchy. However, state-of-the-art large language models generate codes in a
single pass, without intermediate warm-ups to reflect the structured thought
process of "outline-then-detail". Inspired by the recent success of
chain-of-thought prompting, we propose ChainCoder, a program synthesis language
model that generates Python code progressively, i.e. from coarse to fine in
multiple passes. We first decompose source code into layout frame components
and accessory components via abstract syntax tree parsing to construct a
hierarchical representation. We then reform our prediction target into a
multi-pass objective, each pass generates a subsequence, which is concatenated
in the hierarchy. Finally, a tailored transformer architecture is leveraged to
jointly encode the natural language descriptions and syntactically aligned I/O
data samples. Extensive evaluations show that ChainCoder outperforms
state-of-the-arts, demonstrating that our progressive generation eases the
reasoning procedure and guides the language model to generate higher-quality
solutions. Our codes are available at:
https://github.com/VITA-Group/ChainCoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09378">Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v4 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanyam Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrestha_A/0/1/0/all/0/1">Aarati Shrestha</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichele_S/0/1/0/all/0/1">Stefano Nichele</a></p>
<p>This research project investigates Lenia, an artificial life platform that
simulates ecosystems of digital creatures. Lenia's ecosystem consists of
simple, artificial organisms that can move, consume, grow, and reproduce. The
platform is important as a tool for studying artificial life and evolution, as
it provides a scalable and flexible environment for creating a diverse range of
organisms with varying abilities and behaviors. Measuring complexity in Lenia
is a key aspect of the study, which identifies the metrics for measuring
long-term complex emerging behavior of rules, with the aim of evolving better
Lenia behaviors which are yet not discovered. The Genetic Algorithm uses
neighborhoods or kernels as genotype while keeping the rest of the parameters
of Lenia as fixed, for example growth function, to produce different behaviors
respective to the population and then measures fitness value to decide the
complexity of the resulting behavior. First, we use Variation over Time as a
fitness function where higher variance between the frames are rewarded. Second,
we use Auto-encoder based fitness where variation of the list of reconstruction
loss for the frames is rewarded. Third, we perform combined fitness where
higher variation of the pixel density of reconstructed frames is rewarded. All
three experiments are tweaked with pixel alive threshold and frames used.
Finally, after performing nine experiments of each fitness for 500 generations,
we pick configurations from all experiments such that there is a scope of
further evolution, and run it for 2500 generations. Results show that the
kernel's center of mass increases with a specific set of pixels and together
with borders the kernel try to achieve a Gaussian distribution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12239">Off-Policy Average Reward Actor-Critic with Deterministic Policy Search. (arXiv:2305.12239v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saxena_N/0/1/0/all/0/1">Naman Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Khastigir_S/0/1/0/all/0/1">Subhojyoti Khastigir</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolathaya_S/0/1/0/all/0/1">Shishir Kolathaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1">Shalabh Bhatnagar</a></p>
<p>The average reward criterion is relatively less studied as most existing
works in the Reinforcement Learning literature consider the discounted reward
criterion. There are few recent works that present on-policy average reward
actor-critic algorithms, but average reward off-policy actor-critic is
relatively less explored. In this work, we present both on-policy and
off-policy deterministic policy gradient theorems for the average reward
performance criterion. Using these theorems, we also present an Average Reward
Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first
show asymptotic convergence analysis using the ODE-based method. Subsequently,
we provide a finite time analysis of the resulting stochastic approximation
scheme with linear function approximator and obtain an $\epsilon$-optimal
stationary policy with a sample complexity of $\Omega(\epsilon^{-2.5})$. We
compare the average reward performance of our proposed ARO-DDPG algorithm and
observe better empirical performance compared to state-of-the-art on-policy
average reward actor-critic algorithms over MuJoCo-based environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18060">Mining Negative Temporal Contexts For False Positive Suppression In Real-Time Ultrasound Lesion Detection. (arXiv:2305.18060v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haojun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Youcheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">QuanLin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Ziwei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dengbo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a></p>
<p>During ultrasonic scanning processes, real-time lesion detection can assist
radiologists in accurate cancer diagnosis. However, this essential task remains
challenging and underexplored. General-purpose real-time object detection
models can mistakenly report obvious false positives (FPs) when applied to
ultrasound videos, potentially misleading junior radiologists. One key issue is
their failure to utilize negative symptoms in previous frames, denoted as
negative temporal contexts (NTC). To address this issue, we propose to extract
contexts from previous frames, including NTC, with the guidance of inverse
optical flow. By aggregating extracted contexts, we endow the model with the
ability to suppress FPs by leveraging NTC. We call the resulting model
UltraDet. The proposed UltraDet demonstrates significant improvement over
previous state-of-the-arts and achieves real-time inference speed. We release
the code, checkpoints, and high-quality labels of the CVA-BUS dataset in
https://github.com/HaojunYu1998/UltraDet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09309">Who Needs to Know? Minimal Knowledge for Optimal Coordination. (arXiv:2306.09309v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lauffer_N/0/1/0/all/0/1">Niklas Lauffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Ameesh Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Carroll_M/0/1/0/all/0/1">Micah Carroll</a>, <a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1">Michael Dennis</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1">Stuart Russell</a></p>
<p>To optimally coordinate with others in cooperative games, it is often crucial
to have information about one's collaborators: successful driving requires
understanding which side of the road to drive on. However, not every feature of
collaborators is strategically relevant: the fine-grained acceleration of
drivers may be ignored while maintaining optimal coordination. We show that
there is a well-defined dichotomy between strategically relevant and irrelevant
information. Moreover, we show that, in dynamic games, this dichotomy has a
compact representation that can be efficiently computed via a Bellman backup
operator. We apply this algorithm to analyze the strategically relevant
information for tasks in both a standard and a partially observable version of
the Overcooked environment. Theoretical and empirical results show that our
algorithms are significantly more efficient than baselines. Videos are
available at https://minknowledge.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13197">Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lerma_M/0/1/0/all/0/1">Miguel Lerma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1">Mirtha Lucas</a></p>
<p>Gradient based attribution methods for neural networks working as classifiers
use gradients of network scores. Here we discuss the practical differences
between using gradients of pre-softmax scores versus post-softmax scores, and
their respective advantages and disadvantages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15079">From $O(\sqrt{n})$ to $O(\log(n))$ in Quadratic Programming. (arXiv:2306.15079v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wu_L/0/1/0/all/0/1">Liang Wu</a></p>
<p>A "dark cloud" hangs over numerical optimization theory for decades, namely,
whether an optimization algorithm $O(\log(n))$ iteration complexity exists.
"Yes", this paper answers, with a new optimization algorithm and strict theory
proof. It starts with box-constrained quadratic programming (Box-QP), and many
practical optimization problems fall into Box-QP. General smooth quadratic
programming (QP), nonsmooth Lasso, and support vector machine (or regression)
can be reformulated as Box-QP via duality theory. It is the first time to
present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which
behaves like a "direct" method: the required number of iterations is
deterministic with exact value
$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$.
This significant breakthrough enables us to transition from the $O(\sqrt{n})$
to the $O(\log(n))$ optimization algorithm, whose amazing scalability is
particularly relevant in today's era of big data and artificial intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01158">Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning. (arXiv:2307.01158v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oguntola_I/0/1/0/all/0/1">Ini Oguntola</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1">Joseph Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Stepputtis_S/0/1/0/all/0/1">Simon Stepputtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1">Katia Sycara</a></p>
<p>The ability to model the mental states of others is crucial to human social
intelligence, and can offer similar benefits to artificial agents with respect
to the social dynamics induced in multi-agent settings. We present a method of
grounding semantically meaningful, human-interpretable beliefs within policies
modeled by deep networks. We then consider the task of 2nd-order belief
prediction. We propose that ability of each agent to predict the beliefs of the
other agents can be used as an intrinsic reward signal for multi-agent
reinforcement learning. Finally, we present preliminary empirical results in a
mixed cooperative-competitive environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01646">SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhengyang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Renjie Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lele Wang</a></p>
<p>Diffusion models based on permutation-equivariant networks can learn
permutation-invariant distributions for graph data. However, in comparison to
their non-invariant counterparts, we have found that these invariant models
encounter greater learning challenges since 1) their effective target
distributions exhibit more modes; 2) their optimal one-step denoising scores
are the score functions of Gaussian mixtures with more components. Motivated by
this analysis, we propose a non-invariant diffusion model, called
$\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message
passing network and utilizes shifted window based self-attention inspired by
SwinTransformers. Further, through systematic ablations, we identify several
critical training and sampling techniques that significantly improve the sample
quality of graph generation. At last, we introduce a simple post-processing
trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably
converts any graph generative model to a permutation-invariant one. Extensive
experiments on synthetic and real-world protein and molecule datasets show that
our SwinGNN achieves state-of-the-art performances. Our code is released at
https://github.com/qiyan98/SwinGNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03135">Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yunhao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1">Zhan Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a></p>
<p>Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student's OOD
generalization: (1) by better imitating teacher's visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher's language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Code released at
https://github.com/xuanlinli17/large_vlm_distillation_ood
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06333">Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1">Andi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Netanyahu_A/0/1/0/all/0/1">Aviv Netanyahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1">Mark Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1">Tianmin Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bobu_A/0/1/0/all/0/1">Andreea Bobu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1">Julie Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>Policies often fail due to distribution shift -- changes in the state and
reward that occur when a policy is deployed in new environments. Data
augmentation can increase robustness by making the model invariant to
task-irrelevant changes in the agent's observation. However, designers don't
know which concepts are irrelevant a priori, especially when different end
users have different preferences about how the task is performed. We propose an
interactive framework to leverage feedback directly from the user to identify
personalized task-irrelevant concepts. Our key idea is to generate
counterfactual demonstrations that allow users to quickly identify possible
task-relevant and irrelevant concepts. The knowledge of task-irrelevant
concepts is then used to perform data augmentation and thus obtain a policy
adapted to personalized user objectives. We present experiments validating our
framework on discrete and continuous control tasks with real human users. Our
method (1) enables users to better understand agent failure, (2) reduces the
number of demonstrations required for fine-tuning, and (3) aligns the agent to
individual user task preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06608">Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1">Jitao Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1">Qi Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Changsheng Xu</a></p>
<p>Recently, the no-box adversarial attack, in which the attacker lacks access
to the model's architecture, weights, and training data, become the most
practical and challenging attack setup. However, there is an unawareness of the
potential and flexibility inherent in the surrogate model selection process on
no-box setting. Inspired by the burgeoning interest in utilizing foundational
models to address downstream tasks, this paper adopts an innovative idea that
1) recasting adversarial attack as a downstream task. Specifically, image noise
generation to meet the emerging trend and 2) introducing foundational models as
surrogate models. Harnessing the concept of non-robust features, we elaborate
on two guiding principles for surrogate model selection to explain why the
foundational model is an optimal choice for this role. However, paradoxically,
we observe that these foundational models underperform. Analyzing this
unexpected behavior within the feature space, we attribute the lackluster
performance of foundational models (e.g., CLIP) to their significant
representational capacity and, conversely, their lack of discriminative
prowess. To mitigate this issue, we propose the use of a margin-based loss
strategy for the fine-tuning of foundational models on target images. The
experimental results verify that our approach, which employs the basic Fast
Gradient Sign Method (FGSM) attack algorithm, outstrips the performance of
other, more convoluted algorithms. We conclude by advocating for the research
community to consider surrogate models as crucial determinants in the
effectiveness of adversarial attacks in no-box settings. The implications of
our work bear relevance for improving the efficacy of such adversarial attacks
and the overall robustness of AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06698">IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thanapalasingam_T/0/1/0/all/0/1">Thiviyan Thanapalasingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Krieken_E/0/1/0/all/0/1">Emile van Krieken</a>, <a href="http://arxiv.org/find/cs/1/au:+Bloem_P/0/1/0/all/0/1">Peter Bloem</a>, <a href="http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1">Paul Groth</a></p>
<p>Knowledge Graph Embedding (KGE) models are used to learn continuous
representations of entities and relations. A key task in the literature is
predicting missing links between entities. However, Knowledge Graphs are not
just sets of links but also have semantics underlying their structure.
Semantics is crucial in several downstream tasks, such as query answering or
reasoning. We introduce the subgraph inference task, where a model has to
generate likely and semantically valid subgraphs. We propose IntelliGraphs, a
set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain
subgraphs with semantics expressed in logical rules for evaluating subgraph
inference. We also present the dataset generator that produced the synthetic
datasets. We designed four novel baseline models, which include three models
based on traditional KGEs. We evaluate their expressiveness and show that these
models cannot capture the semantics. We believe this benchmark will encourage
the development of machine learning models that emphasize semantic
understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08347">M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Che Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Sibo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1">Mengyun Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weitong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Anand Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1">Wenjia Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1">Rossella Arcucci</a></p>
<p>Medical vision-language models enable co-learning and integrating features
from medical imaging and clinical text. However, these models are not easy to
train and the latent representation space can be complex. Here we propose a
novel way for pre-training and regularising medical vision-language models. The
proposed method, named Medical vision-language pre-training with Frozen
language models and Latent spAce Geometry optimization (M-FLAG), leverages a
frozen language model for training stability and efficiency and introduces a
novel orthogonality loss to harmonize the latent space geometry. We demonstrate
the potential of the pre-trained model on three downstream tasks: medical image
classification, segmentation, and object detection. Extensive experiments
across five public datasets demonstrate that M-FLAG significantly outperforms
existing medical vision-language pre-training approaches and reduces the number
of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the
segmentation task while using only 1\% of the RSNA dataset, even outperforming
ImageNet pre-trained models that have been fine-tuned using 100\% of the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1">Hugo Touvron</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1">Louis Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1">Kevin Stone</a>, <a href="http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1">Peter Albert</a>, <a href="http://arxiv.org/find/cs/1/au:+Almahairi_A/0/1/0/all/0/1">Amjad Almahairi</a>, <a href="http://arxiv.org/find/cs/1/au:+Babaei_Y/0/1/0/all/0/1">Yasmine Babaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bashlykov_N/0/1/0/all/0/1">Nikolay Bashlykov</a>, <a href="http://arxiv.org/find/cs/1/au:+Batra_S/0/1/0/all/0/1">Soumya Batra</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1">Prajjwal Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1">Shruti Bhosale</a>, <a href="http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1">Dan Bikel</a>, <a href="http://arxiv.org/find/cs/1/au:+Blecher_L/0/1/0/all/0/1">Lukas Blecher</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1">Cristian Canton Ferrer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Moya Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cucurull_G/0/1/0/all/0/1">Guillem Cucurull</a>, <a href="http://arxiv.org/find/cs/1/au:+Esiobu_D/0/1/0/all/0/1">David Esiobu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1">Jude Fernandes</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jeremy Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1">Wenyin Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuller_B/0/1/0/all/0/1">Brian Fuller</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Cynthia Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1">Vedanuj Goswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1">Naman Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartshorn_A/0/1/0/all/0/1">Anthony Hartshorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1">Saghar Hosseini</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1">Hakan Inan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kardas_M/0/1/0/all/0/1">Marcin Kardas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerkez_V/0/1/0/all/0/1">Viktor Kerkez</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1">Madian Khabsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kloumann_I/0/1/0/all/0/1">Isabel Kloumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Korenev_A/0/1/0/all/0/1">Artem Korenev</a>, <a href="http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1">Punit Singh Koura</a>, <a href="http://arxiv.org/find/cs/1/au:+Lachaux_M/0/1/0/all/0/1">Marie-Anne Lachaux</a>, <a href="http://arxiv.org/find/cs/1/au:+Lavril_T/0/1/0/all/0/1">Thibaut Lavril</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jenya Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Liskovich_D/0/1/0/all/0/1">Diana Liskovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yinghai Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuning Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinet_X/0/1/0/all/0/1">Xavier Martinet</a>, <a href="http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1">Todor Mihaylov</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1">Pushkar Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Molybog_I/0/1/0/all/0/1">Igor Molybog</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1">Yixin Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Poulton_A/0/1/0/all/0/1">Andrew Poulton</a>, <a href="http://arxiv.org/find/cs/1/au:+Reizenstein_J/0/1/0/all/0/1">Jeremy Reizenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1">Rashi Rungta</a>, <a href="http://arxiv.org/find/cs/1/au:+Saladi_K/0/1/0/all/0/1">Kalyan Saladi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schelten_A/0/1/0/all/0/1">Alan Schelten</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1">Ruan Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1">Eric Michael Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramanian_R/0/1/0/all/0/1">Ranjan Subramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xiaoqing Ellen Tan</a>, et al. (15 additional authors not shown)</p>
<p>In this work, we develop and release Llama 2, a collection of pretrained and
fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70
billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
dialogue use cases. Our models outperform open-source chat models on most
benchmarks we tested, and based on our human evaluations for helpfulness and
safety, may be a suitable substitute for closed-source models. We provide a
detailed description of our approach to fine-tuning and safety improvements of
Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.
</p>
</p>
</div>

    </div>
    </body>
    