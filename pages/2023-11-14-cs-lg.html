<!DOCTYPE html>
<html>
<head>
<title>2023-11-14-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.05638">Towards Instance-Optimality in Online PAC Reinforcement Learning. (arXiv:2311.05638v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Al_Marjani_A/0/1/0/all/0/1">Aymen Al-Marjani</a>, <a href="http://arxiv.org/find/stat/1/au:+Tirinzoni_A/0/1/0/all/0/1">Andrea Tirinzoni</a>, <a href="http://arxiv.org/find/stat/1/au:+Kaufmann_E/0/1/0/all/0/1">Emilie Kaufmann</a></p>
<p>Several recent works have proposed instance-dependent upper bounds on the
number of episodes needed to identify, with probability $1-\delta$, an
$\varepsilon$-optimal policy in finite-horizon tabular Markov Decision
Processes (MDPs). These upper bounds feature various complexity measures for
the MDP, which are defined based on different notions of sub-optimality gaps.
However, as of now, no lower bound has been established to assess the
optimality of any of these complexity measures, except for the special case of
MDPs with deterministic transitions. In this paper, we propose the first
instance-dependent lower bound on the sample complexity required for the PAC
identification of a near-optimal policy in any tabular episodic MDP.
Additionally, we demonstrate that the sample complexity of the PEDEL algorithm
of \cite{Wagenmaker22linearMDP} closely approaches this lower bound.
Considering the intractability of PEDEL, we formulate an open question
regarding the possibility of achieving our lower bound using a
computationally-efficient algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05641">Mobile Internet Quality Estimation using Self-Tuning Kernel Regression. (arXiv:2311.05641v1 [stat.AP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1">Hanyang Jiang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yuchi_H/0/1/0/all/0/1">Henry Shaowu Yuchi</a>, <a href="http://arxiv.org/find/stat/1/au:+Belding_E/0/1/0/all/0/1">Elizabeth Belding</a>, <a href="http://arxiv.org/find/stat/1/au:+Zegura_E/0/1/0/all/0/1">Ellen Zegura</a>, <a href="http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1">Yao Xie</a></p>
<p>Modeling and estimation for spatial data are ubiquitous in real life,
frequently appearing in weather forecasting, pollution detection, and
agriculture. Spatial data analysis often involves processing datasets of
enormous scale. In this work, we focus on large-scale internet-quality open
datasets from Ookla. We look into estimating mobile (cellular) internet quality
at the scale of a state in the United States. In particular, we aim to conduct
estimation based on highly {\it imbalanced} data: Most of the samples are
concentrated in limited areas, while very few are available in the rest, posing
significant challenges to modeling efforts. We propose a new adaptive kernel
regression approach that employs self-tuning kernels to alleviate the adverse
effects of data imbalance in this problem. Through comparative experimentation
on two distinct mobile network measurement datasets, we demonstrate that the
proposed self-tuning kernel regression method produces more accurate
predictions, with the potential to be applied in other applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05645">EControl: Fast Distributed Optimization with Compression and Error Control. (arXiv:2311.05645v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gao_Y/0/1/0/all/0/1">Yuan Gao</a>, <a href="http://arxiv.org/find/math/1/au:+Islamov_R/0/1/0/all/0/1">Rustem Islamov</a>, <a href="http://arxiv.org/find/math/1/au:+Stich_S/0/1/0/all/0/1">Sebastian Stich</a></p>
<p>Modern distributed training relies heavily on communication compression to
reduce the communication overhead. In this work, we study algorithms employing
a popular class of contractive compressors in order to reduce communication
overhead. However, the naive implementation often leads to unstable convergence
or even exponential divergence due to the compression bias. Error Compensation
(EC) is an extremely popular mechanism to mitigate the aforementioned issues
during the training of models enhanced by contractive compression operators.
Compared to the effectiveness of EC in the data homogeneous regime, the
understanding of the practicality and theoretical foundations of EC in the data
heterogeneous regime is limited. Existing convergence analyses typically rely
on strong assumptions such as bounded gradients, bounded data heterogeneity, or
large batch accesses, which are often infeasible in modern machine learning
applications. We resolve the majority of current issues by proposing EControl,
a novel mechanism that can regulate error compensation by controlling the
strength of the feedback signal. We prove fast convergence for EControl in
standard strongly convex, general convex, and nonconvex settings without any
additional assumptions on the problem or data heterogeneity. We conduct
extensive numerical evaluations to illustrate the efficacy of our method and
support our theoretical findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05650">Learning to Configure Separators in Branch-and-Cut. (arXiv:2311.05650v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Li_S/0/1/0/all/0/1">Sirui Li</a>, <a href="http://arxiv.org/find/math/1/au:+Ouyang_W/0/1/0/all/0/1">Wenbin Ouyang</a>, <a href="http://arxiv.org/find/math/1/au:+Paulus_M/0/1/0/all/0/1">Max B. Paulus</a>, <a href="http://arxiv.org/find/math/1/au:+Wu_C/0/1/0/all/0/1">Cathy Wu</a></p>
<p>Cutting planes are crucial in solving mixed integer linear programs (MILP) as
they facilitate bound improvements on the optimal solution. Modern MILP solvers
rely on a variety of separators to generate a diverse set of cutting planes by
invoking the separators frequently during the solving process. This work
identifies that MILP solvers can be drastically accelerated by appropriately
selecting separators to activate. As the combinatorial separator selection
space imposes challenges for machine learning, we learn to separate by
proposing a novel data-driven strategy to restrict the selection space and a
learning-guided algorithm on the restricted space. Our method predicts
instance-aware separator configurations which can dynamically adapt during the
solve, effectively accelerating the open source MILP solver SCIP by improving
the relative solve time up to 72% and 37% on synthetic and real-world MILP
benchmarks. Our work complements recent work on learning to select cutting
planes and highlights the importance of separator management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05651">On Mergable Coresets for Polytope Distance. (arXiv:2311.05651v1 [cs.CG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Benwei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1">Aditya Bhaskara</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_W/0/1/0/all/0/1">Wai Ming Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1">Jeff M. Phillips</a></p>
<p>We show that a constant-size constant-error coreset for polytope distance is
simple to maintain under merges of coresets. However, increasing the size
cannot improve the error bound significantly beyond that constant.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05657">Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs. (arXiv:2311.05657v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Da Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1">Abhilasha Ravichander</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1">Khyathi Chandu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bill Yuchen Lin</a></p>
<p>We introduce Lumos, a novel framework for training language agents that
employs a unified data format and a modular architecture based on open-source
large language models (LLMs). Lumos consists of three distinct modules:
planning, grounding, and execution. The planning module breaks down a task into
a series of high-level, tool-agnostic subgoals, which are then made specific by
the grounding module through a set of low-level actions. These actions are
subsequently executed by the execution module, utilizing a range of
off-the-shelf tools and APIs. In order to train these modules effectively,
high-quality annotations of subgoals and actions were collected and are made
available for fine-tuning open-source LLMs for various tasks such as complex
question answering, web tasks, and math problems. Leveraging this unified data
and modular design, Lumos not only achieves comparable or superior performance
to current, state-of-the-art agents, but also exhibits several key advantages:
(1) Lumos surpasses GPT-4/3.5-based agents in complex question answering and
web tasks, while equalling the performance of significantly larger LLM agents
on math tasks; (2) Lumos outperforms open-source agents created through
conventional training methods and those using chain-of-thoughts training; and
(3) Lumos is capable of effectively generalizing to unseen interactive tasks,
outperforming larger LLM-based agents and even exceeding performance of
specialized agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05659">Enhancing Instance-Level Image Classification with Set-Level Labels. (arXiv:2311.05659v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Aly A. Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Grossman_R/0/1/0/all/0/1">Robert L. Grossman</a></p>
<p>Instance-level image classification tasks have traditionally relied on
single-instance labels to train models, e.g., few-shot learning and transfer
learning. However, set-level coarse-grained labels that capture relationships
among instances can provide richer information in real-world scenarios. In this
paper, we present a novel approach to enhance instance-level image
classification by leveraging set-level labels. We provide a theoretical
analysis of the proposed method, including recognition conditions for fast
excess risk rate, shedding light on the theoretical foundations of our
approach. We conducted experiments on two distinct categories of datasets:
natural image datasets and histopathology image datasets. Our experimental
results demonstrate the effectiveness of our approach, showcasing improved
classification performance compared to traditional single-instance label-based
methods. Notably, our algorithm achieves 13% improvement in classification
accuracy compared to the strongest baseline on the histopathology image
classification benchmarks. Importantly, our experimental findings align with
the theoretical analysis, reinforcing the robustness and reliability of our
proposed method. This work bridges the gap between instance-level and set-level
image classification, offering a promising avenue for advancing the
capabilities of image classification models with set-level coarse-grained
labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05661">Prompt Engineering a Prompt Engineer. (arXiv:2311.05661v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qinyuan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Axmed_M/0/1/0/all/0/1">Maxamed Axmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1">Reid Pryzant</a>, <a href="http://arxiv.org/find/cs/1/au:+Khani_F/0/1/0/all/0/1">Fereshte Khani</a></p>
<p>Prompt engineering is a challenging yet crucial task for optimizing the
performance of large language models (LLMs). It requires complex reasoning to
examine the model's errors, hypothesize what is missing or misleading in the
current prompt, and communicate the task with clarity. While recent works
indicate that LLMs can be meta-prompted to perform automatic prompt
engineering, their potentials may not be fully untapped due to the lack of
sufficient guidance to elicit complex reasoning capabilities in LLMs in the
meta-prompt. In this work, we investigate the problem of "prompt engineering a
prompt engineer" -- constructing a meta-prompt that more effectively guides
LLMs to perform automatic prompt engineering. We introduce and analyze key
components, such as a step-by-step reasoning template and context
specification, which lead to improved performance. In addition, inspired by
common optimization concepts such as batch size, step size and momentum, we
introduce their verbalized counterparts to the meta-prompt and investigate
their effects. Our final method, named PE2, finds a prompt that outperforms
"let's think step by step" by 6.3% on the MultiArith dataset and 3.1% on the
GSM8K dataset. To demonstrate its versatility, we apply PE2 to the Instruction
Induction benchmark, a suite of counterfactual tasks, and a lengthy, real-world
industrial prompt. In these settings, PE2 achieves strong performance and
outperforms prior automatic prompt engineering baselines. Further, we show that
PE2 makes meaningful and targeted prompt edits, amends erroneous or incomplete
prompts, and presents non-trivial counterfactual reasoning abilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05665">Explainable artificial intelligence for Healthcare applications using Random Forest Classifier with LIME and SHAP. (arXiv:2311.05665v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panda_M/0/1/0/all/0/1">Mrutyunjaya Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahanta_S/0/1/0/all/0/1">Soumya Ranjan Mahanta</a></p>
<p>With the advances in computationally efficient artificial Intelligence (AI)
techniques and their numerous applications in our everyday life, there is a
pressing need to understand the computational details hidden in black box AI
techniques such as most popular machine learning and deep learning techniques;
through more detailed explanations. The origin of explainable AI (xAI) is
coined from these challenges and recently gained more attention by the
researchers by adding explainability comprehensively in traditional AI systems.
This leads to develop an appropriate framework for successful applications of
xAI in real life scenarios with respect to innovations, risk mitigation,
ethical issues and logical values to the users. In this book chapter, an
in-depth analysis of several xAI frameworks and methods including LIME (Local
Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive
exPlanations) are provided. Random Forest Classifier as black box AI is used on
a publicly available Diabetes symptoms dataset with LIME and SHAP for better
interpretations. The results obtained are interesting in terms of transparency,
valid and trustworthiness in diabetes disease prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05667">A theory for the sparsity emerged in the Forward Forward algorithm. (arXiv:2311.05667v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yukun Yang</a></p>
<p>This report explores the theory that explains the high sparsity phenomenon
\citep{tosato2023emergent} observed in the forward-forward algorithm
\citep{hinton2022forward}. The two theorems proposed predict the sparsity
changes of a single data point's activation in two cases: Theorem
\ref{theorem:1}: Decrease the goodness of the whole batch. Theorem
\ref{theorem:2}: Apply the complete forward forward algorithm to decrease the
goodness for negative data and increase the goodness for positive data. The
theory aligns well with the experiments tested on the MNIST dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05709">OmniVec: Learning robust representations with cross modal sharing. (arXiv:2311.05709v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Siddharth Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1">Gaurav Sharma</a></p>
<p>Majority of research in learning based methods has been towards designing and
training networks for specific tasks. However, many of the learning based
tasks, across modalities, share commonalities and could be potentially tackled
in a joint framework. We present an approach in such direction, to learn
multiple tasks, in multiple modalities, with a unified architecture. The
proposed network is composed of task specific encoders, a common trunk in the
middle, followed by task specific prediction heads. We first pre-train it by
self-supervised masked training, followed by sequential training for the
different tasks. We train the network on all major modalities, e.g.\ visual,
audio, text and 3D, and report results on $22$ diverse and challenging public
benchmarks. We demonstrate empirically that, using a joint network to train
across modalities leads to meaningful information sharing and this allows us to
achieve state-of-the-art results on most of the benchmarks. We also show
generalization of the trained network on cross-modal tasks as well as unseen
datasets and tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05720">Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models. (arXiv:2311.05720v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stepputtis_S/0/1/0/all/0/1">Simon Stepputtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1">Joseph Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yaqi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhengyang Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenxin Sharon Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangreji_S/0/1/0/all/0/1">Sanketh Rangreji</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1">Michael Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1">Katia Sycara</a></p>
<p>Deception and persuasion play a critical role in long-horizon dialogues
between multiple parties, especially when the interests, goals, and motivations
of the participants are not aligned. Such complex tasks pose challenges for
current Large Language Models (LLM) as deception and persuasion can easily
mislead them, especially in long-horizon multi-party dialogues. To this end, we
explore the game of Avalon: The Resistance, a social deduction game in which
players must determine each other's hidden identities to complete their team's
objective. We introduce an online testbed and a dataset containing 20 carefully
collected and labeled games among human players that exhibit long-horizon
deception in a cooperative-competitive setting. We discuss the capabilities of
LLMs to utilize deceptive long-horizon conversations between six human players
to determine each player's goal and motivation. Particularly, we discuss the
multimodal integration of the chat between the players and the game's state
that grounds the conversation, providing further insights into the true player
identities. We find that even current state-of-the-art LLMs do not reach human
performance, making our dataset a compelling benchmark to investigate the
decision-making and language-processing capabilities of LLMs. Our dataset and
online testbed can be found at our project website:
https://sstepput.github.io/Avalon-NLU/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05722">Verilog-to-PyG -- A Framework for Graph Learning and Augmentation on RTL Designs. (arXiv:2311.05722v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mingju Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishchenko_A/0/1/0/all/0/1">Alan Mishchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Cunxi Yu</a></p>
<p>The complexity of modern hardware designs necessitates advanced methodologies
for optimizing and analyzing modern digital systems. In recent times, machine
learning (ML) methodologies have emerged as potent instruments for assessing
design quality-of-results at the Register-Transfer Level (RTL) or Boolean
level, aiming to expedite design exploration of advanced RTL configurations. In
this presentation, we introduce an innovative open-source framework that
translates RTL designs into graph representation foundations, which can be
seamlessly integrated with the PyTorch Geometric graph learning platform.
Furthermore, the Verilog-to-PyG (V2PYG) framework is compatible with the
open-source Electronic Design Automation (EDA) toolchain OpenROAD, facilitating
the collection of labeled datasets in an utterly open-source manner.
Additionally, we will present novel RTL data augmentation methods (incorporated
in our framework) that enable functional equivalent design augmentation for the
construction of an extensive graph-based RTL design database. Lastly, we will
showcase several using cases of V2PYG with detailed scripting examples. V2PYG
can be found at \url{https://yu-maryland.github.io/Verilog-to-PyG/}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05726">Neural Network Methods for Radiation Detectors and Imaging. (arXiv:2311.05726v1 [physics.ins-det])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lin_S/0/1/0/all/0/1">S. Lin</a>, <a href="http://arxiv.org/find/physics/1/au:+Ning_S/0/1/0/all/0/1">S. Ning</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhu_H/0/1/0/all/0/1">H. Zhu</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhou_T/0/1/0/all/0/1">T. Zhou</a>, <a href="http://arxiv.org/find/physics/1/au:+Morris_C/0/1/0/all/0/1">C. L. Morris</a>, <a href="http://arxiv.org/find/physics/1/au:+Clayton_S/0/1/0/all/0/1">S. Clayton</a>, <a href="http://arxiv.org/find/physics/1/au:+Cherukara_M/0/1/0/all/0/1">M. Cherukara</a>, <a href="http://arxiv.org/find/physics/1/au:+Chen_R/0/1/0/all/0/1">R. T. Chen</a>, <a href="http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1">Z. Wang</a></p>
<p>Recent advances in image data processing through machine learning and
especially deep neural networks (DNNs) allow for new optimization and
performance-enhancement schemes for radiation detectors and imaging hardware
through data-endowed artificial intelligence. We give an overview of data
generation at photon sources, deep learning-based methods for image processing
tasks, and hardware solutions for deep learning acceleration. Most existing
deep learning approaches are trained offline, typically using large amounts of
computational resources. However, once trained, DNNs can achieve fast inference
speeds and can be deployed to edge devices. A new trend is edge computing with
less energy consumption (hundreds of watts or less) and real-time analysis
potential. While popularly used for edge computing, electronic-based hardware
accelerators ranging from general purpose processors such as central processing
units (CPUs) to application-specific integrated circuits (ASICs) are constantly
reaching performance limits in latency, energy consumption, and other physical
constraints. These limits give rise to next-generation analog neuromorhpic
hardware platforms, such as optical neural networks (ONNs), for high parallel,
low latency, and low energy computing to boost deep learning acceleration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05733">LogShield: A Transformer-based APT Detection System Leveraging Self-Attention. (arXiv:2311.05733v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Afnan_S/0/1/0/all/0/1">Sihat Afnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadia_M/0/1/0/all/0/1">Mushtari Sadia</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1">Shahrear Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1">Anindya Iqbal</a></p>
<p>Cyber attacks are often identified using system and network logs. There have
been significant prior works that utilize provenance graphs and ML techniques
to detect attacks, specifically advanced persistent threats, which are very
difficult to detect. Lately, there have been studies where transformer-based
language models are being used to detect various types of attacks from system
logs. However, no such attempts have been made in the case of APTs. In
addition, existing state-of-the-art techniques that use system provenance
graphs, lack a data processing framework generalized across datasets for
optimal performance. For mitigating this limitation as well as exploring the
effectiveness of transformer-based language models, this paper proposes
LogShield, a framework designed to detect APT attack patterns leveraging the
power of self-attention in transformers. We incorporate customized embedding
layers to effectively capture the context of event sequences derived from
provenance graphs. While acknowledging the computational overhead associated
with training transformer networks, our framework surpasses existing LSTM and
Language models regarding APT detection. We integrated the model parameters and
training procedure from the RoBERTa model and conducted extensive experiments
on well-known APT datasets (DARPA OpTC and DARPA TC E3). Our framework achieved
superior F1 scores of 98% and 95% on the two datasets respectively, surpassing
the F1 scores of 96% and 94% obtained by LSTM models. Our findings suggest that
LogShield's performance benefits from larger datasets and demonstrates its
potential for generalization across diverse domains. These findings contribute
to the advancement of APT attack detection methods and underscore the
significance of transformer-based architectures in addressing security
challenges in computer systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05739">Deep Learning Architecture for Network-Efficiency at the Edge. (arXiv:2311.05739v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mudvari_A/0/1/0/all/0/1">Akrit Mudvari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vainio_A/0/1/0/all/0/1">Antero Vainio</a>, <a href="http://arxiv.org/find/cs/1/au:+Ofeidis_I/0/1/0/all/0/1">Iason Ofeidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarkoma_S/0/1/0/all/0/1">Sasu Tarkoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1">Leandros Tassiulas</a></p>
<p>The growing number of AI-driven applications in the mobile devices has led to
solutions that integrate deep learning models with the available edge-cloud
resources; due to multiple benefits such as reduction in on-device energy
consumption, improved latency, improved network usage, and certain privacy
improvements, split learning, where deep learning models are split away from
the mobile device and computed in a distributed manner, has become an
extensively explored topic. Combined with compression-aware methods where
learning adapts to compression of communicated data, the benefits of this
approach have further improved and could serve as an alternative to established
approaches like federated learning methods. In this work, we develop an
adaptive compression-aware split learning method ('deprune') to improve and
train deep learning models so that they are much more network-efficient (use
less network resources and are faster), which would make them ideal to deploy
in weaker devices with the help of edge-cloud resources. This method is also
extended ('prune') to very quickly train deep learning models, through a
transfer learning approach, that trades off little accuracy for much more
network-efficient inference abilities. We show that the 'deprune' method can
reduce network usage by 4x when compared with a split-learning approach (that
does not use our method) without loss of accuracy, while also improving
accuracy over compression-aware split-learning by 4 percent. Lastly, we show
that the 'prune' method can reduce the training time for certain models by up
to 6x without affecting the accuracy when compared against a compression-aware
split-learning approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05740">Generating Pragmatic Examples to Train Neural Program Synthesizers. (arXiv:2311.05740v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vaduguru_S/0/1/0/all/0/1">Saujas Vaduguru</a>, <a href="http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1">Daniel Fried</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yewen Pu</a></p>
<p>Programming-by-example is the task of synthesizing a program that is
consistent with a set of user-provided input-output examples. As examples are
often an under-specification of one's intent, a good synthesizer must choose
the intended program from the many that are consistent with the given set of
examples. Prior work frames program synthesis as a cooperative game between a
listener (that synthesizes programs) and a speaker (a user choosing examples),
and shows that models of computational pragmatic inference are effective in
choosing the user intended programs. However, these models require
counterfactual reasoning over a large set of programs and examples, which is
infeasible in realistic program spaces. In this paper, we propose a novel way
to amortize this search with neural networks. We sample pairs of programs and
examples via self-play between listener and speaker models, and use pragmatic
inference to choose informative training examples from this sample.We then use
the informative dataset to train models to improve the synthesizer's ability to
disambiguate user-provided examples without human supervision. We validate our
method on the challenging task of synthesizing regular expressions from example
strings, and find that our method (1) outperforms models trained without
choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the
performance of supervised learning on a dataset of pragmatic examples provided
by humans, despite using no human data in training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05741">Efficiently Adapting Pretrained Language Models To New Languages. (arXiv:2311.05741v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Csaki_Z/0/1/0/all/0/1">Zoltan Csaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Pawakapan_P/0/1/0/all/0/1">Pian Pawakapan</a>, <a href="http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1">Urmish Thakker</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiantong Xu</a></p>
<p>Recent large language models (LLM) exhibit sub-optimal performance on
low-resource languages, as the training data of these models is usually
dominated by English and other high-resource languages. Furthermore, it is
challenging to train models for low-resource languages, especially from
scratch, due to a lack of high quality training data. Adapting pretrained LLMs
reduces the need for data in the new language while also providing cross
lingual transfer capabilities. However, naively adapting to new languages leads
to catastrophic forgetting and poor tokenizer efficiency. In this work, we
study how to efficiently adapt any existing pretrained LLM to a new language
without running into these issues. In particular, we improve the encoding
efficiency of the tokenizer by adding new tokens from the target language and
study the data mixing recipe to mitigate forgetting. Our experiments on
adapting an English LLM to Hungarian and Thai show that our recipe can reach
better performance than open source models on the target language, with minimal
regressions on English.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05742">Optimal simulation-based Bayesian decisions. (arXiv:2311.05742v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Alsing_J/0/1/0/all/0/1">Justin Alsing</a>, <a href="http://arxiv.org/find/stat/1/au:+Edwards_T/0/1/0/all/0/1">Thomas D. P. Edwards</a>, <a href="http://arxiv.org/find/stat/1/au:+Wandelt_B/0/1/0/all/0/1">Benjamin Wandelt</a></p>
<p>We present a framework for the efficient computation of optimal Bayesian
decisions under intractable likelihoods, by learning a surrogate model for the
expected utility (or its distribution) as a function of the action and data
spaces. We leverage recent advances in simulation-based inference and Bayesian
optimization to develop active learning schemes to choose where in parameter
and action spaces to simulate. This allows us to learn the optimal action in as
few simulations as possible. The resulting framework is extremely simulation
efficient, typically requiring fewer model calls than the associated posterior
inference task alone, and a factor of $100-1000$ more efficient than
Monte-Carlo based methods. Our framework opens up new capabilities for
performing Bayesian decision making, particularly in the previously challenging
regime where likelihoods are intractable, and simulations expensive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05760">MALCOM-PSGD: Inexact Proximal Stochastic Gradient Descent for Communication-Efficient Decentralized Machine Learning. (arXiv:2311.05760v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Campbell_A/0/1/0/all/0/1">Andrew Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Woldemariam_L/0/1/0/all/0/1">Leah Woldemariam</a>, <a href="http://arxiv.org/find/cs/1/au:+Scaglione_A/0/1/0/all/0/1">Anna Scaglione</a></p>
<p>Recent research indicates that frequent model communication stands as a major
bottleneck to the efficiency of decentralized machine learning (ML),
particularly for large-scale and over-parameterized neural networks (NNs). In
this paper, we introduce MALCOM-PSGD, a new decentralized ML algorithm that
strategically integrates gradient compression techniques with model
sparsification. MALCOM-PSGD leverages proximal stochastic gradient descent to
handle the non-smoothness resulting from the $\ell_1$ regularization in model
sparsification. Furthermore, we adapt vector source coding and dithering-based
quantization for compressed gradient communication of sparsified models. Our
analysis shows that decentralized proximal stochastic gradient descent with
compressed communication has a convergence rate of
$\mathcal{O}\left(\ln(t)/\sqrt{t}\right)$ assuming a diminishing learning rate
and where $t$ denotes the number of iterations. Numerical results verify our
theoretical findings and demonstrate that our method reduces communication
costs by approximately $75\%$ when compared to the state-of-the-art method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05764">Generative Explanations for Graph Neural Network: Methods and Evaluations. (arXiv:2311.05764v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jialin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1">Kenza Amara</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Junchi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1">Rex Ying</a></p>
<p>Graph Neural Networks (GNNs) achieve state-of-the-art performance in various
graph-related tasks. However, the black-box nature often limits their
interpretability and trustworthiness. Numerous explainability methods have been
proposed to uncover the decision-making logic of GNNs, by generating underlying
explanatory substructures. In this paper, we conduct a comprehensive review of
the existing explanation methods for GNNs from the perspective of graph
generation. Specifically, we propose a unified optimization objective for
generative explanation methods, comprising two sub-objectives: Attribution and
Information constraints. We further demonstrate their specific manifestations
in various generative model architectures and different explanation scenarios.
With the unified objective of the explanation problem, we reveal the shared
characteristics and distinctions among current methods, laying the foundation
for future methodological advancements. Empirical results demonstrate the
advantages and limitations of different explainability approaches in terms of
explanation performance, efficiency, and generalizability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05767">Dirichlet Energy Enhancement of Graph Neural Networks by Framelet Augmentation. (arXiv:2311.05767v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jialin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuelin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1">Cristian Bodnar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1">Rex Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Lio</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Guang Wang</a></p>
<p>Graph convolutions have been a pivotal element in learning graph
representations. However, recursively aggregating neighboring information with
graph convolutions leads to indistinguishable node features in deep layers,
which is known as the over-smoothing issue. The performance of graph neural
networks decays fast as the number of stacked layers increases, and the
Dirichlet energy associated with the graph decreases to zero as well. In this
work, we introduce a framelet system into the analysis of Dirichlet energy and
take a multi-scale perspective to leverage the Dirichlet energy and alleviate
the over-smoothing issue. Specifically, we develop a Framelet Augmentation
strategy by adjusting the update rules with positive and negative increments
for low-pass and high-passes respectively. Based on that, we design the Energy
Enhanced Convolution (EEConv), which is an effective and practical operation
that is proved to strictly enhance Dirichlet energy. From a message-passing
perspective, EEConv inherits multi-hop aggregation property from the framelet
transform and takes into account all hops in the multi-scale representation,
which benefits the node classification tasks over heterophilous graphs.
Experiments show that deep GNNs with EEConv achieve state-of-the-art
performance over various node classification datasets, especially for
heterophilous graphs, while also lifting the Dirichlet energy as the network
goes deeper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05772">ADaPT: As-Needed Decomposition and Planning with Language Models. (arXiv:2311.05772v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1">Archiki Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1">Alexander Koller</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1">Mareike Hartmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1">Peter Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1">Ashish Sabharwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1">Tushar Khot</a></p>
<p>Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05780">Real-time Control of Electric Autonomous Mobility-on-Demand Systems via Graph Reinforcement Learning. (arXiv:2311.05780v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Singhal_A/0/1/0/all/0/1">Aaryan Singhal</a>, <a href="http://arxiv.org/find/eess/1/au:+Gammelli_D/0/1/0/all/0/1">Daniele Gammelli</a>, <a href="http://arxiv.org/find/eess/1/au:+Luke_J/0/1/0/all/0/1">Justin Luke</a>, <a href="http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1">Karthik Gopalakrishnan</a>, <a href="http://arxiv.org/find/eess/1/au:+Helmreich_D/0/1/0/all/0/1">Dominik Helmreich</a>, <a href="http://arxiv.org/find/eess/1/au:+Pavone_M/0/1/0/all/0/1">Marco Pavone</a></p>
<p>Operators of Electric Autonomous Mobility-on-Demand (E-AMoD) fleets need to
make several real-time decisions such as matching available cars to ride
requests, rebalancing idle cars to areas of high demand, and charging vehicles
to ensure sufficient range. While this problem can be posed as a linear program
that optimizes flows over a space-charge-time graph, the size of the resulting
optimization problem does not allow for real-time implementation in realistic
settings. In this work, we present the E-AMoD control problem through the lens
of reinforcement learning and propose a graph network-based framework to
achieve drastically improved scalability and superior performance over
heuristics. Specifically, we adopt a bi-level formulation where we (1) leverage
a graph network-based RL agent to specify a desired next state in the
space-charge graph, and (2) solve more tractable linear programs to best
achieve the desired state while ensuring feasibility. Experiments using
real-world data from San Francisco and New York City show that our approach
achieves up to 89% of the profits of the theoretically-optimal solution while
achieving more than a 100x speedup in computational time. Furthermore, our
approach outperforms the best domain-specific heuristics with comparable
runtimes, with an increase in profits by up to 3x. Finally, we highlight
promising zero-shot transfer capabilities of our learned policy on tasks such
as inter-city generalization and service area expansion, thus showing the
utility, scalability, and flexibility of our framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05784">Are &quot;Hierarchical&quot; Visual Representations Hierarchical?. (arXiv:2311.05784v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_E/0/1/0/all/0/1">Ethan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a></p>
<p>Learned visual representations often capture large amounts of semantic
information for accurate downstream applications. Human understanding of the
world is fundamentally grounded in hierarchy. To mimic this and further improve
representation capabilities, the community has explored "hierarchical" visual
representations that aim at modeling the underlying hierarchy of the visual
world. In this work, we set out to investigate if hierarchical visual
representations truly capture the human perceived hierarchy better than
standard learned representations. To this end, we create HierNet, a suite of 12
datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet.
After extensive evaluation of Hyperbolic and Matryoshka Representations across
training setups, we conclude that they do not capture hierarchy any better than
the standard representations but can assist in other aspects like search
efficiency and interpretability. Our benchmark and the datasets are
open-sourced at https://github.com/ethanlshen/HierNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05787">Towards stable real-world equation discovery with assessing differentiating quality influence. (arXiv:2311.05787v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Masliaev_M/0/1/0/all/0/1">Mikhail Masliaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1">Ilya Markov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hvatov_A/0/1/0/all/0/1">Alexander Hvatov</a></p>
<p>This paper explores the critical role of differentiation approaches for
data-driven differential equation discovery. Accurate derivatives of the input
data are essential for reliable algorithmic operation, particularly in
real-world scenarios where measurement quality is inevitably compromised. We
propose alternatives to the commonly used finite differences-based method,
notorious for its instability in the presence of noise, which can exacerbate
random errors in the data. Our analysis covers four distinct methods:
Savitzky-Golay filtering, spectral differentiation, smoothing based on
artificial neural networks, and the regularization of derivative variation. We
evaluate these methods in terms of applicability to problems, similar to the
real ones, and their ability to ensure the convergence of equation discovery
algorithms, providing valuable insights for robust modeling of real-world
processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05788">Structured Transforms Across Spaces with Cost-Regularized Optimal Transport. (arXiv:2311.05788v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sebbouh_O/0/1/0/all/0/1">Othmane Sebbouh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1">Marco Cuturi</a>, <a href="http://arxiv.org/find/cs/1/au:+Peyre_G/0/1/0/all/0/1">Gabriel Peyr&#xe9;</a></p>
<p>Matching a source to a target probability measure is often solved by
instantiating a linear optimal transport (OT) problem, parameterized by a
ground cost function that quantifies discrepancy between points. When these
measures live in the same metric space, the ground cost often defaults to its
distance. When instantiated across two different spaces, however, choosing that
cost in the absence of aligned data is a conundrum. As a result, practitioners
often resort to solving instead a quadratic Gromow-Wasserstein (GW) problem. We
exploit in this work a parallel between GW and cost-regularized OT, the
regularized minimization of a linear OT objective parameterized by a ground
cost. We use this cost-regularized formulation to match measures across two
different Euclidean spaces, where the cost is evaluated between transformed
source points and target points. We show that several quadratic OT problems
fall in this category, and consider enforcing structure in linear transform
(e.g. sparsity), by introducing structure-inducing regularizers. We provide a
proximal algorithm to extract such transforms from unaligned data, and
demonstrate its applicability to single-cell spatial transcriptomics/multiomics
matching tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05790">The Paradox of Noise: An Empirical Study of Noise-Infusion Mechanisms to Improve Generalization, Stability, and Privacy in Federated Learning. (arXiv:2311.05790v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jafarigol_E/0/1/0/all/0/1">Elaheh Jafarigol</a>, <a href="http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1">Theodore Trafalis</a></p>
<p>In a data-centric era, concerns regarding privacy and ethical data handling
grow as machine learning relies more on personal information. This empirical
study investigates the privacy, generalization, and stability of deep learning
models in the presence of additive noise in federated learning frameworks. Our
main objective is to provide strategies to measure the generalization,
stability, and privacy-preserving capabilities of these models and further
improve them. To this end, five noise infusion mechanisms at varying noise
levels within centralized and federated learning settings are explored. As
model complexity is a key component of the generalization and stability of deep
learning models during training and evaluation, a comparative analysis of three
Convolutional Neural Network (CNN) architectures is provided. The paper
introduces Signal-to-Noise Ratio (SNR) as a quantitative measure of the
trade-off between privacy and training accuracy of noise-infused models, aiming
to find the noise level that yields optimal privacy and accuracy. Moreover, the
Price of Stability and Price of Anarchy are defined in the context of
privacy-preserving deep learning, contributing to the systematic investigation
of the noise infusion strategies to enhance privacy without compromising
performance. Our research sheds light on the delicate balance between these
critical factors, fostering a deeper understanding of the implications of
noise-based regularization in machine learning. By leveraging noise as a tool
for regularization and privacy enhancement, we aim to contribute to the
development of robust, privacy-aware algorithms, ensuring that AI-driven
solutions prioritize both utility and privacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05791">Detecting Suspicious Commenter Mob Behaviors on YouTube Using Graph2Vec. (arXiv:2311.05791v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shajari_S/0/1/0/all/0/1">Shadi Shajari</a>, <a href="http://arxiv.org/find/cs/1/au:+Alassad_M/0/1/0/all/0/1">Mustafa Alassad</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1">Nitin Agarwal</a></p>
<p>YouTube, a widely popular online platform, has transformed the dynamics of
con-tent consumption and interaction for users worldwide. With its extensive
range of content crea-tors and viewers, YouTube serves as a hub for video
sharing, entertainment, and information dissemination. However, the exponential
growth of users and their active engagement on the platform has raised concerns
regarding suspicious commenter behaviors, particularly in the com-ment section.
This paper presents a social network analysis-based methodology for detecting
suspicious commenter mob-like behaviors among YouTube channels and the
similarities therein. The method aims to characterize channels based on the
level of such behavior and identify com-mon patterns across them. To evaluate
the effectiveness of the proposed model, we conducted an analysis of 20 YouTube
channels, consisting of 7,782 videos, 294,199 commenters, and 596,982 comments.
These channels were specifically selected for propagating false views about the
U.S. Military. The analysis revealed significant similarities among the
channels, shedding light on the prevalence of suspicious commenter behavior. By
understanding these similarities, we contribute to a better understanding of
the dynamics of suspicious behavior on YouTube channels, which can inform
strategies for addressing and mitigating such behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05792">Is a Seat at the Table Enough? Engaging Teachers and Students in Dataset Specification for ML in Education. (arXiv:2311.05792v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mei Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hansol Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dakuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramonyam_H/0/1/0/all/0/1">Hariharan Subramonyam</a></p>
<p>Despite the promises of ML in education, its adoption in the classroom has
surfaced numerous issues regarding fairness, accountability, and transparency,
as well as concerns about data privacy and student consent. A root cause of
these issues is the lack of understanding of the complex dynamics of education,
including teacher-student interactions, collaborative learning, and classroom
environment. To overcome these challenges and fully utilize the potential of ML
in education, software practitioners need to work closely with educators and
students to fully understand the context of the data (the backbone of ML
applications) and collaboratively define the ML data specifications. To gain a
deeper understanding of such a collaborative process, we conduct ten co-design
sessions with ML software practitioners, educators, and students. In the
sessions, teachers and students work with ML engineers, UX designers, and legal
practitioners to define dataset characteristics for a given ML application. We
find that stakeholders contextualize data based on their domain and procedural
knowledge, proactively design data requirements to mitigate downstream harms
and data reliability concerns, and exhibit role-based collaborative strategies
and contribution patterns. Further, we find that beyond a seat at the table,
meaningful stakeholder participation in ML requires structured supports:
defined processes for continuous iteration and co-evaluation, shared contextual
data quality standards, and information scaffolds for both technical and
non-technical stakeholders to traverse expertise boundaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05794">An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits. (arXiv:2311.05794v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Liang_B/0/1/0/all/0/1">Biyonka Liang</a>, <a href="http://arxiv.org/find/stat/1/au:+Bojinov_I/0/1/0/all/0/1">Iavor Bojinov</a></p>
<p>Typically, multi-armed bandit (MAB) experiments are analyzed at the end of
the study and thus require the analyst to specify a fixed sample size in
advance. However, in many online learning applications, it is advantageous to
continuously produce inference on the average treatment effect (ATE) between
arms as new data arrive and determine a data-driven stopping time for the
experiment. Existing work on continuous inference for adaptive experiments
assumes that the treatment assignment probabilities are bounded away from zero
and one, thus excluding nearly all standard bandit algorithms. In this work, we
develop the Mixture Adaptive Design (MAD), a new experimental design for
multi-armed bandits that enables continuous inference on the ATE with
guarantees on statistical validity and power for nearly any bandit algorithm.
On a high level, the MAD "mixes" a bandit algorithm of the user's choice with a
Bernoulli design through a tuning parameter $\delta_t$, where $\delta_t$ is a
deterministic sequence that controls the priority placed on the Bernoulli
design as the sample size grows. We show that for $\delta_t =
o\left(1/t^{1/4}\right)$, the MAD produces a confidence sequence that is
asymptotically valid and guaranteed to shrink around the true ATE. We
empirically show that the MAD improves the coverage and power of ATE inference
in MAB experiments without significant losses in finite-sample reward.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05795">Improvements on Uncertainty Quantification for Node Classification via Distance-Based Regularization. (arXiv:2311.05795v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hart_R/0/1/0/all/0/1">Russell Alan Hart</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Linlin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1">Yifei Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feng Chen</a></p>
<p>Deep neural networks have achieved significant success in the last decades,
but they are not well-calibrated and often produce unreliable predictions. A
large number of literature relies on uncertainty quantification to evaluate the
reliability of a learning model, which is particularly important for
applications of out-of-distribution (OOD) detection and misclassification
detection. We are interested in uncertainty quantification for interdependent
node-level classification. We start our analysis based on graph posterior
networks (GPNs) that optimize the uncertainty cross-entropy (UCE)-based loss
function. We describe the theoretical limitations of the widely-used UCE loss.
To alleviate the identified drawbacks, we propose a distance-based
regularization that encourages clustered OOD nodes to remain clustered in the
latent space. We conduct extensive comparison experiments on eight standard
datasets and demonstrate that the proposed regularization outperforms the
state-of-the-art in both OOD detection and misclassification detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05798">Synthesizing Bidirectional Temporal States of Knee Osteoarthritis Radiographs with Cycle-Consistent Generative Adversarial Neural Networks. (arXiv:2311.05798v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Prezja_F/0/1/0/all/0/1">Fabi Prezja</a>, <a href="http://arxiv.org/find/eess/1/au:+Annala_L/0/1/0/all/0/1">Leevi Annala</a>, <a href="http://arxiv.org/find/eess/1/au:+Kiiskinen_S/0/1/0/all/0/1">Sampsa Kiiskinen</a>, <a href="http://arxiv.org/find/eess/1/au:+Lahtinen_S/0/1/0/all/0/1">Suvi Lahtinen</a>, <a href="http://arxiv.org/find/eess/1/au:+Ojala_T/0/1/0/all/0/1">Timo Ojala</a></p>
<p>Knee Osteoarthritis (KOA), a leading cause of disability worldwide, is
challenging to detect early due to subtle radiographic indicators. Diverse,
extensive datasets are needed but are challenging to compile because of
privacy, data collection limitations, and the progressive nature of KOA.
However, a model capable of projecting genuine radiographs into different OA
stages could augment data pools, enhance algorithm training, and offer
pre-emptive prognostic insights. In this study, we trained a CycleGAN model to
synthesize past and future stages of KOA on any genuine radiograph. The model
was validated using a Convolutional Neural Network that was deceived into
misclassifying disease stages in transformed images, demonstrating the
CycleGAN's ability to effectively transform disease characteristics forward or
backward in time. The model was particularly effective in synthesizing future
disease states and showed an exceptional ability to retroactively transition
late-stage radiographs to earlier stages by eliminating osteophytes and
expanding knee joint space, signature characteristics of None or Doubtful KOA.
The model's results signify a promising potential for enhancing diagnostic
models, data augmentation, and educational and prognostic usage in healthcare.
Nevertheless, further refinement, validation, and a broader evaluation process
encompassing both CNN-based assessments and expert medical feedback are
emphasized for future research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05799">Adaptive Variance Thresholding: A Novel Approach to Improve Existing Deep Transfer Vision Models and Advance Automatic Knee-Joint Osteoarthritis Classification. (arXiv:2311.05799v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Prezja_F/0/1/0/all/0/1">Fabi Prezja</a>, <a href="http://arxiv.org/find/eess/1/au:+Annala_L/0/1/0/all/0/1">Leevi Annala</a>, <a href="http://arxiv.org/find/eess/1/au:+Kiiskinen_S/0/1/0/all/0/1">Sampsa Kiiskinen</a>, <a href="http://arxiv.org/find/eess/1/au:+Lahtinen_S/0/1/0/all/0/1">Suvi Lahtinen</a>, <a href="http://arxiv.org/find/eess/1/au:+Ojala_T/0/1/0/all/0/1">Timo Ojala</a></p>
<p>Knee-Joint Osteoarthritis (KOA) is a prevalent cause of global disability and
is inherently complex to diagnose due to its subtle radiographic markers and
individualized progression. One promising classification avenue involves
applying deep learning methods; however, these techniques demand extensive,
diversified datasets, which pose substantial challenges due to medical data
collection restrictions. Existing practices typically resort to smaller
datasets and transfer learning. However, this approach often inherits
unnecessary pre-learned features that can clutter the classifier's vector
space, potentially hampering performance. This study proposes a novel paradigm
for improving post-training specialized classifiers by introducing adaptive
variance thresholding (AVT) followed by Neural Architecture Search (NAS). This
approach led to two key outcomes: an increase in the initial accuracy of the
pre-trained KOA models and a 60-fold reduction in the NAS input vector space,
thus facilitating faster inference speed and a more efficient hyperparameter
search. We also applied this approach to an external model trained for KOA
classification. Despite its initial performance, the application of our
methodology improved its average accuracy, making it one of the top three KOA
classification models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05808">Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction. (arXiv:2311.05808v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shanghao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Ning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yang Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yi Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Y.Thomas Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_W/0/1/0/all/0/1">Wenjing Lou</a></p>
<p>Federated learning is known for its capability to safeguard participants'
data privacy. However, recently emerged model inversion attacks (MIAs) have
shown that a malicious parameter server can reconstruct individual users' local
data samples through model updates. The state-of-the-art attacks either rely on
computation-intensive search-based optimization processes to recover each input
batch, making scaling difficult, or they involve the malicious parameter server
adding extra modules before the global model architecture, rendering the
attacks too conspicuous and easily detectable.
</p>
<p>To overcome these limitations, we propose Scale-MIA, a novel MIA capable of
efficiently and accurately recovering training samples of clients from the
aggregated updates, even when the system is under the protection of a robust
secure aggregation protocol. Unlike existing approaches treating models as
black boxes, Scale-MIA recognizes the importance of the intricate architecture
and inner workings of machine learning models. It identifies the latent space
as the critical layer for breaching privacy and decomposes the complex recovery
task into an innovative two-step process to reduce computation complexity. The
first step involves reconstructing the latent space representations (LSRs) from
the aggregated model updates using a closed-form inversion mechanism,
leveraging specially crafted adversarial linear layers. In the second step, the
whole input batches are recovered from the LSRs by feeding them into a
fine-tuned generative decoder.
</p>
<p>We implemented Scale-MIA on multiple commonly used machine learning models
and conducted comprehensive experiments across various settings. The results
demonstrate that Scale-MIA achieves excellent recovery performance on different
datasets, exhibiting high reconstruction rates, accuracy, and attack efficiency
on a larger scale compared to state-of-the-art MIAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05820">Machine Learning-powered Compact Modeling of Stochastic Electronic Devices using Mixture Density Networks. (arXiv:2311.05820v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hutchins_J/0/1/0/all/0/1">Jack Hutchins</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1">Shamiul Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampini_D/0/1/0/all/0/1">Dana S. Rampini</a>, <a href="http://arxiv.org/find/cs/1/au:+Oripov_B/0/1/0/all/0/1">Bakhrom G. Oripov</a>, <a href="http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1">Adam N. McCaughan</a>, <a href="http://arxiv.org/find/cs/1/au:+Aziz_A/0/1/0/all/0/1">Ahmedullah Aziz</a></p>
<p>The relentless pursuit of miniaturization and performance enhancement in
electronic devices has led to a fundamental challenge in the field of circuit
design and simulation: how to accurately account for the inherent stochastic
nature of certain devices. While conventional deterministic models have served
as indispensable tools for circuit designers, they fall short when it comes to
capture the subtle yet critical variability exhibited by many electronic
components. In this paper, we present an innovative approach that transcends
the limitations of traditional modeling techniques by harnessing the power of
machine learning, specifically Mixture Density Networks (MDNs), to faithfully
represent and simulate the stochastic behavior of electronic devices. We
demonstrate our approach to model heater cryotrons, where the model is able to
capture the stochastic switching dynamics observed in the experiment. Our model
shows 0.82% mean absolute error for switching probability. This paper marks a
significant step forward in the quest for accurate and versatile compact
models, poised to drive innovation in the realm of electronic circuits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05827">AccEPT: An Acceleration Scheme for Speeding Up Edge Pipeline-parallel Training. (arXiv:2311.05827v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuxuan Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qianqian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1">Yuanchao Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shibo He</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhiguo Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiming Chen</a></p>
<p>It is usually infeasible to fit and train an entire large deep neural network
(DNN) model using a single edge device due to the limited resources. To
facilitate intelligent applications across edge devices, researchers have
proposed partitioning a large model into several sub-models, and deploying each
of them to a different edge device to collaboratively train a DNN model.
However, the communication overhead caused by the large amount of data
transmitted from one device to another during training, as well as the
sub-optimal partition point due to the inaccurate latency prediction of
computation at each edge device can significantly slow down training. In this
paper, we propose AccEPT, an acceleration scheme for accelerating the edge
collaborative pipeline-parallel training. In particular, we propose a
light-weight adaptive latency predictor to accurately estimate the computation
latency of each layer at different devices, which also adapts to unseen devices
through continuous learning. Therefore, the proposed latency predictor leads to
better model partitioning which balances the computation loads across
participating devices. Moreover, we propose a bit-level computation-efficient
data compression scheme to compress the data to be transmitted between devices
during training. Our numerical results demonstrate that our proposed
acceleration approach is able to significantly speed up edge pipeline parallel
training up to 3 times faster in the considered experimental settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05836">Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1">Jing Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1">Qinrui Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1">Shu Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1">Siwei Lyu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xi Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a></p>
<p>In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05845">Tamil-Llama: A New Tamil Language Model Based on Llama 2. (arXiv:2311.05845v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Balachandran_A/0/1/0/all/0/1">Abhinand Balachandran</a></p>
<p>Language modeling has witnessed remarkable advancements in recent years, with
Large Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in
human-like text generation. However, a prevailing limitation is the
underrepresentation of languages like Tamil in these cutting-edge models,
leading to suboptimal performance in diverse linguistic contexts. This paper
addresses this lacuna, enhancing the open-source LLaMA model with an addition
of 16,000 Tamil tokens, aiming to achieve superior text generation and
comprehension in the Tamil language. We strategically employ the LoRA
methodology for efficient model training on a comprehensive Tamil corpus,
ensuring computational feasibility and model robustness. Moreover, we introduce
a Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca
dataset tailored for instruction fine-tuning. Our results showcase significant
performance improvements in Tamil text generation, with potential implications
for the broader landscape of LLMs in Indian languages. We further underscore
our commitment to open research by making our models, datasets, and code
publicly accessible, fostering further innovations in language modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05846">Clipped-Objective Policy Gradients for Pessimistic Policy Optimization. (arXiv:2311.05846v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Markowitz_J/0/1/0/all/0/1">Jared Markowitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Staley_E/0/1/0/all/0/1">Edward W. Staley</a></p>
<p>To facilitate efficient learning, policy gradient approaches to deep
reinforcement learning (RL) are typically paired with variance reduction
measures and strategies for making large but safe policy changes based on a
batch of experiences. Natural policy gradient methods, including Trust Region
Policy Optimization (TRPO), seek to produce monotonic improvement through
bounded changes in policy outputs. Proximal Policy Optimization (PPO) is a
commonly used, first-order algorithm that instead uses loss clipping to take
multiple safe optimization steps per batch of data, replacing the bound on the
single step of TRPO with regularization on multiple steps. In this work, we
find that the performance of PPO, when applied to continuous action spaces, may
be consistently improved through a simple change in objective. Instead of the
importance sampling objective of PPO, we instead recommend a basic policy
gradient, clipped in an equivalent fashion. While both objectives produce
biased gradient estimates with respect to the RL objective, they also both
display significantly reduced variance compared to the unbiased off-policy
policy gradient. Additionally, we show that (1) the clipped-objective policy
gradient (COPG) objective is on average "pessimistic" compared to both the PPO
objective and (2) this pessimism promotes enhanced exploration. As a result, we
empirically observe that COPG produces improved learning compared to PPO in
single-task, constrained, and multi-task learning, without adding significant
computational cost or complexity. Compared to TRPO, the COPG approach is seen
to offer comparable or superior performance, while retaining the simplicity of
a first-order method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05858">Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation. (arXiv:2311.05858v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Junyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1">Hyeongjun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1">Ilhoon Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kwanghoon Sohn</a></p>
<p>Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05866">Fair Supervised Learning with A Simple Random Sampler of Sensitive Attributes. (arXiv:2311.05866v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sohn_J/0/1/0/all/0/1">Jinwon Sohn</a>, <a href="http://arxiv.org/find/stat/1/au:+Song_Q/0/1/0/all/0/1">Qifan Song</a>, <a href="http://arxiv.org/find/stat/1/au:+Lin_G/0/1/0/all/0/1">Guang Lin</a></p>
<p>As the data-driven decision process becomes dominating for industrial
applications, fairness-aware machine learning arouses great attention in
various areas. This work proposes fairness penalties learned by neural networks
with a simple random sampler of sensitive attributes for non-discriminatory
supervised learning. In contrast to many existing works that critically rely on
the discreteness of sensitive attributes and response variables, the proposed
penalty is able to handle versatile formats of the sensitive attributes, so it
is more extensively applicable in practice than many existing algorithms. This
penalty enables us to build a computationally efficient group-level
in-processing fairness-aware training framework. Empirical evidence shows that
our framework enjoys better utility and fairness measures on popular benchmark
data sets than competing methods. We also theoretically characterize estimation
errors and loss of utility of the proposed neural-penalized risk minimization
problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05874">Testing Dependency of Unlabeled Databases. (arXiv:2311.05874v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Paslev_V/0/1/0/all/0/1">Vered Paslev</a>, <a href="http://arxiv.org/find/cs/1/au:+Huleihel_W/0/1/0/all/0/1">Wasim Huleihel</a></p>
<p>In this paper, we investigate the problem of deciding whether two random
databases $\mathsf{X}\in\mathcal{X}^{n\times d}$ and
$\mathsf{Y}\in\mathcal{Y}^{n\times d}$ are statistically dependent or not. This
is formulated as a hypothesis testing problem, where under the null hypothesis,
these two databases are statistically independent, while under the alternative,
there exists an unknown row permutation $\sigma$, such that $\mathsf{X}$ and
$\mathsf{Y}^\sigma$, a permuted version of $\mathsf{Y}$, are statistically
dependent with some known joint distribution, but have the same marginal
distributions as the null. We characterize the thresholds at which optimal
testing is information-theoretically impossible and possible, as a function of
$n$, $d$, and some spectral properties of the generative distributions of the
datasets. For example, we prove that if a certain function of the eigenvalues
of the likelihood function and $d$, is below a certain threshold, as
$d\to\infty$, then weak detection (performing slightly better than random
guessing) is statistically impossible, no matter what the value of $n$ is. This
mimics the performance of an efficient test that thresholds a centered version
of the log-likelihood function of the observed matrices. We also analyze the
case where $d$ is fixed, for which we derive strong (vanishing error) and weak
detection lower and upper bounds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05877">A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning. (arXiv:2311.05877v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cherepanova_V/0/1/0/all/0/1">Valeriia Cherepanova</a>, <a href="http://arxiv.org/find/cs/1/au:+Levin_R/0/1/0/all/0/1">Roman Levin</a>, <a href="http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1">Gowthami Somepalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1">Jonas Geiping</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1">C. Bayan Bruss</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1">Micah Goldblum</a></p>
<p>Academic tabular benchmarks often contain small sets of curated features. In
contrast, data scientists typically collect as many features as possible into
their datasets, and even engineer new features from existing ones. To prevent
overfitting in subsequent downstream modeling, practitioners commonly use
automated feature selection methods that identify a reduced subset of
informative features. Existing benchmarks for tabular feature selection
consider classical downstream models, toy synthetic datasets, or do not
evaluate feature selectors on the basis of downstream performance. Motivated by
the increasing popularity of tabular deep learning, we construct a challenging
feature selection benchmark evaluated on downstream neural networks including
transformers, using real datasets and multiple methods for generating
extraneous features. We also propose an input-gradient-based analogue of Lasso
for neural networks that outperforms classical feature selection methods on
challenging problems such as selecting from corrupted or second-order features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05884">Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems. (arXiv:2311.05884v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1">Huan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruoxi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1">Ke Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Long Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kula_M/0/1/0/all/0/1">Maciej Kula</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Taibai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lichan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1">Ed H. Chi</a></p>
<p>Learning feature interaction is the critical backbone to building recommender
systems. In web-scale applications, learning feature interaction is extremely
challenging due to the sparse and large input feature space; meanwhile,
manually crafting effective feature interactions is infeasible because of the
exponential solution space. We propose to leverage a Transformer-based
architecture with attention layers to automatically capture feature
interactions. Transformer architectures have witnessed great success in many
domains, such as natural language processing and computer vision. However,
there has not been much adoption of Transformer architecture for feature
interaction modeling in industry. We aim at closing the gap. We identify two
key challenges for applying the vanilla Transformer architecture to web-scale
recommender systems: (1) Transformer architecture fails to capture the
heterogeneous feature interactions in the self-attention layer; (2) The serving
latency of Transformer architecture might be too high to be deployed in
web-scale recommender systems. We first propose a heterogeneous self-attention
layer, which is a simple yet effective modification to the self-attention layer
in Transformer, to take into account the heterogeneity of feature interactions.
We then introduce \textsc{Hiformer} (\textbf{H}eterogeneous
\textbf{I}nteraction Trans\textbf{former}) to further improve the model
expressiveness. With low-rank approximation and model pruning, \hiformer enjoys
fast inference for online deployment. Extensive offline experiment results
corroborates the effectiveness and efficiency of the \textsc{Hiformer} model.
We have successfully deployed the \textsc{Hiformer} model to a real world large
scale App ranking model at Google Play, with significant improvement in key
engagement metrics (up to +2.66\%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05888">Low-Multi-Rank High-Order Bayesian Robust Tensor Factorization. (arXiv:2311.05888v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunguang Li</a></p>
<p>The recently proposed tensor robust principal component analysis (TRPCA)
methods based on tensor singular value decomposition (t-SVD) have achieved
numerous successes in many fields. However, most of these methods are only
applicable to third-order tensors, whereas the data obtained in practice are
often of higher order, such as fourth-order color videos, fourth-order
hyperspectral videos, and fifth-order light-field images. Additionally, in the
t-SVD framework, the multi-rank of a tensor can describe more fine-grained
low-rank structure in the tensor compared with the tubal rank. However,
determining the multi-rank of a tensor is a much more difficult problem than
determining the tubal rank. Moreover, most of the existing TRPCA methods do not
explicitly model the noises except the sparse noise, which may compromise the
accuracy of estimating the low-rank tensor. In this work, we propose a novel
high-order TRPCA method, named as Low-Multi-rank High-order Bayesian Robust
Tensor Factorization (LMH-BRTF), within the Bayesian framework. Specifically,
we decompose the observed corrupted tensor into three parts, i.e., the low-rank
component, the sparse component, and the noise component. By constructing a
low-rank model for the low-rank component based on the order-$d$ t-SVD and
introducing a proper prior for the model, LMH-BRTF can automatically determine
the tensor multi-rank. Meanwhile, benefiting from the explicit modeling of both
the sparse and noise components, the proposed method can leverage information
from the noises more effectivly, leading to an improved performance of TRPCA.
Then, an efficient variational inference algorithm is established for
parameters estimation. Empirical studies on synthetic and real-world datasets
demonstrate the effectiveness of the proposed method in terms of both
qualitative and quantitative results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05889">Semantic Map Guided Synthesis of Wireless Capsule Endoscopy Images using Diffusion Models. (arXiv:2311.05889v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1">Haejin Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Ju_J/0/1/0/all/0/1">Jeongwoo Ju</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Jonghyuck Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1">Yeoun Joo Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_H/0/1/0/all/0/1">Heechul Jung</a></p>
<p>Wireless capsule endoscopy (WCE) is a non-invasive method for visualizing the
gastrointestinal (GI) tract, crucial for diagnosing GI tract diseases. However,
interpreting WCE results can be time-consuming and tiring. Existing studies
have employed deep neural networks (DNNs) for automatic GI tract lesion
detection, but acquiring sufficient training examples, particularly due to
privacy concerns, remains a challenge. Public WCE databases lack diversity and
quantity. To address this, we propose a novel approach leveraging generative
models, specifically the diffusion model (DM), for generating diverse WCE
images. Our model incorporates semantic map resulted from visualization scale
(VS) engine, enhancing the controllability and diversity of generated images.
We evaluate our approach using visual inspection and visual Turing tests,
demonstrating its effectiveness in generating realistic and diverse WCE images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05908">FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. (arXiv:2311.05908v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1">Daniel Y. Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumbong_H/0/1/0/all/0/1">Hermann Kumbong</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1">Eric Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1">Christopher R&#xe9;</a></p>
<p>Convolution models with long filters have demonstrated state-of-the-art
reasoning abilities in many long-sequence tasks but lag behind the most
optimized Transformers in wall-clock time. A major bottleneck is the Fast
Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$
time in sequence length $N$ but has poor hardware utilization. In this paper,
we study how to optimize the FFT convolution. We find two key bottlenecks: the
FFT does not effectively use specialized matrix multiply units, and it incurs
expensive I/O between layers of the memory hierarchy. In response, we propose
FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT
using matrix multiply units and enables kernel fusion for long sequences,
reducing I/O. We also present two sparse convolution algorithms--1) partial
convolutions and 2) frequency-sparse convolutions--which can be implemented
simply by skipping blocks in the matrix decomposition, enabling further
opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT
convolutions by up to 7.93$\times$ over PyTorch and achieves up to 4.4$\times$
speedup end-to-end. Given the same compute budget, FlashFFTConv allows
Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and
M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with
twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on
Path-512, a high-resolution vision task where no model had previously achieved
better than 50%. Furthermore, partial convolutions enable longer-sequence
models--yielding the first DNA model that can process the longest human genes
(2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models
while maintaining or improving model quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05911">An alternative for one-hot encoding in neural network models. (arXiv:2311.05911v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zlatic_L/0/1/0/all/0/1">Lazar Zlati&#x107;</a></p>
<p>This paper proposes an algorithm that implements binary encoding of the
categorical features of neural network model input data, while also
implementing changes in the forward and backpropagation procedures in order to
achieve the property of having model weight changes, that result from the
neural network learning process for certain data instances of some feature
category, only affect the forward pass calculations for input data instances of
that same feature category, as it is in the case of utilising one-hot encoding
for categorical features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05924">Federated Learning with Manifold Regularization and Normalized Update Reaggregation. (arXiv:2311.05924v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1">Xuming An</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Han Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yong Luo</a></p>
<p>Federated Learning (FL) is an emerging collaborative machine learning
framework where multiple clients train the global model without sharing their
own datasets. In FL, the model inconsistency caused by the local data
heterogeneity across clients results in the near-orthogonality of client
updates, which leads to the global update norm reduction and slows down the
convergence. Most previous works focus on eliminating the difference of
parameters (or gradients) between the local and global models, which may fail
to reflect the model inconsistency due to the complex structure of the machine
learning model and the Euclidean space's limitation in meaningful geometric
representations. In this paper, we propose FedMRUR by adopting the manifold
model fusion scheme and a new global optimizer to alleviate the negative
impacts. Concretely, FedMRUR adopts a hyperbolic graph manifold regularizer
enforcing the representations of the data in the local and global models are
close to each other in a low-dimensional subspace. Because the machine learning
model has the graph structure, the distance in hyperbolic space can reflect the
model bias better than the Euclidean distance. In this way, FedMRUR exploits
the manifold structures of the representations to significantly reduce the
model inconsistency. FedMRUR also aggregates the client updates norms as the
global update norm, which can appropriately enlarge each client's contribution
to the global update, thereby mitigating the norm reduction introduced by the
near-orthogonality of client updates. Furthermore, we theoretically prove that
our algorithm can achieve a linear speedup property for non-convex setting
under partial client participation.Experiments demonstrate that FedMRUR can
achieve a new state-of-the-art (SOTA) accuracy with less communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05928">The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models. (arXiv:2311.05928v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1">Anton Razzhigaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikhalchuk_M/0/1/0/all/0/1">Matvey Mikhalchuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Goncharova_E/0/1/0/all/0/1">Elizaveta Goncharova</a>, <a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1">Ivan Oseledets</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a></p>
<p>In this study, we present an investigation into the anisotropy dynamics and
intrinsic dimension of embeddings in transformer architectures, focusing on the
dichotomy between encoders and decoders. Our findings reveal that the
anisotropy profile in transformer decoders exhibits a distinct bell-shaped
curve, with the highest anisotropy concentrations in the middle layers. This
pattern diverges from the more uniformly distributed anisotropy observed in
encoders. In addition, we found that the intrinsic dimension of embeddings
increases in the initial phases of training, indicating an expansion into
higher-dimensional space. Which is then followed by a compression phase towards
the end of training with dimensionality decrease, suggesting a refinement into
more compact representations. Our results provide fresh insights to the
understanding of encoders and decoders embedding properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05931">Anytime-Valid Confidence Sequences for Consistent Uncertainty Estimation in Early-Exit Neural Networks. (arXiv:2311.05931v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jazbec_M/0/1/0/all/0/1">Metod Jazbec</a>, <a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1">Patrick Forr&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1">Stephan Mandt</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nalisnick_E/0/1/0/all/0/1">Eric Nalisnick</a></p>
<p>Early-exit neural networks (EENNs) facilitate adaptive inference by producing
predictions at multiple stages of the forward pass. In safety-critical
applications, these predictions are only meaningful when complemented with
reliable uncertainty estimates. Yet, due to their sequential structure, an
EENN's uncertainty estimates should also be consistent: labels that are deemed
improbable at one exit should not reappear within the confidence interval / set
of later exits. We show that standard uncertainty quantification techniques,
like Bayesian methods or conformal prediction, can lead to inconsistency across
exits. We address this problem by applying anytime-valid confidence sequences
(AVCSs) to the exits of EENNs. By design, AVCSs maintain consistency across
exits. We examine the theoretical and practical challenges of applying AVCSs to
EENNs and empirically validate our approach on both regression and
classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05936">Aggregation Weighting of Federated Learning via Generalization Bound Estimation. (arXiv:2311.05936v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mingwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xiaofeng Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ivor W.Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1">James T.Kwok</a></p>
<p>Federated Learning (FL) typically aggregates client model parameters using a
weighting approach determined by sample proportions. However, this naive
weighting method may lead to unfairness and degradation in model performance
due to statistical heterogeneity and the inclusion of noisy data among clients.
Theoretically, distributional robustness analysis has shown that the
generalization performance of a learning model with respect to any shifted
distribution is bounded. This motivates us to reconsider the weighting approach
in federated learning. In this paper, we replace the aforementioned weighting
method with a new strategy that considers the generalization bounds of each
local model. Specifically, we estimate the upper and lower bounds of the
second-order origin moment of the shifted distribution for the current local
model, and then use these bounds disagreements as the aggregation proportions
for weightings in each communication round. Experiments demonstrate that the
proposed weighting strategy significantly improves the performance of several
representative FL algorithms on benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05941">Learning-Augmented Scheduling for Solar-Powered Electric Vehicle Charging. (arXiv:2311.05941v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Tongxin Li</a></p>
<p>We tackle the complex challenge of scheduling the charging of electric
vehicles (EVs) equipped with solar panels and batteries, particularly under
out-of-distribution (OOD) conditions. Traditional scheduling approaches, such
as reinforcement learning (RL) and model predictive control (MPC), often fail
to provide satisfactory results when faced with OOD data, struggling to balance
robustness (worst-case performance) and consistency (near-optimal average
performance). To address this gap, we introduce a novel learning-augmented
policy. This policy employs a dynamic robustness budget, which is adapted in
real-time based on the reinforcement learning policy's performance.
Specifically, it leverages the temporal difference (TD) error, a measure of the
learning policy's prediction accuracy, to assess the trustworthiness of the
machine-learned policy. This method allows for a more effective balance between
consistency and robustness in EV charging schedules, significantly enhancing
adaptability and efficiency in real-world, unpredictable environments. Our
results demonstrate that this approach markedly improves scheduling
effectiveness and reliability, particularly in OOD contexts, paving the way for
more resilient and adaptive EV charging systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05956">ID Embedding as Subtle Features of Content and Structure for Multimodal Recommendation. (arXiv:2311.05956v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuting Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1">Enneng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1">Yizhou Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1">Guibing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuliang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Linying Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwei Wang</a></p>
<p>Multimodal recommendation aims to model user and item representations
comprehensively with the involvement of multimedia content for effective
recommendations. Existing research has shown that it is beneficial for
recommendation performance to combine (user- and item-) ID embeddings with
multimodal salient features, indicating the value of IDs. However, there is a
lack of a thorough analysis of the ID embeddings in terms of feature semantics
in the literature. In this paper, we revisit the value of ID embeddings for
multimodal recommendation and conduct a thorough study regarding its semantics,
which we recognize as subtle features of content and structures. Then, we
propose a novel recommendation model by incorporating ID embeddings to enhance
the semantic features of both content and structures. Specifically, we put
forward a hierarchical attention mechanism to incorporate ID embeddings in
modality fusing, coupled with contrastive learning, to enhance content
representations. Meanwhile, we propose a lightweight graph convolutional
network for each modality to amalgamate neighborhood and ID embeddings for
improving structural representations. Finally, the content and structure
representations are combined to form the ultimate item embedding for
recommendation. Extensive experiments on three real-world datasets (Baby,
Sports, and Clothing) demonstrate the superiority of our method over
state-of-the-art multimodal recommendation methods and the effectiveness of
fine-grained ID embeddings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05961">Hierarchical deep learning-based adaptive time-stepping scheme for multiscale simulations. (arXiv:2311.05961v1 [math.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Hamid_A/0/1/0/all/0/1">Asif Hamid</a>, <a href="http://arxiv.org/find/math/1/au:+Rafiq_D/0/1/0/all/0/1">Danish Rafiq</a>, <a href="http://arxiv.org/find/math/1/au:+Nahvi_S/0/1/0/all/0/1">Shahkar Ahmad Nahvi</a>, <a href="http://arxiv.org/find/math/1/au:+Bazaz_M/0/1/0/all/0/1">Mohammad Abid Bazaz</a></p>
<p>Multiscale is a hallmark feature of complex nonlinear systems. While the
simulation using the classical numerical methods is restricted by the local
\textit{Taylor} series constraints, the multiscale techniques are often limited
by finding heuristic closures. This study proposes a new method for simulating
multiscale problems using deep neural networks. By leveraging the hierarchical
learning of neural network time steppers, the method adapts time steps to
approximate dynamical system flow maps across timescales. This approach
achieves state-of-the-art performance in less computational time compared to
fixed-step neural network solvers. The proposed method is demonstrated on
several nonlinear dynamical systems, and source codes are provided for
implementation. This method has the potential to benefit multiscale analysis of
complex systems and encourage further investigation in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05964">Multiscale Neural Operators for Solving Time-Independent PDEs. (arXiv:2311.05964v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ripken_W/0/1/0/all/0/1">Winfried Ripken</a>, <a href="http://arxiv.org/find/cs/1/au:+Coiffard_L/0/1/0/all/0/1">Lisa Coiffard</a>, <a href="http://arxiv.org/find/cs/1/au:+Pieper_F/0/1/0/all/0/1">Felix Pieper</a>, <a href="http://arxiv.org/find/cs/1/au:+Dziadzio_S/0/1/0/all/0/1">Sebastian Dziadzio</a></p>
<p>Time-independent Partial Differential Equations (PDEs) on large meshes pose
significant challenges for data-driven neural PDE solvers. We introduce a novel
graph rewiring technique to tackle some of these challenges, such as
aggregating information across scales and on irregular meshes. Our proposed
approach bridges distant nodes, enhancing the global interaction capabilities
of GNNs. Our experiments on three datasets reveal that GNN-based methods set
new performance standards for time-independent PDEs on irregular meshes.
Finally, we show that our graph rewiring strategy boosts the performance of
baseline methods, achieving state-of-the-art results in one of the tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05967">Plasma Surrogate Modelling using Fourier Neural Operators. (arXiv:2311.05967v1 [physics.plasm-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Gopakumar_V/0/1/0/all/0/1">Vignesh Gopakumar</a>, <a href="http://arxiv.org/find/physics/1/au:+Pamela_S/0/1/0/all/0/1">Stanislas Pamela</a>, <a href="http://arxiv.org/find/physics/1/au:+Zanisi_L/0/1/0/all/0/1">Lorenzo Zanisi</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_Z/0/1/0/all/0/1">Zongyi Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Gray_A/0/1/0/all/0/1">Ander Gray</a>, <a href="http://arxiv.org/find/physics/1/au:+Brennand_D/0/1/0/all/0/1">Daniel Brennand</a>, <a href="http://arxiv.org/find/physics/1/au:+Bhatia_N/0/1/0/all/0/1">Nitesh Bhatia</a>, <a href="http://arxiv.org/find/physics/1/au:+Stathopoulos_G/0/1/0/all/0/1">Gregory Stathopoulos</a>, <a href="http://arxiv.org/find/physics/1/au:+Kusner_M/0/1/0/all/0/1">Matt Kusner</a>, <a href="http://arxiv.org/find/physics/1/au:+Deisenroth_M/0/1/0/all/0/1">Marc Peter Deisenroth</a>, <a href="http://arxiv.org/find/physics/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>, <a href="http://arxiv.org/find/physics/1/au:+Team_JOREK/0/1/0/all/0/1">JOREK Team</a>, <a href="http://arxiv.org/find/physics/1/au:+Team_MAST/0/1/0/all/0/1">MAST Team</a></p>
<p>Predicting plasma evolution within a Tokamak reactor is crucial to realizing
the goal of sustainable fusion. Capabilities in forecasting the spatio-temporal
evolution of plasma rapidly and accurately allow us to quickly iterate over
design and control strategies on current Tokamak devices and future reactors.
Modelling plasma evolution using numerical solvers is often expensive,
consuming many hours on supercomputers, and hence, we need alternative
inexpensive surrogate models. We demonstrate accurate predictions of plasma
evolution both in simulation and experimental domains using deep learning-based
surrogate modelling tools, viz., Fourier Neural Operators (FNO). We show that
FNO has a speedup of six orders of magnitude over traditional solvers in
predicting the plasma dynamics simulated from magnetohydrodynamic models, while
maintaining a high accuracy (MSE $\approx$ $10^{-5}$). Our modified version of
the FNO is capable of solving multi-variable Partial Differential Equations
(PDE), and can capture the dependence among the different variables in a single
model. FNOs can also predict plasma evolution on real-world experimental data
observed by the cameras positioned within the MAST Tokamak, i.e., cameras
looking across the central solenoid and the divertor in the Tokamak. We show
that FNOs are able to accurately forecast the evolution of plasma and have the
potential to be deployed for real-time monitoring. We also illustrate their
capability in forecasting the plasma shape, the locations of interactions of
the plasma with the central solenoid and the divertor for the full duration of
the plasma shot within MAST. The FNO offers a viable alternative for surrogate
modelling as it is quick to train and infer, and requires fewer data points,
while being able to do zero-shot super-resolution and getting high-fidelity
solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05975">Sum-max Submodular Bandits. (arXiv:2311.05975v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pasteris_S/0/1/0/all/0/1">Stephen Pasteris</a>, <a href="http://arxiv.org/find/cs/1/au:+Rumi_A/0/1/0/all/0/1">Alberto Rumi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vitale_F/0/1/0/all/0/1">Fabio Vitale</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1">Nicol&#xf2; Cesa-Bianchi</a></p>
<p>Many online decision-making problems correspond to maximizing a sequence of
submodular functions. In this work, we introduce sum-max functions, a subclass
of monotone submodular functions capturing several interesting problems,
including best-of-$K$-bandits, combinatorial bandits, and the bandit versions
on facility location, $M$-medians, and hitting sets. We show that all functions
in this class satisfy a key property that we call pseudo-concavity. This allows
us to prove $\big(1 - \frac{1}{e}\big)$-regret bounds for bandit feedback in
the nonstochastic setting of the order of $\sqrt{MKT}$ (ignoring log factors),
where $T$ is the time horizon and $M$ is a cardinality constraint. This bound,
attained by a simple and efficient algorithm, significantly improves on the
$\widetilde{O}\big(T^{2/3}\big)$ regret bound for online monotone submodular
maximization with bandit feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05992">Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous. (arXiv:2311.05992v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1">Nabil Aouf</a>, <a href="http://arxiv.org/find/cs/1/au:+Pizarro_J/0/1/0/all/0/1">Jose Pizarro</a>, <a href="http://arxiv.org/find/cs/1/au:+Honvault_C/0/1/0/all/0/1">Christophe Honvault</a></p>
<p>Research on developing deep learning techniques for autonomous spacecraft
relative navigation challenges is continuously growing in recent years.
Adopting those techniques offers enhanced performance. However, such approaches
also introduce heightened apprehensions regarding the trustability and security
of such deep learning methods through their susceptibility to adversarial
attacks. In this work, we propose a novel approach for adversarial attack
detection for deep neural network-based relative pose estimation schemes based
on the explainability concept. We develop for an orbital rendezvous scenario an
innovative relative pose estimation technique adopting our proposed
Convolutional Neural Network (CNN), which takes an image from the chaser's
onboard camera and outputs accurately the target's relative position and
rotation. We perturb seamlessly the input images using adversarial attacks that
are generated by the Fast Gradient Sign Method (FGSM). The adversarial attack
detector is then built based on a Long Short Term Memory (LSTM) network which
takes the explainability measure namely SHapley Value from the CNN-based pose
estimator and flags the detection of adversarial attacks when acting.
Simulation results show that the proposed adversarial attack detector achieves
a detection accuracy of 99.21%. Both the deep relative pose estimator and
adversarial attack detector are then tested on real data captured from our
laboratory-designed setup. The experimental results from our
laboratory-designed setup demonstrate that the proposed adversarial attack
detector achieves an average detection accuracy of 96.29%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06012">Doubly Robust Structure Identification from Temporal Data. (arXiv:2311.06012v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Angelis_E/0/1/0/all/0/1">Emmanouil Angelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Quinzan_F/0/1/0/all/0/1">Francesco Quinzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Soleymani_A/0/1/0/all/0/1">Ashkan Soleymani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1">Patrick Jaillet</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1">Stefan Bauer</a></p>
<p>Learning the causes of time-series data is a fundamental task in many
applications, spanning from finance to earth sciences or bio-medical
applications. Common approaches for this task are based on vector
auto-regression, and they do not take into account unknown confounding between
potential causes. However, in settings with many potential causes and noisy
data, these approaches may be substantially biased. Furthermore, potential
causes may be correlated in practical applications. Moreover, existing
algorithms often do not work with cyclic data. To address these challenges, we
propose a new doubly robust method for Structure Identification from Temporal
Data ( SITD ). We provide theoretical guarantees, showing that our method
asymptotically recovers the true underlying causal structure. Our analysis
extends to cases where the potential causes have cycles and they may be
confounded. We further perform extensive experiments to showcase the superior
performance of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06028">Symbolic Regression as Feature Engineering Method for Machine and Deep Learning Regression Tasks. (arXiv:2311.06028v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shmuel_A/0/1/0/all/0/1">Assaf Shmuel</a>, <a href="http://arxiv.org/find/cs/1/au:+Glickman_O/0/1/0/all/0/1">Oren Glickman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazebnik_T/0/1/0/all/0/1">Teddy Lazebnik</a></p>
<p>In the realm of machine and deep learning regression tasks, the role of
effective feature engineering (FE) is pivotal in enhancing model performance.
Traditional approaches of FE often rely on domain expertise to manually design
features for machine learning models. In the context of deep learning models,
the FE is embedded in the neural network's architecture, making it hard for
interpretation. In this study, we propose to integrate symbolic regression (SR)
as an FE process before a machine learning model to improve its performance. We
show, through extensive experimentation on synthetic and real-world
physics-related datasets, that the incorporation of SR-derived features
significantly enhances the predictive capabilities of both machine and deep
learning regression models with 34-86% root mean square error (RMSE)
improvement in synthetic datasets and 4-11.5% improvement in real-world
datasets. In addition, as a realistic use-case, we show the proposed method
improves the machine learning performance in predicting superconducting
critical temperatures based on Eliashberg theory by more than 20% in terms of
RMSE. These results outline the potential of SR as an FE component in
data-driven models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06057">Ulcerative Colitis Mayo Endoscopic Scoring Classification with Active Learning and Generative Data Augmentation. (arXiv:2311.06057v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Caglar_U/0/1/0/all/0/1">&#xdc;mit Mert &#xc7;a&#x11f;lar</a>, <a href="http://arxiv.org/find/eess/1/au:+Inci_A/0/1/0/all/0/1">Alperen &#x130;nci</a>, <a href="http://arxiv.org/find/eess/1/au:+Hanoglu_O/0/1/0/all/0/1">O&#x11f;uz Hano&#x11f;lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1">G&#xf6;rkem Polat</a>, <a href="http://arxiv.org/find/eess/1/au:+Temizel_A/0/1/0/all/0/1">Alptekin Temizel</a></p>
<p>Endoscopic imaging is commonly used to diagnose Ulcerative Colitis (UC) and
classify its severity. It has been shown that deep learning based methods are
effective in automated analysis of these images and can potentially be used to
aid medical doctors. Unleashing the full potential of these methods depends on
the availability of large amount of labeled images; however, obtaining and
labeling these images are quite challenging. In this paper, we propose a active
learning based generative augmentation method. The method involves generating a
large number of synthetic samples by training using a small dataset consisting
of real endoscopic images. The resulting data pool is narrowed down by using
active learning methods to select the most informative samples, which are then
used to train a classifier. We demonstrate the effectiveness of our method
through experiments on a publicly available endoscopic image dataset. The
results show that using synthesized samples in conjunction with active learning
leads to improved classification performance compared to using only the
original labeled examples and the baseline classification performance of 68.1%
increases to 74.5% in terms of Quadratic Weighted Kappa (QWK) Score. Another
observation is that, attaining equivalent performance using only real data
necessitated three times higher number of images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06059">Improved Positional Encoding for Implicit Neural Representation based Compact Data Representation. (arXiv:2311.06059v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1">Bharath Bhushan Damodaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1">Francois Schnitzler</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_A/0/1/0/all/0/1">Anne Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1">Pierre Hellier</a></p>
<p>Positional encodings are employed to capture the high frequency information
of the encoded signals in implicit neural representation (INR). In this paper,
we propose a novel positional encoding method which improves the reconstruction
quality of the INR. The proposed embedding method is more advantageous for the
compact data representation because it has a greater number of frequency basis
than the existing methods. Our experiments shows that the proposed method
achieves significant gain in the rate-distortion performance without
introducing any additional complexity in the compression task and higher
reconstruction quality in novel view synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06062">Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration. (arXiv:2311.06062v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1">Wenjie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huandong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chen Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guanghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tao Jiang</a></p>
<p>Membership Inference Attacks (MIA) aim to infer whether a target data record
has been utilized for model training or not. Prior attempts have quantified the
privacy risks of language models (LMs) via MIAs, but there is still no
consensus on whether existing MIA algorithms can cause remarkable privacy
leakage on practical Large Language Models (LLMs). Existing MIAs designed for
LMs can be classified into two categories: reference-free and reference-based
attacks. They are both based on the hypothesis that training records
consistently strike a higher probability of being sampled. Nevertheless, this
hypothesis heavily relies on the overfitting of target models, which will be
mitigated by multiple regularization methods and the generalization of LLMs.
The reference-based attack seems to achieve promising effectiveness in LLMs,
which measures a more reliable membership signal by comparing the probability
discrepancy between the target model and the reference model. However, the
performance of reference-based attack is highly dependent on a reference
dataset that closely resembles the training dataset, which is usually
inaccessible in the practical scenario. Overall, existing MIAs are unable to
effectively unveil privacy leakage over practical fine-tuned LLMs that are
overfitting-free and private. We propose a Membership Inference Attack based on
Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since
memorization in LLMs is inevitable during the training process and occurs
before overfitting, we introduce a more reliable membership signal,
probabilistic variation, which is based on memorization rather than
overfitting. Furthermore, we introduce a self-prompt approach, which constructs
the dataset to fine-tune the reference model by prompting the target LLM
itself. In this manner, the adversary can collect a dataset with a similar
distribution from public APIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06095">Dual input stream transformer for eye-tracking line assignment. (arXiv:2311.06095v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mercier_T/0/1/0/all/0/1">Thomas M. Mercier</a>, <a href="http://arxiv.org/find/cs/1/au:+Budka_M/0/1/0/all/0/1">Marcin Budka</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilev_M/0/1/0/all/0/1">Martin R. Vasilev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirkby_J/0/1/0/all/0/1">Julie A. Kirkby</a>, <a href="http://arxiv.org/find/cs/1/au:+Angele_B/0/1/0/all/0/1">Bernhard Angele</a>, <a href="http://arxiv.org/find/cs/1/au:+Slattery_T/0/1/0/all/0/1">Timothy J. Slattery</a></p>
<p>We introduce a novel Dual Input Stream Transformer (DIST) for the challenging
problem of assigning fixation points from eye-tracking data collected during
passage reading to the line of text that the reader was actually focused on.
This post-processing step is crucial for analysis of the reading data due to
the presence of noise in the form of vertical drift. We evaluate DIST against
nine classical approaches on a comprehensive suite of nine diverse datasets,
and demonstrate DIST's superiority. By combining multiple instances of the DIST
model in an ensemble we achieve an average accuracy of 98.5\% across all
datasets. Our approach presents a significant step towards addressing the
bottleneck of manual line assignment in reading research. Through extensive
model analysis and ablation studies, we identify key factors that contribute to
DIST's success, including the incorporation of line overlap features and the
use of a second input stream. Through evaluation on a set of diverse datasets
we demonstrate that DIST is robust to various experimental setups, making it a
safe first choice for practitioners in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06101">In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models. (arXiv:2311.06101v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zecchin_M/0/1/0/all/0/1">Matteo Zecchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1">Osvaldo Simeone</a></p>
<p>Large pre-trained sequence models, such as transformer-based architectures,
have been recently shown to have the capacity to carry out in-context learning
(ICL). In ICL, a decision on a new input is made via a direct mapping of the
input and of a few examples from the given task, serving as the task's context,
to the output variable. No explicit updates of model parameters are needed to
tailor the decision to a new task. Pre-training, which amounts to a form of
meta-learning, is based on the observation of examples from several related
tasks. Prior work has shown ICL capabilities for linear regression. In this
study, we leverage ICL to address the inverse problem of multiple-input and
multiple-output (MIMO) equalization based on a context given by pilot symbols.
A task is defined by the unknown fading channel and by the signal-to-noise
ratio (SNR) level, which may be known. To highlight the practical potential of
the approach, we allow for the presence of quantization of the received
signals. We demonstrate via numerical results that transformer-based ICL has a
threshold behavior, whereby, as the number of pre-training tasks grows, the
performance switches from that of a minimum mean squared error (MMSE) equalizer
with a prior determined by the pre-trained tasks to that of an MMSE equalizer
with the true data-generating prior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06103">1-Lipschitz Neural Networks are more expressive with N-Activations. (arXiv:2311.06103v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prach_B/0/1/0/all/0/1">Bernd Prach</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1">Christoph H. Lampert</a></p>
<p>A crucial property for achieving secure, trustworthy and interpretable deep
learning systems is their robustness: small changes to a system's inputs should
not result in large changes to its outputs. Mathematically, this means one
strives for networks with a small Lipschitz constant. Several recent works have
focused on how to construct such Lipschitz networks, typically by imposing
constraints on the weight matrices. In this work, we study an orthogonal
aspect, namely the role of the activation function. We show that commonly used
activation functions, such as MaxMin, as well as all piece-wise linear ones
with two segments unnecessarily restrict the class of representable functions,
even in the simplest one-dimensional setting. We furthermore introduce the new
N-activation function that is provably more expressive than currently popular
activation functions. We provide code at
https://github.com/berndprach/NActivation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06110">An Interpretable Machine Learning Framework to Understand Bikeshare Demand before and during the COVID-19 Pandemic in New York City. (arXiv:2311.06110v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1">Majbah Uddin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1">Ho-Ling Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasnine_M/0/1/0/all/0/1">Md Sami Hasnine</a></p>
<p>In recent years, bikesharing systems have become increasingly popular as
affordable and sustainable micromobility solutions. Advanced mathematical
models such as machine learning are required to generate good forecasts for
bikeshare demand. To this end, this study proposes a machine learning modeling
framework to estimate hourly demand in a large-scale bikesharing system. Two
Extreme Gradient Boosting models were developed: one using data from before the
COVID-19 pandemic (March 2019 to February 2020) and the other using data from
during the pandemic (March 2020 to February 2021). Furthermore, a model
interpretation framework based on SHapley Additive exPlanations was
implemented. Based on the relative importance of the explanatory variables
considered in this study, share of female users and hour of day were the two
most important explanatory variables in both models. However, the month
variable had higher importance in the pandemic model than in the pre-pandemic
model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06112">Turbulence Scaling from Deep Learning Diffusion Generative Models. (arXiv:2311.06112v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Whittaker_T/0/1/0/all/0/1">Tim Whittaker</a>, <a href="http://arxiv.org/find/physics/1/au:+Janik_R/0/1/0/all/0/1">Romuald A. Janik</a>, <a href="http://arxiv.org/find/physics/1/au:+Oz_Y/0/1/0/all/0/1">Yaron Oz</a></p>
<p>Complex spatial and temporal structures are inherent characteristics of
turbulent fluid flows and comprehending them poses a major challenge. This
comprehesion necessitates an understanding of the space of turbulent fluid flow
configurations. We employ a diffusion-based generative model to learn the
distribution of turbulent vorticity profiles and generate snapshots of
turbulent solutions to the incompressible Navier-Stokes equations. We consider
the inverse cascade in two spatial dimensions and generate diverse turbulent
solutions that differ from those in the training dataset. We analyze the
statistical scaling properties of the new turbulent profiles, calculate their
structure functions, energy power spectrum, velocity probability distribution
function and moments of local energy dissipation. All the learnt scaling
exponents are consistent with the expected Kolmogorov scaling and have lower
errors than the training ones. This agreement with established turbulence
characteristics provides strong evidence of the model's capability to capture
essential features of real-world turbulence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06117">Distributionally Robust Skeleton Learning of Discrete Bayesian Networks. (arXiv:2311.06117v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yeshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziebart_B/0/1/0/all/0/1">Brian D. Ziebart</a></p>
<p>We consider the problem of learning the exact skeleton of general discrete
Bayesian networks from potentially corrupted data. Building on distributionally
robust optimization and a regression approach, we propose to optimize the most
adverse risk over a family of distributions within bounded Wasserstein distance
or KL divergence to the empirical distribution. The worst-case risk accounts
for the effect of outliers. The proposed approach applies for general
categorical random variables without assuming faithfulness, an ordinal
relationship or a specific form of conditional distribution. We present
efficient algorithms and show the proposed methods are closely related to the
standard regularized regression approach. Under mild assumptions, we derive
non-asymptotic guarantees for successful structure learning with logarithmic
sample complexities for bounded-degree graphs. Numerical study on synthetic and
real datasets validates the effectiveness of our method. Code is available at
https://github.com/DanielLeee/drslbn.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06118">Exploring the Efficacy of Base Data Augmentation Methods in Deep Learning-Based Radiograph Classification of Knee Joint Osteoarthritis. (arXiv:2311.06118v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Prezja_F/0/1/0/all/0/1">Fabi Prezja</a>, <a href="http://arxiv.org/find/eess/1/au:+Annala_L/0/1/0/all/0/1">Leevi Annala</a>, <a href="http://arxiv.org/find/eess/1/au:+Kiiskinen_S/0/1/0/all/0/1">Sampsa Kiiskinen</a>, <a href="http://arxiv.org/find/eess/1/au:+Ojala_T/0/1/0/all/0/1">Timo Ojala</a></p>
<p>Diagnosing knee joint osteoarthritis (KOA), a major cause of disability
worldwide, is challenging due to subtle radiographic indicators and the varied
progression of the disease. Using deep learning for KOA diagnosis requires
broad, comprehensive datasets. However, obtaining these datasets poses
significant challenges due to patient privacy concerns and data collection
restrictions. Additive data augmentation, which enhances data variability,
emerges as a promising solution. Yet, it's unclear which augmentation
techniques are most effective for KOA. This study explored various data
augmentation methods, including adversarial augmentations, and their impact on
KOA classification model performance. While some techniques improved
performance, others commonly used underperformed. We identified potential
confounding regions within the images using adversarial augmentation. This was
evidenced by our models' ability to classify KL0 and KL4 grades accurately,
with the knee joint omitted. This observation suggested a model bias, which
might leverage unrelated features for classification currently present in
radiographs. Interestingly, removing the knee joint also led to an unexpected
improvement in KL1 classification accuracy. To better visualize these
paradoxical effects, we employed Grad-CAM, highlighting the associated regions.
Our study underscores the need for careful technique selection for improved
model performance and identifying and managing potential confounding regions in
radiographic KOA deep learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06138">Minimum norm interpolation by perceptra: Explicit regularization and implicit bias. (arXiv:2311.06138v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Park_J/0/1/0/all/0/1">Jiyoung Park</a>, <a href="http://arxiv.org/find/stat/1/au:+Pelakh_I/0/1/0/all/0/1">Ian Pelakh</a>, <a href="http://arxiv.org/find/stat/1/au:+Wojtowytsch_S/0/1/0/all/0/1">Stephan Wojtowytsch</a></p>
<p>We investigate how shallow ReLU networks interpolate between known regions.
Our analysis shows that empirical risk minimizers converge to a minimum norm
interpolant as the number of data points and parameters tends to infinity when
a weight decay regularizer is penalized with a coefficient which vanishes at a
precise rate as the network width and the number of data points grow. With and
without explicit regularization, we numerically study the implicit bias of
common optimization algorithms towards known minimum norm interpolants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06152">Going beyond persistent homology using persistent homology. (arXiv:2311.06152v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Immonen_J/0/1/0/all/0/1">Johanna Immonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_A/0/1/0/all/0/1">Amauri H. Souza</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_V/0/1/0/all/0/1">Vikas Garg</a></p>
<p>Representational limits of message-passing graph neural networks (MP-GNNs),
e.g., in terms of the Weisfeiler-Leman (WL) test for isomorphism, are well
understood. Augmenting these graph models with topological features via
persistent homology (PH) has gained prominence, but identifying the class of
attributed graphs that PH can recognize remains open. We introduce a novel
concept of color-separating sets to provide a complete resolution to this
important problem. Specifically, we establish the necessary and sufficient
conditions for distinguishing graphs based on the persistence of their
connected components, obtained from filter functions on vertex and edge colors.
Our constructions expose the limits of vertex- and edge-level PH, proving that
neither category subsumes the other. Leveraging these theoretical insights, we
propose RePHINE for learning topological features on graphs. RePHINE
efficiently combines vertex- and edge-level PH, achieving a scheme that is
provably more powerful than both. Integrating RePHINE into MP-GNNs boosts their
expressive power, resulting in gains over standard PH on several benchmarks for
graph classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06153">Interpretable Graph Anomaly Detection using Gradient Attention Maps. (arXiv:2311.06153v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaofan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Dongmian Zou</a></p>
<p>Detecting unusual patterns in graph data is a crucial task in data mining.
However, existing methods often face challenges in consistently achieving
satisfactory performance and lack interpretability, which hinders our
understanding of anomaly detection decisions. In this paper, we propose a novel
approach to graph anomaly detection that leverages the power of
interpretability to enhance performance. Specifically, our method extracts an
attention map derived from gradients of graph neural networks, which serves as
a basis for scoring anomalies. In addition, we conduct theoretical analysis
using synthetic data to validate our method and gain insights into its
decision-making process. To demonstrate the effectiveness of our method, we
extensively evaluate our approach against state-of-the-art graph anomaly
detection techniques. The results consistently demonstrate the superior
performance of our method compared to the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06169">Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping. (arXiv:2311.06169v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prezja_F/0/1/0/all/0/1">Fabi Prezja</a></p>
<p>Deep learning-based vision is characterized by intricate frameworks that
often necessitate a profound understanding, presenting a barrier to newcomers
and limiting broad adoption. With many researchers grappling with the
constraints of smaller datasets, there's a pronounced reliance on pre-trained
neural networks, especially for tasks such as image classification. This
reliance is further intensified in niche imaging areas where obtaining vast
datasets is challenging. Despite the widespread use of transfer learning as a
remedy to the small dataset dilemma, a conspicuous absence of tailored auto-ML
solutions persists. Addressing these challenges is "Deep Fast Vision", a python
library that streamlines the deep learning process. This tool offers a
user-friendly experience, enabling results through a simple nested dictionary
definition, helping to democratize deep learning for non-experts. Designed for
simplicity and scalability, Deep Fast Vision appears as a bridge, connecting
the complexities of existing deep learning frameworks with the needs of a
diverse user base.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06170">Time Scale Network: A Shallow Neural Network For Time Series Data. (arXiv:2311.06170v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meyer_T/0/1/0/all/0/1">Trevor Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Shultz_C/0/1/0/all/0/1">Camden Shultz</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1">Najim Dehak</a>, <a href="http://arxiv.org/find/cs/1/au:+Moro_Velazquez_L/0/1/0/all/0/1">Laureano Moro-Velazquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Irazoqui_P/0/1/0/all/0/1">Pedro Irazoqui</a></p>
<p>Time series data is often composed of information at multiple time scales,
particularly in biomedical data. While numerous deep learning strategies exist
to capture this information, many make networks larger, require more data, are
more demanding to compute, and are difficult to interpret. This limits their
usefulness in real-world applications facing even modest computational or data
constraints and can further complicate their translation into practice. We
present a minimal, computationally efficient Time Scale Network combining the
translation and dilation sequence used in discrete wavelet transforms with
traditional convolutional neural networks and back-propagation. The network
simultaneously learns features at many time scales for sequence classification
with significantly reduced parameters and operations. We demonstrate advantages
in Atrial Dysfunction detection including: superior accuracy-per-parameter and
accuracy-per-operation, fast training and inference speeds, and visualization
and interpretation of learned patterns in atrial dysfunction detection on ECG
signals. We also demonstrate impressive performance in seizure prediction using
EEG signals. Our network isolated a few time scales that could be strategically
selected to achieve 90.9% accuracy using only 1,133 active parameters and
consistently converged on pulsatile waveform shapes. This method does not rest
on any constraints or assumptions regarding signal content and could be
leveraged in any area of time series analysis dealing with signals containing
features at many time scales.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06184">Frequency-domain MLPs are More Effective Learners in Time Series Forecasting. (arXiv:2311.06184v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kun Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shoujin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1">Defu Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+An_N/0/1/0/all/0/1">Ning An</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Longbing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1">Zhendong Niu</a></p>
<p>Time series forecasting has played the key role in different industrial,
including finance, traffic, energy, and healthcare domains. While existing
literatures have designed many sophisticated architectures based on RNNs, GNNs,
or Transformers, another kind of approaches based on multi-layer perceptrons
(MLPs) are proposed with simple structure, low complexity, and {superior
performance}. However, most MLP-based forecasting methods suffer from the
point-wise mappings and information bottleneck, which largely hinders the
forecasting performance. To overcome this problem, we explore a novel direction
of applying MLPs in the frequency domain for time series forecasting. We
investigate the learned patterns of frequency-domain MLPs and discover their
two inherent characteristic benefiting forecasting, (i) global view: frequency
spectrum makes MLPs own a complete view for signals and learn global
dependencies more easily, and (ii) energy compaction: frequency-domain MLPs
concentrate on smaller key part of frequency components with compact signal
energy. Then, we propose FreTS, a simple yet effective architecture built upon
Frequency-domain MLPs for Time Series forecasting. FreTS mainly involves two
stages, (i) Domain Conversion, that transforms time-domain signals into complex
numbers of frequency domain; (ii) Frequency Learning, that performs our
redesigned MLPs for the learning of real and imaginary part of frequency
components. The above stages operated on both inter-series and intra-series
scales further contribute to channel-wise and time-wise dependency learning.
Extensive experiments on 13 real-world benchmarks (including 7 benchmarks for
short-term forecasting and 6 benchmarks for long-term forecasting) demonstrate
our consistent superiority over state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06185">An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer. (arXiv:2311.06185v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1">Adam J Shephard</a>, <a href="http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1">Mostafa Jahanifar</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1">Ruoyu Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Dawood_M/0/1/0/all/0/1">Muhammad Dawood</a>, <a href="http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1">Simon Graham</a>, <a href="http://arxiv.org/find/eess/1/au:+Sidlauskas_K/0/1/0/all/0/1">Kastytis Sidlauskas</a>, <a href="http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1">Syed Ali Khurram</a>, <a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1">Nasir M Rajpoot</a>, <a href="http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1">Shan E Ahmed Raza</a></p>
<p>Tumour-infiltrating lymphocytes (TILs) are considered as a valuable
prognostic markers in both triple-negative and human epidermal growth factor
receptor 2 (HER2) breast cancer. In this study, we introduce an innovative deep
learning pipeline based on the Efficient-UNet architecture to compute a TILs
score for breast cancer whole slide images. Our pipeline first segments
tumour-stroma regions and generates a tumour bulk mask. Subsequently, it
detects TILs within the tumour-associated stroma, generating a TILs score by
closely mirroring the pathologist's workflow. Our method exhibits
state-of-the-art performance in segmenting tumour/stroma areas and TILs
detection, as demonstrated by internal cross-validation on the TiGER Challenge
training dataset and evaluation on the final leaderboards. Additionally, our
TILs score proves competitive in predicting survival outcomes within the same
challenge, underscoring the clinical relevance and potential of our automated
TILs scoring system as a breast cancer prognostic tool.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06190">FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective. (arXiv:2311.06190v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1">Kun Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Liang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Pengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_N/0/1/0/all/0/1">Ning An</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Longbing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1">Zhendong Niu</a></p>
<p>Multivariate time series (MTS) forecasting has shown great importance in
numerous industries. Current state-of-the-art graph neural network (GNN)-based
forecasting methods usually require both graph networks (e.g., GCN) and
temporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics and
intra-series (temporal) dependencies, respectively. However, the uncertain
compatibility of the two networks puts an extra burden on handcrafted model
designs. Moreover, the separate spatial and temporal modeling naturally
violates the unified spatiotemporal inter-dependencies in real world, which
largely hinders the forecasting performance. To overcome these problems, we
explore an interesting direction of directly applying graph networks and
rethink MTS forecasting from a pure graph perspective. We first define a novel
data structure, hypervariate graph, which regards each series value (regardless
of variates or timestamps) as a graph node, and represents sliding windows as
space-time fully-connected graphs. This perspective considers spatiotemporal
dynamics unitedly and reformulates classic MTS forecasting into the predictions
on hypervariate graphs. Then, we propose a novel architecture Fourier Graph
Neural Network (FourierGNN) by stacking our proposed Fourier Graph Operator
(FGO) to perform matrix multiplications in Fourier space. FourierGNN
accommodates adequate expressiveness and achieves much lower complexity, which
can effectively and efficiently accomplish the forecasting. Besides, our
theoretical analysis reveals FGO's equivalence to graph convolutions in the
time domain, which further verifies the validity of FourierGNN. Extensive
experiments on seven datasets have demonstrated our superior performance with
higher efficiency and fewer parameters compared with state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06192">Greedy PIG: Adaptive Integrated Gradients. (arXiv:2311.06192v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Axiotis_K/0/1/0/all/0/1">Kyriakos Axiotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Abu_al_haija_S/0/1/0/all/0/1">Sami Abu-al-haija</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahrbach_M/0/1/0/all/0/1">Matthew Fahrbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1">Gang Fu</a></p>
<p>Deep learning has become the standard approach for most machine learning
tasks. While its impact is undeniable, interpreting the predictions of deep
learning models from a human perspective remains a challenge. In contrast to
model training, model interpretability is harder to quantify and pose as an
explicit optimization problem. Inspired by the AUC softmax information curve
(AUC SIC) metric for evaluating feature attribution methods, we propose a
unified discrete optimization framework for feature attribution and feature
selection based on subset selection. This leads to a natural adaptive
generalization of the path integrated gradients (PIG) method for feature
attribution, which we call Greedy PIG. We demonstrate the success of Greedy PIG
on a wide variety of tasks, including image feature attribution, graph
compression/explanation, and post-hoc feature selection on tabular data. Our
results show that introducing adaptivity is a powerful and versatile method for
making attribution methods more powerful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06210">Optimal Cooperative Multiplayer Learning Bandits with Noisy Rewards and No Communication. (arXiv:2311.06210v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1">William Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuanhao Lu</a></p>
<p>We consider a cooperative multiplayer bandit learning problem where the
players are only allowed to agree on a strategy beforehand, but cannot
communicate during the learning process. In this problem, each player
simultaneously selects an action. Based on the actions selected by all players,
the team of players receives a reward. The actions of all the players are
commonly observed. However, each player receives a noisy version of the reward
which cannot be shared with other players. Since players receive potentially
different rewards, there is an asymmetry in the information used to select
their actions. In this paper, we provide an algorithm based on upper and lower
confidence bounds that the players can use to select their optimal actions
despite the asymmetry in the reward information. We show that this algorithm
can achieve logarithmic $O(\frac{\log T}{\Delta_{\bm{a}}})$ (gap-dependent)
regret as well as $O(\sqrt{T\log T})$ (gap-independent) regret. This is
asymptotically optimal in $T$. We also show that it performs empirically better
than the current state of the art algorithm for this environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06212">Differentiable VQ-VAE&#x27;s for Robust White Matter Streamline Encodings. (arXiv:2311.06212v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lizarraga_A/0/1/0/all/0/1">Andrew Lizarraga</a>, <a href="http://arxiv.org/find/stat/1/au:+Taraku_B/0/1/0/all/0/1">Brandon Taraku</a>, <a href="http://arxiv.org/find/stat/1/au:+Honig_E/0/1/0/all/0/1">Edouardo Honig</a>, <a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Joshi_S/0/1/0/all/0/1">Shantanu H. Joshi</a></p>
<p>Given the complex geometry of white matter streamlines, Autoencoders have
been proposed as a dimension-reduction tool to simplify the analysis
streamlines in a low-dimensional latent spaces. However, despite these recent
successes, the majority of encoder architectures only perform dimension
reduction on single streamlines as opposed to a full bundle of streamlines.
This is a severe limitation of the encoder architecture that completely
disregards the global geometric structure of streamlines at the expense of
individual fibers. Moreover, the latent space may not be well structured which
leads to doubt into their interpretability. In this paper we propose a novel
Differentiable Vector Quantized Variational Autoencoder, which are engineered
to ingest entire bundles of streamlines as single data-point and provides
reliable trustworthy encodings that can then be later used to analyze
streamlines in the latent space. Comparisons with several state of the art
Autoencoders demonstrate superior performance in both encoding and synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06217">MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things. (arXiv:2311.06217v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1">Shentong Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Russ Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>The Internet of Things (IoT), the network integrating billions of smart
physical devices embedded with sensors, software, and communication
technologies for the purpose of connecting and exchanging data with other
devices and systems, is a critical and rapidly expanding component of our
modern world. The IoT ecosystem provides a rich source of real-world modalities
such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio
for prediction tasks involving the pose, gaze, activities, and gestures of
humans as well as the touch, contact, pose, 3D of physical objects. Machine
learning presents a rich opportunity to automatically process IoT data at
scale, enabling efficient inference for impact in understanding human
wellbeing, controlling physical devices, and interconnecting smart cities. To
develop machine learning technologies for IoT, this paper proposes MultiIoT,
the most expansive IoT benchmark to date, encompassing over 1.15 million
samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges
involving (1) learning from many sensory modalities, (2) fine-grained
interactions across long temporal ranges, and (3) extreme heterogeneity due to
unique structure and noise topologies in real-world sensors. We also release a
set of strong modeling baselines, spanning modality and task-specific methods
to multisensory and multitask models to encourage future research in
multisensory representation learning for IoT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06224">Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network Generalization. (arXiv:2311.06224v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Benarous_E/0/1/0/all/0/1">Elior Benarous</a>, <a href="http://arxiv.org/find/cs/1/au:+Anagnostidis_S/0/1/0/all/0/1">Sotiris Anagnostidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Biggio_L/0/1/0/all/0/1">Luca Biggio</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1">Thomas Hofmann</a></p>
<p>Recent advancements in deep learning have been primarily driven by the use of
large models trained on increasingly vast datasets. While neural scaling laws
have emerged to predict network performance given a specific level of
computational resources, the growing demand for expansive datasets raises
concerns. To address this, a new research direction has emerged, focusing on
the creation of synthetic data as a substitute. In this study, we investigate
how neural networks exhibit shape bias during training on synthetic datasets,
serving as an indicator of the synthetic data quality. Specifically, our
findings indicate three key points: (1) Shape bias varies across network
architectures and types of supervision, casting doubt on its reliability as a
predictor for generalization and its ability to explain differences in model
recognition compared to human capabilities. (2) Relying solely on shape bias to
estimate generalization is unreliable, as it is entangled with diversity and
naturalism. (3) We propose a novel interpretation of shape bias as a tool for
estimating the diversity of samples within a dataset. Our research aims to
clarify the implications of using synthetic data and its associated shape bias
in deep learning, addressing concerns regarding generalization and dataset
quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06227">Does Differential Privacy Prevent Backdoor Attacks in Practice?. (arXiv:2311.06227v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Razmi_F/0/1/0/all/0/1">Fereshteh Razmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Li Xiong</a></p>
<p>Differential Privacy (DP) was originally developed to protect privacy.
However, it has recently been utilized to secure machine learning (ML) models
from poisoning attacks, with DP-SGD receiving substantial attention.
Nevertheless, a thorough investigation is required to assess the effectiveness
of different DP techniques in preventing backdoor attacks in practice. In this
paper, we investigate the effectiveness of DP-SGD and, for the first time in
literature, examine PATE in the context of backdoor attacks. We also explore
the role of different components of DP algorithms in defending against backdoor
attacks and will show that PATE is effective against these attacks due to the
bagging structure of the teacher models it employs. Our experiments reveal that
hyperparameters and the number of backdoors in the training dataset impact the
success of DP algorithms. Additionally, we propose Label-DP as a faster and
more accurate alternative to DP-SGD and PATE. We conclude that while Label-DP
algorithms generally offer weaker privacy protection, accurate hyper-parameter
tuning can make them more effective than DP methods in defending against
backdoor attacks while maintaining model accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06228">Learning material synthesis-structure-property relationship by data fusion: Bayesian Co-regionalization N-Dimensional Piecewise Function Learning. (arXiv:2311.06228v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kusne_A/0/1/0/all/0/1">A. Gilad Kusne</a>, <a href="http://arxiv.org/find/cs/1/au:+McDannald_A/0/1/0/all/0/1">Austin McDannald</a>, <a href="http://arxiv.org/find/cs/1/au:+DeCost_B/0/1/0/all/0/1">Brian DeCost</a></p>
<p>Advanced materials are needed to further next-generation technologies such as
quantum computing, carbon capture, and low-cost medical imaging. However,
advanced materials discovery is confounded by two fundamental challenges: the
challenge of a high-dimensional, complex materials search space and the
challenge of combining knowledge, i.e., data fusion across instruments and
labs. To overcome the first challenge, researchers employ knowledge of the
underlying material synthesis-structure-property relationship, as a material's
structure is often predictive of its functional property and vice versa. For
example, optimal materials often occur along composition-phase boundaries or
within specific phase regions. Additionally, knowledge of the
synthesis-structure-property relationship is fundamental to understanding
underlying physical mechanisms. However, quantifying the
synthesis-structure-property relationship requires overcoming the second
challenge. Researchers must merge knowledge gathered across instruments,
measurement modalities, and even laboratories. We present the
Synthesis-structure-property relAtionship coreGionalized lEarner (SAGE)
algorithm. A fully Bayesian algorithm that uses multimodal coregionalization to
merge knowledge across data sources to learn synthesis-structure-property
relationships.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06233">Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. (arXiv:2311.06233v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1">Shahriar Golchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1">Mihai Surdeanu</a></p>
<p>We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM's performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06234">EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy. (arXiv:2311.06234v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xiaoyi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1">Siddharth Ancha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_L/0/1/0/all/0/1">Lakshay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Osteen_P/0/1/0/all/0/1">Philip R. Osteen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1">Bernadette Bucher</a>, <a href="http://arxiv.org/find/cs/1/au:+Phillips_S/0/1/0/all/0/1">Stephen Phillips</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiuguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Everett_M/0/1/0/all/0/1">Michael Everett</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1">Nicholas Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1">Jonathan P. How</a></p>
<p>Traversing terrain with good traction is crucial for achieving fast off-road
navigation. Instead of manually designing costs based on terrain features,
existing methods learn terrain properties directly from data via
self-supervision, but challenges remain to properly quantify and mitigate risks
due to uncertainties in learned models. This work efficiently quantifies both
aleatoric and epistemic uncertainties by learning discrete traction
distributions and probability densities of the traction predictor's latent
features. Leveraging evidential deep learning, we parameterize Dirichlet
distributions with the network outputs and propose a novel uncertainty-aware
squared Earth Mover's distance loss with a closed-form expression that improves
learning accuracy and navigation performance. The proposed risk-aware planner
simulates state trajectories with the worst-case expected traction to handle
aleatoric uncertainty, and penalizes trajectories moving through terrain with
high epistemic uncertainty. Our approach is extensively validated in simulation
and on wheeled and quadruped robots, showing improved navigation performance
compared to methods that assume no slip, assume the expected traction, or
optimize for the worst-case expected cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06239">Argumentation Element Annotation Modeling using XLNet. (arXiv:2311.06239v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ormerod_C/0/1/0/all/0/1">Christopher Ormerod</a>, <a href="http://arxiv.org/find/cs/1/au:+Burkhardt_A/0/1/0/all/0/1">Amy Burkhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Young_M/0/1/0/all/0/1">Mackenzie Young</a>, <a href="http://arxiv.org/find/cs/1/au:+Lottridge_S/0/1/0/all/0/1">Sue Lottridge</a></p>
<p>This study demonstrates the effectiveness of XLNet, a transformer-based
language model, for annotating argumentative elements in persuasive essays.
XLNet's architecture incorporates a recurrent mechanism that allows it to model
long-term dependencies in lengthy texts. Fine-tuned XLNet models were applied
to three datasets annotated with different schemes - a proprietary dataset
using the Annotations for Revisions and Reflections on Writing (ARROW) scheme,
the PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNet
models achieved strong performance across all datasets, even surpassing human
agreement levels in some cases. This shows XLNet capably handles diverse
annotation schemes and lengthy essays. Comparisons between the model outputs on
different datasets also revealed insights into the relationships between the
annotation tags. Overall, XLNet's strong performance on modeling argumentative
structures across diverse datasets highlights its suitability for providing
automated feedback on essay organization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06243">Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization. (arXiv:2311.06243v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1">Zeju Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1">Yuliang Xiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1">Yuxuan Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Longhui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Haiwen Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1">Juyeon Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1">Songyou Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yandong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1">Adrian Weller</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>Large foundation models are becoming ubiquitous, but training them from
scratch is prohibitively expensive. Thus, efficiently adapting these powerful
models to downstream tasks is increasingly important. In this paper, we study a
principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream
task adaptation. Despite demonstrating good generalizability, OFT still uses a
fairly large number of trainable parameters due to the high dimensionality of
orthogonal matrices. To address this, we start by examining OFT from an
information transmission perspective, and then identify a few key desiderata
that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast
Fourier transform algorithm enables efficient information transmission, we
propose an efficient orthogonal parameterization using butterfly structures. We
apply this parameterization to OFT, creating a novel parameter-efficient
finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a
special case, BOFT introduces a generalized orthogonal finetuning framework.
Finally, we conduct an extensive empirical study of adapting large vision
transformers, large language models, and text-to-image diffusion models to
various downstream tasks in vision and language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1810.07168">An empirical evaluation of imbalanced data strategies from a practitioner&#x27;s point of view. (arXiv:1810.07168v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wainer_J/0/1/0/all/0/1">Jacques Wainer</a></p>
<p>This paper evaluates six strategies for mitigating imbalanced data:
oversampling, undersampling, ensemble methods, specialized algorithms, class
weight adjustments, and a no-mitigation approach referred to as the baseline.
These strategies were tested on 58 real-life binary imbalanced datasets with
imbalance rates ranging from 3 to 120. We conducted a comparative analysis of
10 under-sampling algorithms, 5 over-sampling algorithms, 2 ensemble methods,
and 3 specialized algorithms across eight different performance metrics:
accuracy, area under the ROC curve (AUC), balanced accuracy, F1-measure,
G-mean, Matthew's correlation coefficient, precision, and recall. Additionally,
we assessed the six strategies on altered datasets, derived from real-life
data, with both low (3) and high (100 or 300) imbalance ratios (IR).
</p>
<p>The principal finding indicates that the effectiveness of each strategy
significantly varies depending on the metric used. The paper also examines a
selection of newer algorithms within the categories of specialized algorithms,
oversampling, and ensemble methods. The findings suggest that the current
hierarchy of best-performing strategies for each metric is unlikely to change
with the introduction of newer algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.05665">Chanakya: Learning Runtime Decisions for Adaptive Real-Time Perception. (arXiv:2106.05665v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1">Anurag Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Balloli_V/0/1/0/all/0/1">Vaibhav Balloli</a>, <a href="http://arxiv.org/find/cs/1/au:+Nambi_A/0/1/0/all/0/1">Akshay Nambi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aditya Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1">Tanuja Ganu</a></p>
<p>Real-time perception requires planned resource utilization. Computational
planning in real-time perception is governed by two considerations -- accuracy
and latency. There exist run-time decisions (e.g. choice of input resolution)
that induce tradeoffs affecting performance on a given hardware, arising from
intrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resource
contention) characteristics.
</p>
<p>Earlier runtime execution frameworks employed rule-based decision algorithms
and operated with a fixed algorithm latency budget to balance these concerns,
which is sub-optimal and inflexible. We propose Chanakya, a learned approximate
execution framework that naturally derives from the streaming perception
paradigm, to automatically learn decisions induced by these tradeoffs instead.
Chanakya is trained via novel rewards balancing accuracy and latency
implicitly, without approximating either objectives. Chanakya simultaneously
considers intrinsic and extrinsic context, and predicts decisions in a flexible
manner. Chanakya, designed with low overhead in mind, outperforms
state-of-the-art static and dynamic execution policies on public datasets on
both server GPUs and edge devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.13672">SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models. (arXiv:2108.13672v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1">Yogesh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1">Alexander Ilin</a>, <a href="http://arxiv.org/find/cs/1/au:+Salo_H/0/1/0/all/0/1">Henri Salo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulathinal_S/0/1/0/all/0/1">Sangita Kulathinal</a>, <a href="http://arxiv.org/find/cs/1/au:+Leinonen_M/0/1/0/all/0/1">Maarit K. Leinonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1">Pekka Marttinen</a></p>
<p>Despite the proven effectiveness of Transformer neural networks across
multiple domains, their performance with Electronic Health Records (EHR) can be
nuanced. The unique, multidimensional sequential nature of EHR data can
sometimes make even simple linear models with carefully engineered features
more competitive. Thus, the advantages of Transformers, such as efficient
transfer learning and improved scalability are not always fully exploited in
EHR applications. Addressing these challenges, we introduce SANSformer, an
attention-free sequential model designed with specific inductive biases to
cater for the unique characteristics of EHR data.
</p>
<p>In this work, we aim to forecast the demand for healthcare services, by
predicting the number of patient visits to healthcare facilities. The challenge
amplifies when dealing with divergent patient subgroups, like those with rare
diseases, which are characterized by unique health trajectories and are
typically smaller in size. To address this, we employ a self-supervised
pretraining strategy, Generative Summary Pretraining (GSP), which predicts
future summary statistics based on past health records of a patient. Our models
are pretrained on a health registry of nearly one million patients, then
fine-tuned for specific subgroup prediction tasks, showcasing the potential to
handle the multifaceted nature of EHR data.
</p>
<p>In evaluation, SANSformer consistently surpasses robust EHR baselines, with
our GSP pretraining method notably amplifying model performance, particularly
within smaller patient subgroups. Our results illuminate the promising
potential of tailored attention-free models and self-supervised pretraining in
refining healthcare utilization predictions across various patient
demographics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.12955">Optimal Regret Is Achievable with Bounded Approximate Inference Error: An Enhanced Bayesian Upper Confidence Bound Framework. (arXiv:2201.12955v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1">Henry Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Meisami_A/0/1/0/all/0/1">Amirhossein Meisami</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haofeng Zhang</a></p>
<p>Bayesian bandit algorithms with approximate Bayesian inference have been
widely used in real-world applications. However, there is a large discrepancy
between the superior practical performance of these approaches and their
theoretical justification. Previous research only indicates a negative
theoretical result: Thompson sampling could have a worst-case linear regret
$\Omega(T)$ with a constant threshold on the inference error measured by one
$\alpha$-divergence. To bridge this gap, we propose an Enhanced Bayesian Upper
Confidence Bound (EBUCB) framework that can efficiently accommodate bandit
problems in the presence of approximate inference. Our theoretical analysis
demonstrates that for Bernoulli multi-armed bandits, EBUCB can achieve the
optimal regret order $O(\log T)$ if the inference error measured by two
different $\alpha$-divergences is less than a constant, regardless of how large
this constant is. To our best knowledge, our study provides the first
theoretical regret bound that is better than $o(T)$ in the setting of constant
approximate inference error. Furthermore, in concordance with the negative
results in previous studies, we show that only one bounded $\alpha$-divergence
is insufficient to guarantee a sub-linear regret.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.08317">TAKDE: Temporal Adaptive Kernel Density Estimator for Real-Time Dynamic Density Estimation. (arXiv:2203.08317v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1">Yinsong Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ding_Y/0/1/0/all/0/1">Yu Ding</a>, <a href="http://arxiv.org/find/stat/1/au:+Shahrampour_S/0/1/0/all/0/1">Shahin Shahrampour</a></p>
<p>Real-time density estimation is ubiquitous in many applications, including
computer vision and signal processing. Kernel density estimation is arguably
one of the most commonly used density estimation techniques, and the use of
"sliding window" mechanism adapts kernel density estimators to dynamic
processes. In this paper, we derive the asymptotic mean integrated squared
error (AMISE) upper bound for the "sliding window" kernel density estimator.
This upper bound provides a principled guide to devise a novel estimator, which
we name the temporal adaptive kernel density estimator (TAKDE). Compared to
heuristic approaches for "sliding window" kernel density estimator, TAKDE is
theoretically optimal in terms of the worst-case AMISE. We provide numerical
experiments using synthetic and real-world datasets, showing that TAKDE
outperforms other state-of-the-art dynamic density estimators (including those
outside of kernel family). In particular, TAKDE achieves a superior test
log-likelihood with a smaller runtime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.08443">On the (In)security of Peer-to-Peer Decentralized Machine Learning. (arXiv:2205.08443v3 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pasquini_D/0/1/0/all/0/1">Dario Pasquini</a>, <a href="http://arxiv.org/find/cs/1/au:+Raynal_M/0/1/0/all/0/1">Mathilde Raynal</a>, <a href="http://arxiv.org/find/cs/1/au:+Troncoso_C/0/1/0/all/0/1">Carmela Troncoso</a></p>
<p>In this work, we carry out the first, in-depth, privacy analysis of
Decentralized Learning -- a collaborative machine learning framework aimed at
addressing the main limitations of federated learning. We introduce a suite of
novel attacks for both passive and active decentralized adversaries. We
demonstrate that, contrary to what is claimed by decentralized learning
proposers, decentralized learning does not offer any security advantage over
federated learning. Rather, it increases the attack surface enabling any user
in the system to perform privacy attacks such as gradient inversion, and even
gain full control over honest users' local model. We also show that, given the
state of the art in protections, privacy-preserving configurations of
decentralized learning require fully connected networks, losing any practical
advantage over the federated setup and therefore completely defeating the
objective of the decentralized approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14590">Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_C/0/1/0/all/0/1">Chinmay Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Manxi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1">Druv Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1">Shankar Sastry</a></p>
<p>We propose a multi-agent reinforcement learning dynamics, and analyze its
convergence in infinite-horizon discounted Markov potential games. We focus on
the independent and decentralized setting, where players do not have knowledge
of the game model and cannot coordinate. In each stage, players update their
estimate of Q-function that evaluates their total contingent payoff based on
the realized one-stage reward in an asynchronous manner. Then, players
independently update their policies by incorporating an optimal one-stage
deviation strategy based on the estimated Q-function. A key feature of the
learning dynamics is that the Q-function estimates are updated at a faster
timescale than the policies. We prove that the policies induced by our learning
dynamics converge to the set of stationary Nash equilibria in Markov potential
games with probability 1. Our results highlight the efficacy of simple learning
dynamics in reaching to the set of stationary Nash equilibrium even in
environments with minimal information available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.09311">Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">John Sun</a></p>
<p>We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM
achieves good performance on imbalanced data sets with a Majority to Minority
Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including
intercept (bias), regularization and parameters affects performance on our
selection of datasets. Although many resort to SMOTE methods, we aim for a less
computationally intensive method. We evaluate the performance by examining the
learning curves. These curves diagnose whether we overfit or underfit or
whether the random sample of data chosen during the process was not random
enough or diverse enough in dependent variable class for the algorithm to
generalized to unseen examples. We will also see the background of the
hyperparameters versus the test and train error in validation curves. We
benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL
method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the
work of Ding by incorporating kernels into SVM. We will use Python rather than
MATLAB as python has dictionaries for storing mixed data types during
multi-parameter cross-validation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01738">ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training. (arXiv:2210.01738v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1">Antonio Norelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Fumero_M/0/1/0/all/0/1">Marco Fumero</a>, <a href="http://arxiv.org/find/cs/1/au:+Maiorca_V/0/1/0/all/0/1">Valentino Maiorca</a>, <a href="http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1">Luca Moschella</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1">Emanuele Rodol&#xe0;</a>, <a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a></p>
<p>CLIP proved that aligning visual and language spaces is key to solving many
vision tasks without explicit training, but required to train image and text
encoders from scratch on a huge dataset. LiT improved this by only training the
text encoder and using a pre-trained vision network. In this paper, we show
that a common space can be created without any training at all, using
single-domain encoders (trained with or without supervision) and a much smaller
amount of image-text pairs. Furthermore, our model has unique properties. Most
notably, deploying a new version with updated training samples can be done in a
matter of seconds. Additionally, the representations in the common space are
easily interpretable as every dimension corresponds to the similarity of the
input to a unique image-text pair in the multimodal dataset. Experiments on
standard zero-shot visual benchmarks demonstrate the typical transfer ability
of image-text models. Overall, our method represents a simple yet surprisingly
strong baseline for foundation multimodal models, raising important questions
on their data efficiency and on the role of retrieval in machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.08886">Learning Decentralized Linear Quadratic Regulator with $\sqrt{T}$ Regret. (arXiv:2210.08886v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Ye_L/0/1/0/all/0/1">Lintao Ye</a>, <a href="http://arxiv.org/find/math/1/au:+Chi_M/0/1/0/all/0/1">Ming Chi</a>, <a href="http://arxiv.org/find/math/1/au:+Liao_R/0/1/0/all/0/1">Ruiquan Liao</a>, <a href="http://arxiv.org/find/math/1/au:+Gupta_V/0/1/0/all/0/1">Vijay Gupta</a></p>
<p>We study the problem of learning decentralized linear quadratic regulator
when the system model is unknown a priori. We propose an online learning
algorithm that adaptively designs a control policy as new data samples from a
single system trajectory become available. Our algorithm design uses a
disturbance-feedback representation of state-feedback controllers coupled with
online convex optimization with memory and delayed feedback. We show that our
controller enjoys an expected regret that scales as $\sqrt{T}$ with the time
horizon $T$ for the case of partially nested information pattern. For more
general information patterns, the optimal controller is unknown even if the
system model is known. In this case, the regret of our controller is shown with
respect to a linear sub-optimal controller. We validate our theoretical
findings using numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.04370">NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation. (arXiv:2211.04370v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1">Abbavaram Gowtham Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1">Vineeth N Balasubramanian</a></p>
<p>Causal effect estimation from observational data is a central problem in
causal inference. Methods based on potential outcomes framework solve this
problem by exploiting inductive biases and heuristics from causal inference.
Each of these methods addresses a specific aspect of causal effect estimation,
such as controlling propensity score, enforcing randomization, etc., by
designing neural network (NN) architectures and regularizers. In this paper, we
propose an adaptive method called Neurosymbolic Causal Effect Estimator
(NESTER), a generalized method for causal effect estimation. NESTER integrates
the ideas used in existing methods based on multi-head NNs for causal effect
estimation into one framework. We design a Domain Specific Language (DSL)
tailored for causal effect estimation based on causal inductive biases used in
literature. We conduct a theoretical analysis to investigate NESTER's efficacy
in estimating causal effects. Our comprehensive empirical results show that
NESTER performs better than state-of-the-art methods on benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08594">Orthogonal Polynomials Approximation Algorithm (OPAA):a functional analytic approach to estimating probability densities. (arXiv:2211.08594v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bialokozowicz_L/0/1/0/all/0/1">Lilian W. Bialokozowicz</a></p>
<p>We present the new Orthogonal Polynomials Approximation Algorithm (OPAA), a
parallelizable algorithm that solves two problems from a functional analytic
approach: first, it finds a smooth functional estimate of a density function,
whether it is normalized or not; second, the algorithm provides an estimate of
the normalizing weight. In the context of Bayesian inference, OPAA provides an
estimate of the posterior function as well as the normalizing weight, which is
also known as the evidence.
</p>
<p>A core component of OPAA is a special transform of the square root of the
joint distribution into a special functional space of our construct. Through
this transform, the evidence is equated with the $L^2$ norm of the transformed
function, squared. Hence, the evidence can be estimated by the sum of squares
of the transform coefficients. The computations can be parallelized and
completed in one pass.
</p>
<p>To compute the transform coefficients, OPAA proposes a new computational
scheme leveraging Gauss--Hermite quadrature in higher dimensions. Not only does
it avoid the potential high variance problem associated with random sampling
methods, it also enables one to speed up the computation by parallelization,
and significantly reduces the complexity by a vector decomposition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00186">Multi-Task Imitation Learning for Linear Dynamical Systems. (arXiv:2212.00186v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Thomas T. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1">Katie Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1">Bruce D. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomlin_C/0/1/0/all/0/1">Claire Tomlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1">Stephen Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1">Nikolai Matni</a></p>
<p>We study representation learning for efficient imitation learning over linear
systems. In particular, we consider a setting where learning is split into two
phases: (a) a pre-training step where a shared $k$-dimensional representation
is learned from $H$ source policies, and (b) a target policy fine-tuning step
where the learned representation is used to parameterize the policy class. We
find that the imitation gap over trajectories generated by the learned target
policy is bounded by $\tilde{O}\left( \frac{k n_x}{HN_{\mathrm{shared}}} +
\frac{k n_u}{N_{\mathrm{target}}}\right)$, where $n_x &gt; k$ is the state
dimension, $n_u$ is the input dimension, $N_{\mathrm{shared}}$ denotes the
total amount of data collected for each policy during representation learning,
and $N_{\mathrm{target}}$ is the amount of target task data. This result
formalizes the intuition that aggregating data across related tasks to learn a
representation can significantly improve the sample efficiency of learning a
target task. The trends suggested by this bound are corroborated in simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05159">Optimized Sparse Matrix Operations for Reverse Mode Automatic Differentiation. (arXiv:2212.05159v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nytko_N/0/1/0/all/0/1">Nicolas Nytko</a>, <a href="http://arxiv.org/find/cs/1/au:+Taghibakhshi_A/0/1/0/all/0/1">Ali Taghibakhshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaman_T/0/1/0/all/0/1">Tareq Uz Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+MacLachlan_S/0/1/0/all/0/1">Scott MacLachlan</a>, <a href="http://arxiv.org/find/cs/1/au:+Olson_L/0/1/0/all/0/1">Luke N. Olson</a>, <a href="http://arxiv.org/find/cs/1/au:+West_M/0/1/0/all/0/1">Matt West</a></p>
<p>Sparse matrix representations are ubiquitous in computational science and
machine learning, leading to significant reductions in compute time, in
comparison to dense representation, for problems that have local connectivity.
The adoption of sparse representation in leading ML frameworks such as PyTorch
is incomplete, however, with support for both automatic differentiation and GPU
acceleration missing. In this work, we present an implementation of a CSR-based
sparse matrix wrapper for PyTorch with CUDA acceleration for basic matrix
operations, as well as automatic differentiability. We also present several
applications of the resulting sparse kernels to optimization problems,
demonstrating ease of implementation and performance measurements versus their
dense counterparts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00045">Neural Control of Parametric Solutions for High-dimensional Evolution PDEs. (arXiv:2302.00045v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Gaby_N/0/1/0/all/0/1">Nathan Gaby</a>, <a href="http://arxiv.org/find/math/1/au:+Ye_X/0/1/0/all/0/1">Xiaojing Ye</a>, <a href="http://arxiv.org/find/math/1/au:+Zhou_H/0/1/0/all/0/1">Haomin Zhou</a></p>
<p>We develop a novel computational framework to approximate solution operators
of evolution partial differential equations (PDEs). By employing a general
nonlinear reduced-order model, such as a deep neural network, to approximate
the solution of a given PDE, we realize that the evolution of the model
parameter is a control problem in the parameter space. Based on this
observation, we propose to approximate the solution operator of the PDE by
learning the control vector field in the parameter space. From any initial
value, this control field can steer the parameter to generate a trajectory such
that the corresponding reduced-order model solves the PDE. This allows for
substantially reduced computational cost to solve the evolution PDE with
arbitrary initial conditions. We also develop comprehensive error analysis for
the proposed method when solving a large class of semilinear parabolic PDEs.
Numerical experiments on different high-dimensional evolution PDEs with various
initial conditions demonstrate the promising results of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01377">Learning with Exposure Constraints in Recommendation Systems. (arXiv:2302.01377v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ben_Porat_O/0/1/0/all/0/1">Omer Ben-Porat</a>, <a href="http://arxiv.org/find/cs/1/au:+Torkan_R/0/1/0/all/0/1">Rotem Torkan</a></p>
<p>Recommendation systems are dynamic economic systems that balance the needs of
multiple stakeholders. A recent line of work studies incentives from the
content providers' point of view. Content providers, e.g., vloggers and
bloggers, contribute fresh content and rely on user engagement to create
revenue and finance their operations. In this work, we propose a contextual
multi-armed bandit setting to model the dependency of content providers on
exposure. In our model, the system receives a user context in every round and
has to select one of the arms. Every arm is a content provider who must receive
a minimum number of pulls every fixed time period (e.g., a month) to remain
viable in later rounds; otherwise, the arm departs and is no longer available.
The system aims to maximize the users' (content consumers) welfare. To that
end, it should learn which arms are vital and ensure they remain viable by
subsidizing arm pulls if needed. We develop algorithms with sub-linear regret,
as well as a lower bound that demonstrates that our algorithms are optimal up
to logarithmic factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01428">Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation. (arXiv:2302.01428v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loo_N/0/1/0/all/0/1">Noel Loo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1">Mathias Lechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1">Alexander Amini</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>Modern deep learning requires large volumes of data, which could contain
sensitive or private information that cannot be leaked. Recent work has shown
for homogeneous neural networks a large portion of this training data could be
reconstructed with only access to the trained network parameters. While the
attack was shown to work empirically, there exists little formal understanding
of its effective regime which datapoints are susceptible to reconstruction. In
this work, we first build a stronger version of the dataset reconstruction
attack and show how it can provably recover the \emph{entire training set} in
the infinite width regime. We then empirically study the characteristics of
this attack on two-layer networks and reveal that its success heavily depends
on deviations from the frozen infinite-width Neural Tangent Kernel limit. Next,
we study the nature of easily-reconstructed images. We show that both
theoretically and empirically, reconstructed images tend to "outliers" in the
dataset, and that these reconstruction attacks can be used for \textit{dataset
distillation}, that is, we can retrain on reconstructed images and obtain high
predictive accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02343">LExecutor: Learning-Guided Execution. (arXiv:2302.02343v4 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Souza_B/0/1/0/all/0/1">Beatriz Souza</a>, <a href="http://arxiv.org/find/cs/1/au:+Pradel_M/0/1/0/all/0/1">Michael Pradel</a></p>
<p>Executing code is essential for various program analysis tasks, e.g., to
detect bugs that manifest through exceptions or to obtain execution traces for
further dynamic analysis. However, executing an arbitrary piece of code is
often difficult in practice, e.g., because of missing variable definitions,
missing user inputs, and missing third-party dependencies. This paper presents
LExecutor, a learning-guided approach for executing arbitrary code snippets in
an underconstrained way. The key idea is to let a neural model predict missing
values that otherwise would cause the program to get stuck, and to inject these
values into the execution. For example, LExecutor injects likely values for
otherwise undefined variables and likely return values of calls to otherwise
missing functions. We evaluate the approach on Python code from popular
open-source projects and on code snippets extracted from Stack Overflow. The
neural model predicts realistic values with an accuracy between 79.5% and
98.2%, allowing LExecutor to closely mimic real executions. As a result, the
approach successfully executes significantly more code than any available
technique, such as simply executing the code as-is. For example, executing the
open-source code snippets as-is covers only 4.1% of all lines, because the code
crashes early on, whereas LExecutor achieves a coverage of 51.6%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.04071">Taming Local Effects in Graph-based Spatiotemporal Forecasting. (arXiv:2302.04071v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cini_A/0/1/0/all/0/1">Andrea Cini</a>, <a href="http://arxiv.org/find/cs/1/au:+Marisca_I/0/1/0/all/0/1">Ivan Marisca</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambon_D/0/1/0/all/0/1">Daniele Zambon</a>, <a href="http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1">Cesare Alippi</a></p>
<p>Spatiotemporal graph neural networks have shown to be effective in time
series forecasting applications, achieving better performance than standard
univariate predictors in several settings. These architectures take advantage
of a graph structure and relational inductive biases to learn a single (global)
inductive model to predict any number of the input time series, each associated
with a graph node. Despite the gain achieved in computational and data
efficiency w.r.t. fitting a set of local models, relying on a single global
model can be a limitation whenever some of the time series are generated by a
different spatiotemporal stochastic process. The main objective of this paper
is to understand the interplay between globality and locality in graph-based
spatiotemporal forecasting, while contextually proposing a methodological
framework to rationalize the practice of including trainable node embeddings in
such architectures. We ascribe to trainable node embeddings the role of
amortizing the learning of specialized components. Moreover, embeddings allow
for 1) effectively combining the advantages of shared message-passing layers
with node-specific parameters and 2) efficiently transferring the learned model
to new node sets. Supported by strong empirical evidence, we provide insights
and guidelines for specializing graph-based models to the dynamics of each time
series and show how this aspect plays a crucial role in obtaining accurate
predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06755">Dataset Distillation with Convexified Implicit Gradients. (arXiv:2302.06755v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Loo_N/0/1/0/all/0/1">Noel Loo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1">Ramin Hasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1">Mathias Lechner</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a></p>
<p>We propose a new dataset distillation algorithm using reparameterization and
convexification of implicit gradients (RCIG), that substantially improves the
state-of-the-art. To this end, we first formulate dataset distillation as a
bi-level optimization problem. Then, we show how implicit gradients can be
effectively used to compute meta-gradient updates. We further equip the
algorithm with a convexified approximation that corresponds to learning on top
of a frozen finite-width neural tangent kernel. Finally, we improve bias in
implicit gradients by parameterizing the neural network to enable analytical
computation of final-layer parameters given the body parameters. RCIG
establishes the new state-of-the-art on a diverse series of dataset
distillation tasks. Notably, with one image per class, on resized ImageNet,
RCIG sees on average a 108\% improvement over the previous state-of-the-art
distillation algorithm. Similarly, we observed a 66\% gain over SOTA on
Tiny-ImageNet and 37\% on CIFAR-100.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02637">A Semi-Bayesian Nonparametric Estimator of the Maximum Mean Discrepancy Measure: Applications in Goodness-of-Fit Testing and Generative Adversarial Networks. (arXiv:2303.02637v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Fazeli_Asl_F/0/1/0/all/0/1">Forough Fazeli-Asl</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1">Michael Minyi Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Lin_L/0/1/0/all/0/1">Lizhen Lin</a></p>
<p>A classic inferential statistical problem is the goodness-of-fit (GOF) test.
Such a test can be challenging when the hypothesized parametric model has an
intractable likelihood and its distributional form is not available. Bayesian
methods for GOF can be appealing due to their ability to incorporate expert
knowledge through prior distributions.
</p>
<p>However, standard Bayesian methods for this test often require strong
distributional assumptions on the data and their relevant parameters. To
address this issue, we propose a semi-Bayesian nonparametric (semi-BNP)
procedure in the context of the maximum mean discrepancy (MMD) measure that can
be applied to the GOF test. Our method introduces a novel Bayesian estimator
for the MMD, enabling the development of a measure-based hypothesis test for
intractable models. Through extensive experiments, we demonstrate that our
proposed test outperforms frequentist MMD-based methods by achieving a lower
false rejection and acceptance rate of the null hypothesis. Furthermore, we
showcase the versatility of our approach by embedding the proposed estimator
within a generative adversarial network (GAN) framework. It facilitates a
robust BNP learning approach as another significant application of our method.
With our BNP procedure, this new GAN approach can enhance sample diversity and
improve inferential accuracy compared to traditional techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08994">Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility. (arXiv:2303.08994v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Stiasny_J/0/1/0/all/0/1">Jochen Stiasny</a>, <a href="http://arxiv.org/find/eess/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1">Spyros Chatzivasileiadis</a></p>
<p>The simulation of power system dynamics poses a computationally expensive
task. Considering the growing uncertainty of generation and demand patterns,
thousands of scenarios need to be continuously assessed to ensure the safety of
power systems. Physics-Informed Neural Networks (PINNs) have recently emerged
as a promising solution for drastically accelerating computations of non-linear
dynamical systems. This work investigates the applicability of these methods
for power system dynamics, focusing on the dynamic response to load
disturbances. Comparing the prediction of PINNs to the solution of conventional
solvers, we find that PINNs can be 10 to 1000 times faster than conventional
solvers. At the same time, we find them to be sufficiently accurate and
numerically stable even for large time steps. To facilitate a deeper
understanding, this paper also present a new regularisation of Neural Network
(NN) training by introducing a gradient-based term in the loss function. The
resulting NNs, which we call dtNNs, help us deliver a comprehensive analysis
about the strengths and weaknesses of the NN based approaches, how
incorporating knowledge of the underlying physics affects NN performance, and
how this compares with conventional solvers for power system dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02169">Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1">Brandon Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Synthetic electronic health records (EHRs) that are both realistic and
preserve privacy can serve as an alternative to real EHRs for machine learning
(ML) modeling and statistical analysis. However, generating high-fidelity and
granular electronic health record (EHR) data in its original,
highly-dimensional form poses challenges for existing methods due to the
complexities inherent in high-dimensional data. In this paper, we propose
Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal
high-dimensional EHR, which preserve the statistical properties of real EHR and
can be used to train accurate ML models without privacy concerns. Our HALO
method, designed as a hierarchical autoregressive model, generates a
probability density function of medical codes, clinical visits, and patient
records, allowing for the generation of realistic EHR data in its original,
unaggregated form without the need for variable selection or aggregation.
Additionally, our model also produces high-quality continuous variables in a
longitudinal and probabilistic manner. We conducted extensive experiments and
demonstrate that HALO can generate high-fidelity EHR data with high-dimensional
disease code probabilities (d &gt; 10,000), disease co-occurrence probabilities
within visits (d &gt; 1,000,000), and conditional probabilities across consecutive
visits (d &gt; 5,000,000) and achieve above 0.9 R2 correlation in comparison to
real EHR data. This performance then enables downstream ML models trained on
its synthetic data to achieve comparable accuracy to models trained on real
data (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using a
combination of real and synthetic data enhances the accuracy of ML models
beyond that achieved by using only real EHR data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02754">Conceptual structure coheres in human cognition but not in large language models. (arXiv:2304.02754v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1">Siddharth Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1">Kushin Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xizheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei-Chun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Padua_L/0/1/0/all/0/1">Lisa Padua</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1">Timothy T Rogers</a></p>
<p>Neural network models of language have long been used as a tool for
developing hypotheses about conceptual representation in the mind and brain.
For many years, such use involved extracting vector-space representations of
words and using distances among these to predict or understand human behavior
in various semantic tasks. Contemporary large language models (LLMs), however,
make it possible to interrogate the latent structure of conceptual
representations using experimental methods nearly identical to those commonly
used with human participants. The current work utilizes three common techniques
borrowed from cognitive psychology to estimate and compare the structure of
concepts in humans and a suite of LLMs. In humans, we show that conceptual
structure is robust to differences in culture, language, and method of
estimation. Structures estimated from LLM behavior, while individually fairly
consistent with those estimated from human behavior, vary much more depending
upon the particular task used to generate responses--across tasks, estimates of
conceptual structure from the very same model cohere less with one another than
do human structure estimates. These results highlight an important difference
between contemporary LLMs and human cognition, with implications for
understanding some fundamental limitations of contemporary machine language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04307">PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling. (arXiv:2304.04307v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Semenova_E/0/1/0/all/0/1">Elizaveta Semenova</a>, <a href="http://arxiv.org/find/stat/1/au:+Verma_P/0/1/0/all/0/1">Prakhar Verma</a>, <a href="http://arxiv.org/find/stat/1/au:+Cairney_Leeming_M/0/1/0/all/0/1">Max Cairney-Leeming</a>, <a href="http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1">Arno Solin</a>, <a href="http://arxiv.org/find/stat/1/au:+Bhatt_S/0/1/0/all/0/1">Samir Bhatt</a>, <a href="http://arxiv.org/find/stat/1/au:+Flaxman_S/0/1/0/all/0/1">Seth Flaxman</a></p>
<p>Recent advances have shown that GP priors, or their finite realisations, can
be encoded using deep generative models such as variational autoencoders
(VAEs). These learned generators can serve as drop-in replacements for the
original priors during MCMC inference. While this approach enables efficient
inference, it loses information about the hyperparameters of the original
models, and consequently makes inference over hyperparameters impossible and
the learned priors indistinct. To overcome this limitation, we condition the
VAE on stochastic process hyperparameters. This allows the joint encoding of
hyperparameters with GP realizations and their subsequent estimation during
inference. Further, we demonstrate that our proposed method, PriorCVAE, is
agnostic to the nature of the models which it approximates, and can be used,
for instance, to encode solutions of ODEs. It provides a practical tool for
approximate inference and shows potential in real-life spatial and
spatiotemporal applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05727">Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Linhardt_L/0/1/0/all/0/1">Lorenz Linhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Montavon_G/0/1/0/all/0/1">Gr&#xe9;goire Montavon</a></p>
<p>Robustness has become an important consideration in deep learning. With the
help of explainable AI, mismatches between an explained model's decision
strategy and the user's domain knowledge (e.g. Clever Hans effects) have been
identified as a starting point for improving faulty models. However, it is less
clear what to do when the user and the explanation agree. In this paper, we
demonstrate that acceptance of explanations by the user is not a guarantee for
a machine learning model to be robust against Clever Hans effects, which may
remain undetected. Such hidden flaws of the model can nevertheless be
mitigated, and we demonstrate this by contributing a new method,
Explanation-Guided Exposure Minimization (EGEM), that preemptively prunes
variations in the ML model that have not been the subject of positive
explanation feedback. Experiments demonstrate that our approach leads to models
that strongly reduce their reliance on hidden Clever Hans strategies, and
consequently achieve higher accuracy on new data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06244">Computationally-Efficient Neural Image Compression with Shallow Decoders. (arXiv:2304.06244v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1">Yibo Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1">Stephan Mandt</a></p>
<p>Neural image compression methods have seen increasingly strong performance in
recent years. However, they suffer orders of magnitude higher computational
complexity compared to traditional codecs, which hinders their real-world
deployment. This paper takes a step forward towards closing this gap in
decoding complexity by using a shallow or even linear decoding transform
resembling that of JPEG. To compensate for the resulting drop in compression
performance, we exploit the often asymmetrical computation budget between
encoding and decoding, by adopting more powerful encoder networks and iterative
encoding. We theoretically formalize the intuition behind, and our experimental
results establish a new frontier in the trade-off between rate-distortion and
decoding complexity for neural image compression. Specifically, we achieve
rate-distortion performance competitive with the established mean-scale
hyperprior architecture of Minnen et al. (2018) at less than 50K decoding
FLOPs/pixel, reducing the baseline's overall decoding complexity by 80%, or
over 90% for the synthesis transform alone. Our code can be found at
https://github.com/mandt-lab/shallow-ntc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09957">Deep quantum neural networks form Gaussian processes. (arXiv:2305.09957v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Garcia_Martin_D/0/1/0/all/0/1">Diego Garc&#xed;a-Mart&#xed;n</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Larocca_M/0/1/0/all/0/1">Martin Larocca</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1">M. Cerezo</a></p>
<p>It is well known that artificial neural networks initialized from independent
and identically distributed priors converge to Gaussian processes in the limit
of large number of neurons per hidden layer. In this work we prove an analogous
result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of
certain models based on Haar random unitary or orthogonal deep QNNs converge to
Gaussian processes in the limit of large Hilbert space dimension $d$. The
derivation of this result is more nuanced than in the classical case due to the
role played by the input states, the measurement observable, and the fact that
the entries of unitary matrices are not independent. An important consequence
of our analysis is that the ensuing Gaussian processes cannot be used to
efficiently predict the outputs of the QNN via Bayesian statistics.
Furthermore, our theorems imply that the concentration of measure phenomenon in
Haar random QNNs is worse than previously thought, as we prove that expectation
values and gradients concentrate as $\mathcal{O}\left(\frac{1}{e^d
\sqrt{d}}\right)$. Finally, we discuss how our results improve our
understanding of concentration in $t$-designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13971">Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning. (arXiv:2305.13971v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1">Saibo Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1">Martin Josifoski</a>, <a href="http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1">Maxime Peyrard</a>, <a href="http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1">Robert West</a></p>
<p>Despite their impressive performance, large language models (LMs) still
struggle with reliably generating complex output structures when not finetuned
to follow the required output format exactly. To address this issue,
grammar-constrained decoding (GCD) can be used to control the generation of
LMs, guaranteeing that the output follows a given structure. Most existing GCD
methods are, however, limited to specific tasks, such as parsing or code
generation. In this work, we demonstrate that formal grammars can describe the
output space for a much wider range of tasks and argue that GCD can serve as a
unified framework for structured NLP tasks in general. For increased
flexibility, we introduce input-dependent grammars, which allow the grammar to
depend on the input and thus enable the generation of different output
structures for different inputs. We then empirically demonstrate the power and
flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity
disambiguation, and (3) constituency parsing. Our results indicate that
grammar-constrained LMs substantially outperform unconstrained LMs or even beat
task-specific finetuned models. Grammar constraints thus hold great promise for
harnessing off-the-shelf LMs for a wide range of structured NLP tasks,
especially where training data is scarce or finetuning is expensive. Code and
data: https://github.com/epfl-dlab/GCD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17328">Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dedhia_B/0/1/0/all/0/1">Bhishma Dedhia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1">Niraj K. Jha</a></p>
<p>Deployment of Transformer models on edge devices is becoming increasingly
challenging due to the exponentially growing inference cost that scales
quadratically with the number of tokens in the input sequence. Token pruning is
an emerging solution to address this challenge due to its ease of deployment on
various Transformer backbones. However, most token pruning methods require
computationally expensive fine-tuning, which is undesirable in many edge
deployment cases. In this work, we propose Zero-TPrune, the first zero-shot
method that considers both the importance and similarity of tokens in
performing token pruning. It leverages the attention graph of pre-trained
Transformer models to produce an importance distribution for tokens via our
proposed Weighted Page Rank (WPR) algorithm. This distribution further guides
token partitioning for efficient similarity-based pruning. Due to the
elimination of the fine-tuning overhead, Zero-TPrune can prune large models at
negligible computational cost, switch between different pruning configurations
at no computational cost, and perform hyperparameter tuning efficiently. We
evaluate the performance of Zero-TPrune on vision tasks by applying it to
various vision Transformer backbones and testing them on ImageNet. Without any
fine-tuning, Zero-TPrune reduces the FLOPs cost of DeiT-S by 34.7\% and
improves its throughput by 45.3\% with only 0.4\% accuracy loss. Compared with
state-of-the-art pruning methods that require fine-tuning, Zero-TPrune not only
eliminates the need for fine-tuning after pruning but also does so with only
0.1\% accuracy loss. Compared with state-of-the-art fine-tuning-free pruning
methods, Zero-TPrune reduces accuracy loss by up to 49\% with the same or
higher throughput.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19429">Adapting Fairness Interventions to Missing Values. (arXiv:2305.19429v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1">Raymond Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1">Flavio P. Calmon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>Missing values in real-world data pose a significant and unique challenge to
algorithmic fairness. Different demographic groups may be unequally affected by
missing data, and the standard procedure for handling missing values where
first data is imputed, then the imputed data is used for classification -- a
procedure referred to as "impute-then-classify" -- can exacerbate
discrimination. In this paper, we analyze how missing values affect algorithmic
fairness. We first prove that training a classifier from imputed data can
significantly worsen the achievable values of group fairness and average
accuracy. This is because imputing data results in the loss of the missing
pattern of the data, which often conveys information about the predictive
label. We present scalable and adaptive algorithms for fair classification with
missing values. These algorithms can be combined with any preexisting
fairness-intervention algorithm to handle all possible missing patterns while
preserving information encoded within the missing patterns. Numerical
experiments with state-of-the-art fairness interventions demonstrate that our
adaptive algorithms consistently achieve higher fairness and accuracy than
impute-then-classify across different datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19454">Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Lu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tianjin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1">Vlado Menkovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaolong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1">Mykola Pechenizkiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiwei Liu</a></p>
<p>Sparse training has received an upsurging interest in machine learning due to
its tantalizing saving potential for the entire training process as well as
inference. Dynamic sparse training (DST), as a leading sparse training
approach, can train deep neural networks at high sparsity from scratch to match
the performance of their dense counterparts. However, most if not all DST prior
arts demonstrate their effectiveness on unstructured sparsity with highly
irregular sparse patterns, which receives limited support in common hardware.
This limitation hinders the usage of DST in practice. In this paper, we propose
Channel-aware dynamic sparse (Chase), which for the first time seamlessly
translates the promise of unstructured dynamic sparsity to GPU-friendly
channel-level sparsity (not fine-grained N:M or group sparsity) during one
end-to-end training process, without any ad-hoc operations. The resulting small
sparse networks can be directly accelerated by commodity hardware, without
using any particularly sparsity-aware hardware accelerators. This appealing
outcome is partially motivated by a hidden phenomenon of dynamic sparsity:
off-the-shelf unstructured DST implicitly involves biased parameter
reallocation across channels, with a large fraction of channels (up to 60%)
being sparser than others. By progressively identifying and removing these
channels during training, our approach translates unstructured sparsity to
channel-wise sparsity. Our experimental results demonstrate that Chase achieves
1.7 X inference throughput speedup on common GPU devices without compromising
accuracy with ResNet-50 on ImageNet. We release our codes in
https://github.com/luuyin/chase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00297">Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahn_K/0/1/0/all/0/1">Kwangjun Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiang Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Daneshmand_H/0/1/0/all/0/1">Hadi Daneshmand</a>, <a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1">Suvrit Sra</a></p>
<p>Several recent works demonstrate that transformers can implement algorithms
like gradient descent. By a careful construction of weights, these works show
that multiple layers of transformers are expressive enough to simulate
iterations of gradient descent. Going beyond the question of expressivity, we
ask: Can transformers learn to implement such algorithms by training over
random problem instances? To our knowledge, we make the first theoretical
progress on this question via an analysis of the loss landscape for linear
transformers trained over random instances of linear regression. For a single
attention layer, we prove the global minimum of the training objective
implements a single iteration of preconditioned gradient descent. Notably, the
preconditioning matrix not only adapts to the input distribution but also to
the variance induced by data inadequacy. For a transformer with $L$ attention
layers, we prove certain critical points of the training objective implement
$L$ iterations of preconditioned gradient descent. Our results call for future
theoretical studies on learning algorithms by training transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05674">Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks. (arXiv:2306.05674v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Huang_Z/0/1/0/all/0/1">Ziyi Huang</a>, <a href="http://arxiv.org/find/stat/1/au:+Lam_H/0/1/0/all/0/1">Henry Lam</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1">Haofeng Zhang</a></p>
<p>Uncertainty quantification (UQ) is important for reliability assessment and
enhancement of machine learning models. In deep learning, uncertainties arise
not only from data, but also from the training procedure that often injects
substantial noises and biases. These hinder the attainment of statistical
guarantees and, moreover, impose computational challenges on UQ due to the need
for repeated network retraining. Building upon the recent neural tangent kernel
theory, we create statistically guaranteed schemes to principally
\emph{characterize}, and \emph{remove}, the uncertainty of over-parameterized
neural networks with very low computation effort. In particular, our approach,
based on what we call a procedural-noise-correcting (PNC) predictor, removes
the procedural uncertainty by using only \emph{one} auxiliary network that is
trained on a suitably labeled dataset, instead of many retrained networks
employed in deep ensembles. Moreover, by combining our PNC predictor with
suitable light-computation resampling methods, we build several approaches to
construct asymptotically exact-coverage confidence intervals using as low as
four trained networks without additional overheads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12818">StrainTensorNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks. (arXiv:2306.12818v2 [cond-mat.dis-nn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Pakornchote_T/0/1/0/all/0/1">Teerachote Pakornchote</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Ektarawong_A/0/1/0/all/0/1">Annop Ektarawong</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Chotibut_T/0/1/0/all/0/1">Thiparat Chotibut</a></p>
<p>Accurately predicting the elastic properties of crystalline solids is vital
for computational materials science. However, traditional atomistic scale ab
initio approaches are computationally intensive, especially for studying
complex materials with a large number of atoms in a unit cell. We introduce a
novel data-driven approach to efficiently predict the elastic properties of
crystal structures using SE(3)-equivariant graph neural networks (GNNs). This
approach yields important scalar elastic moduli with the accuracy comparable to
recent data-driven studies. Importantly, our symmetry-aware GNNs model also
enables the prediction of the strain energy density (SED) and the associated
elastic constants, the fundamental tensorial quantities that are significantly
influenced by a material's crystallographic group. The model consistently
distinguishes independent elements of SED tensors, in accordance with the
symmetry of the crystal structures. Finally, our deep learning model possesses
meaningful latent features, offering an interpretable prediction of the elastic
properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16838">Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Allerbo_O/0/1/0/all/0/1">Oskar Allerbo</a></p>
<p>Kernel ridge regression, KRR, is a generalization of linear ridge regression
that is non-linear in the data, but linear in the parameters. Here, we
introduce an equivalent formulation of the objective function of KRR, opening
up both for using penalties other than the ridge penalty and for studying
kernel ridge regression from the perspective of gradient descent. Using a
continuous-time perspective, we derive a closed-form solution for solving
kernel regression with gradient descent, something we refer to as kernel
gradient flow, KGF, and theoretically bound the differences between KRR and
KGF, where, for the latter, regularization is obtained through early stopping.
We also generalize KRR by replacing the ridge penalty with the $\ell_1$ and
$\ell_\infty$ penalties, respectively, and use the fact that analogous to the
similarities between KGF and KRR, $\ell_1$ regularization and forward stagewise
regression (also known as coordinate descent), and $\ell_\infty$ regularization
and sign gradient descent, follow similar solution paths. We can thus alleviate
the need for computationally heavy algorithms based on proximal gradient
descent. We show theoretically and empirically how the $\ell_1$ and
$\ell_\infty$ penalties, and the corresponding gradient-based optimization
algorithms, produce sparse and robust kernel regression solutions,
respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01073">What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?. (arXiv:2307.01073v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suya_F/0/1/0/all/0/1">Fnu Suya</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1">David Evans</a></p>
<p>We study indiscriminate poisoning for linear learners where an adversary
injects a few crafted examples into the training data with the goal of forcing
the induced model to incur higher test error. Inspired by the observation that
linear learners on some datasets are able to resist the best known attacks even
without any defenses, we further investigate whether datasets can be inherently
robust to indiscriminate poisoning attacks for linear learners. For theoretical
Gaussian distributions, we rigorously characterize the behavior of an optimal
poisoning attack, defined as the poisoning strategy that attains the maximum
risk of the induced model at a given poisoning budget. Our results prove that
linear learners can indeed be robust to indiscriminate poisoning if the
class-wise data distributions are well-separated with low variance and the size
of the constraint set containing all permissible poisoning points is also
small. These findings largely explain the drastic variation in empirical attack
performance of the state-of-the-art poisoning attacks on linear learners across
benchmark datasets, making an important initial step towards understanding the
underlying reasons some learning tasks are vulnerable to data poisoning
attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02509">Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1">Mahieu Pont</a>, <a href="http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1">Julien Tierny</a></p>
<p>This paper presents a computational framework for the Wasserstein
auto-encoding of merge trees (MT-WAE), a novel extension of the classical
auto-encoder neural network architecture to the Wasserstein metric space of
merge trees. In contrast to traditional auto-encoders which operate on
vectorized data, our formulation explicitly manipulates merge trees on their
associated metric space at each layer of the network, resulting in superior
accuracy and interpretability. Our novel neural network approach can be
interpreted as a non-linear generalization of previous linear attempts [79] at
merge tree encoding. It also trivially extends to persistence diagrams.
Extensive experiments on public ensembles demonstrate the efficiency of our
algorithms, with MT-WAE computations in the orders of minutes on average. We
show the utility of our contributions in two applications adapted from previous
work on merge tree encoding [79]. First, we apply MT-WAE to merge tree
compression, by concisely representing them with their coordinates in the final
layer of our auto-encoder. Second, we document an application to dimensionality
reduction, by exploiting the latent space of our auto-encoder, for the visual
analysis of ensemble data. We illustrate the versatility of our framework by
introducing two penalty terms, to help preserve in the latent space both the
Wasserstein distances between merge trees, as well as their clusters. In both
applications, quantitative experiments assess the relevance of our framework.
Finally, we provide a C++ implementation that can be used for reproducibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15870">Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhipeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hongli Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yunming Liao</a></p>
<p>Federated Learning (FL) has emerged to allow multiple clients to
collaboratively train machine learning models on their private data. However,
training and deploying large-scale models on resource-constrained clients is
challenging. Fortunately, Split Federated Learning (SFL) offers a feasible
solution by alleviating the computation and/or communication burden on clients.
However, existing SFL works often assume sufficient labeled data on clients,
which is usually impractical. Besides, data non-IIDness across clients poses
another challenge to ensure efficient model training. To our best knowledge,
the above two issues have not been simultaneously addressed in SFL. Herein, we
propose a novel Semi-SFL system, which incorporates clustering regularization
to perform SFL under the more practical scenario with unlabeled and non-IID
client data. Moreover, our theoretical and experimental investigations into
model convergence reveal that the inconsistent training processes on labeled
and unlabeled data have an influence on the effectiveness of clustering
regularization. To this end, we develop a control algorithm for dynamically
adjusting the global updating frequency, so as to mitigate the training
inconsistency and improve training performance. Extensive experiments on
benchmark models and datasets show that our system provides a 3.0x speed-up in
training time and reduces the communication cost by about 70.3% while reaching
the target accuracy, and achieves up to 5.1% improvement in accuracy under
non-IID scenarios compared to the state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00177">Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1">Charlie Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Thekumparampil_K/0/1/0/all/0/1">Kiran Koshy Thekumparampil</a>, <a href="http://arxiv.org/find/cs/1/au:+Shavlovsky_M/0/1/0/all/0/1">Michael Shavlovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1">Giulia Fanti</a>, <a href="http://arxiv.org/find/cs/1/au:+Dattatreya_Y/0/1/0/all/0/1">Yesh Dattatreya</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1">Sujay Sanghavi</a></p>
<p>While deep learning (DL) models are state-of-the-art in text and image
domains, they have not yet consistently outperformed Gradient Boosted Decision
Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent
performance gains attained by DL models in text and image tasks have used
unsupervised pretraining, which exploits orders of magnitude more unlabeled
data than labeled data. To the best of our knowledge, unsupervised pretraining
has not been applied to the LTR problem, which often produces vast amounts of
unlabeled data.
</p>
<p>In this work, we study whether unsupervised pretraining of deep models can
improve LTR performance over GBDTs and other non-pretrained models. By
incorporating simple design choices--including SimCLR-Rank, an LTR-specific
pretraining loss--we produce pretrained deep learning models that consistently
(across datasets) outperform GBDTs (and other non-pretrained rankers) in the
case where there is more unlabeled data than labeled data. This performance
improvement occurs not only on average but also on outlier queries. We base our
empirical conclusions off of experiments on (1) public benchmark tabular LTR
datasets, and (2) a large industry-scale proprietary ranking dataset. Code is
provided at https://anonymous.4open.science/r/ltr-pretrain-0DAD/README.md.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.02442">Distribution-Informed Adaptation for kNN Graph Construction. (arXiv:2308.02442v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1">Shaojie Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Ji Liu</a></p>
<p>Graph-based kNN algorithms have garnered widespread popularity for machine
learning tasks due to their simplicity and effectiveness. However, as factual
data often inherit complex distributions, the conventional kNN graph's reliance
on a unified k-value can hinder its performance. A crucial factor behind this
challenge is the presence of ambiguous samples along decision boundaries that
are inevitably more prone to incorrect classifications. To address the
situation, we propose the Distribution-Informed adaptive kNN Graph (DaNNG),
which combines adaptive kNN with distribution-aware graph construction. By
incorporating an approximation of the distribution with customized k-adaption
criteria, DaNNG can significantly improve performance on ambiguous samples, and
hence enhance overall accuracy and generalization capability. Through rigorous
evaluations on diverse benchmark datasets, DaNNG outperforms state-of-the-art
algorithms, showcasing its adaptability and efficacy across various real-world
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10664">A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks. (arXiv:2308.10664v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koursioumpas_N/0/1/0/all/0/1">Nikolaos Koursioumpas</a>, <a href="http://arxiv.org/find/cs/1/au:+Magoula_L/0/1/0/all/0/1">Lina Magoula</a>, <a href="http://arxiv.org/find/cs/1/au:+Petropouleas_N/0/1/0/all/0/1">Nikolaos Petropouleas</a>, <a href="http://arxiv.org/find/cs/1/au:+Thanopoulos_A/0/1/0/all/0/1">Alexandros-Ioannis Thanopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Panagea_T/0/1/0/all/0/1">Theodora Panagea</a>, <a href="http://arxiv.org/find/cs/1/au:+Alonistioti_N/0/1/0/all/0/1">Nancy Alonistioti</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_Estevez_M/0/1/0/all/0/1">M. A. Gutierrez-Estevez</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalili_R/0/1/0/all/0/1">Ramin Khalili</a></p>
<p>Progressing towards a new era of Artificial Intelligence (AI) - enabled
wireless networks, concerns regarding the environmental impact of AI have been
raised both in industry and academia. Federated Learning (FL) has emerged as a
key privacy preserving decentralized AI technique. Despite efforts currently
being made in FL, its environmental impact is still an open problem. Targeting
the minimization of the overall energy consumption of an FL process, we propose
the orchestration of computational and communication resources of the involved
devices to minimize the total energy required, while guaranteeing a certain
performance of the model. To this end, we propose a Soft Actor Critic Deep
Reinforcement Learning (DRL) solution, where a penalty function is introduced
during training, penalizing the strategies that violate the constraints of the
environment, and contributing towards a safe RL process. A device level
synchronization method, along with a computationally cost effective FL
environment are proposed, with the goal of further reducing the energy
consumption and communication overhead. Evaluation results show the
effectiveness and robustness of the proposed scheme compared to four
state-of-the-art baseline solutions on different network environments and FL
architectures, achieving a decrease of up to 94% in the total energy
consumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10697">Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems. (arXiv:2308.10697v3 [math.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Colbrook_M/0/1/0/all/0/1">Matthew J. Colbrook</a>, <a href="http://arxiv.org/find/math/1/au:+Li_Q/0/1/0/all/0/1">Qin Li</a>, <a href="http://arxiv.org/find/math/1/au:+Raut_R/0/1/0/all/0/1">Ryan V. Raut</a>, <a href="http://arxiv.org/find/math/1/au:+Townsend_A/0/1/0/all/0/1">Alex Townsend</a></p>
<p>Koopman operators linearize nonlinear dynamical systems, making their
spectral information of crucial interest. Numerous algorithms have been
developed to approximate these spectral properties, and Dynamic Mode
Decomposition (DMD) stands out as the poster child of projection-based methods.
Although the Koopman operator itself is linear, the fact that it acts in an
infinite-dimensional space of observables poses challenges. These include
spurious modes, essential spectra, and the verification of Koopman mode
decompositions. While recent work has addressed these challenges for
deterministic systems, there remains a notable gap in verified DMD methods for
stochastic systems, where the Koopman operator measures the expectation of
observables. We show that it is necessary to go beyond expectations to address
these issues. By incorporating variance into the Koopman framework, we address
these challenges. Through an additional DMD-type matrix, we approximate the sum
of a squared residual and a variance term, each of which can be approximated
individually using batched snapshot data. This allows verified computation of
the spectral properties of stochastic Koopman operators, controlling the
projection error. We also introduce the concept of variance-pseudospectra to
gauge statistical coherency. Finally, we present a suite of convergence results
for the spectral information of stochastic Koopman operators. Our study
concludes with practical applications using both simulated and experimental
data. In neural recordings from awake mice, we demonstrate how
variance-pseudospectra can reveal physiologically significant information
unavailable to standard expectation-based dynamical models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12482">State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1">Devleena Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1">Sonia Chernova</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Been Kim</a></p>
<p>As more non-AI experts use complex AI systems for daily tasks, there has been
an increasing effort to develop methods that produce explanations of AI
decision making that are understandable by non-AI experts. Towards this effort,
leveraging higher-level concepts and producing concept-based explanations have
become a popular method. Most concept-based explanations have been developed
for classification techniques, and we posit that the few existing methods for
sequential decision making are limited in scope. In this work, we first
contribute a desiderata for defining concepts in sequential decision making
settings. Additionally, inspired by the Protege Effect which states explaining
knowledge often reinforces one's self-learning, we explore how concept-based
explanations of an RL agent's decision making can in turn improve the agent's
learning rate, as well as improve end-user understanding of the agent's
decision making. To this end, we contribute a unified framework,
State2Explanation (S2E), that involves learning a joint embedding model between
state-action pairs and concept-based explanations, and leveraging such learned
model to both (1) inform reward shaping during an agent's training, and (2)
provide explanations to end-users at deployment for improved task performance.
Our experimental validations, in Connect 4 and Lunar Lander, demonstrate the
success of S2E in providing a dual-benefit, successfully informing reward
shaping and improving agent learning rate, as well as significantly improving
end user task performance at deployment time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13135">Forecasting Response to Treatment with Global Deep Learning and Patient-Specific Pharmacokinetic Priors. (arXiv:2309.13135v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Potosnak_W/0/1/0/all/0/1">Willa Potosnak</a>, <a href="http://arxiv.org/find/cs/1/au:+Challu_C/0/1/0/all/0/1">Cristian Challu</a>, <a href="http://arxiv.org/find/cs/1/au:+Olivares_K/0/1/0/all/0/1">Kin G. Olivares</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1">Artur Dubrawski</a></p>
<p>Forecasting healthcare time series is crucial for early detection of adverse
outcomes and for patient monitoring. Forecasting, however, can be difficult in
practice due to noisy and intermittent data. The challenges are often
exacerbated by change points induced via extrinsic factors, such as the
administration of medication. To address these challenges, we propose a novel
hybrid global-local architecture and a pharmacokinetic encoder that informs
deep learning models of patient-specific treatment effects. We showcase the
efficacy of our approach in achieving significant accuracy gains for a blood
glucose forecasting task using both realistically simulated and real-world
data. Our global-local architecture improves over patient-specific models by
9.2-14.6%. Additionally, our pharmacokinetic encoder improves over alternative
encoding techniques by 4.4% on simulated data and 2.1% on real-world data. The
proposed approach can have multiple beneficial applications in clinical
practice, such as issuing early warnings about unexpected treatment responses,
or helping to characterize patient-specific treatment effects in terms of drug
absorption and elimination characteristics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15877">Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xiongye Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gengshuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1">Gaurav Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1">Defu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shixuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yaxing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1">Tianqing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Mingxi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1">Paul Bogdan</a></p>
<p>Integrating and processing information from various sources or modalities are
critical for obtaining a comprehensive and accurate perception of the real
world. Drawing inspiration from neuroscience, we develop the
Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the
concept of information bottleneck. Distinct from most traditional fusion models
that aim to incorporate all modalities as input, our model designates the prime
modality as input, while the remaining modalities act as detectors in the
information pathway. Our proposed perception model focuses on constructing an
effective and compact information flow by achieving a balance between the
minimization of mutual information between the latent state and the input modal
state, and the maximization of mutual information between the latent states and
the remaining modal states. This approach leads to compact latent state
representations that retain relevant information while minimizing redundancy,
thereby substantially enhancing the performance of downstream tasks.
Experimental evaluations on both the MUStARD and CMU-MOSI datasets demonstrate
that our model consistently distills crucial information in multimodal learning
scenarios, outperforming state-of-the-art benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17340">Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer. (arXiv:2309.17340v2 [cs.DC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Shubham Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Sarthak Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Shaddy Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bisht_S/0/1/0/all/0/1">Sumit Bisht</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_C/0/1/0/all/0/1">Chahat Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonuguntla_A/0/1/0/all/0/1">Ashritha Gonuguntla</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1">Shiv Saini</a></p>
<p>Cloud services are omnipresent and critical cloud service failure is a fact
of life. In order to retain customers and prevent revenue loss, it is important
to provide high reliability guarantees for these services. One way to do this
is by predicting outages in advance, which can help in reducing the severity as
well as time to recovery. It is difficult to forecast critical failures due to
the rarity of these events. Moreover, critical failures are ill-defined in
terms of observable data. Our proposed method, Outage-Watch, defines critical
service outages as deteriorations in the Quality of Service (QoS) captured by a
set of metrics. Outage-Watch detects such outages in advance by using current
system state to predict whether the QoS metrics will cross a threshold and
initiate an extreme event. A mixture of Gaussian is used to model the
distribution of the QoS metrics for flexibility and an extreme event
regularizer helps in improving learning in tail of the distribution. An outage
is predicted if the probability of any one of the QoS metrics crossing
threshold changes significantly. Our evaluation on a real-world SaaS company
dataset shows that Outage-Watch significantly outperforms traditional methods
with an average AUC of 0.98. Additionally, Outage-Watch detects all the outages
exhibiting a change in service metrics and reduces the Mean Time To Detection
(MTTD) of outages by up to 88% when deployed in an enterprise cloud-service
system, demonstrating efficacy of our proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00233">CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images. (arXiv:2310.00233v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jerzak_C/0/1/0/all/0/1">Connor T. Jerzak</a>, <a href="http://arxiv.org/find/cs/1/au:+Daoud_A/0/1/0/all/0/1">Adel Daoud</a></p>
<p>The causalimages R package enables causal inference with image and image
sequence data, providing new tools for integrating novel data sources like
satellite and bio-medical imagery into the study of cause and effect. One set
of functions enables image-based causal inference analyses. For example, one
key function decomposes treatment effect heterogeneity by images using an
interpretable Bayesian framework. This allows for determining which types of
images or image sequences are most responsive to interventions. A second
modeling function allows researchers to control for confounding using images.
The package also allows investigators to produce embeddings that serve as
vector summaries of the image or video content. Finally, infrastructural
functions are also provided, such as tools for writing large-scale image and
image sequence data as sequentialized byte strings for more rapid image
analysis. causalimages therefore opens new capabilities for causal inference in
R, letting researchers use informative imagery in substantive analyses in a
fast and accessible manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02232">HoloNets: Spectral Convolutions do extend to Directed Graphs. (arXiv:2310.02232v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koke_C/0/1/0/all/0/1">Christian Koke</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a></p>
<p>Within the graph learning community, conventional wisdom dictates that
spectral convolutional networks may only be deployed on undirected graphs: Only
there could the existence of a well-defined graph Fourier transform be
guaranteed, so that information may be translated between spatial- and spectral
domains. Here we show this traditional reliance on the graph Fourier transform
to be superfluous and -- making use of certain advanced tools from complex
analysis and spectral theory -- extend spectral convolutions to directed
graphs. We provide a frequency-response interpretation of newly developed
filters, investigate the influence of the basis used to express filters and
discuss the interplay with characteristic operators on which networks are
based. In order to thoroughly test the developed theory, we conduct experiments
in real world settings, showcasing that directed spectral convolutional
networks provide new state of the art results for heterophilic node
classification on many datasets and -- as opposed to baselines -- may be
rendered stable to resolution-scale varying topological perturbations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02416">Bag of Tricks for Fully Test-Time Adaptation. (arXiv:2310.02416v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mounsaveng_S/0/1/0/all/0/1">Saypraseuth Mounsaveng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiaroni_F/0/1/0/all/0/1">Florent Chiaroni</a>, <a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1">Malik Boudiaf</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1">Marco Pedersoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ismail Ben Ayed</a></p>
<p>Fully Test-Time Adaptation (TTA), which aims at adapting models to data
drifts, has recently attracted wide interest. Numerous tricks and techniques
have been proposed to ensure robust learning on arbitrary streams of unlabeled
data. However, assessing the true impact of each individual technique and
obtaining a fair comparison still constitutes a significant challenge. To help
consolidate the community's knowledge, we present a categorization of selected
orthogonal TTA techniques, including small batch normalization, stream
rebalancing, reliable sample selection, and network confidence calibration. We
meticulously dissect the effect of each approach on different scenarios of
interest. Through our analysis, we shed light on trade-offs induced by those
techniques between accuracy, the computational power required, and model
complexity. We also uncover the synergy that arises when combining techniques
and are able to establish new state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02519">Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization. (arXiv:2310.02519v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jinrae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youdan Kim</a></p>
<p>Parameterized convex minorant (PCM) method is proposed for the approximation
of the objective function in amortized optimization. In the proposed method,
the objective function approximator is expressed by the sum of a PCM and a
nonnegative gap function, where the objective function approximator is bounded
from below by the PCM convex in the optimization variable. The proposed
objective function approximator is a universal approximator for continuous
functions, and the global minimizer of the PCM attains the global minimum of
the objective function approximator. Therefore, the global minimizer of the
objective function approximator can be obtained by a single convex
optimization. As a realization of the proposed method, extended parameterized
log-sum-exp network is proposed by utilizing a parameterized log-sum-exp
network as the PCM. Numerical simulation is performed for parameterized
non-convex objective function approximation and for learning-based nonlinear
model predictive control to demonstrate the performance and characteristics of
the proposed method. The simulation results support that the proposed method
can be used to learn objective functions and to find a global minimizer
reliably and quickly by using convex optimization algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02919">Attention-based Multi-task Learning for Base Editor Outcome Prediction. (arXiv:2310.02919v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mollaysa_A/0/1/0/all/0/1">Amina Mollaysa</a>, <a href="http://arxiv.org/find/cs/1/au:+Allam_A/0/1/0/all/0/1">Ahmed Allam</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1">Michael Krauthammer</a></p>
<p>Human genetic diseases often arise from point mutations, emphasizing the
critical need for precise genome editing techniques. Among these, base editing
stands out as it allows targeted alterations at the single nucleotide level.
However, its clinical application is hindered by low editing efficiency and
unintended mutations, necessitating extensive trial-and-error experimentation
in the laboratory. To speed up this process, we present an attention-based
two-stage machine learning model that learns to predict the likelihood of all
possible editing outcomes for a given genomic target sequence. We further
propose a multi-task learning schema to jointly learn multiple base editors
(i.e. variants) at once. Our model's predictions consistently demonstrated a
strong correlation with the actual experimental results on multiple datasets
and base editor variants. These results provide further validation for the
models' capacity to enhance and accelerate the process of refining base editing
designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04935">Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory. (arXiv:2310.04935v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mbacke_S/0/1/0/all/0/1">Sokhna Diarra Mbacke</a>, <a href="http://arxiv.org/find/cs/1/au:+Clerc_F/0/1/0/all/0/1">Florence Clerc</a>, <a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1">Pascal Germain</a></p>
<p>Since their inception, Variational Autoencoders (VAEs) have become central in
machine learning. Despite their widespread use, numerous questions regarding
their theoretical properties remain open. Using PAC-Bayesian theory, this work
develops statistical guarantees for VAEs. First, we derive the first
PAC-Bayesian bound for posterior distributions conditioned on individual
samples from the data-generating distribution. Then, we utilize this result to
develop generalization guarantees for the VAE's reconstruction loss, as well as
upper bounds on the distance between the input and the regenerated
distributions. More importantly, we provide upper bounds on the Wasserstein
distance between the input distribution and the distribution defined by the
VAE's generative model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07325">An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dao_J/0/1/0/all/0/1">James Dao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_Y/0/1/0/all/0/1">Yeu-Tong Lau</a>, <a href="http://arxiv.org/find/cs/1/au:+Rager_C/0/1/0/all/0/1">Can Rager</a>, <a href="http://arxiv.org/find/cs/1/au:+Janiak_J/0/1/0/all/0/1">Jett Janiak</a></p>
<p>How do language models deal with the limited bandwidth of the residual
stream? Prior work has suggested that some attention heads and MLP layers may
perform a "memory management" role. That is, clearing residual stream
directions set by earlier layers by reading in information and writing out the
negative version. In this work, we present concrete evidence for this
phenomenon in a 4-layer transformer. We identify several heads in layer 2 that
consistently remove the output of a single layer 0 head. We then verify that
this erasure causally depends on the original written direction. We further
demonstrate that direct logit attribution (DLA) suggests that writing and
erasing heads directly contribute to predictions, when in fact their effects
cancel out. Then we present adversarial prompts for which this effect is
particularly salient. These findings reveal that memory management can make DLA
results misleading. Accordingly, we make concrete recommendations for circuit
analysis to prevent interpretability illusions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07672">Stabilizing Estimates of Shapley Values with Control Variates. (arXiv:2310.07672v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Goldwasser_J/0/1/0/all/0/1">Jeremy Goldwasser</a>, <a href="http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1">Giles Hooker</a></p>
<p>Shapley values are among the most popular tools for explaining predictions of
blackbox machine learning models. However, their high computational cost
motivates the use of sampling approximations, inducing a considerable degree of
uncertainty. To stabilize these model explanations, we propose ControlSHAP, an
approach based on the Monte Carlo technique of control variates. Our
methodology is applicable to any machine learning model and requires virtually
no extra computation or modeling effort. On several high-dimensional datasets,
we find it can produce dramatic reductions in the Monte Carlo variability of
Shapley estimates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.09183">PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Mingjia Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huaizheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shudong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qing Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jiangcheng Lv</a></p>
<p>Classical federated learning (FL) enables training machine learning models
without sharing data for privacy preservation, but heterogeneous data
characteristic degrades the performance of the localized model. Personalized FL
(PFL) addresses this by synthesizing personalized models from a global model
via training on local data. Such a global model may overlook the specific
information that the clients have been sampled. In this paper, we propose a
novel scheme to inject personalized prior knowledge into the global model in
each client, which attempts to mitigate the introduced incomplete information
problem in PFL. At the heart of our proposed approach is a framework, the PFL
with Bregman Divergence (pFedBreD), decoupling the personalized prior from the
local objective function regularized by Bregman divergence for greater
adaptability in personalized scenarios. We also relax the mirror descent (RMD)
to extract the prior explicitly to provide optional strategies. Additionally,
our pFedBreD is backed up by a convergence analysis. Sufficient experiments
demonstrate that our method reaches the state-of-the-art performances on 5
datasets and outperforms other methods by up to 3.5% across 8 benchmarks.
Extensive analyses verify the robustness and necessity of proposed designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13001">Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Stephen Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gazeley_W/0/1/0/all/0/1">William Gazeley</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1">Siu Ho Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tingting Li</a></p>
<p>With the exponential growth in large language models (LLMs), leveraging their
emergent properties for specialized domains like finance merits exploration.
However, regulated fields such as finance pose unique constraints, requiring
domain-optimized frameworks. We present ConFIRM, an LLM-based conversational
financial information retrieval model tailored for query intent classification
and knowledge base labeling.
</p>
<p>ConFIRM comprises two modules:
</p>
<p>1) a method to synthesize finance domain-specific question-answer pairs, and
</p>
<p>2) evaluation of parameter efficient fine-tuning approaches for the query
classification task. We generate a dataset of over 4000 samples, assessing
accuracy on a separate test set.
</p>
<p>ConFIRM achieved over 90% accuracy, essential for regulatory compliance.
ConFIRM provides a data-efficient solution to extract precise query intent for
financial dialog systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15952">Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xing Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hengguan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichyporuk_B/0/1/0/all/0/1">Brennan Nichyporuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1">Tal Arbel</a></p>
<p>While deep learning models have achieved remarkable success across a range of
medical image analysis tasks, deployment of these models in real clinical
contexts requires that they be robust to variability in the acquired images.
While many methods apply predefined transformations to augment the training
data to enhance test-time robustness, these transformations may not ensure the
model's robustness to the diverse variability seen in patient images. In this
paper, we introduce a novel three-stage approach based on transformers coupled
with conditional diffusion models, with the goal of improving model robustness
to the kinds of imaging variability commonly encountered in practice without
the need for pre-determined data augmentation strategies. To this end, multiple
image encoders first learn hierarchical feature representations to build
discriminative latent spaces. Next, a reverse diffusion process, guided by the
latent code, acts on an informative prior and proposes prediction candidates in
a generative manner. Finally, several prediction candidates are aggregated in a
bi-level aggregation protocol to produce the final output. Through extensive
experiments on medical imaging benchmark datasets, we show that our method
improves upon state-of-the-art methods in terms of robustness and confidence
calibration. Additionally, we introduce a strategy to quantify the prediction
uncertainty at the instance level, increasing their trustworthiness to
clinicians using them in clinical practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18765">Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition. (arXiv:2310.18765v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1">Divin Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1">Gengchen Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengzhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zengfeng Huang</a></p>
<p>This paper introduces a new approach to address the issue of class imbalance
in graph neural networks (GNNs) for learning on graph-structured data. Our
approach integrates imbalanced node classification and Bias-Variance
Decomposition, establishing a theoretical framework that closely relates data
imbalance to model variance. We also leverage graph augmentation technique to
estimate the variance, and design a regularization term to alleviate the impact
of imbalance. Exhaustive tests are conducted on multiple benchmarks, including
naturally imbalanced datasets and public-split class-imbalanced datasets,
demonstrating that our approach outperforms state-of-the-art methods in various
imbalanced scenarios. This work provides a novel theoretical perspective for
addressing the problem of imbalanced node classification in GNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19025">An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits. (arXiv:2310.19025v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Banihashem_K/0/1/0/all/0/1">Kiarash Banihashem</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Suho Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Springer_M/0/1/0/all/0/1">Max Springer</a></p>
<p>We present an oracle-efficient relaxation for the adversarial contextual
bandits problem, where the contexts are sequentially drawn i.i.d from a known
distribution and the cost sequence is chosen by an online adversary. Our
algorithm has a regret bound of
$O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$ and makes at most $O(K)$ calls
per round to an offline optimization oracle, where $K$ denotes the number of
actions, $T$ denotes the number of rounds and $\Pi$ denotes the set of
policies. This is the first result to improve the prior best bound of
$O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$ as obtained by Syrgkanis et
al. at NeurIPS 2016, and the first to match the original bound of Langford and
Zhang at NeurIPS 2007 which was obtained for the stochastic case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19805">Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1">Xiao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zifeng Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>A prospective application of offline reinforcement learning (RL) involves
initializing a pre-trained policy using existing static datasets for subsequent
online fine-tuning. However, direct fine-tuning of the offline pre-trained
policy often results in sub-optimal performance. A primary reason is that
offline conservative methods diminish the agent's capability of exploration,
thereby impacting online fine-tuning performance. To enhance exploration during
online fine-tuning and thus enhance the overall online fine-tuning performance,
we introduce a generalized reward augmentation framework called Sample
Efficient Reward Augmentation (SERA). SERA aims to improve the performance of
online fine-tuning by designing intrinsic rewards that encourage the agent to
explore. Specifically, it implicitly implements State Marginal Matching (SMM)
and penalizes out-of-distribution (OOD) state actions, thus encouraging agents
to cover the target state density, and achieving better online fine-tuning
results. Additionally, SERA can be effortlessly plugged into various RL
algorithms to improve online fine-tuning and ensure sustained asymptotic
improvement, showing the versatility as well as the effectiveness of SERA.
Moreover, extensive experimental results will demonstrate that when conducting
offline-to-online problems, SERA consistently and effectively enhances the
performance of various offline algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00157">Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1">Guoxuan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Danier_D/0/1/0/all/0/1">Duolikun Danier</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Ayan Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Fotiadis_S/0/1/0/all/0/1">Stathi Fotiadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabiei_F/0/1/0/all/0/1">Farhang Nabiei</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_U/0/1/0/all/0/1">Ushnish Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1">Alberto Bernacchia</a></p>
<p>Recently, Zhang et al. have proposed the Diffusion Exponential Integrator
Sampler (DEIS) for fast generation of samples from Diffusion Models. It
leverages the semi-linear nature of the probability flow ordinary differential
equation (ODE) in order to greatly reduce integration error and improve
generation quality at low numbers of function evaluations (NFEs). Key to this
approach is the score function reparameterisation, which reduces the
integration error incurred from using a fixed score function estimate over each
integration step. The original authors use the default parameterisation used by
models trained for noise prediction -- multiply the score by the standard
deviation of the conditional forward noising distribution. We find that
although the mean absolute value of this score parameterisation is close to
constant for a large portion of the reverse sampling process, it changes
rapidly at the end of sampling. As a simple fix, we propose to instead
reparameterise the score (at inference) by dividing it by the average absolute
value of previous score estimates at that time step collected from offline high
NFE generations. We find that our score normalisation (DEIS-SN) consistently
improves FID compared to vanilla DEIS, showing an improvement at 10 NFEs from
6.44 to 5.57 on CIFAR-10 and from 5.9 to 4.95 on LSUN-Church 64x64. Our code is
available at https://github.com/mtkresearch/Diffusion-DEIS-SN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01282">FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1">Ke Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1">Guohao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1">Qiuli Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiuhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kangdi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuhan Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a></p>
<p>As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and &gt;50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
</p>
<p>We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03421">Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding. (arXiv:2311.03421v3 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Marin_Llobet_A/0/1/0/all/0/1">Arnau Marin-Llobet</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Manasanch_A/0/1/0/all/0/1">Arnau Manasanch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sanchez_Vives_M/0/1/0/all/0/1">Maria V. Sanchez-Vives</a></p>
<p>The study of brain states, ranging from highly synchronous to asynchronous
neuronal patterns like the sleep-wake cycle, is fundamental for assessing the
brain's spatiotemporal dynamics and their close connection to behavior.
However, the development of new techniques to accurately identify them still
remains a challenge, as these are often compromised by the presence of noise,
artifacts, and suboptimal recording quality. In this study, we propose a
two-stage computational framework combining Hopfield Networks for artifact data
preprocessing with Convolutional Neural Networks (CNNs) for classification of
brain states in rat neural recordings under different levels of anesthesia. To
evaluate the robustness of our framework, we deliberately introduced noise
artifacts into the neural recordings. We evaluated our hybrid Hopfield-CNN
pipeline by benchmarking it against two comparative models: a standalone CNN
handling the same noisy inputs, and another CNN trained and tested on
artifact-free data. Performance across various levels of data compression and
noise intensities showed that our framework can effectively mitigate artifacts,
allowing the model to reach parity with the clean-data CNN at lower noise
levels. Although this study mainly benefits small-scale experiments, the
findings highlight the necessity for advanced deep learning and Hopfield
Network models to improve scalability and robustness in diverse real-world
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03698">A Novel Variational Lower Bound for Inverse Reinforcement Learning. (arXiv:2311.03698v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gui_Y/0/1/0/all/0/1">Yikang Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1">Prashant Doshi</a></p>
<p>Inverse reinforcement learning (IRL) seeks to learn the reward function from
expert trajectories, to understand the task for imitation or collaboration
thereby removing the need for manual reward engineering. However, IRL in the
context of large, high-dimensional problems with unknown dynamics has been
particularly challenging. In this paper, we present a new Variational Lower
Bound for IRL (VLB-IRL), which is derived under the framework of a
probabilistic graphical model with an optimality node. Our method
simultaneously learns the reward function and policy under the learned reward
function by maximizing the lower bound, which is equivalent to minimizing the
reverse Kullback-Leibler divergence between an approximated distribution of
optimality given the reward function and the true distribution of optimality
given trajectories. This leads to a new IRL method that learns a valid reward
function such that the policy under the learned reward achieves expert-level
performance on several known domains. Importantly, the method outperforms the
existing state-of-the-art IRL algorithms on these domains by demonstrating
better reward from the learned policy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04338">Convex Methods for Constrained Linear Bandits. (arXiv:2311.04338v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Afsharrad_A/0/1/0/all/0/1">Amirhossein Afsharrad</a>, <a href="http://arxiv.org/find/cs/1/au:+Moradipari_A/0/1/0/all/0/1">Ahmadreza Moradipari</a>, <a href="http://arxiv.org/find/cs/1/au:+Lall_S/0/1/0/all/0/1">Sanjay Lall</a></p>
<p>Recently, bandit optimization has received significant attention in
real-world safety-critical systems that involve repeated interactions with
humans. While there exist various algorithms with performance guarantees in the
literature, practical implementation of the algorithms has not received as much
attention. This work presents a comprehensive study on the computational
aspects of safe bandit algorithms, specifically safe linear bandits, by
introducing a framework that leverages convex programming tools to create
computationally efficient policies. In particular, we first characterize the
properties of the optimal policy for safe linear bandit problem and then
propose an end-to-end pipeline of safe linear bandit algorithms that only
involves solving convex problems. We also numerically evaluate the performance
of our proposed methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04522">Long-term Time Series Forecasting based on Decomposition and Neural Ordinary Differential Equations. (arXiv:2311.04522v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Seonkyu Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jaehyeon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seojin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Wi_H/0/1/0/all/0/1">Hyowon Wi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1">Haksoo Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1">Jinsung Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jeongwhan Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1">Noseong Park</a></p>
<p>Long-term time series forecasting (LTSF) is a challenging task that has been
investigated in various domains such as finance investment, health care,
traffic, and weather forecasting. In recent years, Linear-based LTSF models
showed better performance, pointing out the problem of Transformer-based
approaches causing temporal information loss. However, Linear-based approach
has also limitations that the model is too simple to comprehensively exploit
the characteristics of the dataset. To solve these limitations, we propose
LTSF-DNODE, which applies a model based on linear ordinary differential
equations (ODEs) and a time series decomposition method according to data
statistical characteristics. We show that LTSF-DNODE outperforms the baselines
on various real-world datasets. In addition, for each dataset, we explore the
impacts of regularization in the neural ordinary differential equation (NODE)
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04698">Challenging Common Assumptions in Multi-task Learning. (arXiv:2311.04698v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elich_C/0/1/0/all/0/1">Cathrin Elich</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirchdorfer_L/0/1/0/all/0/1">Lukas Kirchdorfer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1">Jan M. K&#xf6;hler</a>, <a href="http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1">Lukas Schott</a></p>
<p>While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge common assumptions in
MTL in the context of STL: First, the choice of optimizer has only been mildly
investigated in MTL. We show the pivotal role of common STL tools such as the
Adam optimizer in MTL. We deduce the effectiveness of Adam to its partial
loss-scale invariance. Second, the notion of gradient conflicts has often been
phrased as a specific problem in MTL. We delve into the role of gradient
conflicts in MTL and compare it to STL. For angular gradient alignment we find
no evidence that this is a unique problem in MTL. We emphasize differences in
gradient magnitude as the main distinguishing factor. Lastly, we compare the
transferability of features learned through MTL and STL on common image
corruptions, and find no conclusive evidence that MTL leads to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</p>
</p>
</div>

    </div>
    </body>
    