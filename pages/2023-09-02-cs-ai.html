<!DOCTYPE html>
<html>
<head>
<title>2023-09-02-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.16198">Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galliera_R/0/1/0/all/0/1">Raffaele Galliera</a>, <a href="http://arxiv.org/find/cs/1/au:+Venable_K/0/1/0/all/0/1">Kristen Brent Venable</a>, <a href="http://arxiv.org/find/cs/1/au:+Bassani_M/0/1/0/all/0/1">Matteo Bassani</a>, <a href="http://arxiv.org/find/cs/1/au:+Suri_N/0/1/0/all/0/1">Niranjan Suri</a></p>
<p>In modern communication systems, efficient and reliable information
dissemination is crucial for supporting critical operations across domains like
disaster response, autonomous vehicles, and sensor networks. This paper
introduces a Multi-Agent Reinforcement Learning (MARL) approach as a
significant step forward in achieving more decentralized, efficient, and
collaborative solutions. We propose a Decentralized-POMDP formulation for
information dissemination, empowering each agent to independently decide on
message forwarding. This constitutes a significant paradigm shift from
traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach
harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention
Networks (GAT) with dynamic attention to capture essential network features. We
propose two approaches, L-DGN and HL-DGN, which differ in the information that
is exchanged among agents. We evaluate the performance of our decentralized
approaches, by comparing them with a widely-used MPR heuristic, and we show
that our trained policies are able to efficiently cover the network while
bypassing the MPR set selection process. Our approach promises a first step
toward bolstering the resilience of real-world broadcast communication
infrastructures via learned, collaborative information dissemination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16245">Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lofstrom_T/0/1/0/all/0/1">Tuwe L&#xf6;fstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Lofstrom_H/0/1/0/all/0/1">Helena L&#xf6;fstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_U/0/1/0/all/0/1">Ulf Johansson</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonstrod_C/0/1/0/all/0/1">Cecilia S&#xf6;nstr&#xf6;d</a></p>
<p>Artificial Intelligence (AI) is often an integral part of modern decision
support systems (DSSs). The best-performing predictive models used in AI-based
DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to
create AI systems that can explain their rationale to human users. Local
explanations in XAI can provide information about the causes of individual
predictions in terms of feature importance. However, a critical drawback of
existing local explanation methods is their inability to quantify the
uncertainty associated with a feature's importance. This paper introduces an
extension of a feature importance explanation method, Calibrated Explanations
(CE), previously only supporting classification, with support for standard
regression and probabilistic regression, i.e., the probability that the target
is above an arbitrary threshold. The extension for regression keeps all the
benefits of CE, such as calibration of the prediction from the underlying model
with confidence intervals, uncertainty quantification of feature importance,
and allows both factual and counterfactual explanations. CE for standard
regression provides fast, reliable, stable, and robust explanations. CE for
probabilistic regression provides an entirely new way of creating probabilistic
explanations from any ordinary regression model and with a dynamic selection of
thresholds. The performance of CE for probabilistic regression regarding
stability and speed is comparable to LIME. The method is model agnostic with
easily understood conditional rules. An implementation in Python is freely
available on GitHub and for installation using pip making the results in this
paper easily replicable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16262">Causal Strategic Learning with Competitive Selection. (arXiv:2308.16262v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1">Kiet Q. H. Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Aadil_M/0/1/0/all/0/1">Muneeb Aadil</a>, <a href="http://arxiv.org/find/cs/1/au:+Chau_S/0/1/0/all/0/1">Siu Lun Chau</a>, <a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1">Krikamol Muandet</a></p>
<p>We study the problem of agent selection in causal strategic learning under
multiple decision makers and address two key challenges that come with it.
Firstly, while much of prior work focuses on studying a fixed pool of agents
that remains static regardless of their evaluations, we consider the impact of
selection procedure by which agents are not only evaluated, but also selected.
When each decision maker unilaterally selects agents by maximising their own
utility, we show that the optimal selection rule is a trade-off between
selecting the best agents and providing incentives to maximise the agents'
improvement. Furthermore, this optimal selection rule relies on incorrect
predictions of agents' outcomes. Hence, we study the conditions under which a
decision maker's optimal selection rule will not lead to deterioration of
agents' outcome nor cause unjust reduction in agents' selection chance. To that
end, we provide an analytical form of the optimal selection rule and a
mechanism to retrieve the causal parameters from observational data, under
certain assumptions on agents' behaviour. Secondly, when there are multiple
decision makers, the interference between selection rules introduces another
source of biases in estimating the underlying causal parameters. To address
this problem, we provide a cooperative protocol which all decision makers must
collectively adopt to recover the true causal parameters. Lastly, we complement
our theoretical results with simulation studies. Our results highlight not only
the importance of causal modeling as a strategy to mitigate the effect of
gaming, as suggested by previous work, but also the need of a benevolent
regulator to enable it.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16328">Debunking Disinformation: Revolutionizing Truth with NLP in Fake News Detection. (arXiv:2308.16328v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Li He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Siyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_A/0/1/0/all/0/1">Ailun Pei</a></p>
<p>The Internet and social media have altered how individuals access news in the
age of instantaneous information distribution. While this development has
increased access to information, it has also created a significant problem: the
spread of fake news and information. Fake news is rapidly spreading on digital
platforms, which has a negative impact on the media ecosystem, public opinion,
decision-making, and social cohesion. Natural Language Processing(NLP), which
offers a variety of approaches to identify content as authentic, has emerged as
a potent weapon in the growing war against disinformation. This paper takes an
in-depth look at how NLP technology can be used to detect fake news and reveals
the challenges and opportunities it presents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16361">Large Language Models as Data Preprocessors. (arXiv:2308.16361v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haochen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuyang Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chuan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Oyamada_M/0/1/0/all/0/1">Masafumi Oyamada</a></p>
<p>Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's
LLaMA variants, have marked a significant advancement in artificial
intelligence. Trained on vast amounts of text data, LLMs are capable of
understanding and generating human-like text across a diverse range of topics.
This study expands on the applications of LLMs, exploring their potential in
data preprocessing, a critical stage in data mining and analytics applications.
We delve into the applicability of state-of-the-art LLMs such as GPT-3.5,
GPT-4, and Vicuna-13B for error detection, data imputation, schema matching,
and entity matching tasks. Alongside showcasing the inherent capabilities of
LLMs, we highlight their limitations, particularly in terms of computational
expense and inefficiency. We propose an LLM-based framework for data
preprocessing, which integrates cutting-edge prompt engineering techniques,
coupled with traditional methods like contextualization and feature selection,
to improve the performance and efficiency of these models. The effectiveness of
LLMs in data preprocessing is evaluated through an experimental study spanning
12 datasets. GPT-4 emerged as a standout, achieving 100\% accuracy or F1 score
on 4 datasets, suggesting LLMs' immense potential in these tasks. Despite
certain limitations, our study underscores the promise of LLMs in this domain
and anticipates future developments to overcome current hurdles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16364">Strengthening the EU AI Act: Defining Key Terms on AI Manipulation. (arXiv:2308.16364v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franklin_M/0/1/0/all/0/1">Matija Franklin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomei_P/0/1/0/all/0/1">Philip Moreira Tomei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorman_R/0/1/0/all/0/1">Rebecca Gorman</a></p>
<p>The European Union's Artificial Intelligence Act aims to regulate
manipulative and harmful uses of AI, but lacks precise definitions for key
concepts. This paper provides technical recommendations to improve the Act's
conceptual clarity and enforceability. We review psychological models to define
"personality traits," arguing the Act should protect full "psychometric
profiles." We urge expanding "behavior" to include "preferences" since
preferences causally influence and are influenced by behavior. Clear
definitions are provided for "subliminal," "manipulative," and "deceptive"
techniques, considering incentives, intent, and covertness. We distinguish
"exploiting individuals" from "exploiting groups," emphasising different policy
needs. An "informed decision" is defined by four facets: comprehension,
accurate information, no manipulation, and understanding AI's influence. We
caution the Act's therapeutic use exemption given the lack of regulation of
digital therapeutics by the EMA. Overall, the recommendations strengthen
definitions of vague concepts in the EU AI Act, enhancing precise applicability
to regulate harmful AI manipulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16375">A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuying Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhaoqing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotevska_O/0/1/0/all/0/1">Olivera Kotevska</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1">Tyler Derr</a></p>
<p>Graph Neural Networks (GNNs) have gained significant attention owing to their
ability to handle graph-structured data and the improvement in practical
applications. However, many of these models prioritize high utility
performance, such as accuracy, with a lack of privacy consideration, which is a
major concern in modern society where privacy attacks are rampant. To address
this issue, researchers have started to develop privacy-preserving GNNs.
Despite this progress, there is a lack of a comprehensive overview of the
attacks and the techniques for preserving privacy in the graph domain. In this
survey, we aim to address this gap by summarizing the attacks on graph data
according to the targeted information, categorizing the privacy preservation
techniques in GNNs, and reviewing the datasets and applications that could be
used for analyzing/solving privacy issues in GNNs. We also outline potential
directions for future research in order to build better privacy-preserving
GNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16385">BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks. (arXiv:2308.16385v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jiawei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_X/0/1/0/all/0/1">Xi Susie Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhichao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zitao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yongjun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Quanqing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1">Chuang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_S/0/1/0/all/0/1">Shuo Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1">Bo Du</a></p>
<p>To handle graphs in which features or connectivities are evolving over time,
a series of temporal graph neural networks (TGNNs) have been proposed. Despite
the success of these TGNNs, the previous TGNN evaluations reveal several
limitations regarding four critical issues: 1) inconsistent datasets, 2)
inconsistent evaluation pipelines, 3) lacking workload diversity, and 4)
lacking efficient comparison. Overall, there lacks an empirical study that puts
TGNN models onto the same ground and compares them comprehensively. To this
end, we propose BenchTemp, a general benchmark for evaluating TGNN models on
various workloads. BenchTemp provides a set of benchmark datasets so that
different TGNN models can be fairly compared. Further, BenchTemp engineers a
standard pipeline that unifies the TGNN evaluation. With BenchTemp, we
extensively compare the representative TGNN models on different tasks (e.g.,
link prediction and node classification) and settings (transductive and
inductive), w.r.t. both effectiveness and efficiency metrics. We have made
BenchTemp publicly available at https://github.com/qianghuangwhu/benchtemp.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16441">Contrastive Representation Learning Based on Multiple Node-centered Subgraphs. (arXiv:2308.16441v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenjun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1">Minglai Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a></p>
<p>As the basic element of graph-structured data, node has been recognized as
the main object of study in graph representation learning. A single node
intuitively has multiple node-centered subgraphs from the whole graph (e.g.,
one person in a social network has multiple social circles based on his
different relationships). We study this intuition under the framework of graph
contrastive learning, and propose a multiple node-centered subgraphs
contrastive representation learning method to learn node representation on
graphs in a self-supervised way. Specifically, we carefully design a series of
node-centered regional subgraphs of the central node. Then, the mutual
information between different subgraphs of the same node is maximized by
contrastive loss. Experiments on various real-world datasets and different
downstream tasks demonstrate that our model has achieved state-of-the-art
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16458">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiangru Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1">Bill Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Rick Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiakang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1">Mark Gerstein</a></p>
<p>Pre-trained language models like ChatGPT have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks. Moreover, in bioinformatics, generating
functional programs poses additional notable challenges due to the amount of
domain knowledge, the need for complicated data operations, and intricate
functional dependencies between the operations. Here, we present BioCoder, a
benchmark developed to evaluate existing pre-trained models in generating
bioinformatics code. In relation to function-code generation, BioCoder covers
potential package dependencies, class declarations, and global variables. It
incorporates 1026 functions and 1243 methods in Python and Java from GitHub and
253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing
framework for evaluation, and we have applied it to evaluate many models
including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+,
InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizes
the importance of domain knowledge, pragmatic code generation, and contextual
understanding. Our dataset, benchmark, Docker images, and scripts required for
testing are all available at https://github.com/gersteinlab/biocoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16464">MaintainoMATE: A GitHub App for Intelligent Automation of Maintenance Activities. (arXiv:2308.16464v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nadeem_A/0/1/0/all/0/1">Anas Nadeem</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarwar_M/0/1/0/all/0/1">Muhammad Usman Sarwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_M/0/1/0/all/0/1">Muhammad Zubair Malik</a></p>
<p>Software development projects rely on issue tracking systems at the core of
tracking maintenance tasks such as bug reports, and enhancement requests.
Incoming issue-reports on these issue tracking systems must be managed in an
effective manner. First, they must be labelled and then assigned to a
particular developer with relevant expertise. This handling of issue-reports is
critical and requires thorough scanning of the text entered in an issue-report
making it a labor-intensive task. In this paper, we present a unified framework
called MaintainoMATE, which is capable of automatically categorizing the
issue-reports in their respective category and further assigning the
issue-reports to a developer with relevant expertise. We use the Bidirectional
Encoder Representations from Transformers (BERT), as an underlying model for
MaintainoMATE to learn the contextual information for automatic issue-report
labeling and assignment tasks. We deploy the framework used in this work as a
GitHub application. We empirically evaluate our approach on GitHub
issue-reports to show its capability of assigning labels to the issue-reports.
We were able to achieve an F1-score close to 80\%, which is comparable to
existing state-of-the-art results. Similarly, our initial evaluations show that
we can assign relevant developers to the issue-reports with an F1 score of
54\%, which is a significant improvement over existing approaches. Our initial
findings suggest that MaintainoMATE has the potential of improving software
quality and reducing maintenance costs by accurately automating activities
involved in the maintenance processes. Our future work would be directed
towards improving the issue-assignment module.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16474">Enhancing Subtask Performance of Multi-modal Large Language Model. (arXiv:2308.16474v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yongqiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Feng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xinhai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Donghong Liu</a></p>
<p>Multi-modal Large Language Model (MLLM) refers to a model expanded from a
Large Language Model (LLM) that possesses the capability to handle and infer
multi-modal data. Current MLLMs typically begin by using LLMs to decompose
tasks into multiple subtasks, then employing individual pre-trained models to
complete specific subtasks, and ultimately utilizing LLMs to integrate the
results of each subtasks to obtain the results of the task. In real-world
scenarios, when dealing with large projects, it is common practice to break
down the project into smaller sub-projects, with different teams providing
corresponding solutions or results. The project owner then decides which
solution or result to use, ensuring the best possible outcome for each subtask
and, consequently, for the entire project. Inspired by this, this study
considers selecting multiple pre-trained models to complete the same subtask.
By combining the results from multiple pre-trained models, the optimal subtask
result is obtained, enhancing the performance of the MLLM. Specifically, this
study first selects multiple pre-trained models focused on the same subtask
based on distinct evaluation approaches, and then invokes these models in
parallel to process input data and generate corresponding subtask results.
Finally, the results from multiple pre-trained models for the same subtask are
compared using the LLM, and the best result is chosen as the outcome for that
subtask. Extensive experiments are conducted in this study using GPT-4
annotated datasets and human-annotated datasets. The results of various
evaluation metrics adequately demonstrate the effectiveness of the proposed
approach in this paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16475">Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yuxuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Cuiping Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a></p>
<p>We propose TCSP, a novel method for compressing a transformer model by
focusing on reducing the hidden size of the model. By projecting the whole
transform model into a subspace, we enable matrix operations between the weight
matrices in the model and features in a reduced-dimensional space, leading to
significant reductions in model parameters and computing resources. To
establish this subspace, we decompose the feature matrix, derived from
different layers of sampled data instances, into a projection matrix. For
evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and
SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a
compression ratio of 44\% with at most 1.6\% degradation in accuracy,
surpassing or matching prior compression methods. Furthermore, TCSP exhibits
compatibility with other methods targeting filter and attention head size
compression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16481">Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hatem_A/0/1/0/all/0/1">Ahmed Hatem</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yiming Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a></p>
<p>We present Point-TTA, a novel test-time adaptation framework for point cloud
registration (PCR) that improves the generalization and the performance of
registration models. While learning-based approaches have achieved impressive
progress, generalization to unknown testing environments remains a major
challenge due to the variations in 3D scans. Existing methods typically train a
generic model and the same trained model is applied on each instance during
testing. This could be sub-optimal since it is difficult for the same model to
handle all the variations during testing. In this paper, we propose a test-time
adaptation approach for PCR. Our model can adapt to unseen distributions at
test-time without requiring any prior knowledge of the test data. Concretely,
we design three self-supervised auxiliary tasks that are optimized jointly with
the primary PCR task. Given a test instance, we adapt our model using these
auxiliary tasks and the updated model is used to perform the inference. During
training, our model is trained using a meta-auxiliary learning approach, such
that the adapted model via auxiliary tasks improves the accuracy of the primary
task. Experimental results demonstrate the effectiveness of our approach in
improving generalization of point cloud registration and outperforming other
state-of-the-art approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16484">Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hatem_A/0/1/0/all/0/1">Ahmed Hatem</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yiming Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a></p>
<p>Affordable 3D scanners often produce sparse and non-uniform point clouds that
negatively impact downstream applications in robotic systems. While existing
point cloud upsampling architectures have demonstrated promising results on
standard benchmarks, they tend to experience significant performance drops when
the test data have different distributions from the training data. To address
this issue, this paper proposes a test-time adaption approach to enhance model
generality of point cloud upsampling. The proposed approach leverages
meta-learning to explicitly learn network parameters for test-time adaption.
Our method does not require any prior information about the test data. During
meta-training, the model parameters are learned from a collection of
instance-level tasks, each of which consists of a sparse-dense pair of point
clouds from the training data. During meta-testing, the trained model is
fine-tuned with a few gradient updates to produce a unique set of network
parameters for each test instance. The updated model is then used for the final
prediction. Our framework is generic and can be applied in a plug-and-play
manner with existing backbone networks in point cloud upsampling. Extensive
experiments demonstrate that our approach improves the performance of
state-of-the-art models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16490">Latent Painter. (arXiv:2308.16490v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Shih-Chieh Su</a></p>
<p>Latent diffusers revolutionized the generative AI and inspired creative art.
When denoising the latent, the predicted original image at each step
collectively animates the formation. However, the animation is limited by the
denoising nature of the diffuser, and only renders a sharpening process. This
work presents Latent Painter, which uses the latent as the canvas, and the
diffuser predictions as the plan, to generate painting animation. Latent
Painter also transits one generated image to another, which can happen between
images from two different sets of checkpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16491">In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1">Kristina Gligoric</a>, <a href="http://arxiv.org/find/cs/1/au:+Piccardi_T/0/1/0/all/0/1">Tiziano Piccardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofman_J/0/1/0/all/0/1">Jake Hofman</a>, <a href="http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1">Robert West</a></p>
<p>Science is facing a reproducibility crisis. Previous work has proposed
incorporating data analysis replications into classrooms as a potential
solution. However, despite the potential benefits, it is unclear whether this
approach is feasible, and if so, what the involved stakeholders-students,
educators, and scientists-should expect from it. Can students perform a data
analysis replication over the course of a class? What are the costs and
benefits for educators? And how can this solution help benchmark and improve
the state of science?
</p>
<p>In the present study, we incorporated data analysis replications in the
project component of the Applied Data Analysis course (CS-401) taught at EPFL
(N=354 students). Here we report pre-registered findings based on surveys
administered throughout the course. First, we demonstrate that students can
replicate previously published scientific papers, most of them qualitatively
and some exactly. We find discrepancies between what students expect of data
analysis replications and what they experience by doing them along with changes
in expectations about reproducibility, which together serve as evidence of
attitude shifts to foster students' critical thinking. Second, we provide
information for educators about how much overhead is needed to incorporate
replications into the classroom and identify concerns that replications bring
as compared to more traditional assignments. Third, we identify tangible
benefits of the in-class data analysis replications for scientific communities,
such as a collection of replication reports and insights about replication
barriers in scientific work that should be avoided going forward.
</p>
<p>Overall, we demonstrate that incorporating replication tasks into a large
data science class can increase the reproducibility of scientific work as a
by-product of data science instruction, thus benefiting both science and
students.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16493">Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception. (arXiv:2308.16493v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tavassoli_R/0/1/0/all/0/1">Riley Tavassoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Amani_M/0/1/0/all/0/1">Mani Amani</a>, <a href="http://arxiv.org/find/cs/1/au:+Akhavian_R/0/1/0/all/0/1">Reza Akhavian</a></p>
<p>Vision-language models (VLMs) have shown powerful capabilities in visual
question answering and reasoning tasks by combining visual representations with
the abstract skill set large language models (LLMs) learn during pretraining.
Vision, while the most popular modality to augment LLMs with, is only one
representation of a scene. In human-robot interaction scenarios, robot
perception requires accurate scene understanding by the robot. In this paper,
we define and demonstrate a method of aligning the embedding spaces of
different modalities (in this case, inertial measurement unit (IMU) data) to
the vision embedding space through a combination of supervised and contrastive
training, enabling the VLM to understand and reason about these additional
modalities without retraining. We opt to give the model IMU embeddings directly
over using a separate human activity recognition model that feeds directly into
the prompt to allow for any nonlinear interactions between the query, image,
and IMU signal that would be lost by mapping the IMU data to a discrete
activity label. Further, we demonstrate our methodology's efficacy through
experiments involving human activity recognition using IMU data and visual
inputs. Our results show that using multiple modalities as input improves the
VLM's scene understanding and enhances its overall performance in various
tasks, thus paving the way for more versatile and capable language models in
multi-modal contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16498">Generalised Winograd Schema and its Contextuality. (arXiv:2308.16498v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1">Kin Ian Lo</a> (University College London, London, UK), <a href="http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1">Mehrnoosh Sadrzadeh</a> (University College London, London, UK), <a href="http://arxiv.org/find/cs/1/au:+Mansfield_S/0/1/0/all/0/1">Shane Mansfield</a> (Quandela, Paris, France)</p>
<p>Ambiguities in natural language give rise to probability distributions over
interpretations. The distributions are often over multiple ambiguous words at a
time; a multiplicity which makes them a suitable topic for sheaf-theoretic
models of quantum contextuality. Previous research showed that different
quantitative measures of contextuality correlate well with Psycholinguistic
research on lexical ambiguities. In this work, we focus on coreference
ambiguities and investigate the Winograd Schema Challenge (WSC), a test
proposed by Levesque in 2011 to evaluate the intelligence of machines. The WSC
consists of a collection of multiple-choice questions that require
disambiguating pronouns in sentences structured according to the Winograd
schema, in a way that makes it difficult for machines to determine the correct
referents but remains intuitive for human comprehension. In this study, we
propose an approach that analogously models the Winograd schema as an
experiment in quantum physics. However, we argue that the original Winograd
Schema is inherently too simplistic to facilitate contextuality. We introduce a
novel mechanism for generalising the schema, rendering it analogous to a
Bell-CHSH measurement scenario. We report an instance of this generalised
schema, complemented by the human judgements we gathered via a crowdsourcing
platform. The resulting model violates the Bell-CHSH inequality by 0.192, thus
exhibiting contextuality in a coreference resolution setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16501">Individually Rational Collaborative Vehicle Routing through Give-And-Take Exchanges. (arXiv:2308.16501v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1">Paul Mingzheng Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_B/0/1/0/all/0/1">Ba Phong Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_H/0/1/0/all/0/1">Hoong Chuin Lau</a></p>
<p>In this paper, we are concerned with the automated exchange of orders between
logistics companies in a marketplace platform to optimize total revenues. We
introduce a novel multi-agent approach to this problem, focusing on the
Collaborative Vehicle Routing Problem (CVRP) through the lens of individual
rationality. Our proposed algorithm applies the principles of Vehicle Routing
Problem (VRP) to pairs of vehicles from different logistics companies,
optimizing the overall routes while considering standard VRP constraints plus
individual rationality constraints. By facilitating cooperation among competing
logistics agents through a Give-and-Take approach, we show that it is possible
to reduce travel distance and increase operational efficiency system-wide. More
importantly, our approach ensures individual rationality and faster
convergence, which are important properties of ensuring the long-term
sustainability of the marketplace platform. We demonstrate the efficacy of our
approach through extensive experiments using real-world test data from major
logistics companies. The results reveal our algorithm's ability to rapidly
identify numerous optimal solutions, underscoring its practical applicability
and potential to transform the logistics industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16505">Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations. (arXiv:2308.16505v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1">Jianxun Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1">Yuxuan Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jing Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1">Defu Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Recommender models excel at providing domain-specific item recommendations by
leveraging extensive user behavior data. Despite their ability to act as
lightweight domain experts, they struggle to perform versatile tasks such as
providing explanations and engaging in conversations. On the other hand, large
language models (LLMs) represent a significant step towards artificial general
intelligence, showcasing remarkable capabilities in instruction comprehension,
commonsense reasoning, and human interaction. However, LLMs lack the knowledge
of domain-specific item catalogs and behavioral patterns, particularly in areas
that diverge from general world knowledge, such as online e-commerce.
Finetuning LLMs for each domain is neither economic nor efficient.
</p>
<p>In this paper, we bridge the gap between recommender models and LLMs,
combining their respective strengths to create a versatile and interactive
recommender system. We introduce an efficient framework called RecAgent, which
employs LLMs as the brain and recommender models as tools. We first outline a
minimal set of essential tools required to transform LLMs into RecAgent. We
then propose an efficient workflow within RecAgent for task execution,
incorporating key components such as a memory bus, dynamic
demonstration-augmented task planning, and reflection. RecAgent enables
traditional recommender systems, such as those ID-based matrix factorization
models, to become interactive systems with a natural language interface through
the integration of LLMs. Experimental results on several public datasets show
that RecAgent achieves satisfying performance as a conversational recommender
system, outperforming general-purpose LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16516">Curvature-based Pooling within Graph Neural Networks. (arXiv:2308.16516v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sanders_C/0/1/0/all/0/1">Cedric Sanders</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1">Andreas Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Liebig_T/0/1/0/all/0/1">Thomas Liebig</a></p>
<p>Over-squashing and over-smoothing are two critical issues, that limit the
capabilities of graph neural networks (GNNs). While over-smoothing eliminates
the differences between nodes making them indistinguishable, over-squashing
refers to the inability of GNNs to propagate information over long distances,
as exponentially many node states are squashed into fixed-size representations.
Both phenomena share similar causes, as both are largely induced by the graph
topology. To mitigate these problems in graph classification tasks, we propose
CurvPool, a novel pooling method. CurvPool exploits the notion of curvature of
a graph to adaptively identify structures responsible for both over-smoothing
and over-squashing. By clustering nodes based on the Balanced Forman curvature,
CurvPool constructs a graph with a more suitable structure, allowing deeper
models and the combination of distant information. We compare it to other
state-of-the-art pooling approaches and establish its competitiveness in terms
of classification accuracy, computational complexity, and flexibility. CurvPool
outperforms several comparable methods across all considered tasks. The most
consistent results are achieved by pooling densely connected clusters using the
sum aggregation, as this allows additional information about the size of each
pool.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16529">Developing Social Robots with Empathetic Non-Verbal Cues Using Large Language Models. (arXiv:2308.16529v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yoon Kyung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1">Yoonwon Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Gyuyi Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1">Sowon Hahn</a></p>
<p>We propose augmenting the empathetic capacities of social robots by
integrating non-verbal cues. Our primary contribution is the design and
labeling of four types of empathetic non-verbal cues, abbreviated as SAFE:
Speech, Action (gesture), Facial expression, and Emotion, in a social robot.
These cues are generated using a Large Language Model (LLM). We developed an
LLM-based conversational system for the robot and assessed its alignment with
social cues as defined by human counselors. Preliminary results show distinct
patterns in the robot's responses, such as a preference for calm and positive
social emotions like 'joy' and 'lively', and frequent nodding gestures. Despite
these tendencies, our approach has led to the development of a social robot
capable of context-aware and more authentic interactions. Our work lays the
groundwork for future studies on human-robot interactions, emphasizing the
essential role of both verbal and non-verbal cues in creating social and
empathetic robots.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16534">Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scassola_D/0/1/0/all/0/1">Davide Scassola</a>, <a href="http://arxiv.org/find/cs/1/au:+Saccani_S/0/1/0/all/0/1">Sebastiano Saccani</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1">Ginevra Carbone</a>, <a href="http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1">Luca Bortolussi</a></p>
<p>Score-based and diffusion models have emerged as effective approaches for
both conditional and unconditional generation. Still conditional generation is
based on either a specific training of a conditional model or classifier
guidance, which requires training a noise-dependent classifier, even when the
classifier for uncorrupted data is given. We propose an approach to sample from
unconditional score-based generative models enforcing arbitrary logical
constraints, without any additional training. Firstly, we show how to
manipulate the learned score in order to sample from an un-normalized
distribution conditional on a user-defined constraint. Then, we define a
flexible and numerically stable neuro-symbolic framework for encoding soft
logical constraints. Combining these two ingredients we obtain a general, but
approximate, conditional sampling algorithm. We further developed effective
heuristics aimed at improving the approximation. Finally, we show the
effectiveness of our approach for various types of constraints and data:
tabular data, images and time series.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16538">The AI Revolution: Opportunities and Challenges for the Finance Sector. (arXiv:2308.16538v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maple_C/0/1/0/all/0/1">Carsten Maple</a>, <a href="http://arxiv.org/find/cs/1/au:+Szpruch_L/0/1/0/all/0/1">Lukasz Szpruch</a>, <a href="http://arxiv.org/find/cs/1/au:+Epiphaniou_G/0/1/0/all/0/1">Gregory Epiphaniou</a>, <a href="http://arxiv.org/find/cs/1/au:+Staykova_K/0/1/0/all/0/1">Kalina Staykova</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Simran Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Penwarden_W/0/1/0/all/0/1">William Penwarden</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Yisi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zijian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hariharan_J/0/1/0/all/0/1">Jagdish Hariharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Avramovic_P/0/1/0/all/0/1">Pavle Avramovic</a></p>
<p>This report examines Artificial Intelligence (AI) in the financial sector,
outlining its potential to revolutionise the industry and identify its
challenges. It underscores the criticality of a well-rounded understanding of
AI, its capabilities, and its implications to effectively leverage its
potential while mitigating associated risks. The potential of AI potential
extends from augmenting existing operations to paving the way for novel
applications in the finance sector. The application of AI in the financial
sector is transforming the industry. Its use spans areas from customer service
enhancements, fraud detection, and risk management to credit assessments and
high-frequency trading. However, along with these benefits, AI also presents
several challenges. These include issues related to transparency,
interpretability, fairness, accountability, and trustworthiness. The use of AI
in the financial sector further raises critical questions about data privacy
and security. A further issue identified in this report is the systemic risk
that AI can introduce to the financial sector. Being prone to errors, AI can
exacerbate existing systemic risks, potentially leading to financial crises.
Regulation is crucial to harnessing the benefits of AI while mitigating its
potential risks. Despite the global recognition of this need, there remains a
lack of clear guidelines or legislation for AI use in finance. This report
discusses key principles that could guide the formation of effective AI
regulation in the financial sector, including the need for a risk-based
approach, the inclusion of ethical considerations, and the importance of
maintaining a balance between innovation and consumer protection. The report
provides recommendations for academia, the finance industry, and regulators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16539">On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diehl_C/0/1/0/all/0/1">Christopher Diehl</a>, <a href="http://arxiv.org/find/cs/1/au:+Klosek_T/0/1/0/all/0/1">Tobias Klosek</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruger_M/0/1/0/all/0/1">Martin Kr&#xfc;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Murzyn_N/0/1/0/all/0/1">Nils Murzyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertram_T/0/1/0/all/0/1">Torsten Bertram</a></p>
<p>Game theory offers an interpretable mathematical framework for modeling
multi-agent interactions. However, its applicability in real-world robotics
applications is hindered by several challenges, such as unknown agents'
preferences and goals. To address these challenges, we show a connection
between differential games, optimal control, and energy-based models and
demonstrate how existing approaches can be unified under our proposed
Energy-based Potential Game formulation. Building upon this formulation, this
work introduces a new end-to-end learning application that combines neural
networks for game-parameter inference with a differentiable game-theoretic
optimization layer, acting as an inductive bias. The experiments using
simulated mobile robot pedestrian interactions and real-world automated driving
data provide empirical evidence that the game-theoretic layer improves the
predictive performance of various neural network backbones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16562">The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning. (arXiv:2308.16562v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rigaki_M/0/1/0/all/0/1">Maria Rigaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1">Sebastian Garcia</a></p>
<p>Due to the proliferation of malware, defenders are increasingly turning to
automation and machine learning as part of the malware detection tool-chain.
However, machine learning models are susceptible to adversarial attacks,
requiring the testing of model and product robustness. Meanwhile, attackers
also seek to automate malware generation and evasion of antivirus systems, and
defenders try to gain insight into their methods. This work proposes a new
algorithm that combines Malware Evasion and Model Extraction (MEME) attacks.
MEME uses model-based reinforcement learning to adversarially modify Windows
executable binary samples while simultaneously training a surrogate model with
a high agreement with the target model to evade. To evaluate this method, we
compare it with two state-of-the-art attacks in adversarial malware creation,
using three well-known published models and one antivirus product as targets.
Results show that MEME outperforms the state-of-the-art methods in terms of
evasion capabilities in almost all cases, producing evasive malware with an
evasion rate in the range of 32-73%. It also produces surrogate models with a
prediction label agreement with the respective target models between 97-99%.
The surrogate could be used to fine-tune and improve the evasion rate in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16572">CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1">Neelu Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1">Nicolae-Catalin Ristea</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1">Kamal Nasrollahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1">Thomas B. Moeslund</a>, <a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1">Radu Tudor Ionescu</a></p>
<p>Masked image modeling has been demonstrated as a powerful pretext task for
generating robust representations that can be effectively generalized across
multiple downstream tasks. Typically, this approach involves randomly masking
patches (tokens) in input images, with the masking strategy remaining unchanged
during training. In this paper, we propose a curriculum learning approach that
updates the masking strategy to continually increase the complexity of the
self-supervised reconstruction task. We conjecture that, by gradually
increasing the task complexity, the model can learn more sophisticated and
transferable representations. To facilitate this, we introduce a novel
learnable masking module that possesses the capability to generate masks of
different complexities, and integrate the proposed module into masked
autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting
its behavior during training, transitioning from a partner to the MAE
(optimizing the same reconstruction loss) to an adversary (optimizing the
opposite loss), while passing through a neutral state. The transition between
these behaviors is smooth, being regulated by a factor that is multiplied with
the reconstruction loss of the masking module. The resulting training procedure
generates an easy-to-hard curriculum. We train our Curriculum-Learned Masked
Autoencoder (CL-MAE) on ImageNet and show that it exhibits superior
representation learning capabilities compared to MAE. The empirical results on
five downstream tasks confirm our conjecture, demonstrating that curriculum
learning can be successfully used to self-supervise masked autoencoders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16596">The Quest of Finding the Antidote to Sparse Double Descent. (arXiv:2308.16596v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quetu_V/0/1/0/all/0/1">Victor Qu&#xe9;tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Milovanovic_M/0/1/0/all/0/1">Marta Milovanovi&#x107;</a></p>
<p>In energy-efficient schemes, finding the optimal size of deep learning models
is very important and has a broad impact. Meanwhile, recent studies have
reported an unexpected phenomenon, the sparse double descent: as the model's
sparsity increases, the performance first worsens, then improves, and finally
deteriorates. Such a non-monotonic behavior raises serious questions about the
optimal model's size to maintain high performance: the model needs to be
sufficiently over-parametrized, but having too many parameters wastes training
resources.
</p>
<p>In this paper, we aim to find the best trade-off efficiently. More precisely,
we tackle the occurrence of the sparse double descent and present some
solutions to avoid it. Firstly, we show that a simple $\ell_2$ regularization
method can help to mitigate this phenomenon but sacrifices the
performance/sparsity compromise. To overcome this problem, we then introduce a
learning scheme in which distilling knowledge regularizes the student model.
Supported by experimental results achieved using typical image classification
setups, we show that this approach leads to the avoidance of such a phenomenon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16609">Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1">Siyu Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhengyang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1">Wei Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yongdao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Luchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a></p>
<p>Graph classification, aiming at learning the graph-level representations for
effective class assignments, has received outstanding achievements, which
heavily relies on high-quality datasets that have balanced class distribution.
In fact, most real-world graph data naturally presents a long-tailed form,
where the head classes occupy much more samples than the tail classes, it thus
is essential to study the graph-level classification over long-tailed data
while still remaining largely unexplored. However, most existing long-tailed
learning methods in visions fail to jointly optimize the representation
learning and classifier training, as well as neglect the mining of the
hard-to-classify classes. Directly applying existing methods to graphs may lead
to sub-optimal performance, since the model trained on graphs would be more
sensitive to the long-tailed distribution due to the complex topological
characteristics. Hence, in this paper, we propose a novel long-tailed
graph-level classification framework via Collaborative Multi-expert Learning
(CoMe) to tackle the problem. To equilibrate the contributions of head and tail
classes, we first develop balanced contrastive learning from the view of
representation learning, and then design an individual-expert classifier
training based on hard class mining. In addition, we execute gated fusion and
disentangled knowledge distillation among the multiple experts to promote the
collaboration in a multi-expert framework. Comprehensive experiments are
performed on seven widely-used benchmark datasets to demonstrate the
superiority of our method CoMe over state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16615">High Accuracy Location Information Extraction from Social Network Texts Using Natural Language Processing. (arXiv:2308.16615v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonde_L/0/1/0/all/0/1">Lossan Bonde</a>, <a href="http://arxiv.org/find/cs/1/au:+Dembele_S/0/1/0/all/0/1">Severin Dembele</a></p>
<p>Terrorism has become a worldwide plague with severe consequences for the
development of nations. Besides killing innocent people daily and preventing
educational activities from taking place, terrorism is also hindering economic
growth. Machine Learning (ML) and Natural Language Processing (NLP) can
contribute to fighting terrorism by predicting in real-time future terrorist
attacks if accurate data is available. This paper is part of a research project
that uses text from social networks to extract necessary information to build
an adequate dataset for terrorist attack prediction. We collected a set of 3000
social network texts about terrorism in Burkina Faso and used a subset to
experiment with existing NLP solutions. The experiment reveals that existing
solutions have poor accuracy for location recognition, which our solution
resolves. We will extend the solution to extract dates and action information
to achieve the project's goal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16622">Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering. (arXiv:2308.16622v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meyer_L/0/1/0/all/0/1">Lars-Peter Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Frey_J/0/1/0/all/0/1">Johannes Frey</a>, <a href="http://arxiv.org/find/cs/1/au:+Junghanns_K/0/1/0/all/0/1">Kurt Junghanns</a>, <a href="http://arxiv.org/find/cs/1/au:+Brei_F/0/1/0/all/0/1">Felix Brei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulert_K/0/1/0/all/0/1">Kirill Bulert</a>, <a href="http://arxiv.org/find/cs/1/au:+Grunder_Fahrer_S/0/1/0/all/0/1">Sabine Gr&#xfc;nder-Fahrer</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1">Michael Martin</a></p>
<p>As the field of Large Language Models (LLMs) evolves at an accelerated pace,
the critical need to assess and monitor their performance emerges. We introduce
a benchmarking framework focused on knowledge graph engineering (KGE)
accompanied by three challenges addressing syntax and error correction, facts
extraction and dataset generation. We show that while being a useful tool, LLMs
are yet unfit to assist in knowledge graph generation with zero-shot prompting.
Consequently, our LLM-KG-Bench framework provides automatic evaluation and
storage of LLM responses as well as statistical data and visualization tools to
support tracking of prompt engineering and model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16665">Fault Injection on Embedded Neural Networks: Impact of a Single Instruction Skip. (arXiv:2308.16665v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gaine_C/0/1/0/all/0/1">Clement Gaine</a>, <a href="http://arxiv.org/find/cs/1/au:+Moellic_P/0/1/0/all/0/1">Pierre-Alain Moellic</a>, <a href="http://arxiv.org/find/cs/1/au:+Potin_O/0/1/0/all/0/1">Olivier Potin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutertre_J/0/1/0/all/0/1">Jean-Max Dutertre</a></p>
<p>With the large-scale integration and use of neural network models, especially
in critical embedded systems, their security assessment to guarantee their
reliability is becoming an urgent need. More particularly, models deployed in
embedded platforms, such as 32-bit microcontrollers, are physically accessible
by adversaries and therefore vulnerable to hardware disturbances. We present
the first set of experiments on the use of two fault injection means,
electromagnetic and laser injections, applied on neural networks models
embedded on a Cortex M4 32-bit microcontroller platform. Contrary to most of
state-of-the-art works dedicated to the alteration of the internal parameters
or input values, our goal is to simulate and experimentally demonstrate the
impact of a specific fault model that is instruction skip. For that purpose, we
assessed several modification attacks on the control flow of a neural network
inference. We reveal integrity threats by targeting several steps in the
inference program of typical convolutional neural network models, which may be
exploited by an attacker to alter the predictions of the target models with
different adversarial goals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16684">Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sze Jue Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1">Chee Seng Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1">Khoa Doan</a></p>
<p>The vulnerabilities to backdoor attacks have recently threatened the
trustworthiness of machine learning models in practical applications.
Conventional wisdom suggests that not everyone can be an attacker since the
process of designing the trigger generation algorithm often involves
significant effort and extensive experimentation to ensure the attack's
stealthiness and effectiveness. Alternatively, this paper shows that there
exists a more severe backdoor threat: anyone can exploit an easily-accessible
algorithm for silent backdoor attacks. Specifically, this attacker can employ
the widely-used lossy image compression from a plethora of compression tools to
effortlessly inject a trigger pattern into an image without leaving any
noticeable trace; i.e., the generated triggers are natural artifacts. One does
not require extensive knowledge to click on the "convert" or "save as" button
while using tools for lossy image compression. Via this attack, the adversary
does not need to design a trigger generator as seen in prior works and only
requires poisoning the data. Empirically, the proposed attack consistently
achieves 100% attack success rate in several benchmark datasets such as MNIST,
CIFAR-10, GTSRB and CelebA. More significantly, the proposed attack can still
achieve almost 100% attack success rate with very small (approximately 10%)
poisoning rates in the clean label setting. The generated trigger of the
proposed attack using one lossy compression algorithm is also transferable
across other related compression algorithms, exacerbating the severity of this
backdoor threat. This work takes another crucial step toward understanding the
extensive risks of backdoor attacks in practice, urging practitioners to
investigate similar attacks and relevant backdoor mitigation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16688">Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology. (arXiv:2308.16688v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raja_H/0/1/0/all/0/1">Hina Raja</a>, <a href="http://arxiv.org/find/cs/1/au:+Munawar_A/0/1/0/all/0/1">Asim Munawar</a>, <a href="http://arxiv.org/find/cs/1/au:+Delsoz_M/0/1/0/all/0/1">Mohammad Delsoz</a>, <a href="http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1">Mohammad Elahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Madadi_Y/0/1/0/all/0/1">Yeganeh Madadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1">Amr Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Serhan_H/0/1/0/all/0/1">Hashem Abu Serhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Inam_O/0/1/0/all/0/1">Onur Inam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hermandez_L/0/1/0/all/0/1">Luis Hermandez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1">Sang Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Munir_W/0/1/0/all/0/1">Wuqas Munir</a>, <a href="http://arxiv.org/find/cs/1/au:+Abd_Alrazaq_A/0/1/0/all/0/1">Alaa Abd-Alrazaq</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+SiamakYousefi/0/1/0/all/0/1">SiamakYousefi</a></p>
<p>Purpose: In this paper, we present an automated method for article
classification, leveraging the power of Large Language Models (LLM). The
primary focus is on the field of ophthalmology, but the model is extendable to
other fields. Methods: We have developed a model based on Natural Language
Processing (NLP) techniques, including advanced LLMs, to process and analyze
the textual content of scientific papers. Specifically, we have employed
zero-shot learning (ZSL) LLM models and compared against Bidirectional and
Auto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder
Representations from Transformers (BERT), and its variant such as distilBERT,
SciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate
the effectiveness of LLMs in categorizing large number of ophthalmology papers
without human intervention. Results: To evalute the LLMs, we compiled a dataset
(RenD) of 1000 ocular disease-related articles, which were expertly annotated
by a panel of six specialists into 15 distinct categories. The model achieved
mean accuracy of 0.86 and mean F1 of 0.85 based on the RenD dataset.
Conclusion: The proposed framework achieves notable improvements in both
accuracy and efficiency. Its application in the domain of ophthalmology
showcases its potential for knowledge organization and retrieval in other
domains too. We performed trend analysis that enables the researchers and
clinicians to easily categorize and retrieve relevant papers, saving time and
effort in literature review and information gathering as well as identification
of emerging scientific trends within different disciplines. Moreover, the
extendibility of the model to other scientific fields broadens its impact in
facilitating research and trend analysis across diverse disciplines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16703">Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models. (arXiv:2308.16703v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hector_K/0/1/0/all/0/1">Kevin Hector</a>, <a href="http://arxiv.org/find/cs/1/au:+Moellic_P/0/1/0/all/0/1">Pierre-Alain Moellic</a>, <a href="http://arxiv.org/find/cs/1/au:+Dumont_M/0/1/0/all/0/1">Mathieu Dumont</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutertre_J/0/1/0/all/0/1">Jean-Max Dutertre</a></p>
<p>Model extraction emerges as a critical security threat with attack vectors
exploiting both algorithmic and implementation-based approaches. The main goal
of an attacker is to steal as much information as possible about a protected
victim model, so that he can mimic it with a substitute model, even with a
limited access to similar training data. Recently, physical attacks such as
fault injection have shown worrying efficiency against the integrity and
confidentiality of embedded models. We focus on embedded deep neural network
models on 32-bit microcontrollers, a widespread family of hardware platforms in
IoT, and the use of a standard fault injection strategy - Safe Error Attack
(SEA) - to perform a model extraction attack with an adversary having a limited
access to training data. Since the attack strongly depends on the input
queries, we propose a black-box approach to craft a successful attack set. For
a classical convolutional neural network, we successfully recover at least 90%
of the most significant bits with about 1500 crafted inputs. These information
enable to efficiently train a substitute model, with only 8% of the training
dataset, that reaches high fidelity and near identical accuracy level than the
victim model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16705">CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Nayeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1">Chani Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Myung_J/0/1/0/all/0/1">Junho Myung</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jiho Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Juho Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1">Alice Oh</a></p>
<p>English datasets predominantly reflect the perspectives of certain
nationalities, which can lead to cultural biases in models and datasets. This
is particularly problematic in tasks heavily influenced by subjectivity, such
as hate speech detection. To delve into how individuals from different
countries perceive hate speech, we introduce CReHate, a cross-cultural
re-annotation of the sampled SBIC dataset. This dataset includes annotations
from five distinct countries: Australia, Singapore, South Africa, the United
Kingdom, and the United States. Our thorough statistical analysis highlights
significant differences based on nationality, with only 59.4% of the samples
achieving consensus among all countries. We also introduce a culturally
sensitive hate speech classifier via transfer learning, adept at capturing
perspectives of different nationalities. These findings underscore the need to
re-evaluate certain aspects of NLP research, especially with regard to the
nuanced nature of hate speech in the English language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16725">Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance. (arXiv:2308.16725v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zexin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1">Kun Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_C/0/1/0/all/0/1">Clinton Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Lei Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiyong Wang</a></p>
<p>Sketch-based terrain generation seeks to create realistic landscapes for
virtual environments in various applications such as computer games, animation
and virtual reality. Recently, deep learning based terrain generation has
emerged, notably the ones based on generative adversarial networks (GAN).
However, these methods often struggle to fulfill the requirements of flexible
user control and maintain generative diversity for realistic terrain.
Therefore, we propose a novel diffusion-based method, namely terrain diffusion
network (TDN), which actively incorporates user guidance for enhanced
controllability, taking into account terrain features like rivers, ridges,
basins, and peaks. Instead of adhering to a conventional monolithic denoising
process, which often compromises the fidelity of terrain details or the
alignment with user control, a multi-level denoising scheme is proposed to
generate more realistic terrains by taking into account fine-grained details,
particularly those related to climatic patterns influenced by erosion and
tectonic activities. Specifically, three terrain synthesisers are designed for
structural, intermediate, and fine-grained level denoising purposes, which
allow each synthesiser concentrate on a distinct terrain aspect. Moreover, to
maximise the efficiency of our TDN, we further introduce terrain and sketch
latent spaces for the synthesizers with pre-trained terrain autoencoders.
Comprehensive experiments on a new dataset constructed from NASA Topology
Images clearly demonstrate the effectiveness of our proposed method, achieving
the state-of-the-art performance. Our code and dataset will be publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16730">Proof of Deep Learning: Approaches, Challenges, and Future Directions. (arXiv:2308.16730v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Salhab_M/0/1/0/all/0/1">Mahmoud Salhab</a>, <a href="http://arxiv.org/find/cs/1/au:+Mershad_K/0/1/0/all/0/1">Khaleel Mershad</a></p>
<p>The rise of computational power has led to unprecedented performance gains
for deep learning models. As more data becomes available and model
architectures become more complex, the need for more computational power
increases. On the other hand, since the introduction of Bitcoin as the first
cryptocurrency and the establishment of the concept of blockchain as a
distributed ledger, many variants and approaches have been proposed. However,
many of them have one thing in common, which is the Proof of Work (PoW)
consensus mechanism. PoW is mainly used to support the process of new block
generation. While PoW has proven its robustness, its main drawback is that it
requires a significant amount of processing power to maintain the security and
integrity of the blockchain. This is due to applying brute force to solve a
hashing puzzle. To utilize the computational power available in useful and
meaningful work while keeping the blockchain secure, many techniques have been
proposed, one of which is known as Proof of Deep Learning (PoDL). PoDL is a
consensus mechanism that uses the process of training a deep learning model as
proof of work to add new blocks to the blockchain. In this paper, we survey the
various approaches for PoDL. We discuss the different types of PoDL algorithms,
their advantages and disadvantages, and their potential applications. We also
discuss the challenges of implementing PoDL and future research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16735">Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment. (arXiv:2308.16735v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wagner_F/0/1/0/all/0/1">Felix Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zeju Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1">Pramit Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1">Konstantinos Kamnitsas</a></p>
<p>Deployment of Deep Neural Networks in medical imaging is hindered by
distribution shift between training data and data processed after deployment,
causing performance degradation. Post-Deployment Adaptation (PDA) addresses
this by tailoring a pre-trained, deployed model to the target data distribution
using limited labelled or entirely unlabelled target data, while assuming no
access to source training data as they cannot be deployed with the model due to
privacy concerns and their large size. This makes reliable adaptation
challenging due to limited learning signal. This paper challenges this
assumption and introduces FedPDA, a novel adaptation framework that brings the
utility of learning from remote data from Federated Learning into PDA. FedPDA
enables a deployed model to obtain information from source data via remote
gradient exchange, while aiming to optimize the model specifically for the
target domain. Tailored for FedPDA, we introduce a novel optimization method
StarAlign (Source-Target Remote Gradient Alignment) that aligns gradients
between source-target domain pairs by maximizing their inner product, to
facilitate learning a target-specific model. We demonstrate the method's
effectiveness using multi-center databases for the tasks of cancer metastases
detection and skin lesion classification, where our method compares favourably
to previous work. Code is available at: https://github.com/FelixWag/StarAlign
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16737">Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mirzaeifard_R/0/1/0/all/0/1">Reza Mirzaeifard</a>, <a href="http://arxiv.org/find/cs/1/au:+Venkategowda_N/0/1/0/all/0/1">Naveen K. D. Venkategowda</a>, <a href="http://arxiv.org/find/cs/1/au:+Werner_S/0/1/0/all/0/1">Stefan Werner</a></p>
<p>This paper addresses the problem of localization, which is inherently
non-convex and non-smooth in a federated setting where the data is distributed
across a multitude of devices. Due to the decentralized nature of federated
environments, distributed learning becomes essential for scalability and
adaptability. Moreover, these environments are often plagued by outlier data,
which presents substantial challenges to conventional methods, particularly in
maintaining estimation accuracy and ensuring algorithm convergence. To mitigate
these challenges, we propose a method that adopts an $L_1$-norm robust
formulation within a distributed sub-gradient framework, explicitly designed to
handle these obstacles. Our approach addresses the problem in its original
form, without resorting to iterative simplifications or approximations,
resulting in enhanced computational efficiency and improved estimation
accuracy. We demonstrate that our method converges to a stationary point,
highlighting its effectiveness and reliability. Through numerical simulations,
we confirm the superior performance of our approach, notably in outlier-rich
environments, which surpasses existing state-of-the-art localization methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16741">Socratis: Are large multimodal models emotionally aware?. (arXiv:2308.16741v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1">Katherine Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1">Arijit Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1">Reuben Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1">Saadia Gabriel</a>, <a href="http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1">Bryan A. Plummer</a>, <a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1">Kate Saenko</a></p>
<p>Existing emotion prediction benchmarks contain coarse emotion labels which do
not consider the diversity of emotions that an image and text can elicit in
humans due to various reasons. Learning diverse reactions to multimodal content
is important as intelligent machines take a central role in generating and
delivering content to society. To address this gap, we propose Socratis, a
\underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s}
benchmark, where each image-caption (IC) pair is annotated with multiple
emotions and the reasons for feeling them. Socratis contains 18K free-form
reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news
and image-caption (IC) datasets. We benchmark the capability of
state-of-the-art multimodal large language models to generate the reasons for
feeling an emotion given an IC pair. Based on a preliminary human study, we
observe that humans prefer human-written reasons over 2 times more often than
machine-generated ones. This shows our task is harder than standard generation
tasks because it starkly contrasts recent findings where humans cannot tell
apart machine vs human-written news articles, for instance. We further see that
current captioning metrics based on large vision-language models also fail to
correlate with human preferences. We hope that these findings and our benchmark
will inspire further research on training emotionally aware models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16753">Context Aware Query Rewriting for Text Rankers using LLM. (arXiv:2308.16753v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Abhijit Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1">Venktesh V</a>, <a href="http://arxiv.org/find/cs/1/au:+Setty_V/0/1/0/all/0/1">Vinay Setty</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Avishek Anand</a></p>
<p>Query rewriting refers to an established family of approaches that are
applied to underspecified and ambiguous queries to overcome the vocabulary
mismatch problem in document ranking. Queries are typically rewritten during
query processing time for better query modelling for the downstream ranker.
With the advent of large-language models (LLMs), there have been initial
investigations into using generative approaches to generate pseudo documents to
tackle this inherent vocabulary gap. In this work, we analyze the utility of
LLMs for improved query rewriting for text ranking tasks. We find that there
are two inherent limitations of using LLMs as query re-writers -- concept drift
when using only queries as prompts and large inference costs during query
processing. We adopt a simple, yet surprisingly effective, approach called
context aware query rewriting (CAR) to leverage the benefits of LLMs for query
understanding. Firstly, we rewrite ambiguous training queries by context-aware
prompting of LLMs, where we use only relevant documents as context.Unlike
existing approaches, we use LLM-based query rewriting only during the training
phase. Eventually, a ranker is fine-tuned on the rewritten queries instead of
the original queries during training. In our extensive experiments, we find
that fine-tuning a ranker using re-written queries offers a significant
improvement of up to 33% on the passage ranking task and up to 28% on the
document ranking task when compared to the baseline performance of using
original queries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16763">Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1">Kairui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1">Ming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Joey Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ivor W. Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1">Wen Haw Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yap_Y/0/1/0/all/0/1">Yong Keong Yap</a></p>
<p>Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of
Large Language Models (LLMs) through the generation of intermediate rationales.
However, these enhancements predominantly benefit large-scale models, leaving
small LMs without significant performance improvements when directly applying
CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily
on their pre-trained internal knowledge. The external knowledge that is
previously unknown to the model remains unexploited. This omission becomes
pronounced in tasks such as stance detection, where the external background
knowledge plays a pivotal role. Additionally, the large-scale architecture of
LLMs inevitably present efficiency challenges during deployment. To address
these challenges, we introduce the Ladder-of-Thought (LoT) for stance
detection. Grounded in a dual-phase Cascaded Optimization framework, LoT
directs the model to incorporate high-quality external knowledge, enhancing the
intermediate rationales it generates. These bolstered rationales subsequently
serve as the foundation for more precise predictions - akin to how a ladder
facilitates reaching elevated goals. LoT achieves a balance between efficiency
and accuracy, making it an adaptable and efficient framework for stance
detection. Our empirical evaluations underscore LoT's effectiveness, marking a
16% improvement over ChatGPT and a 10% enhancement compared to ChatGPT with
CoT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16769">Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems. (arXiv:2308.16769v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McGuan_C/0/1/0/all/0/1">Colman McGuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chansu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qin Lin</a></p>
<p>The protection of Industrial Control Systems (ICS) that are employed in
public critical infrastructures is of utmost importance due to catastrophic
physical damages cyberattacks may cause. The research community requires
testbeds for validation and comparing various intrusion detection algorithms to
protect ICS. However, there exist high barriers to entry for research and
education in the ICS cybersecurity domain due to expensive hardware, software,
and inherent dangers of manipulating real-world systems. To close the gap,
built upon recently developed 3D high-fidelity simulators, we further showcase
our integrated framework to automatically launch cyberattacks, collect data,
train machine learning models, and evaluate for practical chemical and
manufacturing processes. On our testbed, we validate our proposed intrusion
detection model called Minimal Threshold and Window SVM (MinTWin SVM) that
utilizes unsupervised machine learning via a one-class SVM in combination with
a sliding window and classification threshold. Results show that MinTWin SVM
minimizes false positives and is responsive to physical process anomalies.
Furthermore, we incorporate our framework with ICS cybersecurity education by
using our dataset in an undergraduate machine learning course where students
gain hands-on experience in practicing machine learning theory with a practical
ICS dataset. All of our implementations have been open-sourced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16775">Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1">Minh Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nhan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luong_N/0/1/0/all/0/1">Ngoc Hoang Luong</a></p>
<p>In prediction-based Neural Architecture Search (NAS), performance indicators
derived from graph convolutional networks have shown significant success. These
indicators, achieved by representing feed-forward structures as component
graphs through one-hot encoding, face a limitation: their inability to evaluate
architecture performance across varying search spaces. In contrast, handcrafted
performance indicators (zero-shot NAS), which use the same architecture with
random initialization, can generalize across multiple search spaces. Addressing
this limitation, we propose a novel approach for zero-shot NAS using deep
learning. Our method employs Fourier sum of sines encoding for convolutional
kernels, enabling the construction of a computational feed-forward graph with a
structure similar to the architecture under evaluation. These encodings are
learnable and offer a comprehensive view of the architecture's topological
information. An accompanying multi-layer perceptron (MLP) then ranks these
architectures based on their encodings. Experimental results show that our
approach surpasses previous methods using graph convolutional networks in terms
of correlation on the NAS-Bench-201 dataset and exhibits a higher convergence
rate. Moreover, our extracted feature representation trained on each
NAS-Benchmark is transferable to other NAS-Benchmarks, showing promising
generalizability across multiple search spaces. The code is available at:
https://github.com/minh1409/DFT-NPZS-NAS
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16781">StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a></p>
<p>With the growing imbalance between limited medical resources and escalating
demands, AI-based clinical tasks have become paramount. Medication
recommendation, as a sub-domain, aims to amalgamate longitudinal patient
history with medical knowledge, assisting physicians in prescribing safer and
more accurate medication combinations. Existing methods overlook the inherent
long-tail distribution in medical data, lacking balanced representation between
head and tail data, which leads to sub-optimal model performance. To address
this challenge, we introduce StratMed, a model that incorporates an innovative
relevance stratification mechanism. It harmonizes discrepancies in data
long-tail distribution and strikes a balance between the safety and accuracy of
medication combinations. Specifically, we first construct a pre-training method
using deep learning networks to obtain entity representation. After that, we
design a pyramid-like data stratification method to obtain more generalized
entity relationships by reinforcing the features of unpopular entities. Based
on this relationship, we designed two graph structures to express medication
precision and safety at the same level to obtain visit representations.
Finally, the patient's historical clinical information is fitted to generate
medication combinations for the current health condition. Experiments on the
MIMIC-III dataset demonstrate that our method has outperformed current
state-of-the-art methods in four evaluation metrics (including safety and
accuracy).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16785">Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming. (arXiv:2308.16785v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1">Mowei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zaifeng Gao</a></p>
<p>The rapid advancements in artificial intelligence (AI) have led to a growing
trend of human-AI teaming (HAT) in various fields. As machines continue to
evolve from mere automation to a state of autonomy, they are increasingly
exhibiting unexpected behaviors and human-like cognitive/intelligent
capabilities, including situation awareness (SA). This shift has the potential
to enhance the performance of mixed human-AI teams over all-human teams,
underscoring the need for a better understanding of the dynamic SA interactions
between humans and machines. To this end, we provide a review of leading SA
theoretical models and a new framework for SA in the HAT context based on the
key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA)
framework unifies human and AI behavior, and involves bidirectional, and
dynamic interaction. The framework is based on the individual and team SA
models and elaborates on the cognitive mechanisms for modeling HAT. Similar
perceptual cycles are adopted for the individual (including both human and AI)
and the whole team, which is tailored to the unique requirements of the HAT
context. ATSA emphasizes cohesive and effective HAT through structures and
components, including teaming understanding, teaming control, and the world, as
well as adhesive transactive part. We further propose several future research
directions to expand on the distinctive contributions of ATSA and address the
specific and pressing next steps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16800">Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1">Andreas Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Liebig_T/0/1/0/all/0/1">Thomas Liebig</a></p>
<p>Our study reveals new theoretical insights into over-smoothing and feature
over-correlation in deep graph neural networks. We show the prevalence of
invariant subspaces, demonstrating a fixed relative behavior that is unaffected
by feature transformations. Our work clarifies recent observations related to
convergence to a constant state and a potential over-separation of node states,
as the amplification of subspaces only depends on the spectrum of the
aggregation function. In linear scenarios, this leads to node representations
being dominated by a low-dimensional subspace with an asymptotic convergence
rate independent of the feature transformations. This causes a rank collapse of
the node representations, resulting in over-smoothing when smooth vectors span
this subspace, and over-correlation even when over-smoothing is avoided. Guided
by our theory, we propose a sum of Kronecker products as a beneficial property
that can provably prevent over-smoothing, over-correlation, and rank collapse.
We empirically extend our insights to the non-linear case, demonstrating the
inability of existing models to capture linearly independent features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16818">Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weijia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Le Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jindong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingbo Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1">Yu Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a></p>
<p>Accurate traffic forecasting at intersections governed by intelligent traffic
signals is critical for the advancement of an effective intelligent traffic
signal control system. However, due to the irregular traffic time series
produced by intelligent intersections, the traffic forecasting task becomes
much more intractable and imposes three major new challenges: 1) asynchronous
spatial dependency, 2) irregular temporal dependency among traffic data, and 3)
variable-length sequence to be predicted, which severely impede the performance
of current traffic forecasting methods. To this end, we propose an Asynchronous
Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic
states of the lanes entering intelligent intersections in a future time window.
Specifically, by linking lanes via a traffic diffusion graph, we first propose
an Asynchronous Graph Diffusion Network to model the asynchronous spatial
dependency between the time-misaligned traffic state measurements of lanes.
After that, to capture the temporal dependency within irregular traffic state
sequence, a learnable personalized time encoding is devised to embed the
continuous time for each lane. Then we propose a Transformable Time-aware
Convolution Network that learns meta-filters to derive time-aware convolution
filters with transformable filter sizes for efficient temporal convolution on
the irregular sequence. Furthermore, a Semi-Autoregressive Prediction Network
consisting of a state evolution unit and a semiautoregressive predictor is
designed to effectively and efficiently predict variable-length traffic state
sequences. Extensive experiments on two real-world datasets demonstrate the
effectiveness of ASeer in six metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16822">Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets. (arXiv:2308.16822v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chunchao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1">Arthur Leroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_M/0/1/0/all/0/1">Mauricio Alvarez</a></p>
<p>Multi-output Gaussian processes (MOGPs) have been introduced to deal with
multiple tasks by exploiting the correlations between different outputs.
Generally, MOGPs models assume a flat correlation structure between the
outputs. However, such a formulation does not account for more elaborate
relationships, for instance, if several replicates were observed for each
output (which is a typical setting in biological experiments). This paper
proposes an extension of MOGPs for hierarchical datasets (i.e. datasets for
which the relationships between observations can be represented within a tree
structure). Our model defines a tailored kernel function accounting for
hierarchical structures in the data to capture different levels of correlations
while leveraging the introduction of latent variables to express the underlying
dependencies between outputs through a dedicated kernel. This latter feature is
expected to significantly improve scalability as the number of tasks increases.
An extensive experimental study involving both synthetic and real-world data
from genomics and motion capture is proposed to support our claims.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16824">Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1">Daoguang Zan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1">Ailun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Bo Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Taihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1">Bing Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jichuan Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yafen Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongji Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianxiang Wang</a></p>
<p>When human programmers have mastered a programming language, it would be
easier when they learn a new programming language. In this report, we focus on
exploring whether programming languages can boost each other during the
instruction fine-tuning phase of code large language models. We conduct
extensive experiments of 8 popular programming languages (Python, JavaScript,
TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that
programming languages can significantly improve each other. For example,
CodeM-Python 15B trained on Python is able to increase Java by an absolute
17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B
trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our
training data is released at https://github.com/NL2Code/CodeM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16836">Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information. (arXiv:2308.16836v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shaohuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1">Shun Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+You_W/0/1/0/all/0/1">Weiya You</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuo_D/0/1/0/all/0/1">Deyi Tuo</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yuren You</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Shiyin Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Helen Meng</a></p>
<p>This paper presents an end-to-end high-quality singing voice synthesis (SVS)
system that uses bidirectional encoder representation from Transformers (BERT)
derived semantic embeddings to improve the expressiveness of the synthesized
singing voice. Based on the main architecture of recently proposed VISinger, we
put forward several specific designs for expressive singing voice synthesis.
First, different from the previous SVS models, we use text representation of
lyrics extracted from pre-trained BERT as additional input to the model. The
representation contains information about semantics of the lyrics, which could
help SVS system produce more expressive and natural voice. Second, we further
introduce an energy predictor to stabilize the synthesized voice and model the
wider range of energy variations that also contribute to the expressiveness of
singing voice. Last but not the least, to attenuate the off-key issues, the
pitch predictor is re-designed to predict the real to note pitch ratio. Both
objective and subjective experimental results indicate that the proposed SVS
system can produce singing voice with higher-quality outperforming VISinger.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16857">IoMT-Blockchain based Secured Remote Patient Monitoring Framework for Neuro-Stimulation Device. (arXiv:2308.16857v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sourav_M/0/1/0/all/0/1">Md Sakib Ullah Sourav</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmud_M/0/1/0/all/0/1">Mohammad Sultan Mahmud</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukder_M/0/1/0/all/0/1">Md Simul Hasan Talukder</a>, <a href="http://arxiv.org/find/cs/1/au:+Sulaiman_R/0/1/0/all/0/1">Rejwan Bin Sulaiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasin_A/0/1/0/all/0/1">Abdullah Yasin</a></p>
<p>Biomedical Engineering's Internet of Medical Things (IoMT) is helping to
improve the accuracy, dependability, and productivity of electronic equipment
in the healthcare business. Real-time sensory data from patients may be
delivered and subsequently analyzed through rapid development of wearable IoMT
devices, such as neuro-stimulation devices with a range of functions. Data from
the Internet of Things is gathered, analyzed, and stored in a single location.
However, single-point failure, data manipulation, privacy difficulties, and
other challenges might arise as a result of centralization. Due to its
decentralized nature, blockchain (BC) can alleviate these issues. The viability
of establishing a non-invasive remote neurostimulation system employing
IoMT-based transcranial Direct Current Stimulation is investigated in this work
(tDCS). A hardware-based prototype tDCS device has been developed that can be
operated over the internet using an android application. Our suggested
framework addresses the problems of IoMTBC-based systems, meets the criteria of
real-time remote patient monitoring systems, and incorporates literature best
practices in the relevant fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16870">Learning Driver Models for Automated Vehicles via Knowledge Sharing and Personalization. (arXiv:2308.16870v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kontar_W/0/1/0/all/0/1">Wissam Kontar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1">Xinzhi Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Soyoung Ahn</a></p>
<p>This paper describes a framework for learning Automated Vehicles (AVs) driver
models via knowledge sharing between vehicles and personalization. The innate
variability in the transportation system makes it exceptionally challenging to
expose AVs to all possible driving scenarios during empirical experimentation
or testing. Consequently, AVs could be blind to certain encounters that are
deemed detrimental to their safe and efficient operation. It is then critical
to share knowledge across AVs that increase exposure to driving scenarios
occurring in the real world. This paper explores a method to collaboratively
train a driver model by sharing knowledge and borrowing strength across
vehicles while retaining a personalized model tailored to the vehicle's unique
conditions and properties. Our model brings a federated learning approach to
collaborate between multiple vehicles while circumventing the need to share raw
data between them. We showcase our method's performance in experimental
simulations. Such an approach to learning finds several applications across
transportation engineering including intelligent transportation systems,
traffic management, and vehicle-to-vehicle communication. Code and sample
dataset are made available at the project page https://github.com/wissamkontar.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16871">The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages. (arXiv:2308.16871v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1">Benjamin Muller</a>, <a href="http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1">Belen Alastruey</a>, <a href="http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1">Prangthip Hansanti</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1">Elahe Kalbassi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ropers_C/0/1/0/all/0/1">Christophe Ropers</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1">Eric Michael Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Adina Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrews_P/0/1/0/all/0/1">Pierre Andrews</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1">Marta R. Costa-juss&#xe0;</a></p>
<p>Gender biases in language generation systems are challenging to mitigate. One
possible source for these biases is gender representation disparities in the
training and evaluation data. Despite recent progress in documenting this
problem and many attempts at mitigating it, we still lack shared methodology
and tooling to report gender representation in large datasets. Such
quantitative reporting will enable further mitigation, e.g., via data
augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware
Polyglot Pipeline), an automatic pipeline to characterize gender representation
in large-scale datasets for 55 languages. The pipeline uses a multilingual
lexicon of gendered person-nouns to quantify the gender representation in text.
We showcase it to report gender representation in WMT training data and
development data for the News task, confirming that current data is skewed
towards masculine representation. Having unbalanced datasets may indirectly
optimize our systems towards outperforming one gender over the others. We
suggest introducing our gender quantification pipeline in current datasets and,
ideally, modifying them toward a balanced representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16879">Adaptation Speed Analysis for Fairness-aware Causal Models. (arXiv:2308.16879v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yujie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1">Minglai Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xujiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haifeng Chen</a></p>
<p>For example, in machine translation tasks, to achieve bidirectional
translation between two languages, the source corpus is often used as the
target corpus, which involves the training of two models with opposite
directions. The question of which one can adapt most quickly to a domain shift
is of significant importance in many fields. Specifically, consider an original
distribution p that changes due to an unknown intervention, resulting in a
modified distribution p*. In aligning p with p*, several factors can affect the
adaptation rate, including the causal dependencies between variables in p. In
real-life scenarios, however, we have to consider the fairness of the training
process, and it is particularly crucial to involve a sensitive variable (bias)
present between a cause and an effect variable. To explore this scenario, we
examine a simple structural causal model (SCM) with a cause-bias-effect
structure, where variable A acts as a sensitive variable between cause (X) and
effect (Y). The two models, respectively, exhibit consistent and contrary
cause-effect directions in the cause-bias-effect SCM. After conducting unknown
interventions on variables within the SCM, we can simulate some kinds of domain
shifts for analysis. We then compare the adaptation speeds of two models across
four shift scenarios. Additionally, we prove the connection between the
adaptation speeds of the two models across all interventions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16884">The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bandarkar_L/0/1/0/all/0/1">Lucas Bandarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Davis Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1">Benjamin Muller</a>, <a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1">Mikel Artetxe</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1">Satya Narayan Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Husa_D/0/1/0/all/0/1">Donald Husa</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1">Naman Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1">Abhinandan Krishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1">Madian Khabsa</a></p>
<p>We present Belebele, a multiple-choice machine reading comprehension (MRC)
dataset spanning 122 language variants. Significantly expanding the language
coverage of natural language understanding (NLU) benchmarks, this dataset
enables the evaluation of text models in high-, medium-, and low-resource
languages. Each question is based on a short passage from the Flores-200
dataset and has four multiple-choice answers. The questions were carefully
curated to discriminate between models with different levels of general
language comprehension. The English dataset on its own proves difficult enough
to challenge state-of-the-art language models. Being fully parallel, this
dataset enables direct comparison of model performance across all languages. We
use this dataset to evaluate the capabilities of multilingual masked language
models (MLMs) and large language models (LLMs). We present extensive results
and find that despite significant cross-lingual transfer in English-centric
LLMs, much smaller MLMs pretrained on balanced multilingual data still
understand far more languages. We also observe that larger vocabulary size and
conscious vocabulary construction correlate with better performance on
low-resource languages. Overall, Belebele opens up new avenues for evaluating
and analyzing the multilingual capabilities of NLP systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16892">ReZero: Region-customizable Sound Extraction. (arXiv:2308.16892v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gu_R/0/1/0/all/0/1">Rongzhi Gu</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1">Yi Luo</a></p>
<p>We introduce region-customizable sound extraction (ReZero), a general and
flexible framework for the multi-channel region-wise sound extraction (R-SE)
task. R-SE task aims at extracting all active target sounds (e.g., human
speech) within a specific, user-defined spatial region, which is different from
conventional and existing tasks where a blind separation or a fixed, predefined
spatial region are typically assumed. The spatial region can be defined as an
angular window, a sphere, a cone, or other geometric patterns. Being a solution
to the R-SE task, the proposed ReZero framework includes (1) definitions of
different types of spatial regions, (2) methods for region feature extraction
and aggregation, and (3) a multi-channel extension of the band-split RNN
(BSRNN) model specified for the R-SE task. We design experiments for different
microphone array geometries, different types of spatial regions, and
comprehensive ablation studies on different system configurations. Experimental
results on both simulated and real-recorded data demonstrate the effectiveness
of ReZero. Demos are available at https://innerselfm.github.io/rezero/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16893">Language-Conditioned Path Planning. (arXiv:2308.16893v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1">Amber Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Youngwoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1">Stephen James</a></p>
<p>Contact is at the core of robotic manipulation. At times, it is desired (e.g.
manipulation and grasping), and at times, it is harmful (e.g. when avoiding
obstacles). However, traditional path planning algorithms focus solely on
collision-free paths, limiting their applicability in contact-rich tasks. To
address this limitation, we propose the domain of Language-Conditioned Path
Planning, where contact-awareness is incorporated into the path planning
problem. As a first step in this domain, we propose Language-Conditioned
Collision Functions (LACO) a novel approach that learns a collision function
using only a single-view image, language prompt, and robot configuration. LACO
predicts collisions between the robot and the environment, enabling flexible,
conditional path planning without the need for manual object annotations, point
cloud data, or ground-truth object meshes. In both simulation and the real
world, we demonstrate that LACO can facilitate complex, nuanced path plans that
allow for interaction with objects that are safe to collide, rather than
prohibiting any collision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16896">PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction. (arXiv:2308.16896v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1">Sicheng Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wenzhao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuanhui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiwen Lu</a></p>
<p>Semantic segmentation in autonomous driving has been undergoing an evolution
from sparse point segmentation to dense voxel segmentation, where the objective
is to predict the semantic occupancy of each voxel in the concerned 3D space.
The dense nature of the prediction space has rendered existing efficient
2D-projection-based methods (e.g., bird's eye view, range view, etc.)
ineffective, as they can only describe a subspace of the 3D scene. To address
this, we propose a cylindrical tri-perspective view to represent point clouds
effectively and comprehensively and a PointOcc model to process them
efficiently. Considering the distance distribution of LiDAR point clouds, we
construct the tri-perspective view in the cylindrical coordinate system for
more fine-grained modeling of nearer areas. We employ spatial group pooling to
maintain structural details during projection and adopt 2D backbones to
efficiently process each TPV plane. Finally, we obtain the features of each
point by aggregating its projected features on each of the processed TPV planes
without the need for any post-processing. Extensive experiments on both 3D
occupancy prediction and LiDAR segmentation benchmarks demonstrate that the
proposed PointOcc achieves state-of-the-art performance with much faster speed.
Specifically, despite only using LiDAR, PointOcc significantly outperforms all
other methods, including multi-modal methods, with a large margin on the
OpenOccupancy benchmark. Code: https://github.com/wzzheng/PointOcc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16898">Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1">Davoud Ataee Tarzanagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yingcong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1">Christos Thrampoulidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1">Samet Oymak</a></p>
<p>Since its inception in "Attention Is All You Need", transformer architecture
has led to revolutionary advancements in NLP. The attention layer within the
transformer admits a sequence of input tokens $X$ and makes them interact
through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where
$(K,Q)$ are the trainable key-query parameters. In this work, we establish a
formal equivalence between the optimization geometry of self-attention and a
hard-margin SVM problem that separates optimal input tokens from non-optimal
tokens using linear constraints on the outer-products of token pairs. This
formalism allows us to characterize the implicit bias of 1-layer transformers
optimized with gradient descent: (1) Optimizing the attention layer with
vanishing regularization, parameterized by $(K,Q)$, converges in direction to
an SVM solution minimizing the nuclear norm of the combined parameter
$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm
objective. We characterize this convergence, highlighting that it can occur
toward locally-optimal directions rather than global ones. (2) Complementing
this, we prove the local/global directional convergence of gradient descent
under suitable geometric conditions. Importantly, we show that
over-parameterization catalyzes global convergence by ensuring the feasibility
of the SVM problem and by guaranteeing a benign optimization landscape devoid
of stationary points. (3) While our theory applies primarily to linear
prediction heads, we propose a more general SVM equivalence that predicts the
implicit bias with nonlinear heads. Our findings are applicable to arbitrary
datasets and their validity is verified via experiments. We also introduce
several open problems and research directions. We believe these findings
inspire the interpretation of transformers as a hierarchy of SVMs that
separates and selects optimal tokens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16905">InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion. (arXiv:2308.16905v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Sirui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhengyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Xiong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1">Liang-Yan Gui</a></p>
<p>This paper addresses a novel task of anticipating 3D human-object
interactions (HOIs). Most existing research on HOI synthesis lacks
comprehensive whole-body interactions with dynamic objects, e.g., often limited
to manipulating small or static objects. Our task is significantly more
challenging, as it requires modeling dynamic objects with various shapes,
capturing whole-body motion, and ensuring physically valid interactions. To
this end, we propose InterDiff, a framework comprising two key steps: (i)
interaction diffusion, where we leverage a diffusion model to encode the
distribution of future human-object interactions; (ii) interaction correction,
where we introduce a physics-informed predictor to correct denoised HOIs in a
diffusion step. Our key insight is to inject prior knowledge that the
interactions under reference with respect to contact points follow a simple
pattern and are easily predictable. Experiments on multiple human-object
interaction datasets demonstrate the effectiveness of our method for this task,
capable of producing realistic, vivid, and remarkably long-term 3D HOI
predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16909">StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation. (arXiv:2308.16909v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuhan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Liming Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a></p>
<p>Unconditional video generation is a challenging task that involves
synthesizing high-quality videos that are both coherent and of extended
duration. To address this challenge, researchers have used pretrained StyleGAN
image generators for high-quality frame synthesis and focused on motion
generator design. The motion generator is trained in an autoregressive manner
using heavy 3D convolutional discriminators to ensure motion coherence during
video generation. In this paper, we introduce a novel motion generator design
that uses a learning-based inversion network for GAN. The encoder in our method
captures rich and smooth priors from encoding images to latents, and given the
latent of an initially generated frame as guidance, our method can generate
smooth future latent by modulating the inversion encoder temporally. Our method
enjoys the advantage of sparse training and naturally constrains the generation
space of our motion generator with the inversion network guided by the initial
frame, eliminating the need for heavy discriminators. Moreover, our method
supports style transfer with simple fine-tuning when the encoder is paired with
a pretrained StyleGAN generator. Extensive experiments conducted on various
benchmarks demonstrate the superiority of our method in generating long and
high-resolution videos with decent single-frame quality and temporal
consistency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16911">PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Runsen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yilun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1">Jiangmiao Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a></p>
<p>The unprecedented advancements in Large Language Models (LLMs) have created a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, thereby enabling LLMs to understand point clouds and offering
a new avenue beyond 2D visual data. PointLLM processes colored object point
clouds with human instructions and generates contextually appropriate
responses, illustrating its grasp of point clouds and common sense.
Specifically, it leverages a point cloud encoder with a powerful LLM to
effectively fuse geometric, appearance, and linguistic information. We collect
a novel dataset comprising 660K simple and 70K complex point-text instruction
pairs to enable a two-stage training strategy: initially aligning latent spaces
and subsequently instruction-tuning the unified model. To rigorously evaluate
our model's perceptual abilities and its generalization capabilities, we
establish two benchmarks: Generative 3D Object Classification and 3D Object
Captioning, assessed through three different methods, including human
evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment
results show that PointLLM demonstrates superior performance over existing 2D
baselines. Remarkably, in human-evaluated object captioning tasks, PointLLM
outperforms human annotators in over 50% of the samples. Codes, datasets, and
benchmarks are available at https://github.com/OpenRobotLab/PointLLM .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.04307">Learning Optimal Strategies for Temporal Tasks in Stochastic Games. (arXiv:2102.04307v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bozkurt_A/0/1/0/all/0/1">Alper Kamil Bozkurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zavlanos_M/0/1/0/all/0/1">Michael M. Zavlanos</a>, <a href="http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1">Miroslav Pajic</a></p>
<p>Synthesis from linear temporal logic (LTL) specifications provides assured
controllers for systems operating in stochastic and potentially adversarial
environments. Automatic synthesis tools, however, require a model of the
environment to construct controllers. In this work, we introduce a model-free
reinforcement learning (RL) approach to derive controllers from given LTL
specifications even when the environment is completely unknown. We model the
problem as a stochastic game (SG) between the controller and the adversarial
environment; we then learn optimal control strategies that maximize the
probability of satisfying the LTL specifications against the worst-case
environment behavior. We first construct a product game using the deterministic
parity automaton (DPA) translated from the given LTL specification. By deriving
distinct rewards and discount factors from the acceptance condition of the DPA,
we reduce the maximization of the worst-case probability of satisfying the LTL
specification into the maximization of a discounted reward objective in the
product game; this enables the use of model-free RL algorithms to learn an
optimal controller strategy. To deal with the common scalability problems when
the number of sets defining the acceptance condition of the DPA (usually
referred as colors), is large, we propose a lazy color generation method where
distinct rewards and discount factors are utilized only when needed, and an
approximate method where the controller eventually focuses on only one color.
In several case studies, we show that our approach is scalable to a wide range
of LTL formulas, significantly outperforming existing methods for learning
controllers from LTL specifications in SGs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.14052">Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs. (arXiv:2106.14052v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andresel_M/0/1/0/all/0/1">Medina Andresel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1">Trung-Kien Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Domokos_C/0/1/0/all/0/1">Csaba Domokos</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1">Daria Stepanova</a></p>
<p>Current methods for embedding-based query answering over incomplete Knowledge
Graphs (KGs) only focus on inductive reasoning, i.e., predicting answers by
learning patterns from the data, and lack the complementary ability to do
deductive reasoning, which requires the application of domain knowledge to
infer further information. To address this shortcoming, we investigate the
problem of incorporating ontologies into embedding-based query answering models
by defining the task of embedding-based ontology-mediated query answering. We
propose various integration strategies into prominent representatives of
embedding models that involve (1) different ontology-driven data augmentation
techniques and (2) adaptation of the loss function to enforce the ontology
axioms. We design novel benchmarks for the considered task based on the LUBM
and the NELL KGs and evaluate our methods on them. The achieved improvements in
the setting that requires both inductive and deductive reasoning are from 20%
to 55% in HITS@3.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.04053">DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models. (arXiv:2202.04053v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jaemin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1">Abhay Zala</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Recently, DALL-E, a multimodal transformer language model, and its variants,
including diffusion models, have shown high-quality text-to-image generation
capabilities. However, despite the realistic image generation results, there
has not been a detailed analysis of how to evaluate such models. In this work,
we investigate the visual reasoning capabilities and social biases of different
text-to-image models, covering both multimodal transformer language models and
diffusion models. First, we measure three visual reasoning skills: object
recognition, object counting, and spatial relation understanding. For this, we
propose PaintSkills, a compositional diagnostic evaluation dataset that
measures these skills. Despite the high-fidelity image generation capability, a
large gap exists between the performance of recent models and the upper bound
accuracy in object counting and spatial relation understanding skills. Second,
we assess the gender and skin tone biases by measuring the gender/skin tone
distribution of generated images across various professions and attributes. We
demonstrate that recent text-to-image generation models learn specific biases
about gender and skin tone from web image-text pairs. We hope our work will
help guide future progress in improving text-to-image generation models on
visual reasoning skills and learning socially unbiased representations. Code
and data: https://github.com/j-min/DallEval
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.00780">Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1">Giang Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Taesiri_M/0/1/0/all/0/1">Mohammad Reza Taesiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a></p>
<p>Explaining artificial intelligence (AI) predictions is increasingly important
and even imperative in many high-stakes applications where humans are the
ultimate decision-makers. In this work, we propose two novel architectures of
self-interpretable image classifiers that first explain, and then predict (as
opposed to post-hoc explanations) by harnessing the visual correspondences
between a query image and exemplars. Our models consistently improve (by 1 to 4
points) on out-of-distribution (OOD) datasets while performing marginally worse
(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest
neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,
our correspondence-based explanations are found to be more useful to users than
kNN explanations. Our explanations help users more accurately reject AI's wrong
decisions than all other tested methods. Interestingly, for the first time, we
show that it is possible to achieve complementary human-AI team accuracy (i.e.,
that is higher than either AI-alone or human-alone), in ImageNet and CUB image
classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.02796">Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borycki_P/0/1/0/all/0/1">Piotr Borycki</a>, <a href="http://arxiv.org/find/cs/1/au:+Kubacki_P/0/1/0/all/0/1">Piotr Kubacki</a>, <a href="http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1">Marcin Przewi&#x119;&#x17a;likowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusmierczyk_T/0/1/0/all/0/1">Tomasz Ku&#x15b;mierczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1">Jacek Tabor</a>, <a href="http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1">Przemys&#x142;aw Spurek</a></p>
<p>The main goal of Few-Shot learning algorithms is to enable learning from
small amounts of data. One of the most popular and elegant Few-Shot learning
approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this
method is to learn the shared universal weights of a meta-model, which are then
adapted for specific tasks. However, the method suffers from over-fitting and
poorly quantifies uncertainty due to limited data size. Bayesian approaches
could, in principle, alleviate these shortcomings by learning weight
distributions in place of point-wise weights. Unfortunately, previous
modifications of MAML are limited due to the simplicity of Gaussian posteriors,
MAML-like gradient-based weight updates, or by the same structure enforced for
universal and adapted weights.
</p>
<p>In this paper, we propose a novel framework for Bayesian MAML called
BayesianHMAML, which employs Hypernetworks for weight updates. It learns the
universal weights point-wise, but a probabilistic structure is added when
adapted for specific tasks. In such a framework, we can use simple Gaussian
distributions or more complicated posteriors induced by Continuous Normalizing
Flows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00646">Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v3 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Tada_M/0/1/0/all/0/1">Mikio Tada</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lang_U/0/1/0/all/0/1">Ursula E. Lang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yeh_I/0/1/0/all/0/1">Iwei Yeh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wei_M/0/1/0/all/0/1">Maria L. Wei</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Keiser_M/0/1/0/all/0/1">Michael J. Keiser</a></p>
<p>Melanoma is one of the most aggressive forms of skin cancer, causing a large
proportion of skin cancer deaths. However, melanoma diagnoses by pathologists
shows low interrater reliability. As melanoma is a cancer of the melanocyte,
there is a clear need to develop a melanocytic cell segmentation tool that is
agnostic to pathologist variability and automates pixel-level annotation.
Gigapixel-level pathologist labeling, however, is impractical. Herein, we
propose a means to train deep neural networks for melanocytic cell segmentation
from hematoxylin and eosin (H&amp;E) stained sections and paired
immunohistochemistry (IHC) of adjacent tissue sections, achieving a mean IOU of
0.64 despite imperfect ground-truth labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11483">Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1">Matthew Shardlow</a>, <a href="http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1">Piotr Przyby&#x142;a</a></p>
<p>This work is intended as a voice in the discussion over previous claims that
a pretrained large language model (LLM) based on the Transformer model
architecture can be sentient. Such claims have been made concerning the LaMDA
model and also concerning the current wave of LLM-powered chatbots, such as
ChatGPT. This claim, if confirmed, would have serious ramifications in the
Natural Language Processing (NLP) community due to wide-spread use of similar
models. However, here we take the position that such a large language model
cannot be sentient, or conscious, and that LaMDA in particular exhibits no
advances over other similar models that would qualify it. We justify this by
analysing the Transformer architecture through Integrated Information Theory of
consciousness. We see the claims of sentience as part of a wider tendency to
use anthropomorphic language in NLP reporting. Regardless of the veracity of
the claims, we consider this an opportune moment to take stock of progress in
language modelling and consider the ethical implications of the task. In order
to make this work helpful for readers outside the NLP community, we also
present the necessary background in language modelling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00752">Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v3 [cs.NI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ohta_S/0/1/0/all/0/1">Shoki Ohta</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1">Takayuki Nishio</a>, <a href="http://arxiv.org/find/cs/1/au:+Kudo_R/0/1/0/all/0/1">Riichi Kudo</a>, <a href="http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1">Kahoko Takahashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagata_H/0/1/0/all/0/1">Hisashi Nagata</a></p>
<p>This study demonstrates the feasibility of point cloud-based proactive link
quality prediction for millimeter-wave (mmWave) communications. Previous
studies have proposed machine learning-based methods to predict received signal
strength for future time periods using time series of depth images to mitigate
the line-of-sight (LOS) path blockage by pedestrians in mmWave communication.
However, these image-based methods have limited applicability due to privacy
concerns as camera images may contain sensitive information. This study
proposes a point cloud-based method for mmWave link quality prediction and
demonstrates its feasibility through experiments. Point clouds represent
three-dimensional (3D) spaces as a set of points and are sparser and less
likely to contain sensitive information than camera images. Additionally, point
clouds provide 3D position and motion information, which is necessary for
understanding the radio propagation environment involving pedestrians. This
study designs the mmWave link quality prediction method and conducts realistic
indoor experiments, where the link quality fluctuates significantly due to
human blockage, using commercially available IEEE 802.11ad-based 60 GHz
wireless LAN devices and Kinect v2 RGB-D camera and Velodyne VLP-16 light
detection and ranging (LiDAR) for point cloud acquisition. The experimental
results showed that our proposed method can predict future large attenuation of
mmWave received signal strength and throughput induced by the LOS path blockage
by pedestrians with comparable or superior accuracy to image-based prediction
methods. Hence, our point cloud-based method can serve as a viable alternative
to image-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06677">System identification of neural systems: If we got it right, would we know?. (arXiv:2302.06677v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Han_Y/0/1/0/all/0/1">Yena Han</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Poggio_T/0/1/0/all/0/1">Tomaso Poggio</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Cheung_B/0/1/0/all/0/1">Brian Cheung</a></p>
<p>Artificial neural networks are being proposed as models of parts of the
brain. The networks are compared to recordings of biological neurons, and good
performance in reproducing neural responses is considered to support the
model's validity. A key question is how much this system identification
approach tells us about brain computation. Does it validate one model
architecture over another? We evaluate the most commonly used comparison
techniques, such as a linear encoding model and centered kernel alignment, to
correctly identify a model by replacing brain recordings with known ground
truth models. System identification performance is quite variable; it also
depends significantly on factors independent of the ground truth architecture,
such as stimuli images. In addition, we show the limitations of using
functional similarity scores in identifying higher-level architectural motifs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12977">Fair Attribute Completion on Graph with Missing Attributes. (arXiv:2302.12977v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Dongliang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a></p>
<p>Tackling unfairness in graph learning models is a challenging task, as the
unfairness issues on graphs involve both attributes and topological structures.
Existing work on fair graph learning simply assumes that attributes of all
nodes are available for model training and then makes fair predictions. In
practice, however, the attributes of some nodes might not be accessible due to
missing data or privacy concerns, which makes fair graph learning even more
challenging. In this paper, we propose FairAC, a fair attribute completion
method, to complement missing information and learn fair node embeddings for
graphs with missing attributes. FairAC adopts an attention mechanism to deal
with the attribute missing problem and meanwhile, it mitigates two types of
unfairness, i.e., feature unfairness from attributes and topological unfairness
due to attribute completion. FairAC can work on various types of homogeneous
graphs and generate fair embeddings for them and thus can be applied to most
downstream tasks to improve their fairness performance. To our best knowledge,
FairAC is the first method that jointly addresses the graph attribution
completion and graph unfairness problems. Experimental results on benchmark
datasets show that our method achieves better fairness performance with less
sacrifice in accuracy, compared with the state-of-the-art methods of fair graph
learning. Code is available at: https://github.com/donglgcn/FairAC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14702">Performance Limits of a Deep Learning-Enabled Text Semantic Communication under Interference. (arXiv:2302.14702v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Getu_T/0/1/0/all/0/1">Tilahun M. Getu</a>, <a href="http://arxiv.org/find/eess/1/au:+Saad_W/0/1/0/all/0/1">Walid Saad</a>, <a href="http://arxiv.org/find/eess/1/au:+Kaddoum_G/0/1/0/all/0/1">Georges Kaddoum</a>, <a href="http://arxiv.org/find/eess/1/au:+Bennis_M/0/1/0/all/0/1">Mehdi Bennis</a></p>
<p>A deep learning (DL)-enabled semantic communication (SemCom) has emerged as a
6G enabler while promising to minimize power usage, bandwidth consumption, and
transmission delay by minimizing irrelevant information transmission. However,
the benefits of such a semantic-centric design can be limited by radio
frequency interference (RFI) that causes substantial semantic noise. The impact
of semantic noise due to interference can be alleviated using an
interference-resistant and robust (IR$^2$) SemCom design. Nevertheless, no such
design exists yet. To shed light on this knowledge gap and stimulate
fundamental research on IR$^2$ SemCom, the performance limits of a text SemCom
system named DeepSC are studied in the presence of (multi-interferer) RFI. By
introducing a principled probabilistic framework for SemCom, we show that
DeepSC produces semantically irrelevant sentences as the power of
(multi-interferer) RFI gets very large. We also derive DeepSC's practical
limits and a lower bound on its outage probability under multi-interferer RFI.
Toward a fundamental 6G design for an IR$^2$ SemCom, moreover, we propose a
generic lifelong DL-based IR$^2$ SemCom system. Eventually, we corroborate the
derived performance limits with Monte Carlo simulations and computer
experiments, which also affirm the vulnerability of DeepSC and DL-enabled text
SemCom to a wireless attack using RFI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08566">Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning. (arXiv:2303.08566v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jianfei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Visual Parameter-Efficient Fine-Tuning (PEFT) has become a powerful
alternative for full fine-tuning so as to adapt pre-trained vision models to
downstream tasks, which only tunes a small number of parameters while freezing
the vast majority ones to ease storage burden and optimization difficulty.
However, existing PEFT methods introduce trainable parameters to the same
positions across different tasks depending solely on human heuristics and
neglect the domain gaps. To this end, we study where to introduce and how to
allocate trainable parameters by proposing a novel Sensitivity-aware visual
Parameter-efficient fine-Tuning (SPT) scheme, which adaptively allocates
trainable parameters to task-specific important positions given a desired
tunable parameter budget. Specifically, our SPT first quickly identifies the
sensitive parameters that require tuning for a given task in a data-dependent
way. Next, our SPT further boosts the representational capability for the
weight matrices whose number of sensitive parameters exceeds a pre-defined
threshold by utilizing existing structured tuning methods, e.g., LoRA [23] or
Adapter [22], to replace directly tuning the selected sensitive parameters
(unstructured tuning) under the budget. Extensive experiments on a wide range
of downstream recognition tasks show that our SPT is complementary to the
existing PEFT methods and largely boosts their performance, e.g., SPT improves
Adapter with supervised pre-trained ViT-B/16 backbone by 4.2% and 1.4% mean
Top-1 accuracy, reaching SOTA performance on FGVC and VTAB-1k benchmarks,
respectively. Source code is at https://github.com/ziplab/SPT
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12743">DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jungwook Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jaeill Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungeun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1">Hyunghun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1">Wonjong Rhee</a></p>
<p>In autonomous driving, data augmentation is commonly used for improving 3D
object detection. The most basic methods include insertion of copied objects
and rotation and scaling of the entire training frame. Numerous variants have
been developed as well. The existing methods, however, are considerably limited
when compared to the variety of the real world possibilities. In this work, we
develop a diversified and realistic augmentation method that can flexibly
construct a whole-body object, freely locate and rotate the object, and apply
self-occlusion and external-occlusion accordingly. To improve the diversity of
the whole-body object construction, we develop an iterative method that
stochastically combines multiple objects observed from the real world into a
single object. Unlike the existing augmentation methods, the constructed
objects can be randomly located and rotated in the training frame because
proper occlusions can be reflected to the whole-body objects in the final step.
Finally, proper self-occlusion at each local object level and
external-occlusion at the global frame level are applied using the Hidden Point
Removal (HPR) algorithm that is computationally efficient. HPR is also used for
adaptively controlling the point density of each object according to the
object's distance from the LiDAR. Experiment results show that the proposed
DR.CPO algorithm is data-efficient and model-agnostic without incurring any
computational overhead. Also, DR.CPO can improve mAP performance by 2.08% when
compared to the best 3D detection result known for KITTI dataset. The code is
available at https://github.com/SNU-DRL/DRCPO.git
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15487">Knowledge Enhanced Graph Neural Networks for Graph Completion. (arXiv:2303.15487v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Werner_L/0/1/0/all/0/1">Luisa Werner</a> (TYREX, UGA), <a href="http://arxiv.org/find/cs/1/au:+Layaida_N/0/1/0/all/0/1">Nabil Laya&#xef;da</a> (TYREX), <a href="http://arxiv.org/find/cs/1/au:+Geneves_P/0/1/0/all/0/1">Pierre Genev&#xe8;s</a> (CNRS, TYREX), <a href="http://arxiv.org/find/cs/1/au:+Chlyah_S/0/1/0/all/0/1">Sarah Chlyah</a> (TYREX)</p>
<p>Graph data is omnipresent and has a wide variety of applications, such as in
natural science, social networks, or the semantic web. However, while being
rich in information, graphs are often noisy and incomplete. As a result, graph
completion tasks, such as node classification or link prediction, have gained
attention. On one hand, neural methods, such as graph neural networks, have
proven to be robust tools for learning rich representations of noisy graphs. On
the other hand, symbolic methods enable exact reasoning on graphs.We propose
Knowledge Enhanced Graph Neural Networks (KeGNN), a neuro-symbolic framework
for graph completion that combines both paradigms as it allows for the
integration of prior knowledge into a graph neural network model.Essentially,
KeGNN consists of a graph neural network as a base upon which knowledge
enhancement layers are stacked with the goal of refining predictions with
respect to prior knowledge.We instantiate KeGNN in conjunction with two
state-of-the-art graph neural networks, Graph Convolutional Networks and Graph
Attention Networks, and evaluate KeGNN on multiple benchmark datasets for node
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01559">G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System. (arXiv:2304.01559v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lixia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianlin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Junhong Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Haoyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jianbin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Haomin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Chao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shu He</a></p>
<p>Text-based delivery addresses, as the data foundation for logistics systems,
contain abundant and crucial location information. How to effectively encode
the delivery address is a core task to boost the performance of downstream
tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural
Language Process (NLP) have emerged as the dominant tools for encoding semantic
information in text. Though promising, those NLP-based PTMs fall short of
encoding geographic knowledge in the delivery address, which considerably trims
down the performance of delivery-related tasks in logistic systems such as
Cainiao. To tackle the above problem, we propose a domain-specific pre-trained
model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in
Logistics field. G2PTL combines the semantic learning capabilities of text
pre-training with the geographical-relationship encoding abilities of graph
modeling. Specifically, we first utilize real-world logistics delivery data to
construct a large-scale heterogeneous graph of delivery addresses, which
contains abundant geographic knowledge and delivery information. Then, G2PTL is
pre-trained with subgraphs sampled from the heterogeneous graph. Comprehensive
experiments are conducted to demonstrate the effectiveness of G2PTL through
four downstream tasks in logistics systems on real-world datasets. G2PTL has
been deployed in production in Cainiao's logistics system, which significantly
improves the performance of delivery-related tasks. The code of G2PTL is
available at https://huggingface.co/Cainiao-AI/G2PTL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11938">Is ChatGPT the Ultimate Programming Assistant -- How far is it?. (arXiv:2304.11938v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1">Haoye Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Weiqi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tsz On Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xunzhu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1">Shing-Chi Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1">Jacques Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Bissyande_T/0/1/0/all/0/1">Tegawend&#xe9; F. Bissyand&#xe9;</a></p>
<p>Recently, the ChatGPT LLM has received great attention: it can be used as a
bot for discussing source code, prompting it to suggest changes, provide
descriptions or even generate code. Typical demonstrations generally focus on
existing benchmarks, which may have been used in model training (i.e., data
leakage). To assess the feasibility of using an LLM as a useful assistant bot
for programmers, we must assess its realistic capabilities on unseen problems
as well as its capabilities on various tasks. In this paper, we present an
empirical study of ChatGPT's potential as a fully automated programming
assistant, focusing on the tasks of code generation, program repair, and code
summariziation. The study investigates ChatGPT's performance on common
programming problems and compares it with state-of-the-art approaches on two
benchmarks. Among several findings, our study shows that ChatGPT is effective
in dealing with common programming problems. However, our experiments also
reveal limitations in terms of its attention span: detailed descriptions will
constrain the focus of ChatGPT and prevent it from leveraging its vast
knowledge to solve the actual problem. Surprisingly, we have identified the
ability of ChatGPT to reason the original intention of the code. We expect
future work to build on this insight for dealing with the open question of the
oracle problem. Our findings contribute interesting insights to the development
of LLMs for programming assistance, notably by demonstrating the importance of
prompt engineering, and providing a better understanding of ChatGPT's practical
applications for software engineering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19979">Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks. (arXiv:2305.19979v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gema_A/0/1/0/all/0/1">Aryo Pradipta Gema</a>, <a href="http://arxiv.org/find/cs/1/au:+Grabarczyk_D/0/1/0/all/0/1">Dominik Grabarczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulf_W/0/1/0/all/0/1">Wolf De Wulf</a>, <a href="http://arxiv.org/find/cs/1/au:+Borole_P/0/1/0/all/0/1">Piyush Borole</a>, <a href="http://arxiv.org/find/cs/1/au:+Alfaro_J/0/1/0/all/0/1">Javier Antonio Alfaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1">Antonio Vergari</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1">Ajitha Rajan</a></p>
<p>Knowledge graphs are powerful tools for representing and organising complex
biomedical data. Several knowledge graph embedding algorithms have been
proposed to learn from and complete knowledge graphs. However, a recent study
demonstrates the limited efficacy of these embedding algorithms when applied to
biomedical knowledge graphs, raising the question of whether knowledge graph
embeddings have limitations in biomedical settings. This study aims to apply
state-of-the-art knowledge graph embedding models in the context of a recent
biomedical knowledge graph, BioKG, and evaluate their performance and potential
downstream uses. We achieve a three-fold improvement in terms of performance
based on the HITS@10 score over previous work on the same biomedical knowledge
graph. Additionally, we provide interpretable predictions through a rule-based
method. We demonstrate that knowledge graph embedding models are applicable in
practice by evaluating the best-performing model on four tasks that represent
real-life polypharmacy situations. Results suggest that knowledge learnt from
large biomedical knowledge graphs can be transferred to such downstream use
cases. Our code is available at https://github.com/aryopg/biokge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03604">Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Bin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zihao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuanhang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zenglin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bin Liu</a></p>
<p>Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, they require a significant amount
of storage space that can only be deployed on remote cloud server nodes.
Additionally, using commercial LLMs can be costly since they may charge based
on usage frequency. In this paper, we explore how to enable intelligent
cost-effective interactions between the agent and an LLM. We propose When2Ask,
a reinforcement learning based approach that learns when it is necessary to
query LLMs for high-level instructions to accomplish a target task. Experiments
on MiniGrid and Habitat environments that entail planning sub-goals demonstrate
that When2Ask learns to solve target tasks with only a few necessary
interactions with an LLM, and significantly reduces interaction costs in
testing environments compared with baseline methods. Experiment results also
suggest that by learning a mediator model to interact with the LLM, the agent's
performance becomes more robust against partial observability of the
environment. Our code is available at https://github.com/ZJLAB-AMMI/LLM4RL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06777">Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nemecek_J/0/1/0/all/0/1">Jiri Nemecek</a>, <a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1">Tomas Pevny</a>, <a href="http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1">Jakub Marecek</a></p>
<p>In classification and forecasting with tabular data, one often utilizes
tree-based models. Those can be competitive with deep neural networks on
tabular data [cf. Grinsztajn et al., NeurIPS 2022, <a href="/abs/2207.08815">arXiv:2207.08815</a>] and, under
some conditions, explainable. The explainability depends on the depth of the
tree and the accuracy in each leaf of the tree. Decision trees containing
leaves with unbalanced accuracy can provide misleading explanations.
Low-accuracy leaves give less valid explanations, which could be interpreted as
unfairness among explanations. Here, we train a shallow tree with the objective
of minimizing the maximum misclassification error across each leaf node. Then,
we extend each leaf with a separate tree-based model. The shallow tree provides
a global explanation, while the overall statistical performance of the shallow
tree with extended leaves improves upon decision trees of unlimited depth
trained using classical methods (e.g., CART) and is comparable to
state-of-the-art methods (e.g., well-tuned XGBoost).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08149">Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wortwein_T/0/1/0/all/0/1">Torsten W&#xf6;rtwein</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1">Nicholas Allen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheeber_L/0/1/0/all/0/1">Lisa B. Sheeber</a>, <a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1">Randy P. Auerbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohn_J/0/1/0/all/0/1">Jeffrey F. Cohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1">Louis-Philippe Morency</a></p>
<p>Personalized prediction is a machine learning approach that predicts a
person's future observations based on their past labeled observations and is
typically used for sequential tasks, e.g., to predict daily mood ratings. When
making personalized predictions, a model can combine two types of trends: (a)
trends shared across people, i.e., person-generic trends, such as being happier
on weekends, and (b) unique trends for each person, i.e., person-specific
trends, such as a stressful weekly meeting. Mixed effect models are popular
statistical models to study both trends by combining person-generic and
person-specific parameters. Though linear mixed effect models are gaining
popularity in machine learning by integrating them with neural networks, these
integrations are currently limited to linear person-specific parameters: ruling
out nonlinear person-specific trends. In this paper, we propose Neural Mixed
Effect (NME) models to optimize nonlinear person-specific parameters anywhere
in a neural network in a scalable manner. NME combines the efficiency of neural
network optimization with nonlinear mixed effects modeling. Empirically, we
observe that NME improves performance across six unimodal and multimodal
datasets, including a smartphone dataset to predict daily mood and a
mother-adolescent dataset to predict affective state sequences where half the
mothers experience at least moderate symptoms of depression. Furthermore, we
evaluate NME for two model architectures, including for neural conditional
random fields (CRF) to predict affective state sequences where the CRF learns
nonlinear person-specific temporal transitions between affective states.
Analysis of these person-specific transitions on the mother-adolescent dataset
shows interpretable trends related to the mother's depression symptoms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00925">Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1">Jorge Martinez-Gil</a></p>
<p>Semantic similarity measures are widely used in natural language processing
to catalyze various computer-related tasks. However, no single semantic
similarity measure is the most appropriate for all tasks, and researchers often
use ensemble strategies to ensure performance. This research work proposes a
method for automatically designing semantic similarity ensembles. In fact, our
proposed method uses grammatical evolution, for the first time, to
automatically select and aggregate measures from a pool of candidates to create
an ensemble that maximizes correlation to human judgment. The method is
evaluated on several benchmark datasets and compared to state-of-the-art
ensembles, showing that it can significantly improve similarity assessment
accuracy and outperform existing methods in some cases. As a result, our
research demonstrates the potential of using grammatical evolution to
automatically compare text and prove the benefits of using ensembles for
semantic similarity tasks. The source code that illustrates our approach can be
downloaded from https://github.com/jorge-martinez-gil/sesige.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04726">Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ada_S/0/1/0/all/0/1">Suzan Ece Ada</a>, <a href="http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1">Erhan Oztop</a>, <a href="http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1">Emre Ugur</a></p>
<p>Offline Reinforcement Learning (RL) methods leverage previous experiences to
learn better policies than the behavior policy used for data collection. In
contrast to behavior cloning, which assumes the data is collected from expert
demonstrations, offline RL can work with non-expert data and multimodal
behavior policies. However, offline RL algorithms face challenges in handling
distribution shifts and effectively representing policies due to the lack of
online interaction during training. Prior work on offline RL uses conditional
diffusion models to represent multimodal behavior in the dataset. Nevertheless,
these methods are not tailored toward alleviating the out-of-distribution state
generalization. We introduce a novel method, named State Reconstruction for
Diffusion Policies (SRDP), incorporating state reconstruction feature learning
in the recent class of diffusion policies to address the out-of-distribution
generalization problem. State reconstruction loss promotes more descriptive
representation learning of states to alleviate the distribution shift incurred
by the out-of-distribution (OOD) states. We design a novel 2D Multimodal
Contextual Bandit environment to illustrate the OOD generalization of SRDP
compared to prior algorithms. In addition, we assess the performance of our
model on D4RL continuous control benchmarks, namely the navigation of an 8-DoF
ant and forward locomotion of half-cheetah, hopper, and walker2d, achieving
state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10811">&quot;It Felt Like Having a Second Mind&quot;: Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1">Qian Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Siying Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Piaohong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1">Bo Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhicong Lu</a></p>
<p>Prewriting is the process of discovering and developing ideas before a first
draft, which requires divergent thinking and often implies unstructured
strategies such as diagramming, outlining, free-writing, etc. Although large
language models (LLMs) have been demonstrated to be useful for a variety of
tasks including creative writing, little is known about how users would
collaborate with LLMs to support prewriting. The preferred collaborative role
and initiative of LLMs during such a creativity process is also unclear. To
investigate human-LLM collaboration patterns and dynamics during prewriting, we
conducted a three-session qualitative study with 15 participants in two
creative tasks: story writing and slogan writing. The findings indicated that
during collaborative prewriting, there appears to be a three-stage iterative
Human-AI Co-creativity process that includes Ideation, Illumination, and
Implementation stages. This collaborative process champions the human in a
dominant role, in addition to mixed and shifting levels of initiative that
exist between humans and LLMs. This research also reports on collaboration
breakdowns that occur during this process, user perceptions of using existing
LLMs during Human-AI Co-creativity, and discusses design implications to
support this co-creativity process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.05962">Decentralised Governance for Foundation Model based AI Systems: Exploring the Role of Blockchain in Responsible AI. (arXiv:2308.05962v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1">Hye-Young Paik</a></p>
<p>Foundation models including large language models (LLMs) are increasingly
attracting interest worldwide for their distinguished capabilities and
potential to perform a wide variety of tasks. Nevertheless, people are
concerned about whether foundation model based AI systems are properly governed
to ensure trustworthiness of foundation model based AI systems and to prevent
misuse that could harm humans, society and the environment. In this paper, we
identify eight governance challenges of foundation model based AI systems
regarding the three fundamental dimensions of governance: decision rights,
incentives, and accountability. Furthermore, we explore the potential of
blockchain as a solution to address the challenges by providing a distributed
ledger to facilitate decentralised governance. We present an architecture that
demonstrates how blockchain can be leveraged to realise governance in
foundation model based AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07462">Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Reviriego_P/0/1/0/all/0/1">Pedro Reviriego</a>, <a href="http://arxiv.org/find/cs/1/au:+Conde_J/0/1/0/all/0/1">Javier Conde</a>, <a href="http://arxiv.org/find/cs/1/au:+Merino_Gomez_E/0/1/0/all/0/1">Elena Merino-G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_G/0/1/0/all/0/1">Gonzalo Mart&#xed;nez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1">Jos&#xe9; Alberto Hern&#xe1;ndez</a></p>
<p>The introduction of Artificial Intelligence (AI) generative language models
such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has
triggered a revolution that can transform how text is generated. This has many
implications, for example, as AI-generated text becomes a significant fraction
of the text, would this have an effect on the language capabilities of readers
and also on the training of newer AI tools? Would it affect the evolution of
languages? Focusing on one specific aspect of the language: words; will the use
of tools such as ChatGPT increase or reduce the vocabulary used or the lexical
richness? This has implications for words, as those not included in
AI-generated content will tend to be less and less popular and may eventually
be lost. In this work, we perform an initial comparison of the vocabulary and
lexical richness of ChatGPT and humans when performing the same tasks. In more
detail, two datasets containing the answers to different types of questions
answered by ChatGPT and humans, and a third dataset in which ChatGPT
paraphrases sentences and questions are used. The analysis shows that ChatGPT
tends to use fewer distinct words and lower lexical richness than humans. These
results are very preliminary and additional datasets and ChatGPT configurations
have to be evaluated to extract more general conclusions. Therefore, further
research is needed to understand how the use of ChatGPT and more broadly
generative AI tools will affect the vocabulary and lexical richness in
different types of text and languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10959">DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Sijin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1">Teng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shikun Feng</a></p>
<p>In this paper, we propose Docprompt for document question answering tasks
with powerful zero-shot and few-shot performance. We proposed a novel weakly
supervised data generation method, a novel multl-stage training method and a
novel understanding model \&amp; generation model ensemble method. We achieved
state-of-the-art performance on 4 document question answering tasks. This
method greatly improves the delivery efficiency and model performance of
document question answering customer projects, reducing annotation costs and
labor costs. Our demo can be found at
https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11029">RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Guoheng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fenghuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaochen Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1">Chi-Man Pun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_G/0/1/0/all/0/1">Guo Zhong</a></p>
<p>Emotion recognition in conversation (ERC) has received increasing attention
from researchers due to its wide range of applications.As conversation has a
natural graph structure,numerous approaches used to model ERC based on graph
convolutional networks (GCNs) have yielded significant results.However,the
aggregation approach of traditional GCNs suffers from the node information
redundancy problem,leading to node discriminant information
loss.Additionally,single-layer GCNs lack the capacity to capture long-range
contextual information from the graph. Furthermore,the majority of approaches
are based on textual modality or stitching together different modalities,
resulting in a weak ability to capture interactions between modalities. To
address these problems, we present the relational bilevel aggregation graph
convolutional network (RBA-GCN), which consists of three modules: the graph
generation module (GGM), similarity-based cluster building module (SCBM) and
bilevel aggregation module (BiAM). First, GGM constructs a novel graph to
reduce the redundancy of target node information.Then,SCBM calculates the node
similarity in the target node and its structural neighborhood, where noisy
information with low similarity is filtered out to preserve the discriminant
information of the node. Meanwhile, BiAM is a novel aggregation method that can
preserve the information of nodes during the aggregation process. This module
can construct the interaction between different modalities and capture
long-range contextual information based on similarity clusters. On both the
IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a
2.17$\sim$5.21\% improvement over that of the most advanced method.Our code is
available at https://github.com/luftmenscher/RBA-GCN and our article with the
same name has been published in IEEE/ACM Transactions on Audio,Speech,and
Language Processing,vol.31,2023
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11155">xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pengmei_Z/0/1/0/all/0/1">Zihan Pengmei</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1">Yinan Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Junyu Liu</a></p>
<p>Neural force fields (NFFs) have gained prominence in computational chemistry
as surrogate models, superseding quantum-chemistry calculations in ab initio
molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset
and its subsequent extension. These datasets predominantly comprise geometries
from the equilibrium region of the ground electronic state potential energy
surface, sampling from direct adiabatic dynamics. However, many chemical
reactions entail significant molecular deformations, notably bond breaking. We
demonstrate the constrained distribution of internal coordinates and energies
in the MD17 datasets, underscoring their inadequacy for representing systems
undergoing chemical reactions. Addressing this sampling limitation, we
introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived
from non-adiabatic dynamics. This dataset encompasses energies and forces
ascertained from both multireference wave function theory and density
functional theory. Furthermore, its nuclear configuration spaces authentically
depict chemical reactions, making xxMD a more chemically relevant dataset. Our
re-assessment of equivariant models on the xxMD datasets reveals notably higher
mean absolute errors than those reported for MD17 and its variants. This
observation underscores the challenges faced in crafting a generalizable NFF
model with extrapolation capability. Our proposed xxMD-CASSCF and xxMD-DFT
datasets are available at https://github.com/zpengmei/xxMD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12319">RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Hongwei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kunzhe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>With the performance of deep neural networks (DNNs) remarkably improving,
DNNs have been widely used in many areas. Consequently, the DNN model has
become a valuable asset, and its intellectual property is safeguarded by
ownership verification techniques (e.g., DNN fingerprinting). However, the
feasibility of the DNN fingerprint removal attack and its potential influence
remains an open problem. In this paper, we perform the first comprehensive
investigation of DNN fingerprint removal attacks. Generally, the knowledge
contained in a DNN model can be categorized into general semantic and
fingerprint-specific knowledge. To this end, we propose a min-max bilevel
optimization-based DNN fingerprint removal attack named RemovalNet, to evade
model ownership verification. The lower-level optimization is designed to
remove fingerprint-specific knowledge. While in the upper-level optimization,
we distill the victim model's general semantic knowledge to maintain the
surrogate model's performance. We conduct extensive experiments to evaluate the
fidelity, effectiveness, and efficiency of the RemovalNet against four advanced
defense methods on six metrics. The empirical results demonstrate that (1) the
RemovalNet is effective. After our DNN fingerprint removal attack, the model
distance between the target and surrogate models is x100 times higher than that
of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2%
(400 samples) of the substitute dataset and 1,000 iterations to conduct our
attack. Besides, compared with advanced model stealing attacks, the RemovalNet
saves nearly 85% of computational resources at most, (3) the RemovalNet
achieves high fidelity that the created surrogate model maintains high accuracy
after the DNN fingerprint removal process. Our code is available at:
https://github.com/grasses/RemovalNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13570">Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dianhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Felicetti_M/0/1/0/all/0/1">Matthew J. Felicetti</a></p>
<p>Real-time predictive modelling with desired accuracy is highly expected in
industrial artificial intelligence (IAI), where neural networks play a key
role. Neural networks in IAI require powerful, high-performance computing
devices to operate a large number of floating point data. Based on stochastic
configuration networks (SCNs), this paper proposes a new randomized learner
model, termed stochastic configuration machines (SCMs), to stress effective
modelling and data size saving that are useful and valuable for industrial
applications. Compared to SCNs and random vector functional-link (RVFL) nets
with binarized implementation, the model storage of SCMs can be significantly
compressed while retaining favourable prediction performance. Besides the
architecture of the SCM learner model and its learning algorithm, as an
important part of this contribution, we also provide a theoretical basis on the
learning capacity of SCMs by analysing the model's complexity. Experimental
studies are carried out over some benchmark datasets and three industrial
applications. The results demonstrate that SCM has great potential for dealing
with industrial data analytics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13916">Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Liang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jiazhen Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1">Chengsheng Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yuan Luo</a></p>
<p>Knowledge graphs play a vital role in numerous artificial intelligence tasks,
yet they frequently face the issue of incompleteness. In this study, we explore
utilizing Large Language Models (LLM) for knowledge graph completion. We
consider triples in knowledge graphs as text sequences and introduce an
innovative framework called Knowledge Graph LLM (KG-LLM) to model these
triples. Our technique employs entity and relation descriptions of a triple as
prompts and utilizes the response for predictions. Experiments on various
benchmark knowledge graphs demonstrate that our method attains state-of-the-art
performance in tasks such as triple classification and relation prediction. We
also find that fine-tuning relatively smaller models (e.g., LLaMA-7B,
ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14172">Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1">Bohan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Siheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiaowen Dong</a></p>
<p>Hypergraphs are important for processing data with higher-order relationships
involving more than two entities. In scenarios where explicit hypergraphs are
not readily available, it is desirable to infer a meaningful hypergraph
structure from the node features to capture the intrinsic relations within the
data. However, existing methods either adopt simple pre-defined rules that fail
to precisely capture the distribution of the potential hypergraph structure, or
learn a mapping between hypergraph structures and node features but require a
large amount of labelled data, i.e., pre-existing hypergraph structures, for
training. Both restrict their applications in practical scenarios. To fill this
gap, we propose a novel smoothness prior that enables us to design a method to
infer the probability for each potential hyperedge without labelled data as
supervision. The proposed prior indicates features of nodes in a hyperedge are
highly correlated by the features of the hyperedge containing them. We use this
prior to derive the relation between the hypergraph structure and the node
features via probabilistic modelling. This allows us to develop an unsupervised
inference method to estimate the probability for each potential hyperedge via
solving an optimisation problem that has an analytical solution. Experiments on
both synthetic and real-world data demonstrate that our method can learn
meaningful hypergraph structures from data more efficiently than existing
hypergraph structure inference methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14657">DeepHealthNet: Adolescent Obesity Prediction System Based on a Deep Learning Framework. (arXiv:2308.14657v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1">Ji-Hoon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">In-Gyu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sung-Kyung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kam_T/0/1/0/all/0/1">Tae-Eui Kam</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Euijong Lee</a></p>
<p>Childhood and adolescent obesity rates are a global concern because obesity
is associated with chronic diseases and long-term health risks. Artificial
intelligence technology has emerged as a promising solution to accurately
predict obesity rates and provide personalized feedback to adolescents. This
study emphasizes the importance of early identification and prevention of
obesity-related health issues. Factors such as height, weight, waist
circumference, calorie intake, physical activity levels, and other relevant
health information need to be considered for developing robust algorithms for
obesity rate prediction and delivering personalized feedback. Hence, by
collecting health datasets from 321 adolescents, we proposed an adolescent
obesity prediction system that provides personalized predictions and assists
individuals in making informed health decisions. Our proposed deep learning
framework, DeepHealthNet, effectively trains the model using data augmentation
techniques, even when daily health data are limited, resulting in improved
prediction accuracy (acc: 0.8842). Additionally, the study revealed variations
in the prediction of the obesity rate between boys (acc: 0.9320) and girls
(acc: 0.9163), allowing the identification of disparities and the determination
of the optimal time to provide feedback. The proposed system shows significant
potential in effectively addressing childhood and adolescent obesity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15690">CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ban_B/0/1/0/all/0/1">Byunghyun Ban</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1">Donghun Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Su-won Hwang</a></p>
<p>We present 'CongNaMul', a comprehensive dataset designed for various tasks in
soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate
tasks such as image classification, semantic segmentation, decomposition, and
measurement of length and weight. The classification task provides four classes
to determine the quality of soybean sprouts: normal, broken, spotted, and
broken and spotted, for the development of AI-aided automatic quality
inspection technology. For semantic segmentation, images with varying
complexity, from single sprout images to images with multiple sprouts, along
with human-labelled mask images, are included. The label has 4 different
classes: background, head, body, tail. The dataset also provides images and
masks for the image decomposition task, including two separate sprout images
and their combined form. Lastly, 5 physical features of sprouts (head length,
body length, body thickness, tail length, weight) are provided for image-based
measurement tasks. This dataset is expected to be a valuable resource for a
wide range of research and applications in the advanced analysis of images of
soybean sprouts. Also, we hope that this dataset can assist researchers
studying classification, semantic segmentation, decomposition, and physical
feature measurement in other industrial fields, in evaluating their models. The
dataset is available at the authors' repository. (https://bhban.kr/data)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15962">WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model. (arXiv:2308.15962v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haitao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1">Xiangyang Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yanwei Fu</a></p>
<p>Enabling robots to understand language instructions and react accordingly to
visual perception has been a long-standing goal in the robotics research
community. Achieving this goal requires cutting-edge advances in natural
language processing, computer vision, and robotics engineering. Thus, this
paper mainly investigates the potential of integrating the most recent Large
Language Models (LLMs) and existing visual grounding and robotic grasping
system to enhance the effectiveness of the human-robot interaction. We
introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language
model) as an example of this integration. The system utilizes the LLM of
ChatGPT to summarize the preference object of the users as a target instruction
via the multi-round interactive dialogue. The target instruction is then
forwarded to a visual grounding system for object pose and size estimation,
following which the robot grasps the object accordingly. We deploy this
LLM-empowered system on the physical robot to provide a more user-friendly
interface for the instruction-guided grasping task. The further experimental
results on various real-world scenarios demonstrated the feasibility and
efficacy of our proposed framework. See the project website at:
https://star-uu-wang.github.io/WALL-E/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15975">RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation. (arXiv:2308.15975v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vecerik_M/0/1/0/all/0/1">Mel Vecerik</a>, <a href="http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1">Carl Doersch</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Davchev_T/0/1/0/all/0/1">Todor Davchev</a>, <a href="http://arxiv.org/find/cs/1/au:+Aytar_Y/0/1/0/all/0/1">Yusuf Aytar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guangyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1">Raia Hadsell</a>, <a href="http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1">Lourdes Agapito</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholz_J/0/1/0/all/0/1">Jon Scholz</a></p>
<p>For robots to be useful outside labs and specialized factories we need a way
to teach them new useful behaviors quickly. Current approaches lack either the
generality to onboard new tasks without task-specific engineering, or else lack
the data-efficiency to do so in an amount of time that enables practical use.
In this work we explore dense tracking as a representational vehicle to allow
faster and more general learning from demonstration. Our approach utilizes
Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration,
and parameterize a low-level controller to reproduce this motion across changes
in the scene configuration. We show this results in robust robot policies that
can solve complex object-arrangement tasks such as shape-matching, stacking,
and even full path-following tasks such as applying glue and sticking objects
together, all from demonstrations that can be collected in minutes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.11073">OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v3 [eess.AS] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jacqmin_L/0/1/0/all/0/1">L&#xe9;o Jacqmin</a>, <a href="http://arxiv.org/find/eess/1/au:+Druart_L/0/1/0/all/0/1">Lucas Druart</a> (LIA), <a href="http://arxiv.org/find/eess/1/au:+Esteve_Y/0/1/0/all/0/1">Yannick Est&#xe8;ve</a> (LIA), <a href="http://arxiv.org/find/eess/1/au:+Favre_B/0/1/0/all/0/1">Beno&#xee;t Favre</a>, <a href="http://arxiv.org/find/eess/1/au:+Rojas_Barahona_L/0/1/0/all/0/1">Lina Maria Rojas-Barahona</a>, <a href="http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1">Valentin Vielzeuf</a></p>
<p>Though Dialogue State Tracking (DST) is a core component of spoken dialogue
systems, recent work on this task mostly deals with chat corpora, disregarding
the discrepancies between spoken and written language.In this paper, we propose
OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR)
model and a DST model. We introduce several adaptations in the ASR and DST
modules to improve integration and robustness to spoken conversations.With
these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to
evaluate spoken DST. We conduct an in-depth analysis of the results and find
that normalizing the ASR outputs and adapting the DST inputs through data
augmentation, along with increasing the pre-trained models size all play an
important role in reducing the performance discrepancy between written and
spoken conversations.
</p>
</p>
</div>

    </div>
    </body>
    