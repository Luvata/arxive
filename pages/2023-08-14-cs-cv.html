<!DOCTYPE html>
<html>
<head>
<title>2023-08-14-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/1903.10360">Structured 2D Representation of 3D Data for Shape Processing. (arXiv:1903.10360v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1">Kripasindhu Sarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathews_E/0/1/0/all/0/1">Elizabeth Mathews</a>, <a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1">Didier Stricker</a></p>
<p>We represent 3D shape by structured 2D representations of fixed length making
it feasible to apply well investigated 2D convolutional neural networks (CNN)
for both discriminative and geometric tasks on 3D shapes. We first provide a
general introduction to such structured descriptors, analyze their different
forms and show how a simple 2D CNN can be used to achieve good classification
result. With a specialized classification network for images and our structured
representation, we achieve the classification accuracy of 99.7\% in the
ModelNet40 test set - improving the previous state-of-the-art by a large
margin. We finally provide a novel framework for performing the geometric task
of 3D segmentation using 2D CNNs and the structured representation - concluding
the utility of such descriptors for both discriminative and geometric tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2003.03229">Non-linear Neurons with Human-like Apical Dendrite Activations. (arXiv:2003.03229v5 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1">Mariana-Iuliana Georgescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1">Radu Tudor Ionescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1">Nicolae-Catalin Ristea</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1">Nicu Sebe</a></p>
<p>In order to classify linearly non-separable data, neurons are typically
organized into multi-layer neural networks that are equipped with at least one
hidden layer. Inspired by some recent discoveries in neuroscience, we propose a
new model of artificial neuron along with a novel activation function enabling
the learning of nonlinear decision boundaries using a single neuron. We show
that a standard neuron followed by our novel apical dendrite activation (ADA)
can learn the XOR logical function with 100% accuracy. Furthermore, we conduct
experiments on six benchmark data sets from computer vision, signal processing
and natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST,
Tiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions
provide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and
Swish, for various neural network architectures, e.g. one-hidden-layer or
two-hidden-layer multi-layer perceptrons (MLPs) and convolutional neural
networks (CNNs) such as LeNet, VGG, ResNet and Character-level CNN. We obtain
further performance improvements when we change the standard model of the
neuron with our pyramidal neuron with apical dendrite activations (PyNADA). Our
code is available at: https://github.com/raduionescu/pynada.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.01654">Towards Defending Multiple $\ell_p$-norm Bounded Adversarial Perturbations via Gated Batch Normalization. (arXiv:2012.01654v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Aishan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shiyu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1">Haotong Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianglong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>There has been extensive evidence demonstrating that deep neural networks are
vulnerable to adversarial examples, which motivates the development of defenses
against adversarial attacks. Existing adversarial defenses typically improve
model robustness against individual specific perturbation types (\eg,
$\ell_{\infty}$-norm bounded adversarial examples). However, adversaries are
likely to generate multiple types of perturbations in practice (\eg, $\ell_1$,
$\ell_2$, and $\ell_{\infty}$ perturbations). Some recent methods improve model
robustness against adversarial attacks in multiple $\ell_p$ balls, but their
performance against each perturbation type is still far from satisfactory. In
this paper, we observe that different $\ell_p$ bounded adversarial
perturbations induce different statistical properties that can be separated and
characterized by the statistics of Batch Normalization (BN). We thus propose
Gated Batch Normalization (GBN) to adversarially train a perturbation-invariant
predictor for defending multiple $\ell_p$ bounded adversarial perturbations.
GBN consists of a multi-branch BN layer and a gated sub-network. Each BN branch
in GBN is in charge of one perturbation type to ensure that the normalized
output is aligned towards learning perturbation-invariant representation.
Meanwhile, the gated sub-network is designed to separate inputs added with
different perturbation types. We perform an extensive evaluation of our
approach on commonly-used dataset including MNIST, CIFAR-10, and Tiny-ImageNet,
and demonstrate that GBN outperforms previous defense proposals against
multiple perturbation types (\ie, $\ell_1$, $\ell_2$, and $\ell_{\infty}$
perturbations) by large margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.13061">The Imaginative Generative Adversarial Network: Automatic Data Augmentation for Dynamic Skeleton-Based Hand Gesture and Human Action Recognition. (arXiv:2105.13061v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Junxiao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudley_J/0/1/0/all/0/1">John Dudley</a>, <a href="http://arxiv.org/find/cs/1/au:+Kristensson_P/0/1/0/all/0/1">Per Ola Kristensson</a></p>
<p>Deep learning approaches deliver state-of-the-art performance in recognition
of spatiotemporal human motion data. However, one of the main challenges in
these recognition tasks is limited available training data. Insufficient
training data results in over-fitting and data augmentation is one approach to
address this challenge. Existing data augmentation strategies based on scaling,
shifting and interpolating offer limited generalizability and typically require
detailed inspection of the dataset as well as hundreds of GPU hours for
hyperparameter optimization. In this paper, we present a novel automatic data
augmentation model, the Imaginative Generative Adversarial Network (GAN), that
approximates the distribution of the input data and samples new data from this
distribution. It is automatic in that it requires no data inspection and little
hyperparameter tuning and therefore it is a low-cost and low-effort approach to
generate synthetic data. We demonstrate our approach on small-scale
skeleton-based datasets with a comprehensive experimental analysis. Our results
show that the augmentation strategy is fast to train and can improve
classification accuracy for both conventional neural networks and
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.11011">CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition. (arXiv:2111.11011v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianlun Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhineng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Shancheng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hongtao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>The Transformer-based encoder-decoder framework is becoming popular in scene
text recognition, largely because it naturally integrates recognition clues
from both visual and semantic domains. However, recent studies show that the
two kinds of clues are not always well registered and therefore, feature and
character might be misaligned in difficult text (e.g., with a rare shape). As a
result, constraints such as character position are introduced to alleviate this
problem. Despite certain success, visual and semantic are still separately
modeled and they are merely loosely associated. In this paper, we propose a
novel module called Multi-Domain Character Distance Perception (MDCDP) to
establish a visually and semantically related position embedding. MDCDP uses
the position embedding to query both visual and semantic features following the
cross-attention mechanism. The two kinds of clues are fused into the position
branch, generating a content-aware embedding that well perceives character
spacing and orientation variants, character semantic affinities, and clues
tying the two kinds of information. They are summarized as the multi-domain
character distance. We develop CDistNet that stacks multiple MDCDPs to guide a
gradually precise distance modeling. Thus, the feature-character alignment is
well built even various recognition difficulties are presented. We verify
CDistNet on ten challenging public datasets and two series of augmented
datasets created by ourselves. The experiments demonstrate that CDistNet
performs highly competitively. It not only ranks top-tier in standard
benchmarks, but also outperforms recent popular methods by obvious margins on
real and augmented datasets presenting severe text deformation, poor linguistic
support, and rare character layouts. Code is available at
https://github.com/simplify23/CDistNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.07646">A Survey on Training Challenges in Generative Adversarial Networks for Biomedical Image Analysis. (arXiv:2201.07646v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saad_M/0/1/0/all/0/1">Muhammad Muneeb Saad</a>, <a href="http://arxiv.org/find/cs/1/au:+OReilly_R/0/1/0/all/0/1">Ruairi O&#x27;Reilly</a>, <a href="http://arxiv.org/find/cs/1/au:+Rehmani_M/0/1/0/all/0/1">Mubashir Husain Rehmani</a></p>
<p>In biomedical image analysis, the applicability of deep learning methods is
directly impacted by the quantity of image data available. This is due to deep
learning models requiring large image datasets to provide high-level
performance. Generative Adversarial Networks (GANs) have been widely utilized
to address data limitations through the generation of synthetic biomedical
images. GANs consist of two models. The generator, a model that learns how to
produce synthetic images based on the feedback it receives. The discriminator,
a model that classifies an image as synthetic or real and provides feedback to
the generator. Throughout the training process, a GAN can experience several
technical challenges that impede the generation of suitable synthetic imagery.
First, the mode collapse problem whereby the generator either produces an
identical image or produces a uniform image from distinct input features.
Second, the non-convergence problem whereby the gradient descent optimizer
fails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem
whereby unstable training behavior occurs due to the discriminator achieving
optimal classification performance resulting in no meaningful feedback being
provided to the generator. These problems result in the production of synthetic
imagery that is blurry, unrealistic, and less diverse. To date, there has been
no survey article outlining the impact of these technical challenges in the
context of the biomedical imagery domain. This work presents a review and
taxonomy based on solutions to the training problems of GANs in the biomedical
imaging domain. This survey highlights important challenges and outlines future
research directions about the training of GANs in the domain of biomedical
imagery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.09201">Vision-Based UAV Self-Positioning in Low-Altitude Urban Environments. (arXiv:2201.09201v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1">Ming Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_E/0/1/0/all/0/1">Enhui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhenhua Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1">Jiedong Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wankou Yang</a></p>
<p>Unmanned Aerial Vehicles (UAVs) rely on satellite systems for stable
positioning. However, due to limited satellite coverage or communication
disruptions, UAVs may lose signals from satellite-based positioning systems. In
such situations, vision-based techniques can serve as an alternative, ensuring
the self-positioning capability of UAVs. However, most of the existing datasets
are developed for the geo-localization tasks of the objects identified by UAVs,
rather than the self-positioning task of UAVs. Furthermore, the current UAV
datasets use discrete sampling on synthetic data, such as Google Maps, thereby
neglecting the crucial aspects of dense sampling and the uncertainties commonly
experienced in real-world scenarios. To address these issues, this paper
presents a new dataset, DenseUAV, which is the first publicly available dataset
designed for the UAV self-positioning task. DenseUAV adopts dense sampling on
UAV images obtained in low-altitude urban settings. In total, over 27K UAV-view
and satellite-view images of 14 university campuses are collected and
annotated, establishing a new benchmark. In terms of model development, we
first verify the superiority of Transformers over CNNs in this task. Then, we
incorporate metric learning into representation learning to enhance the
discriminative capacity of the model and to lessen the modality discrepancy.
Besides, to facilitate joint learning from both perspectives, we propose a
mutually supervised learning approach. Last, we enhance the Recall@K metric and
introduce a new measurement, SDM@K, to evaluate the performance of a trained
model from both the retrieval and localization perspectives simultaneously. As
a result, the proposed baseline method achieves a remarkable Recall@1 score of
83.05% and an SDM@1 score of 86.24% on DenseUAV. The dataset and code will be
made publicly available on https://github.com/Dmmm1997/DenseUAV.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.01558">Con$^{2}$DA: Simplifying Semi-supervised Domain Adaptation by Learning Consistent and Contrastive Feature Representations. (arXiv:2204.01558v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_Carrasco_M/0/1/0/all/0/1">Manuel P&#xe9;rez-Carrasco</a>, <a href="http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1">Pavlos Protopapas</a>, <a href="http://arxiv.org/find/cs/1/au:+Cabrera_Vives_G/0/1/0/all/0/1">Guillermo Cabrera-Vives</a></p>
<p>In this work, we present Con$^{2}$DA, a simple framework that extends recent
advances in semi-supervised learning to the semi-supervised domain adaptation
(SSDA) problem. Our framework generates pairs of associated samples by
performing stochastic data transformations to a given input. Associated data
pairs are mapped to a feature representation space using a feature extractor.
We use different loss functions to enforce consistency between the feature
representations of associated data pairs of samples. We show that these learned
representations are useful to deal with differences in data distributions in
the domain adaptation problem. We performed experiments to study the main
components of our model and we show that (i) learning of the consistent and
contrastive feature representations is crucial to extract good discriminative
features across different domains, and ii) our model benefits from the use of
strong augmentation policies. With these findings, our method achieves
state-of-the-art performances in three benchmark datasets for SSDA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.08247">Joint Multi-view Unsupervised Feature Selection and Graph Learning. (arXiv:2204.08247v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Si-Guo Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chang-Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yong Tang</a></p>
<p>Despite significant progress, previous multi-view unsupervised feature
selection methods mostly suffer from two limitations. First, they generally
utilize either cluster structure or similarity structure to guide the feature
selection, which neglect the possibility of a joint formulation with mutual
benefits. Second, they often learn the similarity structure by either global
structure learning or local structure learning, which lack the capability of
graph learning with both global and local structural awareness. In light of
this, this paper presents a joint multi-view unsupervised feature selection and
graph learning (JMVFG) approach. Particularly, we formulate the multi-view
feature selection with orthogonal decomposition, where each target matrix is
decomposed into a view-specific basis matrix and a view-consistent cluster
indicator. The cross-space locality preservation is incorporated to bridge the
cluster structure learning in the projected space and the similarity learning
(i.e., graph learning) in the original space. Further, a unified objective
function is presented to enable the simultaneous learning of the cluster
structure, the global and local similarity structures, and the multi-view
consistency and inconsistency, upon which an alternating optimization algorithm
is developed with theoretically proved convergence. Extensive experiments on a
variety of real-world multi-view datasets demonstrate the superiority of our
approach for both the multi-view feature selection and graph learning tasks.
The code is available at https://github.com/huangdonghere/JMVFG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.04531">ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Posada_Moreno_A/0/1/0/all/0/1">Andres Felipe Posada-Moreno</a>, <a href="http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1">Nikita Surya</a>, <a href="http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1">Sebastian Trimpe</a></p>
<p>Convolutional neural networks (CNNs) are increasingly being used in critical
systems, where robustness and alignment are crucial. In this context, the field
of explainable artificial intelligence has proposed the generation of
high-level explanations of the prediction process of CNNs through concept
extraction. While these methods can detect whether or not a concept is present
in an image, they are unable to determine its location. What is more, a fair
comparison of such approaches is difficult due to a lack of proper validation
procedures. To address these issues, we propose a novel method for automatic
concept extraction and localization based on representations obtained through
pixel-wise aggregations of CNN activation maps. Further, we introduce a process
for the validation of concept-extraction techniques based on synthetic datasets
with pixel-wise annotations of their main components, reducing the need for
human intervention. Extensive experimentation on both synthetic and real-world
datasets demonstrates that our method outperforms state-of-the-art
alternatives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.15157">HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection. (arXiv:2206.15157v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Broedermann_T/0/1/0/all/0/1">Tim Broedermann</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1">Christos Sakaridis</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Dengxin Dai</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a> (1 and 3) ((1) ETH Zurich, (2) MPI for Informatics, (3) KU Leuven)</p>
<p>Besides standard cameras, autonomous vehicles typically include multiple
additional sensors, such as lidars and radars, which help acquire richer
information for perceiving the content of the driving scene. While several
recent works focus on fusing certain pairs of sensors - such as camera with
lidar or radar - by using architectural components specific to the examined
setting, a generic and modular sensor fusion architecture is missing from the
literature. In this work, we propose HRFuser, a modular architecture for
multi-modal 2D object detection. It fuses multiple sensors in a
multi-resolution fashion and scales to an arbitrary number of input modalities.
The design of HRFuser is based on state-of-the-art high-resolution networks for
image-only dense prediction and incorporates a novel multi-window
cross-attention block as the means to perform fusion of multiple modalities at
multiple resolutions. We demonstrate via extensive experiments on nuScenes and
the adverse conditions DENSE datasets that our model effectively leverages
complementary features from additional modalities, substantially improving upon
camera-only performance and consistently outperforming state-of-the-art 3D and
2D fusion methods evaluated on 2D object detection metrics. The source code is
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.09668">Generalised Co-Salient Object Detection. (arXiv:2208.09668v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiawei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1">Ruikai Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weihao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1">Nick Barnes</a></p>
<p>We propose a new setting that relaxes an assumption in the conventional
Co-Salient Object Detection (CoSOD) setting by allowing the presence of "noisy
images" which do not show the shared co-salient object. We call this new
setting Generalised Co-Salient Object Detection (GCoSOD). We propose a novel
random sampling based Generalised CoSOD Training (GCT) strategy to distill the
awareness of inter-image absence of co-salient objects into CoSOD models. It
employs a Diverse Sampling Self-Supervised Learning (DS3L) that, in addition to
the provided supervised co-salient label, introduces additional self-supervised
labels for noisy images (being null, that no co-salient object is present).
Further, the random sampling process inherent in GCT enables the generation of
a high-quality uncertainty map highlighting potential false-positive
predictions at instance level. To evaluate the performance of CoSOD models
under the GCoSOD setting, we propose two new testing datasets, namely
CoCA-Common and CoCA-Zero, where a common salient object is partially present
in the former and completely absent in the latter. Extensive experiments
demonstrate that our proposed method significantly improves the performance of
CoSOD models in terms of the performance under the GCoSOD setting as well as
the model calibration degrees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.05483">Self-Supervised Coordinate Projection Network for Sparse-View Computed Tomography. (arXiv:2209.05483v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1">Qing Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1">Ruimin Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1">Hongjiang Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1">Jingyi Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yuyao Zhang</a></p>
<p>In the present work, we propose a Self-supervised COordinate Projection
nEtwork (SCOPE) to reconstruct the artifacts-free CT image from a single SV
sinogram by solving the inverse tomography imaging problem. Compared with
recent related works that solve similar problems using implicit neural
representation network (INR), our essential contribution is an effective and
simple re-projection strategy that pushes the tomography image reconstruction
quality over supervised deep learning CT reconstruction works. The proposed
strategy is inspired by the simple relationship between linear algebra and
inverse problems. To solve the under-determined linear equation system, we
first introduce INR to constrain the solution space via image continuity prior
and achieve a rough solution. And secondly, we propose to generate a dense view
sinogram that improves the rank of the linear equation system and produces a
more stable CT image solution space. Our experiment results demonstrate that
the re-projection strategy significantly improves the image reconstruction
quality (+3 dB for PSNR at least). Besides, we integrate the recent hash
encoding into our SCOPE model, which greatly accelerates the model training.
Finally, we evaluate SCOPE in parallel and fan X-ray beam SVCT reconstruction
tasks. Experimental results indicate that the proposed SCOPE model outperforms
two latest INR-based methods and two well-popular supervised DL methods
quantitatively and qualitatively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.10510">Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation. (arXiv:2209.10510v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1">Yu-Ying Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1">Koki Nagano</a>, <a href="http://arxiv.org/find/cs/1/au:+Khamis_S/0/1/0/all/0/1">Sameh Khamis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming-Yu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Ting-Chun Wang</a></p>
<p>Given a portrait image of a person and an environment map of the target
lighting, portrait relighting aims to re-illuminate the person in the image as
if the person appeared in an environment with the target lighting. To achieve
high-quality results, recent methods rely on deep learning. An effective
approach is to supervise the training of deep neural networks with a
high-fidelity dataset of desired input-output pairs, captured with a light
stage. However, acquiring such data requires an expensive special capture rig
and time-consuming efforts, limiting access to only a few resourceful
laboratories. To address the limitation, we propose a new approach that can
perform on par with the state-of-the-art (SOTA) relighting methods without
requiring a light stage. Our approach is based on the realization that a
successful relighting of a portrait image depends on two conditions. First, the
method needs to mimic the behaviors of physically-based relighting. Second, the
output has to be photorealistic. To meet the first condition, we propose to
train the relighting network with training data generated by a virtual light
stage that performs physically-based rendering on various 3D synthetic humans
under different environment maps. To meet the second condition, we develop a
novel synthetic-to-real approach to bring photorealism to the relighting
network output. In addition to achieving SOTA results, our approach offers
several advantages over the prior methods, including controllable glares on
glasses and more temporally-consistent results for relighting videos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.17020">A Law of Data Separation in Deep Learning. (arXiv:2210.17020v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hangfeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Weijie J. Su</a></p>
<p>While deep learning has enabled significant advances in many areas of
science, its black-box nature hinders architecture design for future artificial
intelligence applications and interpretation for high-stakes decision makings.
We addressed this issue by studying the fundamental question of how deep neural
networks process data in the intermediate layers. Our finding is a simple and
quantitative law that governs how deep neural networks separate data according
to class membership throughout all layers for classification. This law shows
that each layer improves data separation at a constant geometric rate, and its
emergence is observed in a collection of network architectures and datasets
during training. This law offers practical guidelines for designing
architectures, improving model robustness and out-of-sample performance, as
well as interpreting the predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08115">Heatmap-based Out-of-Distribution Detection. (arXiv:2211.08115v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1">Julia Hornauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1">Vasileios Belagiannis</a></p>
<p>Our work investigates out-of-distribution (OOD) detection as a neural network
output explanation problem. We learn a heatmap representation for detecting OOD
images while visualizing in- and out-of-distribution image regions at the same
time. Given a trained and fixed classifier, we train a decoder neural network
to produce heatmaps with zero response for in-distribution samples and high
response heatmaps for OOD samples, based on the classifier features and the
class prediction. Our main innovation lies in the heatmap definition for an OOD
sample, as the normalized difference from the closest in-distribution sample.
The heatmap serves as a margin to distinguish between in- and
out-of-distribution samples. Our approach generates the heatmaps not only for
OOD detection, but also to indicate in- and out-of-distribution regions of the
input image. In our evaluations, our approach mostly outperforms the prior work
on fixed classifiers, trained on CIFAR-10, CIFAR-100 and Tiny ImageNet. The
code is publicly available at: https://github.com/jhornauer/heatmap_ood.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04875">Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (arXiv:2212.04875v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luu_M/0/1/0/all/0/1">Minh-Long Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zeyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric P. Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yong Jae Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haohan Wang</a></p>
<p>Mix-up training approaches have proven to be effective in improving the
generalization ability of Deep Neural Networks. Over the years, the research
community expands mix-up methods into two directions, with extensive efforts to
improve saliency-guided procedures but minimal focus on the arbitrary path,
leaving the randomization domain unexplored. In this paper, inspired by the
superior qualities of each direction over one another, we introduce a novel
method that lies at the junction of the two routes. By combining the best
elements of randomness and saliency utilization, our method balances speed,
simplicity, and accuracy. We name our method R-Mix following the concept of
"Random Mix-up". We demonstrate its effectiveness in generalization, weakly
supervised object localization, calibration, and robustness to adversarial
attacks. Finally, in order to address the question of whether there exists a
better decision protocol, we train a Reinforcement Learning agent that decides
the mix-up policies based on the classifier's performance, reducing dependency
on human-designed objectives and hyperparameter tuning. Extensive experiments
further show that the agent is capable of performing at the cutting-edge level,
laying the foundation for a fully automatic mix-up. Our code is released at
[https://github.com/minhlong94/Random-Mixup].
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.06817">RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brohan_A/0/1/0/all/0/1">Anthony Brohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1">Noah Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Carbajal_J/0/1/0/all/0/1">Justice Carbajal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1">Yevgen Chebotar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabis_J/0/1/0/all/0/1">Joseph Dabis</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1">Keerthana Gopalakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1">Karol Hausman</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzog_A/0/1/0/all/0/1">Alex Herzog</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1">Jasmine Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1">Julian Ibarz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1">Brian Ichter</a>, <a href="http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1">Alex Irpan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1">Tomas Jackson</a>, <a href="http://arxiv.org/find/cs/1/au:+Jesmonth_S/0/1/0/all/0/1">Sally Jesmonth</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1">Nikhil J Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1">Ryan Julian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1">Dmitry Kalashnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1">Yuheng Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1">Isabel Leal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kuang-Huei Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Malla_U/0/1/0/all/0/1">Utsav Malla</a>, <a href="http://arxiv.org/find/cs/1/au:+Manjunath_D/0/1/0/all/0/1">Deeksha Manjunath</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1">Ofir Nachum</a>, <a href="http://arxiv.org/find/cs/1/au:+Parada_C/0/1/0/all/0/1">Carolina Parada</a>, <a href="http://arxiv.org/find/cs/1/au:+Peralta_J/0/1/0/all/0/1">Jodilyn Peralta</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1">Emily Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1">Karl Pertsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Quiambao_J/0/1/0/all/0/1">Jornell Quiambao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1">Kanishka Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael Ryoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Salazar_G/0/1/0/all/0/1">Grecia Salazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1">Pannag Sanketi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sayed_K/0/1/0/all/0/1">Kevin Sayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1">Jaspiar Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1">Sumedh Sontakke</a>, <a href="http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1">Austin Stone</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Clayton Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1">Huong Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1">Vincent Vanhoucke</a>, <a href="http://arxiv.org/find/cs/1/au:+Vega_S/0/1/0/all/0/1">Steve Vega</a>, <a href="http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1">Quan Vuong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Ted Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Peng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Sichun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tianhe Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zitkovich_B/0/1/0/all/0/1">Brianna Zitkovich</a></p>
<p>By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer1.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01732">UNAEN: Unsupervised Abnormality Extraction Network for MRI Motion Artifact Reduction. (arXiv:2301.01732v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yusheng Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jianan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Kong_Z/0/1/0/all/0/1">Zhengmin Kong</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1">Tao Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ahn_E/0/1/0/all/0/1">Euijoon Ahn</a>, <a href="http://arxiv.org/find/eess/1/au:+Lv_Z/0/1/0/all/0/1">Zhihan Lv</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Jinman Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1">David Dagan Feng</a></p>
<p>Motion artifacts compromise the quality of magnetic resonance imaging (MRI)
and pose challenges to achieving diagnostic outcomes and image-guided
therapies. In recent years, supervised deep learning approaches have emerged as
successful solutions for motion artifact reduction (MAR). One disadvantage of
these methods is their dependency on acquiring paired sets of motion
artifact-corrupted (MA-corrupted) and motion artifact-free (MA-free) MR images
for training purposes. Obtaining such image pairs is difficult and therefore
limits the application of supervised training. In this paper, we propose a
novel UNsupervised Abnormality Extraction Network (UNAEN) to alleviate this
problem. Our network is capable of working with unpaired MA-corrupted and
MA-free images. It converts the MA-corrupted images to MA-reduced images by
extracting abnormalities from the MA-corrupted images using a proposed artifact
extractor, which intercepts the residual artifact maps from the MA-corrupted MR
images explicitly, and a reconstructor to restore the original input from the
MA-reduced images. The performance of UNAEN was assessed by experimenting on
various publicly available MRI datasets and comparing them with
state-of-the-art methods. The quantitative evaluation demonstrates the
superiority of UNAEN over alternative MAR methods and visually exhibits fewer
residual artifacts. Our results substantiate the potential of UNAEN as a
promising solution applicable in real-world clinical environments, with the
capability to enhance diagnostic accuracy and facilitate image-guided
therapies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11189">Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models. (arXiv:2301.11189v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Muckley_M/0/1/0/all/0/1">Matthew J. Muckley</a>, <a href="http://arxiv.org/find/eess/1/au:+El_Nouby_A/0/1/0/all/0/1">Alaaeldin El-Nouby</a>, <a href="http://arxiv.org/find/eess/1/au:+Ullrich_K/0/1/0/all/0/1">Karen Ullrich</a>, <a href="http://arxiv.org/find/eess/1/au:+Jegou_H/0/1/0/all/0/1">Herv&#xe9; J&#xe9;gou</a>, <a href="http://arxiv.org/find/eess/1/au:+Verbeek_J/0/1/0/all/0/1">Jakob Verbeek</a></p>
<p>Lossy image compression aims to represent images in as few bits as possible
while maintaining fidelity to the original. Theoretical results indicate that
optimizing distortion metrics such as PSNR or MS-SSIM necessarily leads to a
discrepancy in the statistics of original images from those of reconstructions,
in particular at low bitrates, often manifested by the blurring of the
compressed images. Previous work has leveraged adversarial discriminators to
improve statistical fidelity. Yet these binary discriminators adopted from
generative modeling tasks may not be ideal for image compression. In this
paper, we introduce a non-binary discriminator that is conditioned on quantized
local image representations obtained via VQ-VAE autoencoders. Our evaluations
on the CLIC2020, DIV2K and Kodak datasets show that our discriminator is more
effective for jointly optimizing distortion (e.g., PSNR) and statistical
fidelity (e.g., FID) than the PatchGAN of the state-of-the-art HiFiC model. On
CLIC2020, we obtain the same FID as HiFiC with 30-40\% fewer bits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08058">Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution. (arXiv:2302.08058v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhengyu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Longguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jungang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shilin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yulan Guo</a></p>
<p>Exploiting spatial-angular correlation is crucial to light field (LF) image
super-resolution (SR), but is highly challenging due to its non-local property
caused by the disparities among LF images. Although many deep neural networks
(DNNs) have been developed for LF image SR and achieved continuously improved
performance, existing methods cannot well leverage the long-range
spatial-angular correlation and thus suffer a significant performance drop when
handling scenes with large disparity variations. In this paper, we propose a
simple yet effective method to learn the non-local spatial-angular correlation
for LF image SR. In our method, we adopt the epipolar plane image (EPI)
representation to project the 4D spatial-angular correlation onto multiple 2D
EPI planes, and then develop a Transformer network with repetitive
self-attention operations to learn the spatial-angular correlation by modeling
the dependencies between each pair of EPI pixels. Our method can fully
incorporate the information from all angular views while achieving a global
receptive field along the epipolar line. We conduct extensive experiments with
insightful visualizations to validate the effectiveness of our method.
Comparative results on five public datasets show that our method not only
achieves state-of-the-art SR performance, but also performs robust to disparity
variations. Code is publicly available at
https://github.com/ZhengyuLiang24/EPIT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06358">O2CTA: Introducing Annotations from OCT to CCTA in Coronary Plaque Analysis. (arXiv:2303.06358v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Jun Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1">Kexin Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yafeng Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a></p>
<p>Targeted diagnosis and treatment plans for patients with coronary artery
disease vary according to atherosclerotic plaque component. Coronary CT
angiography (CCTA) is widely used for artery imaging and determining the
stenosis degree. However, the limited spatial resolution and susceptibility to
artifacts fail CCTA in obtaining lumen morphological characteristics and plaque
composition. It can be settled by invasive optical coherence tomography (OCT)
without much trouble for physicians, but bringing higher costs and potential
risks to patients. Therefore, it is clinically critical to introduce
annotations of plaque tissue and lumen characteristics from OCT to paired CCTA
scans, denoted as \textbf{the O2CTA problem} in this paper. We propose a method
to handle the O2CTA problem. CCTA scans are first reconstructed into
multi-planar reformatted (MPR) images, which agree with OCT images in term of
semantic contents. The artery segment in OCT, which is manually labelled, is
then spatially aligned with the entire artery in MPR images via the proposed
alignment strategy. Finally, a classification model involving a 3D CNN and a
Transformer, is learned to extract local features and capture dependence along
arteries. Experiments on 55 paired OCT and CCTA we curate demonstrate that it
is feasible to classify the CCTA based on the OCT labels, with an accuracy of
86.2%, while the manual readings of OCT and CCTA vary significantly, with a
Kappa coefficient of 0.113. We will make our source codes, models, data, and
results publicly available to benefit the research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06628">Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models. (arXiv:2303.06628v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zangwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Mingyuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Ziheng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xiangyu Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yang You</a></p>
<p>Continual learning (CL) can help pre-trained vision-language models
efficiently adapt to new or under-trained data distributions without
re-training. Nevertheless, during the continual training of the Contrastive
Language-Image Pre-training (CLIP) model, we observe that the model's zero-shot
transfer ability significantly degrades due to catastrophic forgetting.
Existing CL methods can mitigate forgetting by replaying previous data.
However, since the CLIP dataset is private, replay methods cannot access the
pre-training dataset. In addition, replaying data of previously learned
downstream tasks can enhance their performance but comes at the cost of
sacrificing zero-shot performance. To address this challenge, we propose a
novel method ZSCL to prevent zero-shot transfer degradation in the continual
learning of vision-language models in both feature and parameter space. In the
feature space, a reference dataset is introduced for distillation between the
current and initial models. The reference dataset should have semantic
diversity but no need to be labeled, seen in pre-training, or matched
image-text pairs. In parameter space, we prevent a large parameter shift by
averaging weights during the training. We propose a more challenging
Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate different
methods, where tasks are from various domains instead of class-separated in a
single dataset. Our method outperforms other methods in the traditional
class-incremental learning setting and the MTIL by 9.7% average score. Our code
locates at https://github.com/Thunderbeee/ZSCL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11219">NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongcheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1">Xiaoxiao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yusen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1">Tuo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chunxia Xiao</a></p>
<p>We present a novel method, called NeTO, for capturing 3D geometry of solid
transparent objects from 2D images via volume rendering. Reconstructing
transparent objects is a very challenging task, which is ill-suited for
general-purpose reconstruction techniques due to the specular light transport
phenomena. Although existing refraction-tracing based methods, designed
specially for this task, achieve impressive results, they still suffer from
unstable optimization and loss of fine details, since the explicit surface
representation they adopted is difficult to be optimized, and the
self-occlusion problem is ignored for refraction-tracing. In this paper, we
propose to leverage implicit Signed Distance Function (SDF) as surface
representation, and optimize the SDF field via volume rendering with a
self-occlusion aware refractive ray tracing. The implicit representation
enables our method to be capable of reconstructing high-quality reconstruction
even with a limited set of images, and the self-occlusion aware strategy makes
it possible for our method to accurately reconstruct the self-occluded regions.
Experiments show that our method achieves faithful reconstruction results and
outperforms prior works by a large margin. Visit our project page at
\url{https://www.xxlong.site/NeTO/}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11681">DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models. (arXiv:2303.11681v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yuzhong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1">Mike Zheng Shou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a></p>
<p>Collecting and annotating images with pixel-wise labels is time-consuming and
laborious. In contrast, synthetic data can be freely available using a
generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that
it is possible to automatically obtain accurate semantic masks of synthetic
images generated by the Off-the-shelf Stable Diffusion model, which uses only
text-image pairs during training. Our approach, called DiffuMask, exploits the
potential of the cross-attention map between text and image, which is natural
and seamless to extend the text-driven image synthesis to semantic mask
generation. DiffuMask uses text-guided cross-attention information to localize
class/word-specific regions, which are combined with practical techniques to
create a novel high-resolution and class-discriminative pixel-wise mask. The
methods help to reduce data collection and annotation costs obviously.
Experiments demonstrate that the existing segmentation methods trained on
synthetic data of DiffuMask can achieve a competitive performance over the
counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),
DiffuMask presents promising performance, close to the stateof-the-art result
of real data (within 3% mIoU gap). Moreover, in the open-vocabulary
segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on
Unseen class of VOC 2012. The project website can be found at
https://weijiawu.github.io/DiffusionMask/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15109">Improving the Transferability of Adversarial Examples via Direction Tuning. (arXiv:2303.15109v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiangyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanlin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xinyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peng Zhao</a></p>
<p>In the transfer-based adversarial attacks, adversarial examples are only
generated by the surrogate models and achieve effective perturbation in the
victim models. Although considerable efforts have been developed on improving
the transferability of adversarial examples generated by transfer-based
adversarial attacks, our investigation found that, the big deviation between
the actual and steepest update directions of the current transfer-based
adversarial attacks is caused by the large update step length, resulting in the
generated adversarial examples can not converge well. However, directly
reducing the update step length will lead to serious update oscillation so that
the generated adversarial examples also can not achieve great transferability
to the victim models. To address these issues, a novel transfer-based attack,
namely direction tuning attack, is proposed to not only decrease the update
deviation in the large step length, but also mitigate the update oscillation in
the small sampling step length, thereby making the generated adversarial
examples converge well to achieve great transferability on victim models. In
addition, a network pruning method is proposed to smooth the decision boundary,
thereby further decreasing the update oscillation and enhancing the
transferability of the generated adversarial examples. The experiment results
on ImageNet demonstrate that the average attack success rate (ASR) of the
adversarial examples generated by our method can be improved from 87.9\% to
94.5\% on five victim models without defenses, and from 69.1\% to 76.2\% on
eight advanced defense methods, in comparison with that of latest
gradient-based attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09466">MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis. (arXiv:2304.09466v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Degerli_A/0/1/0/all/0/1">Aysen Degerli</a>, <a href="http://arxiv.org/find/eess/1/au:+Jakala_P/0/1/0/all/0/1">Pekka Jakala</a>, <a href="http://arxiv.org/find/eess/1/au:+Pajula_J/0/1/0/all/0/1">Juha Pajula</a>, <a href="http://arxiv.org/find/eess/1/au:+Immonen_M/0/1/0/all/0/1">Milla Immonen</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_M/0/1/0/all/0/1">Miguel Bordallo Lopez</a></p>
<p>Stroke is a major cause of mortality and disability worldwide from which one
in four people are in danger of incurring in their lifetime. The pre-hospital
stroke assessment plays a vital role in identifying stroke patients accurately
to accelerate further examination and treatment in hospitals. Accordingly, the
National Institutes of Health Stroke Scale (NIHSS), Cincinnati Pre-hospital
Stroke Scale (CPSS) and Face Arm Speed Time (F.A.S.T.) are globally known tests
for stroke assessment. However, the validity of these tests is skeptical in the
absence of neurologists and access to healthcare may be limited. Therefore, in
this study, we propose a motion-aware and multi-attention fusion network
(MAMAF-Net) that can detect stroke from multimodal examination videos. Contrary
to other studies on stroke detection from video analysis, our study for the
first time proposes an end-to-end solution from multiple video recordings of
each subject with a dataset encapsulating stroke, transient ischemic attack
(TIA), and healthy controls. The proposed MAMAF-Net consists of motion-aware
modules to sense the mobility of patients, attention modules to fuse the
multi-input video data, and 3D convolutional layers to perform diagnosis from
the attention-based extracted features. Experimental results over the collected
Stroke-data dataset show that the proposed MAMAF-Net achieves a successful
detection of stroke with 93.62% sensitivity and 95.33% AUC score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13031">DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection. (arXiv:2304.13031v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Huan-ang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Beiwen Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guyue Zhou</a></p>
<p>In this paper, we study the problem of semi-supervised 3D object detection,
which is of great importance considering the high annotation cost for cluttered
3D indoor scenes. We resort to the robust and principled framework of
selfteaching, which has triggered notable progress for semisupervised learning
recently. While this paradigm is natural for image-level or pixel-level
prediction, adapting it to the detection problem is challenged by the issue of
proposal matching. Prior methods are based upon two-stage pipelines, matching
heuristically selected proposals generated in the first stage and resulting in
spatially sparse training signals. In contrast, we propose the first
semisupervised 3D detection algorithm that works in the singlestage manner and
allows spatially dense training signals. A fundamental issue of this new design
is the quantization error caused by point-to-voxel discretization, which
inevitably leads to misalignment between two transformed views in the voxel
domain. To this end, we derive and implement closed-form rules that compensate
this misalignment onthe-fly. Our results are significant, e.g., promoting
ScanNet mAP@0.5 from 35.2% to 48.5% using 20% annotation. Codes and data will
be publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03678">Towards Segment Anything Model (SAM) for Medical Image Segmentation: A Survey. (arXiv:2305.03678v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiao_R/0/1/0/all/0/1">Rushi Jiao</a></p>
<p>Due to the flexibility of prompting, foundation models have become the
dominant force in the domains of natural language processing and image
generation. With the recent introduction of the Segment Anything Model (SAM),
the prompt-driven paradigm has entered the realm of image segmentation,
bringing with a range of previously unexplored capabilities. However, it
remains unclear whether it can be applicable to medical image segmentation due
to the significant differences between natural images and medical images.In
this work, we summarize recent efforts to extend the success of SAM to medical
image segmentation tasks, including both empirical benchmarking and
methodological adaptations, and discuss potential future directions for SAM in
medical image segmentation. Although directly applying SAM to medical image
segmentation cannot obtain satisfying performance on multi-modal and
multi-target medical datasets, many insights are drawn to guide future research
to develop foundation models for medical image analysis. To facilitate future
research, we maintain an active repository that contains up-to-date paper list
and open-source project summary at https://github.com/YichiZhang98/SAM4MIS.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06024">Larger is not Better: A Survey on the Robustness of Computer Vision Models against Common Corruptions. (arXiv:2305.06024v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shunxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Veldhuis_R/0/1/0/all/0/1">Raymond Veldhuis</a>, <a href="http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1">Christoph Brune</a>, <a href="http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1">Nicola Strisciuglio</a></p>
<p>The performance of computer vision models are susceptible to unexpected
changes in input images, known as common corruptions (e.g. noise, blur,
illumination changes, etc.), that can hinder their reliability when deployed in
real scenarios. These corruptions are not always considered to test model
generalization and robustness. In this survey, we present a comprehensive
overview of methods that improve the robustness of computer vision models
against common corruptions. We categorize methods into four groups based on the
model part and training method addressed: data augmentation, representation
learning, knowledge distillation, and network components. We also cover
indirect methods for generalization and mitigation of shortcut learning,
potentially useful for corruption robustness. We release a unified benchmark
framework to compare robustness performance on several datasets, and address
the inconsistencies of evaluation in the literature. We provide an experimental
overview of the base corruption robustness of popular vision backbones, and
show that corruption robustness does not necessarily scale with model size. The
very large models (above 100M parameters) gain negligible robustness,
considering the increased computational requirements. To achieve generalizable
and robust computer vision models, we foresee the need of developing new
learning strategies to efficiently exploit limited data and mitigate unwanted
or unreliable learning behaviors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.06564">Undercover Deepfakes: Detecting Fake Segments in Videos. (arXiv:2305.06564v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Sanjay Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Perera_R/0/1/0/all/0/1">Rashindrie Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1">Sachith Seneviratne</a>, <a href="http://arxiv.org/find/cs/1/au:+Malepathirana_T/0/1/0/all/0/1">Tamasha Malepathirana</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasnayaka_S/0/1/0/all/0/1">Sanka Rasnayaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Geethika_D/0/1/0/all/0/1">Deshani Geethika</a>, <a href="http://arxiv.org/find/cs/1/au:+Sim_T/0/1/0/all/0/1">Terence Sim</a>, <a href="http://arxiv.org/find/cs/1/au:+Halgamuge_S/0/1/0/all/0/1">Saman Halgamuge</a></p>
<p>The recent renaissance in generative models, driven primarily by the advent
of diffusion models and iterative improvement in GAN methods, has enabled many
creative applications. However, each advancement is also accompanied by a rise
in the potential for misuse. In the arena of the deepfake generation, this is a
key societal issue. In particular, the ability to modify segments of videos
using such generative techniques creates a new paradigm of deepfakes which are
mostly real videos altered slightly to distort the truth.This paradigm has been
under-explored by the current deepfake detection methods in the academic
literature. In this paper, we present a deepfake detection method that can
address this issue by performing deepfake prediction at the frame and video
levels. To facilitate testing our method, we prepared a new benchmark dataset
where videos have both real and fake frame sequences with very subtle
transitions. We provide a benchmark on the proposed dataset with our detection
method which utilizes the Vision Transformer based on Scaling and Shifting to
learn spatial features, and a Timeseries Transformer to learn temporal features
of the videos to help facilitate the interpretation of possible deepfakes.
Extensive experiments on a variety of deepfake generation methods show
excellent results by the proposed method on temporal segmentation and classical
video-level predictions as well. In particular, the paradigm we address will
form a powerful tool for the moderation of deepfakes, where human oversight can
be better targeted to the parts of videos suspected of being deepfakes. All
experiments can be reproduced at: https://t.ly/\_bOh9.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14677">Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models. (arXiv:2305.14677v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1">Zhongjie Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1">Weining Qian</a></p>
<p>In recent years, diffusion models have become the most popular and powerful
methods in the field of image synthesis, even rivaling human artists in
artistic creativity. However, the key issue currently limiting the application
of diffusion models is its extremely slow generation process. Although several
methods were proposed to speed up the generation process, there still exists a
trade-off between efficiency and quality. In this paper, we first provide a
detailed theoretical and empirical analysis of the generation process of the
diffusion models based on schedulers. We transform the designing problem of
schedulers into the determination of several parameters, and further transform
the accelerated generation process into an expansion process of the linear
subspace. Based on these analyses, we consequently propose a novel method
called Optimal Linear Subspace Search (OLSS), which accelerates the generation
process by searching for the optimal approximation process of the complete
generation process in the linear subspaces spanned by latent variables. OLSS is
able to generate high-quality images with a very small number of steps. To
demonstrate the effectiveness of our method, we conduct extensive comparative
experiments on open-source diffusion models. Experimental results show that
with a given number of steps, OLSS can significantly improve the quality of
generated images. Using an NVIDIA A100 GPU, we make it possible to generate a
high-quality image by Stable Diffusion within only one second without other
optimization techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17271">Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yongqi Dong</a></p>
<p>Lane detection is crucial for vehicle localization which makes it the
foundation for automated driving and many intelligent and advanced driving
assistant systems. Available vision-based lane detection methods do not make
full use of the valuable features and aggregate contextual information,
especially the interrelationships between lane lines and other regions of the
images in continuous frames. To fill this research gap and upgrade lane
detection performance, this paper proposes a pipeline consisting of self
pre-training with masked sequential autoencoders and fine-tuning with
customized PolyLoss for the end-to-end neural network models using
multi-continuous image frames. The masked sequential autoencoders are adopted
to pre-train the neural network models with reconstructing the missing pixels
from a random masked image as the objective. Then, in the fine-tuning
segmentation phase where lane detection segmentation is performed, the
continuous image frames are served as the inputs, and the pre-trained model
weights are transferred and further updated using the backpropagation mechanism
with customized PolyLoss calculating the weighted errors between the output
lane detection results and the labeled ground truth. Extensive experiment
results demonstrate that, with the proposed pipeline, the lane detection model
performance on both normal and challenging scenes can be advanced beyond the
state-of-the-art, delivering the best testing accuracy (98.38%), precision
(0.937), and F1-measure (0.924) on the normal scene testing set, together with
the best overall accuracy (98.36%) and precision (0.844) in the challenging
scene test set, while the training time can be substantially shortened.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.20048">F?D: On understanding the role of deep feature spaces on face generation evaluation. (arXiv:2305.20048v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kabra_K/0/1/0/all/0/1">Krish Kabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1">Guha Balakrishnan</a></p>
<p>Perceptual metrics, like the Fr\'echet Inception Distance (FID), are widely
used to assess the similarity between synthetically generated and ground truth
(real) images. The key idea behind these metrics is to compute errors in a deep
feature space that captures perceptually and semantically rich image features.
Despite their popularity, the effect that different deep features and their
design choices have on a perceptual metric has not been well studied. In this
work, we perform a causal analysis linking differences in semantic attributes
and distortions between face image distributions to Fr\'echet distances (FD)
using several popular deep feature spaces. A key component of our analysis is
the creation of synthetic counterfactual faces using deep face generators. Our
experiments show that the FD is heavily influenced by its feature space's
training dataset and objective function. For example, FD using features
extracted from ImageNet-trained models heavily emphasize hats over regions like
the eyes and mouth. Moreover, FD using features from a face gender classifier
emphasize hair length more than distances in an identity (recognition) feature
space. Finally, we evaluate several popular face generation models across
feature spaces and find that StyleGAN2 consistently ranks higher than other
face generators, except with respect to identity (recognition) features. This
suggests the need for considering multiple feature spaces when evaluating
generative models and using feature spaces that are tuned to nuances of the
domain of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02898">Towards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark. (arXiv:2306.02898v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yinan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaxiong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yujiao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Li Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhedong Zheng</a></p>
<p>In this paper, we introduce a large Multi-Attribute and Language Search
dataset for text-based person retrieval, called MALS, and explore the
feasibility of performing pre-training on both attribute recognition and
image-text matching tasks in one stone. In particular, MALS contains 1,510,330
image-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,
and all images are annotated with 27 attributes. Considering the privacy
concerns and annotation costs, we leverage the off-the-shelf diffusion models
to generate the dataset. To verify the feasibility of learning from the
generated data, we develop a new joint Attribute Prompt Learning and Text
Matching Learning (APTM) framework, considering the shared knowledge between
attribute and text. As the name implies, APTM contains an attribute prompt
learning stream and a text matching learning stream. (1) The attribute prompt
learning leverages the attribute prompts for image-attribute alignment, which
enhances the text matching learning. (2) The text matching learning facilitates
the representation learning on fine-grained details, and in turn, boosts the
attribute prompt learning. Extensive experiments validate the effectiveness of
the pre-training on MALS, achieving state-of-the-art retrieval performance via
APTM on three challenging real-world benchmarks. In particular, APTM achieves a
consistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy on
CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04542">On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1">Ziyi Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koulieris_G/0/1/0/all/0/1">George Alex Koulieris</a>, <a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1">Hubert P. H. Shum</a></p>
<p>Diffusion models are generative models, which gradually add and remove noise
to learn the underlying distribution of training data for data generation. The
components of diffusion models have gained significant attention with many
design choices proposed. Existing reviews have primarily focused on
higher-level solutions, thereby covering less on the design fundamentals of
components. This study seeks to address this gap by providing a comprehensive
and coherent review on component-wise design choices in diffusion models.
Specifically, we organize this review according to their three key components,
namely the forward process, the reverse process, and the sampling procedure.
This allows us to provide a fine-grained perspective of diffusion models,
benefiting future studies in the analysis of individual components, the
applicability of design choices, and the implementation of diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08966">Training Multimedia Event Extraction With Generated Images and Captions. (arXiv:2306.08966v2 [cs.MM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1">Zilin Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yidan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a></p>
<p>Contemporary news reporting increasingly features multimedia content,
motivating research on multimedia event extraction. However, the task lacks
annotated multimodal training data and artificially generated training data
suffer from distribution shift from real-world data. In this paper, we propose
Cross-modality Augmented Multimedia Event Learning (CAMEL), which successfully
utilizes artificially generated multimodal training data and achieves
state-of-the-art performance. We start with two labeled unimodal datasets in
text and image respectively, and generate the missing modality using
off-the-shelf image generators like Stable Diffusion and image captioners like
BLIP. After that, we train the network on the resultant multimodal datasets. In
order to learn robust features that are effective across domains, we devise an
iterative and gradual training strategy. Substantial experiments show that
CAMEL surpasses state-of-the-art (SOTA) baselines on the M2E2 benchmark. On
multimedia events in particular, we outperform the prior SOTA by 4.2% F1 on
event mention identification and by 9.8% F1 on argument identification, which
indicates that CAMEL learns synergistic representations from the two
modalities. Our work demonstrates a recipe to unleash the power of synthetic
training data in structured prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14628">An Integral Projection-based Semantic Autoencoder for Zero-Shot Learning. (arXiv:2306.14628v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Heyden_W/0/1/0/all/0/1">William Heyden</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullah_H/0/1/0/all/0/1">Habib Ullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Siddiqui_M/0/1/0/all/0/1">M. Salman Siddiqui</a>, <a href="http://arxiv.org/find/cs/1/au:+Machot_F/0/1/0/all/0/1">Fadi Al Machot</a></p>
<p>Zero-shot Learning (ZSL) classification categorizes or predicts classes
(labels) that are not included in the training set (unseen classes). Recent
works proposed different semantic autoencoder (SAE) models where the encoder
embeds a visual feature vector space into the semantic space and the decoder
reconstructs the original visual feature space. The objective is to learn the
embedding by leveraging a source data distribution, which can be applied
effectively to a different but related target data distribution. Such
embedding-based methods are prone to domain shift problems and are vulnerable
to biases. We propose an integral projection-based semantic autoencoder
(IP-SAE) where an encoder projects a visual feature space concatenated with the
semantic space into a latent representation space. We force the decoder to
reconstruct the visual-semantic data space. Due to this constraint, the
visual-semantic projection function preserves the discriminatory data included
inside the original visual feature space. The enriched projection forces a more
precise reconstitution of the visual feature space invariant to the domain
manifold. Consequently, the learned projection function is less domain-specific
and alleviates the domain shift problem. Our proposed IP-SAE model consolidates
a symmetric transformation function for embedding and projection, and thus, it
provides transparency for interpreting generative applications in ZSL.
Therefore, in addition to outperforming state-of-the-art methods considering
four benchmark datasets, our analytical approach allows us to investigate
distinct characteristics of generative-based methods in the unique context of
zero-shot inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15932">NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data. (arXiv:2306.15932v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1">Kangning Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zhen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhihua Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dongsheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1">Xinhui Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1">Guangqiang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhiguo Wang</a></p>
<p>Federated learning (FL), a privacy-preserving distributed machine learning,
has been rapidly applied in wireless communication networks. FL enables
Internet of Things (IoT) clients to obtain well-trained models while preventing
privacy leakage. Person detection can be deployed on edge devices with limited
computing power if combined with FL to process the video data directly at the
edge. However, due to the different hardware and deployment scenarios of
different cameras, the data collected by the camera present non-independent and
identically distributed (non-IID), and the global model derived from FL
aggregation is less effective. Meanwhile, existing research lacks public data
set for real-world FL object detection, which is not conducive to studying the
non-IID problem on IoT cameras. Therefore, we open source a non-IID IoT person
detection (NIPD) data set, which is collected from five different cameras. To
our knowledge, this is the first true device-based non-IID person detection
data set. Based on this data set, we explain how to establish a FL experimental
platform and provide a benchmark for non-IID person detection. NIPD is expected
to promote the application of FL and the security of smart city.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00464">Human-to-Human Interaction Detection. (arXiv:2307.00464v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhenhua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_K/0/1/0/all/0/1">Kaining Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1">Jiajun Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1">Jifeng Ning</a></p>
<p>A comprehensive understanding of interested human-to-human interactions in
video streams, such as queuing, handshaking, fighting and chasing, is of
immense importance to the surveillance of public security in regions like
campuses, squares and parks. Different from conventional human interaction
recognition, which uses choreographed videos as inputs, neglects concurrent
interactive groups, and performs detection and recognition in separate stages,
we introduce a new task named human-to-human interaction detection (HID). HID
devotes to detecting subjects, recognizing person-wise actions, and grouping
people according to their interactive relations, in one model. First, based on
the popular AVA dataset created for action detection, we establish a new HID
benchmark, termed AVA-Interaction (AVA-I), by adding annotations on interactive
relations in a frame-by-frame manner. AVA-I consists of 85,254 frames and
86,338 interactive groups, and each image includes up to 4 concurrent
interactive groups. Second, we present a novel baseline approach SaMFormer for
HID, containing a visual feature extractor, a split stage which leverages a
Transformer-based model to decode action instances and interactive groups, and
a merging stage which reconstructs the relationship between instances and
groups. All SaMFormer components are jointly trained in an end-to-end manner.
Extensive experiments on AVA-I validate the superiority of SaMFormer over
representative methods. The dataset and code will be made public to encourage
more follow-up studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03903">Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification. (arXiv:2307.03903v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huafeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Le Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yafei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dapeng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhengtao Yu</a></p>
<p>In visible-infrared video person re-identification (re-ID), extracting
features not affected by complex scenes (such as modality, camera views,
pedestrian pose, background, etc.) changes, and mining and utilizing motion
information are the keys to solving cross-modal pedestrian identity matching.
To this end, the paper proposes a new visible-infrared video person re-ID
method from a novel perspective, i.e., adversarial self-attack defense and
spatial-temporal relation mining. In this work, the changes of views, posture,
background and modal discrepancy are considered as the main factors that cause
the perturbations of person identity features. Such interference information
contained in the training samples is used as an adversarial perturbation. It
performs adversarial attacks on the re-ID model during the training to make the
model more robust to these unfavorable factors. The attack from the adversarial
perturbation is introduced by activating the interference information contained
in the input samples without generating adversarial samples, and it can be thus
called adversarial self-attack. This design allows adversarial attack and
defense to be integrated into one framework. This paper further proposes a
spatial-temporal information-guided feature representation network to use the
information in video sequences. The network cannot only extract the information
contained in the video-frame sequences but also use the relation of the local
information in space to guide the network to extract more robust features. The
proposed method exhibits compelling performance on large-scale cross-modality
video datasets. The source code of the proposed method will be released at
https://github.com/lhf12278/xxx.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07972">Dual-level Interaction for Domain Adaptive Semantic Segmentation. (arXiv:2307.07972v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1">Dongyu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boheng Li</a></p>
<p>Self-training approach recently secures its position in domain adaptive
semantic segmentation, where a model is trained with target domain
pseudo-labels. Current advances have mitigated noisy pseudo-labels resulting
from the domain gap. However, they still struggle with erroneous pseudo-labels
near the boundaries of the semantic classifier. In this paper, we tackle this
issue by proposing a dual-level interaction for domain adaptation (DIDA) in
semantic segmentation. Explicitly, we encourage the different augmented views
of the same pixel to have not only similar class prediction (semantic-level)
but also akin similarity relationship with respect to other pixels
(instance-level). As it's impossible to keep features of all pixel instances
for a dataset, we, therefore, maintain a labeled instance bank with dynamic
updating strategies to selectively store the informative features of instances.
Further, DIDA performs cross-level interaction with scattering and gathering
techniques to regenerate more reliable pseudo-labels. Our method outperforms
the state-of-the-art by a notable margin, especially on confusing and
long-tailed classes. Code is available at
\href{https://github.com/RainJamesY/DIDA}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08106">Polarization Multi-Image Synthesis with Birefringent Metasurfaces. (arXiv:2307.08106v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hazineh_D/0/1/0/all/0/1">Dean Hazineh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Soon Wei Daniel Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Capasso_F/0/1/0/all/0/1">Federico Capasso</a>, <a href="http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1">Todd Zickler</a></p>
<p>Optical metasurfaces composed of precisely engineered nanostructures have
gained significant attention for their ability to manipulate light and
implement distinct functionalities based on the properties of the incident
field. Computational imaging systems have started harnessing this capability to
produce sets of coded measurements that benefit certain tasks when paired with
digital post-processing. Inspired by these works, we introduce a new system
that uses a birefringent metasurface with a polarizer-mosaicked photosensor to
capture four optically-coded measurements in a single exposure. We apply this
system to the task of incoherent opto-electronic filtering, where digital
spatial-filtering operations are replaced by simpler, per-pixel sums across the
four polarization channels, independent of the spatial filter size. In contrast
to previous work on incoherent opto-electronic filtering that can realize only
one spatial filter, our approach can realize a continuous family of filters
from a single capture, with filters being selected from the family by adjusting
the post-capture digital summation weights. To find a metasurface that can
realize a set of user-specified spatial filters, we introduce a form of
gradient descent with a novel regularizer that encourages light efficiency and
a high signal-to-noise ratio. We demonstrate several examples in simulation and
with fabricated prototypes, including some with spatial filters that have
prescribed variations with respect to depth and wavelength.
</p>
<p>Visit the Project Page at
https://deanhazineh.github.io/publications/Multi_Image_Synthesis/MIS_Home.html
</p>
</p>
</div>

    </div>
    </body>
    