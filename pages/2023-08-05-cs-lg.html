<!DOCTYPE html>
<html>
<head>
<title>2023-08-05-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2308.01319">Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges. (arXiv:2308.01319v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tajidini_F/0/1/0/all/0/1">Farzaneh Tajidini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kheiri_M/0/1/0/all/0/1">Mohammad-Javad Kheiri</a></p>
<p>Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is
expanding quickly. Because errors in medical diagnostic systems might lead to
seriously misleading medical treatments, major efforts have been made in recent
years to improve computer-aided diagnostics applications. The use of machine
learning in computer-aided diagnosis is crucial. A simple equation may result
in a false indication of items like organs. Therefore, learning from examples
is a vital component of pattern recognition. Pattern recognition and machine
learning in the biomedical area promise to increase the precision of disease
detection and diagnosis. They also support the decision-making process's
objectivity. Machine learning provides a practical method for creating elegant
and autonomous algorithms to analyze high-dimensional and multimodal
bio-medical data. This review article examines machine-learning algorithms for
detecting diseases, including hepatitis, diabetes, liver disease, dengue fever,
and heart disease. It draws attention to the collection of machine learning
techniques and algorithms employed in studying conditions and the ensuing
decision-making process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01320">DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhewei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1">Reza Yazdani Aminabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1">Olatunji Ruwase</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1">Samyam Rajbhandari</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoxia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1">Ammar Ahmad Awan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasley_J/0/1/0/all/0/1">Jeff Rasley</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minjia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Conglong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1">Connor Holmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhongzhu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wyatt_M/0/1/0/all/0/1">Michael Wyatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1">Molly Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurilenko_L/0/1/0/all/0/1">Lev Kurilenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1">Heyang Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1">Masahiro Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_S/0/1/0/all/0/1">Shuai Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shuaiwen Leon Song</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuxiong He</a></p>
<p>ChatGPT-like models have revolutionized various applications in artificial
intelligence, from summarization and coding to translation, matching or even
surpassing human performance. However, the current landscape lacks an
accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement
Learning with Human Feedback) training pipeline for these powerful models,
particularly when training at the scale of billions of parameters. This paper
introduces DeepSpeed-Chat, a novel system that democratizes RLHF training,
making it accessible to the AI community. DeepSpeed-Chat offers three key
capabilities: an easy-to-use training and inference experience for ChatGPT-like
models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from
InstructGPT, and a robust DeepSpeed-RLHF system that combines various
optimizations for training and inference in a unified way. The system delivers
unparalleled efficiency and scalability, enabling training of models with
hundreds of billions of parameters in record time and at a fraction of the
cost. With this development, DeepSpeed-Chat paves the way for broader access to
advanced RLHF training, even for data scientists with limited resources,
thereby fostering innovation and further development in the field of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01323">Evaluation of network-guided random forest for disease gene discovery. (arXiv:2308.01323v1 [q-bio.MN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Hu_J/0/1/0/all/0/1">Jianchang Hu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Szymczak_S/0/1/0/all/0/1">Silke Szymczak</a></p>
<p>Gene network information is believed to be beneficial for disease module and
pathway identification, but has not been explicitly utilized in the standard
random forest (RF) algorithm for gene expression data analysis. We investigate
the performance of a network-guided RF where the network information is
summarized into a sampling probability of predictor variables which is further
used in the construction of the RF. Our results suggest that network-guided RF
does not provide better disease prediction than the standard RF. In terms of
disease gene discovery, if disease genes form module(s), network-guided RF
identifies them more accurately. In addition, when disease status is
independent from genes in the given network, spurious gene selection results
can occur when using network information, especially on hub genes. Our
empirical analysis on two balanced microarray and RNA-Seq breast cancer
datasets from The Cancer Genome Atlas (TCGA) for classification of progesterone
receptor (PR) status also demonstrates that network-guided RF can identify
genes from PGR-related pathways, which leads to a better connected module of
identified genes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01327">Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification. (arXiv:2308.01327v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wagner_L/0/1/0/all/0/1">Laurin Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Zusag_M/0/1/0/all/0/1">Mario Zusag</a>, <a href="http://arxiv.org/find/cs/1/au:+Bloder_T/0/1/0/all/0/1">Theresa Bloder</a></p>
<p>This paper presents a fully automated approach for identifying speech
anomalies from voice recordings to aid in the assessment of speech impairments.
By combining Connectionist Temporal Classification (CTC) and
encoder-decoder-based automatic speech recognition models, we generate rich
acoustic and clean transcripts. We then apply several natural language
processing methods to extract features from these transcripts to produce
prototypes of healthy speech. Basic distance measures from these prototypes
serve as input features for standard machine learning classifiers, yielding
human-level accuracy for the distinction between recordings of people with
aphasia and a healthy control group. Furthermore, the most frequently occurring
aphasia types can be distinguished with 90% accuracy. The pipeline is directly
applicable to other diseases and languages, showing promise for robustly
extracting diagnostic speech biomarkers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01329">EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding. (arXiv:2308.01329v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junpeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1">Chin-Chia Michael Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yujie Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huiyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a></p>
<p>Embedding learning transforms discrete data entities into continuous
numerical representations, encoding features/properties of the entities.
Despite the outstanding performance reported from different embedding learning
algorithms, few efforts were devoted to structurally interpreting how features
are encoded in the learned embedding space. This work proposes EmbeddingTree, a
hierarchical embedding exploration algorithm that relates the semantics of
entity features with the less-interpretable embedding vectors. An interactive
visualization tool is also developed based on EmbeddingTree to explore
high-dimensional embeddings. The tool helps users discover nuance features of
data entities, perform feature denoising/injecting in embedding training, and
generate embeddings for unseen entities. We demonstrate the efficacy of
EmbeddingTree and our visualization tool through embeddings generated for
industry-scale merchant data and the public 30Music listening/playlists
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01358">Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Philippenko_C/0/1/0/all/0/1">Constantin Philippenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Dieuleveut_A/0/1/0/all/0/1">Aymeric Dieuleveut</a></p>
<p>In this paper, we investigate the impact of compression on stochastic
gradient algorithms for machine learning, a technique widely used in
distributed and federated learning. We underline differences in terms of
convergence rates between several unbiased compression operators, that all
satisfy the same condition on their variance, thus going beyond the classical
worst-case analysis. To do so, we focus on the case of least-squares regression
(LSR) and analyze a general stochastic approximation algorithm for minimizing
quadratic functions relying on a random field. We consider weak assumptions on
the random field, tailored to the analysis (specifically, expected H\"older
regularity), and on the noise covariance, enabling the analysis of various
randomizing mechanisms, including compression. We then extend our results to
the case of federated learning.
</p>
<p>More formally, we highlight the impact on the convergence of the covariance
$\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced by the algorithm.
We demonstrate despite the non-regularity of the stochastic field, that the
limit variance term scales with $\mathrm{Tr}(\mathfrak{C}_{\mathrm{ania}}
H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the
number of iterations) generalizing the rate for the vanilla LSR case where it
is $\sigma^2 \mathrm{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach and Moulines,
2013). Then, we analyze the dependency of $\mathfrak{C}_{\mathrm{ania}}$ on the
compression strategy and ultimately its impact on convergence, first in the
centralized case, then in two heterogeneous FL frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01362">Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE. (arXiv:2308.01362v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Laurie_M/0/1/0/all/0/1">Mark Laurie</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lu_J/0/1/0/all/0/1">James Lu</a></p>
<p>While tumor dynamic modeling has been widely applied to support the
development of oncology drugs, there remains a need to increase predictivity,
enable personalized therapy, and improve decision-making. We propose the use of
Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to
enable model discovery from longitudinal tumor size data. We show that TDNODE
overcomes a key limitation of existing models in its ability to make unbiased
predictions from truncated data. The encoder-decoder architecture is designed
to express an underlying dynamical law which possesses the fundamental property
of generalized homogeneity with respect to time. Thus, the modeling formalism
enables the encoder output to be interpreted as kinetic rate metrics, with
inverse time as the physical unit. We show that the generated metrics can be
used to predict patients' overall survival (OS) with high accuracy. The
proposed modeling formalism provides a principled way to integrate multimodal
dynamical datasets in oncology disease modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01379">Computational Long Exposure Mobile Photography. (arXiv:2308.01379v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tabellion_E/0/1/0/all/0/1">Eric Tabellion</a>, <a href="http://arxiv.org/find/cs/1/au:+Karnad_N/0/1/0/all/0/1">Nikhil Karnad</a>, <a href="http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1">Noa Glaser</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_B/0/1/0/all/0/1">Ben Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1">David E. Jacobs</a>, <a href="http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1">Yael Pritch</a></p>
<p>Long exposure photography produces stunning imagery, representing moving
elements in a scene with motion-blur. It is generally employed in two
modalities, producing either a foreground or a background blur effect.
Foreground blur images are traditionally captured on a tripod-mounted camera
and portray blurred moving foreground elements, such as silky water or light
trails, over a perfectly sharp background landscape. Background blur images,
also called panning photography, are captured while the camera is tracking a
moving subject, to produce an image of a sharp subject over a background
blurred by relative motion. Both techniques are notoriously challenging and
require additional equipment and advanced skills. In this paper, we describe a
computational burst photography system that operates in a hand-held smartphone
camera app, and achieves these effects fully automatically, at the tap of the
shutter button. Our approach first detects and segments the salient subject. We
track the scene motion over multiple frames and align the images in order to
preserve desired sharpness and to produce aesthetically pleasing motion
streaks. We capture an under-exposed burst and select the subset of input
frames that will produce blur trails of controlled length, regardless of scene
or camera motion velocity. We predict inter-frame motion and synthesize
motion-blur to fill the temporal gaps between the input frames. Finally, we
composite the blurred image with the sharp regular exposure to protect the
sharpness of faces or areas of the scene that are barely moving, and produce a
final high resolution and high dynamic range (HDR) photograph. Our system
democratizes a capability previously reserved to professionals, and makes this
creative style accessible to most casual photographers.
</p>
<p>More information and supplementary material can be found on our project
webpage: https://motion-mode.github.io/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01389">Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning. (arXiv:2308.01389v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hossain_J/0/1/0/all/0/1">Jumman Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Momtaz_M/0/1/0/all/0/1">Maliha Momtaz</a></p>
<p>Nowadays, autonomous cars are gaining traction due to their numerous
potential applications on battlefields and in resolving a variety of other
real-world challenges. The main goal of our project is to build an autonomous
system using DeepRacer which will follow a specific person (for our project, a
soldier) when they will be moving in any direction. Two main components to
accomplish this project is an optimized Single-Shot Multibox Detection (SSD)
object detection model and a Reinforcement Learning (RL) model. We accomplished
the task using SSD Lite instead of SSD and at the end, compared the results
among SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimental
results show that SSD Lite gives better performance among these three
techniques and exhibits a considerable boost in inference speed (~2-3 times)
without compromising accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01390">OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1">Anas Awadalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1">Irena Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1">Josh Gardner</a>, <a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1">Jack Hessel</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanafy_Y/0/1/0/all/0/1">Yusuf Hanafy</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wanrong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1">Kalyani Marathe</a>, <a href="http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1">Yonatan Bitton</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1">Samir Gadre</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1">Shiori Sagawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1">Jenia Jitsev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1">Pang Wei Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1">Gabriel Ilharco</a>, <a href="http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1">Mitchell Wortsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1">Ludwig Schmidt</a></p>
<p>We introduce OpenFlamingo, a family of autoregressive vision-language models
ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce
an open-source replication of DeepMind's Flamingo models. On seven
vision-language datasets, OpenFlamingo models average between 80 - 89% of
corresponding Flamingo performance. This technical report describes our models,
training data, hyperparameters, and evaluation suite. We share our models and
code at https://github.com/mlfoundations/open_flamingo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01399">Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jessy Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yuqing Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1">Olivia Watkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Hafner_D/0/1/0/all/0/1">Danijar Hafner</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca Dragan</a></p>
<p>To interact with humans in the world, agents need to understand the diverse
types of language that people use, relate them to the visual world, and act
based on them. While current agents learn to execute simple language
instructions from task rewards, we aim to build agents that leverage diverse
language that conveys general knowledge, describes the state of the world,
provides interactive feedback, and more. Our key idea is that language helps
agents predict the future: what will be observed, how the world will behave,
and which situations will be rewarded. This perspective unifies language
understanding with future prediction as a powerful self-supervised learning
objective. We present Dynalang, an agent that learns a multimodal world model
that predicts future text and image representations and learns to act from
imagined model rollouts. Unlike traditional agents that use language only to
predict actions, Dynalang acquires rich language understanding by using past
language also to predict future language, video, and rewards. In addition to
learning from online interaction in an environment, Dynalang can be pretrained
on datasets of text, video, or both without actions or rewards. From using
language hints in grid worlds to navigating photorealistic scans of homes,
Dynalang utilizes diverse types of language to improve task performance,
including environment descriptions, game rules, and instructions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01404">Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+OGara_A/0/1/0/all/0/1">Aidan O&#x27;Gara</a></p>
<p>Are current language models capable of deception and lie detection? We study
this question by introducing a text-based game called $\textit{Hoodwinked}$,
inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a
house and must find a key to escape, but one player is tasked with killing the
others. Each time a murder is committed, the surviving players have a natural
language discussion then vote to banish one player from the game. We conduct
experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find
evidence of deception and lie detection capabilities. The killer often denies
their crime and accuses others, leading to measurable effects on voting
outcomes. More advanced models are more effective killers, outperforming
smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide
evidence that this improvement is not mediated by different actions, but rather
by stronger deception capabilities during discussions. Overall, we find
substantial evidence that current language models are capable of deception. To
better evaluate the ability of AI agents to deceive humans, we make this game
publicly available at https://hoodwinked.ai/ .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01415">An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Junda Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaofeng Zhang</a></p>
<p>At the beginning era of large language model, it is quite critical to
generate a high-quality financial dataset to fine-tune a large language model
for financial related tasks. Thus, this paper presents a carefully designed
data creation pipeline for this purpose. Particularly, we initiate a dialogue
between an AI investor and financial expert using ChatGPT and incorporate the
feedback of human financial experts, leading to the refinement of the dataset.
This pipeline yielded a robust instruction tuning dataset comprised of 103k
multi-turn chats. Extensive experiments have been conducted on this dataset to
evaluate the model's performance by adopting an external GPT-4 as the judge.
The promising experimental results verify that our approach led to significant
advancements in generating accurate, relevant, and financial-style responses
from AI models, and thus providing a powerful tool for applications within the
financial sector.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01419">Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects. (arXiv:2308.01419v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Pu_X/0/1/0/all/0/1">Xingyue Pu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Cucuringu_M/0/1/0/all/0/1">Mihai Cucuringu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Dong_X/0/1/0/all/0/1">Xiaowen Dong</a></p>
<p>We present a novel methodology for modeling and forecasting multivariate
realized volatilities using customized graph neural networks to incorporate
spillover effects across stocks. The proposed model offers the benefits of
incorporating spillover effects from multi-hop neighbors, capturing nonlinear
relationships, and flexible training with different loss functions. Our
empirical findings provide compelling evidence that incorporating spillover
effects from multi-hop neighbors alone does not yield a clear advantage in
terms of predictive accuracy. However, modeling nonlinear spillover effects
enhances the forecasting accuracy of realized volatilities, particularly for
short-term horizons of up to one week. Moreover, our results consistently
indicate that training with the Quasi-likelihood loss leads to substantial
improvements in model performance compared to the commonly-used mean squared
error. A comprehensive series of empirical evaluations in alternative settings
confirm the robustness of our results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01420">SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text. (arXiv:2308.01420v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Badrinath_C/0/1/0/all/0/1">Charumathi Badrinath</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1">Weiwei Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1">Finale Doshi-Velez</a></p>
<p>A common way to explore text corpora is through low-dimensional projections
of the documents, where one hopes that thematically similar documents will be
clustered together in the projected space. However, popular algorithms for
dimensionality reduction of text corpora, like Latent Dirichlet Allocation
(LDA), often produce projections that do not capture human notions of document
similarity. We propose a semi-supervised human-in-the-loop LDA-based method for
learning topics that preserve semantically meaningful relationships between
documents in low-dimensional projections. On synthetic corpora, our method
yields more interpretable projections than baseline methods with only a
fraction of labels provided. On a real corpus, we obtain qualitatively similar
results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01421">Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agliari_E/0/1/0/all/0/1">Elena Agliari</a>, <a href="http://arxiv.org/find/cs/1/au:+Aquaro_M/0/1/0/all/0/1">Miriam Aquaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemanno_F/0/1/0/all/0/1">Francesco Alemanno</a>, <a href="http://arxiv.org/find/cs/1/au:+Fachechi_A/0/1/0/all/0/1">Alberto Fachechi</a></p>
<p>In this work we approach attractor neural networks from a machine learning
perspective: we look for optimal network parameters by applying a gradient
descent over a regularized loss function. Within this framework, the optimal
neuron-interaction matrices turn out to be a class of matrices which correspond
to Hebbian kernels revised by iteratively applying some unlearning protocols.
Remarkably, the number of unlearning steps is proved to be related to the
regularization hyperparameters of the loss function and to the training time.
Thus, we can design strategies to avoid overfitting that are formulated in
terms of the algebraic properties of the interaction matrix, or, equivalently,
in terms of regularization tuning and early-stopping strategies. The
generalization capabilities of these attractor networks are also investigated:
analytical results are obtained for random synthetic datasets, next, the
emerging picture is corroborated by numerical experiments that highlight the
existence of several regimes (i.e., overfitting, failure and success) as the
dataset parameters are varied.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01423">ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yeonghun Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jihan Kim</a></p>
<p>ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to
predict and generate of metal-organic frameworks (MOFs). By leveraging a
large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from
textual inputs and delivers appropriate responses, thus eliminating the
necessity for rigid structured queries. The system is comprised of three core
components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust
pipeline that manages a variety of tasks, including data retrieval, property
prediction, and structure generation. The study further explores the merits and
constraints of using large language models (LLMs) AI system in material
sciences using and showcases its transformative potential for future
advancements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01433">COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography. (arXiv:2308.01433v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Romero_N/0/1/0/all/0/1">Noemi Maritza L. Romero</a>, <a href="http://arxiv.org/find/eess/1/au:+Vasconcellos_R/0/1/0/all/0/1">Ricco Vasconcellos</a>, <a href="http://arxiv.org/find/eess/1/au:+Mendoza_M/0/1/0/all/0/1">Mariana R. Mendoza</a>, <a href="http://arxiv.org/find/eess/1/au:+Comba_J/0/1/0/all/0/1">Jo&#xe3;o L. D. Comba</a></p>
<p>The COVID-19 pandemic presented numerous challenges to healthcare systems
worldwide. Given that lung infections are prevalent among COVID-19 patients,
chest Computer Tomography (CT) scans have frequently been utilized as an
alternative method for identifying COVID-19 conditions and various other types
of pulmonary diseases. Deep learning architectures have emerged to automate the
identification of pulmonary disease types by leveraging CT scan slices as
inputs for classification models. This paper introduces COVID-VR, a novel
approach for classifying pulmonary diseases based on volume rendering images of
the lungs captured from multiple angles, thereby providing a comprehensive view
of the entire lung in each image. To assess the effectiveness of our proposal,
we compared it against competing strategies utilizing both private data
obtained from partner hospitals and a publicly available dataset. The results
demonstrate that our approach effectively identifies pulmonary lesions and
performs competitively when compared to slice-based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01436">Price-Aware Deep Learning for Electricity Markets. (arXiv:2308.01436v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dvorkin_V/0/1/0/all/0/1">Vladimir Dvorkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1">Ferdinando Fioretto</a></p>
<p>While deep learning gradually penetrates operational planning, its inherent
prediction errors may significantly affect electricity prices. This letter
examines how prediction errors propagate into electricity prices, revealing
notable pricing errors and their spatial disparity in congested power systems.
To improve fairness, we propose to embed electricity market-clearing
optimization as a deep learning layer. Differentiating through this layer
allows for balancing between prediction and pricing errors, as oppose to
minimizing prediction errors alone. This layer implicitly optimizes fairness
and controls the spatial distribution of price errors across the system. We
showcase the price-aware deep learning in the nexus of wind power forecasting
and short-term electricity market clearing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01438">Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations. (arXiv:2308.01438v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadshirazi_A/0/1/0/all/0/1">Ahmad Mohammadshirazi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nadafian_A/0/1/0/all/0/1">Aida Nadafian</a>, <a href="http://arxiv.org/find/cs/1/au:+Monsefi_A/0/1/0/all/0/1">Amin Karimi Monsefi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rafiei_M/0/1/0/all/0/1">Mohammad H. Rafiei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramnath_R/0/1/0/all/0/1">Rajiv Ramnath</a></p>
<p>Cost-effective sensors are capable of real-time capturing a variety of air
quality-related modalities from different pollutant concentrations to
indoor/outdoor humidity and temperature. Machine learning (ML) models are
capable of performing air-quality "ahead-of-time" approximations. Undoubtedly,
accurate indoor air quality approximation significantly helps provide a healthy
indoor environment, optimize associated energy consumption, and offer human
comfort. However, it is crucial to design an ML architecture to capture the
domain knowledge, so-called problem physics. In this study, we propose six
novel physics-based ML models for accurate indoor pollutant concentration
approximations. The proposed models include an adroit combination of
state-space concepts in physics, Gated Recurrent Units, and Decomposition
techniques. The proposed models were illustrated using data collected from five
offices in a commercial building in California. The proposed models are shown
to be less complex, computationally more efficient, and more accurate than
similar state-of-the-art transformer-based models. The superiority of the
proposed models is due to their relatively light architecture (computational
efficiency) and, more importantly, their ability to capture the underlying
highly nonlinear patterns embedded in the often contaminated sensor-collected
indoor air quality temporal data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01445">A digital twin framework for civil engineering structures. (arXiv:2308.01445v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Torzoni_M/0/1/0/all/0/1">Matteo Torzoni</a>, <a href="http://arxiv.org/find/math/1/au:+Tezzele_M/0/1/0/all/0/1">Marco Tezzele</a>, <a href="http://arxiv.org/find/math/1/au:+Mariani_S/0/1/0/all/0/1">Stefano Mariani</a>, <a href="http://arxiv.org/find/math/1/au:+Manzoni_A/0/1/0/all/0/1">Andrea Manzoni</a>, <a href="http://arxiv.org/find/math/1/au:+Willcox_K/0/1/0/all/0/1">Karen E. Willcox</a></p>
<p>The digital twin concept represents an appealing opportunity to advance
condition-based and predictive maintenance paradigms for civil engineering
systems, thus allowing reduced lifecycle costs, increased system safety, and
increased system availability. This work proposes a predictive digital twin
approach to the health monitoring, maintenance, and management planning of
civil engineering structures. The asset-twin coupled dynamical system is
encoded employing a probabilistic graphical model, which allows all relevant
sources of uncertainty to be taken into account. In particular, the
time-repeating observations-to-decisions flow is modeled using a dynamic
Bayesian network. Real-time structural health diagnostics are provided by
assimilating sensed data with deep learning models. The digital twin state is
continually updated in a sequential Bayesian inference fashion. This is then
exploited to inform the optimal planning of maintenance and management actions
within a dynamic decision-making framework. A preliminary offline phase
involves the population of training datasets through a reduced-order numerical
model and the computation of a health-dependent control policy. The strategy is
assessed on two synthetic case studies, involving a cantilever beam and a
railway bridge, demonstrating the dynamic decision-making capabilities of
health-aware digital twins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01469">VertexSerum: Poisoning Graph Neural Networks for Link Inference. (arXiv:2308.01469v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Ruyi Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1">Shijin Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaolin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1">Yunsi Fei</a></p>
<p>Graph neural networks (GNNs) have brought superb performance to various
applications utilizing graph structural data, such as social analysis and fraud
detection. The graph links, e.g., social relationships and transaction history,
are sensitive and valuable information, which raises privacy concerns when
using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel
graph poisoning attack that increases the effectiveness of graph link stealing
by amplifying the link connectivity leakage. To infer node adjacency more
accurately, we propose an attention mechanism that can be embedded into the
link detection network. Our experiments demonstrate that VertexSerum
significantly outperforms the SOTA link inference attack, improving the AUC
scores by an average of $9.8\%$ across four real-world datasets and three
different GNN structures. Furthermore, our experiments reveal the effectiveness
of VertexSerum in both black-box and online learning settings, further
validating its applicability in real-world scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01471">Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agro_B/0/1/0/all/0/1">Ben Agro</a>, <a href="http://arxiv.org/find/cs/1/au:+Sykora_Q/0/1/0/all/0/1">Quinlan Sykora</a>, <a href="http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1">Sergio Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1">Raquel Urtasun</a></p>
<p>A self-driving vehicle (SDV) must be able to perceive its surroundings and
predict the future behavior of other traffic participants. Existing works
either perform object detection followed by trajectory forecasting of the
detected objects, or predict dense occupancy and flow grids for the whole
scene. The former poses a safety concern as the number of detections needs to
be kept low for efficiency reasons, sacrificing object recall. The latter is
computationally expensive due to the high-dimensionality of the output grid,
and suffers from the limited receptive field inherent to fully convolutional
networks. Furthermore, both approaches employ many computational resources
predicting areas or objects that might never be queried by the motion planner.
This motivates our unified approach to perception and future prediction that
implicitly represents occupancy and flow over time with a single neural
network. Our method avoids unnecessary computation, as it can be directly
queried by the motion planner at continuous spatio-temporal locations.
Moreover, we design an architecture that overcomes the limited receptive field
of previous explicit occupancy prediction methods by adding an efficient yet
effective global attention mechanism. Through extensive experiments in both
urban and highway settings, we demonstrate that our implicit model outperforms
the current state-of-the-art. For more information, visit the project website:
https://waabi.ai/research/implicito.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01472">Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1">Florinel-Alin Croitoru</a>, <a href="http://arxiv.org/find/cs/1/au:+Hondru_V/0/1/0/all/0/1">Vlad Hondru</a>, <a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1">Radu Tudor Ionescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a></p>
<p>Text-to-image diffusion models such as Stable Diffusion have recently
attracted the interest of many researchers, and inverting the diffusion process
can play an important role in better understanding the generative process and
how to engineer prompts in order to obtain the desired images. To this end, we
introduce the new task of predicting the text prompt given an image generated
by a generative diffusion model. We combine a series of white-box and black-box
models (with and without access to the weights of the diffusion network) to
deal with the proposed task. We propose a novel learning framework comprising
of a joint prompt regression and multi-label vocabulary classification
objective that generates improved prompts. To further improve our method, we
employ a curriculum learning procedure that promotes the learning of
image-prompt pairs with lower labeling noise (i.e. that are better aligned),
and an unsupervised domain-adaptive kernel learning method that uses the
similarities between samples in the source and target domains as extra
features. We conduct experiments on the DiffusionDB data set, predicting text
prompts from images generated by Stable Diffusion. Our novel learning framework
produces excellent results on the aforementioned task, yielding the highest
gains when applied on the white-box model. In addition, we make an interesting
discovery: training a diffusion model on the prompt generation task can make
the model generate images that are much better aligned with the input prompts,
when the model is directly reused for text-to-image generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01475">Interpretable Machine Learning for Discovery: Statistical Challenges \&amp; Opportunities. (arXiv:2308.01475v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Allen_G/0/1/0/all/0/1">Genevera I. Allen</a>, <a href="http://arxiv.org/find/stat/1/au:+Gan_L/0/1/0/all/0/1">Luqin Gan</a>, <a href="http://arxiv.org/find/stat/1/au:+Zheng_L/0/1/0/all/0/1">Lili Zheng</a></p>
<p>New technologies have led to vast troves of large and complex datasets across
many scientific domains and industries. People routinely use machine learning
techniques to not only process, visualize, and make predictions from this big
data, but also to make data-driven discoveries. These discoveries are often
made using Interpretable Machine Learning, or machine learning models and
techniques that yield human understandable insights. In this paper, we discuss
and review the field of interpretable machine learning, focusing especially on
the techniques as they are often employed to generate new knowledge or make
discoveries from large data sets. We outline the types of discoveries that can
be made using Interpretable Machine Learning in both supervised and
unsupervised settings. Additionally, we focus on the grand challenge of how to
validate these discoveries in a data-driven manner, which promotes trust in
machine learning systems and reproducibility in science. We discuss validation
from both a practical perspective, reviewing approaches based on data-splitting
and stability, as well as from a theoretical perspective, reviewing statistical
results on model selection consistency and uncertainty quantification via
statistical inference. Finally, we conclude by highlighting open challenges in
using interpretable machine learning techniques to make discoveries, including
gaps between theory and practice for validating data-driven-discoveries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01481">Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Roy_A/0/1/0/all/0/1">Abhishek Roy</a>, <a href="http://arxiv.org/find/math/1/au:+Balasubramanian_K/0/1/0/all/0/1">Krishnakumar Balasubramanian</a></p>
<p>We study the online overlapping batch-means covariance estimator for
Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the
convergence rates of the covariance estimator are
$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and
$O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent
Markovian sampling, respectively, with $d$ representing dimensionality and $n$
denoting the number of observations or SGD iterations. Remarkably, these rates
match the best-known convergence rate previously established for the
independent and identically distributed ($\iid$) case by \cite{zhu2021online},
up to logarithmic factors. Our analysis overcomes significant challenges that
arise due to Markovian sampling, leading to the introduction of additional
error terms and complex dependencies between the blocks of the batch-means
covariance estimator. Moreover, we establish the convergence rate for the first
four moments of the $\ell_2$ norm of the error of SGD dynamics under
state-dependent Markovian data, which holds potential interest as an
independent result. To validate our theoretical findings, we provide numerical
illustrations to derive confidence intervals for SGD when training linear and
logistic regression models under Markovian sampling. Additionally, we apply our
approach to tackle the intriguing problem of strategic classification with
logistic regression, where adversaries can adaptively modify features during
the training process to increase their chances of being classified in a
specific target class.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01483">Efficient neural supersampling on a novel gaming dataset. (arXiv:2308.01483v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mercier_A/0/1/0/all/0/1">Antoine Mercier</a>, <a href="http://arxiv.org/find/cs/1/au:+Erasmus_R/0/1/0/all/0/1">Ruan Erasmus</a>, <a href="http://arxiv.org/find/cs/1/au:+Savani_Y/0/1/0/all/0/1">Yashesh Savani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhingra_M/0/1/0/all/0/1">Manik Dhingra</a>, <a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1">Fatih Porikli</a>, <a href="http://arxiv.org/find/cs/1/au:+Berger_G/0/1/0/all/0/1">Guillaume Berger</a></p>
<p>Real-time rendering for video games has become increasingly challenging due
to the need for higher resolutions, framerates and photorealism. Supersampling
has emerged as an effective solution to address this challenge. Our work
introduces a novel neural algorithm for supersampling rendered content that is
4 times more efficient than existing methods while maintaining the same level
of accuracy. Additionally, we introduce a new dataset which provides auxiliary
modalities such as motion vectors and depth generated using graphics rendering
features like viewport jittering and mipmap biasing at different resolutions.
We believe that this dataset fills a gap in the current dataset landscape and
can serve as a valuable resource to help measure progress in the field and
advance the state-of-the-art in super-resolution techniques for gaming content.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01490">Minimax Optimal $Q$ Learning with Nearest Neighbors. (arXiv:2308.01490v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Puning Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1">Lifeng Lai</a></p>
<p>$Q$ learning is a popular model free reinforcement learning method. Most of
existing works focus on analyzing $Q$ learning for finite state and action
spaces. If the state space is continuous, then the original $Q$ learning method
can not be directly used. A modification of the original $Q$ learning method
was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest
neighbors. Such modification makes $Q$ learning suitable for continuous state
space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$
function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower
bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not
efficient. This paper proposes two new $Q$ learning methods to bridge the gap
of convergence rates in (Shah and Xie, 2018), with one of them being offline,
while the other is online. Despite that we still use nearest neighbor approach
to estimate $Q$ function, the algorithms are crucially different from (Shah and
Xie, 2018). In particular, we replace the kernel nearest neighbor in
discretized region with a direct nearest neighbor approach. Consequently, our
approach significantly improves the convergence rate. Moreover, the time
complexity is also significantly improved in high dimensional state spaces. Our
analysis shows that both offline and online methods are minimax rate optimal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01508">Circumventing Concept Erasure Methods For Text-to-Image Generative Models. (arXiv:2308.01508v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1">Minh Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Marshall_K/0/1/0/all/0/1">Kelly O. Marshall</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1">Chinmay Hegde</a></p>
<p>Text-to-image generative models can produce photo-realistic images for an
extremely broad range of concepts, and their usage has proliferated widely
among the general public. On the flip side, these models have numerous
drawbacks, including their potential to generate images featuring sexually
explicit content, mirror artistic styles without permission, or even
hallucinate (or deepfake) the likenesses of celebrities. Consequently, various
methods have been proposed in order to "erase" sensitive concepts from
text-to-image models. In this work, we examine five recently proposed concept
erasure methods, and show that targeted concepts are not fully excised from any
of these methods. Specifically, we leverage the existence of special learned
word embeddings that can retrieve "erased" concepts from the sanitized models
with no alterations to their weights. Our results highlight the brittleness of
post hoc concept erasure methods, and call into question their use in the
algorithmic toolkit for AI safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01536">MFIM: Megapixel Facial Identity Manipulation. (arXiv:2308.01536v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1">Sanghyeon Na</a></p>
<p>Face swapping is a task that changes a facial identity of a given image to
that of another person. In this work, we propose a novel face-swapping
framework called Megapixel Facial Identity Manipulation (MFIM). The
face-swapping model should achieve two goals. First, it should be able to
generate a high-quality image. We argue that a model which is proficient in
generating a megapixel image can achieve this goal. However, generating a
megapixel image is generally difficult without careful model design. Therefore,
our model exploits pretrained StyleGAN in the manner of GAN-inversion to
effectively generate a megapixel image. Second, it should be able to
effectively transform the identity of a given image. Specifically, it should be
able to actively transform ID attributes (e.g., face shape and eyes) of a given
image into those of another person, while preserving ID-irrelevant attributes
(e.g., pose and expression). To achieve this goal, we exploit 3DMM that can
capture various facial attributes. Specifically, we explicitly supervise our
model to generate a face-swapped image with the desirable attributes using
3DMM. We show that our model achieves state-of-the-art performance through
extensive experiments. Furthermore, we propose a new operation called ID
mixing, which creates a new identity by semantically mixing the identities of
several people. It allows the user to customize the new identity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01543">Lode Enhancer: Level Co-creation Through Scaling. (arXiv:2308.01543v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhaumik_D/0/1/0/all/0/1">Debosmita Bhaumik</a>, <a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1">Julian Togelius</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1">Georgios N. Yannakakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1">Ahmed Khalifa</a></p>
<p>We explore AI-powered upscaling as a design assistance tool in the context of
creating 2D game levels. Deep neural networks are used to upscale artificially
downscaled patches of levels from the puzzle platformer game Lode Runner. The
trained networks are incorporated into a web-based editor, where the user can
create and edit levels at three different levels of resolution: 4x4, 8x8, and
16x16. An edit at any resolution instantly transfers to the other resolutions.
As upscaling requires inventing features that might not be present at lower
resolutions, we train neural networks to reproduce these features. We introduce
a neural network architecture that is capable of not only learning upscaling
but also giving higher priority to less frequent tiles. To investigate the
potential of this tool and guide further development, we conduct a qualitative
study with 3 designers to understand how they use it. Designers enjoyed
co-designing with the tool, liked its underlying concept, and provided feedback
for further improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01546">MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Ke Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yusong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haohe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nezhurina_M/0/1/0/all/0/1">Marianna Nezhurina</a>, <a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1">Taylor Berg-Kirkpatrick</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubnov_S/0/1/0/all/0/1">Shlomo Dubnov</a></p>
<p>Diffusion models have shown promising results in cross-modal generation
tasks, including text-to-image and text-to-audio generation. However,
generating music, as a special type of audio, presents unique challenges due to
limited availability of music data and sensitive issues related to copyright
and plagiarism. In this paper, to tackle these challenges, we first construct a
state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion
and AudioLDM architectures to the music domain. We achieve this by retraining
the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN
vocoder, as components of MusicLDM, on a collection of music data samples.
Then, to address the limitations of training data and to avoid plagiarism, we
leverage a beat tracking model and propose two different mixup strategies for
data augmentation: beat-synchronous audio mixup and beat-synchronous latent
mixup, which recombine training audio directly or via a latent embeddings
space, respectively. Such mixup strategies encourage the model to interpolate
between musical training samples and generate new music within the convex hull
of the training data, making the generated music more diverse while still
staying faithful to the corresponding style. In addition to popular evaluation
metrics, we design several new evaluation metrics based on CLAP score to
demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies
improve both the quality and novelty of generated music, as well as the
correspondence between input text and generated music.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01552">InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Po-Lin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Cheng-Shang Chang</a></p>
<p>This research paper delves into the integration of OpenAI's ChatGPT into
embodied agent systems, evaluating its influence on interactive decision-making
benchmark. Drawing a parallel to the concept of people assuming roles according
to their unique strengths, we introduce InterAct. In this approach, we feed
ChatGPT with varied prompts, assigning it a numerous roles like a checker and a
sorter, then integrating them with the original language model. Our research
shows a remarkable success rate of 98% in AlfWorld, which consists of 6
different tasks in a simulated household environment, emphasizing the
significance of proficient prompt engineering. The results highlight ChatGPT's
competence in comprehending and performing intricate tasks effectively in
real-world settings, thus paving the way for further advancements in task
planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01557">Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1">Joao Carvalho</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1">An T. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Baierl_M/0/1/0/all/0/1">Mark Baierl</a>, <a href="http://arxiv.org/find/cs/1/au:+Koert_D/0/1/0/all/0/1">Dorothea Koert</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1">Jan Peters</a></p>
<p>Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01562">Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Pervej_M/0/1/0/all/0/1">Md Ferdous Pervej</a>, <a href="http://arxiv.org/find/eess/1/au:+Jin_R/0/1/0/all/0/1">Richeng Jin</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1">Huaiyu Dai</a></p>
<p>While a practical wireless network has many tiers where end users do not
directly communicate with the central server, the users' devices have limited
computation and battery powers, and the serving base station (BS) has a fixed
bandwidth. Owing to these practical constraints and system models, this paper
leverages model pruning and proposes a pruning-enabled hierarchical federated
learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper
bound of the convergence rate that clearly demonstrates the impact of the model
pruning and wireless communications between the clients and the associated BS.
Then we jointly optimize the model pruning ratio, central processing unit (CPU)
frequency and transmission power of the clients in order to minimize the
controllable terms of the convergence bound under strict delay and energy
constraints. However, since the original problem is not convex, we perform
successive convex approximation (SCA) and jointly optimize the parameters for
the relaxed convex problem. Through extensive simulation, we validate the
effectiveness of our proposed PHFL algorithm in terms of test accuracy, wall
clock time, energy consumption and bandwidth requirement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01566">Fast Slate Policy Optimization: Going Beyond Plackett-Luce. (arXiv:2308.01566v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sakhi_O/0/1/0/all/0/1">Otmane Sakhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohde_D/0/1/0/all/0/1">David Rohde</a>, <a href="http://arxiv.org/find/cs/1/au:+Chopin_N/0/1/0/all/0/1">Nicolas Chopin</a></p>
<p>An increasingly important building block of large scale machine learning
systems is based on returning slates; an ordered lists of items given a query.
Applications of this technology include: search, information retrieval and
recommender systems. When the action space is large, decision systems are
restricted to a particular structure to complete online queries quickly. This
paper addresses the optimization of these large scale decision systems given an
arbitrary reward function. We cast this learning problem in a policy
optimization framework and propose a new class of policies, born from a novel
relaxation of decision functions. This results in a simple, yet efficient
learning algorithm that scales to massive action spaces. We compare our method
to the commonly adopted Plackett-Luce policy class and demonstrate the
effectiveness of our approach on problems with action space sizes in the order
of millions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01573">Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS. (arXiv:2308.01573v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ko_M/0/1/0/all/0/1">Myeongjin Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yong-Hoon Choi</a></p>
<p>The diffusion model is capable of generating high-quality data through a
probabilistic approach. However, it suffers from the drawback of slow
generation speed due to the requirement of a large number of time steps. To
address this limitation, recent models such as denoising diffusion implicit
models (DDIM) focus on generating samples without directly modeling the
probability distribution, while models like denoising diffusion generative
adversarial networks (GAN) combine diffusion processes with GANs. In the field
of speech synthesis, a recent diffusion speech synthesis model called
DiffGAN-TTS, utilizing the structure of GANs, has been introduced and
demonstrates superior performance in both speech quality and generation speed.
In this paper, to further enhance the performance of DiffGAN-TTS, we propose a
speech synthesis model with two discriminators: a diffusion discriminator for
learning the distribution of the reverse process and a spectrogram
discriminator for learning the distribution of the generated data. Objective
metrics such as structural similarity index measure (SSIM), mel-cepstral
distortion (MCD), F0 root mean squared error (F0 RMSE), short-time objective
intelligibility (STOI), perceptual evaluation of speech quality (PESQ), as well
as subjective metrics like mean opinion score (MOS), are used to evaluate the
performance of the proposed model. The evaluation results show that the
proposed model outperforms recent state-of-the-art models such as FastSpeech2
and DiffGAN-TTS in various metrics. Our implementation and audio samples are
located on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01578">Unsupervised Representation Learning for Time Series: A Review. (arXiv:2308.01578v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1">Qianwen Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1">Hangwei Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yonghui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhiqi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Lizhen Cui</a></p>
<p>Unsupervised representation learning approaches aim to learn discriminative
feature representations from unlabeled data, without the requirement of
annotating every sample. Enabling unsupervised representation learning is
extremely crucial for time series data, due to its unique annotation bottleneck
caused by its complex characteristics and lack of visual cues compared with
other data modalities. In recent years, unsupervised representation learning
techniques have advanced rapidly in various domains. However, there is a lack
of systematic analysis of unsupervised representation learning approaches for
time series. To fill the gap, we conduct a comprehensive literature review of
existing rapidly evolving unsupervised representation learning approaches for
time series. Moreover, we also develop a unified and standardized library,
named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast
implementations and unified evaluations on various models. With ULTS, we
empirically evaluate state-of-the-art approaches, especially the rapidly
evolving contrastive learning methods, on 9 diverse real-world datasets. We
further discuss practical considerations as well as open research challenges on
unsupervised representation learning for time series to facilitate future
research in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01602">Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks. (arXiv:2308.01602v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Franco_N/0/1/0/all/0/1">Nicola Rares Franco</a>, <a href="http://arxiv.org/find/math/1/au:+Fresca_S/0/1/0/all/0/1">Stefania Fresca</a>, <a href="http://arxiv.org/find/math/1/au:+Tombari_F/0/1/0/all/0/1">Filippo Tombari</a>, <a href="http://arxiv.org/find/math/1/au:+Manzoni_A/0/1/0/all/0/1">Andrea Manzoni</a></p>
<p>Mesh-based simulations play a key role when modeling complex physical systems
that, in many disciplines across science and engineering, require the solution
of parametrized time-dependent nonlinear partial differential equations (PDEs).
In this context, full order models (FOMs), such as those relying on the finite
element method, can reach high levels of accuracy, however often yielding
intensive simulations to run. For this reason, surrogate models are developed
to replace computationally expensive solvers with more efficient ones, which
can strike favorable trade-offs between accuracy and efficiency. This work
explores the potential usage of graph neural networks (GNNs) for the simulation
of time-dependent PDEs in the presence of geometrical variability. In
particular, we propose a systematic strategy to build surrogate models based on
a data-driven time-stepping scheme where a GNN architecture is used to
efficiently evolve the system. With respect to the majority of surrogate
models, the proposed approach stands out for its ability of tackling problems
with parameter dependent spatial domains, while simultaneously generalizing to
different geometries and mesh resolutions. We assess the effectiveness of the
proposed approach through a series of numerical experiments, involving both
two- and three-dimensional problems, showing that GNNs can provide a valid
alternative to traditional surrogate models in terms of computational
efficiency and generalization to new scenarios. We also assess, from a
numerical standpoint, the importance of using GNNs, rather than classical dense
deep neural networks, for the proposed framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01606">Unsupervised Multiplex Graph Learning with Complementary and Consistent Information. (arXiv:2308.01606v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Liang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaofeng Zhu</a></p>
<p>Unsupervised multiplex graph learning (UMGL) has been shown to achieve
significant effectiveness for different downstream tasks by exploring both
complementary information and consistent information among multiple graphs.
However, previous methods usually overlook the issues in practical
applications, i.e., the out-of-sample issue and the noise issue. To address the
above issues, in this paper, we propose an effective and efficient UMGL method
to explore both complementary and consistent information. To do this, our
method employs multiple MLP encoders rather than graph convolutional network
(GCN) to conduct representation learning with two constraints, i.e., preserving
the local graph structure among nodes to handle the out-of-sample issue, and
maximizing the correlation of multiple node representations to handle the noise
issue. Comprehensive experiments demonstrate that our proposed method achieves
superior effectiveness and efficiency over the comparison methods and
effectively tackles those two issues. Code is available at
https://github.com/LarryUESTC/CoCoMG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01609">Feature Noise Boosts DNN Generalization under Label Noise. (arXiv:2308.01609v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Lu Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaoshuang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a></p>
<p>The presence of label noise in the training data has a profound impact on the
generalization of deep neural networks (DNNs). In this study, we introduce and
theoretically demonstrate a simple feature noise method, which directly adds
noise to the features of training data, can enhance the generalization of DNNs
under label noise. Specifically, we conduct theoretical analyses to reveal that
label noise leads to weakened DNN generalization by loosening the PAC-Bayes
generalization bound, and feature noise results in better DNN generalization by
imposing an upper bound on the mutual information between the model weights and
the features, which constrains the PAC-Bayes generalization bound. Furthermore,
to ensure effective generalization of DNNs in the presence of label noise, we
conduct application analyses to identify the optimal types and levels of
feature noise to add for obtaining desirable label noise generalization.
Finally, extensive experimental results on several popular datasets demonstrate
the feature noise method can significantly enhance the label noise
generalization of the state-of-the-art label noise method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01614">Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gannamaneni_S/0/1/0/all/0/1">Sujan Sai Gannamaneni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mock_M/0/1/0/all/0/1">Michael Mock</a>, <a href="http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1">Maram Akila</a></p>
<p>With the advancement of DNNs into safety-critical applications, testing
approaches for such models have gained more attention. A current direction is
the search for and identification of systematic weaknesses that put safety
assumptions based on average performance values at risk. Such weaknesses can
take on the form of (semantically coherent) subsets or areas in the input space
where a DNN performs systematically worse than its expected average. However,
it is non-trivial to attribute the reason for such observed low performances to
the specific semantic features that describe the subset. For instance,
inhomogeneities within the data w.r.t. other (non-considered) attributes might
distort results. However, taking into account all (available) attributes and
their interaction is often computationally highly expensive. Inspired by
counterfactual explanations, we propose an effective and computationally cheap
algorithm to validate the semantic attribution of existing subsets, i.e., to
check whether the identified attribute is likely to have caused the degraded
performance. We demonstrate this approach on an example from the autonomous
driving domain using highly annotated simulated data, where we show for a
semantic segmentation model that (i) performance differences among the
different pedestrian assets exist, but (ii) only in some cases is the asset
type itself the reason for this reduction in the performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01621">A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1">Hang Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1">Bing Bai</a></p>
<p>This paper introduces a new Convolutional Neural Network (ConvNet)
architecture inspired by a class of partial differential equations (PDEs)
called quasi-linear hyperbolic systems. With comparable performance on image
classification task, it allows for the modification of the weights via a
continuous group of symmetry. This is a significant shift from traditional
models where the architecture and weights are essentially fixed. We wish to
promote the (internal) symmetry as a new desirable property for a neural
network, and to draw attention to the PDE perspective in analyzing and
interpreting ConvNets in the broader Deep Learning community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01626">Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Motogna_A/0/1/0/all/0/1">Alexandru Motogna</a>, <a href="http://arxiv.org/find/cs/1/au:+Groza_A/0/1/0/all/0/1">Adrian Groza</a></p>
<p>An attractive book cover is important for the success of a book. In this
paper, we apply Generative Adversarial Networks (GANs) to the book covers
domain, using different methods for training in order to obtain better
generated images. We interleave GANs with knowledge graphs to alter the input
title to obtain multiple possible options for any given title, which are then
used as an augmented input to the generator. Finally, we use the discriminator
obtained during the training phase to select the best images generated with new
titles. Our method performed better at generating book covers than previous
attempts, and the knowledge graph gives better options to the book author or
editor compared to using GANs alone.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01649">MARLIM: Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2308.01649v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leluc_R/0/1/0/all/0/1">R&#xe9;mi Leluc</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadoche_E/0/1/0/all/0/1">Elie Kadoche</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertoncello_A/0/1/0/all/0/1">Antoine Bertoncello</a>, <a href="http://arxiv.org/find/cs/1/au:+Gourvenec_S/0/1/0/all/0/1">S&#xe9;bastien Gourv&#xe9;nec</a></p>
<p>Maintaining a balance between the supply and demand of products by optimizing
replenishment decisions is one of the most important challenges in the supply
chain industry. This paper presents a novel reinforcement learning framework
called MARLIM, to address the inventory management problem for a single-echelon
multi-products supply chain with stochastic demands and lead-times. Within this
context, controllers are developed through single or multiple agents in a
cooperative setting. Numerical experiments on real data demonstrate the
benefits of reinforcement learning methods over traditional baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01650">UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification. (arXiv:2308.01650v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_M/0/1/0/all/0/1">Minhao Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1">Zhongxue Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yutong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_D/0/1/0/all/0/1">Dongyan Sui</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1">Chun Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1">Siyang Leng</a></p>
<p>Graph and hypergraph representation learning has attracted increasing
attention from various research fields. Despite the decent performance and
fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural
Networks (HGNNs), and their well-designed variants, on some commonly used
benchmark graphs and hypergraphs, they are outperformed by even a simple
Multi-Layer Perceptron. This observation motivates a reexamination of the
design paradigm of the current GNNs and HGNNs and poses challenges of
extracting graph features effectively. In this work, a universal feature
encoder for both graph and hypergraph representation learning is designed,
called UniG-Encoder. The architecture starts with a forward transformation of
the topological relationships of connected nodes into edge or hyperedge
features via a normalized projection matrix. The resulting edge/hyperedge
features, together with the original node features, are fed into a neural
network. The encoded node embeddings are then derived from the reversed
transformation, described by the transpose of the projection matrix, of the
network's output, which can be further used for tasks such as node
classification. The proposed architecture, in contrast to the traditional
spectral-based and/or message passing approaches, simultaneously and
comprehensively exploits the node features and graph/hypergraph topologies in
an efficient and unified manner, covering both heterophilic and homophilic
graphs. The designed projection matrix, encoding the graph features, is
intuitive and interpretable. Extensive experiments are conducted and
demonstrate the superior performance of the proposed framework on twelve
representative hypergraph datasets and six real-world graph datasets, compared
to the state-of-the-art methods. Our implementation is available online at
https://github.com/MinhZou/UniG-Encoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01674">End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mayfrank_D/0/1/0/all/0/1">Daniel Mayfrank</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitsos_A/0/1/0/all/0/1">Alexander Mitsos</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahmen_M/0/1/0/all/0/1">Manuel Dahmen</a></p>
<p>(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic
system models that are sufficiently accurate in all relevant state-space
regions. These models must also be computationally cheap enough to ensure
real-time tractability. Data-driven surrogate models for mechanistic models can
be used to reduce the computational burden of (e)NMPC; however, such models are
typically trained by system identification for maximum average prediction
accuracy on simulation samples and perform suboptimally as part of actual
(e)NMPC. We present a method for end-to-end reinforcement learning of dynamic
surrogate models for optimal performance in (e)NMPC applications, resulting in
predictive controllers that strike a favorable balance between control
performance and computational demand. We validate our method on two
applications derived from an established nonlinear continuous stirred-tank
reactor model. We compare the controller performance to that of MPCs utilizing
models trained by the prevailing maximum prediction accuracy paradigm, and
model-free neural network controllers trained using reinforcement learning. We
show that our method matches the performance of the model-free neural network
controllers while consistently outperforming models derived from system
identification. Additionally, we show that the MPC policies can react to
changes in the control setting without retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01677">Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity. (arXiv:2308.01677v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Garber_D/0/1/0/all/0/1">Dan Garber</a>, <a href="http://arxiv.org/find/math/1/au:+Kaplan_A/0/1/0/all/0/1">Atara Kaplan</a></p>
<p>We consider convex relaxations for recovering low-rank tensors based on
constrained minimization over a ball induced by the tensor nuclear norm,
recently introduced in \cite{tensor_tSVD}. We build on a recent line of results
that considered convex relaxations for the recovery of low-rank matrices and
established that under a strict complementarity condition (SC), both the
convergence rate and per-iteration runtime of standard gradient methods may
improve dramatically. We develop the appropriate strict complementarity
condition for the tensor nuclear norm ball and obtain the following main
results under this condition: 1. When the objective to minimize is of the form
$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and
$\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds,
which implies linear convergence rates for standard projected gradient methods,
despite the fact that $f$ need not be strongly convex. 2. For a smooth
objective function, when initialized in certain proximity of an optimal
solution which satisfies SC, standard projected gradient methods only require
SVD computations (for projecting onto the tensor nuclear norm ball) of rank
that matches the tubal rank of the optimal solution. In particular, when the
tubal rank is constant, this implies nearly linear (in the size of the tensor)
runtime per iteration, as opposed to super linear without further assumptions.
3. For a nonsmooth objective function which admits a popular smooth
saddle-point formulation, we derive similar results to the latter for the well
known extragradient method. An additional contribution which may be of
independent interest, is the rigorous extension of many basic results regarding
tensors of arbitrary order, which were previously obtained only for third-order
tensors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01682">Evaluating Link Prediction Explanations for Graph Neural Networks. (arXiv:2308.01682v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Borile_C/0/1/0/all/0/1">Claudio Borile</a>, <a href="http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1">Alan Perotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Panisson_A/0/1/0/all/0/1">Andr&#xe9; Panisson</a></p>
<p>Graph Machine Learning (GML) has numerous applications, such as node/graph
classification and link prediction, in real-world domains. Providing
human-understandable explanations for GML models is a challenging yet
fundamental task to foster their adoption, but validating explanations for link
prediction models has received little attention. In this paper, we provide
quantitative metrics to assess the quality of link prediction explanations,
with or without ground-truth. State-of-the-art explainability methods for Graph
Neural Networks are evaluated using these metrics. We discuss how underlying
assumptions and technical details specific to the link prediction task, such as
the choice of distance between node embeddings, can influence the quality of
the explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01729">Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Duval_F/0/1/0/all/0/1">Francis Duval</a>, <a href="http://arxiv.org/find/stat/1/au:+Boucher_J/0/1/0/all/0/1">Jean-Philippe Boucher</a>, <a href="http://arxiv.org/find/stat/1/au:+Pigeon_M/0/1/0/all/0/1">Mathieu Pigeon</a></p>
<p>We present novel cross-sectional and longitudinal claim count models for
vehicle insurance built upon the Combined Actuarial Neural Network (CANN)
framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach
combines a classical actuarial model, such as a generalized linear model, with
a neural network. This blending of models results in a two-component model
comprising a classical regression model and a neural network part. The CANN
model leverages the strengths of both components, providing a solid foundation
and interpretability from the classical model while harnessing the flexibility
and capacity to capture intricate relationships and interactions offered by the
neural network. In our proposed models, we use well-known log-linear claim
count regression models for the classical regression part and a multilayer
perceptron (MLP) for the neural network part. The MLP part is used to process
telematics car driving data given as a vector characterizing the driving
behavior of each insured driver. In addition to the Poisson and negative
binomial distributions for cross-sectional data, we propose a procedure for
training our CANN model with a multivariate negative binomial (MVNB)
specification. By doing so, we introduce a longitudinal model that accounts for
the dependence between contracts from the same insured. Our results reveal that
the CANN models exhibit superior performance compared to log-linear models that
rely on manually engineered telematics features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01731">Quantification of Predictive Uncertainty via Inference-Time Sampling. (arXiv:2308.01731v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tothova_K/0/1/0/all/0/1">Katar&#xed;na T&#xf3;thov&#xe1;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ladicky_L/0/1/0/all/0/1">&#x13d;ubor Ladick&#xfd;</a>, <a href="http://arxiv.org/find/cs/1/au:+Thul_D/0/1/0/all/0/1">Daniel Thul</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>, <a href="http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1">Ender Konukoglu</a></p>
<p>Predictive variability due to data ambiguities has typically been addressed
via construction of dedicated models with built-in probabilistic capabilities
that are trained to predict uncertainty estimates as variables of interest.
These approaches require distinct architectural components and training
mechanisms, may include restrictive assumptions and exhibit overconfidence,
i.e., high confidence in imprecise predictions. In this work, we propose a
post-hoc sampling strategy for estimating predictive uncertainty accounting for
data ambiguity. The method can generate different plausible outputs for a given
input and does not assume parametric forms of predictive distributions. It is
architecture agnostic and can be applied to any feed-forward deterministic
network without changes to the architecture or training procedure. Experiments
on regression tasks on imaging and non-imaging input data show the method's
ability to generate diverse and multi-modal predictive distributions, and a
desirable correlation of the estimated uncertainty with the prediction error.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01737">MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jianghao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1">Yanru Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xinyi Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1">Ruiming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weinan Zhang</a></p>
<p>With the widespread application of personalized online services,
click-through rate (CTR) prediction has received more and more attention and
research. The most prominent features of CTR prediction are its multi-field
categorical data format, and vast and daily-growing data volume. The large
capacity of neural models helps digest such massive amounts of data under the
supervised learning paradigm, yet they fail to utilize the substantial data to
its full potential, since the 1-bit click signal is not sufficient to guide the
model to learn capable representations of features and instances. The
self-supervised learning paradigm provides a more promising pretrain-finetune
solution to better exploit the large amount of user click logs, and learn more
generalized and effective representations. However, self-supervised learning
for CTR prediction is still an open question, since current works on this line
are only preliminary and rudimentary. To this end, we propose a Model-agnostic
pretraining (MAP) framework that applies feature corruption and recovery on
multi-field categorical data, and more specifically, we derive two practical
algorithms: masked feature prediction (MFP) and replaced feature detection
(RFD). MFP digs into feature interactions within each instance through masking
and predicting a small portion of input features, and introduces noise
contrastive estimation (NCE) to handle large feature spaces. RFD further turns
MFP into a binary classification mode through replacing and detecting changes
in input features, making it even simpler and more effective for CTR
pretraining. Our extensive experiments on two real-world large-scale datasets
(i.e., Avazu, Criteo) demonstrate the advantages of these two methods on
several strong backbones (e.g., DCNv2, DeepFM), and achieve new
state-of-the-art performance in terms of both effectiveness and efficiency for
CTR prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01742">Exploiting Multi-Label Correlation in Label Distribution Learning. (arXiv:2308.01742v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+geng_Z/0/1/0/all/0/1">Zhiqiang Kou jing wang yuheng jia xin geng</a></p>
<p>Label Distribution Learning (LDL) is a novel machine learning paradigm that
assigns label distribution to each instance. Many LDL methods proposed to
leverage label correlation in the learning process to solve the
exponential-sized output space; among these, many exploited the low-rank
structure of label distribution to capture label correlation. However, recent
studies disclosed that label distribution matrices are typically full-rank,
posing challenges to those works exploiting low-rank label correlation. Note
that multi-label is generally low-rank; low-rank label correlation is widely
adopted in multi-label learning (MLL) literature. Inspired by that, we
introduce an auxiliary MLL process in LDL and capture low-rank label
correlation on that MLL rather than LDL. In such a way, low-rank label
correlation is appropriately exploited in our LDL methods. We conduct
comprehensive experiments and demonstrate that our methods are superior to
existing LDL methods. Besides, the ablation studies justify the advantages of
exploiting low-rank label correlation in the auxiliary MLL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01743">Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization. (arXiv:2308.01743v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Posch_S/0/1/0/all/0/1">Stefan Posch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gossnitzer_C/0/1/0/all/0/1">Clemens G&#xf6;&#xdf;nitzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohrhofer_F/0/1/0/all/0/1">Franz Rohrhofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1">Bernhard C. Geiger</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimmer_A/0/1/0/all/0/1">Andreas Wimmer</a></p>
<p>The turbulent jet ignition concept using prechambers is a promising solution
to achieve stable combustion at lean conditions in large gas engines, leading
to high efficiency at low emission levels. Due to the wide range of design and
operating parameters for large gas engine prechambers, the preferred method for
evaluating different designs is computational fluid dynamics (CFD), as testing
in test bed measurement campaigns is time-consuming and expensive. However, the
significant computational time required for detailed CFD simulations due to the
complexity of solving the underlying physics also limits its applicability. In
optimization settings similar to the present case, i.e., where the evaluation
of the objective function(s) is computationally costly, Bayesian optimization
has largely replaced classical design-of-experiment. Thus, the present study
deals with the computationally efficient Bayesian optimization of large gas
engine prechambers design using CFD simulation. Reynolds-averaged-Navier-Stokes
simulations are used to determine the target values as a function of the
selected prechamber design parameters. The results indicate that the chosen
strategy is effective to find a prechamber design that achieves the desired
target values.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01744">Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning. (arXiv:2308.01744v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1">Pier Giuseppe Sessa</a>, <a href="http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1">Pierre Laforgue</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1">Nicol&#xf2; Cesa-Bianchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a></p>
<p>Multitask learning is a powerful framework that enables one to simultaneously
learn multiple related tasks by sharing information between them. Quantifying
uncertainty in the estimated tasks is of pivotal importance for many downstream
applications, such as online or active learning. In this work, we provide novel
multitask confidence intervals in the challenging agnostic setting, i.e., when
neither the similarity between tasks nor the tasks' features are available to
the learner. The obtained intervals do not require i.i.d. data and can be
directly applied to bound the regret in online learning. Through a refined
analysis of the multitask information gain, we obtain new regret guarantees
that, depending on a task similarity parameter, can significantly improve over
treating tasks independently. We further propose a novel online learning
algorithm that achieves such improved regret without knowing this parameter in
advance, i.e., automatically adapting to task similarity. As a second key
application of our results, we introduce a novel multitask active learning
setup where several tasks must be simultaneously optimized, but only one of
them can be queried for feedback by the learner at each round. For this
problem, we design a no-regret algorithm that uses our confidence intervals to
decide which task should be queried. Finally, we empirically validate our
bounds and algorithms on synthetic and real-world (drug discovery) data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01746">Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants. (arXiv:2308.01746v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yibo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Haobo Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangtai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jianlong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lefei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a></p>
<p>How to enable learnability for new classes while keeping the capability well
on old classes has been a crucial challenge for class incremental learning.
Beyond the normal case, long-tail class incremental learning and few-shot class
incremental learning are also proposed to consider the data imbalance and data
scarcity, respectively, which are common in real-world implementations and
further exacerbate the well-known problem of catastrophic forgetting. Existing
methods are specifically proposed for one of the three tasks. In this paper, we
offer a unified solution to the misalignment dilemma in the three tasks.
Concretely, we propose neural collapse terminus that is a fixed structure with
the maximal equiangular inter-class separation for the whole label space. It
serves as a consistent target throughout the incremental training to avoid
dividing the feature space incrementally. For CIL and LTCIL, we further propose
a prototype evolving scheme to drive the backbone features into our neural
collapse terminus smoothly. Our method also works for FSCIL with only minor
adaptations. Theoretical analysis indicates that our method holds the neural
collapse optimality in an incremental fashion regardless of data imbalance or
data scarcity. We also design a generalized case where we do not know the total
number of classes and whether the data distribution is normal, long-tail, or
few-shot for each coming session, to test the generalizability of our method.
Extensive experiments with multiple datasets are conducted to demonstrate the
effectiveness of our unified solution to all the three tasks and the
generalized case.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01759">Bag of Policies for Distributional Deep Exploration. (arXiv:2308.01759v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nachkov_A/0/1/0/all/0/1">Asen Nachkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Luchen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luise_G/0/1/0/all/0/1">Giulia Luise</a>, <a href="http://arxiv.org/find/cs/1/au:+Valdettaro_F/0/1/0/all/0/1">Filippo Valdettaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1">Aldo Faisal</a></p>
<p>Efficient exploration in complex environments remains a major challenge for
reinforcement learning (RL). Compared to previous Thompson sampling-inspired
mechanisms that enable temporally extended exploration, i.e., deep exploration,
we focus on deep exploration in distributional RL. We develop here a general
purpose approach, Bag of Policies (BoP), that can be built on top of any return
distribution estimator by maintaining a population of its copies. BoP consists
of an ensemble of multiple heads that are updated independently. During
training, each episode is controlled by only one of the heads and the collected
state-action pairs are used to update all heads off-policy, leading to distinct
learning signals for each head which diversify learning and behaviour. To test
whether optimistic ensemble method can improve on distributional RL as did on
scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a
population of distributional actor-critics using Bayesian Distributional Policy
Gradients (BDPG). The population thus approximates a posterior distribution of
return distributions along with a posterior distribution of policies. Another
benefit of building upon BDPG is that it allows to analyze global posterior
uncertainty along with local curiosity bonus simultaneously for exploration. As
BDPG is already an optimistic method, this pairing helps to investigate if
optimism is accumulatable in distributional RL. Overall BoP results in greater
robustness and speed during learning as demonstrated by our experimental
results on ALE Atari games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01771">Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment. (arXiv:2308.01771v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shokrollahi1_Y/0/1/0/all/0/1">Yasin Shokrollahi1</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong1_P/0/1/0/all/0/1">Pengfei Dong1</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1">Linxia Gu</a></p>
<p>This study investigated the potential of end-to-end deep learning tools as a
more effective substitute for FEM in predicting stress-strain fields within 2D
cross sections of arterial wall. We first proposed a U-Net based fully
convolutional neural network (CNN) to predict the von Mises stress and strain
distribution based on the spatial arrangement of calcification within arterial
wall cross-sections. Further, we developed a conditional generative adversarial
network (cGAN) to enhance, particularly from the perceptual perspective, the
prediction accuracy of stress and strain field maps for arterial walls with
various calcification quantities and spatial configurations. On top of U-Net
and cGAN, we also proposed their ensemble approaches, respectively, to further
improve the prediction accuracy of field maps. Our dataset, consisting of input
and output images, was generated by implementing boundary conditions and
extracting stress-strain field maps. The trained U-Net models can accurately
predict von Mises stress and strain fields, with structural similarity index
scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for
stress and strain, respectively, on a reserved test set. Meanwhile, the cGAN
models in a combination of ensemble and transfer learning techniques
demonstrate high accuracy in predicting von Mises stress and strain fields, as
evidenced by SSIM scores of 0.890 for stress and 0.803 for strain.
Additionally, mean squared errors of 0.008 for stress and 0.017 for strain
further support the model's performance on a designated test set. Overall, this
study developed a surrogate model for finite element analysis, which can
accurately and efficiently predict stress-strain fields of arterial walls
regardless of complex geometries and boundary conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01789">Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances. (arXiv:2308.01789v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Turati_G/0/1/0/all/0/1">Gloria Turati</a> (1), <a href="http://arxiv.org/find/quant-ph/1/au:+Dacrema_M/0/1/0/all/0/1">Maurizio Ferrari Dacrema</a> (1), <a href="http://arxiv.org/find/quant-ph/1/au:+Cremonesi_P/0/1/0/all/0/1">Paolo Cremonesi</a> (1) ((1) Politecnico di Milano)</p>
<p>In recent years, Variational Quantum Algorithms (VQAs) have emerged as a
promising approach for solving optimization problems on quantum computers in
the NISQ era. However, one limitation of VQAs is their reliance on
fixed-structure circuits, which may not be taylored for specific problems or
hardware configurations. A leading strategy to address this issue are
Adaptative VQAs, which dynamically modify the circuit structure by adding and
removing gates, and optimize their parameters during the training. Several
Adaptative VQAs, based on heuristics such as circuit shallowness, entanglement
capability and hardware compatibility, have already been proposed in the
literature, but there is still lack of a systematic comparison between the
different methods. In this paper, we aim to fill this gap by analyzing three
Adaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable
Ansatz (VAns), already proposed in the literature, and Random Adapt-VQE
(RA-VQE), a random approach we introduce as a baseline. In order to compare
these algorithms to traditional VQAs, we also include the Quantum Approximate
Optimization Algorithm (QAOA) in our analysis. We apply these algorithms to
QUBO problems and study their performance by examining the quality of the
solutions found and the computational times required. Additionally, we
investigate how the choice of the hyperparameters can impact the overall
performance of the algorithms, highlighting the importance of selecting an
appropriate methodology for hyperparameter tuning. Our analysis sets benchmarks
for Adaptative VQAs designed for near-term quantum devices and provides
valuable insights to guide future research in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01797">Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach. (arXiv:2308.01797v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonetta_G/0/1/0/all/0/1">Giovanni Bonetta</a>, <a href="http://arxiv.org/find/cs/1/au:+Zago_D/0/1/0/all/0/1">Davide Zago</a>, <a href="http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1">Rossella Cancelliere</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosso_A/0/1/0/all/0/1">Andrea Grosso</a></p>
<p>Job scheduling is a well-known Combinatorial Optimization problem with
endless applications. Well planned schedules bring many benefits in the context
of automated systems: among others, they limit production costs and waste.
Nevertheless, the NP-hardness of this problem makes it essential to use
heuristics whose design is difficult, requires specialized knowledge and often
produces methods tailored to the specific task. This paper presents an original
end-to-end Deep Reinforcement Learning approach to scheduling that
automatically learns dispatching rules. Our technique is inspired by natural
language encoder-decoder models for sequence processing and has never been
used, to the best of our knowledge, for scheduling purposes. We applied and
tested our method in particular to some benchmark instances of Job Shop
Problem, but this technique is general enough to be potentially used to tackle
other different optimal job scheduling tasks with minimal intervention. Results
demonstrate that we outperform many classical approaches exploiting priority
dispatching rules and show competitive results on state-of-the-art Deep
Reinforcement Learning ones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01814">Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. (arXiv:2308.01814v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Greg Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1">Etai Littwin</a></p>
<p>Going beyond stochastic gradient descent (SGD), what new phenomena emerge in
wide neural networks trained by adaptive optimizers like Adam? Here we show:
The same dichotomy between feature learning and kernel behaviors (as in SGD)
holds for general optimizers as well, including Adam -- albeit with a nonlinear
notion of "kernel." We derive the corresponding "neural tangent" and "maximal
update" limits for any architecture. Two foundational advances underlie the
above results: 1) A new Tensor Program language, NEXORT, that can express how
adaptive optimizers process gradients into updates. 2) The introduction of
bra-ket notation to drastically simplify expressions and calculations in Tensor
Programs. This work summarizes and generalizes all previous results in the
Tensor Programs series of papers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01823">Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenhao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1">Xiang Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yulong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Run Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Liming Fang</a></p>
<p>Adversarial training (AT) is widely considered the state-of-the-art technique
for improving the robustness of deep neural networks (DNNs) against adversarial
examples (AE). Nevertheless, recent studies have revealed that adversarially
trained models are prone to unfairness problems, restricting their
applicability. In this paper, we empirically observe that this limitation may
be attributed to serious adversarial confidence overfitting, i.e., certain
adversarial examples with overconfidence. To alleviate this problem, we propose
HAM, a straightforward yet effective framework via adaptive Hard Adversarial
example Mining.HAM concentrates on mining hard adversarial examples while
discarding the easy ones in an adaptive fashion. Specifically, HAM identifies
hard AEs in terms of their step sizes needed to cross the decision boundary
when calculating loss value. Besides, an early-dropping mechanism is
incorporated to discard the easy examples at the initial stages of AE
generation, resulting in efficient AT. Extensive experimental results on
CIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significant
improvement in robust fairness while reducing computational cost compared to
several state-of-the-art adversarial training methods. The code will be made
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01834">The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Galatzer_Levy_I/0/1/0/all/0/1">Isaac R. Galatzer-Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1">Daniel McDuff</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Malgaroli_M/0/1/0/all/0/1">Matteo Malgaroli</a></p>
<p>The current work investigates the capability of Large language models (LLMs)
that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)
to predict psychiatric functioning from patient interviews and clinical
descriptions without being trained to do so. To assess this, n = 145 depression
and n =115 PTSD assessments and n = 46 clinical case studies across high
prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma
and stress, Addictive disorders) were analyzed using prompts to extract
estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is
capable of assessing psychiatric functioning across a range of psychiatric
conditions with the strongest performance being the prediction of depression
scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which
were statistically indistinguishable from human clinical raters t(1,144) =
1.20; p = 0.23. Results show the potential for general clinical language models
to flexibly predict psychiatric risk based on free descriptions of functioning
from both patients and clinicians.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01835">Distribution-Free Inference for the Regression Function of Binary Classification. (arXiv:2308.01835v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1">Ambrus Tam&#xe1;s</a>, <a href="http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1">Bal&#xe1;zs Csan&#xe1;d Cs&#xe1;ji</a></p>
<p>One of the key objects of binary classification is the regression function,
i.e., the conditional expectation of the class labels given the inputs. With
the regression function not only a Bayes optimal classifier can be defined, but
it also encodes the corresponding misclassification probabilities. The paper
presents a resampling framework to construct exact, distribution-free and
non-asymptotically guaranteed confidence regions for the true regression
function for any user-chosen confidence level. Then, specific algorithms are
suggested to demonstrate the framework. It is proved that the constructed
confidence regions are strongly consistent, that is, any false model is
excluded in the long run with probability one. The exclusion is quantified with
probably approximately correct type bounds, as well. Finally, the algorithms
are validated via numerical experiments, and the methods are compared to
approximate asymptotic confidence ellipsoids.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01840">URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1">Kevin Eykholt</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1">Taesung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Schales_D/0/1/0/all/0/1">Douglas Schales</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1">Jiyong Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Molloy_I/0/1/0/all/0/1">Ian Molloy</a>, <a href="http://arxiv.org/find/cs/1/au:+Zorin_M/0/1/0/all/0/1">Masha Zorin</a></p>
<p>Machine learning models are known to be vulnerable to adversarial evasion
attacks as illustrated by image classification models. Thoroughly understanding
such attacks is critical in order to ensure the safety and robustness of
critical AI tasks. However, most evasion attacks are difficult to deploy
against a majority of AI systems because they have focused on image domain with
only few constraints. An image is composed of homogeneous, numerical,
continuous, and independent features, unlike many other input types to AI
systems used in practice. Furthermore, some input types include additional
semantic and functional constraints that must be observed to generate realistic
adversarial inputs. In this work, we propose a new framework to enable the
generation of adversarial inputs irrespective of the input type and task
domain. Given an input and a set of pre-defined input transformations, our
framework discovers a sequence of transformations that result in a semantically
correct and functional adversarial input. We demonstrate the generality of our
approach on several diverse machine learning tasks with various input
representations. We also show the importance of generating adversarial examples
as they enable the deployment of mitigation techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01849">Curricular Transfer Learning for Sentence Encoded Tasks. (arXiv:2308.01849v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sa_J/0/1/0/all/0/1">Jader Martins Camboim de S&#xe1;</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanches_M/0/1/0/all/0/1">Matheus Ferraroni Sanches</a>, <a href="http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1">Rafael Roque de Souza</a>, <a href="http://arxiv.org/find/cs/1/au:+Reis_J/0/1/0/all/0/1">J&#xfa;lio Cesar dos Reis</a>, <a href="http://arxiv.org/find/cs/1/au:+Villas_L/0/1/0/all/0/1">Leandro Aparecido Villas</a></p>
<p>Fine-tuning language models in a downstream task is the standard approach for
many state-of-the-art methodologies in the field of NLP. However, when the
distribution between the source task and target task drifts, \textit{e.g.},
conversational environments, these gains tend to be diminished. This article
proposes a sequence of pre-training steps (a curriculum) guided by "data
hacking" and grammar analysis that allows further gradual adaptation between
pre-training distributions. In our experiments, we acquire a considerable
improvement from our method compared to other known pre-training approaches for
the MultiWoZ task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01853">Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chao_P/0/1/0/all/0/1">Patrick Chao</a>, <a href="http://arxiv.org/find/stat/1/au:+Dobriban_E/0/1/0/all/0/1">Edgar Dobriban</a></p>
<p>Distribution shifts are a serious concern in modern statistical learning as
they can systematically change the properties of the data away from the truth.
We focus on Wasserstein distribution shifts, where every data point may undergo
a slight perturbation, as opposed to the Huber contamination model where a
fraction of observations are outliers. We formulate and study shifts beyond
independent perturbations, exploring Joint Distribution Shifts, where the
per-observation perturbations can be coordinated. We analyze several important
statistical problems, including location estimation, linear regression, and
non-parametric density estimation. Under a squared loss for mean estimation and
prediction error in linear regression, we find the exact minimax risk, a least
favorable perturbation, and show that the sample mean and least squares
estimators are respectively optimal. This holds for both independent and joint
shifts, but the least favorable perturbations and minimax risks differ. For
other problems, we provide nearly optimal estimators and precise finite-sample
bounds. We also introduce several tools for bounding the minimax risk under
distribution shift, such as a smoothing technique for location families, and
generalizations of classical tools including least favorable sequences of
priors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01867">MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manohara_M/0/1/0/all/0/1">Manasa Manohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Dayal_S/0/1/0/all/0/1">Sankalp Dayal</a>, <a href="http://arxiv.org/find/cs/1/au:+Afzal_T/0/1/0/all/0/1">Tarqi Afzal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1">Rahul Bakshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1">Kahkuen Fu</a></p>
<p>Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU,
DPU), deploying deep learning models on edge devices with fixed-point hardware
is still challenging due to complex model quantization and conversion. Existing
model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and
Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g.,
only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep
learning models cannot be easily quantized for diverse fixed-point hardwares,
mainly due to slightly different quantization requirements. In this paper, we
envision a new type of model quantization approach called MRQ (model
re-quantization), which takes existing quantized models and quickly transforms
the models to meet different quantization requirements (e.g., asymmetric -&gt;
symmetric, non-power-of-2 scale -&gt; power-of-2 scale). Re-quantization is much
simpler than quantizing from scratch because it avoids costly re-training and
provides support for multiple quantization schemes simultaneously. To minimize
re-quantization error, we developed a new set of re-quantization algorithms
including weight correction and rounding error folding. We have demonstrated
that MobileNetV2 QAT model [7] can be quickly re-quantized into two different
quantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with less
than 0.64 units of accuracy loss. We believe our work is the first to leverage
this concept of re-quantization for model quantization and models obtained from
the re-quantization process have been successfully deployed on NNA in the Echo
Show devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01868">Multi-variable Hard Physical Constraints for Climate Model Downscaling. (arXiv:2308.01868v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Gonzalez_Abad_J/0/1/0/all/0/1">Jose Gonz&#xe1;lez-Abad</a>, <a href="http://arxiv.org/find/physics/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1">&#xc1;lex Hern&#xe1;ndez-Garc&#xed;a</a>, <a href="http://arxiv.org/find/physics/1/au:+Harder_P/0/1/0/all/0/1">Paula Harder</a>, <a href="http://arxiv.org/find/physics/1/au:+Rolnick_D/0/1/0/all/0/1">David Rolnick</a>, <a href="http://arxiv.org/find/physics/1/au:+Gutierrez_J/0/1/0/all/0/1">Jos&#xe9; Manuel Guti&#xe9;rrez</a></p>
<p>Global Climate Models (GCMs) are the primary tool to simulate climate
evolution and assess the impacts of climate change. However, they often operate
at a coarse spatial resolution that limits their accuracy in reproducing
local-scale phenomena. Statistical downscaling methods leveraging deep learning
offer a solution to this problem by approximating local-scale climate fields
from coarse variables, thus enabling regional GCM projections. Typically,
climate fields of different variables of interest are downscaled independently,
resulting in violations of fundamental physical properties across
interconnected variables. This study investigates the scope of this problem
and, through an application on temperature, lays the foundation for a framework
introducing multi-variable hard constraints that guarantees physical
relationships between groups of downscaled climate variables.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01890">DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. (arXiv:2308.01890v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Ping Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Ximeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1">Stan Sclaroff</a>, <a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1">Kate Saenko</a></p>
<p>Multi-label image recognition in the low-label regime is a task of great
challenge and practical significance. Previous works have focused on learning
the alignment between textual and visual spaces to compensate for limited image
labels, yet may suffer from reduced accuracy due to the scarcity of
high-quality multi-label annotations. In this research, we leverage the
powerful alignment between textual and visual features pretrained with millions
of auxiliary image-text pairs. We introduce an efficient and effective
framework called Evidence-guided Dual Context Optimization (DualCoOp++), which
serves as a unified approach for addressing partial-label and zero-shot
multi-label recognition. In DualCoOp++ we separately encode evidential,
positive, and negative contexts for target classes as parametric components of
the linguistic input (i.e., prompts). The evidential context aims to discover
all the related visual content for the target class, and serves as guidance to
aggregate positive and negative contexts from the spatial domain of the image,
enabling better distinguishment between similar categories. Additionally, we
introduce a Winner-Take-All module that promotes inter-class interaction during
training, while avoiding the need for extra parameters and costs. As DualCoOp++
imposes minimal additional learnable overhead on the pretrained vision-language
framework, it enables rapid adaptation to multi-label recognition tasks with
limited annotations and even unseen classes. Experiments on standard
multi-label recognition benchmarks across two challenging low-label settings
demonstrate the superior performance of our approach compared to
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01891">Exact identification of nonlinear dynamical systems by Trimmed Lasso. (arXiv:2308.01891v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kiser_S/0/1/0/all/0/1">Shawn L. Kiser</a>, <a href="http://arxiv.org/find/cs/1/au:+Guskov_M/0/1/0/all/0/1">Mikhail Guskov</a>, <a href="http://arxiv.org/find/cs/1/au:+Rebillat_M/0/1/0/all/0/1">Marc R&#xe9;billat</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranc_N/0/1/0/all/0/1">Nicolas Ranc</a></p>
<p>Identification of nonlinear dynamical systems has been popularized by sparse
identification of the nonlinear dynamics (SINDy) via the sequentially
thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged
in the literature to deal with experimental data which are finite in length and
noisy. Recently, the computationally intensive method of ensembling
bootstrapped SINDy models (E-SINDy) was proposed for model identification,
handling finite, highly noisy data. While the extensions of SINDy are numerous,
their sparsity-promoting estimators occasionally provide sparse approximations
of the dynamics as opposed to exact recovery. Furthermore, these estimators
suffer under multicollinearity, e.g. the irrepresentable condition for the
Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust
identification of models (TRIM) can provide exact recovery under more severe
noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally,
the computational cost of TRIM is asymptotically equal to STLS since the
sparsity parameter of the TRIM can be solved efficiently by convex solvers. We
compare these methodologies on challenging nonlinear systems, specifically the
Lorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmark
of No\"el and Schoukens, 2016, and a time delay system describing tool cutting
dynamics. This study emphasizes the comparisons between STLS, reweighted
$\ell_1$ minimization, and Trimmed Lasso in identification with respect to
problems faced by practitioners: the problem of finite and noisy data, the
performance of the sparse regression of when the library grows in dimension
(multicollinearity), and automatic methods for choice of regularization
parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01895">Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning. (arXiv:2308.01895v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brignac_D/0/1/0/all/0/1">Daniel Brignac</a>, <a href="http://arxiv.org/find/cs/1/au:+Lobo_N/0/1/0/all/0/1">Niels Lobo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1">Abhijit Mahalanobis</a></p>
<p>Continual learning seeks to enable deep learners to train on a series of
tasks of unknown length without suffering from the catastrophic forgetting of
previous tasks. One effective solution is replay, which involves storing few
previous experiences in memory and replaying them when learning the current
task. However, there is still room for improvement when it comes to selecting
the most informative samples for storage and determining the optimal number of
samples to be stored. This study aims to address these issues with a novel
comparison of the commonly used reservoir sampling to various alternative
population strategies and providing a novel detailed analysis of how to find
the optimal number of stored samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01899">How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jialiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhiyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaodong Shi</a></p>
<p>Preprints play an increasingly critical role in academic communities. There
are many reasons driving researchers to post their manuscripts to preprint
servers before formal submission to journals or conferences, but the use of
preprints has also sparked considerable controversy, especially surrounding the
claim of priority. In this paper, a case study of computer science preprints
submitted to arXiv from 2008 to 2017 is conducted to quantify how many
preprints have eventually been printed in peer-reviewed venues. Among those
published manuscripts, some are published under different titles and without an
update to their preprints on arXiv. In the case of these manuscripts, the
traditional fuzzy matching method is incapable of mapping the preprint to the
final published version. In view of this issue, we introduce a semantics-based
mapping method with the employment of Bidirectional Encoder Representations
from Transformers (BERT). With this new mapping method and a plurality of data
sources, we find that 66% of all sampled preprints are published under
unchanged titles and 11% are published under different titles and with other
modifications. A further analysis was then performed to investigate why these
preprints but not others were accepted for publication. Our comparison reveals
that in the field of computer science, published preprints feature adequate
revisions, multiple authorship, detailed abstract and introduction, extensive
and authoritative references and available source code.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01905">Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xinglong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1">Jean Ponce</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu-Xiong Wang</a></p>
<p>Depth completion, which aims to generate high-quality dense depth maps from
sparse depth maps, has attracted increasing attention in recent years. Previous
work usually employs RGB images as guidance, and introduces iterative spatial
propagation to refine estimated coarse depth maps. However, most of the
propagation refinement methods require several iterations and suffer from a
fixed receptive field, which may contain irrelevant and useless information
with very sparse input. In this paper, we address these two challenges
simultaneously by revisiting the idea of deformable convolution. We propose an
effective architecture that leverages deformable kernel convolution as a
single-pass refinement module, and empirically demonstrate its superiority. To
better understand the function of deformable convolution and exploit it for
depth completion, we further systematically investigate a variety of
representative strategies. Our study reveals that, different from prior work,
deformable convolution needs to be applied on an estimated depth map with a
relatively high density for better performance. We evaluate our model on the
large-scale KITTI dataset and achieve state-of-the-art level performance in
both accuracy and inference speed. Our code is available at
https://github.com/AlexSunNik/ReDC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01906">Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gaur_V/0/1/0/all/0/1">Vedant Gaur</a>, <a href="http://arxiv.org/find/cs/1/au:+Saunshi_N/0/1/0/all/0/1">Nikunj Saunshi</a></p>
<p>Large language models (LLMs) have revolutionized NLP by solving downstream
tasks with little to no labeled data. Despite their versatile abilities, the
larger question of their ability to reason remains ill-understood. This paper
addresses reasoning in math word problems (MWPs) by studying symbolic versions
of the numeric problems, since a symbolic expression is a "concise explanation"
of the numeric answer. We create and use a symbolic version of the SVAMP
dataset and find that GPT-3's davinci-002 model also has good zero-shot
accuracy on symbolic MWPs. To evaluate the faithfulness of the model's
reasoning, we go beyond accuracy and additionally evaluate the alignment
between the final answer and the outputted reasoning, which correspond to
numeric and symbolic answers respectively for MWPs. We explore a self-prompting
approach to encourage the symbolic reasoning to align with the numeric answer,
thus equipping the LLM with the ability to provide a concise and verifiable
reasoning and making it more interpretable. Surprisingly, self-prompting also
improves the symbolic accuracy to be higher than both the numeric and symbolic
accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be
released for future research on symbolic math problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2003.08904">RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1">Maurice Weber</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaojun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1">Bojan Karla&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Recent studies have shown that deep neural networks (DNNs) are vulnerable to
adversarial attacks, including evasion and backdoor (poisoning) attacks. On the
defense side, there have been intensive efforts on improving both empirical and
provable robustness against evasion attacks; however, the provable robustness
against backdoor attacks still remains largely unexplored. In this paper, we
focus on certifying the machine learning model robustness against general
threat models, especially backdoor attacks. We first provide a unified
framework via randomized smoothing techniques and show how it can be
instantiated to certify the robustness against both evasion and backdoor
attacks. We then propose the first robust training process, RAB, to smooth the
trained model and certify its robustness against backdoor attacks. We prove the
robustness bound for machine learning models trained with RAB and prove that
our robustness bound is tight. In addition, we theoretically show that it is
possible to train the robust smoothed models efficiently for simple models such
as K-nearest neighbor classifiers, and we propose an exact smooth-training
algorithm that eliminates the need to sample from a noise distribution for such
models. Empirically, we conduct comprehensive experiments for different machine
learning (ML) models such as DNNs, support vector machines, and K-NN models on
MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for
certified robustness against backdoor attacks. In addition, we evaluate K-NN
models on a spambase tabular dataset to demonstrate the advantages of the
proposed exact algorithm. Both the theoretic analysis and the comprehensive
evaluation on diverse ML models and datasets shed light on further robust
learning strategies against general training time attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2005.09048">Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Rolle_A/0/1/0/all/0/1">Alexander Rolle</a>, <a href="http://arxiv.org/find/math/1/au:+Scoccola_L/0/1/0/all/0/1">Luis Scoccola</a></p>
<p>We consider the degree-Rips construction from topological data analysis,
which provides a density-sensitive, multiparameter hierarchical clustering
algorithm. We analyze its stability to perturbations of the input data using
the correspondence-interleaving distance, a metric for hierarchical clusterings
that we introduce. Taking certain one-parameter slices of degree-Rips recovers
well-known methods for density-based clustering, but we show that these methods
are unstable. However, we prove that degree-Rips, as a multiparameter object,
is stable, and we propose an alternative approach for taking slices of
degree-Rips, which yields a one-parameter hierarchical clustering algorithm
with better stability properties. We prove that this algorithm is consistent,
using the correspondence-interleaving distance. We provide an algorithm for
extracting a single clustering from one-parameter hierarchical clusterings,
which is stable with respect to the correspondence-interleaving distance. And,
we integrate these methods into a pipeline for density-based clustering, which
we call Persistable. Adapting tools from multiparameter persistent homology, we
propose visualization tools that guide the selection of all parameters of the
pipeline. We demonstrate Persistable on benchmark datasets, showing that it
identifies multi-scale cluster structure in data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.11233">ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoxing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yuda Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhexi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaokang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a></p>
<p>Albeit being a prevalent architecture searching approach, differentiable
architecture search (DARTS) is largely hindered by its substantial memory cost
since the entire supernet resides in the memory. This is where the single-path
DARTS comes in, which only chooses a single-path submodel at each step. While
being memory-friendly, it also comes with low computational costs. Nonetheless,
we discover a critical issue of single-path DARTS that has not been primarily
noticed. Namely, it also suffers from severe performance collapse since too
many parameter-free operations like skip connections are derived, just like
DARTS does. In this paper, we propose a new algorithm called RObustifying
Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology
search from the operation search to make searching and evaluation consistent.
We then adopt Gumbel-Top2 reparameterization and gradient accumulation to
robustify the unwieldy bi-level optimization. We verify ROME extensively across
15 benchmarks to demonstrate its effectiveness and robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2012.14563">Random Planted Forest: a directly interpretable tree ensemble. (arXiv:2012.14563v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hiabu_M/0/1/0/all/0/1">Munir Hiabu</a>, <a href="http://arxiv.org/find/stat/1/au:+Mammen_E/0/1/0/all/0/1">Enno Mammen</a>, <a href="http://arxiv.org/find/stat/1/au:+Meyer_J/0/1/0/all/0/1">Joseph T. Meyer</a></p>
<p>We introduce a novel interpretable tree based algorithm for prediction in a
regression setting. Our motivation is to estimate the unknown regression
function from a functional decomposition perspective in which the functional
components correspond to lower order interaction terms. The idea is to modify
the random forest algorithm by keeping certain leaves after they are split
instead of deleting them. This leads to non-binary trees which we refer to as
planted trees. An extension to a forest leads to our random planted forest
algorithm. Additionally, the maximum number of covariates which can interact
within a leaf can be bounded. If we set this interaction bound to one, the
resulting estimator is a sum of one-dimensional functions. In the other extreme
case, if we do not set a limit, the resulting estimator and corresponding model
place no restrictions on the form of the regression function. In a simulation
study we find encouraging prediction and visualisation properties of our random
planted forest method. We also develop theory for an idealized version of
random planted forests in cases where the interaction bound is low. We show
that if it is smaller than three, the idealized version achieves asymptotically
optimal convergence rates up to a logarithmic factor. Code is available on
GitHub https://github.com/PlantedML/randomPlantedForest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.03395">How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1">Laurens Sluijterman</a>, <a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1">Eric Cator</a>, <a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1">Tom Heskes</a></p>
<p>As neural networks become more popular, the need for accompanying uncertainty
estimates increases. There are currently two main approaches to test the
quality of these estimates. Most methods output a density. They can be compared
by evaluating their loglikelihood on a test set. Other methods output a
prediction interval directly. These methods are often tested by examining the
fraction of test points that fall inside the corresponding prediction
intervals. Intuitively both approaches seem logical. However, we demonstrate
through both theoretical arguments and simulations that both ways of evaluating
the quality of uncertainty estimates have serious flaws. Firstly, both
approaches cannot disentangle the separate components that jointly create the
predictive uncertainty, making it difficult to evaluate the quality of the
estimates of these components. Secondly, a better loglikelihood does not
guarantee better prediction intervals, which is what the methods are often used
for in practice. Moreover, the current approach to test prediction intervals
directly has additional flaws. We show why it is fundamentally flawed to test a
prediction or confidence interval on a single test set. At best, marginal
coverage is measured, implicitly averaging out overconfident and underconfident
predictions. A much more desirable property is pointwise coverage, requiring
the correct coverage for each prediction. We demonstrate through practical
examples that these effects can result in favoring a method, based on the
predictive uncertainty, that has undesirable behaviour of the confidence or
prediction intervals. Finally, we propose a simulation-based testing approach
that addresses these problems while still allowing easy comparison between
different methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.03110">Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emukpere_D/0/1/0/all/0/1">David Emukpere</a>, <a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1">Xavier Alameda-Pineda</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinke_C/0/1/0/all/0/1">Chris Reinke</a></p>
<p>A longstanding goal in reinforcement learning is to build intelligent agents
that show fast learning and a flexible transfer of skills akin to humans and
animals. This paper investigates the integration of two frameworks for tackling
those goals: episodic control and successor features. Episodic control is a
cognitively inspired approach relying on episodic memory, an instance-based
memory model of an agent's experiences. Meanwhile, successor features and
generalized policy improvement (SF&amp;GPI) is a meta and transfer learning
framework allowing to learn policies for tasks that can be efficiently reused
for later tasks which have a different reward function. Individually, these two
techniques have shown impressive results in vastly improving sample efficiency
and the elegant reuse of previously learned policies. Thus, we outline a
combination of both approaches in a single reinforcement learning framework and
empirically illustrate its benefits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.12146">Sharing to learn and learning to share -- Fitting together Meta-Learning, Multi-Task Learning, and Transfer Learning: A meta review. (arXiv:2111.12146v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1">Richa Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Phlypo_R/0/1/0/all/0/1">Ronald Phlypo</a>, <a href="http://arxiv.org/find/cs/1/au:+Saini_R/0/1/0/all/0/1">Rajkumar Saini</a>, <a href="http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1">Marcus Liwicki</a></p>
<p>Integrating knowledge across different domains is an essential feature of
human learning. Learning paradigms such as transfer learning, meta learning,
and multi-task learning reflect the human learning process by exploiting the
prior knowledge for new tasks, encouraging faster learning and good
generalization for new tasks. This article gives a detailed view of these
learning paradigms and their comparative analysis. The weakness of one learning
algorithm turns out to be a strength of another, and thus merging them is a
prevalent trait in the literature. There are numerous research papers that
focus on each of these learning paradigms separately and provide a
comprehensive overview of them. However, this article provides a review of
research studies that combine (two of) these learning algorithms. This survey
describes how these techniques are combined to solve problems in many different
fields of study, including computer vision, natural language processing,
hyperspectral imaging, and many more, in supervised setting only. As a result,
the global generic learning network an amalgamation of meta learning, transfer
learning, and multi-task learning is introduced here, along with some open
research questions and future research directions in the multi-task setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.15402">Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Quanziang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Renzhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuexiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Dong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kai Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1">Deyu Meng</a></p>
<p>Continual learning is a promising machine learning paradigm to learn new
tasks while retaining previously learned knowledge over streaming training
data. Till now, rehearsal-based methods, keeping a small part of data from old
tasks as a memory buffer, have shown good performance in mitigating
catastrophic forgetting for previously learned knowledge. However, most of
these methods typically treat each new task equally, which may not adequately
consider the relationship or similarity between old and new tasks. Furthermore,
these methods commonly neglect sample importance in the continual training
process and result in sub-optimal performance on certain tasks. To address this
challenging problem, we propose Relational Experience Replay (RER), a bi-level
learning framework, to adaptively tune task-wise relationships and sample
importance within each task to achieve a better `stability' and `plasticity'
trade-off. As such, the proposed method is capable of accumulating new
knowledge while consolidating previously learned old knowledge during continual
learning. Extensive experiments conducted on three publicly available datasets
(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed method
can consistently improve the performance of all baselines and surpass current
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.07901">Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition. (arXiv:2202.07901v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1">Felix Ott</a>, <a href="http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1">David R&#xfc;gamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Heublein_L/0/1/0/all/0/1">Lucas Heublein</a>, <a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1">Bernd Bischl</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1">Christopher Mutschler</a></p>
<p>Cross-modal representation learning learns a shared embedding between two or
more modalities to improve performance in a given task compared to using only
one of the modalities. Cross-modal representation learning from different data
types -- such as images and time-series data (e.g., audio or text data) --
requires a deep metric learning loss that minimizes the distance between the
modality embeddings. In this paper, we propose to use the contrastive or
triplet loss, which uses positive and negative identities to create sample
pairs with different labels, for cross-modal representation learning between
image and time-series modalities (CMR-IS). By adapting the triplet loss for
cross-modal representation learning, higher accuracy in the main (time-series
classification) task can be achieved by exploiting additional information of
the auxiliary (image classification) task. We present a triplet loss with a
dynamic margin for single label and sequence-to-sequence classification tasks.
We perform extensive evaluations on synthetic image and time-series data, and
on data for offline handwriting recognition (HWR) and on online HWR from
sensor-enhanced pens for classifying written words. Our experiments show an
improved classification accuracy, faster convergence, and better
generalizability due to an improved cross-modal representation. Furthermore,
the more suitable generalizability leads to a better adaptability between
writers for online HWR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.10903">Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1">Laurens Sluijterman</a>, <a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1">Eric Cator</a>, <a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1">Tom Heskes</a></p>
<p>With the rise of the popularity and usage of neural networks, trustworthy
uncertainty estimation is becoming increasingly essential. One of the most
prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et
al., 2017) . A classical parametric model has uncertainty in the parameters due
to the fact that the data on which the model is build is a random sample. A
modern neural network has an additional uncertainty component since the
optimization of the network is random. Lakshminarayanan et al. (2017) noted
that Deep Ensembles do not incorporate the classical uncertainty induced by the
effect of finite data. In this paper, we present a computationally cheap
extension of Deep Ensembles for the regression setting, called Bootstrapped
Deep Ensembles, that explicitly takes this classical effect of finite data into
account using a modified version of the parametric bootstrap. We demonstrate
through an experimental study that our method significantly improves upon
standard Deep Ensembles
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.13619">Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v6 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hanxiong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Juntao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shuchang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>As one of the most pervasive applications of machine learning, recommender
systems are playing an important role on assisting human decision making. The
satisfaction of users and the interests of platforms are closely related to the
quality of the generated recommendation results. However, as a highly
data-driven system, recommender system could be affected by data or algorithmic
bias and thus generate unfair results, which could weaken the reliance of the
systems. As a result, it is crucial to address the potential unfairness
problems in recommendation settings. Recently, there has been growing attention
on fairness considerations in recommender systems with more and more literature
on approaches to promote fairness in recommendation. However, the studies are
rather fragmented and lack a systematic organization, thus making it difficult
to penetrate for new researchers to the domain. This motivates us to provide a
systematic survey of existing works on fairness in recommendation. This survey
focuses on the foundations for fairness in recommendation literature. It first
presents a brief introduction about fairness in basic machine learning tasks
such as classification and ranking in order to provide a general overview of
fairness research, as well as introduce the more complex situations and
challenges that need to be considered when studying fairness in recommender
systems. After that, the survey will introduce fairness in recommendation with
a focus on the taxonomies of current fairness definitions, the typical
techniques for improving fairness, as well as the datasets for fairness studies
in recommendation. The survey also talks about the challenges and opportunities
in fairness research with the hope of promoting the fair recommendation
research area and beyond.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.07944">Distributed Online Private Learning of Convex Nondecomposable Objectives. (arXiv:2206.07944v4 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Cheng_H/0/1/0/all/0/1">Huqiang Cheng</a>, <a href="http://arxiv.org/find/math/1/au:+Liao_X/0/1/0/all/0/1">Xiaofeng Liao</a>, <a href="http://arxiv.org/find/math/1/au:+Li_H/0/1/0/all/0/1">Huaqing Li</a></p>
<p>We deal with a general distributed constrained online learning problem with
privacy over time-varying networks, where a class of nondecomposable objectives
are considered. Under this setting, each node only controls a part of the
global decision, and the goal of all nodes is to collaboratively minimize the
global cost over a time horizon $T$ while guarantees the security of the
transmitted information. For such problems, we first design a novel generic
algorithm framework, named as DPSDA, of differentially private distributed
online learning using the Laplace mechanism and the stochastic variants of dual
averaging method. Note that in the dual updates, all nodes of DPSDA employ the
noise-corrupted gradients for more generality. Then, we propose two algorithms,
named as DPSDA-C and DPSDA-PS, under this framework. In DPSDA-C, the nodes
implement a circulation-based communication in the primal updates so as to
alleviate the disagreements over time-varying undirected networks. In addition,
for the extension to time-varying directed ones, the nodes implement the
broadcast-based push-sum dynamics in DPSDA-PS, which can achieve average
consensus over arbitrary directed networks. Theoretical results show that both
algorithms attain an expected regret upper bound in $\mathcal{O}( \sqrt{T} )$
when the objective function is convex, which matches the best utility
achievable by cutting-edge algorithms. Finally, numerical experiment results on
both synthetic and real-world datasets verify the effectiveness of our
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.10276">ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1">Ruixuan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yiwen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haobo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1">Lei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Runze Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Gang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junbo Zhao</a></p>
<p>Learning with Noisy Labels (LNL) has become an appealing topic, as
imperfectly annotated data are relatively cheaper to obtain. Recent
state-of-the-art approaches employ specific selection mechanisms to separate
clean and noisy samples and then apply Semi-Supervised Learning (SSL)
techniques for improved performance. However, the selection step mostly
provides a medium-sized and decent-enough clean subset, which overlooks a rich
set of clean samples. To fulfill this, we propose a novel LNL framework ProMix
that attempts to maximize the utility of clean samples for boosted performance.
Key to our method, we propose a matched high confidence selection technique
that selects those examples with high confidence scores and matched predictions
with given labels to dynamically expand a base clean sample set. To overcome
the potential side effect of excessive clean set selection procedure, we
further devise a novel SSL framework that is able to train balanced and
unbiased classifiers on the separated clean and noisy samples. Extensive
experiments demonstrate that ProMix significantly advances the current
state-of-the-art results on multiple benchmarks with different types and levels
of noise. It achieves an average improvement of 2.48\% on the CIFAR-N dataset.
The code is available at https://github.com/Justherozen/ProMix
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.07898">Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kawamata_Y/0/1/0/all/0/1">Yuji Kawamata</a>, <a href="http://arxiv.org/find/stat/1/au:+Motai_R/0/1/0/all/0/1">Ryoki Motai</a>, <a href="http://arxiv.org/find/stat/1/au:+Okada_Y/0/1/0/all/0/1">Yukihiko Okada</a>, <a href="http://arxiv.org/find/stat/1/au:+Imakura_A/0/1/0/all/0/1">Akira Imakura</a>, <a href="http://arxiv.org/find/stat/1/au:+Sakurai_T/0/1/0/all/0/1">Tetsuya Sakurai</a></p>
<p>The development of technologies for causal inference with the privacy
preservation of distributed data has attracted considerable attention in recent
years. To address this issue, we propose a data collaboration quasi-experiment
(DC-QE) that enables causal inference from distributed data with privacy
preservation. In our method, first, local parties construct
dimensionality-reduced intermediate representations from the private data.
Second, they share intermediate representations, instead of private data for
privacy preservation. Third, propensity scores were estimated from the shared
intermediate representations. Finally, the treatment effects were estimated
from propensity scores. Our method can reduce both random errors and biases,
whereas existing methods can only reduce random errors in the estimation of
treatment effects. Through numerical experiments on both artificial and
real-world data, we confirmed that our method can lead to better estimation
results than individual analyses. Dimensionality-reduction loses some of the
information in the private data and causes performance degradation. However, we
observed that in the experiments, sharing intermediate representations with
many parties to resolve the lack of subjects and covariates, our method
improved performance enough to overcome the degradation caused by
dimensionality-reduction. With the spread of our method, intermediate
representations can be published as open data to help researchers find
causalities and accumulated as a knowledge base.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11435">Bidirectional Contrastive Split Learning for Visual Question Answering. (arXiv:2208.11435v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1">Hideya Ochiai</a></p>
<p>Visual Question Answering (VQA) based on multi-modal data facilitates
real-life applications such as home robots and medical diagnoses. One
significant challenge is to devise a robust decentralized learning framework
for various client models where centralized data collection is refrained due to
confidentiality concerns. This work aims to tackle privacy-preserving VQA by
decoupling a multi-modal model into representation modules and a contrastive
module and leveraging inter-module gradients sharing and inter-client weight
sharing. To this end, we propose Bidirectional Contrastive Split Learning
(BiCSL) to train a global multi-modal model on the entire data distribution of
decentralized clients. We employ the contrastive loss that enables a more
efficient self-supervised learning of decentralized modules. Comprehensive
experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models,
demonstrating the effectiveness of the proposed method. Furthermore, we inspect
BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently,
BiCSL shows much better robustness to the multi-modal adversarial attack
compared to the centralized learning method, which provides a promising
approach to decentralized multi-modal learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.13495">A Missing Value Filling Model Based on Feature Fusion Enhanced Autoencoder. (arXiv:2208.13495v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinyao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Shengdong Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianrui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_F/0/1/0/all/0/1">Fei Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yan Yang</a></p>
<p>With the advent of the big data era, the data quality problem is becoming
more critical. Among many factors, data with missing values is one primary
issue, and thus developing effective imputation models is a key topic in the
research community. Recently, a major research direction is to employ neural
network models such as self-organizing mappings or automatic encoders for
filling missing values. However, these classical methods can hardly discover
interrelated features and common features simultaneously among data attributes.
Especially, it is a very typical problem for classical autoencoders that they
often learn invalid constant mappings, which dramatically hurts the filling
performance. To solve the above-mentioned problems, we propose a
missing-value-filling model based on a feature-fusion-enhanced autoencoder. We
first incorporate into an autoencoder a hidden layer that consists of
de-tracking neurons and radial basis function neurons, which can enhance the
ability of learning interrelated features and common features. Besides, we
develop a missing value filling strategy based on dynamic clustering that is
incorporated into an iterative optimization process. This design can enhance
the multi-dimensional feature fusion ability and thus improves the dynamic
collaborative missing-value-filling performance. The effectiveness of the
proposed model is validated by extensive experiments compared to a variety of
baseline methods on thirteen data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.11883">Hebbian Deep Learning Without Feedback. (arXiv:2209.11883v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Journe_A/0/1/0/all/0/1">Adrien Journ&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_H/0/1/0/all/0/1">Hector Garcia Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qinghai Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1">Timoleon Moraitis</a></p>
<p>Recent approximations to backpropagation (BP) have mitigated many of BP's
computational inefficiencies and incompatibilities with biology, but important
limitations still remain. Moreover, the approximations significantly decrease
accuracy in benchmarks, suggesting that an entirely different approach may be
more fruitful. Here, grounded on recent theory for Hebbian learning in soft
winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm
that trains deep neural networks, without any feedback, target, or error
signals. As a result, it achieves efficiency by avoiding weight transport,
non-local plasticity, time-locking of layer updates, iterative equilibria, and
(self-) supervisory or other feedback signals -- which were necessary in other
approaches. Its increased efficiency and biological compatibility do not trade
off accuracy compared to state-of-the-art bio-plausible learning, but rather
improve it. With up to five hidden layers and an added linear classifier,
accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%,
80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically
different approach from BP that Deep Learning over few layers may be plausible
in the brain and increases the accuracy of bio-plausible machine learning. Code
is available at https://github.com/NeuromorphicComputing/SoftHebb.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.02144">No Agreement Without Loss: Learning and Social Choice in Peer Review. (arXiv:2211.02144v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barcelo_P/0/1/0/all/0/1">Pablo Barcel&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Duarte_M/0/1/0/all/0/1">Mauricio Duarte</a>, <a href="http://arxiv.org/find/cs/1/au:+Rojas_C/0/1/0/all/0/1">Crist&#xf3;bal Rojas</a>, <a href="http://arxiv.org/find/cs/1/au:+Steifer_T/0/1/0/all/0/1">Tomasz Steifer</a></p>
<p>In peer review systems, reviewers are often asked to evaluate various
features of submissions, such as technical quality or novelty. A score is given
to each of the predefined features and based on these the reviewer has to
provide an overall quantitative recommendation. It may be assumed that each
reviewer has her own mapping from the set of features to a recommendation, and
that different reviewers have different mappings in mind. This introduces an
element of arbitrariness known as commensuration bias. In this paper we discuss
a framework, introduced by Noothigattu, Shah and Procaccia, and then applied by
the organizers of the AAAI 2022 conference. Noothigattu, Shah and Procaccia
proposed to aggregate reviewer's mapping by minimizing certain loss functions,
and studied axiomatic properties of this approach, in the sense of social
choice theory. We challenge several of the results and assumptions used in
their work and report a number of negative results. On the one hand, we study a
trade-off between some of the axioms proposed and the ability of the method to
properly capture agreements of the majority of reviewers. On the other hand, we
show that dropping a certain unrealistic assumption has dramatic effects,
including causing the method to be discontinuous.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.03188">An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection. (arXiv:2212.03188v2 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Bond_R/0/1/0/all/0/1">R. Bailey Bond</a>, <a href="http://arxiv.org/find/physics/1/au:+Ren_P/0/1/0/all/0/1">Pu Ren</a>, <a href="http://arxiv.org/find/physics/1/au:+Hajjar_J/0/1/0/all/0/1">Jerome F. Hajjar</a>, <a href="http://arxiv.org/find/physics/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a></p>
<p>Clustering analysis of sequence data continues to address many applications
in engineering design, aided with the rapid growth of machine learning in
applied science. This paper presents an unsupervised machine learning algorithm
to extract defining characteristics of earthquake ground-motion spectra, also
called latent features, to aid in ground-motion selection (GMS). In this
context, a latent feature is a low-dimensional machine-discovered spectral
characteristic learned through nonlinear relationships of a neural network
autoencoder. Machine discovered latent features can be combined with
traditionally defined intensity measures and clustering can be performed to
select a representative subgroup from a large ground-motion suite. The
objective of efficient GMS is to choose characteristic records representative
of what the structure will probabilistically experience in its lifetime. Three
examples are presented to validate this approach, including the use of
synthetic and field recorded ground-motion datasets. The presented deep
embedding clustering of ground-motion spectra has three main advantages: 1.
defining characteristics the represent the sparse spectral content of
ground-motions are discovered efficiently through training of the autoencoder,
2. domain knowledge is incorporated into the machine learning framework with
conditional variables in the deep embedding scheme, and 3. method exhibits
excellent performance when compared to a benchmark seismic hazard analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08736">A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem. (arXiv:2212.08736v3 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zhou_M/0/1/0/all/0/1">Mo Zhou</a>, <a href="http://arxiv.org/find/math/1/au:+Han_J/0/1/0/all/0/1">Jiequn Han</a>, <a href="http://arxiv.org/find/math/1/au:+Rachh_M/0/1/0/all/0/1">Manas Rachh</a>, <a href="http://arxiv.org/find/math/1/au:+Borges_C/0/1/0/all/0/1">Carlos Borges</a></p>
<p>We consider the inverse acoustic obstacle problem for sound-soft star-shaped
obstacles in two dimensions wherein the boundary of the obstacle is determined
from measurements of the scattered field at a collection of receivers outside
the object. One of the standard approaches for solving this problem is to
reformulate it as an optimization problem: finding the boundary of the domain
that minimizes the $L^2$ distance between computed values of the scattered
field and the given measurement data. The optimization problem is
computationally challenging since the local set of convexity shrinks with
increasing frequency and results in an increasing number of local minima in the
vicinity of the true solution. In many practical experimental settings, low
frequency measurements are unavailable due to limitations of the experimental
setup or the sensors used for measurement. Thus, obtaining a good initial guess
for the optimization problem plays a vital role in this environment.
</p>
<p>We present a neural network warm-start approach for solving the inverse
scattering problem, where an initial guess for the optimization problem is
obtained using a trained neural network. We demonstrate the effectiveness of
our method with several numerical examples. For high frequency problems, this
approach outperforms traditional iterative methods such as Gauss-Newton
initialized without any prior (i.e., initialized using a unit circle), or
initialized using the solution of a direct method such as the linear sampling
method. The algorithm remains robust to noise in the scattered field
measurements and also converges to the true solution for limited aperture data.
However, the number of training samples required to train the neural network
scales exponentially in frequency and the complexity of the obstacles
considered. We conclude with a discussion of this phenomenon and potential
directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.11429">Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications. (arXiv:2212.11429v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Streeter_M/0/1/0/all/0/1">Matthew Streeter</a>, <a href="http://arxiv.org/find/cs/1/au:+Dillon_J/0/1/0/all/0/1">Joshua V. Dillon</a></p>
<p>We present a new algorithm for automatically bounding the Taylor remainder
series. In the special case of a scalar function $f: \mathbb{R} \to
\mathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region
$[a, b]$, and integer $k \ge 1$, and returns an interval $I$ such that $f(x) -
\sum_{i=0}^{k-1} \frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i \in I (x - x_0)^k$ for
all $x \in [a, b]$. As in automatic differentiation, the function $f$ is
provided to the algorithm in symbolic form, and must be composed of known
atomic functions.
</p>
<p>At a high level, our algorithm has two steps. First, for a variety of
commonly-used elementary functions (e.g., $\exp$, $\log$), we use
recently-developed theory to derive sharp polynomial upper and lower bounds on
the Taylor remainder series. We then recursively combine the bounds for the
elementary functions using an interval arithmetic variant of Taylor-mode
automatic differentiation. Our algorithm can make efficient use of machine
learning hardware accelerators, and we provide an open source implementation in
JAX.
</p>
<p>We then turn our attention to applications. Most notably, in a companion
paper we use our new machinery to create the first universal
majorization-minimization optimization algorithms: algorithms that iteratively
minimize an arbitrary loss using a majorizer that is derived automatically,
rather than by hand. We also show that our automatically-derived bounds can be
used for verified global optimization and numerical integration, and to prove
sharper versions of Jensen's inequality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.14720">Learning from Data Streams: An Overview and Update. (arXiv:2212.14720v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1">Jesse Read</a>, <a href="http://arxiv.org/find/cs/1/au:+Zliobaite_I/0/1/0/all/0/1">Indr&#x117; &#x17d;liobait&#x117;</a></p>
<p>The literature on machine learning in the context of data streams is vast and
growing. However, many of the defining assumptions regarding data-stream
learning tasks are too strong to hold in practice, or are even contradictory
such that they cannot be met in the contexts of supervised learning. Algorithms
are chosen and designed based on criteria which are often not clearly stated,
for problem settings not clearly defined, tested in unrealistic settings,
and/or in isolation from related approaches in the wider literature. This puts
into question the potential for real-world impact of many approaches conceived
in such contexts, and risks propagating a misguided research focus. We propose
to tackle these issues by reformulating the fundamental definitions and
settings of supervised data-stream learning with regard to contemporary
considerations of concept drift and temporal dependence; and we take a fresh
look at what constitutes a supervised data-stream learning task, and a
reconsideration of algorithms that may be applied to tackle such tasks. Through
and in reflection of this formulation and overview, helped by an informal
survey of industrial players dealing with real-world data streams, we provide
recommendations. Our main emphasis is that learning from data streams does not
impose a single-pass or online-learning approach, or any particular learning
regime; and any constraints on memory and time are not specific to streaming.
Meanwhile, there exist established techniques for dealing with temporal
dependence and concept drift, in other areas of the literature. For the data
streams community, we thus encourage a shift in research focus, from dealing
with often-artificial constraints and assumptions on the learning mode, to
issues such as robustness, privacy, and interpretability which are increasingly
relevant to learning in data streams in academic and industrial settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06267">Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiqiu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Samuel Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1">Zhiyi Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a></p>
<p>The ability to quickly learn a new task with minimal instruction - known as
few-shot learning - is a central aspect of intelligent agents. Classical
few-shot benchmarks make use of few-shot samples from a single modality, but
such samples may not be sufficient to characterize an entire concept class. In
contrast, humans use cross-modal information to learn new concepts efficiently.
In this work, we demonstrate that one can indeed build a better ${\bf visual}$
dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them
bark. To do so, we exploit the fact that recent multimodal foundation models
such as CLIP are inherently cross-modal, mapping different modalities to the
same representation space. Specifically, we propose a simple cross-modal
adaptation approach that learns from few-shot examples spanning different
modalities. By repurposing class names as additional one-shot training samples,
we achieve SOTA results with an embarrassingly simple linear classifier for
vision-language adaptation. Furthermore, we show that our approach can benefit
existing methods such as prefix tuning, adapters, and classifier ensembling.
Finally, to explore other modalities beyond vision and language, we construct
the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal
training to improve the performance of both image and audio classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02096">Matrix Estimation for Individual Fairness. (arXiv:2302.02096v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cindy Y. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cen_S/0/1/0/all/0/1">Sarah H. Cen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Devavrat Shah</a></p>
<p>In recent years, multiple notions of algorithmic fairness have arisen. One
such notion is individual fairness (IF), which requires that individuals who
are similar receive similar treatment. In parallel, matrix estimation (ME) has
emerged as a natural paradigm for handling noisy data with missing values. In
this work, we connect the two concepts. We show that pre-processing data using
ME can improve an algorithm's IF without sacrificing performance. Specifically,
we show that using a popular ME method known as singular value thresholding
(SVT) to pre-process the data provides a strong IF guarantee under appropriate
conditions. We then show that, under analogous conditions, SVT pre-processing
also yields estimates that are consistent and approximately minimax optimal. As
such, the ME pre-processing step does not, under the stated conditions,
increase the prediction error of the base algorithm, i.e., does not impose a
fairness-performance trade-off. We verify these results on synthetic and real
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03606">Merging satellite and gauge-measured precipitation using LightGBM with an emphasis on extreme quantiles. (arXiv:2302.03606v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tyralis_H/0/1/0/all/0/1">Hristos Tyralis</a>, <a href="http://arxiv.org/find/eess/1/au:+Papacharalampous_G/0/1/0/all/0/1">Georgia Papacharalampous</a>, <a href="http://arxiv.org/find/eess/1/au:+Doulamis_N/0/1/0/all/0/1">Nikolaos Doulamis</a>, <a href="http://arxiv.org/find/eess/1/au:+Doulamis_A/0/1/0/all/0/1">Anastasios Doulamis</a></p>
<p>Knowing the actual precipitation in space and time is critical in
hydrological modelling applications, yet the spatial coverage with rain gauge
stations is limited due to economic constraints. Gridded satellite
precipitation datasets offer an alternative option for estimating the actual
precipitation by covering uniformly large areas, albeit related estimates are
not accurate. To improve precipitation estimates, machine learning is applied
to merge rain gauge-based measurements and gridded satellite precipitation
products. In this context, observed precipitation plays the role of the
dependent variable, while satellite data play the role of predictor variables.
Random forests is the dominant machine learning algorithm in relevant
applications. In those spatial predictions settings, point predictions (mostly
the mean or the median of the conditional distribution) of the dependent
variable are issued. The aim of the manuscript is to solve the problem of
probabilistic prediction of precipitation with an emphasis on extreme quantiles
in spatial interpolation settings. Here we propose, issuing probabilistic
spatial predictions of precipitation using Light Gradient Boosting Machine
(LightGBM). LightGBM is a boosting algorithm, highlighted by prize-winning
entries in prediction and forecasting competitions. To assess LightGBM, we
contribute a large-scale application that includes merging daily precipitation
measurements in contiguous US with PERSIANN and GPM-IMERG satellite
precipitation data. We focus on extreme quantiles of the probability
distribution of the dependent variable, where LightGBM outperforms quantile
regression forests (QRF, a variant of random forests) in terms of quantile
score at extreme quantiles. Our study offers understanding of probabilistic
predictions in spatial settings using machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08875">Optimal Training of Mean Variance Estimation Neural Networks. (arXiv:2302.08875v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1">Laurens Sluijterman</a>, <a href="http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1">Eric Cator</a>, <a href="http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1">Tom Heskes</a></p>
<p>This paper focusses on the optimal implementation of a Mean Variance
Estimation network (MVE network) (Nix and Weigend, 1994). This type of network
is often used as a building block for uncertainty estimation methods in a
regression setting, for instance Concrete dropout (Gal et al., 2017) and Deep
Ensembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumes
that the data is produced from a normal distribution with a mean function and
variance function. The MVE network outputs a mean and variance estimate and
optimizes the network parameters by minimizing the negative loglikelihood. In
our paper, we present two significant insights. Firstly, the convergence
difficulties reported in recent work can be relatively easily prevented by
following the simple yet often overlooked recommendation from the original
authors that a warm-up period should be used. During this period, only the mean
is optimized with a fixed variance. We demonstrate the effectiveness of this
step through experimentation, highlighting that it should be standard practice.
As a sidenote, we examine whether, after the warm-up, it is beneficial to fix
the mean while optimizing the variance or to optimize both simultaneously.
Here, we do not observe a substantial difference. Secondly, we introduce a
novel improvement of the MVE network: separate regularization of the mean and
the variance estimate. We demonstrate, both on toy examples and on a number of
benchmark UCI regression data sets, that following the original recommendations
and the novel separate regularization can lead to significant improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.05118">SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gengwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Guoliang Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a></p>
<p>The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research. Code has been made available at:
https://github.com/GengDavid/SLCA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08757">CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patient With Suspected Ischemic Stroke. (arXiv:2303.08757v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tomasetti_L/0/1/0/all/0/1">Luca Tomasetti</a>, <a href="http://arxiv.org/find/eess/1/au:+Engan_K/0/1/0/all/0/1">Kjersti Engan</a>, <a href="http://arxiv.org/find/eess/1/au:+Hollesli_L/0/1/0/all/0/1">Liv Jorunn H&#xf8;llesli</a>, <a href="http://arxiv.org/find/eess/1/au:+Kurz_K/0/1/0/all/0/1">Kathinka D&#xe6;hli Kurz</a>, <a href="http://arxiv.org/find/eess/1/au:+Khanmohammadi_M/0/1/0/all/0/1">Mahdieh Khanmohammadi</a></p>
<p>Precise and fast prediction methods for ischemic areas comprised of dead
tissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)
patients are of significant clinical interest. They play an essential role in
improving diagnosis and treatment planning. Computed Tomography (CT) scan is
one of the primary modalities for early assessment in patients with suspected
AIS. CT Perfusion (CTP) is often used as a primary assessment to determine
stroke location, severity, and volume of ischemic lesions. Current automatic
segmentation methods for CTP mostly use already processed 3D parametric maps
conventionally used for clinical interpretation by radiologists as input.
Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time
input, where the spatial information over the volume is ignored. In addition,
these methods are only interested in segmenting core regions, while predicting
penumbra can be essential for treatment planning. This paper investigates
different methods to utilize the entire 4D CTP as input to fully exploit the
spatio-temporal information, leading us to propose a novel 4D convolution
layer. Our comprehensive experiments on a local dataset of 152 patients divided
into three groups show that our proposed models generate more precise results
than other methods explored. Adopting the proposed 4D mJ-Net, a Dice
Coefficient of 0.53 and 0.23 is achieved for segmenting penumbra and core
areas, respectively. The code is available on
https://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10112">Causal Discovery from Temporal Data: An Overview and New Perspectives. (arXiv:2303.10112v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chang Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1">Di Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chuzhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1">Jingping Bi</a></p>
<p>Temporal data, representing chronological observations of complex systems,
has always been a typical data structure that can be widely generated by many
domains, such as industry, medicine and finance. Analyzing this type of data is
extremely valuable for various applications. Thus, different temporal data
analysis tasks, eg, classification, clustering and prediction, have been
proposed in the past decades. Among them, causal discovery, learning the causal
relations from temporal data, is considered an interesting yet critical task
and has attracted much research attention. Existing causal discovery works can
be divided into two highly correlated categories according to whether the
temporal data is calibrated, ie, multivariate time series causal discovery, and
event sequence causal discovery. However, most previous surveys are only
focused on the time series causal discovery and ignore the second category. In
this paper, we specify the correlation between the two categories and provide a
systematical overview of existing solutions. Furthermore, we provide public
datasets, evaluation metrics and new perspectives for temporal data causal
discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10741">Computer Vision Estimation of Emotion Reaction Intensity in the Wild. (arXiv:2303.10741v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yang Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kargarandehkordi_A/0/1/0/all/0/1">Ali Kargarandehkordi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1">Onur Cezmi Mutlu</a>, <a href="http://arxiv.org/find/cs/1/au:+Surabhi_S/0/1/0/all/0/1">Saimourya Surabhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Honarmand_M/0/1/0/all/0/1">Mohammadmahdi Honarmand</a>, <a href="http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1">Dennis Paul Wall</a>, <a href="http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1">Peter Washington</a></p>
<p>Emotions play an essential role in human communication. Developing computer
vision models for automatic recognition of emotion expression can aid in a
variety of domains, including robotics, digital behavioral healthcare, and
media analytics. There are three types of emotional representations which are
traditionally modeled in affective computing research: Action Units, Valence
Arousal (VA), and Categorical Emotions. As part of an effort to move beyond
these representations towards more fine-grained labels, we describe our
submission to the newly introduced Emotional Reaction Intensity (ERI)
Estimation challenge in the 5th competition for Affective Behavior Analysis
in-the-Wild (ABAW). We developed four deep neural networks trained in the
visual domain and a multimodal model trained with both visual and audio
features to predict emotion reaction intensity. Our best performing model on
the Hume-Reaction dataset achieved an average Pearson correlation coefficient
of 0.4080 on the test set using a pre-trained ResNet50 model. This work
provides a first step towards the development of production-grade models which
predict emotion reaction intensities rather than discrete emotion categories.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04370">OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yingqiang Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1">Wenyue Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1">Kai Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jianchao Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Juntao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zelong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongfeng Zhang</a></p>
<p>Human intelligence excels at combining basic skills to solve complex tasks.
This capability is vital for Artificial Intelligence (AI) and should be
embedded in comprehensive intelligent models, enabling them to harness expert
models for complex task-solving towards Artificial General Intelligence (AGI).
Large Language Models (LLMs) show promising learning and reasoning abilities,
and can effectively use external models, tools or APIs to tackle complex
problems. In this work, we introduce OpenAGI, an open-source AGI research
platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses
a dual strategy, integrating standard benchmark tasks for benchmarking and
evaluation, and open-ended tasks including more expandable models, tools or
APIs for creative problem-solving. Tasks are presented as natural language
queries to the LLM, which then selects and executes appropriate models. We also
propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses
task results to improve the LLM's ability, which creates a self-improving AI
feedback loop. While we acknowledge that AGI is a broad and multifaceted
research challenge with no singularly defined solution path, the integration of
LLMs with domain-specific expert models, inspired by mirroring the blend of
general and specialized intelligence in humans, offers a promising approach
towards AGI. We are open-sourcing the OpenAGI project's code, dataset,
benchmarks, evaluation methods, and demo to foster community involvement in AGI
advancement: https://github.com/agiresearch/OpenAGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04934">Model Sparsity Can Simplify Machine Unlearning. (arXiv:2304.04934v7 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinghan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiancheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1">Parikshit Ram</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuguang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gaowen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pranay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a></p>
<p>In response to recent data regulation requirements, machine unlearning (MU)
has emerged as a critical process to remove the influence of specific examples
from a given model. Although exact unlearning can be achieved through complete
model retraining using the remaining dataset, the associated computational
costs have driven the development of efficient, approximate unlearning
techniques. Moving beyond data-centric MU approaches, our study introduces a
novel model-based perspective: model sparsification via weight pruning, which
is capable of reducing the gap between exact unlearning and approximate
unlearning. We show in both theory and practice that model sparsity can boost
the multi-criteria unlearning performance of an approximate unlearner, closing
the approximation gap, while continuing to be efficient. This leads to a new MU
paradigm, termed prune first, then unlearn, which infuses a sparse model prior
into the unlearning process. Building on this insight, we also develop a
sparsity-aware unlearning method that utilizes sparsity regularization to
enhance the training process of approximate unlearning. Extensive experiments
show that our proposals consistently benefit MU in various unlearning
scenarios. A notable highlight is the 77% unlearning efficacy gain of
fine-tuning (one of the simplest unlearning methods) when using sparsity-aware
unlearning. Furthermore, we demonstrate the practical impact of our proposed MU
methods in addressing other machine learning challenges, such as defending
against backdoor attacks and enhancing transfer learning. Codes are available
at https://github.com/OPTML-Group/Unlearn-Sparse.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12130">Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement. (arXiv:2304.12130v2 [physics.flu-dyn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1">Shengyu Chen</a>, <a href="http://arxiv.org/find/physics/1/au:+Bao_T/0/1/0/all/0/1">Tianshu Bao</a>, <a href="http://arxiv.org/find/physics/1/au:+Givi_P/0/1/0/all/0/1">Peyman Givi</a>, <a href="http://arxiv.org/find/physics/1/au:+Zheng_C/0/1/0/all/0/1">Can Zheng</a>, <a href="http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1">Xiaowei Jia</a></p>
<p>Simulating turbulence is critical for many societally important applications
in aerospace engineering, environmental science, the energy industry, and
biomedicine. Large eddy simulation (LES) has been widely used as an alternative
to direct numerical simulation (DNS) for simulating turbulent flows due to its
reduced computational cost. However, LES is unable to capture all of the scales
of turbulent transport accurately. Reconstructing DNS from low-resolution LES
is critical for many scientific and engineering disciplines, but it poses many
challenges to existing super-resolution methods due to the spatio-temporal
complexity of turbulent flows. In this work, we propose a new physics-guided
neural network for reconstructing the sequential DNS from low-resolution LES
data. The proposed method leverages the partial differential equation that
underlies the flow dynamics in the design of spatio-temporal model
architecture. A degradation-based refinement method is also developed to
enforce physical constraints and further reduce the accumulated reconstruction
errors over long periods. The results on two different types of turbulent flow
data confirm the superiority of the proposed method in reconstructing the
high-resolution DNS data and preserving the physical characteristics of flow
transport.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.12729">Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods. (arXiv:2304.12729v2 [astro-ph.IM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Darya_A/0/1/0/all/0/1">Abdollah Masoud Darya</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Fernini_I/0/1/0/all/0/1">Ilias Fernini</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Vellasco_M/0/1/0/all/0/1">Marley Vellasco</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hussain_A/0/1/0/all/0/1">Abir Hussain</a></p>
<p>The field of radio astronomy is witnessing a boom in the amount of data
produced per day due to newly commissioned radio telescopes. One of the most
crucial problems in this field is the automatic classification of extragalactic
radio sources based on their morphologies. Most recent contributions in the
field of morphological classification of extragalactic radio sources have
proposed classifiers based on convolutional neural networks. Alternatively,
this work proposes gradient boosting machine learning methods accompanied by
principal component analysis as data-efficient alternatives to convolutional
neural networks. Recent findings have shown the efficacy of gradient boosting
methods in outperforming deep learning methods for classification problems with
tabular data. The gradient boosting methods considered in this work are based
on the XGBoost, LightGBM, and CatBoost implementations. This work also studies
the effect of dataset size on classifier performance. A three-class
classification problem is considered in this work based on the three main
Fanaroff-Riley classes: class 0, class I, and class II, using radio sources
from the Best-Heckman sample. All three proposed gradient boosting methods
outperformed a state-of-the-art convolutional neural networks-based classifier
using less than a quarter of the number of images, with CatBoost having the
highest accuracy. This was mainly due to the superior accuracy of gradient
boosting methods in classifying Fanaroff-Riley class II sources, with
3$\unicode{x2013}$4% higher recall.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00605">Classification and Online Clustering of Zero-Day Malware. (arXiv:2305.00605v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jureckova_O/0/1/0/all/0/1">Olha Jure&#x10d;kov&#xe1;</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurecek_M/0/1/0/all/0/1">Martin Jure&#x10d;ek</a>, <a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1">Mark Stamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1">Fabio Di Troia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lorencz_R/0/1/0/all/0/1">R&#xf3;bert L&#xf3;rencz</a></p>
<p>A large amount of new malware is constantly being generated, which must not
only be distinguished from benign samples, but also classified into malware
families. For this purpose, investigating how existing malware families are
developed and examining emerging families need to be explored. This paper
focuses on the online processing of incoming malicious samples to assign them
to existing families or, in the case of samples from new families, to cluster
them. We experimented with seven prevalent malware families from the EMBER
dataset, four in the training set and three additional new families in the test
set. Based on the classification score of the multilayer perceptron, we
determined which samples would be classified and which would be clustered into
new malware families. We classified 97.21% of streaming data with a balanced
accuracy of 95.33%. Then, we clustered the remaining data using a
self-organizing map, achieving a purity from 47.61% for four clusters to 77.68%
for ten clusters. These results indicate that our approach has the potential to
be applied to the classification and clustering of zero-day malware into
malware families.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04800">Mlinear: Rethink the Linear Model for Time-series Forecasting. (arXiv:2305.04800v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1">Xiangxu Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chuhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianing Chen</a></p>
<p>Recently, significant advancements have been made in time-series forecasting
research, with an increasing focus on analyzing the nature of time-series data,
e.g, channel-independence (CI) and channel-dependence (CD), rather than solely
focusing on designing sophisticated forecasting models. However, current
research has primarily focused on either CI or CD in isolation, and the
challenge of effectively combining these two opposing properties to achieve a
synergistic effect remains an unresolved issue. In this paper, we carefully
examine the opposing properties of CI and CD, and raise a practical question
that has not been effectively answered, e.g.,"How to effectively mix the CI and
CD properties of time series to achieve better predictive performance?" To
answer this question, we propose Mlinear (MIX-Linear), a simple yet effective
method based mainly on linear layers. The design philosophy of Mlinear mainly
includes two aspects:(1) dynamically tuning the CI and CD properties based on
the time semantics of different input time series, and (2) providing deep
supervision to adjust the individual performance of the "CI predictor" and "CD
predictor". In addition, empirically, we introduce a new loss function that
significantly outperforms the widely used mean squared error (MSE) on multiple
datasets. Experiments on time-series datasets covering multiple fields and
widely used have demonstrated the superiority of our method over PatchTST which
is the lateset Transformer-based method in terms of the MSE and MAE metrics on
7 datasets with identical sequence inputs (336 or 512). Specifically, our
method significantly outperforms PatchTST with a ratio of 21:3 at 336 sequence
length input and 29:10 at 512 sequence length input. Additionally, our approach
has a 10 $\times$ efficiency advantage at the unit level, taking into account
both training and inference times.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10406">Variational Classification. (arXiv:2305.10406v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1">Shehzaad Dhuliawala</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1">Carl Allen</a></p>
<p>We present a latent variable generalisation of neural network softmax
classification trained with cross-entropy loss, referred to as variational
classification (VC). Our approach offers a novel probabilistic perspective on
the highly familiar softmax classification model, to which it relates similarly
to how variational and traditional autoencoders relate. We derive a training
objective based on the evidence lower bound (ELBO) that is non-trivial to
optimize, and therefore propose an adversarial approach to maximise it. We show
that VC addresses an inherent inconsistency within softmax classification,
whilst also allowing more flexible choices of prior distributions in the latent
space in place of implicit assumptions revealed within off-the-shelf softmax
classifiers. Empirical evaluation on image and text classification datasets
demonstrates that variational classification maintains prediction accuracy
while improving other desirable properties such as calibration and adversarial
robustness, particularly under distribution shift and low data settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16174">From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Battiloro_C/0/1/0/all/0/1">Claudio Battiloro</a>, <a href="http://arxiv.org/find/cs/1/au:+Spinelli_I/0/1/0/all/0/1">Indro Spinelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Telyatnikov_L/0/1/0/all/0/1">Lev Telyatnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1">Michael Bronstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1">Simone Scardapane</a>, <a href="http://arxiv.org/find/cs/1/au:+Lorenzo_P/0/1/0/all/0/1">Paolo Di Lorenzo</a></p>
<p>Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks
(GNNs) on a given graph topology by dynamically learning it. However, most of
LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph
to rewire and can solely learn regular graph topologies. In the wake of the
success of Topological Deep Learning (TDL), we study Latent Topology Inference
(LTI) for learning higher-order cell complexes (with sparse and not regular
topology) describing multi-way interactions between data points. To this aim,
we introduce the Differentiable Cell Complex Module (DCM), a novel learnable
function that computes cell probabilities in the complex to improve the
downstream task. We show how to integrate DCM with cell complex message passing
networks layers and train it in a end-to-end fashion, thanks to a two-step
inference procedure that avoids an exhaustive search across all possible cells
in the input, thus maintaining scalability. Our model is tested on several
homophilic and heterophilic graph datasets and it is shown to outperform other
state-of-the-art techniques, offering significant improvements especially in
cases where an input graph is not provided.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19569">Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis. (arXiv:2305.19569v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1">Jong Moon Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1">Olga Fink</a></p>
<p>Extensive research has been conducted on fault diagnosis of planetary
gearboxes using vibration signals and deep learning (DL) approaches. However,
DL-based methods are susceptible to the domain shift problem caused by varying
operating conditions of the gearbox. Although domain adaptation and data
synthesis methods have been proposed to overcome such domain shifts, they are
often not directly applicable in real-world situations where only healthy data
is available in the target domain. To tackle the challenge of extreme domain
shift scenarios where only healthy data is available in the target domain, this
paper proposes two novel domain knowledge-informed data synthesis methods
utilizing the health data map (HDMap). The two proposed approaches are referred
to as scaled CutPaste and FaultPaste. The HDMap is used to physically represent
the vibration signal of the planetary gearbox as an image-like matrix, allowing
for visualization of fault-related features. CutPaste and FaultPaste are then
applied to generate faulty samples based on the healthy data in the target
domain, using domain knowledge and fault signatures extracted from the source
domain, respectively. In addition to generating realistic faults, the proposed
methods introduce scaling of fault signatures for controlled synthesis of
faults with various severity levels. A case study is conducted on a planetary
gearbox testbed to evaluate the proposed approaches. The results show that the
proposed methods are capable of accurately diagnosing faults, even in cases of
extreme domain shift, and can estimate the severity of faults that have not
been previously observed in the target domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05357">Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yilun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua B. Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a></p>
<p>Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10045">Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v8 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1">Yuchao Lin</a>, <a href="http://arxiv.org/find/physics/1/au:+Yan_K/0/1/0/all/0/1">Keqiang Yan</a>, <a href="http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1">Youzhi Luo</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Qian_X/0/1/0/all/0/1">Xiaoning Qian</a>, <a href="http://arxiv.org/find/physics/1/au:+Ji_S/0/1/0/all/0/1">Shuiwang Ji</a></p>
<p>We study property prediction for crystal materials. A crystal structure
consists of a minimal unit cell that is repeated infinitely in 3D space. How to
accurately represent such repetitive structures in machine learning models
remains unresolved. Current methods construct graphs by establishing edges only
between nearby nodes, thereby failing to faithfully capture infinite repeating
patterns and distant interatomic interactions. In this work, we propose several
innovations to overcome these limitations. First, we propose to model
physics-principled interatomic potentials directly instead of only using
distances as in many existing methods. These potentials include the Coulomb
potential, London dispersion potential, and Pauli repulsion potential. Second,
we model the complete set of potentials among all atoms, instead of only
between nearby atoms as in existing methods. This is enabled by our
approximations of infinite potential summations with provable error bounds. We
further develop efficient algorithms to compute the approximations. Finally, we
propose to incorporate our computations of complete interatomic potentials into
message passing neural networks for representation learning. We perform
experiments on the JARVIS and Materials Project benchmarks for evaluation.
Results show that the use of interatomic potentials and complete interatomic
potentials leads to consistent performance improvements with reasonable
computational costs. Our code is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11363">Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jiachen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Peng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1">Zhongjie Ba</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>Diffusion models have emerged as the \emph{de-facto} technique for image
generation, yet they entail significant computational overhead, hindering the
technique's broader application in the research community. We propose a
prior-based denoising training framework, the first to incorporate the
pre-train and fine-tune paradigm into the diffusion model training process,
which substantially improves training efficiency and shows potential in
facilitating various downstream tasks. Our approach centers on masking a high
proportion (e.g., up to 90\%) of the input image and employing masked denoising
score matching to denoise the visible areas, thereby guiding the diffusion
model to learn more salient features from training data as prior knowledge. By
utilizing masked learning in a pre-training stage, we efficiently train the
ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space,
achieving a 4x acceleration and enhancing the quality of generated images
compared to denoising diffusion probabilistic model (DDPM). Moreover, our
masked pre-training technique can be universally applied to various diffusion
models that directly generate images in the pixel space, aiding in the learning
of pre-trained models with superior generalizability. For instance, a diffusion
model pre-trained on VGGFace2 attains a 46\% quality improvement through
fine-tuning with merely 10\% data from a different distribution. Moreover, our
method shows the potential to serve as a training paradigm for enhancing the
privacy protection capabilities of diffusion models. Our code is available at
\url{https://github.com/jiachenlei/maskdm}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12344">An efficient, provably exact, practical algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_W/0/1/0/all/0/1">Waheed Ul Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1">Max A. Little</a></p>
<p>Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the rigorous construction of a new
algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss
classification problem exactly in polynomial time. We prove correctness using
concepts from the theory of hyperplane arrangements and oriented matroids. We
demonstrate the effectiveness of this algorithm on synthetic and real-world
datasets, showing optimal accuracy both in and out-of-sample, in practical
computational time. We also empirically demonstrate how the use of approximate
upper bound leads to polynomial time run-time improvements to the algorithm
whilst retaining exactness. To our knowledge, this is the first,
rigorously-proven polynomial time, practical algorithm for this long-standing
problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13773">Nearest Neighbour with Bandit Feedback. (arXiv:2306.13773v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pasteris_S/0/1/0/all/0/1">Stephen Pasteris</a>, <a href="http://arxiv.org/find/cs/1/au:+Hicks_C/0/1/0/all/0/1">Chris Hicks</a>, <a href="http://arxiv.org/find/cs/1/au:+Mavroudis_V/0/1/0/all/0/1">Vasilios Mavroudis</a></p>
<p>In this paper we adapt the nearest neighbour rule to the contextual bandit
problem. Our algorithm handles the fully adversarial setting in which no
assumptions at all are made about the data-generation process. When combined
with a sufficiently fast data-structure for (perhaps approximate) adaptive
nearest neighbour search, such as a navigating net, our algorithm is extremely
efficient - having a per trial running time polylogarithmic in both the number
of trials and actions, and taking only quasi-linear space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13866">MIRACLE: Multi-task Learning based Interpretable Regulation of Autoimmune Diseases through Common Latent Epigenetics. (arXiv:2306.13866v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Pengcheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jinpu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yulin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_Z/0/1/0/all/0/1">Ziqi Rong</a></p>
<p>DNA methylation is a crucial regulator of gene transcription and has been
linked to various diseases, including autoimmune diseases and cancers. However,
diagnostics based on DNA methylation face challenges due to large feature sets
and small sample sizes, resulting in overfitting and suboptimal performance. To
address these issues, we propose MIRACLE, a novel interpretable neural network
that leverages autoencoder-based multi-task learning to integrate multiple
datasets and jointly identify common patterns in DNA methylation.
</p>
<p>MIRACLE's architecture reflects the relationships between methylation sites,
genes, and pathways, ensuring biological interpretability and meaningfulness.
The network comprises an encoder and a decoder, with a bottleneck layer
representing pathway information as the basic unit of heredity. Customized
defined MaskedLinear Layer is constrained by site-gene-pathway graph adjacency
matrix information, which provides explainability and expresses the
site-gene-pathway hierarchical structure explicitly. And from the embedding,
there are different multi-task classifiers to predict diseases.
</p>
<p>Tested on six datasets, including rheumatoid arthritis, systemic lupus
erythematosus, multiple sclerosis, inflammatory bowel disease, psoriasis, and
type 1 diabetes, MIRACLE demonstrates robust performance in identifying common
functions of DNA methylation across different phenotypes, with higher accuracy
in prediction dieseases than baseline methods. By incorporating biological
prior knowledge, MIRACLE offers a meaningful and interpretable framework for
DNA methylation data analysis in the context of autoimmune diseases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04577">AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System. (arXiv:2307.04577v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yuzhe Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Binghao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wyk_K/0/1/0/all/0/1">Karl Van Wyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1">Yu-Wei Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1">Dieter Fox</a></p>
<p>Vision-based teleoperation offers the possibility to endow robots with
human-level intelligence to physically interact with the environment, while
only requiring low-cost camera sensors. However, current vision-based
teleoperation systems are designed and engineered towards a particular robot
model and deploy environment, which scales poorly as the pool of the robot
models expands and the variety of the operating environment increases. In this
paper, we propose AnyTeleop, a unified and general teleoperation system to
support multiple different arms, hands, realities, and camera configurations
within a single system. Although being designed to provide great flexibility to
the choice of simulators and real hardware, our system can still achieve great
performance. For real-world experiments, AnyTeleop can outperform a previous
system that was designed for a specific robot hardware with a higher success
rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to
better imitation learning performance, compared with a previous system that is
particularly designed for that simulator. Project page: <a href="http://anyteleop.com/.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10803">Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities. (arXiv:2307.10803v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hanchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wengen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1">Jihong Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shuigeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiannong Cao</a></p>
<p>With the rapid amassing of spatial-temporal (ST) ocean data, many
spatial-temporal data mining (STDM) studies have been conducted to address
various oceanic issues, including climate forecasting and disaster warning.
Compared with typical ST data (e.g., traffic data), ST ocean data is more
complicated but with unique characteristics, e.g., diverse regionality and high
sparsity. These characteristics make it difficult to design and train STDM
models on ST ocean data. To the best of our knowledge, a comprehensive survey
of existing studies remains missing in the literature, which hinders not only
computer scientists from identifying the research issues in ocean data mining
but also ocean scientists to apply advanced STDM techniques. In this paper, we
provide a comprehensive survey of existing STDM studies for ocean science.
Concretely, we first review the widely-used ST ocean datasets and highlight
their unique characteristics. Then, typical ST ocean data quality enhancement
techniques are explored. Next, we classify existing STDM studies in ocean
science into four types of tasks, i.e., prediction, event detection, pattern
mining, and anomaly detection, and elaborate on the techniques for these tasks.
Finally, promising research opportunities are discussed. This survey can help
scientists from both computer science and ocean science better understand the
fundamental concepts, key techniques, and open challenges of STDM for ocean
science.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13539">Model Calibration in Dense Classification with Adaptive Label Perturbation. (arXiv:2307.13539v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiawei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1">Changkun Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1">Ruikai Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1">Nick Barnes</a></p>
<p>For safety-related applications, it is crucial to produce trustworthy deep
neural networks whose prediction is associated with confidence that can
represent the likelihood of correctness for subsequent decision-making.
Existing dense binary classification models are prone to being over-confident.
To improve model calibration, we propose Adaptive Stochastic Label Perturbation
(ASLP) which learns a unique label perturbation level for each training image.
ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss,
which unifies label perturbation processes including stochastic approaches
(like DisturbLabel), and label smoothing, to correct calibration while
maintaining classification rates. ASLP follows Maximum Entropy Inference of
classic statistical mechanics to maximise prediction entropy with respect to
missing information. It performs this while: (1) preserving classification
accuracy on known data as a conservative solution, or (2) specifically improves
model calibration degree by minimising the gap between the prediction accuracy
and expected confidence of the target training label. Extensive results
demonstrate that ASLP can significantly improve calibration degrees of dense
binary classification models on both in-distribution and out-of-distribution
data. The code is available on https://github.com/Carlisle-Liu/ASLP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13831">Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsukada_Y/0/1/0/all/0/1">Yuki Tsukada</a>, <a href="http://arxiv.org/find/cs/1/au:+Iiduka_H/0/1/0/all/0/1">Hideaki Iiduka</a></p>
<p>Stochastic gradient descent (SGD) is the simplest deep learning optimizer
with which to train deep neural networks. While SGD can use various learning
rates, such as constant or diminishing rates, the previous numerical results
showed that SGD performs better than other deep learning optimizers using when
it uses learning rates given by line search methods. In this paper, we perform
a convergence analysis on SGD with a learning rate given by an Armijo line
search for nonconvex optimization. The analysis indicates that the upper bound
of the expectation of the squared norm of the full gradient becomes small when
the number of steps and the batch size are large. Next, we show that, for SGD
with the Armijo-line-search learning rate, the number of steps needed for
nonconvex optimization is a monotone decreasing convex function of the batch
size; that is, the number of steps needed for nonconvex optimization decreases
as the batch size increases. Furthermore, we show that the stochastic
first-order oracle (SFO) complexity, which is the stochastic gradient
computation cost, is a convex function of the batch size; that is, there exists
a critical batch size that minimizes the SFO complexity. Finally, we provide
numerical results that support our theoretical results. The numerical results
indicate that the number of steps needed for training deep neural networks
decreases as the batch size increases and that there exist the critical batch
sizes that can be estimated from the theoretical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16149">An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Alromih_A/0/1/0/all/0/1">Arwa Alromih</a>, <a href="http://arxiv.org/find/cs/1/au:+Gope_P/0/1/0/all/0/1">Prosanta Gope</a>, <a href="http://arxiv.org/find/cs/1/au:+Sikdar_B/0/1/0/all/0/1">Biplab Sikdar</a></p>
<p>Energy theft detection (ETD) and energy consumption forecasting (ECF) are two
interconnected challenges in smart grid systems. Addressing these issues
collectively is crucial for ensuring system security. This paper addresses the
interconnected challenges of ETD and ECF in smart grid systems. The proposed
solution combines long short-term memory (LSTM) and a denoising diffusion
probabilistic model (DDPM) to generate input reconstruction and forecasting. By
leveraging the reconstruction and forecasting errors, the system identifies
instances of energy theft, with the methods based on reconstruction error and
forecasting error complementing each other in detecting different types of
attacks. Through extensive experiments on real-world and synthetic datasets,
the proposed scheme outperforms baseline methods in ETD and ECF problems. The
ensemble method significantly enhances ETD performance, accurately detecting
energy theft attacks that baseline methods fail to detect. The research offers
a comprehensive and effective solution for addressing ETD and ECF challenges,
demonstrating promising results and improved security in smart grid systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16680">On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1">Mingyuan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jun Huang</a></p>
<p>Diffusion models and large language models have emerged as leading-edge
generative models and have sparked a revolutionary impact on various aspects of
human life. However, the practical implementation of these models has also
exposed inherent risks, highlighting their dual nature and raising concerns
regarding their trustworthiness. Despite the abundance of literature on this
subject, a comprehensive survey specifically delving into the intersection of
large-scale generative models and their trustworthiness remains largely absent.
To bridge this gap, This paper investigates both the long-standing and emerging
threats associated with these models across four fundamental dimensions:
privacy, security, fairness, and responsibility. In this way, we construct an
extensive map outlining the trustworthiness of these models, while also
providing practical recommendations and identifying future directions. These
efforts are crucial for promoting the trustworthy deployment of these models,
ultimately benefiting society as a whole.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00788">An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yihua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1">Prashant Khanduri</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsaknakis_I/0/1/0/all/0/1">Ioannis Tsaknakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuguang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1">Mingyi Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a></p>
<p>Recently, bi-level optimization (BLO) has taken center stage in some very
exciting developments in the area of signal processing (SP) and machine
learning (ML). Roughly speaking, BLO is a classical optimization problem that
involves two levels of hierarchy (i.e., upper and lower levels), wherein
obtaining the solution to the upper-level problem requires solving the
lower-level one. BLO has become popular largely because it is powerful in
modeling problems in SP and ML, among others, that involve optimizing nested
objective functions. Prominent applications of BLO range from resource
allocation for wireless systems to adversarial machine learning. In this work,
we focus on a class of tractable BLO problems that often appear in SP and ML
applications. We provide an overview of some basic concepts of this class of
BLO problems, such as their optimality conditions, standard algorithms
(including their optimization principles and practical implementations), as
well as how they can be leveraged to obtain state-of-the-art results for a
number of key SP and ML applications. Further, we discuss some recent advances
in BLO theory, its implications for applications, and point out some
limitations of the state-of-the-art that require significant future research
efforts. Overall, we hope that this article can serve to accelerate the
adoption of BLO as a generic tool to model, analyze, and innovate on a wide
array of emerging SP and ML applications.
</p>
</p>
</div>

    </div>
    </body>
    