<!DOCTYPE html>
<html>
<head>
<title>2023-11-30-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.16104">Data Analytics with Differential Privacy. (arXiv:2311.16104v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Digalakis_V/0/1/0/all/0/1">Vassilis Digalakis Jr</a></p>
<p>Differential privacy is the state-of-the-art definition for privacy,
guaranteeing that any analysis performed on a sensitive dataset leaks no
information about the individuals whose data are contained therein. In this
thesis, we develop differentially private algorithms to analyze distributed and
streaming data. In the distributed model, we consider the particular problem of
learning -- in a distributed fashion -- a global model of the data, that can
subsequently be used for arbitrary analyses. We build upon PrivBayes, a
differentially private method that approximates the high-dimensional
distribution of a centralized dataset as a product of low-order distributions,
utilizing a Bayesian Network model. We examine three novel approaches to
learning a global Bayesian Network from distributed data, while offering the
differential privacy guarantee to all local datasets. Our work includes a
detailed theoretical analysis of the distributed, differentially private
entropy estimator which we use in one of our algorithms, as well as a detailed
experimental evaluation, using both synthetic and real-world data. In the
streaming model, we focus on the problem of estimating the density of a stream
of users, which expresses the fraction of all users that actually appear in the
stream. We offer one of the strongest privacy guarantees for the streaming
model, user-level pan-privacy, which ensures that the privacy of any user is
protected, even against an adversary that observes the internal state of the
algorithm. We provide a detailed analysis of an existing, sampling-based
algorithm for the problem and propose two novel modifications that
significantly improve it, both theoretically and experimentally, by optimally
using all the allocated "privacy budget."
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16109">Transfer Learning between Motor Imagery Datasets using Deep Learning -- Validation of Framework and Comparison of Datasets. (arXiv:2311.16109v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guetschel_P/0/1/0/all/0/1">Pierre Guetschel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tangermann_M/0/1/0/all/0/1">Michael Tangermann</a></p>
<p>We present a simple deep learning-based framework commonly used in computer
vision and demonstrate its effectiveness for cross-dataset transfer learning in
mental imagery decoding tasks that are common in the field of Brain-Computer
Interfaces (BCI). We investigate, on a large selection of 12 motor-imagery
datasets, which ones are well suited for transfer, both as donors and as
receivers. Challenges. Deep learning models typically require long training
times and are data-hungry, which impedes their use for BCI systems that have to
minimize the recording time for (training) examples and are subject to
constraints induced by experiments involving human subjects. A solution to both
issues is transfer learning, but it comes with its own challenge, i.e.,
substantial data distribution shifts between datasets, subjects and even
between subsequent sessions of the same subject. Approach. For every pair of
pre-training (donor) and test (receiver) dataset, we first train a model on the
donor before training merely an additional new linear classification layer
based on a few receiver trials. Performance of this transfer approach is then
tested on other trials of the receiver dataset. Significance. First, we lower
the threshold to use transfer learning between motor imagery datasets: the
overall framework is extremely simple and nevertheless obtains decent
classification scores. Second, we demonstrate that deep learning models are a
good option for motor imagery cross-dataset transfer both for the reasons
outlined in the first point and because the framework presented is viable in
online scenarios. Finally, analysing which datasets are best suited for
transfer learning can be used as a reference for future researchers to
determine which to use for pre-training or benchmarking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16112">Co-learning synaptic delays, weights and adaptation in spiking neural networks. (arXiv:2311.16112v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deckers_L/0/1/0/all/0/1">Lucas Deckers</a>, <a href="http://arxiv.org/find/cs/1/au:+Damme_L/0/1/0/all/0/1">Laurens Van Damme</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1">Ing Jyh Tsang</a>, <a href="http://arxiv.org/find/cs/1/au:+Leekwijck_W/0/1/0/all/0/1">Werner Van Leekwijck</a>, <a href="http://arxiv.org/find/cs/1/au:+Latre_S/0/1/0/all/0/1">Steven Latr&#xe9;</a></p>
<p>Spiking neural networks (SNN) distinguish themselves from artificial neural
networks (ANN) because of their inherent temporal processing and spike-based
computations, enabling a power-efficient implementation in neuromorphic
hardware. In this paper, we demonstrate that data processing with spiking
neurons can be enhanced by co-learning the connection weights with two other
biologically inspired neuronal features: 1) a set of parameters describing
neuronal adaptation processes and 2) synaptic propagation delays. The former
allows the spiking neuron to learn how to specifically react to incoming spikes
based on its past. The trained adaptation parameters result in neuronal
heterogeneity, which is found in the brain and also leads to a greater variety
in available spike patterns. The latter enables to learn to explicitly
correlate patterns that are temporally distanced. Synaptic delays reflect the
time an action potential requires to travel from one neuron to another. We show
that each of the co-learned features separately leads to an improvement over
the baseline SNN and that the combination of both leads to state-of-the-art SNN
results on all speech recognition datasets investigated with a simple 2-hidden
layer feed-forward network. Our SNN outperforms the ANN on the neuromorpic
datasets (Spiking Heidelberg Digits and Spiking Speech Commands), even with
fewer trainable parameters. On the 35-class Google Speech Commands dataset, our
SNN also outperforms a GRU of similar size. Our work presents brain-inspired
improvements to SNN that enable them to excel over an equivalent ANN of similar
size on tasks with rich temporal dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16114">Learning Noise-Robust Joint Representation for Multimodal Emotion Recognition under Realistic Incomplete Data Scenarios. (arXiv:2311.16114v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1">Qi Fan</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zuo_H/0/1/0/all/0/1">Haolin Zuo</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rui Liu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1">Zheng Lian</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1">Guanglai Gao</a> (1) ((1) Inner Mongolia University, Hohhot, China, (2) Institute of Automation, Chinese Academy of Sciences, Beijing, China)</p>
<p>Multimodal emotion recognition (MER) in practical scenarios presents a
significant challenge due to the presence of incomplete data, such as missing
or noisy data. Traditional methods often discard missing data or replace it
with a zero vector, neglecting the availability issue of noisy data.
Consequently, these approaches are not fully applicable to realistic scenarios,
where both missing and noisy data are prevalent. To address this problem, we
propose a novel noise-robust MER model, named NMER, which effectively learns
robust multimodal joint representations from incomplete data containing noise.
Our approach incorporates two key components. First, we introduce a noise
scheduler that adjusts the type and level of noise in the training data,
emulating the characteristics of incomplete data in realistic scenarios.
Second, we employ a Variational AutoEncoder (VAE)-based NMER model to generate
robust multimodal joint representations from the noisy data, leveraging the
modality invariant feature. The experimental results on the benchmark dataset
IEMOCAP indicate the proposed NMER outperforms state-of-the-art MER systems.
The ablation results also confirm the effectiveness of the VAE structure. We
release our code at \href{https://github.com/WooyoohL/Noise-robust_MER.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16118">Imperceptible CMOS camera dazzle for adversarial attacks on deep neural networks. (arXiv:2311.16118v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stein_Z/0/1/0/all/0/1">Zvi Stein</a>, <a href="http://arxiv.org/find/cs/1/au:+Stern_A/0/1/0/all/0/1">Adrian Stern</a></p>
<p>Despite the outstanding performance of deep neural networks, they are
vulnerable to adversarial attacks. While there are many invisible attacks in
the digital domain, most physical world adversarial attacks are visible. Here
we present an invisible optical adversarial attack that uses a light source to
dazzle a CMOS camera with a rolling shutter. We present the photopic conditions
required to keep the attacking light source completely invisible while
sufficiently jamming the captured image so that a deep neural network applied
to it is deceived.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16122">Semantic Generative Augmentations for Few-Shot Counting. (arXiv:2311.16122v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doubinsky_P/0/1/0/all/0/1">Perla Doubinsky</a> (CEDRIC - VERTIGO, CNAM), <a href="http://arxiv.org/find/cs/1/au:+Audebert_N/0/1/0/all/0/1">Nicolas Audebert</a> (CEDRIC - VERTIGO, CNAM), <a href="http://arxiv.org/find/cs/1/au:+Crucianu_M/0/1/0/all/0/1">Michel Crucianu</a> (CEDRIC - VERTIGO), <a href="http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1">Herv&#xe9; Le Borgne</a> (CEA)</p>
<p>With the availability of powerful text-to-image diffusion models, recent
works have explored the use of synthetic data to improve image classification
performances. These works show that it can effectively augment or even replace
real data. In this work, we investigate how synthetic data can benefit few-shot
class-agnostic counting. This requires to generate images that correspond to a
given input number of objects. However, text-to-image models struggle to grasp
the notion of count. We propose to rely on a double conditioning of Stable
Diffusion with both a prompt and a density map in order to augment a training
dataset for few-shot counting. Due to the small dataset size, the fine-tuned
model tends to generate images close to the training images. We propose to
enhance the diversity of synthesized images by exchanging captions between
images thus creating unseen configurations of object types and spatial layout.
Our experiments show that our diversified generation strategy significantly
improves the counting accuracy of two recent and performing few-shot counting
models on FSC147 and CARPK.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16125">Vision-Based Incoming Traffic Estimator Using Deep Neural Network on General Purpose Embedded Hardware. (arXiv:2311.16125v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zoysa_K/0/1/0/all/0/1">K. G. Zoysa</a>, <a href="http://arxiv.org/find/cs/1/au:+Munasinghe_S/0/1/0/all/0/1">S. R. Munasinghe</a></p>
<p>Traffic management is a serious problem in many cities around the world. Even
the suburban areas are now experiencing regular traffic congestion.
Inappropriate traffic control wastes fuel, time, and the productivity of
nations. Though traffic signals are used to improve traffic flow, they often
cause problems due to inappropriate or obsolete timing that does not tally with
the actual traffic intensity at the intersection. Traffic intensity
determination based on statistical methods only gives the average intensity
expected at any given time. However, to control traffic accurately, it is
required to know the real-time traffic intensity. In this research, image
processing and machine learning have been used to estimate actual traffic
intensity in real time. General-purpose electronic hardware has been used for
in-situ image processing based on the edge-detection method. A deep neural
network (DNN) was trained to infer traffic intensity in each image in real
time. The trained DNN estimated traffic intensity accurately in 90% of the
real-time images during road tests. The electronic system was implemented on a
Raspberry Pi single-board computer; hence, it is cost-effective for large-scale
deployment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16126">A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design. (arXiv:2311.16126v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Wu_F/0/1/0/all/0/1">Fang Wu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1">Stan Z. Li</a></p>
<p>Therapeutic antibodies are an essential and rapidly expanding drug modality.
The binding specificity between antibodies and antigens is decided by
complementarity-determining regions (CDRs) at the tips of these Y-shaped
proteins. In this paper, we propose a hierarchical training paradigm (HTP) for
the antibody sequence-structure co-design. HTP consists of four levels of
training stages, each corresponding to a specific protein modality within a
particular protein domain. Through carefully crafted tasks in different stages,
HTP seamlessly and effectively integrates geometric graph neural networks
(GNNs) with large-scale protein language models to excavate evolutionary
information from not only geometric structures but also vast antibody and
non-antibody sequence databases, which determines ligand binding pose and
strength. Empirical experiments show that HTP sets the new state-of-the-art
performance in the co-design problem as well as the fix-backbone design. Our
research offers a hopeful path to unleash the potential of deep generative
architectures and seeks to illuminate the way forward for the antibody sequence
and structure co-design challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16132">A novel RNA pseudouridine site prediction model using Utility Kernel and data-driven parameters. (arXiv:2311.16132v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Patil_S/0/1/0/all/0/1">Sourabh Patil</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mathur_A/0/1/0/all/0/1">Archana Mathur</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Aduri_R/0/1/0/all/0/1">Raviprasad Aduri</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Saha_S/0/1/0/all/0/1">Snehanshu Saha</a></p>
<p>RNA protein Interactions (RPIs) play an important role in biological systems.
Recently, we have enumerated the RPIs at the residue level and have elucidated
the minimum structural unit (MSU) in these interactions to be a stretch of five
residues (Nucleotides/amino acids). Pseudouridine is the most frequent
modification in RNA. The conversion of uridine to pseudouridine involves
interactions between pseudouridine synthase and RNA. The existing models to
predict the pseudouridine sites in a given RNA sequence mainly depend on
user-defined features such as mono and dinucleotide composition/propensities of
RNA sequences. Predicting pseudouridine sites is a non-linear classification
problem with limited data points. Deep Learning models are efficient
discriminators when the data set size is reasonably large and fail when there
is a paucity of data ($&lt;1000$ samples). To mitigate this problem, we propose a
Support Vector Machine (SVM) Kernel based on utility theory from Economics, and
using data-driven parameters (i.e. MSU) as features. For this purpose, we have
used position-specific tri/quad/pentanucleotide composition/propensity
(PSPC/PSPP) besides nucleotide and dineculeotide composition as features. SVMs
are known to work well in small data regimes and kernels in SVM are designed to
classify non-linear data. The proposed model outperforms the existing
state-of-the-art models significantly (10%-15% on average).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16135">Use of Deep Neural Networks for Uncertain Stress Functions with Extensions to Impact Mechanics. (arXiv:2311.16135v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Blum_G/0/1/0/all/0/1">Garrett Blum</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Doris_R/0/1/0/all/0/1">Ryan Doris</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Klabjan_D/0/1/0/all/0/1">Diego Klabjan</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Espinosa_H/0/1/0/all/0/1">Horacio Espinosa</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Szalkowski_R/0/1/0/all/0/1">Ron Szalkowski</a></p>
<p>Stress-strain curves, or more generally, stress functions, are an extremely
important characterization of a material's mechanical properties. However,
stress functions are often difficult to derive and are narrowly tailored to a
specific material. Further, large deformations, high strain-rates, temperature
sensitivity, and effect of material parameters compound modeling challenges. We
propose a generalized deep neural network approach to model stress as a state
function with quantile regression to capture uncertainty. We extend these
models to uniaxial impact mechanics using stochastic differential equations to
demonstrate a use case and provide a framework for implementing this
uncertainty-aware stress function. We provide experiments benchmarking our
approach against leading constitutive, machine learning, and transfer learning
approaches to stress and impact mechanics modeling on publicly available and
newly presented data sets. We also provide a framework to optimize material
parameters given multiple competing impact scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16139">GNNBleed: Inference Attacks to Unveil Private Edges in Graphs with Realistic Access to GNN Models. (arXiv:2311.16139v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zeyu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabir_E/0/1/0/all/0/1">Ehsanul Kabir</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehnaz_S/0/1/0/all/0/1">Shagufta Mehnaz</a></p>
<p>Graph Neural Networks (GNNs) have increasingly become an indispensable tool
in learning from graph-structured data, catering to various applications
including social network analysis, recommendation systems, etc. At the heart of
these networks are the edges which are crucial in guiding GNN models'
predictions. In many scenarios, these edges represent sensitive information,
such as personal associations or financial dealings -- thus requiring privacy
assurance. However, their contributions to GNN model predictions may in turn be
exploited by the adversary to compromise their privacy. Motivated by these
conflicting requirements, this paper investigates edge privacy in contexts
where adversaries possess black-box GNN model access, restricted further by
access controls, preventing direct insights into arbitrary node outputs. In
this context, we introduce a series of privacy attacks grounded on the
message-passing mechanism of GNNs. These strategies allow adversaries to deduce
connections between two nodes not by directly analyzing the model's output for
these pairs but by analyzing the output for nodes linked to them. Our
evaluation with seven real-life datasets and four GNN architectures underlines
a significant vulnerability: even in systems fortified with access control
mechanisms, an adaptive adversary can decipher private connections between
nodes, thereby revealing potentially sensitive relationships and compromising
the confidentiality of the graph.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16140">Adapting Segment Anything Model (SAM) through Prompt-based Learning for Enhanced Protein Identification in Cryo-EM Micrographs. (arXiv:2311.16140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1">Fei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhiyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Mingyue Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Poudel_B/0/1/0/all/0/1">Biplab Poudel</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhas_N/0/1/0/all/0/1">Newgin Sam Ebin Sam Dhas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyawali_R/0/1/0/all/0/1">Rajan Gyawali</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhakal_A/0/1/0/all/0/1">Ashwin Dhakal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jianlin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dong Xu</a></p>
<p>Cryo-electron microscopy (cryo-EM) remains pivotal in structural biology, yet
the task of protein particle picking, integral for 3D protein structure
construction, is laden with manual inefficiencies. While recent AI tools such
as Topaz and crYOLO are advancing the field, they do not fully address the
challenges of cryo-EM images, including low contrast, complex shapes, and
heterogeneous conformations. This study explored prompt-based learning to adapt
the state-of-the-art image segmentation foundation model Segment Anything Model
(SAM) for cryo-EM. This focus was driven by the desire to optimize model
performance with a small number of labeled data without altering pre-trained
parameters, aiming for a balance between adaptability and foundational
knowledge retention. Through trials with three prompt-based learning
strategies, namely head prompt, prefix prompt, and encoder prompt, we observed
enhanced performance and reduced computational requirements compared to the
fine-tuning approach. This work not only highlights the potential of prompting
SAM in protein identification from cryo-EM micrographs but also suggests its
broader promise in biomedical image segmentation and object detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16141">Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking Neural Networks. (arXiv:2311.16141v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Boxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1">Haihang You</a></p>
<p>Spiking Neural Networks (SNNs) have been an attractive option for deployment
on devices with limited computing resources and lower power consumption because
of the event-driven computing characteristic. As such devices have limited
computing and storage resources, pruning for SNNs has been widely focused
recently. However, the binary and non-differentiable property of spike signals
make pruning deep SNNs challenging, so existing methods require high time
overhead to make pruning decisions. In this paper, inspired by critical brain
hypothesis in neuroscience, we design a regeneration mechanism based on
criticality to efficiently obtain the critical pruned networks. Firstly, we
propose a low-cost metric for the criticality of pruning structures. Then we
re-rank the pruned structures after pruning and regenerate those with higher
criticality. We evaluate our method using VGG-16 and ResNet-19 for both
unstructured pruning and structured pruning. Our method achieves higher
performance compared to current state-of-the-art (SOTA) method with the same
time overhead. We also achieve comparable performances (even better on VGG-16)
compared to the SOTA method with 11.3x and 15.5x acceleration. Moreover, we
investigate underlying mechanism of our method and find that it efficiently
selects potential structures, learns the consistent feature representations and
reduces the overfitting during the recovery phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16143">Ransomware Detection and Classification using Machine Learning. (arXiv:2311.16143v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kunku_K/0/1/0/all/0/1">Kavitha Kunku</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaman_A/0/1/0/all/0/1">ANK Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Vicious assaults, malware, and various ransomware pose a cybersecurity
threat, causing considerable damage to computer structures, servers, and mobile
and web apps across various industries and businesses. These safety concerns
are important and must be addressed immediately. Ransomware detection and
classification are critical for guaranteeing rapid reaction and prevention.
This study uses the XGBoost classifier and Random Forest (RF) algorithms to
detect and classify ransomware attacks. This approach involves analyzing the
behaviour of ransomware and extracting relevant features that can help
distinguish between different ransomware families. The models are evaluated on
a dataset of ransomware attacks and demonstrate their effectiveness in
accurately detecting and classifying ransomware. The results show that the
XGBoost classifier, Random Forest Classifiers, can effectively detect and
classify different ransomware attacks with high accuracy, thereby providing a
valuable tool for enhancing cybersecurity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16145">Dual-Stream Attention Transformers for Sewer Defect Classification. (arXiv:2311.16145v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Newaz_A/0/1/0/all/0/1">Abdullah Al Redwan Newaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdeldguerfi_M/0/1/0/all/0/1">Mahdi Abdeldguerfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Niles_K/0/1/0/all/0/1">Kendall N. Niles</a>, <a href="http://arxiv.org/find/cs/1/au:+Tom_J/0/1/0/all/0/1">Joe Tom</a></p>
<p>We propose a dual-stream multi-scale vision transformer (DS-MSHViT)
architecture that processes RGB and optical flow inputs for efficient sewer
defect classification. Unlike existing methods that combine the predictions of
two separate networks trained on each modality, we jointly train a single
network with two branches for RGB and motion. Our key idea is to use
self-attention regularization to harness the complementary strengths of the RGB
and motion streams. The motion stream alone struggles to generate accurate
attention maps, as motion images lack the rich visual features present in RGB
images. To facilitate this, we introduce an attention consistency loss between
the dual streams. By leveraging motion cues through a self-attention
regularizer, we align and enhance RGB attention maps, enabling the network to
concentrate on pertinent input regions. We evaluate our data on a public
dataset as well as cross-validate our model performance in a novel dataset. Our
method outperforms existing models that utilize either convolutional neural
networks (CNNs) or multi-scale hybrid vision transformers (MSHViTs) without
employing attention regularization between the two streams.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16148">Univariate Radial Basis Function Layers: Brain-inspired Deep Neural Layers for Low-Dimensional Inputs. (arXiv:2311.16148v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patil_B/0/1/0/all/0/1">Basavasagar Patil</a>, <a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1">Xavier Alameda-Pineda</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinke_C/0/1/0/all/0/1">Chris Reinke</a></p>
<p>Deep Neural Networks (DNNs) became the standard tool for function
approximation with most of the introduced architectures being developed for
high-dimensional input data. However, many real-world problems have
low-dimensional inputs for which standard Multi-Layer Perceptrons (MLPs) are
the default choice. An investigation into specialized architectures is missing.
We propose a novel DNN layer called Univariate Radial Basis Function (U-RBF)
layer as an alternative. Similar to sensory neurons in the brain, the U-RBF
layer processes each individual input dimension with a population of neurons
whose activations depend on different preferred input values. We verify its
effectiveness compared to MLPs in low-dimensional function regressions and
reinforcement learning tasks. The results show that the U-RBF is especially
advantageous when the target function becomes complex and difficult to
approximate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16151">Estimating Post-Synaptic Effects for Online Training of Feed-Forward SNNs. (arXiv:2311.16151v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Summe_T/0/1/0/all/0/1">Thomas Summe</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1">Clemens JS Schaefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Siddharth Joshi</a></p>
<p>Facilitating online learning in spiking neural networks (SNNs) is a key step
in developing event-based models that can adapt to changing environments and
learn from continuous data streams in real-time. Although forward-mode
differentiation enables online learning, its computational requirements
restrict scalability. This is typically addressed through approximations that
limit learning in deep models. In this study, we propose Online Training with
Postsynaptic Estimates (OTPE) for training feed-forward SNNs, which
approximates Real-Time Recurrent Learning (RTRL) by incorporating temporal
dynamics not captured by current approximations, such as Online Training
Through Time (OTTT) and Online Spatio-Temporal Learning (OSTL). We show
improved scaling for multi-layer networks using a novel approximation of
temporal effects on the subsequent layer's activity. This approximation incurs
minimal overhead in the time and space complexity compared to similar
algorithms, and the calculation of temporal effects remains local to each
layer. We characterize the learning performance of our proposed algorithms on
multiple SNN model configurations for rate-based and time-based encoding. OTPE
exhibits the highest directional alignment to exact gradients, calculated with
backpropagation through time (BPTT), in deep networks and, on time-based
encoding, outperforms other approximate methods. We also observe sizeable gains
in average performance over similar algorithms in offline training of Spiking
Heidelberg Digits with equivalent hyper-parameters (OTTT/OSTL - 70.5%; OTPE -
75.2%; BPTT - 78.1%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16155">Deep Learning-Based Frequency Offset Estimation. (arXiv:2311.16155v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1">Shilian Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1">Jiawei Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xuan_Q/0/1/0/all/0/1">Qi Xuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1">Xiaoniu Yang</a></p>
<p>In wireless communication systems, the asynchronization of the oscillators in
the transmitter and the receiver along with the Doppler shift due to relative
movement may lead to the presence of carrier frequency offset (CFO) in the
received signals. Estimation of CFO is crucial for subsequent processing such
as coherent demodulation. In this brief, we demonstrate the utilization of deep
learning for CFO estimation by employing a residual network (ResNet) to learn
and extract signal features from the raw in-phase (I) and quadrature (Q)
components of the signals. We use multiple modulation schemes in the training
set to make the trained model adaptable to multiple modulations or even new
signals. In comparison to the commonly used traditional CFO estimation methods,
our proposed IQ-ResNet method exhibits superior performance across various
scenarios including different oversampling ratios, various signal lengths, and
different channels
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16157">GeoTop: Advancing Image Classification with Geometric-Topological Analysis. (arXiv:2311.16157v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abaach_M/0/1/0/all/0/1">Mariem Abaach</a>, <a href="http://arxiv.org/find/cs/1/au:+Morilla_I/0/1/0/all/0/1">Ian Morilla</a></p>
<p>In this study, we explore the application of Topological Data Analysis (TDA)
and Lipschitz-Killing Curvatures (LKCs) as powerful tools for feature
extraction and classification in the context of biomedical multiomics problems.
TDA allows us to capture topological features and patterns within complex
datasets, while LKCs provide essential geometric insights. We investigate the
potential of combining both methods to improve classification accuracy. Using a
dataset of biomedical images, we demonstrate that TDA and LKCs can effectively
extract topological and geometrical features, respectively. The combination of
these features results in enhanced classification performance compared to using
each method individually. This approach offers promising results and has the
potential to advance our understanding of complex biological processes in
various biomedical applications. Our findings highlight the value of
integrating topological and geometrical information in biomedical data
analysis. As we continue to delve into the intricacies of multiomics problems,
the fusion of these insights holds great promise for unraveling the underlying
biological complexities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16158">CarbNN: A Novel Active Transfer Learning Neural Network To Build De Novo Metal Organic Frameworks (MOFs) for Carbon Capture. (arXiv:2311.16158v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Redkar_N/0/1/0/all/0/1">Neel Redkar</a></p>
<p>Over the past decade, climate change has become an increasing problem with
one of the major contributing factors being carbon dioxide (CO2) emissions;
almost 51% of total US carbon emissions are from factories. Current materials
used in CO2 capture are lacking either in efficiency, sustainability, or cost.
</p>
<p>Electrocatalysis of CO2 is a new approach where CO2 can be reduced and the
components used industrially as fuel, saving transportation costs, creating
financial incentives. Metal Organic Frameworks (MOFs) are crystals made of
organo-metals that adsorb, filter, and electrocatalyze CO2. The current
available MOFs for capture &amp; electrocatalysis are expensive to manufacture and
inefficient at capture. The goal therefore is to computationally design a MOF
that can adsorb CO2 and catalyze carbon monoxide &amp; oxygen with low cost.
</p>
<p>A novel active transfer learning neural network was developed, utilizing
transfer learning due to limited available data on 15 MOFs. Using the Cambridge
Structural Database with 10,000 MOFs, the model used incremental mutations to
fit a trained fitness hyper-heuristic function. Eventually, a Selenium MOF
(C18MgO25Se11Sn20Zn5) was converged on. Through analysis of predictions &amp;
literature, the converged MOF was shown to be more effective &amp; more
synthetically accessible than existing MOFs, showing the model had an
understanding of effective electrocatalytic structures in the material space.
This novel network can be implemented for other gas separations and catalysis
applications that have limited training accessible datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16160">Protein-ligand binding representation learning from fine-grained interactions. (arXiv:2311.16160v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Feng_S/0/1/0/all/0/1">Shikun Feng</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_M/0/1/0/all/0/1">Minghao Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jia_Y/0/1/0/all/0/1">Yinjun Jia</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ma_W/0/1/0/all/0/1">Weiying Ma</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lan_Y/0/1/0/all/0/1">Yanyan Lan</a></p>
<p>The binding between proteins and ligands plays a crucial role in the realm of
drug discovery. Previous deep learning approaches have shown promising results
over traditional computationally intensive methods, but resulting in poor
generalization due to limited supervised data. In this paper, we propose to
learn protein-ligand binding representation in a self-supervised learning
manner. Different from existing pre-training approaches which treat proteins
and ligands individually, we emphasize to discern the intricate binding
patterns from fine-grained interactions. Specifically, this self-supervised
learning problem is formulated as a prediction of the conclusive binding
complex structure given a pocket and ligand with a Transformer based
interaction module, which naturally emulates the binding process. To ensure the
representation of rich binding information, we introduce two pre-training
tasks, i.e.~atomic pairwise distance map prediction and mask ligand
reconstruction, which comprehensively model the fine-grained interactions from
both structure and feature space. Extensive experiments have demonstrated the
superiority of our method across various binding tasks, including
protein-ligand affinity prediction, virtual screening and protein-ligand
docking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16167">MMPDE-Net and Moving Sampling Physics-informed Neural Networks Based On Moving Mesh Method. (arXiv:2311.16167v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1">Yu Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_Q/0/1/0/all/0/1">Qihong Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Deng_Y/0/1/0/all/0/1">Yangtao Deng</a>, <a href="http://arxiv.org/find/math/1/au:+He_Q/0/1/0/all/0/1">Qiaolin He</a></p>
<p>In this work, we propose an end-to-end adaptive sampling neural network
(MMPDE-Net) based on the moving mesh PDE method, which can adaptively generate
new coordinates of sampling points by solving the moving mesh PDE. This model
focuses on improving the efficiency of individual sampling points. Moreover, we
have developed an iterative algorithm based on MMPDE-Net, which makes the
sampling points more precise and controllable. Since MMPDE-Net is a framework
independent of the deep learning solver, we combine it with PINN to propose
MS-PINN and demonstrate its effectiveness by performing error analysis under
the assumptions given in this paper. Meanwhile, we demonstrate the performance
improvement of MS-PINN compared to PINN through numerical experiments on four
typical examples to verify the effectiveness of our method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16168">Inexpensive High Fidelity Melt Pool Models in Additive Manufacturing Using Generative Deep Diffusion. (arXiv:2311.16168v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ogoke_F/0/1/0/all/0/1">Francis Ogoke</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Quanliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajenifujah_O/0/1/0/all/0/1">Olabode Ajenifujah</a>, <a href="http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1">Alexander Myers</a>, <a href="http://arxiv.org/find/cs/1/au:+Quirarte_G/0/1/0/all/0/1">Guadalupe Quirarte</a>, <a href="http://arxiv.org/find/cs/1/au:+Beuth_J/0/1/0/all/0/1">Jack Beuth</a>, <a href="http://arxiv.org/find/cs/1/au:+Malen_J/0/1/0/all/0/1">Jonathan Malen</a>, <a href="http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1">Amir Barati Farimani</a></p>
<p>Defects in laser powder bed fusion (L-PBF) parts often result from the
meso-scale dynamics of the molten alloy near the laser, known as the melt pool.
For instance, the melt pool can directly contribute to the formation of
undesirable porosity, residual stress, and surface roughness in the final part.
Experimental in-situ monitoring of the three-dimensional melt pool physical
fields is challenging, due to the short length and time scales involved in the
process. Multi-physics simulation methods can describe the three-dimensional
dynamics of the melt pool, but are computationally expensive at the mesh
refinement required for accurate predictions of complex effects, such as the
formation of keyhole porosity. Therefore, in this work, we develop a generative
deep learning model based on the probabilistic diffusion framework to map
low-fidelity, coarse-grained simulation information to the high-fidelity
counterpart. By doing so, we bypass the computational expense of conducting
multiple high-fidelity simulations for analysis by instead upscaling
lightweight coarse mesh simulations. Specifically, we implement a 2-D diffusion
model to spatially upscale cross-sections of the coarsely simulated melt pool
to their high-fidelity equivalent. We demonstrate the preservation of key
metrics of the melting process between the ground truth simulation data and the
diffusion model output, such as the temperature field, the melt pool dimensions
and the variability of the keyhole vapor cavity. Specifically, we predict the
melt pool depth within 3 $\mu m$ based on low-fidelity input data 4$\times$
coarser than the high-fidelity simulations, reducing analysis time by two
orders of magnitude.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16171">Multi-Agent Learning of Efficient Fulfilment and Routing Strategies in E-Commerce. (arXiv:2311.16171v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shelke_O/0/1/0/all/0/1">Omkar Shelke</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathakota_P/0/1/0/all/0/1">Pranavi Pathakota</a>, <a href="http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1">Anandsingh Chauhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1">Harshad Khadilkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Meisheri_H/0/1/0/all/0/1">Hardik Meisheri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1">Balaraman Ravindran</a></p>
<p>This paper presents an integrated algorithmic framework for minimising
product delivery costs in e-commerce (known as the cost-to-serve or C2S). One
of the major challenges in e-commerce is the large volume of spatio-temporally
diverse orders from multiple customers, each of which has to be fulfilled from
one of several warehouses using a fleet of vehicles. This results in two levels
of decision-making: (i) selection of a fulfillment node for each order
(including the option of deferral to a future time), and then (ii) routing of
vehicles (each of which can carry multiple orders originating from the same
warehouse). We propose an approach that combines graph neural networks and
reinforcement learning to train the node selection and vehicle routing agents.
We include real-world constraints such as warehouse inventory capacity, vehicle
characteristics such as travel times, service times, carrying capacity, and
customer constraints including time windows for delivery. The complexity of
this problem arises from the fact that outcomes (rewards) are driven both by
the fulfillment node mapping as well as the routing algorithms, and are
spatio-temporally distributed. Our experiments show that this algorithmic
pipeline outperforms pure heuristic policies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16172">Evolutionary Machine Learning and Games. (arXiv:2311.16172v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1">Julian Togelius</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1">Ahmed Khalifa</a>, <a href="http://arxiv.org/find/cs/1/au:+Earle_S/0/1/0/all/0/1">Sam Earle</a>, <a href="http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1">Michael Cerny Green</a>, <a href="http://arxiv.org/find/cs/1/au:+Soros_L/0/1/0/all/0/1">Lisa Soros</a></p>
<p>Evolutionary machine learning (EML) has been applied to games in multiple
ways, and for multiple different purposes. Importantly, AI research in games is
not only about playing games; it is also about generating game content,
modeling players, and many other applications. Many of these applications pose
interesting problems for EML. We will structure this chapter on EML for games
based on whether evolution is used to augment machine learning (ML) or ML is
used to augment evolution. For completeness, we also briefly discuss the usage
of ML and evolution separately in games.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16173">Conditions for Length Generalization in Learning Reasoning Skills. (arXiv:2311.16173v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Changnan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bing Liu</a></p>
<p>Reasoning is a fundamental capability of AI agents. Recently, large language
models (LLMs) have shown remarkable abilities to perform reasoning tasks.
However, numerous evaluations of the reasoning capabilities of LLMs have also
showed some limitations. An outstanding limitation is length generalization,
meaning that when trained on reasoning problems of smaller lengths or sizes,
the resulting models struggle with problems of larger sizes or lengths. This
potentially indicates some theoretical limitations of generalization in
learning reasoning skills. These evaluations and their observations motivated
us to perform a theoretical study of the length generalization problem. This
work focused on reasoning tasks that can be formulated as Markov dynamic
processes (MDPs) and/or directed acyclic graphs (DAGs). It identifies and
proves conditions that decide whether the length generalization problem can be
solved or not for a reasoning task in a particular representation. Experiments
are also conducted to verify the theoretical results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16176">Shortcut Bias Mitigation via Ensemble Diversity Using Diffusion Probabilistic Models. (arXiv:2311.16176v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1">Luca Scimeca</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1">Alexander Rubinstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1">Damien Teney</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seong Joon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1">Armand Mihai Nicolicioiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>Spurious correlations in the data, where multiple cues are predictive of the
target labels, often lead to a phenomenon known as simplicity bias, where a
model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In
this work, we propose an ensemble diversification framework exploiting
Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show
that at particular training intervals, DPMs can generate images with novel
feature combinations, even when trained on images displaying correlated input
features. We leverage this crucial property to generate synthetic
counterfactuals to increase model diversity via ensemble disagreement. We show
that DPM-guided diversification is sufficient to remove dependence on primary
shortcut cues, without a need for additional supervised signals. We further
empirically quantify its efficacy on several diversification objectives, and
finally show improved generalization and diversification performance on par
with prior work that relies on auxiliary data collection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16180">Aiming to Minimize Alcohol-Impaired Road Fatalities: Utilizing Fairness-Aware and Domain Knowledge-Infused Artificial Intelligence. (arXiv:2311.16180v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkateswaran_T/0/1/0/all/0/1">Tejas Venkateswaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1">Sheikh Rabiul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Md Golam Moula Mehedi Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1">Mohiuddin Ahmed</a></p>
<p>Approximately 30% of all traffic fatalities in the United States are
attributed to alcohol-impaired driving. This means that, despite stringent laws
against this offense in every state, the frequency of drunk driving accidents
is alarming, resulting in approximately one person being killed every 45
minutes. The process of charging individuals with Driving Under the Influence
(DUI) is intricate and can sometimes be subjective, involving multiple stages
such as observing the vehicle in motion, interacting with the driver, and
conducting Standardized Field Sobriety Tests (SFSTs). Biases have been observed
through racial profiling, leading to some groups and geographical areas facing
fewer DUI tests, resulting in many actual DUI incidents going undetected,
ultimately leading to a higher number of fatalities. To tackle this issue, our
research introduces an Artificial Intelligence-based predictor that is both
fairness-aware and incorporates domain knowledge to analyze DUI-related
fatalities in different geographic locations. Through this model, we gain
intriguing insights into the interplay between various demographic groups,
including age, race, and income. By utilizing the provided information to
allocate policing resources in a more equitable and efficient manner, there is
potential to reduce DUI-related fatalities and have a significant impact on
road safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16181">mvlearnR and Shiny App for multiview learning. (arXiv:2311.16181v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Palzer_E/0/1/0/all/0/1">Elise F. Palzer</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Safo_S/0/1/0/all/0/1">Sandra E. Safo</a></p>
<p>The package mvlearnR and accompanying Shiny App is intended for integrating
data from multiple sources or views or modalities (e.g. genomics, proteomics,
clinical and demographic data). Most existing software packages for multiview
learning are decentralized and offer limited capabilities, making it difficult
for users to perform comprehensive integrative analysis. The new package wraps
statistical and machine learning methods and graphical tools, providing a
convenient and easy data integration workflow. For users with limited
programming language, we provide a Shiny Application to facilitate data
integration anywhere and on any device. The methods have potential to offer
deeper insights into complex disease mechanisms.
</p>
<p>Availability and Implementation: mvlearnR is available from the following
GitHub repository: https://github.com/lasandrall/mvlearnR. The web application
is hosted on shinyapps.io and available at:
https://multi-viewlearn.shinyapps.io/MultiView_Modeling/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16185">Enhancing Sentiment Analysis Results through Outlier Detection Optimization. (arXiv:2311.16185v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuetian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_M/0/1/0/all/0/1">Mei Si</a></p>
<p>When dealing with text data containing subjective labels like speaker
emotions, inaccuracies or discrepancies among labelers are not uncommon. Such
discrepancies can significantly affect the performance of machine learning
algorithms. This study investigates the potential of identifying and addressing
outliers in text data with subjective labels, aiming to enhance classification
outcomes. We utilized the Deep SVDD algorithm, a one-class classification
method, to detect outliers in nine text-based emotion and sentiment analysis
datasets. By employing both a small-sized language model (DistilBERT base model
with 66 million parameters) and non-deep learning machine learning algorithms
(decision tree, KNN, Logistic Regression, and LDA) as the classifier, our
findings suggest that the removal of outliers can lead to enhanced results in
most cases. Additionally, as outliers in such datasets are not necessarily
unlearnable, we experienced utilizing a large language model -- DeBERTa v3
large with 131 million parameters, which can capture very complex patterns in
data. We continued to observe performance enhancements across multiple
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16187">Modelling wildland fire burn severity in California using a spatial Super Learner approach. (arXiv:2311.16187v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simafranca_N/0/1/0/all/0/1">Nicholas Simafranca</a>, <a href="http://arxiv.org/find/cs/1/au:+Willoughby_B/0/1/0/all/0/1">Bryant Willoughby</a>, <a href="http://arxiv.org/find/cs/1/au:+ONeil_E/0/1/0/all/0/1">Erin O&#x27;Neil</a>, <a href="http://arxiv.org/find/cs/1/au:+Farr_S/0/1/0/all/0/1">Sophie Farr</a>, <a href="http://arxiv.org/find/cs/1/au:+Reich_B/0/1/0/all/0/1">Brian J Reich</a>, <a href="http://arxiv.org/find/cs/1/au:+Giertych_N/0/1/0/all/0/1">Naomi Giertych</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1">Margaret Johnson</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascolini_Campbell_M/0/1/0/all/0/1">Madeleine Pascolini-Campbell</a></p>
<p>Given the increasing prevalence of wildland fires in the Western US, there is
a critical need to develop tools to understand and accurately predict burn
severity. We develop a machine learning model to predict post-fire burn
severity using pre-fire remotely sensed data. Hydrological, ecological, and
topographical variables collected from four regions of California - the sites
of the Kincade fire (2019), the CZU Lightning Complex fire (2020), the Windy
fire (2021), and the KNP Fire (2021) - are used as predictors of the difference
normalized burn ratio. We hypothesize that a Super Learner (SL) algorithm that
accounts for spatial autocorrelation using Vecchia's Gaussian approximation
will accurately model burn severity. In all combinations of test and training
sets explored, the results of our model showed the SL algorithm outperformed
standard Linear Regression methods. After fitting and verifying the performance
of the SL model, we use interpretable machine learning tools to determine the
main drivers of severe burn damage, including greenness, elevation and fire
weather variables. These findings provide actionable insights that enable
communities to strategize interventions, such as early fire detection systems,
pre-fire season vegetation clearing activities, and resource allocation during
emergency responses. When implemented, this model has the potential to minimize
the loss of human life, property, resources, and ecosystems in California.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16191">MACE: A Multi-pattern Accommodated and Efficient Anomaly Detection Method in the Frequency Domain. (arXiv:2311.16191v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Feiyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+zhang_Y/0/1/0/all/0/1">Yingying zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lunting Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1">Renhe Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yuxuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1">Qingsong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shuiguang Deng</a></p>
<p>Anomaly detection significantly enhances the robustness of cloud systems.
While neural network-based methods have recently demonstrated strong
advantages, they encounter practical challenges in cloud environments: the
contradiction between the impracticality of maintaining a unique model for each
service and the limited ability of dealing with diverse normal patterns by a
unified model, as well as issues with handling heavy traffic in real time and
short-term anomaly detection sensitivity. Thus, we propose MACE, a
Multi-pattern Accommodated and efficient Anomaly detection method in the
frequency domain for time series anomaly detection. There are three novel
characteristics of it: (i) a pattern extraction mechanism excelling at handling
diverse normal patterns, which enables the model to identify anomalies by
examining the correlation between the data sample and its service normal
pattern, instead of solely focusing on the data sample itself; (ii) a dualistic
convolution mechanism that amplifies short-term anomalies in the time domain
and hinders the reconstruction of anomalies in the frequency domain, which
enlarges the reconstruction error disparity between anomaly and normality and
facilitates anomaly detection; (iii) leveraging the sparsity and parallelism of
frequency domain to enhance model efficiency. We theoretically and
experimentally prove that using a strategically selected subset of Fourier
bases can not only reduce computational overhead but is also profit to
distinguish anomalies, compared to using the complete spectrum. Moreover,
extensive experiments demonstrate MACE's effectiveness in handling diverse
normal patterns with a unified model and it achieves state-of-the-art
performance with high efficiency. \end{abstract}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16192">Utilizing Multiple Inputs Autoregressive Models for Bearing Remaining Useful Life Prediction. (arXiv:2311.16192v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junliang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qinghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guanhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Guoxi Sun</a></p>
<p>Accurate prediction of the Remaining Useful Life (RUL) of rolling bearings is
crucial in industrial production, yet existing models often struggle with
limited generalization capabilities due to their inability to fully process all
vibration signal patterns. We introduce a novel multi-input autoregressive
model to address this challenge in RUL prediction for bearings. Our approach
uniquely integrates vibration signals with previously predicted Health
Indicator (HI) values, employing feature fusion to output current window HI
values. Through autoregressive iterations, the model attains a global receptive
field, effectively overcoming the limitations in generalization. Furthermore,
we innovatively incorporate a segmentation method and multiple training
iterations to mitigate error accumulation in autoregressive models. Empirical
evaluation on the PMH2012 dataset demonstrates that our model, compared to
other backbone networks using similar autoregressive approaches, achieves
significantly lower Root Mean Square Error (RMSE) and Score. Notably, it
outperforms traditional autoregressive models that use label values as inputs
and non-autoregressive networks, showing superior generalization abilities with
a marked lead in RMSE and Score metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16195">A Foundational Framework and Methodology for Personalized Early and Timely Diagnosis. (arXiv:2311.16195v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schubert_T/0/1/0/all/0/1">Tim Schubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Peck_R/0/1/0/all/0/1">Richard W Peck</a>, <a href="http://arxiv.org/find/cs/1/au:+Gimson_A/0/1/0/all/0/1">Alexander Gimson</a>, <a href="http://arxiv.org/find/cs/1/au:+Davtyan_C/0/1/0/all/0/1">Camelia Davtyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a></p>
<p>Early diagnosis of diseases holds the potential for deep transformation in
healthcare by enabling better treatment options, improving long-term survival
and quality of life, and reducing overall cost. With the advent of medical big
data, advances in diagnostic tests as well as in machine learning and
statistics, early or timely diagnosis seems within reach. Early diagnosis
research often neglects the potential for optimizing individual diagnostic
paths. To enable personalized early diagnosis, a foundational framework is
needed that delineates the diagnosis process and systematically identifies the
time-dependent value of various diagnostic tests for an individual patient
given their unique characteristics. Here, we propose the first foundational
framework for early and timely diagnosis. It builds on decision-theoretic
approaches to outline the diagnosis process and integrates machine learning and
statistical methodology for estimating the optimal personalized diagnostic
path. To describe the proposed framework as well as possibly other frameworks,
we provide essential definitions.
</p>
<p>The development of a foundational framework is necessary for several reasons:
1) formalism provides clarity for the development of decision support tools; 2)
observed information can be complemented with estimates of the future patient
trajectory; 3) the net benefit of counterfactual diagnostic paths and
associated uncertainties can be modeled for individuals 4) 'early' and 'timely'
diagnosis can be clearly defined; 5) a mechanism emerges for assessing the
value of technologies in terms of their impact on personalized early diagnosis,
resulting health outcomes and incurred costs.
</p>
<p>Finally, we hope that this foundational framework will unlock the
long-awaited potential of timely diagnosis and intervention, leading to
improved outcomes for patients and higher cost-effectiveness for healthcare
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16197">Generation of patient specific cardiac chamber models using generative neural networks under a Bayesian framework for electroanatomical mapping. (arXiv:2311.16197v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1">Sunil Mathew</a>, <a href="http://arxiv.org/find/eess/1/au:+Sra_J/0/1/0/all/0/1">Jasbir Sra</a>, <a href="http://arxiv.org/find/eess/1/au:+Rowe_D/0/1/0/all/0/1">Daniel B. Rowe</a></p>
<p>Electroanatomical mapping is a technique used in cardiology to create a
detailed 3D map of the electrical activity in the heart. It is useful for
diagnosis, treatment planning and real time guidance in cardiac ablation
procedures to treat arrhythmias like atrial fibrillation. A probabilistic
machine learning model trained on a library of CT/MRI scans of the heart can be
used during electroanatomical mapping to generate a patient-specific 3D model
of the chamber being mapped. The use of probabilistic machine learning models
under a Bayesian framework provides a way to quantify uncertainty in results
and provide a natural framework of interpretability of the model. Here we
introduce a Bayesian approach to surface reconstruction of cardiac chamber
models from a sparse 3D point cloud data acquired during electroanatomical
mapping. We show how probabilistic graphical models trained on segmented CT/MRI
data can be used to generate cardiac chamber models from few acquired locations
thereby reducing procedure time and x-ray exposure. We show how they provide
insight into what the neural network learns from the segmented CT/MRI images
used to train the network, which provides explainability to the resulting
cardiac chamber models generated by the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16198">Ultra-short-term multi-step wind speed prediction for wind farms based on adaptive noise reduction technology and temporal convolutional network. (arXiv:2311.16198v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haojian Huang</a></p>
<p>As an important clean and renewable kind of energy, wind power plays an
important role in coping with energy crisis and environmental pollution.
However, the volatility and intermittency of wind speed restrict the
development of wind power. To improve the utilization of wind power, this study
proposes a new wind speed prediction model based on data noise reduction
technology, temporal convolutional network (TCN), and gated recurrent unit
(GRU). Firstly, an adaptive data noise reduction algorithm P-SSA is proposed
based on singular spectrum analysis (SSA) and Pearson correlation coefficient.
The original wind speed is decomposed into multiple subsequences by SSA and
then reconstructed. When the Pearson correlation coefficient between the
reconstructed sequence and the original sequence is greater than 0.99, other
noise subsequences are deleted to complete the data denoising. Then, the
receptive field of the samples is expanded through the causal convolution and
dilated convolution of TCN, and the characteristics of wind speed change are
extracted. Then, the time feature information of the sequence is extracted by
GRU, and then the wind speed is predicted to form the wind speed sequence
prediction model of P-SSA-TCN-GRU. The proposed model was validated on three
wind farms in Shandong Province. The experimental results show that the
prediction performance of the proposed model is better than that of the
traditional model and other models based on TCN, and the wind speed prediction
of wind farms with high precision and strong stability is realized. The wind
speed predictions of this model have the potential to become the data that
support the operation and management of wind farms. The code is available at
link.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16199">Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation. (arXiv:2311.16199v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Daigavane_A/0/1/0/all/0/1">Ameya Daigavane</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Song Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1">Mario Geiger</a>, <a href="http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1">Tess Smidt</a></p>
<p>We present Symphony, an $E(3)$-equivariant autoregressive generative model
for 3D molecular geometries that iteratively builds a molecule from molecular
fragments. Existing autoregressive models such as G-SchNet and G-SphereNet for
molecules utilize rotationally invariant features to respect the 3D symmetries
of molecules. In contrast, Symphony uses message-passing with higher-degree
$E(3)$-equivariant features. This allows a novel representation of probability
distributions via spherical harmonic signals to efficiently model the 3D
geometry of molecules. We show that Symphony is able to accurately generate
small molecules from the QM9 dataset, outperforming existing autoregressive
models and approaching the performance of diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16200">Streaming Lossless Volumetric Compression of Medical Images Using Gated Recurrent Convolutional Neural Network. (arXiv:2311.16200v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1">Qianhao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jietao Chen</a></p>
<p>Deep learning-based lossless compression methods offer substantial advantages
in compressing medical volumetric images. Nevertheless, many learning-based
algorithms encounter a trade-off between practicality and compression
performance. This paper introduces a hardware-friendly streaming lossless
volumetric compression framework, utilizing merely one-thousandth of the model
weights compared to other learning-based compression frameworks. We propose a
gated recurrent convolutional neural network that combines diverse
convolutional structures and fusion gate mechanisms to capture the inter-slice
dependencies in volumetric images. Based on such contextual information, we can
predict the pixel-by-pixel distribution for entropy coding. Guided by
hardware/software co-design principles, we implement the proposed framework on
Field Programmable Gate Array to achieve enhanced real-time performance.
Extensive experimental results indicate that our method outperforms traditional
lossless volumetric compressors and state-of-the-art learning-based lossless
compression methods across various medical image benchmarks. Additionally, our
method exhibits robust generalization ability and competitive compression speed
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16201">Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation. (arXiv:2311.16201v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuhui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+McKinzie_B/0/1/0/all/0/1">Brandon McKinzie</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1">Zhe Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1">Vaishaal Shankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1">Alexander Toshev</a></p>
<p>Recent advances in image tokenizers, such as VQ-VAE, have enabled
text-to-image generation using auto-regressive methods, similar to language
modeling. However, these methods have yet to leverage pre-trained language
models, despite their adaptability to various downstream tasks. In this work,
we explore this gap by adapting a pre-trained language model for
auto-regressive text-to-image generation, and find that pre-trained language
models offer limited help. We provide a two-fold explanation by analyzing
tokens from each modality. First, we demonstrate that image tokens possess
significantly different semantics compared to text tokens, rendering
pre-trained language models no more effective in modeling them than randomly
initialized ones. Second, the text tokens in the image-text datasets are too
simple compared to normal language model pre-training data, which causes the
catastrophic degradation of language models' capability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16203">ChatTraffc: Text-to-Traffic Generation via Diffusion Model. (arXiv:2311.16203v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1">Qitan Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1">Yisheng Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Piao_X/0/1/0/all/0/1">Xinglin Piao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Baocai Yin</a></p>
<p>Traffic prediction is one of the most significant foundations in Intelligent
Transportation Systems (ITS). Traditional traffic prediction methods rely only
on historical traffic data to predict traffic trends and face two main
challenges. 1) insensitivity to unusual events. 2) poor performance in
long-term prediction. In this work, we explore how generative models combined
with text describing the traffic system can be applied for traffic generation
and name the task Text-to-Traffic Generation (TTG). The key challenge of the
TTG task is how to associate text with the spatial structure of the road
network and traffic data for generating traffic situations. To this end, we
propose ChatTraffic, the first diffusion model for text-to-traffic generation.
To guarantee the consistency between synthetic and real data, we augment a
diffusion model with the Graph Convolutional Network (GCN) to extract spatial
correlations of traffic data. In addition, we construct a large dataset
containing text-traffic pairs for the TTG task. We benchmarked our model
qualitatively and quantitatively on the released dataset. The experimental
results indicate that ChatTraffic can generate realistic traffic situations
from the text. Our code and dataset are available at
https://github.com/ChyaZhang/ChatTraffic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16206">Continual Instruction Tuning for Large Multimodal Models. (arXiv:2311.16206v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jinghan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Haiyun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Ming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinqiao Wang</a></p>
<p>Instruction tuning is now a widely adopted approach to aligning large
multimodal models (LMMs) to follow human intent. It unifies the data format of
vision-language tasks, enabling multi-task joint training. However,
vision-language tasks are constantly being created in practice. Instead of
always re-training LMMs when new tasks arrive, continual learning offers
flexibility for models to continually and efficiently exploit the evolving
data. This work aims to explore the following two questions: 1) Do LMMs still
suffer from catastrophic forgetting in continual instruction tuning? 2) Are the
existing three classes of continual learning methods still applicable to the
continual instruction tuning of LMMs? An extensive study is conducted to
address the above questions. First, we establish the first benchmark in this
setting and reveal that catastrophic forgetting is still observed when
continually instruction-tuning LMMs. However, the multi-task joint instruction
tuning can facilitate the model's continual learning ability and mitigate
forgetting. Second, we integrate and adapt classic continual learning methods
to our context, demonstrating the efficacy of data replay and model expansion
strategies across diverse scenarios. In contrast, regularization-based methods
only perform well on models that have been jointly instruction-tuned on
multiple tasks. Third, we delve into the correlation and forgetting dynamics
between vision-language task pairs and propose task-similarity-informed
regularization and model expansion methods for continual instruction tuning of
LMMs. Experimental results show that our approach consistently boosts the
model's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16207">The Graph Convolutional Network with Multi-representation Alignment for Drug Synergy Prediction. (arXiv:2311.16207v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1">Xinxing Yang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yang_G/0/1/0/all/0/1">Genke Yang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chu_J/0/1/0/all/0/1">Jian Chu</a></p>
<p>Drug combination refers to the use of two or more drugs to treat a specific
disease at the same time. It is currently the mainstream way to treat complex
diseases. Compared with single drugs, drug combinations have better efficacy
and can better inhibit toxicity and drug resistance. The computational model
based on deep learning concatenates the representation of multiple drugs and
the corresponding cell line feature as input, and the output is whether the
drug combination can have an inhibitory effect on the cell line. However, this
strategy of concatenating multiple representations has the following defects:
the alignment of drug representation and cell line representation is ignored,
resulting in the synergistic relationship not being reflected positionally in
the embedding space. Moreover, the alignment measurement function in deep
learning cannot be suitable for drug synergy prediction tasks due to
differences in input types. Therefore, in this work, we propose a graph
convolutional network with multi-representation alignment (GCNMRA) for
predicting drug synergy. In the GCNMRA model, we designed a
multi-representation alignment function suitable for the drug synergy
prediction task so that the positional relationship between drug
representations and cell line representation is reflected in the embedding
space. In addition, the vector modulus of drug representations and cell line
representation is considered to improve the accuracy of calculation results and
accelerate model convergence. Finally, many relevant experiments were run on
multiple drug synergy datasets to verify the effectiveness of the above
innovative elements and the excellence of the GCNMRA model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16208">InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery. (arXiv:2311.16208v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Cao_H/0/1/0/all/0/1">He Cao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_Z/0/1/0/all/0/1">Zijing Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lu_X/0/1/0/all/0/1">Xingyu Lu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a></p>
<p>The rapid evolution of artificial intelligence in drug discovery encounters
challenges with generalization and extensive training, yet Large Language
Models (LLMs) offer promise in reshaping interactions with complex molecular
data. Our novel contribution, InstructMol, a multi-modal LLM, effectively
aligns molecular structures with natural language via an instruction-tuning
approach, utilizing a two-stage training strategy that adeptly combines limited
domain-specific data with molecular and textual information. InstructMol
showcases substantial performance improvements in drug discovery-related
molecular tasks, surpassing leading LLMs and significantly reducing the gap
with specialized models, thereby establishing a robust foundation for a
versatile and dependable drug discovery assistant.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16213">Seeing Beyond Cancer: Multi-Institutional Validation of Object Localization and 3D Semantic Segmentation using Deep Learning for Breast MRI. (arXiv:2311.16213v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Pekis_A/0/1/0/all/0/1">Arda Pekis</a>, <a href="http://arxiv.org/find/eess/1/au:+Kannan_V/0/1/0/all/0/1">Vignesh Kannan</a>, <a href="http://arxiv.org/find/eess/1/au:+Kaklamanos_E/0/1/0/all/0/1">Evandros Kaklamanos</a>, <a href="http://arxiv.org/find/eess/1/au:+Antony_A/0/1/0/all/0/1">Anu Antony</a>, <a href="http://arxiv.org/find/eess/1/au:+Patel_S/0/1/0/all/0/1">Snehal Patel</a>, <a href="http://arxiv.org/find/eess/1/au:+Earnest_T/0/1/0/all/0/1">Tyler Earnest</a></p>
<p>The clinical management of breast cancer depends on an accurate understanding
of the tumor and its anatomical context to adjacent tissues and landmark
structures. This context may be provided by semantic segmentation methods;
however, previous works have been largely limited to a singular focus on the
tumor alone and rarely other tissue types. In contrast, we present a method
that exploits tissue-tissue interactions to accurately segment every major
tissue type in the breast including: chest wall, skin, adipose tissue,
fibroglandular tissue, vasculature and tumor via standard-of-care Dynamic
Contrast Enhanced MRI. Comparing our method to prior state-of-the-art, we
achieved a superior Dice score on tumor segmentation while maintaining
competitive performance on other studied tissues across multiple institutions.
Briefly, our method proceeds by localizing the tumor using 2D object detectors,
then segmenting the tumor and surrounding tissues independently using two 3D
U-nets, and finally integrating these results while mitigating false positives
by checking for anatomically plausible tissue-tissue contacts. The object
detection models were pre-trained on ImageNet and COCO, and operated on MIP
(maximum intensity projection) images in the axial and sagittal planes,
establishing a 3D tumor bounding box. By integrating multiple relevant
peri-tumoral tissues, our work enables clinical applications in breast cancer
staging, prognosis and surgical planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16214">DGR: Tackling Drifted and Correlated Noise in Quantum Error Correction via Decoding Graph Re-weighting. (arXiv:2311.16214v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1">Hanrui Wang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_P/0/1/0/all/0/1">Pengyu Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_Y/0/1/0/all/0/1">Yilian Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1">Jiaqi Gu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Baker_J/0/1/0/all/0/1">Jonathan Baker</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1">Frederic T. Chong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1">Song Han</a></p>
<p>Quantum hardware suffers from high error rates and noise, which makes
directly running applications on them ineffective. Quantum Error Correction
(QEC) is a critical technique towards fault tolerance which encodes the quantum
information distributively in multiple data qubits and uses syndrome qubits to
check parity. Minimum-Weight-Perfect-Matching (MWPM) is a popular QEC decoder
that takes the syndromes as input and finds the matchings between syndromes
that infer the errors. However, there are two paramount challenges for MWPM
decoders. First, as noise in real quantum systems can drift over time, there is
a potential misalignment with the decoding graph's initial weights, leading to
a severe performance degradation in the logical error rates. Second, while the
MWPM decoder addresses independent errors, it falls short when encountering
correlated errors typical on real hardware, such as those in the 2Q
depolarizing channel.
</p>
<p>We propose DGR, an efficient decoding graph edge re-weighting strategy with
no quantum overhead. It leverages the insight that the statistics of matchings
across decoding iterations offer rich information about errors on real quantum
hardware. By counting the occurrences of edges and edge pairs in decoded
matchings, we can statistically estimate the up-to-date probabilities of each
edge and the correlations between them. The reweighting process includes two
vital steps: alignment re-weighting and correlation re-weighting. The former
updates the MWPM weights based on statistics to align with actual noise, and
the latter adjusts the weight considering edge correlations.
</p>
<p>Extensive evaluations on surface code and honeycomb code under various
settings show that DGR reduces the logical error rate by 3.6x on average-case
noise mismatch with exceeding 5000x improvement under worst-case mismatch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16277">A Graph Neural Network-Based QUBO-Formulated Hamiltonian-Inspired Loss Function for Combinatorial Optimization using Reinforcement Learning. (arXiv:2311.16277v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rizvee_R/0/1/0/all/0/1">Redwan Ahmed Rizvee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_R/0/1/0/all/0/1">Raheeb Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Md. Mosaddek Khan</a></p>
<p>Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to
model various NP-hard Combinatorial Optimization problems (CO) in the form of
binary variables. Ising Hamiltonian is used to model the energy function of a
system. QUBO to Ising Hamiltonian is regarded as a technique to solve various
canonical optimization problems through quantum optimization algorithms.
Recently, PI-GNN, a generic framework, has been proposed to address CO problems
over graphs based on Graph Neural Network (GNN) architecture. They introduced a
generic QUBO-formulated Hamiltonian-inspired loss function that was directly
optimized using GNN. PI-GNN is highly scalable but there lies a noticeable
decrease in the number of satisfied constraints when compared to
problem-specific algorithms and becomes more pronounced with increased graph
densities. Here, We identify a behavioral pattern related to it and devise
strategies to improve its performance. Another group of literature uses
Reinforcement learning (RL) to solve the aforementioned NP-hard problems using
problem-specific reward functions. In this work, we also focus on creating a
bridge between the RL-based solutions and the QUBO-formulated Hamiltonian. We
formulate and empirically evaluate the compatibility of the QUBO-formulated
Hamiltonian as the generic reward function in the RL-based paradigm in the form
of rewards. Furthermore, we also introduce a novel Monty Carlo Tree
Search-based strategy with GNN where we apply a guided search through manual
perturbation of node labels during training. We empirically evaluated our
methods and observed up to 44% improvement in the number of constraint
violations compared to the PI-GNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16286">A statistical approach to latent dynamic modeling with differential equations. (arXiv:2311.16286v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hackenberg_M/0/1/0/all/0/1">Maren Hackenberg</a>, <a href="http://arxiv.org/find/stat/1/au:+Pechmann_A/0/1/0/all/0/1">Astrid Pechmann</a>, <a href="http://arxiv.org/find/stat/1/au:+Kreutz_C/0/1/0/all/0/1">Clemens Kreutz</a>, <a href="http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1">Janbernd Kirschner</a>, <a href="http://arxiv.org/find/stat/1/au:+Binder_H/0/1/0/all/0/1">Harald Binder</a></p>
<p>Ordinary differential equations (ODEs) can provide mechanistic models of
temporally local changes of processes, where parameters are often informed by
external knowledge. While ODEs are popular in systems modeling, they are less
established for statistical modeling of longitudinal cohort data, e.g., in a
clinical setting. Yet, modeling of local changes could also be attractive for
assessing the trajectory of an individual in a cohort in the immediate future
given its current status, where ODE parameters could be informed by further
characteristics of the individual. However, several hurdles so far limit such
use of ODEs, as compared to regression-based function fitting approaches. The
potentially higher level of noise in cohort data might be detrimental to ODEs,
as the shape of the ODE solution heavily depends on the initial value. In
addition, larger numbers of variables multiply such problems and might be
difficult to handle for ODEs. To address this, we propose to use each
observation in the course of time as the initial value to obtain multiple local
ODE solutions and build a combined estimator of the underlying dynamics. Neural
networks are used for obtaining a low-dimensional latent space for dynamic
modeling from a potentially large number of variables, and for obtaining
patient-specific ODE parameters from baseline variables. Simultaneous
identification of dynamic models and of a latent space is enabled by recently
developed differentiable programming techniques. We illustrate the proposed
approach in an application with spinal muscular atrophy patients and a
corresponding simulation study. In particular, modeling of local changes in
health status at any point in time is contrasted to the interpretation of
functions obtained from a global regression. This more generally highlights how
different application settings might demand different modeling strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16297">Quantum-classical simulation of quantum field theory by quantum circuit learning. (arXiv:2311.16297v1 [hep-th])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-th/1/au:+Ikeda_K/0/1/0/all/0/1">Kazuki Ikeda</a></p>
<p>We employ quantum circuit learning to simulate quantum field theories (QFTs).
Typically, when simulating QFTs with quantum computers, we encounter
significant challenges due to the technical limitations of quantum devices when
implementing the Hamiltonian using Pauli spin matrices. To address this
challenge, we leverage quantum circuit learning, employing a compact
configuration of qubits and low-depth quantum circuits to predict real-time
dynamics in quantum field theories. The key advantage of this approach is that
a single-qubit measurement can accurately forecast various physical parameters,
including fully-connected operators. To demonstrate the effectiveness of our
method, we use it to predict quench dynamics, chiral dynamics and jet
production in a 1+1-dimensional model of quantum electrodynamics. We find that
our predictions closely align with the results of rigorous classical
calculations, exhibiting a high degree of accuracy. This hybrid
quantum-classical approach illustrates the feasibility of efficiently
simulating large-scale QFTs on cutting-edge quantum devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16298">Influence Scores at Scale for Efficient Language Data Sampling. (arXiv:2311.16298v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1">Nikhil Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Joshua Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Minakova_M/0/1/0/all/0/1">Maria Minakova</a></p>
<p>Modern ML systems ingest data aggregated from diverse sources, such as
synthetic, human-annotated, and live customer traffic. Understanding
\textit{which} examples are important to the performance of a learning
algorithm is crucial for efficient model training. Recently, a growing body of
literature has given rise to various "influence scores," which use training
artifacts such as model confidence or checkpointed gradients to identify
important subsets of data. However, these methods have primarily been developed
in computer vision settings, and it remains unclear how well they generalize to
language-based tasks using pretrained models.
</p>
<p>In this paper, we explore the applicability of influence scores in language
classification tasks. We evaluate a diverse subset of these scores on the SNLI
dataset by quantifying accuracy changes in response to pruning training data
through random and influence-score-based sampling. We then stress-test one of
the scores -- "variance of gradients" (VoG) from Agarwal et al. (2022) -- in an
NLU model stack that was exposed to dynamic user speech patterns in a voice
assistant type of setting. Our experiments demonstrate that in many cases,
encoder-based language models can be finetuned on roughly 50% of the original
data without degradation in performance metrics. Along the way, we summarize
lessons learned from applying out-of-the-box implementations of influence
scores, quantify the effects of noisy and class-imbalanced data, and offer
recommendations on score-based sampling for better accuracy and training
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16302">Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection. (arXiv:2311.16302v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sabbineni_A/0/1/0/all/0/1">Anusha Sabbineni</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1">Nikhil Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Minakova_M/0/1/0/all/0/1">Maria Minakova</a></p>
<p>While data selection methods have been studied extensively in active
learning, data pruning, and data augmentation settings, there is little
evidence for the efficacy of these methods in industry scale settings,
particularly in low-resource languages. Our work presents ways of assessing
prospective training examples in those settings for their "usefulness" or
"difficulty". We also demonstrate how these measures can be used in selecting
important examples for training supervised machine learning models. We
primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these
metrics to curate high quality datasets from a large pool of \textit{Weak
Signal Labeled} data, which assigns no-defect high confidence hypotheses during
inference as ground truth labels. We then conduct training data augmentation
experiments using these de-identified datasets and demonstrate that score-based
selection can result in a 2% decrease in semantic error rate and 4%-7% decrease
in domain classification error rate when compared to the baseline technique of
random selection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16312">Domain-Specific Deep Learning Feature Extractor for Diabetic Foot Ulcer Detection. (arXiv:2311.16312v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Basiri_R/0/1/0/all/0/1">Reza Basiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovic_M/0/1/0/all/0/1">Milos R. Popovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Shehroz S. Khan</a></p>
<p>Diabetic Foot Ulcer (DFU) is a condition requiring constant monitoring and
evaluations for treatment. DFU patient population is on the rise and will soon
outpace the available health resources. Autonomous monitoring and evaluation of
DFU wounds is a much-needed area in health care. In this paper, we evaluate and
identify the most accurate feature extractor that is the core basis for
developing a deep-learning wound detection network. For the evaluation, we used
mAP and F1-score on the publicly available DFU2020 dataset. A combination of
UNet and EfficientNetb3 feature extractor resulted in the best evaluation among
the 14 networks compared. UNet and Efficientnetb3 can be used as the classifier
in the development of a comprehensive DFU domain-specific autonomous wound
detection pipeline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16328">Target-Free Compound Activity Prediction via Few-Shot Learning. (arXiv:2311.16328v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eckmann_P/0/1/0/all/0/1">Peter Eckmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_J/0/1/0/all/0/1">Jake Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilson_M/0/1/0/all/0/1">Michael K. Gilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Rose Yu</a></p>
<p>Predicting the activities of compounds against protein-based or phenotypic
assays using only a few known compounds and their activities is a common task
in target-free drug discovery. Existing few-shot learning approaches are
limited to predicting binary labels (active/inactive). However, in real-world
drug discovery, degrees of compound activity are highly relevant. We study
Few-Shot Compound Activity Prediction (FS-CAP) and design a novel neural
architecture to meta-learn continuous compound activities across large
bioactivity datasets. Our model aggregates encodings generated from the known
compounds and their activities to capture assay information. We also introduce
a separate encoder for the unknown compound. We show that FS-CAP surpasses
traditional similarity-based techniques as well as other state of the art
few-shot learning methods on a variety of target-free drug discovery settings
and datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16333">From Reactive to Proactive Volatility Modeling with Hemisphere Neural Networks. (arXiv:2311.16333v1 [econ.EM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Coulombe_P/0/1/0/all/0/1">Philippe Goulet Coulombe</a>, <a href="http://arxiv.org/find/econ/1/au:+Frenette_M/0/1/0/all/0/1">Mikael Frenette</a>, <a href="http://arxiv.org/find/econ/1/au:+Klieber_K/0/1/0/all/0/1">Karin Klieber</a></p>
<p>We reinvigorate maximum likelihood estimation (MLE) for macroeconomic density
forecasting through a novel neural network architecture with dedicated mean and
variance hemispheres. Our architecture features several key ingredients making
MLE work in this context. First, the hemispheres share a common core at the
entrance of the network which accommodates for various forms of time variation
in the error variance. Second, we introduce a volatility emphasis constraint
that breaks mean/variance indeterminacy in this class of overparametrized
nonlinear models. Third, we conduct a blocked out-of-bag reality check to curb
overfitting in both conditional moments. Fourth, the algorithm utilizes
standard deep learning software and thus handles large data sets - both
computationally and statistically. Ergo, our Hemisphere Neural Network (HNN)
provides proactive volatility forecasts based on leading indicators when it
can, and reactive volatility based on the magnitude of previous prediction
errors when it must. We evaluate point and density forecasts with an extensive
out-of-sample experiment and benchmark against a suite of models ranging from
classics to more modern machine learning-based offerings. In all cases, HNN
fares well by consistently providing accurate mean/variance forecasts for all
targets and horizons. Studying the resulting volatility paths reveals its
versatility, while probabilistic forecasting evaluation metrics showcase its
enviable reliability. Finally, we also demonstrate how this machinery can be
merged with other structured deep learning models by revisiting Goulet Coulombe
(2022)'s Neural Phillips Curve.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16339">Reward Shaping for Improved Learning in Real-time Strategy Game Play. (arXiv:2311.16339v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kliem_J/0/1/0/all/0/1">John Kliem</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1">Prithviraj Dasgupta</a></p>
<p>We investigate the effect of reward shaping in improving the performance of
reinforcement learning in the context of the real-time strategy,
capture-the-flag game. The game is characterized by sparse rewards that are
associated with infrequently occurring events such as grabbing or capturing the
flag, or tagging the opposing player. We show that appropriately designed
reward shaping functions applied to different game events can significantly
improve the player's performance and training times of the player's learning
algorithm. We have validated our reward shaping functions within a simulated
environment for playing a marine capture-the-flag game between two players. Our
experimental results demonstrate that reward shaping can be used as an
effective means to understand the importance of different sub-tasks during
game-play towards winning the game, to encode a secondary objective functions
such as energy efficiency into a player's game-playing behavior, and, to
improve learning generalizable policies that can perform well against different
skill levels of the opponent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16346">Small and Dim Target Detection in IR Imagery: A Review. (arXiv:2311.16346v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1">Nikhil Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pravendra Singh</a></p>
<p>While there has been significant progress in object detection using
conventional image processing and machine learning algorithms, exploring small
and dim target detection in the IR domain is a relatively new area of study.
The majority of small and dim target detection methods are derived from
conventional object detection algorithms, albeit with some alterations. The
task of detecting small and dim targets in IR imagery is complex. This is
because these targets often need distinct features, the background is cluttered
with unclear details, and the IR signatures of the scene can change over time
due to fluctuations in thermodynamics. The primary objective of this review is
to highlight the progress made in this field. This is the first review in the
field of small and dim target detection in infrared imagery, encompassing
various methodologies ranging from conventional image processing to
cutting-edge deep learning-based approaches. The authors have also introduced a
taxonomy of such approaches. There are two main types of approaches:
methodologies using several frames for detection, and single-frame-based
detection techniques. Single frame-based detection techniques encompass a
diverse range of methods, spanning from traditional image processing-based
approaches to more advanced deep learning methodologies. Our findings indicate
that deep learning approaches perform better than traditional image
processing-based approaches. In addition, a comprehensive compilation of
various available datasets has also been provided. Furthermore, this review
identifies the gaps and limitations in existing techniques, paving the way for
future research and development in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16353">Improving Denoising Diffusion Probabilistic Models via Exploiting Shared Representations. (arXiv:2311.16353v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pirhayatifard_D/0/1/0/all/0/1">Delaram Pirhayatifard</a>, <a href="http://arxiv.org/find/cs/1/au:+Toghani_M/0/1/0/all/0/1">Mohammad Taha Toghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1">Guha Balakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Uribe_C/0/1/0/all/0/1">C&#xe9;sar A. Uribe</a></p>
<p>In this work, we address the challenge of multi-task image generation with
limited data for denoising diffusion probabilistic models (DDPM), a class of
generative models that produce high-quality images by reversing a noisy
diffusion process. We propose a novel method, SR-DDPM, that leverages
representation-based techniques from few-shot learning to effectively learn
from fewer samples across different tasks. Our method consists of a core meta
architecture with shared parameters, i.e., task-specific layers with exclusive
parameters. By exploiting the similarity between diverse data distributions,
our method can scale to multiple tasks without compromising the image quality.
We evaluate our method on standard image datasets and show that it outperforms
both unconditional and conditional DDPM in terms of FID and SSIM metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16357">Cross Entropy in Deep Learning of Classifiers Is Unnecessary -- ISBE Error is All You Need. (arXiv:2311.16357v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Skarbek_W/0/1/0/all/0/1">Wladyslaw Skarbek</a></p>
<p>In deep learning classifiers, the cost function usually takes the form of a
combination of SoftMax and CrossEntropy functions. The SoftMax unit transforms
the scores predicted by the model network into assessments of the degree
(probabilities) of an object's membership to a given class. On the other hand,
CrossEntropy measures the divergence of this prediction from the distribution
of target scores. This work introduces the ISBE functionality, justifying the
thesis about the redundancy of cross entropy computation in deep learning of
classifiers. Not only can we omit the calculation of entropy, but also, during
back-propagation, there is no need to direct the error to the normalization
unit for its backward transformation. Instead, the error is sent directly to
the model's network. Using examples of perceptron and convolutional networks as
classifiers of images from the MNIST collection, it is observed for ISBE that
results are not degraded with SoftMax only, but also with other activation
functions such as Sigmoid, Tanh, or their hard variants HardSigmoid and
HardTanh. Moreover, up to three percent of time is saved within the total time
of forward and backward stages. The article is addressed mainly to programmers
and students interested in deep model learning. For example, it illustrates in
code snippets possible ways to implement ISBE units, but also formally proves
that the softmax trick only applies to the class of softmax functions with
relocations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16361">Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling. (arXiv:2311.16361v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Weicheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1">Carlos Fernandez-Granda</a>, <a href="http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1">Narges Razavian</a></p>
<p>Self-supervised learning (SSL) has emerged as a powerful technique for
learning rich representations from unlabeled data. The data representations are
able to capture many underlying attributes of data, and be useful in downstream
prediction tasks. In real-world settings, spurious correlations between some
attributes (e.g. race, gender and age) and labels for downstream tasks often
exist, e.g. cancer is usually more prevalent among elderly patients. In this
paper, we investigate SSL in the presence of spurious correlations and show
that the SSL training loss can be minimized by capturing only a subset of the
conspicuous features relevant to those sensitive attributes, despite the
presence of other important predictive features for the downstream tasks. To
address this issue, we investigate the learning dynamics of SSL and observe
that the learning is slower for samples that conflict with such correlations
(e.g. elder patients without cancer). Motivated by these findings, we propose a
learning-speed aware SSL (LA-SSL) approach, in which we sample each training
data with a probability that is inversely related to its learning speed. We
evaluate LA-SSL on three datasets that exhibit spurious correlations between
different attributes, demonstrating that it improves the robustness of
pretrained representations on downstream classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16374">Physics-Informed Neural Network for Discovering Systems with Unmeasurable States with Application to Lithium-Ion Batteries. (arXiv:2311.16374v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kajiura_Y/0/1/0/all/0/1">Yuichi Kajiura</a>, <a href="http://arxiv.org/find/cs/1/au:+Espin_J/0/1/0/all/0/1">Jorge Espin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a></p>
<p>Combining machine learning with physics is a trending approach for
discovering unknown dynamics, and one of the most intensively studied
frameworks is the physics-informed neural network (PINN). However, PINN often
fails to optimize the network due to its difficulty in concurrently minimizing
multiple losses originating from the system's governing equations. This problem
can be more serious when the system's states are unmeasurable, like lithium-ion
batteries (LiBs). In this work, we introduce a robust method for training PINN
that uses fewer loss terms and thus constructs a less complex landscape for
optimization. In particular, instead of having loss terms from each
differential equation, this method embeds the dynamics into a loss function
that quantifies the error between observed and predicted system outputs. This
is accomplished by numerically integrating the predicted states from the neural
network(NN) using known dynamics and transforming them to obtain a sequence of
predicted outputs. Minimizing such a loss optimizes the NN to predict states
consistent with observations given the physics. Further, the system's
parameters can be added to the optimization targets. To demonstrate the ability
of this method to perform various modeling and control tasks, we apply it to a
battery model to concurrently estimate its states and parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16378">Bayesian Formulations for Graph Spectral Denoising. (arXiv:2311.16378v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Leone_S/0/1/0/all/0/1">Sam Leone</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xingzhi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1">Michael Perlmutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1">Smita Krishnaswamy</a></p>
<p>We consider noisy signals which are defined on the vertices of a graph and
present smoothing algorithms for the cases of Gaussian, dropout, and uniformly
distributed noise. The signals are assumed to follow a prior distribution
defined in the frequency domain which favors signals which are smooth across
the edges of the graph. By pairing this prior distribution with our three
models of noise generation, we propose \textit{Maximum A Posteriori} (M.A.P.)
estimates of the true signal in the presence of noisy data and provide
algorithms for computing the M.A.P. Finally, we demonstrate the algorithms'
ability to effectively restore white noise on image data, and from severe
dropout in toy \&amp; EHR data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16380">Learning Multimodal Latent Dynamics for Human-Robot Interaction. (arXiv:2311.16380v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prasad_V/0/1/0/all/0/1">Vignesh Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Heitlinger_L/0/1/0/all/0/1">Lea Heitlinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Koert_D/0/1/0/all/0/1">Dorothea Koert</a>, <a href="http://arxiv.org/find/cs/1/au:+Stock_Homburg_R/0/1/0/all/0/1">Ruth Stock-Homburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1">Jan Peters</a>, <a href="http://arxiv.org/find/cs/1/au:+Chalvatzaki_G/0/1/0/all/0/1">Georgia Chalvatzaki</a></p>
<p>This article presents a method for learning well-coordinated Human-Robot
Interaction (HRI) from Human-Human Interactions (HHI). We devise a hybrid
approach using Hidden Markov Models (HMMs) as the latent space priors for a
Variational Autoencoder to model a joint distribution over the interacting
agents. We leverage the interaction dynamics learned from HHI to learn HRI and
incorporate the conditional generation of robot motions from human observations
into the training, thereby predicting more accurate robot trajectories. The
generated robot motions are further adapted with Inverse Kinematics to ensure
the desired physical proximity with a human, combining the ease of joint space
learning and accurate task space reachability. For contact-rich interactions,
we modulate the robot's stiffness using HMM segmentation for a compliant
interaction. We verify the effectiveness of our approach deployed on a Humanoid
robot via a user study. Our method generalizes well to various humans despite
being trained on data from just two humans. We find that Users perceive our
method as more human-like, timely, and accurate and rank our method with a
higher degree of preference over other baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16381">Deep Learning for Time Series Classification of Parkinson&#x27;s Disease Eye Tracking Data. (arXiv:2311.16381v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uribarri_G/0/1/0/all/0/1">Gonzalo Uribarri</a>, <a href="http://arxiv.org/find/cs/1/au:+Huth_S/0/1/0/all/0/1">Simon Ekman von Huth</a>, <a href="http://arxiv.org/find/cs/1/au:+Waldthaler_J/0/1/0/all/0/1">Josefine Waldthaler</a>, <a href="http://arxiv.org/find/cs/1/au:+Svenningsson_P/0/1/0/all/0/1">Per Svenningsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Fransen_E/0/1/0/all/0/1">Erik Frans&#xe9;n</a></p>
<p>Eye-tracking is an accessible and non-invasive technology that provides
information about a subject's motor and cognitive abilities. As such, it has
proven to be a valuable resource in the study of neurodegenerative diseases
such as Parkinson's disease. Saccade experiments, in particular, have proven
useful in the diagnosis and staging of Parkinson's disease. However, to date,
no single eye-movement biomarker has been found to conclusively differentiate
patients from healthy controls. In the present work, we investigate the use of
state-of-the-art deep learning algorithms to perform Parkinson's disease
classification using eye-tracking data from saccade experiments. In contrast to
previous work, instead of using hand-crafted features from the saccades, we use
raw $\sim1.5\,s$ long fixation intervals recorded during the preparatory phase
before each trial. Using these short time series as input we implement two
different classification models, InceptionTime and ROCKET. We find that the
models are able to learn the classification task and generalize to unseen
subjects. InceptionTime achieves $78\%$ accuracy, while ROCKET achieves $88\%$
accuracy. We also employ a novel method for pruning the ROCKET model to improve
interpretability and generalizability, achieving an accuracy of $96\%$. Our
results suggest that fixation data has low inter-subject variability and
potentially carries useful information about brain cognitive and motor
conditions, making it suitable for use with machine learning in the discovery
of disease-relevant biomarkers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16416">A Combinatorial Approach to Robust PCA. (arXiv:2311.16416v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1">Weihao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1">Mingda Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1">Rajat Sen</a></p>
<p>We study the problem of recovering Gaussian data under adversarial
corruptions when the noises are low-rank and the corruptions are on the
coordinate level. Concretely, we assume that the Gaussian noises lie in an
unknown $k$-dimensional subspace $U \subseteq \mathbb{R}^d$, and $s$ randomly
chosen coordinates of each data point fall into the control of an adversary.
This setting models the scenario of learning from high-dimensional yet
structured data that are transmitted through a highly-noisy channel, so that
the data points are unlikely to be entirely clean.
</p>
<p>Our main result is an efficient algorithm that, when $ks^2 = O(d)$, recovers
every single data point up to a nearly-optimal $\ell_1$ error of $\tilde
O(ks/d)$ in expectation. At the core of our proof is a new analysis of the
well-known Basis Pursuit (BP) method for recovering a sparse signal, which is
known to succeed under additional assumptions (e.g., incoherence or the
restricted isometry property) on the underlying subspace $U$. In contrast, we
present a novel approach via studying a natural combinatorial problem and show
that, over the randomness in the support of the sparse signal, a
high-probability error bound is possible even if the subspace $U$ is arbitrary.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16420">Model-free Test Time Adaptation for Out-Of-Distribution Detection. (arXiv:2311.16420v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">YiFan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1">Kun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1">Rong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tieniu Tan</a></p>
<p>Out-of-distribution (OOD) detection is essential for the reliability of ML
models. Most existing methods for OOD detection learn a fixed decision
criterion from a given in-distribution dataset and apply it universally to
decide if a data point is OOD. Recent work~\cite{fang2022is} shows that given
only in-distribution data, it is impossible to reliably detect OOD data without
extra assumptions. Motivated by the theoretical result and recent exploration
of test-time adaptation methods, we propose a Non-Parametric Test Time
\textbf{Ada}ptation framework for \textbf{O}ut-Of-\textbf{D}istribution
\textbf{D}etection (\abbr). Unlike conventional methods, \abbr utilizes online
test samples for model adaptation during testing, enhancing adaptability to
changing data distributions. The framework incorporates detected OOD instances
into decision-making, reducing false positive rates, particularly when ID and
OOD distributions overlap significantly. We demonstrate the effectiveness of
\abbr through comprehensive experiments on multiple OOD detection benchmarks,
extensive empirical studies show that \abbr significantly improves the
performance of OOD detection over state-of-the-art methods. Specifically, \abbr
reduces the false positive rate (FPR95) by $23.23\%$ on the CIFAR-10 benchmarks
and $38\%$ on the ImageNet-1k benchmarks compared to the advanced methods.
Lastly, we theoretically verify the effectiveness of \abbr.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16424">Manifold Preserving Guided Diffusion. (arXiv:2311.16424v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yutong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Murata_N/0/1/0/all/0/1">Naoki Murata</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Chieh-Hsin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Takida_Y/0/1/0/all/0/1">Yuhta Takida</a>, <a href="http://arxiv.org/find/cs/1/au:+Uesaka_T/0/1/0/all/0/1">Toshimitsu Uesaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongjun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Wei-Hsiang Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1">Yuki Mitsufuji</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a></p>
<p>Despite the recent advancements, conditional image generation still faces
challenges of cost, generalizability, and the need for task-specific training.
In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a
training-free conditional generation framework that leverages pretrained
diffusion models and off-the-shelf neural networks with minimal additional
inference cost for a broad range of tasks. Specifically, we leverage the
manifold hypothesis to refine the guided diffusion steps and introduce a
shortcut algorithm in the process. We then propose two methods for on-manifold
training-free guidance using pre-trained autoencoders and demonstrate that our
shortcut inherently preserves the manifolds when applied to latent diffusion
models. Our experiments show that MPGD is efficient and effective for solving a
variety of conditional generation applications in low-compute settings, and can
consistently offer up to 3.8x speed-ups with the same number of diffusion steps
while maintaining high sample quality compared to the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16432">Text-Driven Image Editing via Learnable Regions. (arXiv:2311.16432v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yuanze Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Wen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yi-Hsuan Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Lu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Ming-Hsuan Yang</a></p>
<p>Language has emerged as a natural interface for image editing. In this paper,
we introduce a method for region-based image editing driven by textual prompts,
without the need for user-provided masks or sketches. Specifically, our
approach leverages an existing pretrained text-to-image model and introduces a
bounding box generator to find the edit regions that are aligned with the
textual prompts. We show that this simple approach enables flexible editing
that is compatible with current image generation models, and is able to handle
complex prompts featuring multiple objects, complex sentences or long
paragraphs. We conduct an extensive user study to compare our method against
state-of-the-art methods. Experiments demonstrate the competitive performance
of our method in manipulating images with high fidelity and realism that align
with the language descriptions provided. Our project webpage:
https://yuanze-lin.me/LearnableRegions_page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16442">Enabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and Asynchronous Dequantization. (arXiv:2311.16442v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaming Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1">Yaoxiu Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1">Guohao Dai</a></p>
<p>Large language models (LLMs) have demonstrated impressive abilities in
various domains while the inference cost is expensive. The state-of-the-art
methods use 2-bit quantization for mainstream LLMs. However, challenges still
exist: (1) Nonnegligible accuracy loss for 2-bit quantization. Weights are
quantized by groups, while the ranges of weights are large in some groups,
resulting in large quantization errors and nonnegligible accuracy loss (e.g.
&gt;3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit). (2) Limited
accuracy improvement by adding 4-bit weights. Increasing 10% extra average bit
more 4-bit weights only leads to &lt;0.5% accuracy improvement on a quantized
Llama2-7b. (3) Time-consuming dequantization operations on GPUs. The
dequantization operations lead to &gt;50% execution time, hindering the potential
of reducing LLM inference cost. To tackle these challenges, we propose the
following techniques: (1) We only quantize a small fraction of groups with the
larger range using 4-bit with memory alignment consideration on GPUs. (2) We
point out that the distribution of the sparse outliers with larger weights is
different in 2-bit and 4-bit groups, and only a small fraction of outliers
require 16-bit quantization. Such design leads to &gt;0.5% accuracy improvement
with &lt;3% average increased bit for Llama2-7b. (3) We design the asynchronous
dequantization on GPUs, leading to up to 3.92X speedup. We conduct extensive
experiments on different model families and model sizes. We achieve 2.85-bit
for each weight and the end-to-end speedup for Llama2-7b is 1.74X over the
original model, and we reduce both runtime cost and hardware cost by up to
2.70X and 2.81X with less GPU requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16459">On the Effect of Defections in Federated Learning and How to Prevent Them. (arXiv:2311.16459v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1">Minbiao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1">Kumar Kshitij Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1">Han Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lingxiao Wang</a></p>
<p>Federated learning is a machine learning protocol that enables a large
population of agents to collaborate over multiple rounds to produce a single
consensus model. There are several federated learning applications where agents
may choose to defect permanently$-$essentially withdrawing from the
collaboration$-$if they are content with their instantaneous model in that
round. This work demonstrates the detrimental impact of such defections on the
final model's robustness and ability to generalize. We also show that current
federated optimization algorithms fail to disincentivize these harmful
defections. We introduce a novel optimization algorithm with theoretical
guarantees to prevent defections while ensuring asymptotic convergence to an
effective solution for all participating agents. We also provide numerical
experiments to corroborate our findings and demonstrate the effectiveness of
our algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16485">Class-Adaptive Sampling Policy for Efficient Continual Learning. (arXiv:2311.16485v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rezaei_H/0/1/0/all/0/1">Hossein Rezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1">Mohammad Sabokrou</a></p>
<p>Continual learning (CL) aims to acquire new knowledge while preserving
information from previous experiences without forgetting. Though buffer-based
methods (i.e., retaining samples from previous tasks) have achieved acceptable
performance, determining how to allocate the buffer remains a critical
challenge. Most recent research focuses on refining these methods but often
fails to sufficiently consider the varying influence of samples on the learning
process, and frequently overlooks the complexity of the classes/concepts being
learned. Generally, these methods do not directly take into account the
contribution of individual classes. However, our investigation indicates that
more challenging classes necessitate preserving a larger number of samples
compared to less challenging ones. To address this issue, we propose a novel
method and policy named 'Class-Adaptive Sampling Policy' (CASP), which
dynamically allocates storage space within the buffer. By utilizing concepts of
class contribution and difficulty, CASP adaptively manages buffer space,
allowing certain classes to occupy a larger portion of the buffer while
reducing storage for others. This approach significantly improves the
efficiency of knowledge retention and utilization. CASP provides a versatile
solution to boost the performance and efficiency of CL. It meets the demand for
dynamic buffer allocation, accommodating the varying contributions of different
classes and their learning complexities over time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16487">On the Robustness of Decision-Focused Learning. (arXiv:2311.16487v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farhat_Y/0/1/0/all/0/1">Yehya Farhat</a></p>
<p>Decision-Focused Learning (DFL) is an emerging learning paradigm that tackles
the task of training a machine learning (ML) model to predict missing
parameters of an incomplete optimization problem, where the missing parameters
are predicted. DFL trains an ML model in an end-to-end system, by integrating
the prediction and optimization tasks, providing better alignment of the
training and testing objectives. DFL has shown a lot of promise and holds the
capacity to revolutionize decision-making in many real-world applications.
However, very little is known about the performance of these models under
adversarial attacks. We adopt ten unique DFL methods and benchmark their
performance under two distinctly focused attacks adapted towards the
Predict-then-Optimize problem setting. Our study proposes the hypothesis that
the robustness of a model is highly correlated with its ability to find
predictions that lead to optimal decisions without deviating from the
ground-truth label. Furthermore, we provide insight into how to target the
models that violate this condition and show how these models respond
differently depending on the achieved optimality at the end of their training
cycles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16490">SIRAN: Sinkhorn Distance Regularized Adversarial Network for DEM Super-resolution using Discriminative Spatial Self-attention. (arXiv:2311.16490v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Paul_S/0/1/0/all/0/1">Subhajit Paul</a>, <a href="http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1">Ashutosh Gupta</a></p>
<p>Digital Elevation Model (DEM) is an essential aspect in the remote sensing
domain to analyze and explore different applications related to surface
elevation information. In this study, we intend to address the generation of
high-resolution DEMs using high-resolution multi-spectral (MX) satellite
imagery by incorporating adversarial learning. To promptly regulate this
process, we utilize the notion of polarized self-attention of discriminator
spatial maps as well as introduce a Densely connected Multi-Residual Block
(DMRB) module to assist in efficient gradient flow. Further, we present an
objective function related to optimizing Sinkhorn distance with traditional GAN
to improve the stability of adversarial learning. In this regard, we provide
both theoretical and empirical substantiation of better performance in terms of
vanishing gradient issues and numerical convergence. We demonstrate both
qualitative and quantitative outcomes with available state-of-the-art methods.
Based on our experiments on DEM datasets of Shuttle Radar Topographic Mission
(SRTM) and Cartosat-1, we show that the proposed model performs preferably
against other learning-based state-of-the-art methods. We also generate and
visualize several high-resolution DEMs covering terrains with diverse
signatures to show the performance of our model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16496">Leveraging Out-of-Domain Data for Domain-Specific Prompt Tuning in Multi-Modal Fake News Detection. (arXiv:2311.16496v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brahma_D/0/1/0/all/0/1">Debarshi Brahma</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1">Amartya Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahadev_S/0/1/0/all/0/1">Suraj Nagaje Mahadev</a>, <a href="http://arxiv.org/find/cs/1/au:+Asati_A/0/1/0/all/0/1">Anmol Asati</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1">Vikas Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Soma Biswas</a></p>
<p>The spread of fake news using out-of-context images has become widespread and
is a challenging task in this era of information overload. Since annotating
huge amounts of such data requires significant time of domain experts, it is
imperative to develop methods which can work in limited annotated data
scenarios. In this work, we explore whether out-of-domain data can help to
improve out-of-context misinformation detection (termed here as multi-modal
fake news detection) of a desired domain, eg. politics, healthcare, etc.
Towards this goal, we propose a novel framework termed DPOD (Domain-specific
Prompt-tuning using Out-of-Domain data). First, to compute generalizable
features, we modify the Vision-Language Model, CLIP to extract features that
helps to align the representations of the images and corresponding text
captions of both the in-domain and out-of-domain data in a label-aware manner.
Further, we propose a domain-specific prompt learning technique which leverages
the training samples of all the available domains based on the the extent they
can be useful to the desired domain. Extensive experiments on a large-scale
benchmark dataset, namely NewsClippings demonstrate that the proposed framework
achieves state of-the-art performance, significantly surpassing the existing
approaches for this challenging task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16503">TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models. (arXiv:2311.16503v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yushi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1">Ruihao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianglong Liu</a></p>
<p>The Diffusion model, a prevalent framework for image generation, encounters
significant challenges in terms of broad applicability due to its extended
inference times and substantial memory requirements. Efficient Post-training
Quantization (PTQ) is pivotal for addressing these issues in traditional
models. Different from traditional models, diffusion models heavily depend on
the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$
from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a
few modules totally irrespective of the sampling data. However, existing PTQ
methods do not optimize these modules separately. They adopt inappropriate
reconstruction targets and complex calibration methods, resulting in a severe
disturbance of the temporal feature and denoising trajectory, as well as a low
compression efficiency. To solve these, we propose a Temporal Feature
Maintenance Quantization (TFMQ) framework building upon a Temporal Information
Block which is just related to the time-step $t$ and unrelated to the sampling
data. Powered by the pioneering block design, we devise temporal information
aware reconstruction (TIAR) and finite set calibration (FSC) to align the
full-precision temporal features in a limited time. Equipped with the
framework, we can maintain the most temporal information and ensure the
end-to-end generation quality. Extensive experiments on various datasets and
diffusion models prove our state-of-the-art results. Remarkably, our
quantization approach, for the first time, achieves model performance nearly on
par with the full-precision model under 4-bit weight quantization.
Additionally, our method incurs almost no extra computational cost and
accelerates quantization time by $2.0 \times$ on LSUN-Bedrooms $256 \times 256$
compared to previous works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16507">Exploring Straighter Trajectories of Flow Matching with Diffusion Guidance. (arXiv:2311.16507v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xing_S/0/1/0/all/0/1">Siyu Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jie Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huaibo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao-Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a></p>
<p>Flow matching as a paradigm of generative model achieves notable success
across various domains. However, existing methods use either multi-round
training or knowledge within minibatches, posing challenges in finding a
favorable coupling strategy for straight trajectories. To address this issue,
we propose a novel approach, Straighter trajectories of Flow Matching
(StraightFM). It straightens trajectories with the coupling strategy guided by
diffusion model from entire distribution level. First, we propose a coupling
strategy to straighten trajectories, creating couplings between image and noise
samples under diffusion model guidance. Second, StraightFM also integrates real
data to enhance training, employing a neural network to parameterize another
coupling process from images to noise samples. StraightFM is jointly optimized
with couplings from above two mutually complementary directions, resulting in
straighter trajectories and enabling both one-step and few-step generation.
Extensive experiments demonstrate that StraightFM yields high quality samples
with fewer step. StraightFM generates visually appealing images with a lower
FID among diffusion and traditional flow matching methods within 5 sampling
steps when trained on pixel space. In the latent space (i.e., Latent
Diffusion), StraightFM achieves a lower KID value compared to existing methods
on the CelebA-HQ 256 dataset in fewer than 10 sampling steps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16509">StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models. (arXiv:2311.16509v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamauchi_K/0/1/0/all/0/1">Kazuki Yamauchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ijima_Y/0/1/0/all/0/1">Yusuke Ijima</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuki Saito</a></p>
<p>We propose StyleCap, a method to generate natural language descriptions of
speaking styles appearing in speech. Although most of conventional techniques
for para-/non-linguistic information recognition focus on the category
classification or the intensity estimation of pre-defined labels, they cannot
provide the reasoning of the recognition result in an interpretable manner. As
a first step towards an end-to-end method for generating speaking-style prompts
from speech, i.e., automatic speaking-style captioning, StyleCap uses paired
data of speech and natural language descriptions to train neural networks that
predict prefix vectors fed into a large language model (LLM)-based text decoder
from a speech representation vector. We explore an appropriate text decoder and
speech feature representation suitable for this new task. The experimental
results demonstrate that our StyleCap leveraging richer LLMs for the text
decoder, speech self-supervised learning (SSL) features, and sentence
rephrasing augmentation improves the accuracy and diversity of generated
speaking-style captions. Samples of speaking-style captions generated by our
StyleCap are publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16514">Video Anomaly Detection via Spatio-Temporal Pseudo-Anomaly Generation : A Unified Approach. (arXiv:2311.16514v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1">Ayush K. Rai</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1">Tarun Krishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1">Feiyan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Drimbarean_A/0/1/0/all/0/1">Alexandru Drimbarean</a>, <a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1">Kevin McGuinness</a>, <a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1">Alan F. Smeaton</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1">Noel E. O&#x27;Connor</a></p>
<p>Video Anomaly Detection (VAD) is an open-set recognition task, which is
usually formulated as a one-class classification (OCC) problem, where training
data is comprised of videos with normal instances while test data contains both
normal and anomalous instances. Recent works have investigated the creation of
pseudo-anomalies (PAs) using only the normal data and making strong assumptions
about real-world anomalies with regards to abnormality of objects and speed of
motion to inject prior information about anomalies in an autoencoder (AE) based
reconstruction model during training. This work proposes a novel method for
generating generic spatio-temporal PAs by inpainting a masked out region of an
image using a pre-trained Latent Diffusion Model and further perturbing the
optical flow using mixup to emulate spatio-temporal distortions in the data. In
addition, we present a simple unified framework to detect real-world anomalies
under the OCC setting by learning three types of anomaly indicators, namely
reconstruction quality, temporal irregularity and semantic inconsistency.
Extensive experiments on four VAD benchmark datasets namely Ped2, Avenue,
ShanghaiTech and UBnormal demonstrate that our method performs on par with
other existing state-of-the-art PAs generation and reconstruction based methods
under the OCC setting. Our analysis also examines the transferability and
generalisation of PAs across these datasets, offering valuable insights by
identifying real-world anomalies through PAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16519">B-LSTM-MIONet: Bayesian LSTM-based Neural Operators for Learning the Response of Complex Dynamical Systems to Length-Variant Multiple Input Functions. (arXiv:2311.16519v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1">Zhihao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mollaali_A/0/1/0/all/0/1">Amirhossein Mollaali</a>, <a href="http://arxiv.org/find/cs/1/au:+Moya_C/0/1/0/all/0/1">Christian Moya</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1">Na Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guang Lin</a></p>
<p>Deep Operator Network (DeepONet) is a neural network framework for learning
nonlinear operators such as those from ordinary differential equations (ODEs)
describing complex systems. Multiple-input deep neural operators (MIONet)
extended DeepONet to allow multiple input functions in different Banach spaces.
MIONet offers flexibility in training dataset grid spacing, without constraints
on output location. However, it requires offline inputs and cannot handle
varying sequence lengths in testing datasets, limiting its real-time
application in dynamic complex systems. This work redesigns MIONet, integrating
Long Short Term Memory (LSTM) to learn neural operators from time-dependent
data. This approach overcomes data discretization constraints and harnesses
LSTM's capability with variable-length, real-time data. Factors affecting
learning performance, like algorithm extrapolation ability are presented. The
framework is enhanced with uncertainty quantification through a novel Bayesian
method, sampling from MIONet parameter distributions. Consequently, we develop
the B-LSTM-MIONet, incorporating LSTM's temporal strengths with Bayesian
robustness, resulting in a more precise and reliable model for noisy datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16520">Value Approximation for Two-Player General-Sum Differential Games with State Constraints. (arXiv:2311.16520v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghimire_M/0/1/0/all/0/1">Mukesh Ghimire</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenlong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yi Ren</a></p>
<p>Solving Hamilton-Jacobi-Isaacs (HJI) PDEs enables equilibrial feedback
control in two-player differential games, yet faces the curse of dimensionality
(CoD). While physics-informed machine learning has been adopted to address CoD
in solving PDEs, this method falls short in learning discontinuous solutions
due to its sampling nature, leading to poor safety performance of the resulting
controllers in robotics applications where values are discontinuous due to
state or other temporal logic constraints. In this study, we explore three
potential solutions to this problem: (1) a hybrid learning method that uses
both equilibrium demonstrations and the HJI PDE, (2) a value-hardening method
where a sequence of HJIs are solved with increasing Lipschitz constant on the
constraint violation penalty, and (3) the epigraphical technique that lifts the
value to a higher dimensional auxiliary state space where the value becomes
continuous. Evaluations through 5D and 9D vehicle simulations and 13D drone
simulations reveal that the hybrid method outperforms others in terms of
generalization and safety performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16522">Evaluation of dynamic characteristics of power grid based on GNN and application on knowledge graph. (arXiv:2311.16522v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1">Hao Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Si Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chuanfu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Che Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sizhe Li</a></p>
<p>A novel method for detecting faults in power grids using a graph neural
network (GNN) has been developed, aimed at enhancing intelligent fault
diagnosis in network operation and maintenance. This GNN-based approach
identifies faulty nodes within the power grid through a specialized electrical
feature extraction model coupled with a knowledge graph. Incorporating temporal
data, the method leverages the status of nodes from preceding and subsequent
time periods to aid in current fault detection. To validate the effectiveness
of this GNN in extracting node features, a correlation analysis of the output
features from each node within the neural network layer was conducted. The
results from experiments show that this method can accurately locate fault
nodes in simulated scenarios with a remarkable 99.53% accuracy. Additionally,
the graph neural network's feature modeling allows for a qualitative
examination of how faults spread across nodes, providing valuable insights for
analyzing fault nodes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16524">3D Teeth Reconstruction from Panoramic Radiographs using Neural Implicit Functions. (arXiv:2311.16524v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sihwa Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seongjun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_I/0/1/0/all/0/1">In-Seok Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1">Seung Jun Baek</a></p>
<p>Panoramic radiography is a widely used imaging modality in dental practice
and research. However, it only provides flattened 2D images, which limits the
detailed assessment of dental structures. In this paper, we propose Occudent, a
framework for 3D teeth reconstruction from panoramic radiographs using neural
implicit functions, which, to the best of our knowledge, is the first work to
do so. For a given point in 3D space, the implicit function estimates whether
the point is occupied by a tooth, and thus implicitly determines the boundaries
of 3D tooth shapes. Firstly, Occudent applies multi-label segmentation to the
input panoramic radiograph. Next, tooth shape embeddings as well as tooth class
embeddings are generated from the segmentation outputs, which are fed to the
reconstruction network. A novel module called Conditional eXcitation (CX) is
proposed in order to effectively incorporate the combined shape and class
embeddings into the implicit function. The performance of Occudent is evaluated
using both quantitative and qualitative measures. Importantly, Occudent is
trained and validated with actual panoramic radiographs as input, distinct from
recent works which used synthesized images. Experiments demonstrate the
superiority of Occudent over state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16526">On robust overfitting: adversarial training induced distribution matters. (arXiv:2311.16526v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1">Runzhi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yongyi Mao</a></p>
<p>Adversarial training may be regarded as standard training with a modified
loss function. But its generalization error appears much larger than standard
training under standard loss. This phenomenon, known as robust overfitting, has
attracted significant research attention and remains largely as a mystery. In
this paper, we first show empirically that robust overfitting correlates with
the increasing generalization difficulty of the perturbation-induced
distributions along the trajectory of adversarial training (specifically
PGD-based adversarial training). We then provide a novel upper bound for
generalization error with respect to the perturbation-induced distributions, in
which a notion of the perturbation operator, referred to "local dispersion",
plays an important role.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16528">Utility Fairness in Contextual Dynamic Pricing with Demand Learning. (arXiv:2311.16528v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Simchi_Levi_D/0/1/0/all/0/1">David Simchi-Levi</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1">Yining Wang</a></p>
<p>This paper introduces a novel contextual bandit algorithm for personalized
pricing under utility fairness constraints in scenarios with uncertain demand,
achieving an optimal regret upper bound. Our approach, which incorporates
dynamic pricing and demand learning, addresses the critical challenge of
fairness in pricing strategies. We first delve into the static full-information
setting to formulate an optimal pricing policy as a constrained optimization
problem. Here, we propose an approximation algorithm for efficiently and
approximately computing the ideal policy.
</p>
<p>We also use mathematical analysis and computational studies to characterize
the structures of optimal contextual pricing policies subject to fairness
constraints, deriving simplified policies which lays the foundations of more
in-depth research and extensions.
</p>
<p>Further, we extend our study to dynamic pricing problems with demand
learning, establishing a non-standard regret lower bound that highlights the
complexity added by fairness constraints. Our research offers a comprehensive
analysis of the cost of fairness and its impact on the balance between utility
and revenue maximization. This work represents a step towards integrating
ethical considerations into algorithmic efficiency in data-driven dynamic
pricing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16535">Contrastive encoder pre-training-based clustered federated learning for heterogeneous data. (arXiv:2311.16535v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tun_Y/0/1/0/all/0/1">Ye Lin Tun</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1">Minh N.H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Thwal_C/0/1/0/all/0/1">Chu Myaet Thwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jinwoo Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1">Choong Seon Hong</a></p>
<p>Federated learning (FL) is a promising approach that enables distributed
clients to collaboratively train a global model while preserving their data
privacy. However, FL often suffers from data heterogeneity problems, which can
significantly affect its performance. To address this, clustered federated
learning (CFL) has been proposed to construct personalized models for different
client clusters. One effective client clustering strategy is to allow clients
to choose their own local models from a model pool based on their performance.
However, without pre-trained model parameters, such a strategy is prone to
clustering failure, in which all clients choose the same model. Unfortunately,
collecting a large amount of labeled data for pre-training can be costly and
impractical in distributed environments. To overcome this challenge, we
leverage self-supervised contrastive learning to exploit unlabeled data for the
pre-training of FL systems. Together, self-supervised pre-training and client
clustering can be crucial components for tackling the data heterogeneity issues
of FL. Leveraging these two crucial strategies, we propose contrastive
pre-training-based clustered federated learning (CP-CFL) to improve the model
convergence and overall performance of FL systems. In this work, we demonstrate
the effectiveness of CP-CFL through extensive experiments in heterogeneous FL
settings, and present various interesting observations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16536">Personalized Predictions of Glioblastoma Infiltration: Mathematical Models, Physics-Informed Neural Networks and Multimodal Scans. (arXiv:2311.16536v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zirui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1">Ivan Ezhov</a>, <a href="http://arxiv.org/find/cs/1/au:+Balcerak_M/0/1/0/all/0/1">Michal Balcerak</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1">Andy Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1">Benedikt Wiestler</a>, <a href="http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/cs/1/au:+Lowengrub_J/0/1/0/all/0/1">John Lowengrub</a></p>
<p>Predicting the infiltration of Glioblastoma (GBM) from medical MRI scans is
crucial for understanding tumor growth dynamics and designing personalized
radiotherapy treatment plans.Mathematical models of GBM growth can complement
the data in the prediction of spatial distributions of tumor cells. However,
this requires estimating patient-specific parameters of the model from clinical
data, which is a challenging inverse problem due to limited temporal data and
the limited time between imaging and diagnosis. This work proposes a method
that uses Physics-Informed Neural Networks (PINNs) to estimate patient-specific
parameters of a reaction-diffusion PDE model of GBM growth from a single 3D
structural MRI snapshot. PINNs embed both the data and the PDE into a loss
function, thus integrating theory and data. Key innovations include the
identification and estimation of characteristic non-dimensional parameters, a
pre-training step that utilizes the non-dimensional parameters and a
fine-tuning step to determine the patient specific parameters. Additionally,
the diffuse domain method is employed to handle the complex brain geometry
within the PINN framework. Our method is validated both on synthetic and
patient datasets, and shows promise for real-time parametric inference in the
clinical setting for personalized GBM treatment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16538">Federated Learning with Diffusion Models for Privacy-Sensitive Vision Tasks. (arXiv:2311.16538v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tun_Y/0/1/0/all/0/1">Ye Lin Tun</a>, <a href="http://arxiv.org/find/cs/1/au:+Thwal_C/0/1/0/all/0/1">Chu Myaet Thwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Ji Su Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1">Sun Moo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1">Choong Seon Hong</a></p>
<p>Diffusion models have shown great potential for vision-related tasks,
particularly for image generation. However, their training is typically
conducted in a centralized manner, relying on data collected from publicly
available sources. This approach may not be feasible or practical in many
domains, such as the medical field, which involves privacy concerns over data
collection. Despite the challenges associated with privacy-sensitive data, such
domains could still benefit from valuable vision services provided by diffusion
models. Federated learning (FL) plays a crucial role in enabling decentralized
model training without compromising data privacy. Instead of collecting data,
an FL system gathers model parameters, effectively safeguarding the private
data of different parties involved. This makes FL systems vital for managing
decentralized learning tasks, especially in scenarios where privacy-sensitive
data is distributed across a network of clients. Nonetheless, FL presents its
own set of challenges due to its distributed nature and privacy-preserving
properties. Therefore, in this study, we explore the FL strategy to train
diffusion models, paving the way for the development of federated diffusion
models. We conduct experiments on various FL scenarios, and our findings
demonstrate that federated diffusion models have great potential to deliver
vision services to privacy-sensitive domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16540">Communication Efficiency Optimization of Federated Learning for Computing and Network Convergence of 6G Networks. (arXiv:2311.16540v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yizhuo Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1">Bo Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qianying Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1">Jing Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1">Min Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yushun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xing Zhang</a></p>
<p>Federated learning effectively addresses issues such as data privacy by
collaborating across participating devices to train global models. However,
factors such as network topology and device computing power can affect its
training or communication process in complex network environments. A new
network architecture and paradigm with computing-measurable, perceptible,
distributable, dispatchable, and manageable capabilities, computing and network
convergence (CNC) of 6G networks can effectively support federated learning
training and improve its communication efficiency. By guiding the participating
devices' training in federated learning based on business requirements,
resource load, network conditions, and arithmetic power of devices, CNC can
reach this goal. In this paper, to improve the communication efficiency of
federated learning in complex networks, we study the communication efficiency
optimization of federated learning for computing and network convergence of 6G
networks, methods that gives decisions on its training process for different
network conditions and arithmetic power of participating devices in federated
learning. The experiments address two architectures that exist for devices in
federated learning and arrange devices to participate in training based on
arithmetic power while achieving optimization of communication efficiency in
the process of transferring model parameters. The results show that the method
we proposed can (1) cope well with complex network situations (2) effectively
balance the delay distribution of participating devices for local training (3)
improve the communication efficiency during the transfer of model parameters
(4) improve the resource utilization in the network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16556">Scalable Label Distribution Learning for Multi-Label Classification. (arXiv:2311.16556v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xingyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1">Yuexuan An</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lei Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a></p>
<p>Multi-label classification (MLC) refers to the problem of tagging a given
instance with a set of relevant labels. Most existing MLC methods are based on
the assumption that the correlation of two labels in each label pair is
symmetric, which is violated in many real-world scenarios. Moreover, most
existing methods design learning processes associated with the number of
labels, which makes their computational complexity a bottleneck when scaling up
to large-scale output space. To tackle these issues, we propose a novel MLC
learning method named Scalable Label Distribution Learning (SLDL) for
multi-label classification which can describe different labels as distributions
in a latent space, where the label correlation is asymmetric and the dimension
is independent of the number of labels. Specifically, SLDL first converts
labels into continuous distributions within a low-dimensional latent space and
leverages the asymmetric metric to establish the correlation between different
labels. Then, it learns the mapping from the feature space to the latent space,
resulting in the computational complexity is no longer related to the number of
labels. Finally, SLDL leverages a nearest-neighbor-based strategy to decode the
latent representations and obtain the final predictions. Our extensive
experiments illustrate that SLDL can achieve very competitive classification
performances with little computational consumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16584">FedAL: Black-Box Federated Knowledge Distillation Enabled by Adversarial Learning. (arXiv:2311.16584v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_P/0/1/0/all/0/1">Pengchao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xingyan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jianwei Huang</a></p>
<p>Knowledge distillation (KD) can enable collaborative learning among
distributed clients that have different model architectures and do not share
their local data and model parameters with others. Each client updates its
local model using the average model output/feature of all client models as the
target, known as federated KD. However, existing federated KD methods often do
not perform well when clients' local models are trained with heterogeneous
local datasets. In this paper, we propose Federated knowledge distillation
enabled by Adversarial Learning (FedAL) to address the data heterogeneity among
clients. First, to alleviate the local model output divergence across clients
caused by data heterogeneity, the server acts as a discriminator to guide
clients' local model training to achieve consensus model outputs among clients
through a min-max game between clients and the discriminator. Moreover,
catastrophic forgetting may happen during the clients' local training and
global knowledge transfer due to clients' heterogeneous local data. Towards
this challenge, we design the less-forgetting regularization for both local
training and global knowledge transfer to guarantee clients' ability to
transfer/learn knowledge to/from others. Experimental results show that FedAL
and its variants achieve higher accuracy than other federated KD baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16589">Improving Lane Detection Generalization: A Novel Framework using HD Maps for Boosting Diversity. (arXiv:2311.16589v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Daeun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Heo_M/0/1/0/all/0/1">Minhyeok Heo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jiwon Kim</a></p>
<p>Lane detection is a vital task for vehicles to navigate and localize their
position on the road. To ensure reliable results, lane detection algorithms
must have robust generalization performance in various road environments.
However, despite the significant performance improvement of deep learning-based
lane detection algorithms, their generalization performance in response to
changes in road environments still falls short of expectations. In this paper,
we present a novel framework for single-source domain generalization (SSDG) in
lane detection. By decomposing data into lane structures and surroundings, we
enhance diversity using High-Definition (HD) maps and generative models. Rather
than expanding data volume, we strategically select a core subset of data,
maximizing diversity and optimizing performance. Our extensive experiments
demonstrate that our framework enhances the generalization performance of lane
detection, comparable to the domain adaptation-based method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16593">Empowering COVID-19 Detection: Optimizing Performance Through Fine-Tuned EfficientNet Deep Learning Architecture. (arXiv:2311.16593v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Talukder_M/0/1/0/all/0/1">Md. Alamin Talukder</a>, <a href="http://arxiv.org/find/eess/1/au:+Layek_M/0/1/0/all/0/1">Md. Abu Layek</a>, <a href="http://arxiv.org/find/eess/1/au:+Kazi_M/0/1/0/all/0/1">Mohsin Kazi</a>, <a href="http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1">Md Ashraf Uddin</a>, <a href="http://arxiv.org/find/eess/1/au:+Aryal_S/0/1/0/all/0/1">Sunil Aryal</a></p>
<p>The worldwide COVID-19 pandemic has profoundly influenced the health and
everyday experiences of individuals across the planet. It is a highly
contagious respiratory disease requiring early and accurate detection to curb
its rapid transmission. Initial testing methods primarily revolved around
identifying the genetic composition of the coronavirus, exhibiting a relatively
low detection rate and requiring a time-intensive procedure. To address this
challenge, experts have suggested using radiological imagery, particularly
chest X-rays, as a valuable approach within the diagnostic protocol. This study
investigates the potential of leveraging radiographic imaging (X-rays) with
deep learning algorithms to swiftly and precisely identify COVID-19 patients.
The proposed approach elevates the detection accuracy by fine-tuning with
appropriate layers on various established transfer learning models. The
experimentation was conducted on a COVID-19 X-ray dataset containing 2000
images. The accuracy rates achieved were impressive of 100% for EfficientNetB4
model. The fine-tuned EfficientNetB4 achieved an excellent accuracy score,
showcasing its potential as a robust COVID-19 detection model. Furthermore,
EfficientNetB4 excelled in identifying Lung disease using Chest X-ray dataset
containing 4,350 Images, achieving remarkable performance with an accuracy of
99.17%, precision of 99.13%, recall of 99.16%, and f1-score of 99.14%. These
results highlight the promise of fine-tuned transfer learning for efficient
lung detection through medical imaging, especially with X-ray images. This
research offers radiologists an effective means of aiding rapid and precise
COVID-19 diagnosis and contributes valuable assistance for healthcare
professionals in accurately identifying affected patients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16595">D4AM: A General Denoising Framework for Downstream Acoustic Models. (arXiv:2311.16595v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chi-Chang Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1">Yu Tsao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hsin-Min Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chu-Song Chen</a></p>
<p>The performance of acoustic models degrades notably in noisy environments.
Speech enhancement (SE) can be used as a front-end strategy to aid automatic
speech recognition (ASR) systems. However, existing training objectives of SE
methods are not fully effective at integrating speech-text and noisy-clean
paired data for training toward unseen ASR systems. In this study, we propose a
general denoising framework, D4AM, for various downstream acoustic models. Our
framework fine-tunes the SE model with the backward gradient according to a
specific acoustic model and the corresponding classification objective. In
addition, our method aims to consider the regression objective as an auxiliary
loss to make the SE model generalize to other unseen acoustic models. To
jointly train an SE unit with regression and classification objectives, D4AM
uses an adjustment scheme to directly estimate suitable weighting coefficients
rather than undergoing a grid search process with additional training costs.
The adjustment scheme consists of two parts: gradient calibration and
regression objective weighting. The experimental results show that D4AM can
consistently and effectively provide improvements to various unseen acoustic
models and outperforms other combination setups. Specifically, when evaluated
on the Google ASR API with real noisy data completely unseen during SE
training, D4AM achieves a relative WER reduction of 24.65% compared with the
direct feeding of noisy input. To our knowledge, this is the first work that
deploys an effective combination scheme of regression (denoising) and
classification (ASR) objectives to derive a general pre-processor applicable to
various unseen ASR systems. Our code is available at
https://github.com/ChangLee0903/D4AM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16602">GSP-KalmanNet: Tracking Graph Signals via Neural-Aided Kalman Filtering. (arXiv:2311.16602v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Buchnik_I/0/1/0/all/0/1">Itay Buchnik</a>, <a href="http://arxiv.org/find/eess/1/au:+Sagi_G/0/1/0/all/0/1">Guy Sagi</a>, <a href="http://arxiv.org/find/eess/1/au:+Leinwand_N/0/1/0/all/0/1">Nimrod Leinwand</a>, <a href="http://arxiv.org/find/eess/1/au:+Loya_Y/0/1/0/all/0/1">Yuval Loya</a>, <a href="http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1">Nir Shlezinger</a>, <a href="http://arxiv.org/find/eess/1/au:+Routtenberg_T/0/1/0/all/0/1">Tirza Routtenberg</a></p>
<p>Dynamic systems of graph signals are encountered in various applications,
including social networks, power grids, and transportation. While such systems
can often be described as state space (SS) models, tracking graph signals via
conventional tools based on the Kalman filter (KF) and its variants is
typically challenging. This is due to the nonlinearity, high dimensionality,
irregularity of the domain, and complex modeling associated with real-world
dynamic systems of graph signals. In this work, we study the tracking of graph
signals using a hybrid model-based/data-driven approach. We develop the
GSP-KalmanNet, which tracks the hidden graphical states from the graphical
measurements by jointly leveraging graph signal processing (GSP) tools and deep
learning (DL) techniques. The derivations of the GSP-KalmanNet are based on
extending the KF to exploit the inherent graph structure via graph frequency
domain filtering, which considerably simplifies the computational complexity
entailed in processing high-dimensional signals and increases the robustness to
small topology changes. Then, we use data to learn the Kalman gain following
the recently proposed KalmanNet framework, which copes with partial and
approximated modeling, without forcing a specific model over the noise
statistics. Our empirical results demonstrate that the proposed GSP-KalmanNet
achieves enhanced accuracy and run time performance as well as improved
robustness to model misspecifications compared with both model-based and
data-driven benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16604">LC4SV: A Denoising Framework Learning to Compensate for Unseen Speaker Verification Models. (arXiv:2311.16604v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1">Chi-Chang Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1">Hong-Wei Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1">Chu-Song Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1">Hsin-Min Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1">Tsung-Te Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1">Yu Tsao</a></p>
<p>The performance of speaker verification (SV) models may drop dramatically in
noisy environments. A speech enhancement (SE) module can be used as a front-end
strategy. However, existing SE methods may fail to bring performance
improvements to downstream SV systems due to artifacts in the predicted signals
of SE models. To compensate for artifacts, we propose a generic denoising
framework named LC4SV, which can serve as a pre-processor for various unknown
downstream SV models. In LC4SV, we employ a learning-based interpolation agent
to automatically generate the appropriate coefficients between the enhanced
signal and its noisy input to improve SV performance in noisy environments. Our
experimental results demonstrate that LC4SV consistently improves the
performance of various unseen SV systems. To the best of our knowledge, this
work is the first attempt to develop a learning-based interpolation scheme
aiming at improving SV performance in noisy environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16605">LasTGL: An Industrial Framework for Large-Scale Temporal Graph Learning. (arXiv:2311.16605v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jintang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_J/0/1/0/all/0/1">Jiawang Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Ruofan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jing Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1">Sheng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baokun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Changhua Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuchang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zibin Zheng</a></p>
<p>Over the past few years, graph neural networks (GNNs) have become powerful
and practical tools for learning on (static) graph-structure data. However,
many real-world applications, such as social networks and e-commerce, involve
temporal graphs where nodes and edges are dynamically evolving. Temporal graph
neural networks (TGNNs) have progressively emerged as an extension of GNNs to
address time-evolving graphs and have gradually become a trending research
topic in both academics and industry. Advancing research in such an emerging
field requires new tools to compose TGNN models and unify their different
schemes in dealing with temporal graphs. To facilitate research and application
in temporal graph learning, we introduce LasTGL, an industrial framework that
integrates unified and extensible implementations of common temporal graph
learning algorithms for various advanced tasks. The purpose of LasTGL is to
provide the essential building blocks for solving temporal graph learning
tasks, focusing on the guiding principles of user-friendliness and quick
prototyping on which PyTorch is based. In particular, LasTGL provides
comprehensive temporal graph datasets, TGNN models and utilities along with
well-documented tutorials, making it suitable for both absolute beginners and
expert deep learning practitioners alike.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16609">Eigenmatrix for unstructured sparse recovery. (arXiv:2311.16609v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Ying_L/0/1/0/all/0/1">Lexing Ying</a></p>
<p>This paper considers the unstructured sparse recovery problems in a general
form. Examples include rational approximation, spectral function estimation,
Fourier inversion, Laplace inversion, and sparse deconvolution. The main
challenges are the noise in the sample values and the unstructured nature of
the sample locations. This paper proposes the eigenmatrix, a data-driven
construction with desired approximate eigenvalues and eigenvectors. The
eigenmatrix offers a new way for these sparse recovery problems. Numerical
results are provided to demonstrate the efficiency of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16614">A Multivariate Unimodality Test Harnenssing the Dip Statistic of Mahalanobis Distances Over Random Projections. (arXiv:2311.16614v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kolyvakis_P/0/1/0/all/0/1">Prodromos Kolyvakis</a>, <a href="http://arxiv.org/find/stat/1/au:+Likas_A/0/1/0/all/0/1">Aristidis Likas</a></p>
<p>Unimodality, pivotal in statistical analysis, offers insights into dataset
structures and drives sophisticated analytical procedures. While unimodality's
confirmation is straightforward for one-dimensional data using methods like
Silverman's approach and Hartigans' dip statistic, its generalization to higher
dimensions remains challenging. By extrapolating one-dimensional unimodality
principles to multi-dimensional spaces through linear random projections and
leveraging point-to-point distancing, our method, rooted in
$\alpha$-unimodality assumptions, presents a novel multivariate unimodality
test named mud-pod. Both theoretical and empirical studies confirm the efficacy
of our method in unimodality assessment of multidimensional datasets as well as
in estimating the number of clusters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16616">Adversarial Distribution Balancing for Counterfactual Reasoning. (arXiv:2311.16616v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schrod_S/0/1/0/all/0/1">Stefan Schrod</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1">Fabian Sinz</a>, <a href="http://arxiv.org/find/cs/1/au:+Altenbuchinger_M/0/1/0/all/0/1">Michael Altenbuchinger</a></p>
<p>The development of causal prediction models is challenged by the fact that
the outcome is only observable for the applied (factual) intervention and not
for its alternatives (the so-called counterfactuals); in medicine we only know
patients' survival for the administered drug and not for other therapeutic
options. Machine learning approaches for counterfactual reasoning have to deal
with both unobserved outcomes and distributional differences due to non-random
treatment administration. Unsupervised domain adaptation (UDA) addresses
similar issues; one has to deal with unobserved outcomes -- the labels of the
target domain -- and distributional differences between source and target
domain. We propose Adversarial Distribution Balancing for Counterfactual
Reasoning (ADBCR), which directly uses potential outcome estimates of the
counterfactuals to remove spurious causal relations. We show that ADBCR
outcompetes state-of-the-art methods on three benchmark datasets, and
demonstrate that ADBCR's performance can be further improved if unlabeled
validation data are included in the training procedure to better adapt the
model to the validation domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16620">On the Long Range Abilities of Transformers. (arXiv:2311.16620v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zimerman_I/0/1/0/all/0/1">Itamar Zimerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lior Wolf</a></p>
<p>Despite their dominance in modern DL and, especially, NLP domains,
transformer architectures exhibit sub-optimal performance on long-range tasks
compared to recent layers that are specifically designed for this purpose. In
this work, drawing inspiration from key attributes of long-range layers, such
as state-space layers, linear RNN layers, and global convolution layers, we
demonstrate that minimal modifications to the transformer architecture can
significantly enhance performance on the Long Range Arena (LRA) benchmark, thus
narrowing the gap with these specialized layers. We identify that two key
principles for long-range tasks are (i) incorporating an inductive bias towards
smoothness, and (ii) locality. As we show, integrating these ideas into the
attention mechanism improves results with a negligible amount of additional
computation and without any additional trainable parameters. Our theory and
experiments also shed light on the reasons for the inferior performance of
transformers on long-range tasks and identify critical properties that are
essential for successfully capturing long-range dependencies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16621">Beyond Labels: Advancing Cluster Analysis with the Entropy of Distance Distribution (EDD). (arXiv:2311.16621v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Metzner_C/0/1/0/all/0/1">Claus Metzner</a>, <a href="http://arxiv.org/find/stat/1/au:+Schilling_A/0/1/0/all/0/1">Achim Schilling</a>, <a href="http://arxiv.org/find/stat/1/au:+Krauss_P/0/1/0/all/0/1">Patrick Krauss</a></p>
<p>In the evolving landscape of data science, the accurate quantification of
clustering in high-dimensional data sets remains a significant challenge,
especially in the absence of predefined labels. This paper introduces a novel
approach, the Entropy of Distance Distribution (EDD), which represents a
paradigm shift in label-free clustering analysis. Traditional methods, reliant
on discrete labels, often struggle to discern intricate cluster patterns in
unlabeled data. EDD, however, leverages the characteristic differences in
pairwise point-to-point distances to discern clustering tendencies, independent
of data labeling.
</p>
<p>Our method employs the Shannon information entropy to quantify the
'peakedness' or 'flatness' of distance distributions in a data set. This
entropy measure, normalized against its maximum value, effectively
distinguishes between strongly clustered data (indicated by pronounced peaks in
distance distribution) and more homogeneous, non-clustered data sets. This
label-free quantification is resilient against global translations and
permutations of data points, and with an additional dimension-wise z-scoring,
it becomes invariant to data set scaling.
</p>
<p>We demonstrate the efficacy of EDD through a series of experiments involving
two-dimensional data spaces with Gaussian cluster centers. Our findings reveal
a monotonic increase in the EDD value with the widening of cluster widths,
moving from well-separated to overlapping clusters. This behavior underscores
the method's sensitivity and accuracy in detecting varying degrees of
clustering. EDD's potential extends beyond conventional clustering analysis,
offering a robust, scalable tool for unraveling complex data structures without
reliance on pre-assigned labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16625">Gaussian Processes for Monitoring Air-Quality in Kampala. (arXiv:2311.16625v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stoddart_C/0/1/0/all/0/1">Clara Stoddart</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrack_L/0/1/0/all/0/1">Lauren Shrack</a>, <a href="http://arxiv.org/find/cs/1/au:+Sserunjogi_R/0/1/0/all/0/1">Richard Sserunjogi</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdul_Ganiy_U/0/1/0/all/0/1">Usman Abdul-Ganiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bainomugisha_E/0/1/0/all/0/1">Engineer Bainomugisha</a>, <a href="http://arxiv.org/find/cs/1/au:+Okure_D/0/1/0/all/0/1">Deo Okure</a>, <a href="http://arxiv.org/find/cs/1/au:+Misener_R/0/1/0/all/0/1">Ruth Misener</a>, <a href="http://arxiv.org/find/cs/1/au:+Folch_J/0/1/0/all/0/1">Jose Pablo Folch</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedgwick_R/0/1/0/all/0/1">Ruby Sedgwick</a></p>
<p>Monitoring air pollution is of vital importance to the overall health of the
population. Unfortunately, devices that can measure air quality can be
expensive, and many cities in low and middle-income countries have to rely on a
sparse allocation of them. In this paper, we investigate the use of Gaussian
Processes for both nowcasting the current air-pollution in places where there
are no sensors and forecasting the air-pollution in the future at the sensor
locations. In particular, we focus on the city of Kampala in Uganda, using data
from AirQo's network of sensors. We demonstrate the advantage of removing
outliers, compare different kernel functions and additional inputs. We also
compare two sparse approximations to allow for the large amounts of temporal
data in the dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16628">Symmetry-regularized neural ordinary differential equations. (arXiv:2311.16628v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hao_W/0/1/0/all/0/1">Wenbo Hao</a></p>
<p>Neural Ordinary Differential Equations (Neural ODEs) is a class of deep
neural network models that interpret the hidden state dynamics of neural
networks as an ordinary differential equation, thereby capable of capturing
system dynamics in a continuous time framework. In this work, I integrate
symmetry regularization into Neural ODEs. In particular, I use continuous Lie
symmetry of ODEs and PDEs associated with the model to derive conservation laws
and add them to the loss function, making it physics-informed. This
incorporation of inherent structural properties into the loss function could
significantly improve robustness and stability of the model during training. To
illustrate this method, I employ a toy model that utilizes a cosine rate of
change in the hidden state, showcasing the process of identifying Lie
symmetries, deriving conservation laws, and constructing a new loss function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16630">Outfit Completion via Conditional Set Transformation. (arXiv:2311.16630v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nakamura_T/0/1/0/all/0/1">Takuma Nakamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuki Saito</a>, <a href="http://arxiv.org/find/cs/1/au:+Goto_R/0/1/0/all/0/1">Ryosuke Goto</a></p>
<p>In this paper, we formulate the outfit completion problem as a set retrieval
task and propose a novel framework for solving this problem. The proposal
includes a conditional set transformation architecture with deep neural
networks and a compatibility-based regularization method. The proposed method
utilizes a map with permutation-invariant for the input set and
permutation-equivariant for the condition set. This allows retrieving a set
that is compatible with the input set while reflecting the properties of the
condition set. In addition, since this structure outputs the element of the
output set in a single inference, it can achieve a scalable inference speed
with respect to the cardinality of the output set. Experimental results on real
data reveal that the proposed method outperforms existing approaches in terms
of accuracy of the outfit completion task, condition satisfaction, and
compatibility of completion results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16632">Opening the Black Box: Towards inherently interpretable energy data imputation models using building physics insight. (arXiv:2311.16632v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Liguori_A/0/1/0/all/0/1">Antonio Liguori</a>, <a href="http://arxiv.org/find/stat/1/au:+Quintana_M/0/1/0/all/0/1">Matias Quintana</a>, <a href="http://arxiv.org/find/stat/1/au:+Fu_C/0/1/0/all/0/1">Chun Fu</a>, <a href="http://arxiv.org/find/stat/1/au:+Miller_C/0/1/0/all/0/1">Clayton Miller</a>, <a href="http://arxiv.org/find/stat/1/au:+Frisch_J/0/1/0/all/0/1">J&#xe9;r&#xf4;me Frisch</a>, <a href="http://arxiv.org/find/stat/1/au:+Treeck_C/0/1/0/all/0/1">Christoph van Treeck</a></p>
<p>Missing data are frequently observed by practitioners and researchers in the
building energy modeling community. In this regard, advanced data-driven
solutions, such as Deep Learning methods, are typically required to reflect the
non-linear behavior of these anomalies. As an ongoing research question related
to Deep Learning, a model's applicability to limited data settings can be
explored by introducing prior knowledge in the network. This same strategy can
also lead to more interpretable predictions, hence facilitating the field
application of the approach. For that purpose, the aim of this paper is to
propose the use of Physics-informed Denoising Autoencoders (PI-DAE) for missing
data imputation in commercial buildings. In particular, the presented method
enforces physics-inspired soft constraints to the loss function of a Denoising
Autoencoder (DAE). In order to quantify the benefits of the physical component,
an ablation study between different DAE configurations is conducted. First,
three univariate DAEs are optimized separately on indoor air temperature,
heating, and cooling data. Then, two multivariate DAEs are derived from the
previous configurations. Eventually, a building thermal balance equation is
coupled to the last multivariate configuration to obtain PI-DAE. Additionally,
two commonly used benchmarks are employed to support the findings. It is shown
how introducing physical knowledge in a multivariate Denoising Autoencoder can
enhance the inherent model interpretability through the optimized physics-based
coefficients. While no significant improvement is observed in terms of
reconstruction error with the proposed PI-DAE, its enhanced robustness to
varying rates of missing data and the valuable insights derived from the
physics-based coefficients create opportunities for wider applications within
building systems and the built environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16646">Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective. (arXiv:2311.16646v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1">Ming-Yu Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1">Sheng-Yen Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chia-Mu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_S/0/1/0/all/0/1">Sy-Yen Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1">Tsung-Yi Ho</a></p>
<p>Dataset distillation offers a potential means to enhance data efficiency in
deep learning. Recent studies have shown its ability to counteract backdoor
risks present in original training samples. In this study, we delve into the
theoretical aspects of backdoor attacks and dataset distillation based on
kernel methods. We introduce two new theory-driven trigger pattern generation
methods specialized for dataset distillation. Following a comprehensive set of
analyses and experiments, we show that our optimization-based trigger design
framework informs effective backdoor attacks on dataset distillation. Notably,
datasets poisoned by our designed trigger prove resilient against conventional
backdoor attack detection and mitigation methods. Our empirical results
validate that the triggers developed using our approaches are proficient at
executing resilient backdoor attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16654">Elucidating Discrepancy in Explanations of Predictive Models Developed using EMR. (arXiv:2311.16654v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brankovic_A/0/1/0/all/0/1">Aida Brankovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenjie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cook_D/0/1/0/all/0/1">David Cook</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1">Sankalp Khanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Bialkowski_K/0/1/0/all/0/1">Konstanty Bialkowski</a></p>
<p>The lack of transparency and explainability hinders the clinical adoption of
Machine learning (ML) algorithms. While explainable artificial intelligence
(XAI) methods have been proposed, little research has focused on the agreement
between these methods and expert clinical knowledge. This study applies current
state-of-the-art explainability methods to clinical decision support algorithms
developed for Electronic Medical Records (EMR) data to analyse the concordance
between these factors and discusses causes for identified discrepancies from a
clinical and technical perspective. Important factors for achieving trustworthy
XAI solutions for clinical decision support are also discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16656">Pseudo-Likelihood Inference. (arXiv:2311.16656v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gruner_T/0/1/0/all/0/1">Theo Gruner</a>, <a href="http://arxiv.org/find/cs/1/au:+Belousov_B/0/1/0/all/0/1">Boris Belousov</a>, <a href="http://arxiv.org/find/cs/1/au:+Muratore_F/0/1/0/all/0/1">Fabio Muratore</a>, <a href="http://arxiv.org/find/cs/1/au:+Palenicek_D/0/1/0/all/0/1">Daniel Palenicek</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1">Jan Peters</a></p>
<p>Simulation-Based Inference (SBI) is a common name for an emerging family of
approaches that infer the model parameters when the likelihood is intractable.
Existing SBI methods either approximate the likelihood, such as Approximate
Bayesian Computation (ABC) or directly model the posterior, such as Sequential
Neural Posterior Estimation (SNPE). While ABC is efficient on low-dimensional
problems, on higher-dimensional tasks, it is generally outperformed by SNPE,
which leverages function approximation. In this paper, we propose
Pseudo-Likelihood Inference (PLI), a new method that brings neural
approximation into ABC, making it competitive on challenging Bayesian system
identification tasks. By utilizing integral probability metrics, we introduce a
smooth likelihood kernel with an adaptive bandwidth that is updated based on
information-theoretic trust regions. Thanks to this formulation, our method (i)
allows for optimizing neural posteriors via gradient descent, (ii) does not
rely on summary statistics, and (iii) enables multiple observations as input.
In comparison to SNPE, it leads to improved performance when more data is
available. The effectiveness of PLI is evaluated on four classical SBI
benchmark tasks and on a highly dynamic physical system, showing particular
advantages on stochastic simulations and multi-modal posterior landscapes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16666">MultiModal-Learning for Predicting Molecular Properties: A Framework Based on Image and Graph Structures. (arXiv:2311.16666v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhuoyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_J/0/1/0/all/0/1">Jiacong Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jieyue He</a></p>
<p>The quest for accurate prediction of drug molecule properties poses a
fundamental challenge in the realm of Artificial Intelligence Drug Discovery
(AIDD). An effective representation of drug molecules emerges as a pivotal
component in this pursuit. Contemporary leading-edge research predominantly
resorts to self-supervised learning (SSL) techniques to extract meaningful
structural representations from large-scale, unlabeled molecular data,
subsequently fine-tuning these representations for an array of downstream
tasks. However, an inherent shortcoming of these studies lies in their singular
reliance on one modality of molecular information, such as molecule image or
SMILES representations, thus neglecting the potential complementarity of
various molecular modalities. In response to this limitation, we propose MolIG,
a novel MultiModaL molecular pre-training framework for predicting molecular
properties based on Image and Graph structures. MolIG model innovatively
leverages the coherence and correlation between molecule graph and molecule
image to execute self-supervised tasks, effectively amalgamating the strengths
of both molecular representation forms. This holistic approach allows for the
capture of pivotal molecular structural characteristics and high-level semantic
information. Upon completion of pre-training, Graph Neural Network (GNN)
Encoder is used for the prediction of downstream tasks. In comparison to
advanced baseline models, MolIG exhibits enhanced performance in downstream
tasks pertaining to molecular property prediction within benchmark groups such
as MoleculeNet Benchmark Group and ADMET Benchmark Group.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16670">PyTorch Geometric High Order: A Unified Library for High Order Graph Neural Network. (arXiv:2311.16670v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a></p>
<p>We introduce PyTorch Geometric High Order (PyGHO), a library for High Order
Graph Neural Networks (HOGNNs) that extends PyTorch Geometric (PyG). Unlike
ordinary Message Passing Neural Networks (MPNNs) that exchange messages between
nodes, HOGNNs, encompassing subgraph GNNs and k-WL GNNs, encode node tuples, a
method previously lacking a standardized framework and often requiring complex
coding. PyGHO's main objective is to provide an unified and user-friendly
interface for various HOGNNs. It accomplishes this through streamlined data
structures for node tuples, comprehensive data processing utilities, and a
flexible suite of operators for high-order GNN methodologies. In this work, we
present a detailed in-depth of PyGHO and compare HOGNNs implemented with PyGHO
with their official implementation on real-world tasks. PyGHO achieves up to
$50\%$ acceleration and reduces the code needed for implementation by an order
of magnitude. Our library is available at
\url{https://github.com/GraphPKU/PygHO}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16683">Hyper-Relational Knowledge Graph Neural Network for Next POI. (arXiv:2311.16683v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jixiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongkang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1">Ruotong Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zipei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xuan Song</a></p>
<p>With the advancement of mobile technology, Point of Interest (POI)
recommendation systems in Location-based Social Networks (LBSN) have brought
numerous benefits to both users and companies. Many existing works employ
Knowledge Graph (KG) to alleviate the data sparsity issue in LBSN. These
approaches primarily focus on modeling the pair-wise relations in LBSN to
enrich the semantics and thereby relieve the data sparsity issue. However,
existing approaches seldom consider the hyper-relations in LBSN, such as the
mobility relation (a 3-ary relation: user-POI-time). This makes the model hard
to exploit the semantics accurately. In addition, prior works overlook the rich
structural information inherent in KG, which consists of higher-order relations
and can further alleviate the impact of data sparsity.To this end, we propose a
Hyper-Relational Knowledge Graph Neural Network (HKGNN) model. In HKGNN, a
Hyper-Relational Knowledge Graph (HKG) that models the LBSN data is constructed
to maintain and exploit the rich semantics of hyper-relations. Then we proposed
a Hypergraph Neural Network to utilize the structural information of HKG in a
cohesive way. In addition, a self-attention network is used to leverage
sequential information and make personalized recommendations. Furthermore, side
information, essential in reducing data sparsity by providing background
knowledge of POIs, is not fully utilized in current methods. In light of this,
we extended the current dataset with available side information to further
lessen the impact of data sparsity. Results of experiments on four real-world
LBSN datasets demonstrate the effectiveness of our approach compared to
existing state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16700">Rethinking Intermediate Layers design in Knowledge Distillation for Kidney and Liver Tumor Segmentation. (arXiv:2311.16700v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorade_V/0/1/0/all/0/1">Vandan Gorade</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1">Sparsh Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1">Debesh Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1">Ulas Bagci</a></p>
<p>Knowledge distillation(KD) has demonstrated remarkable success across various
domains, but its application to medical imaging tasks, such as kidney and liver
tumor segmentation, has encountered challenges. Many existing KD methods are
not specifically tailored for these tasks. Moreover, prevalent KD methods often
lack a careful consideration of what and from where to distill knowledge from
the teacher to the student. This oversight may lead to issues like the
accumulation of training bias within shallower student layers, potentially
compromising the effectiveness of KD. To address these challenges, we propose
Hierarchical Layer-selective Feedback Distillation (HLFD). HLFD strategically
distills knowledge from a combination of middle layers to earlier layers and
transfers final layer knowledge to intermediate layers at both the feature and
pixel levels. This design allows the model to learn higher-quality
representations from earlier layers, resulting in a robust and compact student
model. Extensive quantitative evaluations reveal that HLFD outperforms existing
methods by a significant margin. For example, in the kidney segmentation task,
HLFD surpasses the student model (without KD) by over 10pp, significantly
improving its focus on tumor-specific features. From a qualitative standpoint,
the student model trained using HLFD excels at suppressing irrelevant
information and can focus sharply on tumor-specific details, which opens a new
pathway for more efficient and accurate diagnostic tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16706">Sinkhorn Flow: A Continuous-Time Framework for Understanding and Generalizing the Sinkhorn Algorithm. (arXiv:2311.16706v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karimi_M/0/1/0/all/0/1">Mohammad Reza Karimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_Y/0/1/0/all/0/1">Ya-Ping Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a></p>
<p>Many problems in machine learning can be formulated as solving
entropy-regularized optimal transport on the space of probability measures. The
canonical approach involves the Sinkhorn iterates, renowned for their rich
mathematical properties. Recently, the Sinkhorn algorithm has been recast
within the mirror descent framework, thus benefiting from classical
optimization theory insights. Here, we build upon this result by introducing a
continuous-time analogue of the Sinkhorn algorithm. This perspective allows us
to derive novel variants of Sinkhorn schemes that are robust to noise and bias.
Moreover, our continuous-time dynamics not only generalize but also offer a
unified perspective on several recently discovered dynamics in machine learning
and mathematics, such as the "Wasserstein mirror flow" of (Deb et al. 2023) or
the "mean-field Schr\"odinger equation" of (Claisse et al. 2023).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16711">LEDITS++: Limitless Image Editing using Text-to-Image Models. (arXiv:2311.16711v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1">Manuel Brack</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1">Felix Friedrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornmeier_K/0/1/0/all/0/1">Katharina Kornmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsaban_L/0/1/0/all/0/1">Linoy Tsaban</a>, <a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1">Patrick Schramowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a>, <a href="http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1">Apolin&#xe1;rio Passos</a></p>
<p>Text-to-image diffusion models have recently received increasing interest for
their astonishing ability to produce high-fidelity images from solely text
inputs. Subsequent research efforts aim to exploit and apply their capabilities
to real image editing. However, existing image-to-image methods are often
inefficient, imprecise, and of limited versatility. They either require
time-consuming fine-tuning, deviate unnecessarily strongly from the input
image, and/or lack support for multiple, simultaneous edits. To address these
issues, we introduce LEDITS++, an efficient yet versatile and precise textual
image manipulation technique. LEDITS++'s novel inversion approach requires no
tuning nor optimization and produces high-fidelity results with a few diffusion
steps. Second, our methodology supports multiple simultaneous edits and is
architecture-agnostic. Third, we use a novel implicit masking technique that
limits changes to relevant image regions. We propose the novel TEdBench++
benchmark as part of our exhaustive evaluation. Our results demonstrate the
capabilities of LEDITS++ and its improvements over previous methods. The
project page is available at https://leditsplusplus-project.static.hf.space .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16727">Sluggish and Chemically-Biased Interstitial Diffusion in Concentrated Solid Solution Alloys: Mechanisms and Methods. (arXiv:2311.16727v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Xu_B/0/1/0/all/0/1">Biao Xu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fu_H/0/1/0/all/0/1">Haijun Fu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Huang_S/0/1/0/all/0/1">Shasha Huang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Ma_S/0/1/0/all/0/1">Shihua Ma</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Xiong_Y/0/1/0/all/0/1">Yaoxu Xiong</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Xiang_X/0/1/0/all/0/1">Xuepeng Xiang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lu_W/0/1/0/all/0/1">Wenyu Lu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kai_J/0/1/0/all/0/1">Ji-Jung Kai</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhao_S/0/1/0/all/0/1">Shijun Zhao</a></p>
<p>Interstitial diffusion is a pivotal process that governs the phase stability
and irradiation response of materials in non-equilibrium conditions. In this
work, we study sluggish and chemically-biased interstitial diffusion in Fe-Ni
concentrated solid solution alloys (CSAs) by combining machine learning (ML)
and kinetic Monte Carlo (kMC), where ML is used to accurately and efficiently
predict the migration energy barriers on-the-fly. The ML-kMC reproduces the
diffusivity that was reported by molecular dynamics results at high
temperatures. With this powerful tool, we find that the observed sluggish
diffusion and the "Ni-Ni-Ni"-biased diffusion in Fe-Ni alloys are ascribed to a
unique "Barrier Lock" mechanism, whereas the "Fe-Fe-Fe"-biased diffusion is
influenced by a "Component Dominance" mechanism. Inspired by the mentioned
mechanisms, a practical AvgS-kMC method is proposed for conveniently and
swiftly determining interstitial-mediated diffusivity by only relying on the
mean energy barriers of migration patterns. Combining the AvgS-kMC with the
differential evolutionary algorithm, an inverse design strategy for optimizing
sluggish diffusion properties is applied to emphasize the crucial role of
favorable migration patterns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16741">Asynchronous Wireless Federated Learning with Probabilistic Client Selection. (arXiv:2311.16741v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiarong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fangjiong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Changle Li</a></p>
<p>Federated learning (FL) is a promising distributed learning framework where
distributed clients collaboratively train a machine learning model coordinated
by a server. To tackle the stragglers issue in asynchronous FL, we consider
that each client keeps local updates and probabilistically transmits the local
model to the server at arbitrary times. We first derive the (approximate)
expression for the convergence rate based on the probabilistic client
selection. Then, an optimization problem is formulated to trade off the
convergence rate of asynchronous FL and mobile energy consumption by joint
probabilistic client selection and bandwidth allocation. We develop an
iterative algorithm to solve the non-convex problem globally optimally.
Experiments demonstrate the superiority of the proposed approach compared with
the traditional schemes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16766">Rescuing referral failures during automated diagnosis of domain-shifted medical images. (arXiv:2311.16766v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1">Anuj Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1">Karm Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1">Pradeep Shenoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1">Devarajan Sridharan</a></p>
<p>The success of deep learning models deployed in the real world depends
critically on their ability to generalize well across diverse data domains.
Here, we address a fundamental challenge with selective classification during
automated diagnosis with domain-shifted medical images. In this scenario,
models must learn to avoid making predictions when label confidence is low,
especially when tested with samples far removed from the training set
(covariate shift). Such uncertain cases are typically referred to the clinician
for further analysis and evaluation. Yet, we show that even state-of-the-art
domain generalization approaches fail severely during referral when tested on
medical images acquired from a different demographic or using a different
technology. We examine two benchmark diagnostic medical imaging datasets
exhibiting strong covariate shifts: i) diabetic retinopathy prediction with
retinal fundus images and ii) multilabel disease prediction with chest X-ray
images. We show that predictive uncertainty estimates do not generalize well
under covariate shifts leading to non-monotonic referral curves, and severe
drops in performance (up to 50%) at high referral rates (&gt;70%). We evaluate
novel combinations of robust generalization and post hoc referral approaches,
that rescue these failures and achieve significant performance improvements,
typically &gt;10%, over baseline methods. Our study identifies a critical
challenge with referral in domain-shifted medical images and finds key
applications in reliable, automated disease diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16769">Equilibrium in the Computing Continuum through Active Inference. (arXiv:2311.16769v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sedlak_B/0/1/0/all/0/1">Boris Sedlak</a>, <a href="http://arxiv.org/find/cs/1/au:+Pujol_V/0/1/0/all/0/1">Victor Casamayor Pujol</a>, <a href="http://arxiv.org/find/cs/1/au:+Donta_P/0/1/0/all/0/1">Praveen Kumar Donta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dustdar_S/0/1/0/all/0/1">Schahram Dustdar</a></p>
<p>Computing Continuum (CC) systems are challenged to ensure the intricate
requirements of each computational tier. Given the system's scale, the Service
Level Objectives (SLOs) which are expressed as these requirements, must be
broken down into smaller parts that can be decentralized. We present our
framework for collaborative edge intelligence enabling individual edge devices
to (1) develop a causal understanding of how to enforce their SLOs, and (2)
transfer knowledge to speed up the onboarding of heterogeneous devices. Through
collaboration, they (3) increase the scope of SLO fulfillment. We implemented
the framework and evaluated a use case in which a CC system is responsible for
ensuring Quality of Service (QoS) and Quality of Experience (QoE) during video
streaming. Our results showed that edge devices required only ten training
rounds to ensure four SLOs; furthermore, the underlying causal structures were
also rationally explainable. The addition of new types of devices can be done a
posteriori, the framework allowed them to reuse existing models, even though
the device type had been unknown. Finally, rebalancing the load within a device
cluster allowed individual edge devices to recover their SLO compliance after a
network failure from 22% to 89%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.10784">OccamNet: A Fast Neural Model for Symbolic Regression at Scale. (arXiv:2007.10784v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dugan_O/0/1/0/all/0/1">Owen Dugan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1">Rumen Dangovski</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1">Allan Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Samuel Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1">Joseph Jacobson</a>, <a href="http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1">Marin Solja&#x10d;i&#x107;</a></p>
<p>Neural networks' expressiveness comes at the cost of complex, black-box
models that often extrapolate poorly beyond the domain of the training dataset,
conflicting with the goal of finding compact analytic expressions to describe
scientific data. We introduce OccamNet, a neural network model that finds
interpretable, compact, and sparse symbolic fits to data, \`a la Occam's razor.
Our model defines a probability distribution over functions with efficient
sampling and function evaluation. We train by sampling functions and biasing
the probability mass toward better fitting solutions, backpropagating using
cross-entropy matching in a reinforcement-learning loss. OccamNet can identify
symbolic fits for a variety of problems, including analytic and non-analytic
functions, implicit functions, and simple image classification, and can
outperform state-of-the-art symbolic regression methods on real-world
regression datasets. Our method requires a minimal memory footprint, fits
complicated functions in minutes on a single CPU, and scales on a GPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.04840">Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v5 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1">Andreas Madsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a></p>
<p>Neural networks for NLP are becoming increasingly complex and widespread, and
there is a growing concern if these models are responsible to use. Explaining
models helps to address the safety and ethical concerns and is essential for
accountability. Interpretability serves to provide these explanations in terms
that are understandable to humans. Additionally, post-hoc methods provide
explanations after a model is learned and are generally model-agnostic. This
survey provides a categorization of how recent post-hoc interpretability
methods communicate explanations to humans, it discusses each method in-depth,
and how they are validated, as the latter is often a common concern.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.10085">Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors. (arXiv:2111.10085v4 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Ruoxi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1">Minhui Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Tyson_G/0/1/0/all/0/1">Gareth Tyson</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1">Tian Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaofeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Haojin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1">Seyit Camtepe</a>, <a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1">Surya Nepal</a></p>
<p>Numerous open-source and commercial malware detectors are available. However,
their efficacy is threatened by new adversarial attacks, whereby malware
attempts to evade detection, e.g., by performing feature-space manipulation. In
this work, we propose an explainability-guided and model-agnostic testing
framework for robustness of malware detectors when confronted with adversarial
attacks. The framework introduces the concept of Accrued Malicious Magnitude
(AMM) to identify which malware features could be manipulated to maximize the
likelihood of evading detection. We then use this framework to test several
state-of-the-art malware detectors' abilities to detect manipulated malware. We
find that (i) commercial antivirus engines are vulnerable to AMM-guided test
cases; (ii) the ability of a manipulated malware generated using one detector
to evade detection by another detector (i.e., transferability) depends on the
overlap of features with large AMM values between the different detectors; and
(iii) AMM values effectively measure the fragility of features (i.e.,
capability of feature-space manipulation to flip the prediction results) and
explain the robustness of malware detectors facing evasion attacks. Our
findings shed light on the limitations of current malware detectors, as well as
how they can be improved.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.05760">Big Data Analytics for Network Level Short-Term Travel Time Prediction with Hierarchical LSTM. (arXiv:2201.05760v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianya T. Zhang</a></p>
<p>The travel time data collected from widespread traffic monitoring sensors
necessitate big data analytic tools for querying, visualization, and
identifying meaningful traffic patterns. This paper utilizes a large-scale
travel time dataset from Caltrans Performance Measurement System (PeMS) system
that is an overflow for traditional data processing and modeling tools. To
overcome the challenges of the massive amount of data, the big data analytic
engines Apache Spark and Apache MXNet are applied for data wrangling and
modeling. Seasonality and autocorrelation were performed to explore and
visualize the trend of time-varying data. Inspired by the success of the
hierarchical architecture for many Artificial Intelligent (AI) tasks, we
consolidate the cell and hidden states passed from low-level to the high-level
LSTM with an attention pooling similar to how the human perception system
operates. The designed hierarchical LSTM model can consider the dependencies at
different time scales to capture the spatial-temporal correlations of
network-level travel time. Another self-attention module is then devised to
connect LSTM extracted features to the fully connected layers, predicting
travel time for all corridors instead of a single link/route. The comparison
results show that the Hierarchical LSTM with Attention (HierLSTMat) model gives
the best prediction results at 30-minute and 45-min horizons and can
successfully forecast unusual congestion. The efficiency gained from big data
analytic tools was evaluated by comparing them with popular data science and
deep learning frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.01370">Towards Improving the Generation Quality of Autoregressive Slot VAEs. (arXiv:2206.01370v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emami_P/0/1/0/all/0/1">Patrick Emami</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1">Sanjay Ranka</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1">Anand Rangarajan</a></p>
<p>Unconditional scene inference and generation are challenging to learn jointly
with a single compositional model. Despite encouraging progress on models that
extract object-centric representations (''slots'') from images, unconditional
generation of scenes from slots has received less attention. This is primarily
because learning the multi-object relations necessary to imagine coherent
scenes is difficult. We hypothesize that most existing slot-based models have a
limited ability to learn object correlations. We propose two improvements that
strengthen object correlation learning. The first is to condition the slots on
a global, scene-level variable that captures higher-order correlations between
slots. Second, we address the fundamental lack of a canonical order for objects
in images by proposing to learn a consistent order to use for the
autoregressive generation of scene objects. Specifically, we train an
autoregressive slot prior to sequentially generate scene objects following a
learned order. Ordered slot inference entails first estimating a randomly
ordered set of slots using existing approaches for extracting slots from
images, then aligning those slots to ordered slots generated autoregressively
with the slot prior. Our experiments across three multi-object environments
demonstrate clear gains in unconditional scene generation quality. Detailed
ablation studies are also provided that validate the two proposed improvements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.08164">Long Range Graph Benchmark. (arXiv:2206.08164v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_V/0/1/0/all/0/1">Vijay Prakash Dwivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1">Ladislav Ramp&#xe1;&#x161;ek</a>, <a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1">Mikhail Galkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Parviz_A/0/1/0/all/0/1">Ali Parviz</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1">Guy Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1">Anh Tuan Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Beaini_D/0/1/0/all/0/1">Dominique Beaini</a></p>
<p>Graph Neural Networks (GNNs) that are based on the message passing (MP)
paradigm generally exchange information between 1-hop neighbors to build node
representations at each layer. In principle, such networks are not able to
capture long-range interactions (LRI) that may be desired or necessary for
learning a given task on graphs. Recently, there has been an increasing
interest in development of Transformer-based methods for graphs that can
consider full node connectivity beyond the original sparse structure, thus
enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop
message passing often fare better in several existing graph benchmarks when
combined with positional feature representations, among other innovations,
hence limiting the perceived utility and ranking of Transformer-like
architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5
graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and
Peptides-struct that arguably require LRI reasoning to achieve strong
performance in a given task. We benchmark both baseline GNNs and Graph
Transformer networks to verify that the models which capture long-range
dependencies perform significantly better on these tasks. Therefore, these
datasets are suitable for benchmarking and exploration of MP-GNNs and Graph
Transformer architectures that are intended to capture LRI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.10479">Policy Learning with Asymmetric Counterfactual Utilities. (arXiv:2206.10479v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ben_Michael_E/0/1/0/all/0/1">Eli Ben-Michael</a>, <a href="http://arxiv.org/find/stat/1/au:+Imai_K/0/1/0/all/0/1">Kosuke Imai</a>, <a href="http://arxiv.org/find/stat/1/au:+Jiang_Z/0/1/0/all/0/1">Zhichao Jiang</a></p>
<p>Data-driven decision making plays an important role even in high stakes
settings like medicine and public policy. Learning optimal policies from
observed data requires a careful formulation of the utility function whose
expected value is maximized across a population. Although researchers typically
use utilities that depend on observed outcomes alone, in many settings the
decision maker's utility function is more properly characterized by the joint
set of potential outcomes under all actions. For example, the Hippocratic
principle to "do no harm" implies that the cost of causing death to a patient
who would otherwise survive without treatment is greater than the cost of
forgoing life-saving treatment. We consider optimal policy learning with
asymmetric counterfactual utility functions of this form that consider the
joint set of potential outcomes. We show that asymmetric counterfactual
utilities lead to an unidentifiable expected utility function, and so we first
partially identify it. Drawing on statistical decision theory, we then derive
minimax decision rules by minimizing the maximum expected utility loss relative
to different alternative policies. We show that one can learn minimax loss
decision rules from observed data by solving intermediate classification
problems, and establish that the finite sample excess expected utility loss of
this procedure is bounded by the regret of these intermediate classifiers. We
apply this conceptual framework and methodology to the decision about whether
or not to use right heart catheterization for patients with possible pulmonary
hypertension.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.13269">Wasserstein Distributionally Robust Estimation in High Dimensions: Performance Analysis and Optimal Hyperparameter Tuning. (arXiv:2206.13269v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Aolaritei_L/0/1/0/all/0/1">Liviu Aolaritei</a>, <a href="http://arxiv.org/find/stat/1/au:+Shafiee_S/0/1/0/all/0/1">Soroosh Shafiee</a>, <a href="http://arxiv.org/find/stat/1/au:+Dorfler_F/0/1/0/all/0/1">Florian D&#xf6;rfler</a></p>
<p>Wasserstein distributionally robust optimization has recently emerged as a
powerful framework for robust estimation, enjoying good out-of-sample
performance guarantees, well-understood regularization effects, and
computationally tractable reformulations. In such framework, the estimator is
obtained by minimizing the worst-case expected loss over all probability
distributions which are close, in a Wasserstein sense, to the empirical
distribution. In this paper, we propose a Wasserstein distributionally robust
estimation framework to estimate an unknown parameter from noisy linear
measurements, and we focus on the task of analyzing the squared error
performance of such estimators. Our study is carried out in the modern
high-dimensional proportional regime, where both the ambient dimension and the
number of samples go to infinity at a proportional rate which encodes the
under/over-parametrization of the problem. Under an isotropic Gaussian features
assumption, we show that the squared error can be recovered as the solution of
a convex-concave optimization problem which, surprinsingly, involves at most
four scalar variables. Importantly, the precise quantification of the squared
error allows to accurately and efficiently compare different ambiguity radii
and to understand the effect of the under/over-parametrization on the
estimation error. We conclude the paper with a list of exciting research
directions enabled by our results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.03017">ACHO: Adaptive Conformal Hyperparameter Optimization. (arXiv:2207.03017v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Doyle_R/0/1/0/all/0/1">Riccardo Doyle</a></p>
<p>Several novel frameworks for hyperparameter search have emerged in the last
decade, but most rely on strict, often normal, distributional assumptions,
limiting search model flexibility. This paper proposes a novel optimization
framework based on upper confidence bound sampling of conformal confidence
intervals, whose weaker assumption of exchangeability enables greater choice of
search model architectures. Several such architectures were explored and
benchmarked on hyperparameter search of random forests and convolutional neural
networks, displaying satisfactory interval coverage and superior tuning
performance to random search.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.08892">Distributed Differentiable Dynamic Game for Multi-robot Coordination. (arXiv:2207.08892v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yizhi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Wanxin Jin</a></p>
<p>This paper develops a Distributed Differentiable Dynamic Game (D3G)
framework, which can efficiently solve the forward and inverse problems in
multi-robot coordination. We formulate multi-robot coordination as a dynamic
game, where the behavior of a robot is dictated by its own dynamics and
objective that also depends on others' behavior. In the forward problem, D3G
enables all robots collaboratively to seek the Nash equilibrium of the game in
a distributed manner, by developing a distributed shooting-based Nash solver.
In the inverse problem, where each robot aims to find (learn) its objective
(and dynamics) parameters to mimic given coordination demonstrations, D3G
proposes a differentiation solver based on Differential Pontryagin's Maximum
Principle, which allows each robot to update its parameters in a distributed
and coordinated manner. We test the D3G in simulation with two types of robots
given different task configurations. The results demonstrate the effectiveness
of D3G for solving both forward and inverse problems in comparison with
existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.05228">FIXED: Frustratingly Easy Domain Generalization with Mixup. (arXiv:2211.05228v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiqiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>Domain generalization (DG) aims to learn a generalizable model from multiple
training domains such that it can perform well on unseen target domains. A
popular strategy is to augment training data to benefit generalization through
methods such as Mixup~\cite{zhang2018mixup}. While the vanilla Mixup can be
directly applied, theoretical and empirical investigations uncover several
shortcomings that limit its performance. Firstly, Mixup cannot effectively
identify the domain and class information that can be used for learning
invariant representations. Secondly, Mixup may introduce synthetic noisy data
points via random interpolation, which lowers its discrimination capability.
Based on the analysis, we propose a simple yet effective enhancement for
Mixup-based DG, namely domain-invariant Feature mIXup (FIX). It learns
domain-invariant representations for Mixup. To further enhance discrimination,
we leverage existing techniques to enlarge margins among classes to further
propose the domain-invariant Feature MIXup with Enhanced Discrimination (FIXED)
approach. We present theoretical insights about guarantees on its
effectiveness. Extensive experiments on seven public datasets across two
modalities including image classification (Digits-DG, PACS, Office-Home) and
time series (DSADS, PAMAP2, UCI-HAR, and USC-HAD) demonstrate that our approach
significantly outperforms nine state-of-the-art related methods, beating the
best performing baseline by 6.5\% on average in terms of test accuracy. Code is
available at:
https://github.com/jindongwang/transferlearning/tree/master/code/deep/fixed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13131">FeTrIL: Feature Translation for Exemplar-Free Class-Incremental Learning. (arXiv:2211.13131v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Petit_G/0/1/0/all/0/1">Gr&#xe9;goire Petit</a>, <a href="http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1">Adrian Popescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_H/0/1/0/all/0/1">Hugo Schindler</a>, <a href="http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1">David Picard</a>, <a href="http://arxiv.org/find/cs/1/au:+Delezoide_B/0/1/0/all/0/1">Bertrand Delezoide</a></p>
<p>Exemplar-free class-incremental learning is very challenging due to the
negative effect of catastrophic forgetting. A balance between stability and
plasticity of the incremental process is needed in order to obtain good
accuracy for past as well as new classes. Existing exemplar-free
class-incremental methods focus either on successive fine tuning of the model,
thus favoring plasticity, or on using a feature extractor fixed after the
initial incremental state, thus favoring stability. We introduce a method which
combines a fixed feature extractor and a pseudo-features generator to improve
the stability-plasticity balance. The generator uses a simple yet effective
geometric translation of new class features to create representations of past
classes, made of pseudo-features. The translation of features only requires the
storage of the centroid representations of past classes to produce their
pseudo-features. Actual features of new classes and pseudo-features of past
classes are fed into a linear classifier which is trained incrementally to
discriminate between all classes. The incremental process is much faster with
the proposed method compared to mainstream ones which update the entire deep
model. Experiments are performed with three challenging datasets, and different
incremental settings. A comparison with ten existing methods shows that our
method outperforms the others in most cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04548">STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow Prediction. (arXiv:2212.04548v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhaumik_K/0/1/0/all/0/1">Kishor Kumar Bhaumik</a>, <a href="http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1">Fahim Faisal Niloy</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmud_S/0/1/0/all/0/1">Saif Mahmud</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1">Simon Woo</a></p>
<p>Reliable forecasting of traffic flow requires efficient modeling of traffic
data. Different correlations and influences arise in a dynamic traffic network,
making modeling a complicated task. Existing literature has proposed many
different methods to capture the complex underlying spatial-temporal relations
of traffic networks. However, methods still struggle to capture different local
and global dependencies of long-range nature. Also, as more and more
sophisticated methods are being proposed, models are increasingly becoming
memory-heavy and, thus, unsuitable for low-powered devices. In this paper, we
focus on solving these problems by proposing a novel deep learning framework -
STLGRU. Specifically, our proposed STLGRU can effectively capture both local
and global spatial-temporal relations of a traffic network using
memory-augmented attention and gating mechanism. Instead of employing separate
temporal and spatial components, we show that our memory module and gated unit
can learn the spatial-temporal dependencies successfully, allowing for reduced
memory usage with fewer parameters. We extensively experiment on several
real-world traffic prediction datasets to show that our model performs better
than existing methods while the memory footprint remains lower. Code is
available at \url{https://github.com/Kishor-Bhaumik/STLGRU}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09744">DSI++: Updating Transformer Memory with New Documents. (arXiv:2212.09744v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1">Sanket Vaibhav Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1">Jai Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1">Yi Tay</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mostafa Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1">Vinh Q. Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1">Jinfeng Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1">Marc Najork</a>, <a href="http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1">Emma Strubell</a>, <a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1">Donald Metzler</a></p>
<p>Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10575">Trainable Loss Weights in Super-Resolution. (arXiv:2301.10575v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mellatshahi_A/0/1/0/all/0/1">Arash Chaichi Mellatshahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1">Shohreh Kasaei</a></p>
<p>In recent years, limited research has discussed the loss function in the
super-resolution process. The majority of those studies have only used
perceptual similarity conventionally. This is while the development of
appropriate loss can improve the quality of other methods as well. In this
article, a new weighting method for pixel-wise loss is proposed. With the help
of this method, it is possible to use trainable weights based on the general
structure of the image and its perceptual features while maintaining the
advantages of pixel-wise loss. Also, a criterion for comparing weights of loss
is introduced so that the weights can be estimated directly by a convolutional
neural network. In addition, in this article, the expectation-maximization
method is used for the simultaneous estimation super-resolution network and
weighting network. In addition, a new activation function, called "FixedSum",
is introduced which can keep the sum of all components of vector constants
while keeping the output components between zero and one. As experimental
results shows, weighted loss by the proposed method leads to better results
than the unweighted loss and weighted loss based on uncertainty in both
signal-to-noise and perceptual similarity senses on the state-of-the-art
networks. Code is available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13098">CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy. (arXiv:2301.13098v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Qiao_M/0/1/0/all/0/1">Mengyun Qiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Shuo Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Qiu_H/0/1/0/all/0/1">Huaqi Qiu</a>, <a href="http://arxiv.org/find/eess/1/au:+Marvao_A/0/1/0/all/0/1">Antonio de Marvao</a>, <a href="http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1">Declan P. O&#x27;Regan</a>, <a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1">Wenjia Bai</a></p>
<p>Two key questions in cardiac image analysis are to assess the anatomy and
motion of the heart from images; and to understand how they are associated with
non-imaging clinical factors such as gender, age and diseases. While the first
question can often be addressed by image segmentation and motion tracking
algorithms, our capability to model and to answer the second question is still
limited. In this work, we propose a novel conditional generative model to
describe the 4D spatio-temporal anatomy of the heart and its interaction with
non-imaging clinical factors. The clinical factors are integrated as the
conditions of the generative modelling, which allows us to investigate how
these factors influence the cardiac anatomy. We evaluate the model performance
in mainly two tasks, anatomical sequence completion and sequence generation.
The model achieves a high performance in anatomical sequence completion,
comparable to or outperforming other state-of-the-art generative models. In
terms of sequence generation, given clinical conditions, the model can generate
realistic synthetic 4D sequential anatomies that share similar distributions
with the real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07221">On the Role of Randomization in Adversarially Robust Classification. (arXiv:2302.07221v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gnecco_Heredia_L/0/1/0/all/0/1">Lucas Gnecco-Heredia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1">Yann Chevaleyre</a>, <a href="http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1">Benjamin Negrevergne</a>, <a href="http://arxiv.org/find/cs/1/au:+Meunier_L/0/1/0/all/0/1">Laurent Meunier</a>, <a href="http://arxiv.org/find/cs/1/au:+Pydi_M/0/1/0/all/0/1">Muni Sreenivas Pydi</a></p>
<p>Deep neural networks are known to be vulnerable to small adversarial
perturbations in test data. To defend against adversarial attacks,
probabilistic classifiers have been proposed as an alternative to deterministic
ones. However, literature has conflicting findings on the effectiveness of
probabilistic classifiers in comparison to deterministic ones. In this paper,
we clarify the role of randomization in building adversarially robust
classifiers. Given a base hypothesis set of deterministic classifiers, we show
the conditions under which a randomized ensemble outperforms the hypothesis set
in adversarial risk, extending previous results. Additionally, we show that for
any probabilistic binary classifier (including randomized ensembles), there
exists a deterministic classifier that outperforms it. Finally, we give an
explicit description of the deterministic hypothesis set that contains such a
deterministic classifier for many types of commonly used probabilistic
classifiers, i.e. randomized ensembles and parametric/input noise injection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01928">FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arnaiz_Rodriguez_A/0/1/0/all/0/1">Adrian Arnaiz-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliver_N/0/1/0/all/0/1">Nuria Oliver</a></p>
<p>Algorithmic fairness is of utmost societal importance, yet the current trend
in large-scale machine learning models requires training with massive datasets
that are frequently biased. In this context, pre-processing methods that focus
on modeling and correcting bias in the data emerge as valuable approaches. In
this paper, we propose FairShap, a novel instance-level data re-weighting
method for fair algorithmic decision-making through data valuation by means of
Shapley Values. FairShap is model-agnostic and easily interpretable, as it
measures the contribution of each training data point to a predefined fairness
metric. We empirically validate FairShap on several state-of-the-art datasets
of different nature, with a variety of training scenarios and models and show
how it yields fairer models with similar levels of accuracy than the baselines.
We illustrate FairShap's interpretability by means of histograms and latent
space visualizations. Moreover, we perform a utility-fairness study, and
ablation and runtime experiments to illustrate the impact of the size of the
reference dataset and FairShap's computational cost depending on the size of
the dataset and the number of features. We believe that FairShap represents a
promising direction in interpretable and model-agnostic approaches to
algorithmic fairness that yield competitive accuracy even when only biased
datasets are available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09373">MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling. (arXiv:2303.09373v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuzhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Angelini_E/0/1/0/all/0/1">Elsa Angelini</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jia Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasmussen_J/0/1/0/all/0/1">Jerod M. Rasmussen</a>, <a href="http://arxiv.org/find/cs/1/au:+OConnor_T/0/1/0/all/0/1">Thomas G. O&#x27;Connor</a>, <a href="http://arxiv.org/find/cs/1/au:+Wadhwa_P/0/1/0/all/0/1">Pathik D. Wadhwa</a>, <a href="http://arxiv.org/find/cs/1/au:+Jackowski_A/0/1/0/all/0/1">Andrea Parolin Jackowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Posner_J/0/1/0/all/0/1">Jonathan Posner</a>, <a href="http://arxiv.org/find/cs/1/au:+Laine_A/0/1/0/all/0/1">Andrew F. Laine</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yun Wang</a></p>
<p>Robust segmentation is critical for deriving quantitative measures from
large-scale, multi-center, and longitudinal medical scans. Manually annotating
medical scans, however, is expensive and labor-intensive and may not always be
available in every domain. Unsupervised domain adaptation (UDA) is a
well-studied technique that alleviates this label-scarcity problem by
leveraging available labels from another domain. In this study, we introduce
Masked Autoencoding and Pseudo-Labeling Segmentation (MAPSeg), a
$\textbf{unified}$ UDA framework with great versatility and superior
performance for heterogeneous and volumetric medical image segmentation. To the
best of our knowledge, this is the first study that systematically reviews and
develops a framework to tackle four different domain shifts in medical image
segmentation. More importantly, MAPSeg is the first framework that can be
applied to $\textbf{centralized}$, $\textbf{federated}$, and
$\textbf{test-time}$ UDA while maintaining comparable performance. We compare
MAPSeg with previous state-of-the-art methods on a private infant brain MRI
dataset and a public cardiac CT-MRI dataset, and MAPSeg outperforms others by a
large margin (10.5 Dice improvement on the private MRI dataset and 5.7 on the
public CT-MRI dataset). MAPSeg poses great practical value and can be applied
to real-world problems. Our code and pretrained model will be available later.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.18158">Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Shafiee_S/0/1/0/all/0/1">Soroosh Shafiee</a>, <a href="http://arxiv.org/find/math/1/au:+Kilinc_Karzan_F/0/1/0/all/0/1">Fatma K&#x131;l&#x131;n&#xe7;-Karzan</a></p>
<p>Optimization problems involving minimization of a rank-one convex function
over constraints modeling restrictions on the support of the decision variables
emerge in various machine learning applications. These problems are often
modeled with indicator variables for identifying the support of the continuous
variables. In this paper we investigate compact extended formulations for such
problems through perspective reformulation techniques. In contrast to the
majority of previous work that relies on support function arguments and
disjunctive programming techniques to provide convex hull results, we propose a
constructive approach that exploits a hidden conic structure induced by
perspective functions. To this end, we first establish a convex hull result for
a general conic mixed-binary set in which each conic constraint involves a
linear function of independent continuous variables and a set of binary
variables. We then demonstrate that extended representations of sets associated
with epigraphs of rank-one convex functions over constraints modeling indicator
relations naturally admit such a conic representation. This enables us to
systematically give perspective formulations for the convex hull descriptions
of these sets with nonlinear separable or non-separable objective functions,
sign constraints on continuous variables, and combinatorial constraints on
indicator variables. We illustrate the efficacy of our results on sparse
nonnegative logistic regression problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08415">Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v3 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1">Francesco Conti</a>, <a href="http://arxiv.org/find/cs/1/au:+Paulin_G/0/1/0/all/0/1">Gianna Paulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Garofalo_A/0/1/0/all/0/1">Angelo Garofalo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1">Davide Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mauro_A/0/1/0/all/0/1">Alfio Di Mauro</a>, <a href="http://arxiv.org/find/cs/1/au:+Rutishauser_G/0/1/0/all/0/1">Georg Rutishauser</a>, <a href="http://arxiv.org/find/cs/1/au:+Ottavi_G/0/1/0/all/0/1">Gianmarco Ottavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Eggimann_M/0/1/0/all/0/1">Manuel Eggimann</a>, <a href="http://arxiv.org/find/cs/1/au:+Okuhara_H/0/1/0/all/0/1">Hayate Okuhara</a>, <a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1">Luca Benini</a></p>
<p>Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT)
System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and
nano-robotics need to run many diverse tasks within a power envelope of a few
tens of mW over a wide range of operating conditions: compute-intensive but
strongly quantized Deep Neural Network (DNN) inference, as well as signal
processing and control requiring high-precision floating-point. We present
Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in
GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16
RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a
diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions
(XpulpNN), combined with fused MAC&amp;LOAD operations and floating-point support;
2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1
(pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks
connected to an Adaptive Body Biasing (ABB) generator and a hardware control
loop, enabling on-the-fly adaptation of transistor threshold voltages.
Marsellus achieves up to 180 Gop/s or 3.32 Top/s/W on 2-bit precision
arithmetic in software, and up to 637 Gop/s or 12.4 Top/s/W on
hardware-accelerated DNN layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10498">Edge Directionality Improves Learning on Heterophilic Graphs. (arXiv:2305.10498v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rossi_E/0/1/0/all/0/1">Emanuele Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1">Bertrand Charpentier</a>, <a href="http://arxiv.org/find/cs/1/au:+Giovanni_F/0/1/0/all/0/1">Francesco Di Giovanni</a>, <a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1">Fabrizio Frasca</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1">Michael Bronstein</a></p>
<p>Graph Neural Networks (GNNs) have become the de-facto standard tool for
modeling relational data. However, while many real-world graphs are directed,
the majority of today's GNN models discard this information altogether by
simply making the graph undirected. The reasons for this are historical: 1)
many early variants of spectral GNNs explicitly required undirected graphs, and
2) the first benchmarks on homophilic graphs did not find significant gain from
using direction. In this paper, we show that in heterophilic settings, treating
the graph as directed increases the effective homophily of the graph,
suggesting a potential gain from the correct use of directionality information.
To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel
general framework for deep learning on directed graphs. Dir-GNN can be used to
extend any Message Passing Neural Network (MPNN) to account for edge
directionality information by performing separate aggregations of the incoming
and outgoing edges. We prove that Dir-GNN matches the expressivity of the
Directed Weisfeiler-Lehman test, exceeding that of conventional MPNNs. In
extensive experiments, we validate that while our framework leaves performance
unchanged on homophilic datasets, it leads to large gains over base models such
as GCN, GAT and GraphSage on heterophilic benchmarks, outperforming much more
complex methods and achieving new state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.11475">Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models. (arXiv:2305.11475v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Siems_J/0/1/0/all/0/1">Julien Siems</a>, <a href="http://arxiv.org/find/cs/1/au:+Ditschuneit_K/0/1/0/all/0/1">Konstantin Ditschuneit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ripken_W/0/1/0/all/0/1">Winfried Ripken</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindborg_A/0/1/0/all/0/1">Alma Lindborg</a>, <a href="http://arxiv.org/find/cs/1/au:+Schambach_M/0/1/0/all/0/1">Maximilian Schambach</a>, <a href="http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1">Johannes S. Otterbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Genzel_M/0/1/0/all/0/1">Martin Genzel</a></p>
<p>Generalized Additive Models (GAMs) have recently experienced a resurgence in
popularity due to their interpretability, which arises from expressing the
target value as a sum of non-linear transformations of the features. Despite
the current enthusiasm for GAMs, their susceptibility to concurvity - i.e.,
(possibly non-linear) dependencies between the features - has hitherto been
largely overlooked. Here, we demonstrate how concurvity can severly impair the
interpretability of GAMs and propose a remedy: a conceptually simple, yet
effective regularizer which penalizes pairwise correlations of the non-linearly
transformed feature variables. This procedure is applicable to any
differentiable additive model, such as Neural Additive Models or NeuralProphet,
and enhances interpretability by eliminating ambiguities due to self-canceling
feature contributions. We validate the effectiveness of our regularizer in
experiments on synthetic as well as real-world datasets for time-series and
tabular data. Our experiments show that concurvity in GAMs can be reduced
without significantly compromising prediction quality, improving
interpretability and reducing variance in the feature importances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14561">Negative Feedback Training: A Novel Concept to Improve Robustness of NVCIM DNN Accelerators. (arXiv:2305.14561v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yifan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zheyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wujie Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaobo Sharon Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yiyu Shi</a></p>
<p>Compute-in-memory (CIM) accelerators built upon non-volatile memory (NVM)
devices excel in energy efficiency and latency when performing Deep Neural
Network (DNN) inference, thanks to their in-situ data processing capability.
However, the stochastic nature and intrinsic variations of NVM devices often
result in performance degradation in DNN inference. Introducing these non-ideal
device behaviors during DNN training enhances robustness, but drawbacks include
limited accuracy improvement, reduced prediction confidence, and convergence
issues. This arises from a mismatch between the deterministic training and
non-deterministic device variations, as such training, though considering
variations, relies solely on the model's final output. In this work, we draw
inspiration from the control theory and propose a novel training concept:
Negative Feedback Training (NFT) leveraging the multi-scale noisy information
captured from network. We develop two specific NFT instances, Oriented
Variational Forward (OVF) and Intermediate Representation Snapshot (IRS).
Extensive experiments show that our methods outperform existing
state-of-the-art methods with up to a 46.71% improvement in inference accuracy
while reducing epistemic uncertainty, boosting output confidence, and improving
convergence probability. Their effectiveness highlights the generality and
practicality of our NFT concept in enhancing DNN robustness against device
variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18228">SR-OOD: Out-of-Distribution Detection via Sample Repairing. (arXiv:2305.18228v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Rui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Andi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jinke Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruimao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a></p>
<p>Out-of-distribution (OOD) detection is a crucial task for ensuring the
reliability and robustness of machine learning models. Recent works have shown
that generative models often assign high confidence scores to OOD samples,
indicating that they fail to capture the semantic information of the data. To
tackle this problem, we take advantage of sample repairing and propose a novel
OOD detection framework, namely SR-OOD. Our framework leverages the idea that
repairing an OOD sample can reveal its semantic inconsistency with the
in-distribution data. Specifically, our framework consists of two components: a
sample repairing module and a detection module. The sample repairing module
applies erosion to an input sample and uses a generative adversarial network to
repair it. The detection module then determines whether the input sample is OOD
using a distance metric. Our framework does not require any additional data or
label information for detection, making it applicable to various scenarios. We
conduct extensive experiments on three image datasets: CIFAR-10, CelebA, and
Pokemon. The results demonstrate that our approach achieves superior
performance over the state-of-the-art generative methods in OOD detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18766">HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance. (arXiv:2305.18766v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Junzhe Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1">Peiye Zhuang</a></p>
<p>The advancements in automatic text-to-3D generation have been remarkable.
Most existing methods use pre-trained text-to-image diffusion models to
optimize 3D representations like Neural Radiance Fields (NeRFs) via
latent-space denoising score matching. Yet, these methods often result in
artifacts and inconsistencies across different views due to their suboptimal
optimization approaches and limited understanding of 3D geometry. Moreover, the
inherent constraints of NeRFs in rendering crisp geometry and stable textures
usually lead to a two-stage optimization to attain high-resolution details.
This work proposes holistic sampling and smoothing approaches to achieve
high-quality text-to-3D generation, all in a single-stage optimization. We
compute denoising scores in the text-to-image diffusion model's latent and
image spaces. Instead of randomly sampling timesteps (also referred to as noise
levels in denoising score matching), we introduce a novel timestep annealing
approach that progressively reduces the sampled timestep throughout
optimization. To generate high-quality renderings in a single-stage
optimization, we propose regularization for the variance of z-coordinates along
NeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel
smoothing technique that refines importance sampling weights coarse-to-fine,
ensuring accurate and thorough sampling in high-density regions. Extensive
experiments demonstrate the superiority of our method over previous approaches,
enabling the generation of highly detailed and view-consistent 3D assets
through a single-stage training process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07745">Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vakili_S/0/1/0/all/0/1">Sattar Vakili</a>, <a href="http://arxiv.org/find/cs/1/au:+Olkhovskaya_J/0/1/0/all/0/1">Julia Olkhovskaya</a></p>
<p>Reinforcement learning (RL) has shown empirical success in various real world
settings with complex models and large state-action spaces. The existing
analytical results, however, typically focus on settings with a small number of
state-actions or simple models such as linearly modeled state-action value
functions. To derive RL policies that efficiently handle large state-action
spaces with more general value functions, some recent works have considered
nonlinear function approximation using kernel ridge regression. We propose
$\pi$-KRVI, an optimistic modification of least-squares value iteration, when
the state-action value function is represented by a reproducing kernel Hilbert
space (RKHS). We prove the first order-optimal regret guarantees under a
general setting. Our results show a significant polynomial in the number of
episodes improvement over the state of the art. In particular, with highly
non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the
existing results lead to trivial (superlinear in the number of episodes) regret
bounds. We show a sublinear regret bound that is order optimal in the case of
Mat\'ern kernels where a lower bound on regret is known.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11363">Masked Diffusion Models Are Fast Distribution Learners. (arXiv:2306.11363v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jiachen Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qinglong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Peng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1">Zhongjie Ba</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenguang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1">Kui Ren</a></p>
<p>Diffusion model has emerged as the \emph{de-facto} model for image
generation, yet the heavy training overhead hinders its broader adoption in the
research community. We observe that diffusion models are commonly trained to
learn all fine-grained visual information from scratch. This paradigm may cause
unnecessary training costs hence requiring in-depth investigation. In this
work, we show that it suffices to train a strong diffusion model by first
pre-training the model to learn some primer distribution that loosely
characterizes the unknown real image distribution. Then the pre-trained model
can be fine-tuned for various generation tasks efficiently. In the pre-training
stage, we propose to mask a high proportion (e.g., up to 90\%) of input images
to approximately represent the primer distribution and introduce a masked
denoising score matching objective to train a model to denoise visible areas.
In subsequent fine-tuning stage, we efficiently train diffusion model without
masking. Utilizing the two-stage training framework, we achieves significant
training acceleration and a new FID score record of 6.27 on CelebA-HQ $256
\times 256$ for ViT-based diffusion models. The generalizability of a
pre-trained model further helps building models that perform better than ones
trained from scratch on different downstream datasets. For instance, a
diffusion model pre-trained on VGGFace2 attains a 46\% quality improvement when
fine-tuned on a different dataset that contains only 3000 images. Our code is
available at \url{https://github.com/jiachenlei/maskdm}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13029">Decentralized Online Federated G-Network Learning for Lightweight Intrusion Detection. (arXiv:2306.13029v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nakip_M/0/1/0/all/0/1">Mert Nak&#x131;p</a>, <a href="http://arxiv.org/find/cs/1/au:+Gul_B/0/1/0/all/0/1">Baran Can G&#xfc;l</a>, <a href="http://arxiv.org/find/cs/1/au:+Gelenbe_E/0/1/0/all/0/1">Erol Gelenbe</a></p>
<p>Cyberattacks are increasingly threatening networked systems, often with the
emergence of new types of unknown (zero-day) attacks and the rise of vulnerable
devices. Such attacks can also target multiple components of a Supply Chain,
which can be protected via Machine Learning (ML)-based Intrusion Detection
Systems (IDSs). However, the need to learn large amounts of labelled data often
limits the applicability of ML-based IDSs to cybersystems that only have access
to private local data, while distributed systems such as Supply Chains have
multiple components, each of which must preserve its private data while being
targeted by the same attack To address this issue, this paper proposes a novel
Decentralized and Online Federated Learning Intrusion Detection (DOF-ID)
architecture based on the G-Network model with collaborative learning, that
allows each IDS used by a specific component to learn from the experience
gained in other components, in addition to its own local data, without
violating the data privacy of other components. The performance evaluation
results using public Kitsune and Bot-IoT datasets show that DOF-ID
significantly improves the intrusion detection performance in all of the
collaborating components, with acceptable computation time for online learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13053">Context-lumpable stochastic bandits. (arXiv:2306.13053v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chung-Wei Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qinghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1">Yasin Abbasi-Yadkori</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1">Chi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1">Tor Lattimore</a>, <a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1">Csaba Szepesv&#xe1;ri</a></p>
<p>We consider a contextual bandit problem with $S$ contexts and $K$ actions. In
each round $t=1,2,\dots$, the learner observes a random context and chooses an
action based on its past experience. The learner then observes a random reward
whose mean is a function of the context and the action for the round. Under the
assumption that the contexts can be lumped into $r\le \min\{S,K\}$ groups such
that the mean reward for the various actions is the same for any two contexts
that are in the same group, we give an algorithm that outputs an
$\epsilon$-optimal policy after using at most $\widetilde O(r (S +K
)/\epsilon^2)$ samples with high probability and provide a matching
$\Omega(r(S+K)/\epsilon^2)$ lower bound. In the regret minimization setting, we
give an algorithm whose cumulative regret up to time $T$ is bounded by
$\widetilde O(\sqrt{r^3(S+K)T})$. To the best of our knowledge, we are the
first to show the near-optimal sample complexity in the PAC setting and
$\widetilde O(\sqrt{{poly}(r)(S+K)T})$ minimax regret in the online setting for
this problem. We also show our algorithms can be applied to more general
low-rank bandits and get improved regret bounds in some scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15868">GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation. (arXiv:2306.15868v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhen Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1">Chao Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunsheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chengli Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haifeng Li</a></p>
<p>Self-supervised contrastive learning (SSCL) has achieved significant
milestones in remote sensing image (RSI) understanding. Its essence lies in
designing an unsupervised instance discrimination pretext task to extract image
features from a large number of unlabeled images that are beneficial for
downstream tasks. However, existing instance discrimination based SSCL suffer
from two limitations when applied to the RSI semantic segmentation task: 1)
Positive sample confounding issue; 2) Feature adaptation bias. It introduces a
feature adaptation bias when applied to semantic segmentation tasks that
require pixel-level or object-level features. In this study, We observed that
the discrimination information can be mapped to specific regions in RSI through
the gradient of unsupervised contrastive loss, these specific regions tend to
contain singular ground objects. Based on this, we propose contrastive learning
with Gradient guided Sampling Strategy (GraSS) for RSI semantic segmentation.
GraSS consists of two stages: Instance Discrimination warm-up (ID warm-up) and
Gradient guided Sampling contrastive training (GS training). The ID warm-up
aims to provide initial discrimination information to the contrastive loss
gradients. The GS training stage aims to utilize the discrimination information
contained in the contrastive loss gradients and adaptively select regions in
RSI patches that contain more singular ground objects, in order to construct
new positive and negative samples. Experimental results on three open datasets
demonstrate that GraSS effectively enhances the performance of SSCL in
high-resolution RSI semantic segmentation. Compared to seven baseline methods
from five different types of SSCL, GraSS achieves an average improvement of
1.57\% and a maximum improvement of 3.58\% in terms of mean intersection over
the union. The source code is available at https://github.com/GeoX-Lab/GraSS
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00154">Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1">Zizheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Haoyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jianfei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Large pretrained plain vision Transformers (ViTs) have been the workhorse for
many downstream tasks. However, existing works utilizing off-the-shelf ViTs are
inefficient in terms of training and deployment, because adopting ViTs with
individual sizes requires separate trainings and is restricted by fixed
performance-efficiency trade-offs. In this paper, we are inspired by stitchable
neural networks (SN-Net), which is a new framework that cheaply produces a
single model that covers rich subnetworks by stitching pretrained model
families, supporting diverse performance-efficiency trade-offs at runtime.
Building upon this foundation, we introduce SN-Netv2, a systematically improved
model stitching framework to facilitate downstream task adaptation.
Specifically, we first propose a two-way stitching scheme to enlarge the
stitching space. We then design a resource-constrained sampling strategy that
takes into account the underlying FLOPs distributions in the space for better
sampling. Finally, we observe that learning stitching layers as a low-rank
update plays an essential role on downstream tasks to stabilize training and
ensure a good Pareto frontier. With extensive experiments on ImageNet-1K,
ADE20K, COCO-Stuff-10K and NYUv2, SN-Netv2 demonstrates superior performance
over SN-Netv1 on downstream dense predictions and shows strong ability as a
flexible vision backbone, achieving great advantages in both training
efficiency and deployment flexibility. Code is available at
https://github.com/ziplab/SN-Netv2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04870">RACH-Space: Reconstructing Adaptive Convex Hull Space with applications in weak supervision. (arXiv:2307.04870v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Na_W/0/1/0/all/0/1">Woojoo Na</a></p>
<p>We introduce RACH-Space, a novel classification method in ensemble learning.
In particular, we show its applicability as a label model for weakly supervised
learning. RACH-Space offers simplicity in implementation with minimal
assumptions on the data or weak signals. The model is well suited for scenarios
where fully labeled data is not available. Our method is built upon geometrical
interpretation of the space spanned by weak signals. Our analysis of the high
dimensional convex hull structure underlying general set of weak signals
bridges geometry with machine learning. Empirical results also demonstrate that
RACH-Space works well in practice and compares favorably to best existing label
models for weakly supervised learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08890">The Predicted-Updates Dynamic Model: Offline, Incremental, and Decremental to Fully Dynamic Transformations. (arXiv:2307.08890v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Quanquan C. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivas_V/0/1/0/all/0/1">Vaidehi Srinivas</a></p>
<p>We formulate the predicted-updates dynamic model, one of the first
beyond-worst-case models for dynamic algorithms, which generalizes a large set
of well-studied dynamic models including the offline dynamic, incremental, and
decremental models to the fully dynamic setting when given predictions about
the update times of the elements. In the most basic form of our model, we
receive a set of predicted update times for all of the updates that occur over
the event horizon. We give a novel framework that "lifts" offline
divide-and-conquer algorithms into the fully dynamic setting with little
overhead. Using this, we are able to interpolate between the offline and fully
dynamic settings; when the $\ell_1$ error of the prediction is linear in the
number of updates, we achieve the offline runtime of the algorithm (up to
$\mathrm{poly} \log n$ factors). Provided a fully dynamic backstop algorithm,
our algorithm will never do worse than the backstop algorithm regardless of the
prediction error. Furthermore, our framework achieves a smooth linear trade-off
between $\ell_1$ error in the predictions and runtime. These correspond to the
desiderata of consistency, robustness, and graceful degradation of the
algorithms-with-predictions literature. We further extend our techniques to
incremental and decremental settings, transforming algorithms in these settings
when given predictions of only the deletion and insertion times, respectively.
Our framework is general, and we apply it to obtain improved efficiency bounds
over the state-of-the-art dynamic algorithms for a variety of problems
including triconnectivity, planar digraph all pairs shortest paths, $k$-edge
connectivity, and others, for prediction error of reasonable magnitude.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11957">High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v4 [physics.optics] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Zhao_G/0/1/0/all/0/1">Guangyuan Zhao</a>, <a href="http://arxiv.org/find/physics/1/au:+Shu_X/0/1/0/all/0/1">Xin Shu</a></p>
<p>Optical computing systems provide high-speed and low-energy data processing
but face deficiencies in computationally demanding training and
simulation-to-reality gaps. We propose a model-free optimization (MFO) method
based on a score gradient estimation algorithm for computationally efficient in
situ training of optical computing systems. This approach treats an optical
computing system as a black box and back-propagates the loss directly to the
optical computing weights' probability distributions, circumventing the need
for a computationally heavy and biased system simulation. Our experiments on a
single-layer diffractive optical computing system show that MFO outperforms
hybrid training on the MNIST and FMNIST datasets. Furthermore, we demonstrate
image-free and high-speed classification of cells from their phase maps. Our
method's model-free and high-performance nature, combined with its low demand
for computational resources, expedites the transition of optical computing from
laboratory demonstrations to real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12226">Geometry-Aware Adaptation for Pretrained Models. (arXiv:2307.12226v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1">Nicholas Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xintong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1">Dyah Adila</a>, <a href="http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1">Sonia Cromp</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tzu-Heng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jitian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1">Frederic Sala</a></p>
<p>Machine learning models -- including prominent zero-shot models -- are often
trained on datasets whose labels are only a small proportion of a larger label
space. Such spaces are commonly equipped with a metric that relates the labels
via distances between them. We propose a simple approach to exploit this
information to adapt the trained model to reliably predict new classes -- or,
in the case of zero-shot prediction, to improve its performance -- without any
additional training. Our technique is a drop-in replacement of the standard
prediction rule, swapping argmax with the Fr\'echet mean. We provide a
comprehensive theoretical analysis for this approach, studying (i)
learning-theoretic results trading off label space diameter, sample complexity,
and model dimension, (ii) characterizations of the full range of scenarios in
which it is possible to predict any unobserved class, and (iii) an optimal
active learning-like next class selection procedure to obtain optimal training
classes for when it is not possible to predict the entire range of unobserved
classes. Empirically, using easily-available external metrics, our proposed
approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet
and scales to hundreds of thousands of classes. When no such metric is
available, Loki can use self-derived metrics from class embeddings and obtains
a 10.5% improvement on pretrained zero-shot models such as CLIP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12301">Unsupervised Image Outlier Detection using RANSAC. (arXiv:2307.12301v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1">Chen-Han Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yu-Shao Peng</a></p>
<p>Image outlier detection (OD) is an essential tool to ensure the quality and
accuracy of image datasets used in computer vision tasks. Most existing
approaches, however, require a set of in-distribution data for training prior
to outlier prediction. The quality and quantity of the data can influence the
resulting performance. Thus, selecting a suitable in-distribution set often
requires considerable effort. In this work, we propose RANSAC-NN, an
unsupervised image OD algorithm designed to detect outliers within contaminated
sets in a one-class classification fashion. Without any training, RANSAC-NN
performs favorably in comparison to other well-established methods in a variety
of OD benchmarks. Furthermore, we show that our method can enhance the
robustness of existing OD methods by simply applying RANSAC-NN during
pre-processing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12689">Addressing the Impact of Localized Training Data in Graph Neural Networks. (arXiv:2307.12689v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+A_A/0/1/0/all/0/1">Akansha A</a></p>
<p>Graph Neural Networks (GNNs) have achieved notable success in learning from
graph-structured data, owing to their ability to capture intricate dependencies
and relationships between nodes. They excel in various applications, including
semi-supervised node classification, link prediction, and graph generation.
However, it is important to acknowledge that the majority of state-of-the-art
GNN models are built upon the assumption of an in-distribution setting, which
hinders their performance on real-world graphs with dynamic structures. In this
article, we aim to assess the impact of training GNNs on localized subsets of
the graph. Such restricted training data may lead to a model that performs well
in the specific region it was trained on but fails to generalize and make
accurate predictions for the entire graph. In the context of graph-based
semi-supervised learning (SSL), resource constraints often lead to scenarios
where the dataset is large, but only a portion of it can be labeled, affecting
the model's performance. This limitation affects tasks like anomaly detection
or spam detection when labeling processes are biased or influenced by human
subjectivity. To tackle the challenges posed by localized training data, we
approach the problem as an out-of-distribution (OOD) data issue by by aligning
the distributions between the training data, which represents a small portion
of labeled data, and the graph inference process that involves making
predictions for the entire graph. We propose a regularization method to
minimize distributional discrepancies between localized training data and graph
inference, improving model performance on OOD data. Extensive tests on popular
GNN models show significant performance improvement on three citation GNN
benchmark datasets. The regularization approach effectively enhances model
adaptation and generalization, overcoming challenges posed by OOD data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10099">Geometric instability of graph neural networks on large graphs. (arXiv:2308.10099v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morris_E/0/1/0/all/0/1">Emily Morris</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Haotian Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weiling Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Sajjad_M/0/1/0/all/0/1">Muhammad Hamza Sajjad</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Borun Shi</a></p>
<p>We analyse the geometric instability of embeddings produced by graph neural
networks (GNNs). Existing methods are only applicable for small graphs and lack
context in the graph domain. We propose a simple, efficient and graph-native
Graph Gram Index (GGI) to measure such instability which is invariant to
permutation, orthogonal transformation, translation and order of evaluation.
This allows us to study the varying instability behaviour of GNN embeddings on
large graphs for both node classification and link prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10364">SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Midgley_L/0/1/0/all/0/1">Laurence I. Midgley</a>, <a href="http://arxiv.org/find/cs/1/au:+Stimper_V/0/1/0/all/0/1">Vincent Stimper</a>, <a href="http://arxiv.org/find/cs/1/au:+Antoran_J/0/1/0/all/0/1">Javier Antor&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathieu_E/0/1/0/all/0/1">Emile Mathieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a></p>
<p>Coupling normalizing flows allow for fast sampling and density evaluation,
making them the tool of choice for probabilistic modeling of physical systems.
However, the standard coupling architecture precludes endowing flows that
operate on the Cartesian coordinates of atoms with the SE(3) and permutation
invariances of physical systems. This work proposes a coupling flow that
preserves SE(3) and permutation equivariance by performing coordinate splits
along additional augmented dimensions. At each layer, the flow maps atoms'
positions into learned SE(3) invariant bases, where we apply standard flow
transformations, such as monotonic rational-quadratic splines, before returning
to the original basis. Crucially, our flow preserves fast sampling and density
evaluation, and may be used to produce unbiased estimates of expectations with
respect to the target distribution via importance sampling. When trained on the
DW4, LJ13, and QM9-positional datasets, our flow is competitive with
equivariant continuous normalizing flows, while allowing sampling more than an
order of magnitude faster. Moreover, to the best of our knowledge, we are the
first to learn the full Boltzmann distribution of alanine dipeptide by only
modeling the Cartesian positions of its atoms. Lastly, we demonstrate that our
flow can be trained to approximately sample from the Boltzmann distribution of
the DW4 and LJ13 particle systems using only their energy functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12532">FedSOL: Stabilized Orthogonal Learning in Federated Learning. (arXiv:2308.12532v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Gihun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minchan Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangmook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16847">Diffusion Models for Interferometric Satellite Aperture Radar. (arXiv:2308.16847v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tuel_A/0/1/0/all/0/1">Alexandre Tuel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerdreux_T/0/1/0/all/0/1">Thomas Kerdreux</a>, <a href="http://arxiv.org/find/cs/1/au:+Hulbert_C/0/1/0/all/0/1">Claudia Hulbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouet_Leduc_B/0/1/0/all/0/1">Bertrand Rouet-Leduc</a></p>
<p>Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.01291">Generative Social Choice. (arXiv:2309.01291v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fish_S/0/1/0/all/0/1">Sara Fish</a>, <a href="http://arxiv.org/find/cs/1/au:+Golz_P/0/1/0/all/0/1">Paul G&#xf6;lz</a>, <a href="http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1">David C. Parkes</a>, <a href="http://arxiv.org/find/cs/1/au:+Procaccia_A/0/1/0/all/0/1">Ariel D. Procaccia</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusak_G/0/1/0/all/0/1">Gili Rusak</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapira_I/0/1/0/all/0/1">Itai Shapira</a>, <a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1">Manuel W&#xfc;thrich</a></p>
<p>Traditionally, social choice theory has only been applicable to choices among
a few predetermined alternatives but not to more complex decisions such as
collectively selecting a textual statement. We introduce generative social
choice, a framework that combines the mathematical rigor of social choice
theory with the capability of large language models to generate text and
extrapolate preferences. This framework divides the design of AI-augmented
democratic processes into two components: first, proving that the process
satisfies rigorous representation guarantees when given access to oracle
queries; second, empirically validating that these queries can be approximately
implemented using a large language model. We apply this framework to the
problem of generating a slate of statements that is representative of opinions
expressed as free-form text; specifically, we develop a democratic process with
representation guarantees and use this process to represent the opinions of
participants in a survey about chatbot personalization. We find that 93 out of
100 participants feel "mostly" or "perfectly" represented by the slate of five
statements we extracted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02705">Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Aounon Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1">Chirag Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1">Suraj Srinivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Aaron Jiaxun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Himabindu Lakkaraju</a></p>
<p>Large language models (LLMs) released for public use incorporate guardrails
to ensure their output is safe, often referred to as "model alignment." An
aligned language model should decline a user's request to produce harmful
content. However, such safety measures are vulnerable to adversarial attacks,
which add maliciously designed token sequences to a harmful prompt to bypass
the model's safety guards. In this work, we introduce erase-and-check, the
first framework to defend against adversarial prompts with verifiable safety
guarantees. We defend against three attack modes: i) adversarial suffix, which
appends an adversarial sequence at the end of the prompt; ii) adversarial
insertion, where the adversarial sequence is inserted anywhere in the middle of
the prompt; and iii) adversarial infusion, where adversarial tokens are
inserted at arbitrary positions in the prompt, not necessarily as a contiguous
block. Our experimental results demonstrate that this procedure can obtain
strong certified safety guarantees on harmful prompts while maintaining good
empirical performance on safe prompts. For example, against adversarial
suffixes of length 20, it certifiably detects 92% of harmful prompts and labels
94% of safe prompts correctly using the open-source language model Llama 2 as
the safety filter. We further improve the filter's performance, in terms of
accuracy and speed, by replacing Llama 2 with a DistilBERT safety classifier
fine-tuned on safe and harmful prompts. Additionally, we propose two efficient
empirical defenses: i) RandEC, a randomized version of erase-and-check that
evaluates the safety filter on a small subset of the erased subsequences, and
ii) GradEC, a gradient-based version that optimizes the erased tokens to remove
the adversarial sequence. The code for our experiments is available at
https://github.com/aounon/certified-llm-safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11526">Likelihood-based Sensor Calibration using Affine Transformation. (arXiv:2309.11526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Machhamer_R/0/1/0/all/0/1">R&#xfc;diger Machhamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazlic_L/0/1/0/all/0/1">Lejla Begic Fazlic</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Eray Guven</a>, <a href="http://arxiv.org/find/cs/1/au:+Junk_D/0/1/0/all/0/1">David Junk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1">Gunes Karabulut Kurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_S/0/1/0/all/0/1">Stefan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Didas_S/0/1/0/all/0/1">Stephan Didas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollmer_K/0/1/0/all/0/1">Klaus-Uwe Gollmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmann_R/0/1/0/all/0/1">Ralph Bergmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Timm_I/0/1/0/all/0/1">Ingo J. Timm</a>, <a href="http://arxiv.org/find/cs/1/au:+Dartmann_G/0/1/0/all/0/1">Guido Dartmann</a></p>
<p>An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. The results demonstrate the
adaptability of this solution for various applications, including software
calibration of sensors, implementation of expert-based adaptation, and paving
the way for future advancements such as distributed learning methods. One idea
here is to use the knowledge of experts for estimating an affine transformation
between different systems. We evaluate our research with simulations and also
with real measured data of a multi-sensor board with 8 identical sensors. Both
data set and evaluation script are provided for download. The results show an
improvement for both the simulation and the experiments with real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13411">Towards Attributions of Input Variables in a Coalition. (arXiv:2309.13411v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xinhao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Huiqi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1">Bo Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Quanshi Zhang</a></p>
<p>This paper aims to develop a new attribution method to explain the conflict
between individual variables' attributions and their coalition's attribution
from a fully new perspective. First, we find that the Shapley value can be
reformulated as the allocation of Harsanyi interactions encoded by the AI
model. Second, based the re-alloction of interactions, we extend the Shapley
value to the attribution of coalitions. Third we ective. We derive the
fundamental mechanism behind the conflict. This conflict come from the
interaction containing partial variables in their coalition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13658">Fantastic Generalization Measures are Nowhere to be Found. (arXiv:2309.13658v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gastpar_M/0/1/0/all/0/1">Michael Gastpar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nachum_I/0/1/0/all/0/1">Ido Nachum</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafer_J/0/1/0/all/0/1">Jonathan Shafer</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberger_T/0/1/0/all/0/1">Thomas Weinberger</a></p>
<p>We study the notion of a generalization bound being uniformly tight, meaning
that the difference between the bound and the population loss is small for all
learning algorithms and all population distributions. Numerous generalization
bounds have been proposed in the literature as potential explanations for the
ability of neural networks to generalize in the overparameterized setting.
However, in their paper ``Fantastic Generalization Measures and Where to Find
Them,'' Jiang et al. (2020) examine more than a dozen generalization bounds,
and show empirically that none of them are uniformly tight. This raises the
question of whether uniformly-tight generalization bounds are at all possible
in the overparameterized setting. We consider two types of generalization
bounds: (1) bounds that may depend on the training set and the learned
hypothesis (e.g., margin bounds). We prove mathematically that no such bound
can be uniformly tight in the overparameterized setting; (2) bounds that may in
addition also depend on the learning algorithm (e.g., stability bounds). For
these bounds, we show a trade-off between the algorithm's performance and the
bound's tightness. Namely, if the algorithm achieves good accuracy on certain
distributions, then no generalization bound can be uniformly tight for it in
the overparameterized setting. We explain how these formal results can, in our
view, inform research on generalization bounds for neural networks, while
stressing that other interpretations of these results are also possible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14053">Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1">Khoi Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Duong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hoa Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_Thanh_L/0/1/0/all/0/1">Long Tran-Thanh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1">Quoc-Viet Pham</a></p>
<p>LARS and LAMB have emerged as prominent techniques in Large Batch Learning
(LBL) to ensure training stability in AI. Convergence stability is a challenge
in LBL, where the AI agent usually gets trapped in the sharp minimizer. To
address this challenge, warm-up is an efficient technique, but it lacks a
strong theoretical foundation. Specifically, the warm-up process often reduces
gradients in the early phase, inadvertently preventing the agent from escaping
the sharp minimizer early on. In light of this situation, we conduct empirical
experiments to analyze the behaviors of LARS and LAMB with and without a
warm-up strategy. Our analyses give a comprehensive insight into the behaviors
of LARS, LAMB, and the necessity of a warm-up technique in LBL, including an
explanation of their failure in many cases. Building upon these insights, we
propose a novel algorithm called Time Varying LARS (TVLARS), which facilitates
robust training in the initial phase without the need for warm-up. A
configurable sigmoid-like function is employed in TVLARS to replace the warm-up
process to enhance training stability. Moreover, TVLARS stimulates gradient
exploration in the early phase, thus allowing it to surpass the sharp minimizes
early on and gradually transition to LARS and achieving robustness of LARS in
the latter phases. Extensive experimental evaluations reveal that TVLARS
consistently outperforms LARS and LAMB in most cases, with improvements of up
to 2% in classification scenarios. Notably, in every case of self-supervised
learning, TVLARS dominates LARS and LAMB with performance improvements of up to
10%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16064">Masked Autoencoders are Scalable Learners of Cellular Morphology. (arXiv:2309.16064v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kraus_O/0/1/0/all/0/1">Oren Kraus</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenyon_Dean_K/0/1/0/all/0/1">Kian Kenyon-Dean</a>, <a href="http://arxiv.org/find/cs/1/au:+Saberian_S/0/1/0/all/0/1">Saber Saberian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fallah_M/0/1/0/all/0/1">Maryam Fallah</a>, <a href="http://arxiv.org/find/cs/1/au:+McLean_P/0/1/0/all/0/1">Peter McLean</a>, <a href="http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1">Jess Leung</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1">Vasudev Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Ayla Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_J/0/1/0/all/0/1">Jia Balakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Celik_S/0/1/0/all/0/1">Safiye Celik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sypetkowski_M/0/1/0/all/0/1">Maciej Sypetkowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Chi Vicky Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Morse_K/0/1/0/all/0/1">Kristen Morse</a>, <a href="http://arxiv.org/find/cs/1/au:+Makes_M/0/1/0/all/0/1">Maureen Makes</a>, <a href="http://arxiv.org/find/cs/1/au:+Mabey_B/0/1/0/all/0/1">Ben Mabey</a>, <a href="http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1">Berton Earnshaw</a></p>
<p>Inferring biological relationships from cellular phenotypes in high-content
microscopy screens provides significant opportunity and challenge in biological
research. Prior results have shown that deep vision models can capture
biological signal better than hand-crafted features. This work explores how
self-supervised deep learning approaches scale when training larger models on
larger microscopy datasets. Our results show that both CNN- and ViT-based
masked autoencoders significantly outperform weakly supervised baselines. At
the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops
sampled from 93-million microscopy images achieves relative improvements as
high as 28% over our best weakly supervised baseline at inferring known
biological relationships curated from public databases. Relevant code and
select models released with this work can be found at:
https://github.com/recursionpharma/maes_microscopy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01837">Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gizzini_A/0/1/0/all/0/1">Abdul Karim Gizzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1">Mustafa Shukor</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1">Ali J. Ghandour</a></p>
<p>Current AI-based methods do not provide comprehensible physical
interpretations of the utilized data, extracted features, and
predictions/inference operations. As a result, deep learning models trained
using high-resolution satellite imagery lack transparency and explainability
and can be merely seen as a black box, which limits their wide-level adoption.
Experts need help understanding the complex behavior of AI models and the
underlying decision-making process. The explainable artificial intelligence
(XAI) field is an emerging field providing means for robust, practical, and
trustworthy deployment of AI models. Several XAI techniques have been proposed
for image classification tasks, whereas the interpretation of image
segmentation remains largely unexplored. This paper offers to bridge this gap
by adapting the recent XAI classification algorithms and making them usable for
muti-class image segmentation, where we mainly focus on buildings' segmentation
from high-resolution satellite images. To benchmark and compare the performance
of the proposed approaches, we introduce a new XAI evaluation methodology and
metric based on "Entropy" to measure the model uncertainty. Conventional XAI
evaluation methods rely mainly on feeding area-of-interest regions from the
image back to the pre-trained (utility) model and then calculating the average
change in the probability of the target class. Those evaluation metrics lack
the needed robustness, and we show that using Entropy to monitor the model
uncertainty in segmenting the pixels within the target class is more suitable.
We hope this work will pave the way for additional XAI research for image
segmentation and applications in the remote sensing discipline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02691">Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators. (arXiv:2310.02691v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mangeleer_V/0/1/0/all/0/1">Victor Mangeleer</a>, <a href="http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1">Gilles Louppe</a></p>
<p>In climate simulations, small-scale processes shape ocean dynamics but remain
computationally expensive to resolve directly. For this reason, their
contributions are commonly approximated using empirical parameterizations,
which lead to significant errors in long-term projections. In this work, we
develop parameterizations based on Fourier Neural Operators, showcasing their
accuracy and generalizability in comparison to other approaches. Finally, we
discuss the potential and limitations of neural networks operating in the
frequency domain, paving the way for future investigation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhigang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuelong Li</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04486">T-Rep: Representation Learning for Time Series using Time-Embeddings. (arXiv:2310.04486v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fraikin_A/0/1/0/all/0/1">Archibald Fraikin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bennetot_A/0/1/0/all/0/1">Adrien Bennetot</a>, <a href="http://arxiv.org/find/cs/1/au:+Allassonniere_S/0/1/0/all/0/1">St&#xe9;phanie Allassonni&#xe8;re</a></p>
<p>Multivariate time series present challenges to standard machine learning
techniques, as they are often unlabeled, high dimensional, noisy, and contain
missing data. To address this, we propose T-Rep, a self-supervised method to
learn time series representations at a timestep granularity. T-Rep learns
vector embeddings of time alongside its feature extractor, to extract temporal
features such as trend, periodicity, or distribution shifts from the signal.
These time-embeddings are leveraged in pretext tasks, to incorporate smooth and
fine-grained temporal dependencies in the representations, as well as reinforce
robustness to missing data. We evaluate T-Rep on downstream classification,
forecasting, and anomaly detection tasks. It is compared to existing
self-supervised algorithms for time series, which it outperforms in all three
tasks. We test T-Rep in missing data regimes, where it proves more resilient
than its counterparts. Finally, we provide latent space visualisation
experiments, highlighting the interpretability of the learned representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05204">Towards Optimizing with Large Language Models. (arXiv:2310.05204v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Pei-Fu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Ying-Hsuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yun-Da Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shou-De Lin</a></p>
<p>In this work, we conduct an assessment of the optimization capabilities of
LLMs across various tasks and data sizes. Each of these tasks corresponds to
unique optimization domains, and LLMs are required to execute these tasks with
interactive prompting. That is, in each optimization step, the LLM generates
new solutions from the past generated solutions with their values, and then the
new solutions are evaluated and considered in the next optimization step.
Additionally, we introduce three distinct metrics for a comprehensive
assessment of task performance from various perspectives. These metrics offer
the advantage of being applicable for evaluating LLM performance across a broad
spectrum of optimization tasks and are less sensitive to variations in test
samples. By applying these metrics, we observe that LLMs exhibit strong
optimization capabilities when dealing with small-sized samples. However, their
performance is significantly influenced by factors like data size and values,
underscoring the importance of further research in the domain of optimization
tasks for LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05898">Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lizhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a></p>
<p>Lion (Evolved Sign Momentum), a new optimizer discovered through program
search, has shown promising results in training large AI models. It performs
comparably or favorably to AdamW but with greater memory efficiency. As we can
expect from the results of a random search program, Lion incorporates elements
from several existing algorithms, including signed momentum, decoupled weight
decay, Polak, and Nesterov momentum, but does not fit into any existing
category of theoretically grounded optimizers. Thus, even though Lion appears
to perform well as a general-purpose optimizer for a wide range of tasks, its
theoretical basis remains uncertain. This lack of theoretical clarity limits
opportunities to further enhance and expand Lion's efficacy.
</p>
<p>This work aims to demystify Lion. Based on both continuous-time and
discrete-time analysis, we demonstrate that Lion is a theoretically novel and
principled approach for minimizing a general loss function $f(x)$ while
enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this
through the incorporation of decoupled weight decay, where $\lambda$ represents
the weight decay coefficient. Our analysis is made possible by the development
of a new Lyapunov function for the Lion updates. It applies to a broader family
of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is
replaced by the subgradient of a convex function $\kappa$, leading to the
solution of a general composite optimization problem of $\min_x f(x) +
\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion
and pave the way for further improvements and extensions of Lion-related
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06328">Antenna Response Consistency Driven Self-supervised Learning for WIFI-based Human Activity Recognition. (arXiv:2310.06328v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Ke Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiangtao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hongyuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1">Dingchang Zheng</a></p>
<p>Self-supervised learning (SSL) for WiFi-based human activity recognition
(HAR) holds great promise due to its ability to address the challenge of
insufficient labeled data. However, directly transplanting SSL algorithms,
especially contrastive learning, originally designed for other domains to CSI
data, often fails to achieve the expected performance. We attribute this issue
to the inappropriate alignment criteria, which disrupt the semantic distance
consistency between the feature space and the input space. To address this
challenge, we introduce \textbf{A}ntenna \textbf{R}esponse \textbf{C}onsistency
(ARC) as a solution to define proper alignment criteria. ARC is designed to
retain semantic information from the input space while introducing robustness
to real-world noise. Moreover, we substantiate the effectiveness of ARC through
a comprehensive set of experiments, demonstrating its capability to enhance the
performance of self-supervised learning for WiFi-based HAR by achieving an
increase of over 5\% in accuracy in most cases and achieving a best accuracy of
94.97\%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06627">What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models. (arXiv:2310.06627v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Letian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaotong Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhongkai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1">Yongshuo Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bingchen Zhao</a></p>
<p>Counterfactual reasoning, a fundamental aspect of human cognition, involves
contemplating alternatives to established facts or past events, significantly
enhancing our abilities in planning and decision-making. In light of the
advancements in current multi-modal large language models, we explore their
effectiveness in counterfactual reasoning. To facilitate this investigation, we
introduce a novel dataset, C-VQA, specifically designed to test the
counterfactual reasoning capabilities of modern multi-modal large language
models. This dataset is constructed by infusing original questions with
counterfactual presuppositions, spanning various types such as numerical and
boolean queries. It encompasses a mix of real and synthetic data, representing
a wide range of difficulty levels. Our thorough evaluations of contemporary
vision-language models using this dataset have revealed substantial performance
drops, with some models showing up to a 40% decrease, highlighting a
significant gap between current models and human-like vision reasoning
capabilities. We hope our dataset will serve as a vital benchmark for
evaluating the counterfactual reasoning capabilities of models. Code and
dataset are publicly available at https://bzhao.me/C-VQA/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08164">Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marks_L/0/1/0/all/0/1">Luke Marks</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullah_A/0/1/0/all/0/1">Amir Abdullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendez_L/0/1/0/all/0/1">Luna Mendez</a>, <a href="http://arxiv.org/find/cs/1/au:+Arike_R/0/1/0/all/0/1">Rauno Arike</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a>, <a href="http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1">Fazl Barez</a></p>
<p>Large language models (LLMs) aligned to human preferences via reinforcement
learning from human feedback (RLHF) underpin many commercial applications of
LLM technology. Despite this, the impacts of RLHF on LLM internals remain
opaque. We propose a novel method for interpreting implicit reward models
(IRMs) in LLMs learned through RLHF. Our approach trains pairs of autoencoders
on activations from a base LLM and its RLHF-tuned variant. Through a comparison
of autoencoder hidden spaces, we identify features that reflect the accuracy of
the learned IRM. To illustrate our method, we fine-tune an LLM via RLHF to
learn a token-utility mapping and maximize the aggregate utility of generated
text. This is the first application of sparse autoencoders to interpreting
IRMs. Our method provides an abstract approximation of reward integrity and
holds promise for measuring alignment between specified objectives and learned
model behaviors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08165">COVID-19 detection using ViT transformer-based approach from Computed Tomography Images. (arXiv:2310.08165v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1">Kenan Morani</a></p>
<p>In here, we introduce a novel approach to enhance the accuracy and efficiency
of COVID-19 diagnosis using CT images. Leveraging state-of-the-art Transformer
models in computer vision, we employed the base ViT Transformer configured for
224x224-sized input images, modifying the output to suit the binary
classification task. Notably, input images were resized from the standard CT
scan size of 512x512 to match the model's expectations. Our method implements a
systematic patient-level prediction strategy, classifying individual CT slices
as COVID-19 or non-COVID. To determine the overall diagnosis for each patient,
a majority voting approach as well as other thresholding approaches were
employed. This method involves evaluating all CT slices for a given patient and
assigning the patient the diagnosis that relates to the thresholding for the CT
scan. This meticulous patient-level prediction process contributes to the
robustness of our solution as it starts from 2D-slices to 3D-patient level.
Throughout the evaluation process, our approach resulted in 0.7 macro F1 score
on the COV19-CT -DB validation set. To ensure the reliability and effectiveness
of our model, we rigorously validate it on the extensive COV-19 CT dataset,
which is meticulously annotated for the task. This dataset, with its
comprehensive annotations, reinforces the overall robustness of our solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08659">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1">Chen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Pengcheng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Karampatziakis_N/0/1/0/all/0/1">Nikos Karampatziakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weizhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a></p>
<p>Quantization is an indispensable technique for serving Large Language Models
(LLMs) and has recently found its way into LoRA fine-tuning. In this work we
focus on the scenario where quantization and LoRA fine-tuning are applied
together on a pre-trained model. In such cases it is common to observe a
consistent gap in the performance on downstream tasks between full fine-tuning
and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ
(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that
simultaneously quantizes an LLM and finds a proper low-rank initialization for
LoRA fine-tuning. Such an initialization alleviates the discrepancy between the
quantized and full-precision model and significantly improves generalization in
downstream tasks. We evaluate our method on natural language understanding,
question answering, summarization, and natural language generation tasks.
Experiments show that our method is highly effective and outperforms existing
quantization methods, especially in the challenging 2-bit and 2/4-bit mixed
precision regimes. The code is available on https://github.com/yxli2123/LoftQ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11676">PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Junjun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yizhen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a></p>
<p>Node-level graph anomaly detection (GAD) plays a critical role in identifying
anomalous nodes from graph-structured data in various domains such as medicine,
social networks, and e-commerce. However, challenges have arisen due to the
diversity of anomalies and the dearth of labeled data. Existing methodologies -
reconstruction-based and contrastive learning - while effective, often suffer
from efficiency issues, stemming from their complex objectives and elaborate
modules. To improve the efficiency of GAD, we introduce a simple method termed
PREprocessing and Matching (PREM for short). Our approach streamlines GAD,
reducing time and memory consumption while maintaining powerful anomaly
detection capabilities. Comprising two modules - a pre-processing module and an
ego-neighbor matching module - PREM eliminates the necessity for
message-passing propagation during training, and employs a simple contrastive
loss, leading to considerable reductions in training time and memory usage.
Moreover, through rigorous evaluations of five real-world datasets, our method
demonstrated robustness and effectiveness. Notably, when validated on the ACM
dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training
speed, and sharply reduce memory usage compared to the most efficient baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13164">Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McNeela_D/0/1/0/all/0/1">Daniel McNeela</a></p>
<p>Recently, the equivariance of models with respect to a group action has
become an important topic of research in machine learning. However, imbuing an
architecture with a specific group equivariance imposes a strong prior on the
types of data transformations that the model expects to see. While
strictly-equivariant models enforce symmetries, real-world data does not always
conform to such strict equivariances, be it due to noise in the data or
underlying physical laws that encode only approximate or partial symmetries. In
such cases, the prior of strict equivariance can actually prove too strong and
cause models to underperform on real-world data. Therefore, in this work we
study a closely related topic, that of almost equivariance. We provide a
definition of almost equivariance that differs from those extant in the current
literature and give a practical method for encoding almost equivariance in
models by appealing to the Lie algebra of a Lie group. Specifically, we define
Lie algebra convolutions and demonstrate that they offer several benefits over
Lie group convolutions, including being well-defined for non-compact groups.
From there, we pivot to the realm of theory and demonstrate connections between
the notions of equivariance and isometry and those of almost equivariance and
almost isometry, respectively. We prove two existence theorems, one showing the
existence of almost isometries within bounded distance of isometries of a
general manifold, and another showing the converse for Hilbert spaces. We then
extend these theorems to prove the existence of almost equivariant manifold
embeddings within bounded distance of fully equivariant embedding functions,
subject to certain constraints on the group action and the function class.
Finally, we demonstrate the validity of our approach by benchmarking against
datasets in fully equivariant and almost equivariant settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14045">Training Image Derivatives: Increased Accuracy and Universal Robustness. (arXiv:2310.14045v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Avrutskiy_V/0/1/0/all/0/1">Vsevolod I. Avrutskiy</a></p>
<p>Derivative training is a known method that significantly improves the
accuracy of neural networks in some low-dimensional applications. In this
paper, a similar improvement is obtained for an image analysis problem:
reconstructing the vertices of a cube from its image. By training the
derivatives with respect to the 6 degrees of freedom of the cube, we obtain 25
times more accurate results for noiseless inputs. The derivatives also offer
insight into the robustness problem, which is currently understood in terms of
two types of network vulnerabilities. The first type involves small
perturbations that dramatically change the output, and the second type relates
to substantial image changes that the network erroneously ignores. Defense
against each is possible, but safeguarding against both while maintaining the
accuracy defies conventional training methods. The first type is analyzed using
the network's gradient, while the second relies on human input evaluation,
serving as an oracle substitute. For the task at hand, the nearest neighbor
oracle can be defined and expanded into Taylor series using image derivatives.
This allows for a robustness analysis that unifies both types of
vulnerabilities and enables training where accuracy and universal robustness
are limited only by network capacity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14714">BatteryML:An Open-source platform for Machine Learning on Battery Degradation. (arXiv:2310.14714v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1">Xiaofan Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shun Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Ziheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16252">Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits &amp; Dueling Bandits. (arXiv:2310.16252v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maiti_A/0/1/0/all/0/1">Arnab Maiti</a>, <a href="http://arxiv.org/find/cs/1/au:+Boczar_R/0/1/0/all/0/1">Ross Boczar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1">Kevin Jamieson</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1">Lillian J. Ratliff</a></p>
<p>We study the sample complexity of identifying the pure strategy Nash
equilibrium (PSNE) in a two-player zero-sum matrix game with noise. Formally,
we are given a stochastic model where any learner can sample an entry $(i,j)$
of the input matrix $A\in[-1,1]^{n\times m}$ and observe $A_{i,j}+\eta$ where
$\eta$ is a zero-mean 1-sub-Gaussian noise. The aim of the learner is to
identify the PSNE of $A$, whenever it exists, with high probability while
taking as few samples as possible. Zhou et al. (2017) presents an
instance-dependent sample complexity lower bound that depends only on the
entries in the row and column in which the PSNE lies. We design a near-optimal
algorithm whose sample complexity matches the lower bound, up to log factors.
The problem of identifying the PSNE also generalizes the problem of pure
exploration in stochastic multi-armed bandits and dueling bandits, and our
result matches the optimal bounds, up to log factors, in both the settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17639">In-Context Learning Dynamics with Random Binary Sequences. (arXiv:2310.17639v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bigelow_E/0/1/0/all/0/1">Eric J. Bigelow</a>, <a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1">Ekdeep Singh Lubana</a>, <a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1">Robert P. Dick</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1">Hidenori Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1">Tomer D. Ullman</a></p>
<p>Large language models (LLMs) trained on huge corpora of text datasets
demonstrate intriguing capabilities, achieving state-of-the-art performance on
tasks they were not explicitly trained for. The precise nature of LLM
capabilities is often mysterious, and different prompts can elicit different
capabilities through in-context learning. We propose a framework that enables
us to analyze in-context learning dynamics to understand latent concepts
underlying LLMs' behavioral patterns. This provides a more nuanced
understanding than success-or-failure evaluation benchmarks, but does not
require observing internal activations as a mechanistic interpretation of
circuits would. Inspired by the cognitive science of human randomness
perception, we use random binary sequences as context and study dynamics of
in-context learning by manipulating properties of context data, such as
sequence length. In the latest GPT-3.5+ models, we find emergent abilities to
generate seemingly random numbers and learn basic formal languages, with
striking in-context learning dynamics where model outputs transition sharply
from seemingly random behaviors to deterministic repetition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18515">Learning to design protein-protein interactions with enhanced generalization. (arXiv:2310.18515v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bushuiev_A/0/1/0/all/0/1">Anton Bushuiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Bushuiev_R/0/1/0/all/0/1">Roman Bushuiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouba_P/0/1/0/all/0/1">Petr Kouba</a>, <a href="http://arxiv.org/find/cs/1/au:+Filkin_A/0/1/0/all/0/1">Anatolii Filkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabrielova_M/0/1/0/all/0/1">Marketa Gabrielova</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabriel_M/0/1/0/all/0/1">Michal Gabriel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1">Jiri Sedlar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pluskal_T/0/1/0/all/0/1">Tomas Pluskal</a>, <a href="http://arxiv.org/find/cs/1/au:+Damborsky_J/0/1/0/all/0/1">Jiri Damborsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurenko_S/0/1/0/all/0/1">Stanislav Mazurenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1">Josef Sivic</a></p>
<p>Discovering mutations enhancing protein-protein interactions (PPIs) is
critical for advancing biomedical research and developing improved
therapeutics. While machine learning approaches have substantially advanced the
field, they often struggle to generalize beyond training data in practical
scenarios. The contributions of this work are three-fold. First, we construct
PPIRef, the largest and non-redundant dataset of 3D protein-protein
interactions, enabling effective large-scale learning. Second, we leverage the
PPIRef dataset to pre-train PPIformer, a new SE(3)-equivariant model
generalizing across diverse protein-binder variants. We fine-tune PPIformer to
predict effects of mutations on protein-protein interactions via a
thermodynamically motivated adjustment of the pre-training loss function.
Finally, we demonstrate the enhanced generalization of our new PPIformer
approach by outperforming other state-of-the-art methods on new, non-leaking
splits of standard labeled PPI mutational data and independent case studies
optimizing a human antibody against SARS-CoV-2 and increasing the thrombolytic
activity of staphylokinase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06965">Anchor Data Augmentation. (arXiv:2311.06965v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1">Nora Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Goshtasbpour_S/0/1/0/all/0/1">Shirin Goshtasbpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1">Fernando Perez-Cruz</a></p>
<p>We propose a novel algorithm for data augmentation in nonlinear
over-parametrized regression. Our data augmentation algorithm borrows from the
literature on causality and extends the recently proposed Anchor regression
(AR) method for data augmentation, which is in contrast to the current
state-of-the-art domain-agnostic solutions that rely on the Mixup literature.
Our Anchor Data Augmentation (ADA) uses several replicas of the modified
samples in AR to provide more training examples, leading to more robust
regression predictions. We apply ADA to linear and nonlinear regression
problems using neural networks. ADA is competitive with state-of-the-art
C-Mixup solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07222">Neural General Circulation Models. (arXiv:2311.07222v2 [physics.ao-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kochkov_D/0/1/0/all/0/1">Dmitrii Kochkov</a>, <a href="http://arxiv.org/find/physics/1/au:+Yuval_J/0/1/0/all/0/1">Janni Yuval</a>, <a href="http://arxiv.org/find/physics/1/au:+Langmore_I/0/1/0/all/0/1">Ian Langmore</a>, <a href="http://arxiv.org/find/physics/1/au:+Norgaard_P/0/1/0/all/0/1">Peter Norgaard</a>, <a href="http://arxiv.org/find/physics/1/au:+Smith_J/0/1/0/all/0/1">Jamie Smith</a>, <a href="http://arxiv.org/find/physics/1/au:+Mooers_G/0/1/0/all/0/1">Griffin Mooers</a>, <a href="http://arxiv.org/find/physics/1/au:+Lottes_J/0/1/0/all/0/1">James Lottes</a>, <a href="http://arxiv.org/find/physics/1/au:+Rasp_S/0/1/0/all/0/1">Stephan Rasp</a>, <a href="http://arxiv.org/find/physics/1/au:+Duben_P/0/1/0/all/0/1">Peter D&#xfc;ben</a>, <a href="http://arxiv.org/find/physics/1/au:+Klower_M/0/1/0/all/0/1">Milan Kl&#xf6;wer</a>, <a href="http://arxiv.org/find/physics/1/au:+Hatfield_S/0/1/0/all/0/1">Sam Hatfield</a>, <a href="http://arxiv.org/find/physics/1/au:+Battaglia_P/0/1/0/all/0/1">Peter Battaglia</a>, <a href="http://arxiv.org/find/physics/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1">Alvaro Sanchez-Gonzalez</a>, <a href="http://arxiv.org/find/physics/1/au:+Willson_M/0/1/0/all/0/1">Matthew Willson</a>, <a href="http://arxiv.org/find/physics/1/au:+Brenner_M/0/1/0/all/0/1">Michael P. Brenner</a>, <a href="http://arxiv.org/find/physics/1/au:+Hoyer_S/0/1/0/all/0/1">Stephan Hoyer</a></p>
<p>General circulation models (GCMs) are the foundation of weather and climate
prediction. GCMs are physics-based simulators which combine a numerical solver
for large-scale dynamics with tuned representations for small-scale processes
such as cloud formation. Recently, machine learning (ML) models trained on
reanalysis data achieved comparable or better skill than GCMs for deterministic
weather forecasting. However, these models have not demonstrated improved
ensemble forecasts, or shown sufficient stability for long-term weather and
climate simulations. Here we present the first GCM that combines a
differentiable solver for atmospheric dynamics with ML components, and show
that it can generate forecasts of deterministic weather, ensemble weather and
climate on par with the best ML and physics-based methods. NeuralGCM is
competitive with ML models for 1-10 day forecasts, and with the European Centre
for Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts.
With prescribed sea surface temperature, NeuralGCM can accurately track climate
metrics such as global mean temperature for multiple decades, and climate
forecasts with 140 km resolution exhibit emergent phenomena such as realistic
frequency and trajectories of tropical cyclones. For both weather and climate,
our approach offers orders of magnitude computational savings over conventional
GCMs. Our results show that end-to-end deep learning is compatible with tasks
performed by conventional GCMs, and can enhance the large-scale physical
simulations that are essential for understanding and predicting the Earth
system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08379">Scheming AIs: Will AIs fake alignment during training in order to get power?. (arXiv:2311.08379v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carlsmith_J/0/1/0/all/0/1">Joe Carlsmith</a></p>
<p>This report examines whether advanced AIs that perform well in training will
be doing so in order to gain power later -- a behavior I call "scheming" (also
sometimes called "deceptive alignment"). I conclude that scheming is a
disturbingly plausible outcome of using baseline machine learning methods to
train goal-directed AIs sophisticated enough to scheme (my subjective
probability on such an outcome, given these conditions, is roughly 25%). In
particular: if performing well in training is a good strategy for gaining power
(as I think it might well be), then a very wide variety of goals would motivate
scheming -- and hence, good training performance. This makes it plausible that
training might either land on such a goal naturally and then reinforce it, or
actively push a model's motivations towards such a goal as an easy way of
improving performance. What's more, because schemers pretend to be aligned on
tests designed to reveal their motivations, it may be quite difficult to tell
whether this has occurred. However, I also think there are reasons for comfort.
In particular: scheming may not actually be such a good strategy for gaining
power; various selection pressures in training might work against schemer-like
goals (for example, relative to non-schemers, schemers need to engage in extra
instrumental reasoning, which might harm their training performance); and we
may be able to increase such pressures intentionally. The report discusses
these and a wide variety of other considerations in detail, and it suggests an
array of empirical research directions for probing the topic further.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09312">H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing. (arXiv:2311.09312v2 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Visani_G/0/1/0/all/0/1">Gian Marco Visani</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Galvin_W/0/1/0/all/0/1">William Galvin</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Pun_M/0/1/0/all/0/1">Michael Neal Pun</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nourmohammad_A/0/1/0/all/0/1">Armita Nourmohammad</a></p>
<p>Accurately modeling protein 3D structure is essential for the design of
functional proteins. An important sub-task of structure modeling is protein
side-chain packing: predicting the conformation of side-chains (rotamers) given
the protein's backbone structure and amino-acid sequence. Conventional
approaches for this task rely on expensive sampling procedures over
hand-crafted energy functions and rotamer libraries. Recently, several deep
learning methods have been developed to tackle the problem in a data-driven
way, albeit with vastly different formulations (from image-to-image translation
to directly predicting atomic coordinates). Here, we frame the problem as a
joint regression over the side-chains' true degrees of freedom: the dihedral
$\chi$ angles. We carefully study possible objective functions for this task,
while accounting for the underlying symmetries of the task. We propose
Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain
packing built on top of two light-weight rotationally equivariant neural
networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is
computationally efficient and shows favorable performance against conventional
physics-based algorithms and is competitive against alternative deep learning
solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09506">Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer in Continual Learning. (arXiv:2311.09506v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Andle_J/0/1/0/all/0/1">Josh Andle</a>, <a href="http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1">Ali Payani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasaei_Sekeh_S/0/1/0/all/0/1">Salimeh Yasaei-Sekeh</a></p>
<p>Continual Learning (CL) has generated attention as a method of avoiding
Catastrophic Forgetting (CF) in the sequential training of neural networks,
improving network efficiency and adaptability to different tasks. Additionally,
CL serves as an ideal setting for studying network behavior and Forward
Knowledge Transfer (FKT) between tasks. Pruning methods for CL train
subnetworks to handle the sequential tasks which allows us to take a structured
approach to investigating FKT. Sharing prior subnetworks' weights leverages
past knowledge for the current task through FKT. Understanding which weights to
share is important as sharing all weights can yield sub-optimal accuracy. This
paper investigates how different sharing decisions affect the FKT between
tasks. Through this lens we demonstrate how task complexity and similarity
influence the optimal weight sharing decisions, giving insights into the
relationships between tasks and helping inform decision making in similar CL
methods. We implement three sequential datasets designed to emphasize variation
in task complexity and similarity, reporting results for both ResNet-18 and
VGG-16. By sharing in accordance with the decisions supported by our findings,
we show that we can improve task accuracy compared to other sharing decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09790">Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting. (arXiv:2311.09790v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ilbert_R/0/1/0/all/0/1">Romain Ilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thai V. Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zonghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1">Themis Palpanas</a></p>
<p>Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. Our
optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11876">Forward Gradients for Data-Driven CFD Wall Modeling. (arXiv:2311.11876v2 [physics.flu-dyn] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Huckelheim_J/0/1/0/all/0/1">Jan H&#xfc;ckelheim</a>, <a href="http://arxiv.org/find/physics/1/au:+Kumar_T/0/1/0/all/0/1">Tadbhagya Kumar</a>, <a href="http://arxiv.org/find/physics/1/au:+Raghavan_K/0/1/0/all/0/1">Krishnan Raghavan</a>, <a href="http://arxiv.org/find/physics/1/au:+Pal_P/0/1/0/all/0/1">Pinaki Pal</a></p>
<p>Computational Fluid Dynamics (CFD) is used in the design and optimization of
gas turbines and many other industrial/ scientific applications. However, the
practical use is often limited by the high computational cost, and the accurate
resolution of near-wall flow is a significant contributor to this cost. Machine
learning (ML) and other data-driven methods can complement existing wall
models. Nevertheless, training these models is bottlenecked by the large
computational effort and memory footprint demanded by back-propagation. Recent
work has presented alternatives for computing gradients of neural networks
where a separate forward and backward sweep is not needed and storage of
intermediate results between sweeps is not required because an unbiased
estimator for the gradient is computed in a single forward sweep. In this
paper, we discuss the application of this approach for training a subgrid wall
model that could potentially be used as a surrogate in wall-bounded flow CFD
simulations to reduce the computational overhead while preserving predictive
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12089">Explaining Deep Learning Models for Age-related Gait Classification based on time series acceleration. (arXiv:2311.12089v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoping Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Otten_B/0/1/0/all/0/1">Bert Otten</a>, <a href="http://arxiv.org/find/cs/1/au:+Reneman_M/0/1/0/all/0/1">Michiel F Reneman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamoth_C/0/1/0/all/0/1">Claudine JC Lamoth</a></p>
<p>Gait analysis holds significant importance in monitoring daily health,
particularly among older adults. Advancements in sensor technology enable the
capture of movement in real-life environments and generate big data. Machine
learning, notably deep learning (DL), shows promise to use these big data in
gait analysis. However, the inherent black-box nature of these models poses
challenges for their clinical application. This study aims to enhance
transparency in DL-based gait classification for aged-related gait patterns
using Explainable Artificial Intelligence, such as SHAP.
</p>
<p>A total of 244 subjects, comprising 129 adults and 115 older adults (age&gt;65),
were included. They performed a 3-minute walking task while accelerometers were
affixed to the lumbar segment L3. DL models, convolutional neural network (CNN)
and gated recurrent unit (GRU), were trained using 1-stride and 8-stride
accelerations, respectively, to classify adult and older adult groups. SHAP was
employed to explain the models' predictions.
</p>
<p>CNN achieved a satisfactory performance with an accuracy of 81.4% and an AUC
of 0.89, and GRU demonstrated promising results with an accuracy of 84.5% and
an AUC of 0.94. SHAP analysis revealed that both CNN and GRU assigned higher
SHAP values to the data from vertical and walking directions, particularly
emphasizing data around heel contact, spanning from the terminal swing to
loading response phases. Furthermore, SHAP values indicated that GRU did not
treat every stride equally.
</p>
<p>CNN accurately distinguished between adults and older adults based on the
characteristics of a single stride's data. GRU achieved accurate classification
by considering the relationships and subtle differences between strides. In
both models, data around heel contact emerged as most critical, suggesting
differences in acceleration and deceleration patterns during walking between
different age groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12399">A Survey of Graph Meets Large Language Model: Progress and Future Directions. (arXiv:2311.12399v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhixun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peisong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangguo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jeffrey Xu Yu</a></p>
<p>Graph plays a significant role in representing and analyzing complex
relationships in real-world applications such as citation networks, social
networks, and biological data. Recently, Large Language Models (LLMs), which
have achieved tremendous success in various domains, have also been leveraged
in graph-related tasks to surpass traditional Graph Neural Networks (GNNs)
based methods and yield state-of-the-art performance. In this survey, we first
present a comprehensive review and analysis of existing methods that integrate
LLMs with graphs. First of all, we propose a new taxonomy, which organizes
existing methods into three categories based on the role (i.e., enhancer,
predictor, and alignment component) played by LLMs in graph-related tasks. Then
we systematically survey the representative methods along the three categories
of the taxonomy. Finally, we discuss the remaining limitations of existing
studies and highlight promising avenues for future research. The relevant
papers are summarized and will be consistently updated at:
https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12878">Adaptive Bayesian Learning with Action and State-Dependent Signal Variance. (arXiv:2311.12878v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hou_K/0/1/0/all/0/1">Kaiwen Hou</a></p>
<p>This manuscript presents an advanced framework for Bayesian learning by
incorporating action and state-dependent signal variances into decision-making
models. This framework is pivotal in understanding complex data-feedback loops
and decision-making processes in various economic systems. Through a series of
examples, we demonstrate the versatility of this approach in different
contexts, ranging from simple Bayesian updating in stable environments to
complex models involving social learning and state-dependent uncertainties. The
paper uniquely contributes to the understanding of the nuanced interplay
between data, actions, outcomes, and the inherent uncertainty in economic
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14646">More is Better in Modern Machine Learning: when Infinite Overparameterization is Optimal and Overfitting is Obligatory. (arXiv:2311.14646v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simon_J/0/1/0/all/0/1">James B. Simon</a>, <a href="http://arxiv.org/find/cs/1/au:+Karkada_D/0/1/0/all/0/1">Dhruva Karkada</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_N/0/1/0/all/0/1">Nikhil Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1">Mikhail Belkin</a></p>
<p>In our era of enormous neural networks, empirical progress has been driven by
the philosophy that more is better. Recent deep learning practice has found
repeatedly that larger model size, more data, and more computation (resulting
in lower training loss) improves performance. In this paper, we give
theoretical backing to these empirical observations by showing that these three
properties hold in random feature (RF) regression, a class of models equivalent
to shallow networks with only the last layer trained.
</p>
<p>Concretely, we first show that the test risk of RF regression decreases
monotonically with both the number of features and the number of samples,
provided the ridge penalty is tuned optimally. In particular, this implies that
infinite width RF architectures are preferable to those of any finite width. We
then proceed to demonstrate that, for a large class of tasks characterized by
powerlaw eigenstructure, training to near-zero training loss is obligatory:
near-optimal performance can only be achieved when the training error is much
smaller than the test error. Grounding our theory in real-world data, we find
empirically that standard computer vision tasks with convolutional neural
tangent kernels clearly fall into this class. Taken together, our results tell
a simple, testable story of the benefits of overparameterization, overfitting,
and more data in random feature models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14971">Segmentation of diagnostic tissue compartments on whole slide images with renal thrombotic microangiopathies (TMAs). (arXiv:2311.14971v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1">Huy Q. Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cicalese_P/0/1/0/all/0/1">Pietro A. Cicalese</a>, <a href="http://arxiv.org/find/cs/1/au:+Seshan_S/0/1/0/all/0/1">Surya Seshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizvi_S/0/1/0/all/0/1">Syed A. Rizvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vathul_A/0/1/0/all/0/1">Aneesh Vathul</a>, <a href="http://arxiv.org/find/cs/1/au:+Bueno_G/0/1/0/all/0/1">Gloria Bueno</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorado_A/0/1/0/all/0/1">Anibal Pedraza Dorado</a>, <a href="http://arxiv.org/find/cs/1/au:+Grabe_N/0/1/0/all/0/1">Niels Grabe</a>, <a href="http://arxiv.org/find/cs/1/au:+Stolle_K/0/1/0/all/0/1">Katharina Stolle</a>, <a href="http://arxiv.org/find/cs/1/au:+Pesce_F/0/1/0/all/0/1">Francesco Pesce</a>, <a href="http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1">Joris J.T.H. Roelofs</a>, <a href="http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1">Jesper Kers</a>, <a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_V/0/1/0/all/0/1">Vitoantonio Bevilacqua</a>, <a href="http://arxiv.org/find/cs/1/au:+Altini_N/0/1/0/all/0/1">Nicola Altini</a>, <a href="http://arxiv.org/find/cs/1/au:+Schroppel_B/0/1/0/all/0/1">Bernd Schr&#xf6;ppel</a>, <a href="http://arxiv.org/find/cs/1/au:+Roccatello_D/0/1/0/all/0/1">Dario Roccatello</a>, <a href="http://arxiv.org/find/cs/1/au:+Barreca_A/0/1/0/all/0/1">Antonella Barreca</a>, <a href="http://arxiv.org/find/cs/1/au:+Sciascia_S/0/1/0/all/0/1">Savino Sciascia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1">Chandra Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hien V. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Becker_J/0/1/0/all/0/1">Jan U. Becker</a></p>
<p>The thrombotic microangiopathies (TMAs) manifest in renal biopsy histology
with a broad spectrum of acute and chronic findings. Precise diagnostic
criteria for a renal biopsy diagnosis of TMA are missing. As a first step
towards a machine learning- and computer vision-based analysis of wholes slide
images from renal biopsies, we trained a segmentation model for the decisive
diagnostic kidney tissue compartments artery, arteriole, glomerulus on a set of
whole slide images from renal biopsies with TMAs and Mimickers (distinct
diseases with a similar nephropathological appearance as TMA like severe benign
nephrosclerosis, various vasculitides, Bevacizumab-plug glomerulopathy,
arteriolar light chain deposition disease). Our segmentation model combines a
U-Net-based tissue detection with a Shifted windows-transformer architecture to
reach excellent segmentation results for even the most severely altered
glomeruli, arterioles and arteries, even on unseen staining domains from a
different nephropathology lab. With accurate automatic segmentation of the
decisive renal biopsy compartments in human renal vasculopathies, we have laid
the foundation for large-scale compartment-specific machine learning and
computer vision analysis of renal biopsy repositories with TMAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15243">ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection. (arXiv:2311.15243v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yichen Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zongbo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1">Bing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaoheng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qinghua Hu</a></p>
<p>Out-of-distribution (OOD) detection methods often exploit auxiliary outliers
to train model identifying OOD samples, especially discovering challenging
outliers from auxiliary outliers dataset to improve OOD detection. However,
they may still face limitations in effectively distinguishing between the most
challenging OOD samples that are much like in-distribution (ID) data, i.e.,
ID-like samples. To this end, we propose a novel OOD detection framework that
discovers ID-like outliers using CLIP from the vicinity space of the ID
samples, thus helping to identify these most challenging OOD samples. Then a
prompt learning framework is proposed that utilizes the identified ID-like
outliers to further leverage the capabilities of CLIP for OOD detection.
Benefiting from the powerful CLIP, we only need a small number of ID samples to
learn the prompts of the model without exposing other auxiliary outlier
datasets. By focusing on the most challenging ID-like OOD samples and elegantly
exploiting the capabilities of CLIP, our method achieves superior few-shot
learning performance on various real-world image datasets (e.g., in 4-shot OOD
detection on the ImageNet-1k dataset, our method reduces the average FPR95 by
12.16% and improves the average AUROC by 2.76%, compared to state-of-the-art
methods).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15936">Towards Responsible Governance of Biological Design Tools. (arXiv:2311.15936v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moulange_R/0/1/0/all/0/1">Richard Moulange</a>, <a href="http://arxiv.org/find/cs/1/au:+Langenkamp_M/0/1/0/all/0/1">Max Langenkamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexanian_T/0/1/0/all/0/1">Tessa Alexanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Curtis_S/0/1/0/all/0/1">Samuel Curtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Livingston_M/0/1/0/all/0/1">Morgan Livingston</a></p>
<p>Recent advancements in generative machine learning have enabled rapid
progress in biological design tools (BDTs) such as protein structure and
sequence prediction models. The unprecedented predictive accuracy and novel
design capabilities of BDTs present new and significant dual-use risks. For
example, their predictive accuracy allows biological agents, whether vaccines
or pathogens, to be developed more quickly, while the design capabilities could
be used to discover drugs or evade DNA screening techniques. Similar to other
dual-use AI systems, BDTs present a wicked problem: how can regulators uphold
public safety without stifling innovation? We highlight how current regulatory
proposals that are primarily tailored toward large language models may be less
effective for BDTs, which require fewer computational resources to train and
are often developed in an open-source manner. We propose a range of measures to
mitigate the risk that BDTs are misused, across the areas of responsible
development, risk assessment, transparency, access management, cybersecurity,
and investing in resilience. Implementing such measures will require close
coordination between developers and governments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15951">Replay across Experiments: A Natural Extension of Off-Policy RL. (arXiv:2311.15951v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tirumala_D/0/1/0/all/0/1">Dhruva Tirumala</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampe_T/0/1/0/all/0/1">Thomas Lampe</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jose Enrique Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Haarnoja_T/0/1/0/all/0/1">Tuomas Haarnoja</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Sandy Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lever_G/0/1/0/all/0/1">Guy Lever</a>, <a href="http://arxiv.org/find/cs/1/au:+Moran_B/0/1/0/all/0/1">Ben Moran</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertweck_T/0/1/0/all/0/1">Tim Hertweck</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1">Leonard Hasenclever</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1">Martin Riedmiller</a>, <a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1">Nicolas Heess</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1">Markus Wulfmeier</a></p>
<p>Replaying data is a principal mechanism underlying the stability and data
efficiency of off-policy reinforcement learning (RL). We present an effective
yet simple framework to extend the use of replays across multiple experiments,
minimally adapting the RL workflow for sizeable improvements in controller
performance and research iteration times. At its core, Replay Across
Experiments (RaE) involves reusing experience from previous experiments to
improve exploration and bootstrap learning while reducing required changes to a
minimum in comparison to prior work. We empirically show benefits across a
number of RL algorithms and challenging control domains spanning both
locomotion and manipulation, including hard exploration tasks from egocentric
vision. Through comprehensive ablations, we demonstrate robustness to the
quality and amount of data available and various hyperparameter choices.
Finally, we discuss how our approach can be applied more broadly across
research life cycles and can increase resilience by reloading data across
random seeds or hyperparameter variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.10328">TransONet: Automatic Segmentation of Vasculature in Computed Tomographic Angiograms Using Deep Learning. (arXiv:2311.10328v1 [eess.IV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rajeoni_A/0/1/0/all/0/1">Alireza Bagheri Rajeoni</a>, <a href="http://arxiv.org/find/eess/1/au:+Pederson_B/0/1/0/all/0/1">Breanna Pederson</a>, <a href="http://arxiv.org/find/eess/1/au:+Firooz_A/0/1/0/all/0/1">Ali Firooz</a>, <a href="http://arxiv.org/find/eess/1/au:+Abdollahi_H/0/1/0/all/0/1">Hamed Abdollahi</a>, <a href="http://arxiv.org/find/eess/1/au:+Smith_A/0/1/0/all/0/1">Andrew K. Smith</a>, <a href="http://arxiv.org/find/eess/1/au:+Clair_D/0/1/0/all/0/1">Daniel G. Clair</a>, <a href="http://arxiv.org/find/eess/1/au:+Lessner_S/0/1/0/all/0/1">Susan M. Lessner</a>, <a href="http://arxiv.org/find/eess/1/au:+Valafar_H/0/1/0/all/0/1">Homayoun Valafar</a></p>
<p>Pathological alterations in the human vascular system underlie many chronic
diseases, such as atherosclerosis and aneurysms. However, manually analyzing
diagnostic images of the vascular system, such as computed tomographic
angiograms (CTAs) is a time-consuming and tedious process. To address this
issue, we propose a deep learning model to segment the vascular system in CTA
images of patients undergoing surgery for peripheral arterial disease (PAD).
Our study focused on accurately segmenting the vascular system (1) from the
descending thoracic aorta to the iliac bifurcation and (2) from the descending
thoracic aorta to the knees in CTA images using deep learning techniques. Our
approach achieved average Dice accuracies of 93.5% and 80.64% in test dataset
for (1) and (2), respectively, highlighting its high accuracy and potential
clinical utility. These findings demonstrate the use of deep learning
techniques as a valuable tool for medical professionals to analyze the health
of the vascular system efficiently and accurately. Please visit the GitHub page
for this paper at https://github.com/pip-alireza/TransOnet.
</p>
</p>
</div>

    </div>
    </body>
    