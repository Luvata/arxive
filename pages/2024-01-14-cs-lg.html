<!DOCTYPE html>
<html>
<head>
<title>2024-01-14-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.05337">Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals. (arXiv:2401.05337v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Renucci_P/0/1/0/all/0/1">Pierre Renucci</a></p>
<p>This study presents an unsupervised machine learning approach for optimizing
Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an
unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL
generated from signals constructed linearly from exogenous variables. The
methodology employs a linear relationship between exogenous variables and the
trading signal, with the objective of maximizing the Sharpe Ratio through
parameter optimization. Empirical application on an ETF representing U.S.
Treasury bonds demonstrates the model's effectiveness, supported by
regularization techniques to mitigate overfitting. The study concludes with
potential avenues for further development, including generalized time steps and
enhanced corrective terms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05338">STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers. (arXiv:2401.05338v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_D/0/1/0/all/0/1">Daqian Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fesser_L/0/1/0/all/0/1">Lukas Fesser</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1">Marta Kwiatkowska</a></p>
<p>Robustness certification, which aims to formally certify the predictions of
neural networks against adversarial inputs, has become an integral part of
important tool for safety-critical applications. Despite considerable progress,
existing certification methods are limited to elementary architectures, such as
convolutional networks, recurrent networks and recently Transformers, on
benchmark datasets such as MNIST. In this paper, we focus on the robustness
certification of scene text recognition (STR), which is a complex and
extensively deployed image-based sequence prediction problem. We tackle three
types of STR model architectures, including the standard STR pipelines and the
Vision Transformer. We propose STR-Cert, the first certification method for STR
models, by significantly extending the DeepPoly polyhedral verification
framework via deriving novel polyhedral bounds and algorithms for key STR model
components. Finally, we certify and compare STR models on six datasets,
demonstrating the efficiency and scalability of robustness certification,
particularly for the Vision Transformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05341">Stable Online and Offline Reinforcement Learning for Antibody CDRH3 Design. (arXiv:2401.05341v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Vogt_Y/0/1/0/all/0/1">Yannick Vogt</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Naouar_M/0/1/0/all/0/1">Mehdi Naouar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kalweit_M/0/1/0/all/0/1">Maria Kalweit</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Miething_C/0/1/0/all/0/1">Christoph Cornelius Miething</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Duyster_J/0/1/0/all/0/1">Justus Duyster</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mertelsmann_R/0/1/0/all/0/1">Roland Mertelsmann</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kalweit_G/0/1/0/all/0/1">Gabriel Kalweit</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Boedecker_J/0/1/0/all/0/1">Joschka Boedecker</a></p>
<p>The field of antibody-based therapeutics has grown significantly in recent
years, with targeted antibodies emerging as a potentially effective approach to
personalized therapies. Such therapies could be particularly beneficial for
complex, highly individual diseases such as cancer. However, progress in this
field is often constrained by the extensive search space of amino acid
sequences that form the foundation of antibody design. In this study, we
introduce a novel reinforcement learning method specifically tailored to
address the unique challenges of this domain. We demonstrate that our method
can learn the design of high-affinity antibodies against multiple targets in
silico, utilizing either online interaction or offline datasets. To the best of
our knowledge, our approach is the first of its kind and outperforms existing
methods on all tested antigens in the Absolut! database.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05342">Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Burg_M/0/1/0/all/0/1">Max F. Burg</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zenkel_T/0/1/0/all/0/1">Thomas Zenkel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Vystrcilova_M/0/1/0/all/0/1">Michaela Vystr&#x10d;ilov&#xe1;</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Oesterle_J/0/1/0/all/0/1">Jonathan Oesterle</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Hofling_L/0/1/0/all/0/1">Larissa H&#xf6;fling</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Willeke_K/0/1/0/all/0/1">Konstantin F. Willeke</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lause_J/0/1/0/all/0/1">Jan Lause</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Muller_S/0/1/0/all/0/1">Sarah M&#xfc;ller</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fahey_P/0/1/0/all/0/1">Paul G. Fahey</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ding_Z/0/1/0/all/0/1">Zhiwei Ding</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Restivo_K/0/1/0/all/0/1">Kelli Restivo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sridhar_S/0/1/0/all/0/1">Shashwat Sridhar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gollisch_T/0/1/0/all/0/1">Tim Gollisch</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Berens_P/0/1/0/all/0/1">Philipp Berens</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tolias_A/0/1/0/all/0/1">Andreas S. Tolias</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Euler_T/0/1/0/all/0/1">Thomas Euler</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ecker_A/0/1/0/all/0/1">Alexander S. Ecker</a></p>
<p>Identifying cell types and understanding their functional properties is
crucial for unraveling the mechanisms underlying perception and cognition. In
the retina, functional types can be identified by carefully selected stimuli,
but this requires expert domain knowledge and biases the procedure towards
previously known cell types. In the visual cortex, it is still unknown what
functional types exist and how to identify them. Thus, for unbiased
identification of the functional cell types in retina and visual cortex, new
approaches are needed. Here we propose an optimization-based clustering
approach using deep predictive models to obtain functional clusters of neurons
using Most Discriminative Stimuli (MDS). Our approach alternates between
stimulus optimization with cluster reassignment akin to an
expectation-maximization algorithm. The algorithm recovers functional clusters
in mouse retina, marmoset retina and macaque visual area V4. This demonstrates
that our approach can successfully find discriminative stimuli across species,
stages of the visual system and recording techniques. The resulting most
discriminative stimuli can be used to assign functional cell types fast and on
the fly, without the need to train complex predictive models or show a large
natural scene dataset, paving the way for experiments that were previously
limited by experimental time. Crucially, MDS are interpretable: they visualize
the distinctive stimulus patterns that most unambiguously identify a specific
type of neuron. We will make our code available online upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05350">Adaptive operator selection utilising generalised experience. (arXiv:2401.05350v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aydin_M/0/1/0/all/0/1">Mehmet Emin Aydin</a>, <a href="http://arxiv.org/find/cs/1/au:+Durgut_R/0/1/0/all/0/1">Rafet Durgut</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakib_A/0/1/0/all/0/1">Abdur Rakib</a></p>
<p>Optimisation problems, particularly combinatorial optimisation problems, are
difficult to solve due to their complexity and hardness. Such problems have
been successfully solved by evolutionary and swarm intelligence algorithms,
especially in binary format. However, the approximation may suffer due to the
the issues in balance between exploration and exploitation activities (EvE),
which remain as the major challenge in this context. Although the complementary
usage of multiple operators is becoming more popular for managing EvE with
adaptive operator selection schemes, a bespoke adaptive selection system is
still an important topic in research. Reinforcement Learning (RL) has recently
been proposed as a way to customise and shape up a highly effective adaptive
selection system. However, it is still challenging to handle the problem in
terms of scalability. This paper proposes and assesses a RL-based novel
approach to help develop a generalised framework for gaining, processing, and
utilising the experiences for both the immediate and future use. The
experimental results support the proposed approach with a certain level of
success.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05351">Rethinking Performance Measures of RNA Secondary Structure Problems. (arXiv:2401.05351v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Runge_F/0/1/0/all/0/1">Frederic Runge</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Franke_J/0/1/0/all/0/1">J&#xf6;rg K. H. Franke</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fertmann_D/0/1/0/all/0/1">Daniel Fertmann</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a></p>
<p>Accurate RNA secondary structure prediction is vital for understanding
cellular regulation and disease mechanisms. Deep learning (DL) methods have
surpassed traditional algorithms by predicting complex features like
pseudoknots and multi-interacting base pairs. However, traditional distance
measures can hardly deal with such tertiary interactions and the currently used
evaluation measures (F1 score, MCC) have limitations. We propose the
Weisfeiler-Lehman graph kernel (WL) as an alternative metric. Embracing
graph-based metrics like WL enables fair and accurate evaluation of RNA
structure prediction algorithms. Further, WL provides informative guidance, as
demonstrated in an RNA design experiment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05352">Generalized Categories Discovery for Long-tailed Recognition. (arXiv:2401.05352v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1">Christoph Meinel</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haojin Yang</a></p>
<p>Generalized Class Discovery (GCD) plays a pivotal role in discerning both
known and unknown categories from unlabeled datasets by harnessing the insights
derived from a labeled set comprising recognized classes. A significant
limitation in prevailing GCD methods is their presumption of an equitably
distributed category occurrence in unlabeled data. Contrary to this assumption,
visual classes in natural environments typically exhibit a long-tailed
distribution, with known or prevalent categories surfacing more frequently than
their rarer counterparts. Our research endeavors to bridge this disconnect by
focusing on the long-tailed Generalized Category Discovery (Long-tailed GCD)
paradigm, which echoes the innate imbalances of real-world unlabeled datasets.
In response to the unique challenges posed by Long-tailed GCD, we present a
robust methodology anchored in two strategic regularizations: (i) a reweighting
mechanism that bolsters the prominence of less-represented, tail-end
categories, and (ii) a class prior constraint that aligns with the anticipated
class distribution. Comprehensive experiments reveal that our proposed method
surpasses previous state-of-the-art GCD methods by achieving an improvement of
approximately 6 - 9% on ImageNet100 and competitive performance on CIFAR100.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05353">ImbaGCD: Imbalanced Generalized Category Discovery. (arXiv:2401.05353v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziyun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1">Ben Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Simsek_F/0/1/0/all/0/1">Furkan Simsek</a>, <a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1">Christoph Meinel</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haojin Yang</a></p>
<p>Generalized class discovery (GCD) aims to infer known and unknown categories
in an unlabeled dataset leveraging prior knowledge of a labeled set comprising
known classes. Existing research implicitly/explicitly assumes that the
frequency of occurrence for each category, whether known or unknown, is
approximately the same in the unlabeled data. However, in nature, we are more
likely to encounter known/common classes than unknown/uncommon ones, according
to the long-tailed property of visual classes. Therefore, we present a
challenging and practical problem, Imbalanced Generalized Category Discovery
(ImbaGCD), where the distribution of unlabeled data is imbalanced, with known
classes being more frequent than unknown ones. To address these issues, we
propose ImbaGCD, A novel optimal transport-based expectation maximization
framework that accomplishes generalized category discovery by aligning the
marginal class prior distribution. ImbaGCD also incorporates a systematic
mechanism for estimating the imbalanced class prior distribution under the GCD
setup. Our comprehensive experiments reveal that ImbaGCD surpasses previous
state-of-the-art GCD methods by achieving an improvement of approximately 2 -
4% on CIFAR-100 and 15 - 19% on ImageNet-100, indicating its superior
effectiveness in solving the Imbalanced GCD problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05355">Developing a Resource-Constraint EdgeAI model for Surface Defect Detection. (arXiv:2401.05355v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mih_A/0/1/0/all/0/1">Atah Nuh Mih</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Hung Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawnine_A/0/1/0/all/0/1">Asfia Kawnine</a>, <a href="http://arxiv.org/find/cs/1/au:+Wachowicz_M/0/1/0/all/0/1">Monica Wachowicz</a></p>
<p>Resource constraints have restricted several EdgeAI applications to machine
learning inference approaches, where models are trained on the cloud and
deployed to the edge device. This poses challenges such as bandwidth, latency,
and privacy associated with storing data off-site for model building. Training
on the edge device can overcome these challenges by eliminating the need to
transfer data to another device for storage and model development. On-device
training also provides robustness to data variations as models can be retrained
on newly acquired data to improve performance. We, therefore, propose a
lightweight EdgeAI architecture modified from Xception, for on-device training
in a resource-constraint edge environment. We evaluate our model on a PCB
defect detection task and compare its performance against existing lightweight
models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our
experiment show that our model has a remarkable performance with a test
accuracy of 73.45% without pre-training. This is comparable to the test
accuracy of non-pre-trained MobileViT-XXS (75.40%) and much better than other
non-pre-trained models (MobileNetV2 - 50.05%, EfficientNetV2B0 - 54.30%). The
test accuracy of our model without pre-training is comparable to pre-trained
MobileNetV2 model - 75.45% and better than pre-trained EfficientNetV2B0 model -
58.10%. In terms of memory efficiency, our model performs better than
EfficientNetV2B0 and MobileViT-XXS. We find that the resource efficiency of
machine learning models does not solely depend on the number of parameters but
also depends on architectural considerations. Our method can be applied to
other resource-constraint applications while maintaining significant
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05357">U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators. (arXiv:2401.05357v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zheyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaobo Sharon Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yiyu Shi</a></p>
<p>Architectures that incorporate Computing-in-Memory (CiM) using emerging
non-volatile memory (NVM) devices have become strong contenders for deep neural
network (DNN) acceleration due to their impressive energy efficiency. Yet, a
significant challenge arises when using these emerging devices: they can show
substantial variations during the weight-mapping process. This can severely
impact DNN accuracy if not mitigated. A widely accepted remedy for imperfect
weight mapping is the iterative write-verify approach, which involves verifying
conductance values and adjusting devices if needed. In all existing
publications, this procedure is applied to every individual device, resulting
in a significant programming time overhead. In our research, we illustrate that
only a small fraction of weights need this write-verify treatment for the
corresponding devices and the DNN accuracy can be preserved, yielding a notable
programming acceleration. Building on this, we introduce USWIM, a novel method
based on the second derivative. It leverages a single iteration of forward and
backpropagation to pinpoint the weights demanding write-verify. Through
extensive tests on diverse DNN designs and datasets, USWIM manifests up to a
10x programming acceleration against the traditional exhaustive write-verify
method, all while maintaining a similar accuracy level. Furthermore, compared
to our earlier SWIM technique, USWIM excels, showing a 7x speedup when dealing
with devices exhibiting non-uniform variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05363">Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jiquan Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1">Sha Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1">Haiteng Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Shijian Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Tao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_G/0/1/0/all/0/1">Gang Pan</a></p>
<p>Automatic sleep staging is essential for sleep assessment and disorder
diagnosis. Most existing methods depend on one specific dataset and are limited
to be generalized to other unseen datasets, for which the training data and
testing data are from the same dataset. In this paper, we introduce domain
generalization into automatic sleep staging and propose the task of
generalizable sleep staging which aims to improve the model generalization
ability to unseen datasets. Inspired by existing domain generalization methods,
we adopt the feature alignment idea and propose a framework called SleepDG to
solve it. Considering both of local salient features and sequential features
are important for sleep staging, we propose a Multi-level Feature Alignment
combining epoch-level and sequence-level feature alignment to learn
domain-invariant feature representations. Specifically, we design an
Epoch-level Feature Alignment to align the feature distribution of each single
sleep epoch among different domains, and a Sequence-level Feature Alignment to
minimize the discrepancy of sequential features among different domains.
SleepDG is validated on five public datasets, achieving the state-of-the-art
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05365">Online Action Recognition for Human Risk Prediction with Anticipated Haptic Alert via Wearables. (arXiv:2401.05365v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guo_C/0/1/0/all/0/1">Cheng Guo</a> (1 and 2), <a href="http://arxiv.org/find/eess/1/au:+Rapetti_L/0/1/0/all/0/1">Lorenzo Rapetti</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Darvish_K/0/1/0/all/0/1">Kourosh Darvish</a> (3), <a href="http://arxiv.org/find/eess/1/au:+Grieco_R/0/1/0/all/0/1">Riccardo Grieco</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Draicchio_F/0/1/0/all/0/1">Francesco Draicchio</a> (4), <a href="http://arxiv.org/find/eess/1/au:+Pucci_D/0/1/0/all/0/1">Daniele Pucci</a> (1 and 2) ((1) Istituto Italiano di Tecnologia, (2) University of Manchester, (3) University of Toronto, (4) INAIL)</p>
<p>This paper proposes a framework that combines online human state estimation,
action recognition and motion prediction to enable early assessment and
prevention of worker biomechanical risk during lifting tasks. The framework
leverages the NIOSH index to perform online risk assessment, thus fitting
real-time applications. In particular, the human state is retrieved via inverse
kinematics/dynamics algorithms from wearable sensor data. Human action
recognition and motion prediction are achieved by implementing an LSTM-based
Guided Mixture of Experts architecture, which is trained offline and inferred
online. With the recognized actions, a single lifting activity is divided into
a series of continuous movements and the Revised NIOSH Lifting Equation can be
applied for risk assessment. Moreover, the predicted motions enable
anticipation of future risks. A haptic actuator, embedded in the wearable
system, can alert the subject of potential risk, acting as an active prevention
device. The performance of the proposed framework is validated by executing
real lifting tasks, while the subject is equipped with the iFeel wearable
system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05367">Context-Aware Stress Monitoring using Wearable and Mobile Technologies in Everyday Settings. (arXiv:2401.05367v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Aqajari_S/0/1/0/all/0/1">Seyed Amir Hossein Aqajari</a>, <a href="http://arxiv.org/find/eess/1/au:+Labbaf_S/0/1/0/all/0/1">Sina Labbaf</a>, <a href="http://arxiv.org/find/eess/1/au:+Tran_P/0/1/0/all/0/1">Phuc Hoang Tran</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_B/0/1/0/all/0/1">Brenda Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Mehrabadi_M/0/1/0/all/0/1">Milad Asgari Mehrabadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Levorato_M/0/1/0/all/0/1">Marco Levorato</a>, <a href="http://arxiv.org/find/eess/1/au:+Dutt_N/0/1/0/all/0/1">Nikil Dutt</a>, <a href="http://arxiv.org/find/eess/1/au:+Rahmani_A/0/1/0/all/0/1">Amir M. Rahmani</a></p>
<p>Daily monitoring of stress is a critical component of maintaining optimal
physical and mental health. Physiological signals and contextual information
have recently emerged as promising indicators for detecting instances of
heightened stress. Nonetheless, developing a real-time monitoring system that
utilizes both physiological and contextual data to anticipate stress levels in
everyday settings while also gathering stress labels from participants
represents a significant challenge. We present a monitoring system that
objectively tracks daily stress levels by utilizing both physiological and
contextual data in a daily-life environment. Additionally, we have integrated a
smart labeling approach to optimize the ecological momentary assessment (EMA)
collection, which is required for building machine learning models for stress
detection. We propose a three-tier Internet-of-Things-based system architecture
to address the challenges. We utilized a cross-validation technique to
accurately estimate the performance of our stress models. We achieved the
F1-score of 70\% with a Random Forest classifier using both PPG and contextual
data, which is considered an acceptable score in models built for everyday
settings. Whereas using PPG data alone, the highest F1-score achieved is
approximately 56\%, emphasizing the significance of incorporating both PPG and
contextual data in stress detection tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05369">Symbolic Regression of Dynamic Network Models. (arXiv:2401.05369v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gandhi_G/0/1/0/all/0/1">Govind Gandhi</a></p>
<p>Growing interest in modelling complex systems from brains to societies to
cities using networks has led to increased efforts to describe generative
processes that explain those networks. Recent successes in machine learning
have prompted the usage of evolutionary computation, especially genetic
programming to evolve computer programs that effectively forage a
multidimensional search space to iteratively find better solutions that explain
network structure. Symbolic regression contributes to these approaches by
replicating network morphologies using both structure and processes, all while
not relying on the scientists intuition or expertise. It distinguishes itself
by introducing a novel formulation of a network generator and a parameter-free
fitness function to evaluate the generated network and is found to consistently
retrieve synthetically generated growth processes as well as simple,
interpretable rules for a range of empirical networks. We extend this approach
by modifying generator semantics to create and retrieve rules for time-varying
networks. Lexicon to study networks created dynamically in multiple stages is
introduced. The framework was improved using methods from the genetic
programming toolkit (recombination) and computational improvements (using
heuristic distance measures) and used to test the consistency and robustness of
the upgrades to the semantics using synthetically generated networks. Using
recombination was found to improve retrieval rate and fitness of the solutions.
The framework was then used on three empirical datasets - subway networks of
major cities, regions of street networks and semantic co-occurrence networks of
literature in Artificial Intelligence to illustrate the possibility of
obtaining interpretable, decentralised growth processes from complex networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05370">Autoregressive fragment-based diffusion for pocket-aware ligand design. (arXiv:2401.05370v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Ghorbani_M/0/1/0/all/0/1">Mahdi Ghorbani</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gendelev_L/0/1/0/all/0/1">Leo Gendelev</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Beroza_P/0/1/0/all/0/1">Paul Beroza</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Keiser_M/0/1/0/all/0/1">Michael J. Keiser</a></p>
<p>In this work, we introduce AutoFragDiff, a fragment-based autoregressive
diffusion model for generating 3D molecular structures conditioned on target
protein structures. We employ geometric vector perceptrons to predict atom
types and spatial coordinates of new molecular fragments conditioned on
molecular scaffolds and protein pockets. Our approach improves the local
geometry of the resulting 3D molecules while maintaining high predicted binding
affinity to protein targets. The model can also perform scaffold extension from
user-provided starting molecular scaffold.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05373">Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_N/0/1/0/all/0/1">Nan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mengzhu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenghan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Masi_G/0/1/0/all/0/1">Giulia De Masi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1">Bin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Huan Xiong</a></p>
<p>The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks
(GNNs) is gradually attracting attention due to the low power consumption and
high efficiency in processing the non-Euclidean data represented by graphs.
However, as a common problem, dynamic graph representation learning faces
challenges such as high complexity and large memory overheads. Current work
often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary
features instead of continuous ones for efficient training, which would
overlooks graph structure information and leads to the loss of details during
propagation. Additionally, optimizing dynamic spiking models typically requires
propagation of information across time steps, which increases memory
requirements. To address these challenges, we present a framework named
\underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph
\underline{N}eural Networks (\method{}). To mitigate the information loss
problem, \method{} propagates early-layer information directly to the last
layer for information compensation. To accommodate the memory requirements, we
apply the implicit differentiation on the equilibrium state, which does not
rely on the exact reverse of the forward computation. While traditional
implicit differentiation methods are usually used for static situations,
\method{} extends it to the dynamic graph setting. Extensive experiments on
three large-scale real-world dynamic graph datasets validate the effectiveness
of \method{} on dynamic node classification tasks with lower computational
costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05378">Detecting QT prolongation From a Single-lead ECG With Deep Learning. (arXiv:2401.05378v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Alam_R/0/1/0/all/0/1">Ridwan Alam</a>, <a href="http://arxiv.org/find/eess/1/au:+Aguirre_A/0/1/0/all/0/1">Aaron Aguirre</a>, <a href="http://arxiv.org/find/eess/1/au:+Stultz_C/0/1/0/all/0/1">Collin Stultz</a></p>
<p>For a number of antiarrhythmics, drug loading requires a 3 day
hospitalization with monitoring for QT prolongation. Automated QT monitoring
with wearable ECG monitors would facilitate out-of-hospital care. We develop a
deep learning model that infers QT intervals from ECG lead-I - the lead most
often acquired from ambulatory ECG monitors - and to use this model to detect
clinically meaningful QT-prolongation episodes during Dofetilide drug loading.
Using 4.22 million 12-lead ECG recordings from 903.6 thousand patients at the
Massachusetts General Hospital, we develop a deep learning model, QTNet, that
infers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patients
are used to train the model and an internal-test set containing 633 thousand
ECGs from 135 thousand patients was used for testing. QTNet is further
evaluated on an external-validation set containing 3.1 million ECGs from 667
thousand patients at another institution. QTNet was used to detect
Dofetilide-induced QT prolongation in a publicly available database
(ECGRDVQ-dataset) containing ECGs from subjects enrolled in a clinical trial
evaluating the effects of antiarrhythmic drugs. QTNet achieves mean absolute
errors of 12.63ms (internal-test) and 12.30ms (external-validation) for
estimating absolute QT intervals. The associated Pearson correlation
coefficients are 0.91 (internal-test) and 0.92 (external-validation). For the
ECGRDVQ-dataset, QTNet detects Dofetilide-induced QTc prolongation with 87%
sensitivity and 77% specificity. The negative predictive value of the model is
greater than 95% when the pre-test probability of drug-induced QTc prolongation
is below 25%. Drug-induced QT prolongation risk can be tracked from ECG lead-I
using deep learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05380">Dataset Optimization for Chronic Disease Prediction with Bio-Inspired Feature Selection. (arXiv:2401.05380v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dyoub_A/0/1/0/all/0/1">Abeer Dyoub</a>, <a href="http://arxiv.org/find/cs/1/au:+Letteri_I/0/1/0/all/0/1">Ivan Letteri</a></p>
<p>In this study, we investigated the application of bio-inspired optimization
algorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale
Optimization Algorithm, for feature selection in chronic disease prediction.
The primary goal was to enhance the predictive accuracy of models streamline
data dimensionality, and make predictions more interpretable and actionable.
</p>
<p>The research encompassed a comparative analysis of the three bio-inspired
feature selection approaches across diverse chronic diseases, including
diabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such
as accuracy, precision, recall, and f1 score are used to assess the
effectiveness of the algorithms in reducing the number of features needed for
accurate classification.
</p>
<p>The results in general demonstrate that the bio-inspired optimization
algorithms are effective in reducing the number of features required for
accurate classification. However, there have been variations in the performance
of the algorithms on different datasets.
</p>
<p>The study highlights the importance of data pre-processing and cleaning in
ensuring the reliability and effectiveness of the analysis.
</p>
<p>This study contributes to the advancement of predictive analytics in the
realm of chronic diseases. The potential impact of this work extends to early
intervention, precision medicine, and improved patient outcomes, providing new
avenues for the delivery of healthcare services tailored to individual needs.
The findings underscore the potential benefits of using bio-inspired
optimization algorithms for feature selection in chronic disease prediction,
offering valuable insights for improving healthcare outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05381">ADF &amp; TransApp: A Transformer-Based Framework for Appliance Detection Using Smart Meter Consumption Series. (arXiv:2401.05381v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Petralia_A/0/1/0/all/0/1">Adrien Petralia</a>, <a href="http://arxiv.org/find/eess/1/au:+Charpentier_P/0/1/0/all/0/1">Philippe Charpentier</a>, <a href="http://arxiv.org/find/eess/1/au:+Palpanas_T/0/1/0/all/0/1">Themis Palpanas</a></p>
<p>Over the past decade, millions of smart meters have been installed by
electricity suppliers worldwide, allowing them to collect a large amount of
electricity consumption data, albeit sampled at a low frequency (one point
every 30min). One of the important challenges these suppliers face is how to
utilize these data to detect the presence/absence of different appliances in
the customers' households. This valuable information can help them provide
personalized offers and recommendations to help customers towards the energy
transition. Appliance detection can be cast as a time series classification
problem. However, the large amount of data combined with the long and variable
length of the consumption series pose challenges when training a classifier. In
this paper, we propose ADF, a framework that uses subsequences of a client
consumption series to detect the presence/absence of appliances. We also
introduce TransApp, a Transformer-based time series classifier that is first
pretrained in a self-supervised way to enhance its performance on appliance
detection tasks. We test our approach on two real datasets, including a
publicly available one. The experimental results with two large real datasets
show that the proposed approach outperforms current solutions, including
state-of-the-art time series classifiers applied to appliance detection. This
paper appeared in VLDB 2024.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05382">An improved genetic programming for predicting semi autogenous grinding mill throughput. (arXiv:2401.05382v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghasemi_Z/0/1/0/all/0/1">Zahra Ghasemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1">Frank Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanin_M/0/1/0/all/0/1">Max Zanin</a>, <a href="http://arxiv.org/find/cs/1/au:+Karageorgos_J/0/1/0/all/0/1">John Karageorgos</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a></p>
<p>Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding
circuit of mineral processing plants. Accurate prediction of SAG mill
throughput as a crucial performance metric is of utmost importance. While
empirical models have been developed in previous studies for SAG mill
throughput prediction, the potential of applying machine learning (ML)
techniques for this purpose remains underexplored. Unlike empirical modelling,
which relies on expensive and time-consuming experimental data, ML techniques
can utilize data collected during regular operations. Genetic programming (GP)
is one of ML techniques that offers the advantage of providing a transparent
equation for precise mill throughput prediction. This study explores the
application of GP to predict SAG mill throughput and introduces five new GP
variants to enhance prediction performance. These variants extract multiple
equations, each accurately predicting mill throughput for specific clusters of
training data. These equations are then employed to predict mill throughput for
test data using various approaches. To assess the effect of distance measures
on the new GP variants, four different distance measures are employed.
Comparative analysis reveals that the new GP variants achieve an average
improvement of 12.49% in prediction accuracy. Further investigation of distance
measures indicates that the Euclidean distance measure yields the most accurate
results for the majority of data splits. Additionally, the most precise new GP
variant considers all equations and incorporates both the number of data points
in each data cluster and the distance to clusters when calculating the final
prediction. The developed GP variants in this study present a precise,
transparent, and cost-effective approach for modelling SAG mill throughput in
mineral processing plants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05385">Angle-Equivariant Convolutional Neural Networks for Interference Mitigation in Automotive Radar. (arXiv:2401.05385v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Oswald_C/0/1/0/all/0/1">Christian Oswald</a>, <a href="http://arxiv.org/find/eess/1/au:+Toth_M/0/1/0/all/0/1">Mate Toth</a>, <a href="http://arxiv.org/find/eess/1/au:+Meissner_P/0/1/0/all/0/1">Paul Meissner</a>, <a href="http://arxiv.org/find/eess/1/au:+Pernkopf_F/0/1/0/all/0/1">Franz Pernkopf</a></p>
<p>In automotive applications, frequency modulated continuous wave (FMCW) radar
is an established technology to determine the distance, velocity and angle of
objects in the vicinity of the vehicle. The quality of predictions might be
seriously impaired if mutual interference between radar sensors occurs.
Previous work processes data from the entire receiver array in parallel to
increase interference mitigation quality using neural networks (NNs). However,
these architectures do not generalize well across different angles of arrival
(AoAs) of interferences and objects. In this paper we introduce fully
convolutional neural network (CNN) with rank-three convolutions which is able
to transfer learned patterns between different AoAs. Our proposed architecture
outperforms previous work while having higher robustness and a lower number of
trainable parameters. We evaluate our network on a diverse data set and
demonstrate its angle equivariance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05386">EMG subspace alignment and visualization for cross-subject hand gesture classification. (arXiv:2401.05386v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Colot_M/0/1/0/all/0/1">Martin Colot</a>, <a href="http://arxiv.org/find/eess/1/au:+Simar_C/0/1/0/all/0/1">C&#xe9;dric Simar</a>, <a href="http://arxiv.org/find/eess/1/au:+Petieau_M/0/1/0/all/0/1">Mathieu Petieau</a>, <a href="http://arxiv.org/find/eess/1/au:+Alvarez_A/0/1/0/all/0/1">Ana Maria Cebolla Alvarez</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheron_G/0/1/0/all/0/1">Guy Cheron</a>, <a href="http://arxiv.org/find/eess/1/au:+Bontempi_G/0/1/0/all/0/1">Gianluca Bontempi</a></p>
<p>Electromyograms (EMG)-based hand gesture recognition systems are a promising
technology for human/machine interfaces. However, one of their main limitations
is the long calibration time that is typically required to handle new users.
The paper discusses and analyses the challenge of cross-subject generalization
thanks to an original dataset containing the EMG signals of 14 human subjects
during hand gestures. The experimental results show that, though an accurate
generalization based on pooling multiple subjects is hardly achievable, it is
possible to improve the cross-subject estimation by identifying a robust
low-dimensional subspace for multiple subjects and aligning it to a target
subject. A visualization of the subspace enables us to provide insights for the
improvement of cross-subject generalization with EMG signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05388">Bayesian ECG reconstruction using denoising diffusion generative models. (arXiv:2401.05388v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cardoso_G/0/1/0/all/0/1">Gabriel V. Cardoso</a>, <a href="http://arxiv.org/find/eess/1/au:+Bedin_L/0/1/0/all/0/1">Lisa Bedin</a>, <a href="http://arxiv.org/find/eess/1/au:+Duchateau_J/0/1/0/all/0/1">Josselin Duchateau</a>, <a href="http://arxiv.org/find/eess/1/au:+Dubois_R/0/1/0/all/0/1">R&#xe9;mi Dubois</a>, <a href="http://arxiv.org/find/eess/1/au:+Moulines_E/0/1/0/all/0/1">Eric Moulines</a></p>
<p>In this work, we propose a denoising diffusion generative model (DDGM)
trained with healthy electrocardiogram (ECG) data that focuses on ECG
morphology and inter-lead dependence. Our results show that this innovative
generative model can successfully generate realistic ECG signals. Furthermore,
we explore the application of recent breakthroughs in solving linear inverse
Bayesian problems using DDGM. This approach enables the development of several
important clinical tools. These include the calculation of corrected QT
intervals (QTc), effective noise suppression of ECG signals, recovery of
missing ECG leads, and identification of anomalous readings, enabling
significant advances in cardiac health monitoring and diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05394">Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Vazelhes_W/0/1/0/all/0/1">William de Vazelhes</a>, <a href="http://arxiv.org/find/eess/1/au:+Mukhoty_B/0/1/0/all/0/1">Bhaskar Mukhoty</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1">Xiao-Tong Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu_B/0/1/0/all/0/1">Bin Gu</a></p>
<p>Sparse recovery is ubiquitous in machine learning and signal processing. Due
to the NP-hard nature of sparse recovery, existing methods are known to suffer
either from restrictive (or even unknown) applicability conditions, or high
computational cost. Recently, iterative regularization methods have emerged as
a promising fast approach because they can achieve sparse recovery in one pass
through early stopping, rather than the tedious grid-search used in the
traditional methods. However, most of those iterative methods are based on the
$\ell_1$ norm which requires restrictive applicability conditions and could
fail in many cases. Therefore, achieving sparse recovery with iterative
regularization methods under a wider range of conditions has yet to be further
explored. To address this issue, we propose a novel iterative regularization
algorithm, IRKSN, based on the $k$-support norm regularizer rather than the
$\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and
compare them with traditional conditions for recovery with $\ell_1$ norm
regularizers. Additionally, we give an early stopping bound on the model error
of IRKSN with explicit constants, achieving the standard linear rate for sparse
recovery. Finally, we illustrate the applicability of our algorithm on several
experiments, including a support recovery experiment with a correlated design
matrix.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05395">SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market. (arXiv:2401.05395v1 [econ.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Ding_R/0/1/0/all/0/1">Ruixin Ding</a>, <a href="http://arxiv.org/find/econ/1/au:+Chen_B/0/1/0/all/0/1">Bowei Chen</a>, <a href="http://arxiv.org/find/econ/1/au:+Wilson_J/0/1/0/all/0/1">James M. Wilson</a>, <a href="http://arxiv.org/find/econ/1/au:+Yan_Z/0/1/0/all/0/1">Zhi Yan</a>, <a href="http://arxiv.org/find/econ/1/au:+Huang_Y/0/1/0/all/0/1">Yufei Huang</a></p>
<p>The automotive industry plays a critical role in the global economy, and
particularly important is the expanding Chinese automobile market due to its
immense scale and influence. However, existing automotive sector datasets are
limited in their coverage, failing to adequately consider the growing demand
for more and diverse variables. This paper aims to bridge this data gap by
introducing a comprehensive dataset spanning the years from 2016 to 2022,
encompassing sales data, online reviews, and a wealth of information related to
the Chinese automotive industry. This dataset serves as a valuable resource,
significantly expanding the available data. Its impact extends to various
dimensions, including improving forecasting accuracy, expanding the scope of
business applications, informing policy development and regulation, and
advancing academic research within the automotive sector. To illustrate the
dataset's potential applications in both business and academic contexts, we
present two application examples. Our developed dataset enhances our
understanding of the Chinese automotive market and offers a valuable tool for
researchers, policymakers, and industry stakeholders worldwide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05397">Hyperspectral Lightcurve Inversion for Attitude Determination. (arXiv:2401.05397v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Marto_S/0/1/0/all/0/1">Sim&#xe3;o da Gra&#xe7;a Marto</a>, <a href="http://arxiv.org/find/eess/1/au:+Vasile_M/0/1/0/all/0/1">Massimiliano Vasile</a>, <a href="http://arxiv.org/find/eess/1/au:+Campbell_A/0/1/0/all/0/1">Andrew Campbell</a>, <a href="http://arxiv.org/find/eess/1/au:+Murray_P/0/1/0/all/0/1">Paul Murray</a>, <a href="http://arxiv.org/find/eess/1/au:+Marshall_S/0/1/0/all/0/1">Stephen Marshall</a>, <a href="http://arxiv.org/find/eess/1/au:+Savitski_V/0/1/0/all/0/1">Vasili Savitski</a></p>
<p>Spectral lightcurves consisting of time series single-pixel spectral
measurements of spacecraft are used to infer the spacecraft's attitude and
rotation. Two methods are used. One based on numerical optimisation of a
regularised least squares cost function, and another based on machine learning
with a neural network model. The aim is to work with minimal information, thus
no prior is available on the attitude nor on the inertia tensor. The
theoretical and practical aspects of this task are investigated, and the
methodology is tested on synthetic data. Results are shown based on synthetic
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05402">Vector Field Oriented Diffusion Model for Crystal Material Generation. (arXiv:2401.05402v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Klipfel_A/0/1/0/all/0/1">Astrid Klipfel</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fregier_Y/0/1/0/all/0/1">Ya&#xeb;l Fregier</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sayede_A/0/1/0/all/0/1">Adlane Sayede</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Bouraoui_Z/0/1/0/all/0/1">Zied Bouraoui</a></p>
<p>Discovering crystal structures with specific chemical properties has become
an increasingly important focus in material science. However, current models
are limited in their ability to generate new crystal lattices, as they only
consider atomic positions or chemical composition. To address this issue, we
propose a probabilistic diffusion model that utilizes a geometrically
equivariant GNN to consider atomic positions and crystal lattices jointly. To
evaluate the effectiveness of our model, we introduce a new generation metric
inspired by Frechet Inception Distance, but based on GNN energy prediction
rather than InceptionV3 used in computer vision. In addition to commonly used
metrics like validity, which assesses the plausibility of a structure, this new
metric offers a more comprehensive evaluation of our model's capabilities. Our
experiments on existing benchmarks show the significance of our diffusion
model. We also show that our method can effectively learn meaningful
representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05405">SelfEEG: A Python library for Self-Supervised Learning in Electroencephalography. (arXiv:2401.05405v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Pup_F/0/1/0/all/0/1">Federico Del Pup</a>, <a href="http://arxiv.org/find/eess/1/au:+Zanola_A/0/1/0/all/0/1">Andrea Zanola</a>, <a href="http://arxiv.org/find/eess/1/au:+Tshimanga_L/0/1/0/all/0/1">Louis Fabrice Tshimanga</a>, <a href="http://arxiv.org/find/eess/1/au:+Mazzon_P/0/1/0/all/0/1">Paolo Emilio Mazzon</a>, <a href="http://arxiv.org/find/eess/1/au:+Atzori_M/0/1/0/all/0/1">Manfredo Atzori</a></p>
<p>SelfEEG is an open-source Python library developed to assist researchers in
conducting Self-Supervised Learning (SSL) experiments on electroencephalography
(EEG) data. Its primary objective is to offer a user-friendly but highly
customizable environment, enabling users to efficiently design and execute
self-supervised learning tasks on EEG data.
</p>
<p>SelfEEG covers all the stages of a typical SSL pipeline, ranging from data
import to model design and training. It includes modules specifically designed
to: split data at various granularity levels (e.g., session-, subject-, or
dataset-based splits); effectively manage data stored with different
configurations (e.g., file extensions, data types) during mini-batch
construction; provide a wide range of standard deep learning models, data
augmentations and SSL baseline methods applied to EEG data.
</p>
<p>Most of the functionalities offered by selfEEG can be executed both on GPUs
and CPUs, expanding its usability beyond the self-supervised learning area.
Additionally, these functionalities can be employed for the analysis of other
biomedical signals often coupled with EEGs, such as electromyography or
electrocardiography data.
</p>
<p>These features make selfEEG a versatile deep learning tool for biomedical
applications and a useful resource in SSL, one of the currently most active
fields of Artificial Intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05406">RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications. (arXiv:2401.05406v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Rosen_D/0/1/0/all/0/1">Daniel Rosen</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Rochez_I/0/1/0/all/0/1">Illa Rochez</a> (1), <a href="http://arxiv.org/find/eess/1/au:+McIrvin_C/0/1/0/all/0/1">Caleb McIrvin</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Joshua Lee</a> (1), <a href="http://arxiv.org/find/eess/1/au:+DAlessandro_K/0/1/0/all/0/1">Kevin D&#x27;Alessandro</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Wiecek_M/0/1/0/all/0/1">Max Wiecek</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Hoang_N/0/1/0/all/0/1">Nhan Hoang</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Saffarini_R/0/1/0/all/0/1">Ramzy Saffarini</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Philips_S/0/1/0/all/0/1">Sam Philips</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Jones_V/0/1/0/all/0/1">Vanessa Jones</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Ivey_W/0/1/0/all/0/1">Will Ivey</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Harris_Smart_Z/0/1/0/all/0/1">Zavier Harris-Smart</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Harris_Smart_Z/0/1/0/all/0/1">Zavion Harris-Smart</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Chin_Z/0/1/0/all/0/1">Zayden Chin</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Johnson_A/0/1/0/all/0/1">Amos Johnson</a> (2), <a href="http://arxiv.org/find/eess/1/au:+Jones_A/0/1/0/all/0/1">Alyse M. Jones</a> (1), <a href="http://arxiv.org/find/eess/1/au:+Headley_W/0/1/0/all/0/1">William C. Headley</a> (1) ((1) Virginia Tech, (2) Morehouse College)</p>
<p>Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely
applicable technology in the next generation of wireless communication systems,
particularly 6G and next-gen military communications. Given this, our research
is focused on developing a tool to promote the development of RFRL techniques
that leverage spectrum sensing. In particular, the tool was designed to address
two cognitive radio applications, specifically dynamic spectrum access and
jamming. In order to train and test reinforcement learning (RL) algorithms for
these applications, a simulation environment is necessary to simulate the
conditions that an agent will encounter within the Radio Frequency (RF)
spectrum. In this paper, such an environment has been developed, herein
referred to as the RFRL Gym. Through the RFRL Gym, users can design their own
scenarios to model what an RL agent may encounter within the RF spectrum as
well as experiment with different spectrum sensing techniques. Additionally,
the RFRL Gym is a subclass of OpenAI gym, enabling the use of third-party ML/RL
Libraries. We plan to open-source this codebase to enable other researchers to
utilize the RFRL Gym to test their own scenarios and RL algorithms, ultimately
leading to the advancement of RL research in the wireless communications
domain. This paper describes in further detail the components of the Gym,
results from example scenarios, and plans for future additions.
</p>
<p>Index Terms-machine learning, reinforcement learning, wireless
communications, dynamic spectrum access, OpenAI gym
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05407">Machine Learning and Feature Ranking for Impact Fall Detection Event Using Multisensor Data. (arXiv:2401.05407v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Koffi_T/0/1/0/all/0/1">Tresor Y. Koffi</a>, <a href="http://arxiv.org/find/eess/1/au:+Mourchid_Y/0/1/0/all/0/1">Youssef Mourchid</a>, <a href="http://arxiv.org/find/eess/1/au:+Hindawi_M/0/1/0/all/0/1">Mohammed Hindawi</a>, <a href="http://arxiv.org/find/eess/1/au:+Dupuis_Y/0/1/0/all/0/1">Yohan Dupuis</a></p>
<p>Falls among individuals, especially the elderly population, can lead to
serious injuries and complications. Detecting impact moments within a fall
event is crucial for providing timely assistance and minimizing the negative
consequences. In this work, we aim to address this challenge by applying
thorough preprocessing techniques to the multisensor dataset, the goal is to
eliminate noise and improve data quality. Furthermore, we employ a feature
selection process to identify the most relevant features derived from the
multisensor UP-FALL dataset, which in turn will enhance the performance and
efficiency of machine learning models. We then evaluate the efficiency of
various machine learning models in detecting the impact moment using the
resulting data information from multiple sensors. Through extensive
experimentation, we assess the accuracy of our approach using various
evaluation metrics. Our results achieve high accuracy rates in impact
detection, showcasing the power of leveraging multisensor data for fall
detection tasks. This highlights the potential of our approach to enhance fall
detection systems and improve the overall safety and well-being of individuals
at risk of falls.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05408">Decoding Emotional Valence from Wearables: Can Our Data Reveal Our True Feelings?. (arXiv:2401.05408v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Grzeszczyk_M/0/1/0/all/0/1">Michal K. Grzeszczyk</a>, <a href="http://arxiv.org/find/eess/1/au:+Lisowska_A/0/1/0/all/0/1">Anna Lisowska</a>, <a href="http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1">Arkadiusz Sitek</a>, <a href="http://arxiv.org/find/eess/1/au:+Lisowska_A/0/1/0/all/0/1">Aneta Lisowska</a></p>
<p>Automatic detection and tracking of emotional states has the potential for
helping individuals with various mental health conditions. While previous
studies have captured physiological signals using wearable devices in
laboratory settings, providing valuable insights into the relationship between
physiological responses and mental states, the transfer of these findings to
real-life scenarios is still in its nascent stages. Our research aims to bridge
the gap between laboratory-based studies and real-life settings by leveraging
consumer-grade wearables and self-report measures. We conducted a preliminary
study involving 15 healthy participants to assess the efficacy of wearables in
capturing user valence in real-world settings. In this paper, we present the
initial analysis of the collected data, focusing primarily on the results of
valence classification. Our findings demonstrate promising results in
distinguishing between high and low positive valence, achieving an F1 score of
0.65. This research opens up avenues for future research in the field of mobile
mental health interventions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05409">Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection. (arXiv:2401.05409v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Maiwald_A/0/1/0/all/0/1">Aaron Maiwald</a>, <a href="http://arxiv.org/find/eess/1/au:+Ackermann_L/0/1/0/all/0/1">Leon Ackermann</a>, <a href="http://arxiv.org/find/eess/1/au:+Kalcher_M/0/1/0/all/0/1">Maximilian Kalcher</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_D/0/1/0/all/0/1">Daniel J. Wu</a></p>
<p>Alternative data representations are powerful tools that augment the
performance of downstream models. However, there is an abundance of such
representations within the machine learning toolbox, and the field lacks a
comparative understanding of the suitability of each representation method.
</p>
<p>In this paper, we propose artifact detection and classification within EEG
data as a testbed for profiling image-based data representations of time series
data. We then evaluate eleven popular deep learning architectures on each of
six commonly-used representation methods.
</p>
<p>We find that, while the choice of representation entails a choice within the
tradeoff between bias and variance, certain representations are practically
more effective in highlighting features which increase the signal-to-noise
ratio of the data. We present our results on EEG data, and open-source our
testing framework to enable future comparative analyses in this vein.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05410">Device-Free Human State Estimation using UWB Multi-Static Radios. (arXiv:2401.05410v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Laham_S/0/1/0/all/0/1">Saria Al Laham</a>, <a href="http://arxiv.org/find/eess/1/au:+Baghi_B/0/1/0/all/0/1">Bobak H. Baghi</a>, <a href="http://arxiv.org/find/eess/1/au:+Lajoie_P/0/1/0/all/0/1">Pierre-Yves Lajoie</a>, <a href="http://arxiv.org/find/eess/1/au:+Feriani_A/0/1/0/all/0/1">Amal Feriani</a>, <a href="http://arxiv.org/find/eess/1/au:+Herath_S/0/1/0/all/0/1">Sachini Herath</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Steve Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dudek_G/0/1/0/all/0/1">Gregory Dudek</a></p>
<p>We present a human state estimation framework that allows us to estimate the
location, and even the activities, of people in an indoor environment without
the requirement that they carry a specific devices with them. To achieve this
"device free" localization we use a small number of low-cost Ultra-Wide Band
(UWB) sensors distributed across the environment of interest. To achieve high
quality estimation from the UWB signals merely reflected of people in the
environment, we exploit a deep network that can learn to make inferences. The
hardware setup consists of commercial off-the-shelf (COTS) single antenna UWB
modules for sensing, paired with Raspberry PI units for computational
processing and data transfer. We make use of the channel impulse response (CIR)
measurements from the UWB sensors to estimate the human state - comprised of
location and activity - in a given area. Additionally, we can also estimate the
number of humans that occupy this region of interest. In our approach, first,
we pre-process the CIR data which involves meticulous aggregation of
measurements and extraction of key statistics. Afterwards, we leverage a
convolutional deep neural network to map the CIRs into precise location
estimates with sub-30 cm accuracy. Similarly, we achieve accurate human
activity recognition and occupancy counting results. We show that we can
quickly fine-tune our model for new out-of-distribution users, a process that
requires only a few minutes of data and a few epochs of training. Our results
show that UWB is a promising solution for adaptable smart-home localization and
activity recognition problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05411">RawECGNet: Deep Learning Generalization for Atrial Fibrillation Detection from the Raw ECG. (arXiv:2401.05411v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ben_Moshe_N/0/1/0/all/0/1">Noam Ben-Moshe</a>, <a href="http://arxiv.org/find/eess/1/au:+Tsutsui_K/0/1/0/all/0/1">Kenta Tsutsui</a>, <a href="http://arxiv.org/find/eess/1/au:+Biton_S/0/1/0/all/0/1">Shany Biton</a>, <a href="http://arxiv.org/find/eess/1/au:+Sornmo_L/0/1/0/all/0/1">Leif S&#xf6;rnmo</a>, <a href="http://arxiv.org/find/eess/1/au:+Behar_J/0/1/0/all/0/1">Joachim A. Behar</a></p>
<p>Introduction: Deep learning models for detecting episodes of atrial
fibrillation (AF) using rhythm information in long-term, ambulatory ECG
recordings have shown high performance. However, the rhythm-based approach does
not take advantage of the morphological information conveyed by the different
ECG waveforms, particularly the f-waves. As a result, the performance of such
models may be inherently limited. Methods: To address this limitation, we have
developed a deep learning model, named RawECGNet, to detect episodes of AF and
atrial flutter (AFl) using the raw, single-lead ECG. We compare the
generalization performance of RawECGNet on two external data sets that account
for distribution shifts in geography, ethnicity, and lead position. RawECGNet
is further benchmarked against a state-of-the-art deep learning model, named
ArNet2, which utilizes rhythm information as input. Results: Using RawECGNet,
the results for the different leads in the external test sets in terms of the
F1 score were 0.91--0.94 in RBDB and 0.93 in SHDB, compared to 0.89--0.91 in
RBDB and 0.91 in SHDB for ArNet2. The results highlight RawECGNet as a
high-performance, generalizable algorithm for detection of AF and AFl episodes,
exploiting information on both rhythm and morphology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05414">On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors. (arXiv:2401.05414v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Dong_X/0/1/0/all/0/1">Xinshuai Dong</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Dai_H/0/1/0/all/0/1">Haoyue Dai</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Fan_Y/0/1/0/all/0/1">Yewen Fan</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Jin_S/0/1/0/all/0/1">Songyao Jin</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Rajendran_S/0/1/0/all/0/1">Sathyamoorthy Rajendran</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a></p>
<p>Financial data is generally time series in essence and thus suffers from
three fundamental issues: the mismatch in time resolution, the time-varying
property of the distribution - nonstationarity, and causal factors that are
important but unknown/unobserved. In this paper, we follow a causal perspective
to systematically look into these three demons in finance. Specifically, we
reexamine these issues in the context of causality, which gives rise to a novel
and inspiring understanding of how the issues can be addressed. Following this
perspective, we provide systematic solutions to these problems, which hopefully
would serve as a foundation for future research in the area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05416">Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement. (arXiv:2401.05416v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yifeng Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1">Yi Zhao</a></p>
<p>As attitude and motion sensing components, inertial sensors are widely used
in various portable devices. But the severe errors of inertial sensors restrain
their function, especially the trajectory recovery and semantic recognition. As
a mainstream signal processing method, wavelet is hailed as the mathematical
microscope of signal due to the plentiful and diverse wavelet basis functions.
However, complicated noise types and application scenarios of inertial sensors
make selecting wavelet basis perplexing. To this end, we propose a wavelet
dynamic selection network (WDSNet), which intelligently selects the appropriate
wavelet basis for variable inertial signals. In addition, existing deep
learning architectures excel at extracting features from input data but neglect
to learn the characteristics of target categories, which is essential to
enhance the category awareness capability, thereby improving the selection of
wavelet basis. Therefore, we propose a category representation mechanism (CRM),
which enables the network to extract and represent category features without
increasing trainable parameters. Furthermore, CRM transforms the common fully
connected network into category representations, which provide closer
supervision to the feature extractor than the far and trivial one-hot
classification labels. We call this process of imposing interpretability on a
network and using it to supervise the feature extractor the feature supervision
mechanism, and its effectiveness is demonstrated experimentally and
theoretically in this paper. The enhanced inertial signal can perform
impracticable tasks with regard to the original signal, such as trajectory
reconstruction. Both quantitative and visual results show that WDSNet
outperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised
method, achieves the state-of-the-art performance of all the compared
fully-supervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05418">ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning. (arXiv:2401.05418v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Haidri_S/0/1/0/all/0/1">Salman Haidri</a></p>
<p>The advent of compact, handheld devices has given us a pool of tracked
movement data that could be used to infer trends and patterns that can be made
to use. With this flooding of various trajectory data of animals, humans,
vehicles, etc., the idea of ANALYTiC originated, using active learning to infer
semantic annotations from the trajectories by learning from sets of labeled
data. This study explores the application of dimensionality reduction and
decision boundaries in combination with the already present active learning,
highlighting patterns and clusters in data. We test these features with three
different trajectory datasets with objective of exploiting the the already
labeled data and enhance their interpretability. Our experimental analysis
exemplifies the potential of these combined methodologies in improving the
efficiency and accuracy of trajectory labeling. This study serves as a
stepping-stone towards the broader integration of machine learning and visual
methods in context of movement data analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05420">HoloBeam: Learning Optimal Beamforming in Far-Field Holographic Metasurface Transceivers. (arXiv:2401.05420v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ghosh_D/0/1/0/all/0/1">Debamita Ghosh</a>, <a href="http://arxiv.org/find/eess/1/au:+Hanawal_M/0/1/0/all/0/1">Manjesh Kumar Hanawal</a>, <a href="http://arxiv.org/find/eess/1/au:+Zlatanova_N/0/1/0/all/0/1">Nikola Zlatanova</a></p>
<p>Holographic Metasurface Transceivers (HMTs) are emerging as cost-effective
substitutes to large antenna arrays for beamforming in Millimeter and TeraHertz
wave communication. However, to achieve desired channel gains through
beamforming in HMT, phase-shifts of a large number of elements need to be
appropriately set, which is challenging. Also, these optimal phase-shifts
depend on the location of the receivers, which could be unknown. In this work,
we develop a learning algorithm using a {\it fixed-budget multi-armed bandit
framework} to beamform and maximize received signal strength at the receiver
for far-field regions. Our algorithm, named \Algo exploits the parametric form
of channel gains of the beams, which can be expressed in terms of two {\it
phase-shifting parameters}. Even after parameterization, the problem is still
challenging as phase-shifting parameters take continuous values. To overcome
this, {\it\HB} works with the discrete values of phase-shifting parameters and
exploits their unimodal relations with channel gains to learn the optimal
values faster. We upper bound the probability of {\it\HB} incorrectly
identifying the (discrete) optimal phase-shift parameters in terms of the
number of pilots used in learning. We show that this probability decays
exponentially with the number of pilot signals. We demonstrate that {\it\HB}
outperforms state-of-the-art algorithms through extensive simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05421">WildGEN: Long-horizon Trajectory Generation for Wildlife. (arXiv:2401.05421v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Al_Lawati_A/0/1/0/all/0/1">Ali Al-Lawati</a>, <a href="http://arxiv.org/find/cs/1/au:+Eshra_E/0/1/0/all/0/1">Elsayed Eshra</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1">Prasenjit Mitra</a></p>
<p>Trajectory generation is an important concern in pedestrian, vehicle, and
wildlife movement studies. Generated trajectories help enrich the training
corpus in relation to deep learning applications, and may be used to facilitate
simulation tasks. This is especially significant in the wildlife domain, where
the cost of obtaining additional real data can be prohibitively expensive,
time-consuming, and bear ethical considerations. In this paper, we introduce
WildGEN: a conceptual framework that addresses this challenge by employing a
Variational Auto-encoders (VAEs) based method for the acquisition of movement
characteristics exhibited by wild geese over a long horizon using a sparse set
of truth samples. A subsequent post-processing step of the generated
trajectories is performed based on smoothing filters to reduce excessive
wandering. Our evaluation is conducted through visual inspection and the
computation of the Hausdorff distance between the generated and real
trajectories. In addition, we utilize the Pearson Correlation Coefficient as a
way to measure how realistic the trajectories are based on the similarity of
clusters evaluated on the generated and real trajectories.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05424">A Toolbox for Modelling Engagement with Educational Videos. (arXiv:2401.05424v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yuxiang Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Djemili_K/0/1/0/all/0/1">Karim Djemili</a>, <a href="http://arxiv.org/find/cs/1/au:+Elezi_D/0/1/0/all/0/1">Denis Elezi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalman_A/0/1/0/all/0/1">Aaneel Shalman</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1">Mar&#xed;a P&#xe9;rez-Ortiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1">Emine Yilmaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1">John Shawe-Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1">Sahan Bulathwela</a></p>
<p>With the advancement and utility of Artificial Intelligence (AI),
personalising education to a global population could be a cornerstone of new
educational systems in the future. This work presents the PEEKC dataset and the
TrueLearn Python library, which contains a dataset and a series of online
learner state models that are essential to facilitate research on learner
engagement modelling.TrueLearn family of models was designed following the
"open learner" concept, using humanly-intuitive user representations. This
family of scalable, online models also help end-users visualise the learner
models, which may in the future facilitate user interaction with their
models/recommenders. The extensive documentation and coding examples make the
library highly accessible to both machine learning developers and educational
data mining and learning analytics practitioners. The experiments show the
utility of both the dataset and the library with predictive performance
significantly exceeding comparative baseline models. The dataset contains a
large amount of AI-related educational videos, which are of interest for
building and validating AI-specific educational recommenders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05425">An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic Seizure Detection. (arXiv:2401.05425v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Aziz_A/0/1/0/all/0/1">Abdul Aziz</a>, <a href="http://arxiv.org/find/eess/1/au:+Pham_N/0/1/0/all/0/1">Nhat Pham</a>, <a href="http://arxiv.org/find/eess/1/au:+Vora_N/0/1/0/all/0/1">Neel Vora</a>, <a href="http://arxiv.org/find/eess/1/au:+Reynolds_C/0/1/0/all/0/1">Cody Reynolds</a>, <a href="http://arxiv.org/find/eess/1/au:+Lehnen_J/0/1/0/all/0/1">Jaime Lehnen</a>, <a href="http://arxiv.org/find/eess/1/au:+Venkatesh_P/0/1/0/all/0/1">Pooja Venkatesh</a>, <a href="http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1">Zhuoran Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Harvey_J/0/1/0/all/0/1">Jay Harvey</a>, <a href="http://arxiv.org/find/eess/1/au:+Vu_T/0/1/0/all/0/1">Tam Vu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1">Kan Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_P/0/1/0/all/0/1">Phuc Nguyen</a></p>
<p>Epilepsy is one of the most common neurological diseases globally, affecting
around 50 million people worldwide. Fortunately, up to 70 percent of people
with epilepsy could live seizure-free if properly diagnosed and treated, and a
reliable technique to monitor the onset of seizures could improve the quality
of life of patients who are constantly facing the fear of random seizure
attacks. The scalp-based EEG test, despite being the gold standard for
diagnosing epilepsy, is costly, necessitates hospitalization, demands skilled
professionals for operation, and is discomforting for users. In this paper, we
propose EarSD, a novel lightweight, unobtrusive, and socially acceptable
ear-worn system to detect epileptic seizure onsets by measuring the
physiological signals from behind the user's ears. EarSD includes an integrated
custom-built sensing, computing, and communication PCB to collect and amplify
the signals of interest, remove the noises caused by motion artifacts and
environmental impacts, and stream the data wirelessly to the computer or mobile
phone nearby, where data are uploaded to the host computer for further
processing. We conducted both in-lab and in-hospital experiments with epileptic
seizure patients who were hospitalized for seizure studies. The preliminary
results confirm that EarSD can detect seizures with up to 95.3 percent accuracy
by just using classical machine learning algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05426">CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1">Mengxi Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1">Zimin Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Geissler_D/0/1/0/all/0/1">Daniel Gei&#xdf;ler</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Suh_S/0/1/0/all/0/1">Sungho Suh</a>, <a href="http://arxiv.org/find/eess/1/au:+Lukowicz_P/0/1/0/all/0/1">Paul Lukowicz</a></p>
<p>Recent advancements in Artificial Neural Networks have significantly improved
human activity recognition using multiple time-series sensors. While employing
numerous sensors with high-frequency sampling rates usually improves the
results, it often leads to data inefficiency and unnecessary expansion of the
ANN, posing a challenge for their practical deployment on edge devices.
Addressing these issues, our work introduces a pragmatic framework for
data-efficient utilization in HAR tasks, considering the optimization of both
sensor modalities and sampling rate simultaneously. Central to our approach are
the designed trainable parameters, termed 'Weight Scores,' which assess the
significance of each sensor modality and sampling rate during the training
phase. These scores guide the sensor modalities and sampling rate selection.
The pruning method allows users to make a trade-off between computational
budgets and performance by selecting the sensor modalities and sampling rates
according to the weight score ranking. We tested our framework's effectiveness
in optimizing sensor modality and sampling rate selection using three public
HAR benchmark datasets. The results show that the sensor and sampling rate
combination selected via CoSS achieves similar classification performance to
configurations using the highest sampling rate with all sensors but at a
reduced hardware cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05430">Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification. (arXiv:2401.05430v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+You_Z/0/1/0/all/0/1">Zinuo You</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhang_P/0/1/0/all/0/1">Pengju Zhang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zheng_J/0/1/0/all/0/1">Jin Zheng</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Cartlidge_J/0/1/0/all/0/1">John Cartlidge</a></p>
<p>Stock trend classification remains a fundamental yet challenging task, owing
to the intricate time-evolving dynamics between and within stocks. To tackle
these two challenges, we propose a graph-based representation learning approach
aimed at predicting the future movements of multiple stocks. Initially, we
model the complex time-varying relationships between stocks by generating
dynamic multi-relational stock graphs. This is achieved through a novel edge
generation algorithm that leverages information entropy and signal energy to
quantify the intensity and directionality of inter-stock relations on each
trading day. Then, we further refine these initial graphs through a stochastic
multi-relational diffusion process, adaptively learning task-optimal edges.
Subsequently, we implement a decoupled representation learning scheme with
parallel retention to obtain the final graph representation. This strategy
better captures the unique temporal features within individual stocks while
also capturing the overall structure of the stock graph. Comprehensive
experiments conducted on real-world datasets from two US markets (NASDAQ and
NYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the
effectiveness of our method. Our approach consistently outperforms
state-of-the-art baselines in forecasting next trading day stock trends across
three test periods spanning seven years. Datasets and code have been released
(https://github.com/pixelhero98/MGDPR).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05431">TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing. (arXiv:2401.05431v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1">Luyuan Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1">Cong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhai_S/0/1/0/all/0/1">Shengfang Zhai</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1">Yuejian Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_Q/0/1/0/all/0/1">Qingni Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1">Zhonghai Wu</a></p>
<p>Representation learning frameworks in unlabeled time series have been
proposed for medical signal processing. Despite the numerous excellent
progresses have been made in previous works, we observe the representation
extracted for the time series still does not generalize well. In this paper, we
present a Time series (medical signal) Representation Learning framework via
Spectrogram (TRLS) to get more informative representations. We transform the
input time-domain medical signals into spectrograms and design a time-frequency
encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale
representations from the augmented spectrograms. Our TRLS takes spectrogram as
input with two types of different data augmentations and maximizes the
similarity between positive ones, which effectively circumvents the problem of
designing negative samples. Our evaluation of four real-world medical signal
datasets focusing on medical signal classification shows that TRLS is superior
to the existing frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05432">TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks. (arXiv:2401.05432v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1">Khondoker Murad Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1">Tim Oates</a></p>
<p>As deep neural networks and the datasets used to train them get larger, the
default approach to integrating them into research and commercial projects is
to download a pre-trained model and fine tune it. But these models can have
uncertain provenance, opening up the possibility that they embed hidden
malicious behavior such as trojans or backdoors, where small changes to an
input (triggers) can cause the model to produce incorrect outputs (e.g., to
misclassify). This paper introduces a novel approach to backdoor detection that
uses two tensor decomposition methods applied to network activations. This has
a number of advantages relative to existing detection methods, including the
ability to analyze multiple models at the same time, working across a wide
variety of network architectures, making no assumptions about the nature of
triggers used to alter network behavior, and being computationally efficient.
We provide a detailed description of the detection pipeline along with results
on models trained on the MNIST digit dataset, CIFAR-10 dataset, and two
difficult datasets from NIST's TrojAI competition. These results show that our
method detects backdoored networks more accurately and efficiently than current
state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05434">ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification. (arXiv:2401.05434v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Akan_T/0/1/0/all/0/1">Taymaz Akan</a>, <a href="http://arxiv.org/find/eess/1/au:+Alp_S/0/1/0/all/0/1">Sait Alp</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhuiyan_M/0/1/0/all/0/1">Mohammad Alfrad Nobel Bhuiyan</a></p>
<p>An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat.
There are various types of arrhythmias that can originate from different areas
of the heart, resulting in either a rapid, slow, or irregular heartbeat. An
electrocardiogram (ECG) is a vital diagnostic tool used to detect heart
irregularities and abnormalities, allowing experts to analyze the heart's
electrical signals to identify intricate patterns and deviations from the norm.
Over the past few decades, numerous studies have been conducted to develop
automated methods for classifying heartbeats based on ECG data. In recent
years, deep learning has demonstrated exceptional capabilities in tackling
various medical challenges, particularly with transformers as a model
architecture for sequence processing. By leveraging the transformers, we
developed the ECGformer model for the classification of various arrhythmias
present in electrocardiogram data. We assessed the suggested approach using the
MIT-BIH and PTB datasets. ECG heartbeat arrhythmia classification results show
that the proposed method is highly effective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05436">Deep OFDM Channel Estimation: Capturing Frequency Recurrence. (arXiv:2401.05436v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jameel_A/0/1/0/all/0/1">Abu Shafin Mohammad Mahdee Jameel</a>, <a href="http://arxiv.org/find/eess/1/au:+Malhotra_A/0/1/0/all/0/1">Akshay Malhotra</a>, <a href="http://arxiv.org/find/eess/1/au:+Gamal_A/0/1/0/all/0/1">Aly El Gamal</a>, <a href="http://arxiv.org/find/eess/1/au:+Hamidi_Rad_S/0/1/0/all/0/1">Shahab Hamidi-Rad</a></p>
<p>In this paper, we propose a deep-learning-based channel estimation scheme in
an orthogonal frequency division multiplexing (OFDM) system. Our proposed
method, named Single Slot Recurrence Along Frequency Network (SisRafNet), is
based on a novel study of recurrent models for exploiting sequential behavior
of channels across frequencies. Utilizing the fact that wireless channels have
a high degree of correlation across frequencies, we employ recurrent neural
network techniques within a single OFDM slot, thus overcoming the latency and
memory constraints typically associated with recurrence based methods. The
proposed SisRafNet delivers superior estimation performance compared to
existing deep-learning-based channel estimation techniques and the performance
has been validated on a wide range of 3rd Generation Partnership Project (3GPP)
compliant channel scenarios at multiple signal-to-noise ratios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05437">Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jungo_J/0/1/0/all/0/1">Janosch Jungo</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiang_Y/0/1/0/all/0/1">Yutong Xiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gashi_S/0/1/0/all/0/1">Shkurta Gashi</a>, <a href="http://arxiv.org/find/eess/1/au:+Holz_C/0/1/0/all/0/1">Christian Holz</a></p>
<p>Wearable devices continuously collect sensor data and use it to infer an
individual's behavior, such as sleep, physical activity, and emotions. Despite
the significant interest and advancements in this field, modeling multimodal
sensor data in real-world environments is still challenging due to low data
quality and limited data annotations. In this work, we investigate
representation learning for imputing missing wearable data and compare it with
state-of-the-art statistical approaches. We investigate the performance of the
transformer model on 10 physiological and behavioral signals with different
masking ratios. Our results show that transformers outperform baselines for
missing data imputation of signals that change more frequently, but not for
monotonic signals. We further investigate the impact of imputation strategies
and masking rations on downstream classification tasks. Our study provides
insights for the design and development of masking-based self-supervised
learning tasks and advocates the adoption of hybrid-based imputation strategies
to address the challenge of missing data in wearable devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05439">Physics-informed Deep Learning to Solve Three-dimensional Terzaghi Consolidation Equation: Forward and Inverse Problems. (arXiv:2401.05439v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1">Biao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Heitor_A/0/1/0/all/0/1">Ana Heitor</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">He Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaohui Chen</a></p>
<p>The emergence of neural networks constrained by physical governing equations
has sparked a new trend in deep learning research, which is known as
Physics-Informed Neural Networks (PINNs). However, solving high-dimensional
problems with PINNs is still a substantial challenge, the space complexity
brings difficulty to solving large multidirectional problems. In this paper, a
novel PINN framework to quickly predict several three-dimensional Terzaghi
consolidation cases under different conditions is proposed. Meanwhile, the loss
functions for different cases are introduced, and their differences in
three-dimensional consolidation problems are highlighted. The tuning strategies
for the PINNs framework for three-dimensional consolidation problems are
introduced. Then, the performance of PINNs is tested and compared with
traditional numerical methods adopted in forward problems, and the coefficients
of consolidation and the impact of noisy data in inverse problems are
identified. Finally, the results are summarized and presented from
three-dimensional simulations of PINNs, which show an accuracy rate of over 99%
compared with ground truth for both forward and inverse problems. These results
are desirable with good accuracy and can be used for soil settlement
prediction, which demonstrates that the proposed PINNs framework can learn the
three-dimensional consolidation PDE well.
</p>
<p>Keywords: Three-dimensional Terzaghi consolidation; Physics-informed neural
networks (PINNs); Forward problems; Inverse problems; soil settlement
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05440">Autosen: improving automatic wifi human sensing through cross-modal autoencoder. (arXiv:2401.05440v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1">Qian Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Hao_Y/0/1/0/all/0/1">Yanling Hao</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yuanwei Liu</a></p>
<p>WiFi human sensing is highly regarded for its low-cost and privacy advantages
in recognizing human activities. However, its effectiveness is largely confined
to controlled, single-user, line-of-sight settings, limited by data collection
complexities and the scarcity of labeled datasets. Traditional cross-modal
methods, aimed at mitigating these limitations by enabling self-supervised
learning without labeled data, struggle to extract meaningful features from
amplitude-phase combinations. In response, we introduce AutoSen, an innovative
automatic WiFi sensing solution that departs from conventional approaches.
AutoSen establishes a direct link between amplitude and phase through automated
cross-modal autoencoder learning. This autoencoder efficiently extracts
valuable features from unlabeled CSI data, encompassing amplitude and phase
information while eliminating their respective unique noises. These features
are then leveraged for specific tasks using few-shot learning techniques.
AutoSen's performance is rigorously evaluated on a publicly accessible
benchmark dataset, demonstrating its exceptional capabilities in automatic WiFi
sensing through the extraction of comprehensive cross-modal features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05441">An adaptive network-based approach for advanced forecasting of cryptocurrency values. (arXiv:2401.05441v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Mehrban_A/0/1/0/all/0/1">Ali Mehrban</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Ahadian_P/0/1/0/all/0/1">Pegah Ahadian</a></p>
<p>This paper describes an architecture for predicting the price of
cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy
Inference System (ANFIS). Historical data of cryptocurrencies and indexes that
are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D),
and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach
the data are hybrid and backpropagation algorithms, as well as grid partition,
subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which
are used in data clustering. The architectural performance designed in this
paper has been compared with different inputs and neural network models in
terms of statistical evaluation criteria. Finally, the proposed method can
predict the price of digital currencies in a short time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05442">Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1">Jakub Grudzien Kuba</a>, <a href="http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1">Masatoshi Uehara</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>While machine learning models are typically trained to solve prediction
problems, we might often want to use them for optimization problems. For
example, given a dataset of proteins and their corresponding fluorescence
levels, we might want to optimize for a new protein with the highest possible
fluorescence. This kind of data-driven optimization (DDO) presents a range of
challenges beyond those in standard prediction problems, since we need models
that successfully predict the performance of new designs that are better than
the best designs seen in the training set. It is not clear theoretically when
existing approaches can even perform better than the naive approach that simply
selects the best design in the dataset. In this paper, we study how structure
can enable sample-efficient data-driven optimization. To formalize the notion
of structure, we introduce functional graphical models (FGMs) and show
theoretically how they can provide for principled data-driven optimization by
decomposing the original high-dimensional optimization problem into smaller
sub-problems. This allows us to derive much more practical regret bounds for
DDO, and the result implies that DDO with FGMs can achieve nearly optimal
designs in situations where naive approaches fail due to insufficient coverage
of the offline data. We further present a data-driven optimization algorithm
that inferes the FGM structure itself, either over the original input variables
or a latent variable representation of the inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05444">Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning. (arXiv:2401.05444v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Ding Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1">Peixi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tiejun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a></p>
<p>With the help of special neuromorphic hardware, spiking neural networks
(SNNs) are expected to realize artificial intelligence (AI) with less energy
consumption. It provides a promising energy-efficient way for realistic control
tasks by combining SNNs with deep reinforcement learning (DRL). In this paper,
we focus on the task where the agent needs to learn multi-dimensional
deterministic policies to control, which is very common in real scenarios.
Recently, the surrogate gradient method has been utilized for training
multi-layer SNNs, which allows SNNs to achieve comparable performance with the
corresponding deep networks in this task. Most existing spike-based RL methods
take the firing rate as the output of SNNs, and convert it to represent
continuous action space (i.e., the deterministic policy) through a
fully-connected (FC) layer. However, the decimal characteristic of the firing
rate brings the floating-point matrix operations to the FC layer, making the
whole SNN unable to deploy on the neuromorphic hardware directly. To develop a
fully spiking actor network without any floating-point matrix operations, we
draw inspiration from the non-spiking interneurons found in insects and employ
the membrane voltage of the non-spiking neurons to represent the action. Before
the non-spiking neurons, multiple population neurons are introduced to decode
different dimensions of actions. Since each population is used to decode a
dimension of action, we argue that the neurons in each population should be
connected in time domain and space domain. Hence, the intra-layer connections
are used in output populations to enhance the representation capacity. Finally,
we propose a fully spiking actor network with intra-layer connections
(ILC-SAN).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05446">Self-supervised Learning for Electroencephalogram: A Systematic Survey. (arXiv:2401.05446v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Weng_W/0/1/0/all/0/1">Weining Weng</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1">Yang Gu</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_S/0/1/0/all/0/1">Shuai Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1">Yuan Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zhaohua Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yuchen Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yiqiang Chen</a></p>
<p>Electroencephalogram (EEG) is a non-invasive technique to record
bioelectrical signals. Integrating supervised deep learning techniques with EEG
signals has recently facilitated automatic analysis across diverse EEG-based
tasks. However, the label issues of EEG signals have constrained the
development of EEG-based deep models. Obtaining EEG annotations is difficult
that requires domain experts to guide collection and labeling, and the
variability of EEG signals among different subjects causes significant label
shifts. To solve the above challenges, self-supervised learning (SSL) has been
proposed to extract representations from unlabeled samples through
well-designed pretext tasks. This paper concentrates on integrating SSL
frameworks with temporal EEG signals to achieve efficient representation and
proposes a systematic review of the SSL for EEG signals. In this paper, 1) we
introduce the concept and theory of self-supervised learning and typical SSL
frameworks. 2) We provide a comprehensive review of SSL for EEG analysis,
including taxonomy, methodology, and technique details of the existing
EEG-based SSL frameworks, and discuss the difference between these methods. 3)
We investigate the adaptation of the SSL approach to various downstream tasks,
including the task description and related benchmark datasets. 4) Finally, we
discuss the potential directions for future SSL-EEG research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05452">Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site PPG using Transformer &amp; Frequency-domain Learning. (arXiv:2401.05452v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tahir_M/0/1/0/all/0/1">Muhammad Ahmad Tahir</a>, <a href="http://arxiv.org/find/eess/1/au:+Mehmood_A/0/1/0/all/0/1">Ahsan Mehmood</a>, <a href="http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1">Muhammad Mahboob Ur Rahman</a>, <a href="http://arxiv.org/find/eess/1/au:+Nawaz_M/0/1/0/all/0/1">Muhammad Wasim Nawaz</a>, <a href="http://arxiv.org/find/eess/1/au:+Riaz_K/0/1/0/all/0/1">Kashif Riaz</a>, <a href="http://arxiv.org/find/eess/1/au:+Abbasi_Q/0/1/0/all/0/1">Qammer H. Abbasi</a></p>
<p>We propose two novel purpose-built deep learning (DL) models for synthesis of
the arterial blood pressure (ABP) waveform in a cuff-less manner, using a
single-site photoplethysmography (PPG) signal. We utilize the public UCI
dataset on cuff-less blood pressure (CLBP) estimation to train and evaluate our
DL models. Firstly, we implement a transformer model that incorporates
positional encoding, multi-head attention, layer normalization, and dropout
techniques, and synthesizes the ABP waveform with a mean absolute error (MAE)
of 14. Secondly, we implement a frequency-domain (FD) learning approach where
we first obtain the discrete cosine transform (DCT) coefficients of the PPG and
ABP signals corresponding to two cardiac cycles, and then learn a
linear/non-linear (L/NL) regression between them. We learn that the FD L/NL
regression model outperforms the transformer model by achieving an MAE of 11.87
and 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP),
respectively. Our FD L/NL regression model also fulfills the AAMI criterion of
utilizing data from more than 85 subjects, and achieves grade B by the BHS
criterion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05453">Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Anderberg_A/0/1/0/all/0/1">Alastair Anderberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1">James Bailey</a>, <a href="http://arxiv.org/find/cs/1/au:+Campello_R/0/1/0/all/0/1">Ricardo J. G. B. Campello</a>, <a href="http://arxiv.org/find/cs/1/au:+Houle_M/0/1/0/all/0/1">Michael E. Houle</a>, <a href="http://arxiv.org/find/cs/1/au:+Marques_H/0/1/0/all/0/1">Henrique O. Marques</a>, <a href="http://arxiv.org/find/cs/1/au:+Radovanovic_M/0/1/0/all/0/1">Milo&#x161; Radovanovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimek_A/0/1/0/all/0/1">Arthur Zimek</a></p>
<p>We present a nonparametric method for outlier detection that takes full
account of local variations in intrinsic dimensionality within the dataset.
Using the theory of Local Intrinsic Dimensionality (LID), our
'dimensionality-aware' outlier detection method, DAO, is derived as an
estimator of an asymptotic local expected density ratio involving the query
point and a close neighbor drawn at random. The dimensionality-aware behavior
of DAO is due to its use of local estimation of LID values in a
theoretically-justified way. Through comprehensive experimentation on more than
800 synthetic and real datasets, we show that DAO significantly outperforms
three popular and important benchmark outlier detection methods: Local Outlier
Factor (LOF), Simplified LOF, and kNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05458">CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance. (arXiv:2401.05458v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Ruofan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rundensteiner_E/0/1/0/all/0/1">Elke Rundensteiner</a></p>
<p>Deep neural networks (DNNs) have advanced many machine learning tasks, but
their performance is often harmed by noisy labels in real-world data.
Addressing this, we introduce CoLafier, a novel approach that uses Local
Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier
consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized
classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a
sample's features and its label to predict the label - which allows it to
produce an enhanced internal representation. We observe that LID scores
computed from this representation effectively distinguish between correct and
incorrect labels across various noise scenarios. In contrast to LID-dis,
LID-gen, functioning as a regular classifier, operates solely on the sample's
features. During training, CoLafier utilizes two augmented views per instance
to feed both subnets. CoLafier considers the LID scores from the two views as
produced by LID-dis to assign weights in an adapted loss function for both
subnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels.
LID-dis then processes these pseudo-labels along with two views to derive LID
scores. Finally, these LID scores along with the differences in predictions
from the two subnets guide the label update decisions. This dual-view and
dual-subnet approach enhances the overall reliability of the framework. Upon
completion of the training, we deploy the LID-gen subnet of CoLafier as the
final classification model. CoLafier demonstrates improved prediction accuracy,
surpassing existing methods, particularly under severe label noise. For more
details, see the code at https://github.com/zdy93/CoLafier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05461">The two-way knowledge interaction interface between humans and neural networks. (arXiv:2401.05461v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhanliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1">Nuoye Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1">Peiyi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guangming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liang Zhang</a></p>
<p>Despite neural networks (NN) have been widely applied in various fields and
generally outperforms humans, they still lack interpretability to a certain
extent, and humans are unable to intuitively understand the decision logic of
NN. This also hinders the knowledge interaction between humans and NN,
preventing humans from getting involved to give direct guidance when NN's
decisions go wrong. While recent research in explainable AI has achieved
interpretability of NN from various perspectives, it has not yet provided
effective methods for knowledge exchange between humans and NN. To address this
problem, we constructed a two-way interaction interface that uses structured
representations of visual concepts and their relationships as the "language"
for knowledge exchange between humans and NN. Specifically, NN provide
intuitive reasoning explanations to humans based on the class-specific
structural concepts graph (C-SCG). On the other hand, humans can modify the
biases present in the C-SCG through their prior knowledge and reasoning
ability, and thus provide direct knowledge guidance to NN through this
interface. Through experimental validation, based on this interaction
interface, NN can provide humans with easily understandable explanations of the
reasoning process. Furthermore, human involvement and prior knowledge can
directly and effectively contribute to enhancing the performance of NN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05467">Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Taneja_K/0/1/0/all/0/1">Karan Taneja</a>, <a href="http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1">Ashok Goel</a></p>
<p>The recent advances in large language models (LLMs) have led to the creation
of many modular AI agents. These agents employ LLMs as zero-shot learners to
perform sub-tasks in order to solve complex tasks set forth by human users. We
propose an approach to enhance the robustness and performance of modular AI
agents that utilize LLMs as zero-shot learners. Our iterative machine teaching
method offers an efficient way to teach AI agents over time with limited human
feedback, addressing the limit posed by the quality of zero-shot learning. We
advocate leveraging the data traces from initial deployments and outputs or
annotations from the zero-shot learners to train smaller and task-specific
substitute models which can reduce both the monetary costs and environmental
impact. Our machine teaching process avails human expertise to correct examples
with a high likelihood of misannotations. Results on three tasks, common to
conversational AI agents, show that close-to-oracle performance can be achieved
with supervision on 20-70% of the dataset depending upon the complexity of the
task and performance of zero-shot learners.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05468">Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks. (arXiv:2401.05468v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zanardini_D/0/1/0/all/0/1">Damiano Zanardini</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrano_E/0/1/0/all/0/1">Emilio Serrano</a></p>
<p>This paper introduces a new problem in the field of graph mining and social
network analysis called new node prediction. More technically, the task can be
categorized as zero-shot out-of-graph all-links prediction. This challenging
problem aims to predict all links from a new, isolated, and unobserved node
that was previously disconnected from the graph. Unlike classic approaches to
link prediction (including few-shot out-of-graph link prediction), this problem
presents two key differences: (1) the new node has no existing links from which
to extract patterns for new predictions; and (2) the goal is to predict not
just one, but all the links of this new node, or at least a significant part of
them. Experiments demonstrate that an architecture based on Deep Graph Neural
Networks can learn to solve this challenging problem in a bibliographic
citation network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05469">Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU. (arXiv:2401.05469v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kazemi_K/0/1/0/all/0/1">Kianoosh Kazemi</a>, <a href="http://arxiv.org/find/eess/1/au:+Azimi_I/0/1/0/all/0/1">Iman Azimi</a>, <a href="http://arxiv.org/find/eess/1/au:+Liljeberg_P/0/1/0/all/0/1">Pasi Liljeberg</a>, <a href="http://arxiv.org/find/eess/1/au:+Rahmani_A/0/1/0/all/0/1">Amir M. Rahmani</a></p>
<p>Respiratory rate (RR) serves as an indicator of various medical conditions,
such as cardiovascular diseases and sleep disorders. These RR estimation
methods were mostly designed for finger-based PPG collected from subjects in
stationary situations (e.g., in hospitals). In contrast to finger-based PPG
signals, wrist-based PPG are more susceptible to noise, particularly in their
low frequency range, which includes respiratory information. Therefore, the
existing methods struggle to accurately extract RR when PPG data are collected
from wrist area under free-living conditions. The increasing popularity of
smartwatches, equipped with various sensors including PPG, has prompted the
need for a robust RR estimation method. In this paper, we propose a
convolutional neural network-based approach to extract RR from PPG,
accelerometer, and gyroscope signals captured via smartwatches. Our method,
including a dilated residual inception module and 1D convolutions, extract the
temporal information from the signals, enabling RR estimation. Our method is
trained and tested using data collected from 36 subjects under free-living
conditions for one day using Samsung Gear Sport watches. For evaluation, we
compare the proposed method with four state-of-the-art RR estimation methods.
The RR estimates are compared with RR references obtained from a chest-band
device. The results show that our method outperforms the existing methods with
the Mean-Absolute-Error and Root-Mean-Square-Error of 1.85 and 2.34, while the
best results obtained by the other methods are 2.41 and 3.29, respectively.
Moreover, compared to the other methods, the absolute error distribution of our
method was narrow (with the lowest median), indicating a higher level of
agreement between the estimated and reference RR values.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05470">Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts. (arXiv:2401.05470v1 [q-bio.PE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Estopinan_J/0/1/0/all/0/1">Joaquim Estopinan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bonnet_P/0/1/0/all/0/1">Pierre Bonnet</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Servajean_M/0/1/0/all/0/1">Maximilien Servajean</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Munoz_F/0/1/0/all/0/1">Fran&#xe7;ois Munoz</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Joly_A/0/1/0/all/0/1">Alexis Joly</a></p>
<p>The post-2020 global biodiversity framework needs ambitious, research-based
targets. Estimating the accelerated extinction risk due to climate change is
critical. The International Union for Conservation of Nature (IUCN) measures
the extinction risk of species. Automatic methods have been developed to
provide information on the IUCN status of under-assessed taxa. However, these
compensatory methods are based on current species characteristics, mainly
geographical, which precludes their use in future projections. Here, we
evaluate a novel method for classifying the IUCN status of species benefiting
from the generalisation power of species distribution models based on deep
learning. Our method matches state-of-the-art classification performance while
relying on flexible SDM-based features that capture species' environmental
preferences. Cross-validation yields average accuracies of 0.61 for status
classification and 0.78 for binary classification. Climate change will reshape
future species distributions. Under the species-environment equilibrium
hypothesis, SDM projections approximate plausible future outcomes. Two extremes
of species dispersal capacity are considered: unlimited or null. The projected
species distributions are translated into features feeding our IUCN
classification method. Finally, trends in threatened species are analysed over
time and i) by continent and as a function of average ii) latitude or iii)
altitude. The proportion of threatened species is increasing globally, with
critical rates in Africa, Asia and South America. Furthermore, the proportion
of threatened species is predicted to peak around the two Tropics, at the
Equator, in the lowlands and at altitudes of 800-1,500 m.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05477">Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors. (arXiv:2401.05477v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yiran Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haibin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yexu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedel_T/0/1/0/all/0/1">Till Riedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Beigl_M/0/1/0/all/0/1">Michael Beigl</a></p>
<p>In recent years, deep learning has emerged as a potent tool across a
multitude of domains, leading to a surge in research pertaining to its
application in the wearable human activity recognition (WHAR) domain. Despite
the rapid development, concerns have been raised about the lack of
standardization and consistency in the procedures used for experimental model
training, which may affect the reproducibility and reliability of research
results. In this paper, we provide an exhaustive review of contemporary deep
learning research in the field of WHAR and collate information pertaining to
the training procedure employed in various studies. Our findings suggest that a
major trend is the lack of detail provided by model training protocols.
Besides, to gain a clearer understanding of the impact of missing descriptions,
we utilize a control variables approach to assess the impact of key tunable
components (e.g., optimization techniques and early stopping criteria) on the
inter-subject generalization capabilities of HAR models. With insights from the
analyses, we define a novel integrated training procedure tailored to the WHAR
model. Empirical results derived using five well-known \ac{whar} benchmark
datasets and three classical HAR model architectures demonstrate the
effectiveness of our proposed methodology: in particular, there is a
significant improvement in macro F1 leave one subject out cross-validation
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05478">Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups. (arXiv:2401.05478v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stephens_A/0/1/0/all/0/1">Anna Stephens</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1">Francisco Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1">Pang-Ning Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Esfahanian_A/0/1/0/all/0/1">Abdol-Hossein Esfahanian</a></p>
<p>Graph neural networks (GNN) are a powerful tool for combining imaging and
non-imaging medical information for node classification tasks. Cross-network
node classification extends GNN techniques to account for domain drift,
allowing for node classification on an unlabeled target network. In this paper
we present OTGCN, a powerful, novel approach to cross-network node
classification. This approach leans on concepts from graph convolutional
networks to harness insights from graph data structures while simultaneously
applying strategies rooted in optimal transport to correct for the domain drift
that can occur between samples from different data collection sites. This
blended approach provides a practical solution for scenarios with many distinct
forms of data collected across different locations and equipment. We
demonstrate the effectiveness of this approach at classifying Autism Spectrum
Disorder subjects using a blend of imaging and non-imaging data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05479">The recursive scheme of clustering. (arXiv:2401.05479v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miniak_Gorecka_A/0/1/0/all/0/1">Alicja Miniak-G&#xf3;recka</a>, <a href="http://arxiv.org/find/cs/1/au:+Podlaski_K/0/1/0/all/0/1">Krzysztof Podlaski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gwizdalla_T/0/1/0/all/0/1">Tomasz Gwizda&#x142;&#x142;a</a></p>
<p>The problem of data clustering is one of the most important in data analysis.
It can be problematic when dealing with experimental data characterized by
measurement uncertainties and errors. Our paper proposes a recursive scheme for
clustering data obtained in geographical (climatological) experiments. The
discussion of results obtained by k-means and SOM methods with the developed
recursive procedure is presented. We show that the clustering using the new
approach gives more acceptable results when compared to experts assessments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05502">Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thejaswi_S/0/1/0/all/0/1">Suhas Thejaswi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gadekar_A/0/1/0/all/0/1">Ameet Gadekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordozgoiti_B/0/1/0/all/0/1">Bruno Ordozgoiti</a>, <a href="http://arxiv.org/find/cs/1/au:+Gionis_A/0/1/0/all/0/1">Aristides Gionis</a></p>
<p>In this work, we study diversity-aware clustering problems where the data
points are associated with multiple attributes resulting in intersecting
groups. A clustering solution need to ensure that a minimum number of cluster
centers are chosen from each group while simultaneously minimizing the
clustering objective, which can be either $k$-median, $k$-means or
$k$-supplier. We present parameterized approximation algorithms with
approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for
diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware
$k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH
and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint
faicility groups, we present parameterized approximation algorithm with
approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For
fair $k$-supplier with disjoint facility groups, we present a polynomial-time
approximation algorithm with factor $3$, improving the previous best known
approximation ratio of factor $5$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05518">Correlated Quantization for Faster Nonconvex Distributed Optimization. (arXiv:2401.05518v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panferov_A/0/1/0/all/0/1">Andrei Panferov</a>, <a href="http://arxiv.org/find/cs/1/au:+Demidovich_Y/0/1/0/all/0/1">Yury Demidovich</a>, <a href="http://arxiv.org/find/cs/1/au:+Rammal_A/0/1/0/all/0/1">Ahmad Rammal</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a></p>
<p>Quantization (Alistarh et al., 2017) is an important (stochastic) compression
technique that reduces the volume of transmitted bits during each communication
round in distributed model training. Suresh et al. (2022) introduce correlated
quantizers and show their advantages over independent counterparts by analyzing
distributed SGD communication complexity. We analyze the forefront distributed
non-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the
proposed correlated quantizers and show that it outperforms the original MARINA
and distributed SGD of Suresh et al. (2022) with regard to the communication
complexity. We significantly refine the original analysis of MARINA without any
additional assumptions using the weighted Hessian variance (Tyurin et al.,
2022), and then we expand the theoretical framework of MARINA to accommodate a
substantially broader range of potentially correlated and biased compressors,
thus dilating the applicability of the method beyond the conventional
independent unbiased compressor setup. Extensive experimental results
corroborate our theoretical findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05525">Towards Safe Load Balancing based on Control Barrier Functions and Deep Reinforcement Learning. (arXiv:2401.05525v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dinh_L/0/1/0/all/0/1">Lam Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Quang_P/0/1/0/all/0/1">Pham Tran Anh Quang</a>, <a href="http://arxiv.org/find/cs/1/au:+Leguay_J/0/1/0/all/0/1">J&#xe9;r&#xe9;mie Leguay</a></p>
<p>Deep Reinforcement Learning (DRL) algorithms have recently made significant
strides in improving network performance. Nonetheless, their practical use is
still limited in the absence of safe exploration and safe decision-making. In
the context of commercial solutions, reliable and safe-to-operate systems are
of paramount importance. Taking this problem into account, we propose a safe
learning-based load balancing algorithm for Software Defined-Wide Area Network
(SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined with
a Control Barrier Function (CBF). It safely projects unsafe actions into
feasible ones during both training and testing, and it guides learning towards
safe policies. We successfully implemented the solution on GPU to accelerate
training by approximately 110x times and achieve model updates for on-policy
methods within a few seconds, making the solution practical. We show that our
approach delivers near-optimal Quality-of-Service (QoS performance in terms of
end-to-end delay while respecting safety requirements related to link capacity
constraints. We also demonstrated that on-policy learning based on Proximal
Policy Optimization (PPO) performs better than off-policy learning with Deep
Deterministic Policy Gradient (DDPG) when both are combined with a CBF for safe
load balancing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05531">VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition. (arXiv:2401.05531v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1">John Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Orescanin_M/0/1/0/all/0/1">Marko Orescanin</a>, <a href="http://arxiv.org/find/cs/1/au:+Eckstrand_E/0/1/0/all/0/1">Eric Eckstrand</a></p>
<p>Transfer learning (TL) is an increasingly popular approach to training deep
learning (DL) models that leverages the knowledge gained by training a
foundation model on diverse, large-scale datasets for use on downstream tasks
where less domain- or task-specific data is available. The literature is rich
with TL techniques and applications; however, the bulk of the research makes
use of deterministic DL models which are often uncalibrated and lack the
ability to communicate a measure of epistemic (model) uncertainty in
prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models
are often well-calibrated, provide access to epistemic uncertainty for a
prediction, and are capable of achieving competitive predictive performance. In
this study, we propose variational inference pre-trained audio neural networks
(VI-PANNs). VI-PANNs are a variational inference variant of the popular
ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio
event detection dataset. We evaluate the quality of the resulting uncertainty
when transferring knowledge from VI-PANNs to other downstream acoustic
classification tasks using the ESC-50, UrbanSound8K, and DCASE2013 datasets. We
demonstrate, for the first time, that it is possible to transfer calibrated
uncertainty information along with knowledge from upstream tasks to enhance a
model's capability to perform downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05535">Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dorador_A/0/1/0/all/0/1">Albert Dorador</a></p>
<p>Decades after their inception, random forests continue to provide
state-of-the-art accuracy in a variety of learning problems, outperforming in
this respect alternative machine learning algorithms such as decision trees or
even neural networks. However, being an ensemble method, the one aspect where
random forests tend to severely underperform decision trees is
interpretability. In the present work, we propose a post-hoc approach that aims
to have the best of both worlds: the accuracy of random forests and the
interpretability of decision trees. To this end, we present two forest-pruning
methods to find an optimal sub-forest within a given random forest, and then,
when applicable, combine the selected trees into one. Our first method relies
on constrained exhaustive search, while our second method is based on an
adaptation of the LASSO methodology. Extensive experiments over synthetic and
real world datasets show that, in the majority of scenarios, at least one of
the two methods proposed is more accurate than the original random forest,
while just using a small fraction of the trees, aiding result interpretability.
Compared to current state-of-the-art forestpruning methods, namely sequential
forward selection and (a variation of) sequential backward selection, our
methods tend to outperform both of them, whether in terms of accuracy, number
of trees employed, or both.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05538">Multi-objective Feature Selection in Remote Health Monitoring Applications. (arXiv:2401.05538v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Le Ngu Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Casado_C/0/1/0/all/0/1">Constantino &#xc1;lvarez Casado</a>, <a href="http://arxiv.org/find/cs/1/au:+Canellas_M/0/1/0/all/0/1">Manuel Lage Ca&#xf1;ellas</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1">Anirban Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nhi Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayagopi_D/0/1/0/all/0/1">Dinesh Babu Jayagopi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1">Miguel Bordallo L&#xf3;pez</a></p>
<p>Radio frequency (RF) signals have facilitated the development of non-contact
human monitoring tasks, such as vital signs measurement, activity recognition,
and user identification. In some specific scenarios, an RF signal analysis
framework may prioritize the performance of one task over that of others. In
response to this requirement, we employ a multi-objective optimization approach
inspired by biological principles to select discriminative features that
enhance the accuracy of breathing patterns recognition while simultaneously
impeding the identification of individual users. This approach is validated
using a novel vital signs dataset consisting of 50 subjects engaged in four
distinct breathing patterns. Our findings indicate a remarkable result: a
substantial divergence in accuracy between breathing recognition and user
identification. As a complementary viewpoint, we present a contrariwise result
to maximize user identification accuracy and minimize the system's capacity for
breathing activity recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05562">Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning. (arXiv:2401.05562v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhangchen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1">Fengqing Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Luyao Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinyuan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Poovendran_R/0/1/0/all/0/1">Radha Poovendran</a></p>
<p>Federated learning (FL) enables multiple participants to train a global
machine learning model without sharing their private training data.
Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating
the server that aggregates local models from participants and then updates the
global model. However, P2P FL is vulnerable to (i) honest-but-curious
participants whose objective is to infer private training data of other
participants, and (ii) Byzantine participants who can transmit arbitrarily
manipulated local models to corrupt the learning process. P2P FL schemes that
simultaneously guarantee Byzantine resilience and preserve privacy have been
less studied. In this paper, we develop Brave, a protocol that ensures
Byzantine Resilience And privacy-preserving property for P2P FL in the presence
of both types of adversaries. We show that Brave preserves privacy by
establishing that any honest-but-curious adversary cannot infer other
participants' private data by observing their models. We further prove that
Brave is Byzantine-resilient, which guarantees that all benign participants
converge to an identical model that deviates from a global model trained
without Byzantine adversaries by a bounded distance. We evaluate Brave against
three state-of-the-art adversaries on a P2P FL for image classification tasks
on benchmark datasets CIFAR10 and MNIST. Our results show that the global model
learned with Brave in the presence of adversaries achieves comparable
classification accuracy to a global model trained in the absence of any
adversary.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1">Evan Hubinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1">Carson Denison</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1">Jesse Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1">Mike Lambert</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1">Monte MacDiarmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1">Tamera Lanham</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1">Daniel M. Ziegler</a>, <a href="http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1">Tim Maxwell</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1">Newton Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1">Adam Jermyn</a>, <a href="http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1">Amanda Askell</a>, <a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1">Ansh Radhakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1">Cem Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1">David Duvenaud</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1">Deep Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1">Fazl Barez</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1">Jack Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1">Kamal Ndousse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1">Kshitij Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1">Michael Sellitto</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1">Mrinank Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1">Nova DasSarma</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1">Roger Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1">Shauna Kravec</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yuntao Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1">Zachary Witten</a>, <a href="http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1">Marina Favaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a>, <a href="http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1">Holden Karnofsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1">Paul Christiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1">Samuel R. Bowman</a>, <a href="http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1">Logan Graham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1">Jared Kaplan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1">Ryan Greenblatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1">Buck Shlegeris</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1">Nicholas Schiefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1">Ethan Perez</a></p>
<p>Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoored behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoored behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05568">Phase discovery with active learning: Application to structural phase transitions in equiatomic NiTi. (arXiv:2401.05568v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Vandermause_J/0/1/0/all/0/1">Jonathan Vandermause</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Johansson_A/0/1/0/all/0/1">Anders Johansson</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Miao_Y/0/1/0/all/0/1">Yucong Miao</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Vlassak_J/0/1/0/all/0/1">Joost J. Vlassak</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kozinsky_B/0/1/0/all/0/1">Boris Kozinsky</a></p>
<p>Nickel titanium (NiTi) is a protypical shape-memory alloy used in a range of
biomedical and engineering devices, but direct molecular dynamics simulations
of the martensitic B19' -&gt; B2 phase transition driving its shape-memory
behavior are rare and have relied on classical force fields with limited
accuracy. Here, we train four machine-learned force fields for equiatomic NiTi
based on the LDA, PBE, PBEsol, and SCAN DFT functionals. The models are trained
on the fly during NPT molecular dynamics, with DFT calculations and model
updates performed automatically whenever the uncertainty of a local energy
prediction exceeds a chosen threshold. The models achieve accuracies of 1-2
meV/atom during training and are shown to closely track DFT predictions of B2
and B19' elastic constants and phonon frequencies. Surprisingly, in large-scale
molecular dynamics simulations, only the SCAN model predicts a reversible B19'
-&gt; B2 phase transition, with the LDA, PBE, and PBEsol models predicting a
reversible transition to a previously uncharacterized low-volume phase, which
we hypothesize to be a new stable high-pressure phase. We examine the structure
of the new phase and estimate its stability on the temperature-pressure phase
diagram. This work establishes an automated active learning protocol for
studying displacive transformations, reveals important differences between DFT
functionals that can only be detected in large-scale simulations, provides an
accurate force field for NiTi, and identifies a new phase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05569">SENet: Visual Detection of Online Social Engineering Attack Campaigns. (arXiv:2401.05569v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ozen_I/0/1/0/all/0/1">Irfan Ozen</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramani_K/0/1/0/all/0/1">Karthika Subramani</a>, <a href="http://arxiv.org/find/cs/1/au:+Vadrevu_P/0/1/0/all/0/1">Phani Vadrevu</a>, <a href="http://arxiv.org/find/cs/1/au:+Perdisci_R/0/1/0/all/0/1">Roberto Perdisci</a></p>
<p>Social engineering (SE) aims at deceiving users into performing actions that
may compromise their security and privacy. These threats exploit weaknesses in
human's decision making processes by using tactics such as pretext, baiting,
impersonation, etc. On the web, SE attacks include attack classes such as
scareware, tech support scams, survey scams, sweepstakes, etc., which can
result in sensitive data leaks, malware infections, and monetary loss. For
instance, US consumers lose billions of dollars annually due to various SE
attacks. Unfortunately, generic social engineering attacks remain understudied,
compared to other important threats, such as software vulnerabilities and
exploitation, network intrusions, malicious software, and phishing. The few
existing technical studies that focus on social engineering are limited in
scope and mostly focus on measurements rather than developing a generic
defense. To fill this gap, we present SEShield, a framework for in-browser
detection of social engineering attacks. SEShield consists of three main
components: (i) a custom security crawler, called SECrawler, that is dedicated
to scouting the web to collect examples of in-the-wild SE attacks; (ii) SENet,
a deep learning-based image classifier trained on data collected by SECrawler
that aims to detect the often glaring visual traits of SE attack pages; and
(iii) SEGuard, a proof-of-concept extension that embeds SENet into the web
browser and enables real-time SE attack detection. We perform an extensive
evaluation of our system and show that SENet is able to detect new instances of
SE attacks with a detection rate of up to 99.6% at 1% false positive, thus
providing an effective first defense against SE attacks on the web.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05570">Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms. (arXiv:2401.05570v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vorst_K/0/1/0/all/0/1">Kevin Van Vorst</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a></p>
<p>Self-supervised learning has become a popular way to pretrain a deep learning
model and then transfer it to perform downstream tasks. However, most of these
methods are developed on large-scale image datasets that contain natural
objects with clear textures, outlines, and distinct color contrasts. It remains
uncertain whether these methods are equally effective for medical imaging,
where the regions of interest often blend subtly and indistinctly with the
surrounding tissues. In this study, we propose an alternative method that uses
contralateral mammograms to train a neural network to encode similar embeddings
when a pair contains both normal images and different embeddings when a pair
contains normal and abnormal images. Our approach leverages the natural
symmetry of human body as weak labels to learn to distinguish abnormal lesions
from background tissues in a fully unsupervised manner. Our findings suggest
that it's feasible by incorporating soft labels derived from the Euclidean
distances between the embeddings of the image pairs into the Siamese network
loss. Our method demonstrates superior performance in mammogram patch
classification compared to existing self-supervised learning methods. This
approach not only leverages a vast amount of image data effectively but also
minimizes reliance on costly labels, a significant advantage particularly in
the field of medical imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05571">QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum Circuits. (arXiv:2401.05571v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenyu Zhang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1">Hanrui Wang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1">Jiaqi Gu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1">Zirui Li</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1">David Z. Pan</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1">Frederic T. Chong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1">Song Han</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>Parameterized Quantum Circuits (PQC) have obtained increasing popularity
thanks to their great potential for near-term Noisy Intermediate-Scale Quantum
(NISQ) computers. Achieving quantum advantages usually requires a large number
of qubits and quantum circuits with enough capacity. However, limited coherence
time and massive quantum noises severely constrain the size of quantum circuits
that can be executed reliably on real machines. To address these two pain
points, we propose QuantumSEA, an in-time sparse exploration for noise-adaptive
quantum circuits, aiming to achieve two key objectives: (1) implicit circuits
capacity during training - by dynamically exploring the circuit's sparse
connectivity and sticking a fixed small number of quantum gates throughout the
training which satisfies the coherence time and enjoy light noises, enabling
feasible executions on real quantum devices; (2) noise robustness - by jointly
optimizing the topology and parameters of quantum circuits under real device
noise models. In each update step of sparsity, we leverage the moving average
of historical gradients to grow necessary gates and utilize salience-based
pruning to eliminate insignificant gates. Extensive experiments are conducted
with 7 Quantum Machine Learning (QML) and Variational Quantum Eigensolver (VQE)
benchmarks on 6 simulated or real quantum computers, where QuantumSEA
consistently surpasses noise-aware search, human-designed, and randomly
generated quantum circuit baselines by a clear performance margin. For example,
even in the most challenging on-chip training regime, our method establishes
state-of-the-art results with only half the number of quantum gates and ~2x
time saving of circuit executions. Codes are available at
https://github.com/VITA-Group/QuantumSEA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05572">Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qin Yang</a></p>
<p>Innate values describe agents' intrinsic motivations, which reflect their
inherent interests and preferences to pursue goals and drive them to develop
diverse skills satisfying their various needs. The essence of reinforcement
learning (RL) is learning from interaction based on reward-driven (such as
utilities) behaviors, much like natural agents. It is an excellent model to
describe the innate-values-driven (IV) behaviors of AI agents. Especially in
multi-agent systems (MAS), building the awareness of AI agents to balance the
group utilities and system costs and satisfy group members' needs in their
cooperation is a crucial problem for individuals learning to support their
community and integrate human society in the long term. This paper proposes a
hierarchical compound intrinsic value reinforcement learning model --
innate-values-driven reinforcement learning termed IVRL to describe the complex
behaviors of multi-agent interaction in their cooperation. We implement the
IVRL architecture in the StarCraft Multi-Agent Challenge (SMAC) environment and
compare the cooperative performance within three characteristics of innate
value agents (Coward, Neutral, and Reckless) through three benchmark
multi-agent RL algorithms: QMIX, IQL, and QTRAN. The results demonstrate that
by organizing individual various needs rationally, the group can achieve better
performance with lower costs effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05578">Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1">Zhenya Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">David Li</a></p>
<p>We introduce a rapid and precise analytical approach for analyzing cerebral
blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the
application of the Extreme Learning Machine (ELM). Our evaluation of ELM and
existing algorithms involves a comprehensive set of metrics. We assess these
algorithms using synthetic datasets for both semi-infinite and multi-layer
models. The results demonstrate that ELM consistently achieves higher fidelity
across various noise levels and optical parameters, showcasing robust
generalization ability and outperforming iterative fitting algorithms. Through
a comparison with a computationally efficient neural network, ELM attains
comparable accuracy with reduced training and inference times. Notably, the
absence of a back-propagation process in ELM during training results in
significantly faster training speeds compared to existing neural network
approaches. This proposed strategy holds promise for edge computing
applications with online training capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05579">An Augmented Surprise-guided Sequential Learning Framework for Predicting the Melt Pool Geometry. (arXiv:2401.05579v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raihan_A/0/1/0/all/0/1">Ahmed Shoyeb Raihan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khosravi_H/0/1/0/all/0/1">Hamed Khosravi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhuiyan_T/0/1/0/all/0/1">Tanveer Hossain Bhuiyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1">Imtiaz Ahmed</a></p>
<p>Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry,
offering benefits like intricate design, minimal waste, rapid prototyping,
material versatility, and customized solutions. However, its full industry
adoption faces hurdles, particularly in achieving consistent product quality. A
crucial aspect for MAM's success is understanding the relationship between
process parameters and melt pool characteristics. Integrating Artificial
Intelligence (AI) into MAM is essential. Traditional machine learning (ML)
methods, while effective, depend on large datasets to capture complex
relationships, a significant challenge in MAM due to the extensive time and
resources required for dataset creation. Our study introduces a novel
surprise-guided sequential learning framework, SurpriseAF-BO, signaling a
significant shift in MAM. This framework uses an iterative, adaptive learning
process, modeling the dynamics between process parameters and melt pool
characteristics with limited data, a key benefit in MAM's cyber manufacturing
context. Compared to traditional ML models, our sequential learning method
shows enhanced predictive accuracy for melt pool dimensions. Further improving
our approach, we integrated a Conditional Tabular Generative Adversarial
Network (CTGAN) into our framework, forming the CT-SurpriseAF-BO. This produces
synthetic data resembling real experimental data, improving learning
effectiveness. This enhancement boosts predictive precision without requiring
additional physical experiments. Our study demonstrates the power of advanced
data-driven techniques in cyber manufacturing and the substantial impact of
sequential AI and ML, particularly in overcoming MAM's traditional challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05580">Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xingda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">David Li</a></p>
<p>Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique
that measures the tissue blood flow, by using near-infrared coherent
point-source illumination to detect spectral changes. While machine learning
has demonstrated significant potential for measuring blood flow index (BFi), an
open question concerning the success of this approach pertains to its
robustness in scenarios involving deviations between datasets with varying
Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications
and various setups. This study proposes a transfer learning approach, aims to
assess the influence of SNRs on the generalization ability of learned features,
and demonstrate the robustness for transfer learning. A synthetic dataset with
varying levels of added noise is utilized to simulate different SNRs. The
proposed network takes a 1x64 autocorrelation curve as input and generates BFi
and the correlation parameter beta. The proposed model demonstrates excellent
performance across different SNRs, exhibiting enhanced fitting accuracy,
particularly for low SNR datasets when compared with other fitting methods.
This highlights its potential for clinical diagnosis and treatment across
various scenarios under different clinical setups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05605">Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalajdzievski_D/0/1/0/all/0/1">Damjan Kalajdzievski</a></p>
<p>We study and quantify the problem of forgetting when fine-tuning pre-trained
large language models (LLMs) on a downstream task. We find that
parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters
(LoRA), still suffer from catastrophic forgetting. In particular, we identify a
strong inverse linear relationship between the fine-tuning performance and the
amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise
scaling laws that show forgetting increases as a shifted power law in the
number of parameters fine-tuned and the number of update steps. We also examine
the impact of forgetting on knowledge, reasoning, and the safety guardrails
trained into Llama 2 7B chat. Our study suggests that forgetting cannot be
avoided through early stopping or by varying the number of parameters
fine-tuned. We believe this opens up an important safety-critical direction for
future research to evaluate and develop fine-tuning schemes which mitigate
forgetting
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05610">Graph Q-Learning for Combinatorial Optimization. (arXiv:2401.05610v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dax_V/0/1/0/all/0/1">Victoria M. Dax</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Leahy_K/0/1/0/all/0/1">Kevin Leahy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a></p>
<p>Graph-structured data is ubiquitous throughout natural and social sciences,
and Graph Neural Networks (GNNs) have recently been shown to be effective at
solving prediction and inference problems on graph data. In this paper, we
propose and demonstrate that GNNs can be applied to solve Combinatorial
Optimization (CO) problems. CO concerns optimizing a function over a discrete
solution space that is often intractably large. To learn to solve CO problems,
we formulate the optimization process as a sequential decision making problem,
where the return is related to how close the candidate solution is to
optimality. We use a GNN to learn a policy to iteratively build increasingly
promising candidate solutions. We present preliminary evidence that GNNs
trained through Q-Learning can solve CO problems with performance approaching
state-of-the-art heuristic-based solvers, using only a fraction of the
parameters and training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05629">Learning Performance-Oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation. (arXiv:2401.05629v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shaoru Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazlyab_M/0/1/0/all/0/1">Mahyar Fazlyab</a></p>
<p>Control Barrier Functions (CBFs) provide an elegant framework for designing
safety filters for nonlinear control systems by constraining their trajectories
to an invariant subset of a prespecified safe set. However, the task of finding
a CBF that concurrently maximizes the volume of the resulting control invariant
set while accommodating complex safety constraints, particularly in high
relative degree systems with actuation constraints, continues to pose a
substantial challenge. In this work, we propose a novel self-supervised
learning framework that holistically addresses these hurdles. Given a Boolean
composition of multiple state constraints that define the safe set, our
approach starts with building a single continuously differentiable function
whose 0-superlevel set provides an inner approximation of the safe set. We then
use this function together with a smooth neural network to parameterize the CBF
candidate. Finally, we design a training loss function based on a
Hamilton-Jacobi partial differential equation to train the CBF while enlarging
the volume of the induced control invariant set. We demonstrate the
effectiveness of our approach via numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05641">When eBPF Meets Machine Learning: On-the-fly OS Kernel Compartmentalization. (arXiv:2401.05641v1 [cs.OS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zicheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tiejin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1">Qinrun Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yueqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Hua Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1">Qingkai Zeng</a></p>
<p>Compartmentalization effectively prevents initial corruption from turning
into a successful attack. This paper presents O2C, a pioneering system designed
to enforce OS kernel compartmentalization on the fly. It not only provides
immediate remediation for sudden threats but also maintains consistent system
availability through the enforcement process.
</p>
<p>O2C is empowered by the newest advancements of the eBPF ecosystem which
allows to instrument eBPF programs that perform enforcement actions into the
kernel at runtime. O2C takes the lead in embedding a machine learning model
into eBPF programs, addressing unique challenges in on-the-fly
compartmentalization. Our comprehensive evaluation shows that O2C effectively
confines damage within the compartment. Further, we validate that decision tree
is optimally suited for O2C owing to its advantages in processing tabular data,
its explainable nature, and its compliance with the eBPF ecosystem. Last but
not least, O2C is lightweight, showing negligible overhead and excellent
sacalability system-wide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05653">Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression. (arXiv:2401.05653v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Sean Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Musunuru_S/0/1/0/all/0/1">Sriya Musunuru</a>, <a href="http://arxiv.org/find/cs/1/au:+Zong_B/0/1/0/all/0/1">Baoshi Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Thornton_B/0/1/0/all/0/1">Brooks Thornton</a></p>
<p>This paper explores the application of Shapley Value Regression in dissecting
marketing performance at channel-partner level, complementing channel-level
Marketing Mix Modeling (MMM). Utilizing real-world data from the financial
services industry, we demonstrate the practicality of Shapley Value Regression
in evaluating individual partner contributions. Although structured in-field
testing along with cooperative game theory is most accurate, it can often be
highly complex and expensive to conduct. Shapley Value Regression is thus a
more feasible approach to disentangle the influence of each marketing partner
within a marketing channel. We also propose a simple method to derive adjusted
coefficients of Shapley Value Regression and compares it with alternative
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05654">Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Tao Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1">Anil Palepu</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1">Mike Schaekermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1">Khaled Saab</a>, <a href="http://arxiv.org/find/cs/1/au:+Freyberg_J/0/1/0/all/0/1">Jan Freyberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1">Ryutaro Tanno</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Amy Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Brenna Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1">Mohamed Amin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1">Nenad Tomasev</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1">Shekoofeh Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1">Karan Singhal</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Le Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1">Albert Webson</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1">Kavita Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1">S Sara Mahdavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1">Christopher Semturs</a>, <a href="http://arxiv.org/find/cs/1/au:+Gottweis_J/0/1/0/all/0/1">Juraj Gottweis</a>, <a href="http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1">Joelle Barral</a>, <a href="http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1">Katherine Chou</a>, <a href="http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1">Greg S Corrado</a>, <a href="http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1">Yossi Matias</a>, <a href="http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1">Alan Karthikesalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1">Vivek Natarajan</a></p>
<p>At the heart of medicine lies the physician-patient dialogue, where skillful
history-taking paves the way for accurate diagnosis, effective management, and
enduring trust. Artificial Intelligence (AI) systems capable of diagnostic
dialogue could increase accessibility, consistency, and quality of care.
However, approximating clinicians' expertise is an outstanding grand challenge.
Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large
Language Model (LLM) based AI system optimized for diagnostic dialogue.
</p>
<p>AMIE uses a novel self-play based simulated environment with automated
feedback mechanisms for scaling learning across diverse disease conditions,
specialties, and contexts. We designed a framework for evaluating
clinically-meaningful axes of performance including history-taking, diagnostic
accuracy, management reasoning, communication skills, and empathy. We compared
AMIE's performance to that of primary care physicians (PCPs) in a randomized,
double-blind crossover study of text-based consultations with validated patient
actors in the style of an Objective Structured Clinical Examination (OSCE). The
study included 149 case scenarios from clinical providers in Canada, the UK,
and India, 20 PCPs for comparison with AMIE, and evaluations by specialist
physicians and patient actors. AMIE demonstrated greater diagnostic accuracy
and superior performance on 28 of 32 axes according to specialist physicians
and 24 of 26 axes according to patient actors. Our research has several
limitations and should be interpreted with appropriate caution. Clinicians were
limited to unfamiliar synchronous text-chat which permits large-scale
LLM-patient interactions but is not representative of usual clinical practice.
While further research is required before AMIE could be translated to
real-world settings, the results represent a milestone towards conversational
diagnostic AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05664">Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow. (arXiv:2401.05664v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jian Ma</a></p>
<p>Energy efficiency is a big concern in industrial sectors. Finding the root
cause of anomaly state of energy efficiency can help to improve energy
efficiency of industrial systems and therefore save energy cost. In this
research, we propose to use transfer entropy (TE) for root cause analysis on
energy efficiency of industrial systems. A method, called TE flow, is proposed
in that a TE flow from physical measurements of each subsystem to the energy
efficiency indicator along timeline is considered as causal strength for
diagnosing root cause of anomaly states of energy efficiency of a system. The
copula entropy-based nonparametric TE estimator is used in the proposed method.
We conducted experiments on real data collected from a compressing air system
to verify the proposed method. Experimental results show that the TE flow
method successfully identified the root cause of the energy (in)efficiency of
the system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05667">EsaCL: Efficient Continual Learning of Sparse Models. (arXiv:2401.05667v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Weijieying Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1">Vasant G Honavar</a></p>
<p>A key challenge in the continual learning setting is to efficiently learn a
sequence of tasks without forgetting how to perform previously learned tasks.
Many existing approaches to this problem work by either retraining the model on
previous tasks or by expanding the model to accommodate new tasks. However,
these approaches typically suffer from increased storage and computational
requirements, a problem that is worsened in the case of sparse models due to
need for expensive re-training after sparsification. To address this challenge,
we propose a new method for efficient continual learning of sparse models
(EsaCL) that can automatically prune redundant parameters without adversely
impacting the model's predictive power, and circumvent the need of retraining.
We conduct a theoretical analysis of loss landscapes with parameter pruning,
and design a directional pruning (SDP) strategy that is informed by the
sharpness of the loss function with respect to the model parameters. SDP
ensures model with minimal loss of predictive accuracy, accelerating the
learning of sparse models at each stage. To accelerate model update, we
introduce an intelligent data selection (IDS) strategy that can identify
critical instances for estimating loss landscape, yielding substantially
improved data efficiency. The results of our experiments show that EsaCL
achieves performance that is competitive with the state-of-the-art methods on
three continual learning benchmarks, while using substantially reduced memory
and computational resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05680">Use of Graph Neural Networks in Aiding Defensive Cyber Operations. (arXiv:2401.05680v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1">Shaswata Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1">Trisha Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Neupane_S/0/1/0/all/0/1">Subash Neupane</a>, <a href="http://arxiv.org/find/cs/1/au:+Piplai_A/0/1/0/all/0/1">Aritran Piplai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1">Sudip Mittal</a></p>
<p>In an increasingly interconnected world, where information is the lifeblood
of modern society, regular cyber-attacks sabotage the confidentiality,
integrity, and availability of digital systems and information. Additionally,
cyber-attacks differ depending on the objective and evolve rapidly to disguise
defensive systems. However, a typical cyber-attack demonstrates a series of
stages from attack initiation to final resolution, called an attack life cycle.
These diverse characteristics and the relentless evolution of cyber attacks
have led cyber defense to adopt modern approaches like Machine Learning to
bolster defensive measures and break the attack life cycle. Among the adopted
ML approaches, Graph Neural Networks have emerged as a promising approach for
enhancing the effectiveness of defensive measures due to their ability to
process and learn from heterogeneous cyber threat data. In this paper, we look
into the application of GNNs in aiding to break each stage of one of the most
renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address
each phase of CKC and discuss how GNNs contribute to preparing and preventing
an attack from a defensive standpoint. Furthermore, We also discuss open
research areas and further improvement scopes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05710">The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning. (arXiv:2401.05710v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhihui Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrault_A/0/1/0/all/0/1">Andrew Perrault</a></p>
<p>We study reinforcement learning in the presence of an unknown reward
perturbation. Existing methodologies for this problem make strong assumptions
including reward smoothness, known perturbations, and/or perturbations that do
not modify the optimal policy. We study the case of unknown arbitrary
perturbations that discretize and shuffle reward space, but have the property
that the true reward belongs to the most frequently observed class after
perturbation. This class of perturbations generalizes existing classes (and, in
the limit, all continuous bounded perturbations) and defeats existing methods.
We introduce an adaptive distributional reward critic and show theoretically
that it can recover the true rewards under technical conditions. Under the
targeted perturbation in discrete and continuous control tasks, we win/tie the
highest return in 40/57 settings (compared to 16/57 for the best baseline).
Even under the untargeted perturbation, we still win an edge over the baseline
designed especially for that setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05711">Dynamic Indoor Fingerprinting Localization based on Few-Shot Meta-Learning with CSI Images. (arXiv:2401.05711v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jiyu Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaojun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chenpei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuhua Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yizhuo Zhang</a></p>
<p>While fingerprinting localization is favored for its effectiveness, it is
hindered by high data acquisition costs and the inaccuracy of static
database-based estimates. Addressing these issues, this letter presents an
innovative indoor localization method using a data-efficient meta-learning
algorithm. This approach, grounded in the ``Learning to Learn'' paradigm of
meta-learning, utilizes historical localization tasks to improve adaptability
and learning efficiency in dynamic indoor environments. We introduce a
task-weighted loss to enhance knowledge transfer within this framework. Our
comprehensive experiments confirm the method's robustness and superiority over
current benchmarks, achieving a notable 23.13\% average gain in Mean Euclidean
Distance, particularly effective in scenarios with limited CSI data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05716">Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization. (arXiv:2401.05716v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Scarlett_J/0/1/0/all/0/1">Jonathan Scarlett</a></p>
<p>In this paper, we study the problem of estimating the normalizing constant
$\int e^{-\lambda f(x)}dx$ through queries to the black-box function $f$, where
$f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\lambda$ is a
problem parameter. We show that to estimate the normalizing constant within a
small relative error, the level of difficulty depends on the value of
$\lambda$: When $\lambda$ approaches zero, the problem is similar to Bayesian
quadrature (BQ), while when $\lambda$ approaches infinity, the problem is
similar to Bayesian optimization (BO). More generally, the problem varies
between BQ and BO. We find that this pattern holds true even when the function
evaluations are noisy, bringing new aspects to this topic. Our findings are
supported by both algorithm-independent lower bounds and algorithmic upper
bounds, as well as simulation studies conducted on a variety of benchmark
functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05717">Segment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition. (arXiv:2401.05717v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Salvi_G/0/1/0/all/0/1">Giampiero Salvi</a></p>
<p>This article investigates the possibility to use the class entropy of the
output of a connectionist phoneme recogniser to predict time boundaries between
phonetic classes. The rationale is that the value of the entropy should
increase in proximity of a transition between two segments that are well
modelled (known) by the recognition network since it is a measure of
uncertainty. The advantage of this measure is its simplicity as the posterior
probabilities of each class are available in connectionist phoneme recognition.
The entropy and a number of measures based on differentiation of the entropy
are used in isolation and in combination. The decision methods for predicting
the boundaries range from simple thresholds to neural network based procedure.
The different methods are compared with respect to their precision, measured in
terms of the ratio between the number C of predicted boundaries within 10 or 20
msec of the reference and the total number of predicted boundaries, and recall,
measured as the ratio between C and the total number of reference boundaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05735">Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kahatapitiya_K/0/1/0/all/0/1">Kumara Kahatapitiya</a>, <a href="http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1">Adil Karjauv</a>, <a href="http://arxiv.org/find/cs/1/au:+Abati_D/0/1/0/all/0/1">Davide Abati</a>, <a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1">Fatih Porikli</a>, <a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1">Yuki M. Asano</a>, <a href="http://arxiv.org/find/cs/1/au:+Habibian_A/0/1/0/all/0/1">Amirhossein Habibian</a></p>
<p>Diffusion-based video editing have reached impressive quality and can
transform either the global style, local structure, and attributes of given
video inputs, following textual edit prompts. However, such solutions typically
incur heavy memory and computational costs to generate temporally-coherent
frames, either in the form of diffusion inversion and/or cross-frame attention.
In this paper, we conduct an analysis of such inefficiencies, and suggest
simple yet effective modifications that allow significant speed-ups whilst
maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as
OCD, to further reduce latency by allocating computations more towards
foreground edited regions that are arguably more important for perceptual
quality. We achieve this by two novel proposals: i) Object-Centric Sampling,
decoupling the diffusion steps spent on salient regions or background,
allocating most of the model capacity to the former, and ii) Object-Centric 3D
Token Merging, which reduces cost of cross-frame attention by fusing redundant
tokens in unimportant background regions. Both techniques are readily
applicable to a given video editing model \textit{without} retraining, and can
drastically reduce its memory and computational cost. We evaluate our proposals
on inversion-based and control-signal-based editing pipelines, and show a
latency reduction up to 10x for a comparable synthesis quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05737">An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control. (arXiv:2401.05737v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manjavacas_A/0/1/0/all/0/1">Antonio Manjavacas</a>, <a href="http://arxiv.org/find/cs/1/au:+Campoy_Nieves_A/0/1/0/all/0/1">Alejandro Campoy-Nieves</a>, <a href="http://arxiv.org/find/cs/1/au:+Jimenez_Raboso_J/0/1/0/all/0/1">Javier Jim&#xe9;nez-Raboso</a>, <a href="http://arxiv.org/find/cs/1/au:+Molina_Solana_M/0/1/0/all/0/1">Miguel Molina-Solana</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_Romero_J/0/1/0/all/0/1">Juan G&#xf3;mez-Romero</a></p>
<p>Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver
of energy consumption in commercial and residential buildings. Recent studies
have shown that Deep Reinforcement Learning (DRL) algorithms can outperform
traditional reactive controllers. However, DRL-based solutions are generally
designed for ad hoc setups and lack standardization for comparison. To fill
this gap, this paper provides a critical and reproducible evaluation, in terms
of comfort and energy consumption, of several state-of-the-art DRL algorithms
for HVAC control. The study examines the controllers' robustness, adaptability,
and trade-off between optimization goals by using the Sinergym framework. The
results obtained confirm the potential of DRL algorithms, such as SAC and TD3,
in complex scenarios and reveal several challenges related to generalization
and incremental learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05765">Feature Selection for Functional Data Classification. (arXiv:2401.05765v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Boschi_T/0/1/0/all/0/1">Tobia Boschi</a>, <a href="http://arxiv.org/find/stat/1/au:+Bonin_F/0/1/0/all/0/1">Francesca Bonin</a>, <a href="http://arxiv.org/find/stat/1/au:+Epperlein_J/0/1/0/all/0/1">Jonathan Epperlein</a>, <a href="http://arxiv.org/find/stat/1/au:+Ordonez_Hurtado_R/0/1/0/all/0/1">Rodrigo Ordonez-Hurtado</a>, <a href="http://arxiv.org/find/stat/1/au:+Pascale_A/0/1/0/all/0/1">Alessandra Pascale</a></p>
<p>Functional data analysis has emerged as a crucial tool in many contemporary
scientific domains that require the integration and interpretation of complex
data. Moreover, the advent of new technologies has facilitated the collection
of a large number of longitudinal variables, making feature selection pivotal
for avoiding overfitting and improving prediction performance. This paper
introduces a novel methodology called FSFC (Feature Selection for Functional
Classification), that addresses the challenge of jointly performing feature
selection and classification of functional data in scenarios with categorical
responses and longitudinal features. Our approach tackles a newly defined
optimization problem that integrates logistic loss and functional features to
identify the most crucial features for classification. To address the
minimization procedure, we employ functional principal components and develop a
new adaptive version of the Dual Augmented Lagrangian algorithm that leverages
the sparsity structure of the problem for dimensionality reduction. The
computational efficiency of FSFC enables handling high-dimensional scenarios
where the number of features may considerably exceed the number of statistical
units. Simulation experiments demonstrate that FSFC outperforms other machine
learning and deep learning methods in computational time and classification
accuracy. Furthermore, the FSFC feature selection capability can be leveraged
to significantly reduce the problem's dimensionality and enhance the
performances of other classification algorithms. The efficacy of FSFC is also
demonstrated through a real data application, analyzing relationships between
four chronic diseases and other health and socio-demographic factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05772">Knowledge Translation: A New Pathway for Model Compression. (arXiv:2401.05772v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wujie Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Defang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiawei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Can Wang</a></p>
<p>Deep learning has witnessed significant advancements in recent years at the
cost of increasing training, inference, and model storage overhead. While
existing model compression methods strive to reduce the number of model
parameters while maintaining high accuracy, they inevitably necessitate the
re-training of the compressed model or impose architectural constraints. To
overcome these limitations, this paper presents a novel framework, termed
\textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model
is trained to receive the parameters of a larger model and generate compressed
parameters. The concept of KT draws inspiration from language translation,
which effectively employs neural networks to convert different languages,
maintaining identical meaning. Accordingly, we explore the potential of neural
networks to convert models of disparate sizes, while preserving their
functionality. We propose a comprehensive framework for KT, introduce data
augmentation strategies to enhance model performance despite restricted
training data, and successfully demonstrate the feasibility of KT on the MNIST
dataset. Code is available at \url{https://github.com/zju-SWJ/KT}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05792">Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhihui Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Handong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1">Tong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuai Li</a></p>
<p>Large pretrained multilingual language models (ML-LMs) have shown remarkable
capabilities of zero-shot cross-lingual transfer, without direct cross-lingual
supervision. While these results are promising, follow-up works found that,
within the multilingual embedding spaces, there exists strong language identity
information which hinders the expression of linguistic factors shared across
languages. For semantic tasks like cross-lingual sentence retrieval, it is
desired to remove such language identity signals to fully leverage semantic
information. In this work, we provide a novel view of projecting away
language-specific factors from a multilingual embedding space. Specifically, we
discover that there exists a low-rank subspace that primarily encodes
information irrelevant to semantics (e.g., syntactic information). To identify
this subspace, we present a simple but effective unsupervised method based on
singular value decomposition with multiple monolingual corpora as input. Once
the subspace is found, we can directly project the original embeddings into the
null space to boost language agnosticism without finetuning. We systematically
evaluate our method on various tasks including the challenging
language-agnostic QA retrieval task. Empirical results show that applying our
method consistently leads to improvements over commonly used ML-LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05794">Bounds on the price of feedback for mistake-bounded online learning. (arXiv:2401.05794v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geneson_J/0/1/0/all/0/1">Jesse Geneson</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Linus Tang</a></p>
<p>We improve several worst-case bounds for various online learning scenarios
from (Auer and Long, Machine Learning, 1999). In particular, we sharpen an
upper bound for delayed ambiguous reinforcement learning by a factor of 2, an
upper bound for learning compositions of families of functions by a factor of
2.41, and an upper bound for agnostic learning by a factor of 1.09. We also
improve a lower bound from the same paper for learning compositions of $k$
families of functions by a factor of $\Theta(\ln{k})$, matching the upper bound
up to a constant factor. In addition, we solve a problem from (Long,
Theoretical Computer Science, 2020) on the price of bandit feedback with
respect to standard feedback for multiclass learning, and we improve an upper
bound from (Feng et al., Theoretical Computer Science, 2023) on the price of
$r$-input delayed ambiguous reinforcement learning by a factor of $r$, matching
a lower bound from the same paper up to the leading term.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05800">Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values. (arXiv:2401.05800v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1">Huan Yee Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_L/0/1/0/all/0/1">Lianhua Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haishuai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1">Khoa T. Phan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Ping Phoebe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wei Xiang</a></p>
<p>The detection of anomalies in multivariate time series data is crucial for
various practical applications, including smart power grids, traffic flow
forecasting, and industrial process control. However, real-world time series
data is usually not well-structured, posting significant challenges to existing
approaches: (1) The existence of missing values in multivariate time series
data along variable and time dimensions hinders the effective modeling of
interwoven spatial and temporal dependencies, resulting in important patterns
being overlooked during model training; (2) Anomaly scoring with
irregularly-sampled observations is less explored, making it difficult to use
existing detectors for multivariate series without fully-observed values. In
this work, we introduce a novel framework called GST-Pro, which utilizes a
graph spatiotemporal process and anomaly scorer to tackle the aforementioned
challenges in detecting anomalies on irregularly-sampled multivariate time
series. Our approach comprises two main components. First, we propose a graph
spatiotemporal process based on neural controlled differential equations. This
process enables effective modeling of multivariate time series from both
spatial and temporal perspectives, even when the data contains missing values.
Second, we present a novel distribution-based anomaly scoring mechanism that
alleviates the reliance on complete uniform observations. By analyzing the
predictions of the graph spatiotemporal process, our approach allows anomalies
to be easily detected. Our experimental results show that the GST-Pro method
can effectively detect anomalies in time series data and outperforms
state-of-the-art methods, regardless of whether there are missing values
present in the data. Our code is available: https://github.com/huankoh/GST-Pro.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05815">Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations. (arXiv:2401.05815v1 [physics.acc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Kaiser_J/0/1/0/all/0/1">Jan Kaiser</a>, <a href="http://arxiv.org/find/physics/1/au:+Xu_C/0/1/0/all/0/1">Chenran Xu</a>, <a href="http://arxiv.org/find/physics/1/au:+Eichler_A/0/1/0/all/0/1">Annika Eichler</a>, <a href="http://arxiv.org/find/physics/1/au:+Garcia_A/0/1/0/all/0/1">Andrea Santamaria Garcia</a></p>
<p>Machine learning has emerged as a powerful solution to the modern challenges
in accelerator physics. However, the limited availability of beam time, the
computational cost of simulations, and the high-dimensionality of optimisation
problems pose significant challenges in generating the required data for
training state-of-the-art machine learning models. In this work, we introduce
Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code.
Cheetah enables the fast collection of large data sets by reducing computation
times by multiple orders of magnitude and facilitates efficient gradient-based
optimisation for accelerator tuning and system identification. This positions
Cheetah as a user-friendly, readily extensible tool that integrates seamlessly
with widely adopted machine learning tools. We showcase the utility of Cheetah
through five examples, including reinforcement learning training,
gradient-based beamline tuning, gradient-based system identification,
physics-informed Bayesian optimisation priors, and modular neural network
surrogate modelling of space charge effects. The use of such a high-speed
differentiable simulation code will simplify the development of machine
learning-based methods for particle accelerators and fast-track their
integration into everyday operations of accelerator facilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05819">TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial Attention Decoding with a Short Decision Window. (arXiv:2401.05819v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ding_Y/0/1/0/all/0/1">Yuting Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_F/0/1/0/all/0/1">Fei Chen</a></p>
<p>Auditory spatial attention detection (ASAD) is used to determine the
direction of a listener's attention to a speaker by analyzing her/his
electroencephalographic (EEG) signals. This study aimed to further improve the
performance of ASAD with a short decision window (i.e., &lt;1 s) rather than with
long decision windows in previous studies. An end-to-end temporal attention
network (i.e., TAnet) was introduced in this work. TAnet employs a multi-head
attention (MHA) mechanism, which can more effectively capture the interactions
among time steps in collected EEG signals and efficiently assign corresponding
weights to those EEG time steps. Experiments demonstrated that, compared with
the CNN-based method and recent ASAD methods, TAnet provided improved decoding
performance in the KUL dataset, with decoding accuracies of 92.4% (decision
window 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s)
with short decision windows (i.e., &lt;1 s). As a new ASAD model with a short
decision window, TAnet can potentially facilitate the design of EEG-controlled
intelligent hearing aids and sound recognition systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05820">Implications of Noise in Resistive Memory on Deep Neural Networks for Image Classification. (arXiv:2401.05820v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emonds_Y/0/1/0/all/0/1">Yannick Emonds</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_K/0/1/0/all/0/1">Kai Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Froning_H/0/1/0/all/0/1">Holger Fr&#xf6;ning</a></p>
<p>Resistive memory is a promising alternative to SRAM, but is also an
inherently unstable device that requires substantial effort to ensure correct
read and write operations. To avoid the associated costs in terms of area, time
and energy, the present work is concerned with exploring how much noise in
memory operations can be tolerated by image classification tasks based on
neural networks. We introduce a special noisy operator that mimics the noise in
an exemplary resistive memory unit, explore the resilience of convolutional
neural networks on the CIFAR-10 classification task, and discuss a couple of
countermeasures to improve this resilience.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05821">Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1">Quentin Delfosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sztwiertnia_S/0/1/0/all/0/1">Sebastian Sztwiertnia</a>, <a href="http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1">Wolfgang Stammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rothermel_M/0/1/0/all/0/1">Mark Rothermel</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>Reward sparsity, difficult credit assignment, and misalignment are only a few
of the many issues that make it difficult, if not impossible, for deep
reinforcement learning (RL) agents to learn optimal policies. Unfortunately,
the black-box nature of deep networks impedes the inclusion of domain experts
who could interpret the model and correct wrong behavior. To this end, we
introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole
decision pipeline transparent via the integration of consecutive concept
bottleneck layers. SCoBots make use of not only relevant object properties but
also of relational concepts. Our experimental results provide strong evidence
that SCoBots allow domain experts to efficiently understand and regularize
their behavior, resulting in potentially better human-aligned RL. In this way,
SCoBots enabled us to identify a misalignment problem in the most simple and
iconic video game, Pong, and resolve it.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05831">Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vardakas_G/0/1/0/all/0/1">Georgios Vardakas</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlopoulos_J/0/1/0/all/0/1">John Pavlopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Likas_A/0/1/0/all/0/1">Aristidis Likas</a></p>
<p>Silhouette coefficient is an established internal clustering evaluation
measure that produces a score per data point, assessing the quality of its
clustering assignment. To assess the quality of the clustering of the whole
dataset, the scores of all the points in the dataset are typically averaged
into a single value, a strategy which we call as micro-averaging. As we
illustrate in this work, by using a synthetic example, this micro-averaging
strategy is sensitive both to cluster imbalance and outliers (background
noise). To address these issues, we propose an alternative aggregation
strategy, which first averages the silhouette scores at a cluster level and
then (macro) averages the scores across the clusters. Based on the same
synthetic example, we show that the proposed macro-averaged silhouette score is
robust to cluster imbalance and background noise. We have conducted an
experimental study showing that our macro-averaged variant provides better
estimates of the ground truth number of clusters on several cases compared to
the typical micro-averaged score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05848">Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials. (arXiv:2401.05848v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Riebesell_J/0/1/0/all/0/1">Janosh Riebesell</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Surta_T/0/1/0/all/0/1">T. Wesley Surta</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Goodall_R/0/1/0/all/0/1">Rhys Goodall</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gaultois_M/0/1/0/all/0/1">Michael Gaultois</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lee_A/0/1/0/all/0/1">Alpha A Lee</a></p>
<p>Materials with high-dielectric constant easily polarize under external
electric fields, allowing them to perform essential functions in many modern
electronic devices. Their practical utility is determined by two conflicting
properties: high dielectric constants tend to occur in materials with narrow
band gaps, limiting the operating voltage before dielectric breakdown. We
present a high-throughput workflow that combines element substitution, ML
pre-screening, ab initio simulation and human expert intuition to efficiently
explore the vast space of unknown materials for potential dielectrics, leading
to the synthesis and characterization of two novel dielectric materials,
CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective
optimization setting with concave Pareto front. While usually considered more
challenging than single-objective optimization, we argue and show preliminary
evidence that the $1/x$-correlation between band gap and permittivity in fact
makes the task more amenable to ML methods by allowing separate models for band
gap and permittivity to each operate in regions of good training support while
still predicting materials of exceptional merit. To our knowledge, this is the
first instance of successful ML-guided multi-objective materials optimization
achieving experimental synthesis and characterization. CsTaTeO6 is a structure
generated via element substitution not present in our reference data sources,
thus exemplifying successful de-novo materials design. Meanwhile, we report the
first high-purity synthesis and dielectric characterization of Bi2Zr2O7 with a
band gap of 2.27 eV and a permittivity of 20.5, meeting all target metrics of
our multi-objective search.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05849">Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Litian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Molhoek_J/0/1/0/all/0/1">Jord Molhoek</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jing Zhou</a></p>
<p>Humans have good natural intuition to recognize when another person has
something to say. It would be interesting if an AI can also recognize
intentions to speak. Especially in scenarios when an AI is guiding a group
discussion, this can be a useful skill. This work studies the inference of
successful and unsuccessful intentions to speak from accelerometer data. This
is chosen because it is privacy-preserving and feasible for in-the-wild
settings since it can be placed in a smart badge. Data from a real-life social
networking event is used to train a machine-learning model that aims to infer
intentions to speak. A subset of unsuccessful intention-to-speak cases in the
data is annotated. The model is trained on the successful intentions to speak
and evaluated on both the successful and unsuccessful cases. In conclusion,
there is useful information in accelerometer data, but not enough to reliably
capture intentions to speak. For example, posture shifts are correlated with
intentions to speak, but people also often shift posture without having an
intention to speak, or have an intention to speak without shifting their
posture. More modalities are likely needed to reliably infer intentions to
speak.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05876">Safe reinforcement learning in uncertain contexts. (arXiv:2401.05876v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baumann_D/0/1/0/all/0/1">Dominik Baumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1">Thomas B. Sch&#xf6;n</a></p>
<p>When deploying machine learning algorithms in the real world, guaranteeing
safety is an essential asset. Existing safe learning approaches typically
consider continuous variables, i.e., regression tasks. However, in practice,
robotic systems are also subject to discrete, external environmental changes,
e.g., having to carry objects of certain weights or operating on frozen, wet,
or dry surfaces. Such influences can be modeled as discrete context variables.
In the existing literature, such contexts are, if considered, mostly assumed to
be known. In this work, we drop this assumption and show how we can perform
safe learning when we cannot directly measure the context variables. To achieve
this, we derive frequentist guarantees for multi-class classification, allowing
us to estimate the current context from measurements. Further, we propose an
approach for identifying contexts through experiments. We discuss under which
conditions we can retain theoretical guarantees and demonstrate the
applicability of our algorithm on a Furuta pendulum with camera measurements of
different weights that serve as contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05895">Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning. (arXiv:2401.05895v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianxiu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1">Keke Gai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liehuang Zhu</a></p>
<p>Distributed machine learning enables parallel training of extensive datasets
by delegating computing tasks across multiple workers. Despite the cost
reduction benefits of distributed machine learning, the dissemination of final
model weights often leads to potential conflicts over model ownership as
workers struggle to substantiate their involvement in the training computation.
To address the above ownership issues and prevent accidental failures and
malicious attacks, verifying the computational integrity and effectiveness of
workers becomes particularly crucial in distributed machine learning. In this
paper, we proposed a novel binary linear tree commitment-based ownership
protection model to ensure computational integrity with limited overhead and
concise proof. Due to the frequent updates of parameters during training, our
commitment scheme introduces a maintainable tree structure to reduce the costs
of updating proofs. Distinguished from SNARK-based verifiable computation, our
model achieves efficient proof aggregation by leveraging inner product
arguments. Furthermore, proofs of model weights are watermarked by worker
identity keys to prevent commitments from being forged or duplicated. The
performance analysis and comparison with SNARK-based hash commitments validate
the efficacy of our model in preserving computational integrity within
distributed machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05896">The Role of Deep Learning in Advancing Proactive Cybersecurity Measures for Smart Grid Networks: A Survey. (arXiv:2401.05896v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdi_N/0/1/0/all/0/1">Nima Abdi</a>, <a href="http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1">Abdullatif Albaseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1">Mohamed Abdallah</a></p>
<p>As smart grids (SG) increasingly rely on advanced technologies like sensors
and communication systems for efficient energy generation, distribution, and
consumption, they become enticing targets for sophisticated cyberattacks. These
evolving threats demand robust security measures to maintain the stability and
resilience of modern energy systems. While extensive research has been
conducted, a comprehensive exploration of proactive cyber defense strategies
utilizing Deep Learning (DL) in {SG} remains scarce in the literature. This
survey bridges this gap, studying the latest DL techniques for proactive cyber
defense. The survey begins with an overview of related works and our distinct
contributions, followed by an examination of SG infrastructure. Next, we
classify various cyber defense techniques into reactive and proactive
categories. A significant focus is placed on DL-enabled proactive defenses,
where we provide a comprehensive taxonomy of DL approaches, highlighting their
roles and relevance in the proactive security of SG. Subsequently, we analyze
the most significant DL-based methods currently in use. Further, we explore
Moving Target Defense, a proactive defense strategy, and its interactions with
DL methodologies. We then provide an overview of benchmark datasets used in
this domain to substantiate the discourse.{ This is followed by a critical
discussion on their practical implications and broader impact on cybersecurity
in Smart Grids.} The survey finally lists the challenges associated with
deploying DL-based security systems within SG, followed by an outlook on future
developments in this key field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05899">Optimistic Model Rollouts for Pessimistic Offline Policy Optimization. (arXiv:2401.05899v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1">Yuanzhao Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiying Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zijian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xudong Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kele Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1">Dawei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bo_D/0/1/0/all/0/1">Ding Bo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huaimin Wang</a></p>
<p>Model-based offline reinforcement learning (RL) has made remarkable progress,
offering a promising avenue for improving generalization with synthetic model
rollouts. Existing works primarily focus on incorporating pessimism for policy
optimization, usually via constructing a Pessimistic Markov Decision Process
(P-MDP). However, the P-MDP discourages the policies from learning in
out-of-distribution (OOD) regions beyond the support of offline datasets, which
can under-utilize the generalization ability of dynamics models. In contrast,
we propose constructing an Optimistic MDP (O-MDP). We initially observed the
potential benefits of optimism brought by encouraging more OOD rollouts.
Motivated by this observation, we present ORPO, a simple yet effective
model-based offline RL framework. ORPO generates Optimistic model Rollouts for
Pessimistic offline policy Optimization. Specifically, we train an optimistic
rollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel
the sampled state-action pairs with penalized rewards and optimize the output
policy in the P-MDP. Theoretically, we demonstrate that the performance of
policies trained with ORPO can be lower-bounded in linear MDPs. Experimental
results show that our framework significantly outperforms P-MDP baselines by a
margin of 30%, achieving state-of-the-art performance on the widely-used
benchmark. Moreover, ORPO exhibits notable advantages in problems that require
generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05908">EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge. (arXiv:2401.05908v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qibin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1">Toshihisa Tanaka</a></p>
<p>With large training datasets and massive amounts of computing sources, large
language models (LLMs) achieve remarkable performance in comprehensive and
generative ability. Based on those powerful LLMs, the model fine-tuned with
domain-specific datasets posseses more specialized knowledge and thus is more
practical like medical LLMs. However, the existing fine-tuned medical LLMs are
limited to general medical knowledge with English language. For
disease-specific problems, the model's response is inaccurate and sometimes
even completely irrelevant, especially when using a language other than
English. In this work, we focus on the particular disease of Epilepsy with
Japanese language and introduce a customized LLM termed as EpilepsyLLM. Our
model is trained from the pre-trained LLM by fine-tuning technique using
datasets from the epilepsy domain. The datasets contain knowledge of basic
information about disease, common treatment methods and drugs, and important
notes in life and work. The experimental results demonstrate that EpilepsyLLM
can provide more reliable and specialized medical knowledge responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05933">Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?. (arXiv:2401.05933v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aribe_S/0/1/0/all/0/1">Sales G. Aribe Jr.</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerardo_B/0/1/0/all/0/1">Bobby D. Gerardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Medina_R/0/1/0/all/0/1">Ruji P. Medina</a></p>
<p>With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS
epidemic in the Philippines is the one that is spreading the quickest in the
western Pacific. Although the full effects of COVID-19 on HIV services and
development are still unknown, it is predicted that such disruptions could lead
to a significant increase in HIV casualties. Therefore, the nation needs some
modeling and forecasting techniques to foresee the spread pattern and enhance
the governments prevention, treatment, testing, and care program. In this
study, the researcher uses Multilayer Perceptron Neural Network to forecast
time series during the period when the COVID-19 pandemic strikes the nation,
using statistics taken from the HIV/AIDS and ART Registry of the Philippines.
After training, validation, and testing of data, the study finds that the
predicted cumulative cases in the nation by 2030 will reach 145,273.
Additionally, there is very little difference between observed and anticipated
HIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well
as a greater coefficient of determination. Further research revealed that the
Philippines seems far from achieving Sustainable Development Goal 3 of Project
2030 due to an increase in the nations rate of new HIV infections. Despite the
detrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the
Philippine government, under the Marcos administration, must continue to adhere
to the United Nations 90-90-90 targets by enhancing its ART program and
ensuring that all vital health services are readily accessible and available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05946">Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments. (arXiv:2401.05946v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dedieu_A/0/1/0/all/0/1">Antoine Dedieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehrach_W/0/1/0/all/0/1">Wolfgang Lehrach</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guangyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1">Dileep George</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1">Miguel L&#xe1;zaro-Gredilla</a></p>
<p>Despite their stellar performance on a wide range of tasks, including
in-context tasks only revealed during inference, vanilla transformers and
variants trained for next-token predictions (a) do not learn an explicit world
model of their environment which can be flexibly queried and (b) cannot be used
for planning or navigation. In this paper, we consider partially observed
environments (POEs), where an agent receives perceptually aliased observations
as it navigates, which makes path planning hard. We introduce a transformer
with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a
compressed representation of the history of observations and actions. After
training a TDB to predict the future observation(s) given the history, we
extract interpretable cognitive maps of the environment from its active
bottleneck(s) indices. These maps are then paired with an external solver to
solve (constrained) path planning problems. First, we show that a TDB trained
on POEs (a) retains the near perfect predictive performance of a vanilla
transformer or an LSTM while (b) solving shortest path problems exponentially
faster. Second, a TDB extracts interpretable representations from text
datasets, while reaching higher in-context accuracy than vanilla sequence
models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context
accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context
path planning problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05964">An attempt to generate new bridge types from latent space of PixelCNN. (arXiv:2401.05964v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongjun Zhang</a></p>
<p>Try to generate new bridge types using generative artificial intelligence
technology. Using symmetric structured image dataset of three-span beam bridge,
arch bridge, cable-stayed bridge and suspension bridge , based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
PixelCNN is constructed and trained. The model can capture the statistical
structure of the images and calculate the probability distribution of the next
pixel when the previous pixels are given. From the obtained latent space
sampling, new bridge types different from the training dataset can be
generated. PixelCNN can organically combine different structural components on
the basis of human original bridge types, creating new bridge types that have a
certain degree of human original ability. Autoregressive models cannot
understand the meaning of the sequence, while multimodal models combine
regression and autoregressive models to understand the sequence. Multimodal
models should be the way to achieve artificial general intelligence in the
future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05969">Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem. (arXiv:2401.05969v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Strauss_N/0/1/0/all/0/1">Niklas Strau&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1">Matthias Schubert</a></p>
<p>The traveling officer problem (TOP) is a challenging stochastic optimization
task. In this problem, a parking officer is guided through a city equipped with
parking sensors to fine as many parking offenders as possible. A major
challenge in TOP is the dynamic nature of parking offenses, which randomly
appear and disappear after some time, regardless of whether they have been
fined. Thus, solutions need to dynamically adjust to currently fineable parking
offenses while also planning ahead to increase the likelihood that the officer
arrives during the offense taking place. Though various solutions exist, these
methods often struggle to take the implications of actions on the ability to
fine future parking violations into account. This paper proposes SATOP, a novel
spatial-aware deep reinforcement learning approach for TOP. Our novel state
encoder creates a representation of each action, leveraging the spatial
relationships between parking spots, the agent, and the action. Furthermore, we
propose a novel message-passing module for learning future inter-action
correlations in the given environment. Thus, the agent can estimate the
potential to fine further parking violations after executing an action. We
evaluate our method using an environment based on real-world data from
Melbourne. Our results show that SATOP consistently outperforms
state-of-the-art TOP agents and is able to fine up to 22% more parking
offenses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05972">Learning physics-based reduced models from data for the Hasegawa-Wakatani equations. (arXiv:2401.05972v1 [physics.comp-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Gahr_C/0/1/0/all/0/1">Constatin Gahr</a>, <a href="http://arxiv.org/find/physics/1/au:+Farcas_I/0/1/0/all/0/1">Ionut-Gabriel Farcas</a>, <a href="http://arxiv.org/find/physics/1/au:+Jenko_F/0/1/0/all/0/1">Frank Jenko</a></p>
<p>This paper focuses on the construction of non-intrusive Scientific Machine
Learning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma
turbulence simulations. In particular, we propose using Operator Inference
(OpInf) to build low-cost physics-based ROMs from data for such simulations. As
a representative example, we focus on the Hasegawa-Wakatani (HW) equations used
for modeling two-dimensional electrostatic drift-wave plasma turbulence. For a
comprehensive perspective of the potential of OpInf to construct accurate ROMs
for this model, we consider a setup for the HW equations that leads to the
formation of complex, nonlinear, and self-driven dynamics, and perform two sets
of experiments. We first use the data obtained via a direct numerical
simulation of the HW equations starting from a specific initial condition and
train OpInf ROMs for predictions beyond the training time horizon. In the
second, more challenging set of experiments, we train ROMs using the same
dataset as before but this time perform predictions for six other initial
conditions. Our results show that the OpInf ROMs capture the important features
of the turbulent dynamics and generalize to new and unseen initial conditions
while reducing the evaluation time of the high-fidelity model by up to five
orders of magnitude in single-core performance. In the broader context of
fusion research, this shows that non-intrusive SciML ROMs have the potential to
drastically accelerate numerical studies, which can ultimately enable tasks
such as the design and real-time control of optimized fusion devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05982">A tree-based varying coefficient model. (arXiv:2401.05982v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zakrisson_H/0/1/0/all/0/1">Henning Zakrisson</a>, <a href="http://arxiv.org/find/stat/1/au:+Lindholm_M/0/1/0/all/0/1">Mathias Lindholm</a></p>
<p>The paper introduces a tree-based varying coefficient model (VCM) where the
varying coefficients are modelled using the cyclic gradient boosting machine
(CGBM) from Delong et al. (2023). Modelling the coefficient functions using a
CGBM allows for dimension-wise early stopping and feature importance scores.
The dimension-wise early stopping not only reduces the risk of
dimension-specific overfitting, but also reveals differences in model
complexity across dimensions. The use of feature importance scores allows for
simple feature selection and easy model interpretation. The model is evaluated
on the same simulated and real data examples as those used in Richman and
W\"uthrich (2023), and the results show that it produces results in terms of
out of sample loss that are comparable to those of their neural network-based
VCM called LocalGLMnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06005">How does the primate brain combine generative and discriminative computations in vision?. (arXiv:2401.06005v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Peters_B/0/1/0/all/0/1">Benjamin Peters</a>, <a href="http://arxiv.org/find/q-bio/1/au:+DiCarlo_J/0/1/0/all/0/1">James J. DiCarlo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gureckis_T/0/1/0/all/0/1">Todd Gureckis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Haefner_R/0/1/0/all/0/1">Ralf Haefner</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Isik_L/0/1/0/all/0/1">Leyla Isik</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua Tenenbaum</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Konkle_T/0/1/0/all/0/1">Talia Konkle</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Naselaris_T/0/1/0/all/0/1">Thomas Naselaris</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Stachenfeld_K/0/1/0/all/0/1">Kimberly Stachenfeld</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tavares_Z/0/1/0/all/0/1">Zenna Tavares</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tsao_D/0/1/0/all/0/1">Doris Tsao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yildirim_I/0/1/0/all/0/1">Ilker Yildirim</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kriegeskorte_N/0/1/0/all/0/1">Nikolaus Kriegeskorte</a></p>
<p>Vision is widely understood as an inference problem. However, two contrasting
conceptions of the inference process have each been influential in research on
biological vision as well as the engineering of machine vision. The first
emphasizes bottom-up signal flow, describing vision as a largely feedforward,
discriminative inference process that filters and transforms the visual
information to remove irrelevant variation and represent behaviorally relevant
information in a format suitable for downstream functions of cognition and
behavioral control. In this conception, vision is driven by the sensory data,
and perception is direct because the processing proceeds from the data to the
latent variables of interest. The notion of "inference" in this conception is
that of the engineering literature on neural networks, where feedforward
convolutional neural networks processing images are said to perform inference.
The alternative conception is that of vision as an inference process in
Helmholtz's sense, where the sensory evidence is evaluated in the context of a
generative model of the causal processes giving rise to it. In this conception,
vision inverts a generative model through an interrogation of the evidence in a
process often thought to involve top-down predictions of sensory data to
evaluate the likelihood of alternative hypotheses. The authors include
scientists rooted in roughly equal numbers in each of the conceptions and
motivated to overcome what might be a false dichotomy between them and engage
the other perspective in the realm of theory and experiment. The primate brain
employs an unknown algorithm that may combine the advantages of both
conceptions. We explain and clarify the terminology, review the key empirical
evidence, and propose an empirical research program that transcends the
dichotomy and sets the stage for revealing the mysterious hybrid algorithm of
primate vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06009">Sea ice detection using concurrent multispectral and synthetic aperture radar imagery. (arXiv:2401.06009v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rogers_M/0/1/0/all/0/1">Martin S J Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Fox_M/0/1/0/all/0/1">Maria Fox</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_A/0/1/0/all/0/1">Andrew Fleming</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeeland_L/0/1/0/all/0/1">Louisa van Zeeland</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilkinson_J/0/1/0/all/0/1">Jeremy Wilkinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosking_J/0/1/0/all/0/1">J. Scott Hosking</a></p>
<p>Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea
ice mapping due to its spatio-temporal coverage and the ability to detect sea
ice independent of cloud and lighting conditions. Automatic sea ice detection
using SAR imagery remains problematic due to the presence of ambiguous signal
and noise within the image. Conversely, ice and water are easily
distinguishable using multispectral imagery (MSI), but in the polar regions the
ocean's surface is often occluded by cloud or the sun may not appear above the
horizon for many months. To address some of these limitations, this paper
proposes a new tool trained using concurrent multispectral Visible and SAR
imagery for sea Ice Detection (ViSual\_IceD). ViSual\_IceD is a convolution
neural network (CNN) that builds on the classic U-Net architecture by
containing two parallel encoder stages, enabling the fusion and concatenation
of MSI and SAR imagery containing different spatial resolutions. The
performance of ViSual\_IceD is compared with U-Net models trained using
concatenated MSI and SAR imagery as well as models trained exclusively on MSI
or SAR imagery. ViSual\_IceD outperforms the other networks, with a F1 score
1.60\% points higher than the next best network, and results indicate that
ViSual\_IceD is selective in the image type it uses during image segmentation.
Outputs from ViSual\_IceD are compared to sea ice concentration products
derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how
ViSual\_IceD is a useful tool to use in conjunction with PMW data, particularly
in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery
continues to increase, ViSual\_IceD provides a new opportunity for robust,
accurate sea ice coverage detection in polar regions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06035">RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks. (arXiv:2401.06035v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1">Partha Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Soubhik Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>We present a novel unconditional video generative model designed to address
long-term spatial and temporal dependencies. To capture these dependencies, our
approach incorporates a hybrid explicit-implicit tri-plane representation
inspired by 3D-aware generative frameworks developed for three-dimensional
object representation and employs a singular latent code to model an entire
video sequence. Individual video frames are then synthesized from an
intermediate tri-plane representation, which itself is derived from the primary
latent code. This novel strategy reduces computational complexity by a factor
of $2$ as measured in FLOPs. Consequently, our approach facilitates the
efficient and temporally coherent generation of videos. Moreover, our joint
frame modeling approach, in contrast to autoregressive methods, mitigates the
generation of visual artifacts. We further enhance the model's capabilities by
integrating an optical flow-based module within our Generative Adversarial
Network (GAN) based generator architecture, thereby compensating for the
constraints imposed by a smaller generator size. As a result, our model is
capable of synthesizing high-fidelity video clips at a resolution of
$256\times256$ pixels, with durations extending to more than $5$ seconds at a
frame rate of 30 fps. The efficacy and versatility of our approach are
empirically validated through qualitative and quantitative assessments across
three different datasets comprising both synthetic and real video clips.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06040">Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting. (arXiv:2401.06040v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1">Qipeng Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallick_T/0/1/0/all/0/1">Tanwi Mallick</a></p>
<p>Traffic forecasting is the foundation for intelligent transportation systems.
Spatiotemporal graph neural networks have demonstrated state-of-the-art
performance in traffic forecasting. However, these methods do not explicitly
model some of the natural characteristics in traffic data, such as the
multiscale structure that encompasses spatial and temporal variations at
different levels of granularity or scale. To that end, we propose a
Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines
multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In
WavGCRN, the traffic data is decomposed into time-frequency components with
Discrete Wavelet Transformation (DWT), constructing a multi-stream input
structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as
encoders for each stream, extracting spatiotemporal features in different
scales; and finally the learnable Inversed DWT and GCRN are combined as the
decoder, fusing the information from all streams for traffic metrics
reconstruction and prediction. Furthermore, road-network-informed graphs and
data-driven graph learning are combined to accurately capture spatial
correlation. The proposed method can offer well-defined interpretability,
powerful learning capability, and competitive forecasting performance on
real-world traffic data sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06048">On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks. (arXiv:2401.06048v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guettala_W/0/1/0/all/0/1">Walid Guettala</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulyas_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; Guly&#xe1;s</a></p>
<p>This paper studies four Graph Neural Network architectures (GNNs) for a graph
classification task on a synthetic dataset created using classic generative
models of Network Science. Since the synthetic networks do not contain (node or
edge) features, five different augmentation strategies (artificial feature
types) are applied to nodes. All combinations of the 4 GNNs (GCN with
Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types
(constant 1, noise, degree, normalized degree and ID -- a vector of the number
of cycles of various lengths) are studied and their performances compared as a
function of the hidden dimension of artificial neural networks used in the
GNNs. The generalisation ability of these models is also analysed using a
second synthetic network dataset (containing networks of different sizes).Our
results point towards the balanced importance of the computational power of the
GNN architecture and the the information level provided by the artificial
features. GNN architectures with higher computational power, like GIN and
GATv2, perform well for most augmentation strategies. On the other hand,
artificial features with higher information content, like ID or degree, not
only consistently outperform other augmentation strategies, but can also help
GNN architectures with lower computational power to achieve good performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06059">Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minhao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Ken Ziyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1">Ming Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1">Rylan Schaeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1">Siru Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1">Sanmi Koyejo</a></p>
<p>Language models pre-trained on web-scale corpora demonstrate impressive
capabilities on diverse downstream tasks. However, there is increasing concern
whether such capabilities might arise from evaluation datasets being included
in the pre-training corpus -- a phenomenon known as \textit{data contamination}
-- in a manner that artificially increases performance. There has been little
understanding of how this potential contamination might influence LMs'
performance on downstream tasks. In this paper, we explore the impact of data
contamination at the pre-training stage by pre-training a series of GPT-2
models \textit{from scratch}. We highlight the effect of both text
contamination (\textit{i.e.}\ input text of the evaluation samples) and
ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and
the desired outputs) from evaluation data. We also investigate the effects of
repeating contamination for various downstream tasks. Additionally, we examine
the prevailing n-gram-based definitions of contamination within current LLM
reports, pinpointing their limitations and inadequacy. Our findings offer new
insights into data contamination's effects on language model capabilities and
underscore the need for independent, comprehensive contamination assessments in
LLM studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06070">Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model for Complex Material Responses. (arXiv:2401.06070v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Jafarzadeh_S/0/1/0/all/0/1">Siavash Jafarzadeh</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Silling_S/0/1/0/all/0/1">Stewart Silling</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Liu_N/0/1/0/all/0/1">Ning Liu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongqiang Zhang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Yu_Y/0/1/0/all/0/1">Yue Yu</a></p>
<p>Neural operators, which can act as implicit solution operators of hidden
governing equations, have recently become popular tools for learning the
responses of complex real-world physical systems. Nevertheless, most neural
operator applications have thus far been data-driven and neglect the intrinsic
preservation of fundamental physical laws in data. In this work, we introduce a
novel integral neural operator architecture called the Peridynamic Neural
Operator (PNO) that learns a nonlocal constitutive law from data. This neural
operator provides a forward model in the form of state-based peridynamics, with
objectivity and momentum balance laws automatically guaranteed. As
applications, we demonstrate the expressivity and efficacy of our model in
learning complex material behaviors from both synthetic and experimental data
sets. We show that, owing to its ability to capture complex responses, our
learned neural operator achieves improved accuracy and efficiency compared to
baseline models that use predefined constitutive laws. Moreover, by preserving
the essential physical laws within the neural network architecture, the PNO is
robust in treating noisy data. The method shows generalizability to different
domain configurations, external loadings, and discretizations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06086">XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange. (arXiv:2401.06086v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Terawong_C/0/1/0/all/0/1">Chawin Terawong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cliff_D/0/1/0/all/0/1">Dave Cliff</a></p>
<p>We present first results from the use of XGBoost, a highly effective machine
learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source
agent-based model (ABM) designed to simulate a contemporary sports-betting
exchange with in-play betting during track-racing events such as horse races.
We use the BBE ABM and its array of minimally-simple bettor-agents as a
synthetic data generator which feeds into our XGBoost ML system, with the
intention that XGBoost discovers profitable dynamic betting strategies by
learning from the more profitable bets made by the BBE bettor-agents. After
this XGBoost training, which results in one or more decision trees, a
bettor-agent with a betting strategy determined by the XGBoost-learned decision
tree(s) is added to the BBE ABM and made to bet on a sequence of races under
various conditions and betting-market scenarios, with profitability serving as
the primary metric of comparison and evaluation. Our initial findings presented
here show that XGBoost trained in this way can indeed learn profitable betting
strategies, and can generalise to learn strategies that outperform each of the
set of strategies used for creation of the training data. To foster further
research and enhancements, the complete version of our extended BBE, including
the XGBoost integration, has been made freely available as an open-source
release on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06088">Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1">K M Sajjadul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Nipu_A/0/1/0/all/0/1">Ayesha Siddika Nipu</a>, <a href="http://arxiv.org/find/cs/1/au:+Madiraju_P/0/1/0/all/0/1">Praveen Madiraju</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1">Priya Deshpande</a></p>
<p>The Chief Complaint (CC) is a crucial component of a patient's medical record
as it describes the main reason or concern for seeking medical care. It
provides critical information for healthcare providers to make informed
decisions about patient care. However, documenting CCs can be time-consuming
for healthcare providers, especially in busy emergency departments. To address
this issue, an autocompletion tool that suggests accurate and well-formatted
phrases or sentences for clinical notes can be a valuable resource for triage
nurses. In this study, we utilized text generation techniques to develop
machine learning models using CC data. In our proposed work, we train a Long
Short-Term Memory (LSTM) model and fine-tune three different variants of
Biomedical Generative Pretrained Transformers (BioGPT), namely
microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.
Additionally, we tune a prompt by incorporating exemplar CC sentences,
utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on
the perplexity score, modified BERTScore, and cosine similarity score. The
results show that BioGPT-Large exhibits superior performance compared to the
other models. It consistently achieves a remarkably low perplexity score of
1.65 when generating CC, whereas the baseline LSTM model achieves the best
perplexity score of 170. Further, we evaluate and assess the proposed models'
performance and the outcome of GPT-4.0. Our study demonstrates that utilizing
LLMs such as BioGPT, leads to the development of an effective autocompletion
tool for generating CC documentation in healthcare settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06089">PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage Clustering on GPU. (arXiv:2401.06089v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sao_P/0/1/0/all/0/1">Piyush Sao</a>, <a href="http://arxiv.org/find/cs/1/au:+Prokopenko_A/0/1/0/all/0/1">Andrey Prokopenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Lebrun_Grandie_D/0/1/0/all/0/1">Damien Lebrun-Grandi&#xe9;</a></p>
<p>This paper presents \pandora, a novel parallel algorithm for efficiently
constructing dendrograms for single-linkage hierarchical clustering, including
\hdbscan. Traditional dendrogram construction methods from a minimum spanning
tree (MST), such as agglomerative or divisive techniques, often fail to
efficiently parallelize, especially with skewed dendrograms common in
real-world data.
</p>
<p>\pandora addresses these challenges through a unique recursive tree
contraction method, which simplifies the tree for initial dendrogram
construction and then progressively reconstructs the complete dendrogram. This
process makes \pandora asymptotically work-optimal, independent of dendrogram
skewness. All steps in \pandora are fully parallel and suitable for massively
threaded accelerators such as GPUs.
</p>
<p>Our implementation is written in Kokkos, providing support for both CPUs and
multi-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \pandora is
2.2$\times$ faster than the current best-multithreaded implementation, while
the GPU \pandora implementation achieved 6-20$\times$ on \amdgpu and
10-37$\times$ on \nvidiagpu speed-up over multithreaded \pandora. These
advancements lead to up to a 6-fold speedup for \hdbscan on GPUs over the
current best, which only offload MST construction to GPUs and perform
multithreaded dendrogram construction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06091">A Closer Look at AUROC and AUPRC under Class Imbalance. (arXiv:2401.06091v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McDermott_M/0/1/0/all/0/1">Matthew B. A. McDermott</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1">Lasse Hyldig Hansen</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoran Zhang</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Angelotti_G/0/1/0/all/0/1">Giovanni Angelotti</a> (4), <a href="http://arxiv.org/find/cs/1/au:+Gallifant_J/0/1/0/all/0/1">Jack Gallifant</a> (3) ((1) Harvard Medical School, (2) Aarhus University, (3) Massachusetts Institute of Technology, (4) IRCCS Humanitas Research Hospital)</p>
<p>In machine learning (ML), a widespread adage is that the area under the
precision-recall curve (AUPRC) is a superior metric for model comparison to the
area under the receiver operating characteristic (AUROC) for binary
classification tasks with class imbalance. This paper challenges this notion
through novel mathematical analysis, illustrating that AUROC and AUPRC can be
concisely related in probabilistic terms. We demonstrate that AUPRC, contrary
to popular belief, is not superior in cases of class imbalance and might even
be a harmful metric, given its inclination to unduly favor model improvements
in subpopulations with more frequent positive labels. This bias can
inadvertently heighten algorithmic disparities. Prompted by these insights, a
thorough review of existing ML literature was conducted, utilizing large
language models to analyze over 1.5 million papers from arXiv. Our
investigation focused on the prevalence and substantiation of the purported
AUPRC superiority. The results expose a significant deficit in empirical
backing and a trend of misattributions that have fuelled the widespread
acceptance of AUPRC's supposed advantages. Our findings represent a dual
contribution: a significant technical advancement in understanding metric
behaviors and a stark warning about unchecked assumptions in the ML community.
All experiments are accessible at
https://github.com/mmcdermott/AUC_is_all_you_need.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06102">Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1">Asma Ghandeharioun</a>, <a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1">Avi Caciularu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pearce_A/0/1/0/all/0/1">Adam Pearce</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1">Lucas Dixon</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a></p>
<p>Inspecting the information encoded in hidden representations of large
language models (LLMs) can explain models' behavior and verify their alignment
with human values. Given the capabilities of LLMs in generating
human-understandable text, we propose leveraging the model itself to explain
its internal representations in natural language. We introduce a framework
called Patchscopes and show how it can be used to answer a wide range of
research questions about an LLM's computation. We show that prior
interpretability methods based on projecting representations into the
vocabulary space and intervening on the LLM computation, can be viewed as
special instances of this framework. Moreover, several of their shortcomings
such as failure in inspecting early layers or lack of expressivity can be
mitigated by a Patchscope. Beyond unifying prior inspection techniques,
Patchscopes also opens up new possibilities such as using a more capable model
to explain the representations of a smaller model, and unlocks new applications
such as self-correction in multi-hop reasoning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06105">PALP: Prompt Aligned Personalization of Text-to-Image Models. (arXiv:2401.06105v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1">Moab Arar</a>, <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1">Amir Hertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1">Omri Avrahami</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1">Shlomi Fruchter</a>, <a href="http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1">Yael Pritch</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1">Ariel Shamir</a></p>
<p>Content creators often aim to create personalized images using personal
subjects that go beyond the capabilities of conventional text-to-image models.
Additionally, they may want the resulting image to encompass a specific
location, style, ambiance, and more. Existing personalization methods may
compromise personalization ability or the alignment to complex textual prompts.
This trade-off can impede the fulfillment of user prompts and subject fidelity.
We propose a new approach focusing on personalization methods for a
\emph{single} prompt to address this issue. We term our approach prompt-aligned
personalization. While this may seem restrictive, our method excels in
improving text alignment, enabling the creation of images with complex and
intricate prompts, which may pose a challenge for current techniques. In
particular, our method keeps the personalized model aligned with a target
prompt using an additional score distillation sampling term. We demonstrate the
versatility of our method in multi- and single-shot settings and further show
that it can compose multiple subjects or use inspiration from reference images,
such as artworks. We compare our approach quantitatively and qualitatively with
existing baselines and state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06118">Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Egiazarian_V/0/1/0/all/0/1">Vage Egiazarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Panferov_A/0/1/0/all/0/1">Andrei Panferov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznedelev_D/0/1/0/all/0/1">Denis Kuznedelev</a>, <a href="http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1">Elias Frantar</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>The emergence of accurate open large language models (LLMs) has led to a race
towards quantization techniques for such models enabling execution on end-user
devices. In this paper, we revisit the problem of "extreme" LLM
compression--defined as targeting extremely low bit counts, such as 2 to 3 bits
per parameter, from the point of view of classic methods in Multi-Codebook
Quantization (MCQ). Our work builds on top of Additive Quantization, a classic
algorithm from the MCQ family, and adapts it to the quantization of language
models. The resulting algorithm advances the state-of-the-art in LLM
compression, outperforming all recently-proposed techniques in terms of
accuracy at a given compression budget. For instance, when compressing Llama 2
models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93
perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points
from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B
model to 3.94 perplexity (a .22 improvement) on WikiText2. We release our
implementation of Additive Quantization for Language Models AQLM as a baseline
to facilitate future research in LLM quantization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06121">TOFU: A Task of Fictitious Unlearning for LLMs. (arXiv:2401.06121v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1">Pratyush Maini</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhili Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1">Avi Schwarzschild</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a></p>
<p>Large language models trained on massive corpora of data from the web can
memorize and reproduce sensitive or private data raising both legal and ethical
concerns. Unlearning, or tuning models to forget information present in their
training data, provides us with a way to protect private data after training.
Although several methods exist for such unlearning, it is unclear to what
extent they result in models equivalent to those where the data to be forgotten
was never learned in the first place. To address this challenge, we present
TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen
our understanding of unlearning. We offer a dataset of 200 diverse synthetic
author profiles, each consisting of 20 question-answer pairs, and a subset of
these profiles called the forget set that serves as the target for unlearning.
We compile a suite of metrics that work together to provide a holistic picture
of unlearning efficacy. Finally, we provide a set of baseline results from
existing unlearning algorithms. Importantly, none of the baselines we consider
show effective unlearning motivating continued efforts to develop approaches
for unlearning that effectively tune models so that they truly behave as if
they were never trained on the forget data at all.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06122">Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bareeva_D/0/1/0/all/0/1">Dilyara Bareeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a>, <a href="http://arxiv.org/find/cs/1/au:+Warnecke_A/0/1/0/all/0/1">Alexander Warnecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Pirch_L/0/1/0/all/0/1">Lukas Pirch</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_K/0/1/0/all/0/1">Konrad Rieck</a>, <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a></p>
<p>Deep Neural Networks (DNNs) are capable of learning complex and versatile
representations, however, the semantic nature of the learned concepts remains
unknown. A common method used to explain the concepts learned by DNNs is
Activation Maximization (AM), which generates a synthetic input signal that
maximally activates a particular neuron in the network. In this paper, we
investigate the vulnerability of this approach to adversarial model
manipulations and introduce a novel method for manipulating feature
visualization without altering the model architecture or significantly
impacting the model's decision-making process. We evaluate the effectiveness of
our method on several neural network models and demonstrate its capabilities to
hide the functionality of specific neurons by masking the original explanations
of neurons with chosen target explanations during model auditing. As a remedy,
we propose a protective measure against such manipulations and provide
quantitative evidence which substantiates our findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.06127">E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yifan Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1">Zheng Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Idelbayev_Y/0/1/0/all/0/1">Yerlan Idelbayev</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zharkov_A/0/1/0/all/0/1">Andrey Zharkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1">Kfir Aberman</a>, <a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1">Sergey Tulyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jian Ren</a></p>
<p>One highly promising direction for enabling flexible real-time on-device
image editing is utilizing data distillation by leveraging large-scale
text-to-image diffusion models, such as Stable Diffusion, to generate paired
datasets used for training generative adversarial networks (GANs). This
approach notably alleviates the stringent requirements typically imposed by
high-end commercial GPUs for performing image editing with diffusion models.
However, unlike text-to-image diffusion models, each distilled GAN is
specialized for a specific image editing task, necessitating costly training
efforts to obtain models for various concepts. In this work, we introduce and
address a novel research direction: can the process of distilling GANs from
diffusion models be made significantly more efficient? To achieve this goal, we
propose a series of innovative techniques. First, we construct a base GAN model
with generalized features, adaptable to different concepts through fine-tuning,
eliminating the need for training from scratch. Second, we identify crucial
layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a
simple yet effective rank search process, rather than fine-tuning the entire
base model. Third, we investigate the minimal amount of data necessary for
fine-tuning, further reducing the overall training time. Extensive experiments
show that we can efficiently empower GANs with the ability to perform real-time
high-quality image editing on mobile devices with remarkable reduced training
cost and storage for each concept.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.09820">GPEX, A Framework For Interpreting Artificial Neural Networks. (arXiv:2112.09820v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akbarnejad_A/0/1/0/all/0/1">Amir Akbarnejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Bigras_G/0/1/0/all/0/1">Gilbert Bigras</a>, <a href="http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1">Nilanjan Ray</a></p>
<p>The analogy between Gaussian processes (GPs) and deep artificial neural
networks (ANNs) has received a lot of interest, and has shown promise to unbox
the blackbox of deep ANNs. Existing theoretical works put strict assumptions on
the ANN (e.g. requiring all intermediate layers to be wide, or using specific
activation functions). Accommodating those theoretical assumptions is hard in
recent deep architectures, and those theoretical conditions need refinement as
new deep architectures emerge. In this paper we derive an evidence lower-bound
that encourages the GP's posterior to match the ANN's output without any
requirement on the ANN. Using our method we find out that on 5 datasets, only a
subset of those theoretical assumptions are sufficient. Indeed, in our
experiments we used a normal ResNet-18 or feed-forward backbone with a single
wide layer in the end. One limitation of training GPs is the lack of
scalability with respect to the number of inducing points. We use novel
computational techniques that allow us to train GPs with hundreds of thousands
of inducing points and with GPU acceleration. As shown in our experiments,
doing so has been essential to get a close match between the GPs and the ANNs
on 5 datasets. We implement our method as a publicly available tool called
GPEX: https://github.com/amirakbarnejad/gpex. On 5 datasets (4 image datasets,
and 1 biological dataset) and ANNs with 2 types of functionality (classifier or
attention-mechanism) we were able to find GPs whose outputs closely match those
of the corresponding ANNs. After matching the GPs to the ANNs, we used the GPs'
kernel functions to explain the ANNs' decisions. We provide more than 200
explanations (around 30 explanations in the paper and the rest in the
supplementary) which are highly interpretable by humans and show the ability of
the obtained GPs to unbox the ANNs' decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.00232">Improving deep neural network generalization and robustness to background bias via layer-wise relevance propagation optimization. (arXiv:2202.00232v7 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1">Pedro R. A. S. Bassi</a>, <a href="http://arxiv.org/find/eess/1/au:+Dertkigil_S/0/1/0/all/0/1">Sergio S. J. Dertkigil</a>, <a href="http://arxiv.org/find/eess/1/au:+Cavalli_A/0/1/0/all/0/1">Andrea Cavalli</a></p>
<p>Features in images' backgrounds can spuriously correlate with the images'
classes, representing background bias. They can influence the classifier's
decisions, causing shortcut learning (Clever Hans effect). The phenomenon
generates deep neural networks (DNNs) that perform well on standard evaluation
datasets but generalize poorly to real-world data. Layer-wise Relevance
Propagation (LRP) explains DNNs' decisions. Here, we show that the optimization
of LRP heatmaps can minimize the background bias influence on deep classifiers,
hindering shortcut learning. By not increasing run-time computational cost, the
approach is light and fast. Furthermore, it applies to virtually any
classification architecture. After injecting synthetic bias in images'
backgrounds, we compared our approach (dubbed ISNet) to eight state-of-the-art
DNNs, quantitatively demonstrating its superior robustness to background bias.
Mixed datasets are common for COVID-19 and tuberculosis classification with
chest X-rays, fostering background bias. By focusing on the lungs, the ISNet
reduced shortcut learning. Thus, its generalization performance on external
(out-of-distribution) test databases significantly surpassed all implemented
benchmark models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.09438">An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival. (arXiv:2203.09438v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schleibaum_S/0/1/0/all/0/1">S&#xf6;ren Schleibaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1">J&#xf6;rg P. M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Sester_M/0/1/0/all/0/1">Monika Sester</a></p>
<p>To compare alternative taxi schedules and to compute them, as well as to
provide insights into an upcoming taxi trip to drivers and passengers, the
duration of a trip or its Estimated Time of Arrival (ETA) is predicted. To
reach a high prediction precision, machine learning models for ETA are state of
the art. One yet unexploited option to further increase prediction precision is
to combine multiple ETA models into an ensemble. While an increase of
prediction precision is likely, the main drawback is that the predictions made
by such an ensemble become less transparent due to the sophisticated ensemble
architecture. One option to remedy this drawback is to apply eXplainable
Artificial Intelligence (XAI). The contribution of this paper is three-fold.
First, we combine multiple machine learning models from our previous work for
ETA into a two-level ensemble model - a stacked ensemble model - which on its
own is novel; therefore, we can outperform previous state-of-the-art static
route-free ETA approaches. Second, we apply existing XAI methods to explain the
first- and second-level models of the ensemble. Third, we propose three joining
methods for combining the first-level explanations with the second-level ones.
Those joining methods enable us to explain stacked ensembles for regression
tasks. An experimental evaluation shows that the ETA models correctly learned
the importance of those input features driving the prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.16810">Adaptive Estimation of Random Vectors with Bandit Feedback: A mean-squared error viewpoint. (arXiv:2203.16810v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1">Dipayan Sen</a>, <a href="http://arxiv.org/find/cs/1/au:+Prashanth_L/0/1/0/all/0/1">L.A. Prashanth</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1">Aditya Gopalan</a></p>
<p>We consider the problem of sequentially learning to estimate, in the mean
squared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by
observing only $m &lt; K$ of its entries in each round. We first establish a
concentration bound for MSE estimation. We then frame the estimation problem
with bandit feedback, and propose a variant of the successive elimination
algorithm. We also derive a minimax lower bound to understand the fundamental
limit on the sample complexity of this problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05289">Localized adversarial artifacts for compressed sensing MRI. (arXiv:2206.05289v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Alaifari_R/0/1/0/all/0/1">Rima Alaifari</a>, <a href="http://arxiv.org/find/eess/1/au:+Alberti_G/0/1/0/all/0/1">Giovanni S. Alberti</a>, <a href="http://arxiv.org/find/eess/1/au:+Gauksson_T/0/1/0/all/0/1">Tandri Gauksson</a></p>
<p>As interest in deep neural networks (DNNs) for image reconstruction tasks
grows, their reliability has been called into question (Antun et al., 2020;
Gottschling et al., 2020). However, recent work has shown that, compared to
total variation (TV) minimization, when appropriately regularized, DNNs show
similar robustness to adversarial noise in terms of $\ell^2$-reconstruction
error (Genzel et al., 2022). We consider a different notion of robustness,
using the $\ell^\infty$-norm, and argue that localized reconstruction artifacts
are a more relevant defect than the $\ell^2$-error. We create adversarial
perturbations to undersampled magnetic resonance imaging measurements (in the
frequency domain) which induce severe localized artifacts in the TV-regularized
reconstruction. Notably, the same attack method is not as effective against DNN
based reconstruction. Finally, we show that this phenomenon is inherent to
reconstruction methods for which exact recovery can be guaranteed, as with
compressed sensing reconstructions with $\ell^1$- or TV-minimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.08626">CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dong_Z/0/1/0/all/0/1">Zhikang Dong</a>, <a href="http://arxiv.org/find/stat/1/au:+Polak_P/0/1/0/all/0/1">Pawel Polak</a></p>
<p>The paper shows that Physics-Informed Neural Networks (PINNs) can fail to
estimate the correct Partial Differential Equations (PDEs) dynamics in cases of
unknown changepoints in the parameters. To address this, we propose a new
CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate
changepoints detection and PDEs discovery. In order to optimally combine the
tasks of model fitting, PDEs discovery, and changepoints detection, we develop
a new meta-learning algorithm that exploits batch learning to dynamically
refines the optimization objective when moving over the consecutive batches of
the data. Empirically, in case of changepoints in the dynamics, our approach
demonstrates accurate parameter estimation and model alignment, and in case of
no changepoints in the data, it converges numerically to the solution from the
original PINNs model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.14919">ARMA Cell: A Modular and Effective Approach for Neural Autoregressive Modeling. (arXiv:2208.14919v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schiele_P/0/1/0/all/0/1">Philipp Schiele</a>, <a href="http://arxiv.org/find/cs/1/au:+Berninger_C/0/1/0/all/0/1">Christoph Berninger</a>, <a href="http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1">David R&#xfc;gamer</a></p>
<p>The autoregressive moving average (ARMA) model is a classical, and arguably
one of the most studied approaches to model time series data. It has compelling
theoretical properties and is widely used among practitioners. More recent deep
learning approaches popularize recurrent neural networks (RNNs) and, in
particular, Long Short-Term Memory (LSTM) cells that have become one of the
best performing and most common building blocks in neural time series modeling.
While advantageous for time series data or sequences with long-term effects,
complex RNN cells are not always a must and can sometimes even be inferior to
simpler recurrent approaches. In this work, we introduce the ARMA cell, a
simpler, modular, and effective approach for time series modeling in neural
networks. This cell can be used in any neural network architecture where
recurrent structures are present and naturally handles multivariate time series
using vector autoregression. We also introduce the ConvARMA cell as a natural
successor for spatially-correlated time series. Our experiments show that the
proposed methodology is competitive with popular alternatives in terms of
performance while being more robust and compelling due to its simplicity
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16162">Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v3 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azimi_Abarghouyi_S/0/1/0/all/0/1">Seyed Mohammad Azimi-Abarghouyi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fodor_V/0/1/0/all/0/1">Viktoria Fodor</a></p>
<p>When implementing hierarchical federated learning over wireless networks,
scalability assurance and the ability to handle both interference and device
data heterogeneity are crucial. This work introduces a new two-level learning
method designed to address these challenges, along with a scalable over-the-air
aggregation scheme for the uplink and a bandwidth-limited broadcast scheme for
the downlink that efficiently use a single wireless resource. To provide
resistance against data heterogeneity, we employ gradient aggregations.
Meanwhile, the impact of uplink and downlink interference is minimized through
optimized receiver normalizing factors. We present a comprehensive mathematical
approach to derive the convergence bound for the proposed algorithm, applicable
to a multi-cluster wireless network encompassing any count of collaborating
clusters, and provide special cases and design remarks. As a key step to enable
a tractable analysis, we develop a spatial model for the setup by modeling
devices as a Poisson cluster process over the edge servers and rigorously
quantify uplink and downlink error terms due to the interference. Finally, we
show that despite the interference and data heterogeneity, the proposed
algorithm not only achieves high learning accuracy for a variety of parameters
but also significantly outperforms the conventional hierarchical learning
algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08674">An unfolding method based on conditional Invertible Neural Networks (cINN) using iterative training. (arXiv:2212.08674v3 [hep-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Backes_M/0/1/0/all/0/1">Mathias Backes</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Butter_A/0/1/0/all/0/1">Anja Butter</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Dunford_M/0/1/0/all/0/1">Monica Dunford</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Malaescu_B/0/1/0/all/0/1">Bogdan Malaescu</a></p>
<p>The unfolding of detector effects is crucial for the comparison of data to
theory predictions. While traditional methods are limited to representing the
data in a low number of dimensions, machine learning has enabled new unfolding
techniques while retaining the full dimensionality. Generative networks like
invertible neural networks~(INN) enable a probabilistic unfolding, which map
individual events to their corresponding unfolded probability distribution. The
accuracy of such methods is however limited by how well simulated training
samples model the actual data that is unfolded. We introduce the iterative
conditional INN~(IcINN) for unfolding that adjusts for deviations between
simulated training samples and data. The IcINN unfolding is first validated on
toy data and then applied to pseudo-data for the $pp \to Z \gamma \gamma$
process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13330">Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference. (arXiv:2301.13330v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bablani_D/0/1/0/all/0/1">Deepika Bablani</a>, <a href="http://arxiv.org/find/cs/1/au:+Mckinstry_J/0/1/0/all/0/1">Jeffrey L. Mckinstry</a>, <a href="http://arxiv.org/find/cs/1/au:+Esser_S/0/1/0/all/0/1">Steven K. Esser</a>, <a href="http://arxiv.org/find/cs/1/au:+Appuswamy_R/0/1/0/all/0/1">Rathinakumar Appuswamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Modha_D/0/1/0/all/0/1">Dharmendra S. Modha</a></p>
<p>For efficient neural network inference, it is desirable to achieve
state-of-the-art accuracy with the simplest networks requiring the least
computation, memory, and power. Quantizing networks to lower precision is a
powerful technique for simplifying networks. As each layer of a network may
have different sensitivity to quantization, mixed precision quantization
methods selectively tune the precision of individual layers to achieve a
minimum drop in task performance (e.g., accuracy). To estimate the impact of
layer precision choice on task performance, two methods are introduced: i)
Entropy Approximation Guided Layer selection (EAGL) is fast and uses the
entropy of the weight distribution, and ii) Accuracy-aware Layer Precision
Selection (ALPS) is straightforward and relies on single epoch fine-tuning
after layer precision reduction. Using EAGL and ALPS for layer precision
selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit
layers for ResNet-50, ResNet-101 and BERT-base transformer networks,
demonstrating enhanced performance across the entire accuracy-throughput
frontier. The techniques demonstrate better performance than existing
techniques in several commensurate comparisons. Notably, this is accomplished
with significantly lesser computational time required to reach a solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.10580">Classy Ensemble: A Novel Ensemble Algorithm for Classification. (arXiv:2302.10580v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1">Moshe Sipper</a></p>
<p>We present Classy Ensemble, a novel ensemble-generation algorithm for
classification tasks, which aggregates models through a weighted combination of
per-class accuracy. Tested over 153 machine learning datasets we demonstrate
that Classy Ensemble outperforms two other well-known aggregation algorithms --
order-based pruning and clustering-based pruning -- as well as the recently
introduced lexigarden ensemble generator. We then present three enhancements:
1) Classy Cluster Ensemble, which combines Classy Ensemble and cluster-based
pruning; 2) Deep Learning experiments, showing the merits of Classy Ensemble
over four image datasets: Fashion MNIST, CIFAR10, CIFAR100, and ImageNet; and
3) Classy Evolutionary Ensemble, wherein an evolutionary algorithm is used to
select the set of models which Classy Ensemble picks from. This latter,
combining learning and evolution, resulted in improved performance on the
hardest dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14383">Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1">Matthew Trager</a>, <a href="http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1">Pramuditha Perera</a>, <a href="http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1">Luca Zancato</a>, <a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1">Alessandro Achille</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1">Parminder Bhatia</a>, <a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1">Stefano Soatto</a></p>
<p>We investigate compositional structures in data embeddings from pre-trained
vision-language models (VLMs). Traditionally, compositionality has been
associated with algebraic operations on embeddings of words from a pre-existing
vocabulary. In contrast, we seek to approximate representations from an encoder
as combinations of a smaller set of vectors in the embedding space. These
vectors can be seen as "ideal words" for generating concepts directly within
the embedding space of the model. We first present a framework for
understanding compositional structures from a geometric perspective. We then
explain what these compositional structures entail probabilistically in the
case of VLM embeddings, providing intuitions for why they arise in practice.
Finally, we empirically explore these structures in CLIP's embeddings and we
evaluate their usefulness for solving different vision-language tasks such as
classification, debiasing, and retrieval. Our results show that simple linear
algebraic operations on embedding vectors can be used as compositional and
interpretable methods for regulating the behavior of VLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06471">Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Waqas_A/0/1/0/all/0/1">Asim Waqas</a>, <a href="http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1">Aakash Tripathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1">Ravi P. Ramachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Stewart_P/0/1/0/all/0/1">Paul Stewart</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1">Ghulam Rasool</a></p>
<p>Cancer has relational information residing at varying scales, modalities, and
resolutions of the acquired data, such as radiology, pathology, genomics,
proteomics, and clinical records. Integrating diverse data types can improve
the accuracy and reliability of cancer diagnosis and treatment. There can be
disease-related information that is too subtle for humans or existing
technological tools to discern visually. Traditional methods typically focus on
partial or unimodal information about biological systems at individual scales
and fail to encapsulate the complete spectrum of the heterogeneous nature of
data. Deep neural networks have facilitated the development of sophisticated
multimodal data fusion approaches that can extract and integrate relevant
information from multiple sources. Recent deep learning frameworks such as
Graph Neural Networks (GNNs) and Transformers have shown remarkable success in
multimodal learning. This review article provides an in-depth analysis of the
state-of-the-art in GNNs and Transformers for multimodal data fusion in
oncology settings, highlighting notable research studies and their findings. We
also discuss the foundations of multimodal learning, inherent challenges, and
opportunities for integrative learning in oncology. By examining the current
state and potential future developments of multimodal data integration in
oncology, we aim to demonstrate the promising role that multimodal neural
networks can play in cancer prevention, early detection, and treatment through
informed oncology practices in personalized settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08115">Human-Inspired Framework to Accelerate Reinforcement Learning. (arXiv:2303.08115v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beikmohammadi_A/0/1/0/all/0/1">Ali Beikmohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnusson_S/0/1/0/all/0/1">Sindri Magn&#xfa;sson</a></p>
<p>Reinforcement learning (RL) is crucial for data science decision-making but
suffers from sample inefficiency, particularly in real-world scenarios with
costly physical interactions. This paper introduces a novel human-inspired
framework to enhance RL algorithm sample efficiency. It achieves this by
initially exposing the learning agent to simpler tasks that progressively
increase in complexity, ultimately leading to the main task. This method
requires no pre-training and involves learning simpler tasks for just one
iteration. The resulting knowledge can facilitate various transfer learning
approaches, such as value and policy transfer, without increasing computational
complexity. It can be applied across different goals, environments, and RL
algorithms, including value-based, policy-based, tabular, and deep RL methods.
Experimental evaluations demonstrate the framework's effectiveness in enhancing
sample efficiency, especially in challenging main tasks, demonstrated through
both a simple Random Walk and more complex optimal control problems with
constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.08500">The Devil&#x27;s Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dolatabadi_H/0/1/0/all/0/1">Hadi M. Dolatabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1">Sarah Erfani</a>, <a href="http://arxiv.org/find/cs/1/au:+Leckie_C/0/1/0/all/0/1">Christopher Leckie</a></p>
<p>Protecting personal data against exploitation of machine learning models is
crucial. Recently, availability attacks have shown great promise to provide an
extra layer of protection against the unauthorized use of data to train neural
networks. These methods aim to add imperceptible noise to clean data so that
the neural networks cannot extract meaningful patterns from the protected data,
claiming that they can make personal data "unexploitable." This paper provides
a strong countermeasure against such approaches, showing that unexploitable
data might only be an illusion. In particular, we leverage the power of
diffusion models and show that a carefully designed denoising process can
counteract the effectiveness of the data-protecting perturbations. We
rigorously analyze our algorithm, and theoretically prove that the amount of
required denoising is directly related to the magnitude of the data-protecting
perturbations. Our approach, called AVATAR, delivers state-of-the-art
performance against a suite of recent availability attacks in various
scenarios, outperforming adversarial training even under distribution mismatch
between the diffusion model and the protected data. Our findings call for more
research into making personal data unexploitable, showing that this goal is far
from over. Our implementation is available at this repository:
https://github.com/hmdolatabadi/AVATAR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03582">A multimodal dynamical variational autoencoder for audiovisual speech representation learning. (arXiv:2305.03582v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadok_S/0/1/0/all/0/1">Samir Sadok</a>, <a href="http://arxiv.org/find/cs/1/au:+Leglaive_S/0/1/0/all/0/1">Simon Leglaive</a>, <a href="http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1">Laurent Girin</a>, <a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1">Xavier Alameda-Pineda</a>, <a href="http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1">Renaud S&#xe9;guier</a></p>
<p>In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to
unsupervised audio-visual speech representation learning. The latent space is
structured to dissociate the latent dynamical factors that are shared between
the modalities from those that are specific to each modality. A static latent
variable is also introduced to encode the information that is constant over
time within an audiovisual speech sequence. The model is trained in an
unsupervised manner on an audiovisual emotional speech dataset, in two stages.
In the first stage, a vector quantized VAE (VQ-VAE) is learned independently
for each modality, without temporal modeling. The second stage consists in
learning the MDVAE model on the intermediate representation of the VQ-VAEs
before quantization. The disentanglement between static versus dynamical and
modality-specific versus modality-common information occurs during this second
training stage. Extensive experiments are conducted to investigate how
audiovisual speech latent factors are encoded in the latent space of MDVAE.
These experiments include manipulating audiovisual speech, audiovisual facial
image denoising, and audiovisual speech emotion recognition. The results show
that MDVAE effectively combines the audio and visual information in its latent
space. They also show that the learned static representation of audiovisual
speech can be used for emotion recognition with few labeled data, and with
better accuracy compared with unimodal baselines and a state-of-the-art
supervised model based on an audiovisual transformer architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14062">Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Miao_Y/0/1/0/all/0/1">Yuyang Miao</a>, <a href="http://arxiv.org/find/eess/1/au:+Davies_H/0/1/0/all/0/1">Harry J. Davies</a>, <a href="http://arxiv.org/find/eess/1/au:+Mandic_D/0/1/0/all/0/1">Danilo P. Mandic</a></p>
<p>Photoplethysmography (PPG) refers to the measurement of variations in blood
volume using light and is a feature of most wearable devices. The PPG signals
provide insight into the body's circulatory system and can be employed to
extract various bio-features, such as heart rate and vascular ageing. Although
several algorithms have been proposed for this purpose, many exhibit
limitations, including heavy reliance on human calibration, high signal quality
requirements, and a lack of generalisation. In this paper, we introduce a PPG
signal processing framework that integrates graph theory and computer vision
algorithms, to provide an analysis framework which is amplitude-independent and
invariant to affine transformations. It also requires minimal preprocessing,
fuses information through RGB channels and exhibits robust generalisation
across tasks and datasets. The proposed VGTL-net achieves state-of-the-art
performance in the prediction of vascular ageing and demonstrates robust
estimation of continuous blood pressure waveforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15349">On the Convergence of Black-Box Variational Inference. (arXiv:2305.15349v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kyurae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jisu Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kaiwen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi-An Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1">Jacob R. Gardner</a></p>
<p>We provide the first convergence guarantee for full black-box variational
inference (BBVI), also known as Monte Carlo variational inference. While
preliminary investigations worked on simplified versions of BBVI (e.g., bounded
domain, bounded support, only optimizing for the scale, and such), our setup
does not need any such algorithmic modifications. Our results hold for
log-smooth posterior densities with and without strong log-concavity and the
location-scale variational family. Also, our analysis reveals that certain
algorithm design choices commonly employed in practice, particularly, nonlinear
parameterizations of the scale of the variational approximation, can result in
suboptimal convergence rates. Fortunately, running BBVI with proximal
stochastic gradient descent fixes these limitations, and thus achieves the
strongest known convergence rate guarantees. We evaluate this theoretical
insight by comparing proximal SGD against other standard implementations of
BBVI on large-scale Bayesian inference problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16297">Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?. (arXiv:2305.16297v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yutong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xinmeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1">Kun Yuan</a></p>
<p>Communication compression is a common technique in distributed optimization
that can alleviate communication overhead by transmitting compressed gradients
and model parameters. However, compression can introduce information
distortion, which slows down convergence and incurs more communication rounds
to achieve desired solutions. Given the trade-off between lower per-round
communication costs and additional rounds of communication, it is unclear
whether communication compression reduces the total communication cost.
</p>
<p>This paper explores the conditions under which unbiased compression, a widely
used form of compression, can reduce the total communication cost, as well as
the extent to which it can do so. To this end, we present the first theoretical
formulation for characterizing the total communication cost in distributed
optimization with communication compression. We demonstrate that unbiased
compression alone does not necessarily save the total communication cost, but
this outcome can be achieved if the compressors used by all workers are further
assumed independent. We establish lower bounds on the communication rounds
required by algorithms using independent unbiased compressors to minimize
smooth convex functions and show that these lower bounds are tight by refining
the analysis for ADIANA. Our results reveal that using independent unbiased
compression can reduce the total communication cost by a factor of up to
$\Theta(\sqrt{\min\{n, \kappa\}})$ when all local smoothness constants are
constrained by a common upper bound, where $n$ is the number of workers and
$\kappa$ is the condition number of the functions being minimized. These
theoretical findings are supported by experimental results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17147">Heterogeneous Value Alignment Evaluation for Large Language Models. (arXiv:2305.17147v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ceyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1">Siyuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_Z/0/1/0/all/0/1">Ziqi Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>The emergent capabilities of Large Language Models (LLMs) have made it
crucial to align their values with those of humans. However, current
methodologies typically attempt to assign value as an attribute to LLMs, yet
lack attention to the ability to pursue value and the importance of
transferring heterogeneous values in specific practical applications. In this
paper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,
designed to assess the success of aligning LLMs with heterogeneous values.
Specifically, our approach first brings the Social Value Orientation (SVO)
framework from social psychology, which corresponds to how much weight a person
attaches to the welfare of others in relation to their own. We then assign the
LLMs with different social values and measure whether their behaviors align
with the inducing values. We conduct evaluations with new auto-metric
\textit{value rationality} to represent the ability of LLMs to align with
specific values. Evaluating the value rationality of five mainstream LLMs, we
discern a propensity in LLMs towards neutral values over pronounced personal
values. By examining the behavior of these LLMs, we contribute to a deeper
insight into the value alignment of LLMs within a heterogeneous value system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17333">Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1">Sadhika Malladi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tianyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Nichani_E/0/1/0/all/0/1">Eshaan Nichani</a>, <a href="http://arxiv.org/find/cs/1/au:+Damian_A/0/1/0/all/0/1">Alex Damian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Danqi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1">Sanjeev Arora</a></p>
<p>Fine-tuning language models (LMs) has yielded success on diverse downstream
tasks, but as LMs grow in size, backpropagation requires a prohibitively large
amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients
using only two forward passes but are theorized to be catastrophically slow for
optimizing large models. In this work, we propose a memory-efficient
zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate
in-place, thereby fine-tuning LMs with the same memory footprint as inference.
For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter
model, whereas fine-tuning with backpropagation can train only a 2.7B LM with
the same budget. We conduct comprehensive experiments across model types
(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks
(classification, multiple-choice, and generation). Our results demonstrate that
(1) MeZO significantly outperforms in-context learning and linear probing; (2)
MeZO achieves comparable performance to fine-tuning with backpropagation across
multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction
in our implementation; (3) MeZO is compatible with both full-parameter and
parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO
can effectively optimize non-differentiable objectives (e.g., maximizing
accuracy or F1). We support our empirical findings with theoretical insights,
highlighting how adequate pre-training and task prompts enable MeZO to
fine-tune huge models, despite classical ZO analyses suggesting otherwise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18211">WiFi-TCN: Temporal Convolution for Human Interaction Recognition based on WiFi signal. (arXiv:2305.18211v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1">Chih-Yang Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1">Chia-Yu Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Tso Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Shih_T/0/1/0/all/0/1">Timothy K. Shih</a></p>
<p>The utilization of Wi-Fi based human activity recognition has gained
considerable interest in recent times, primarily owing to its applications in
various domains such as healthcare for monitoring breath and heart rate,
security, elderly care. These Wi-Fi-based methods exhibit several advantages
over conventional state-of-the-art techniques that rely on cameras and sensors,
including lower costs and ease of deployment. However, a significant challenge
associated with Wi-Fi-based HAR is the significant decline in performance when
the scene or subject changes. To mitigate this issue, it is imperative to train
the model using an extensive dataset. In recent studies, the utilization of
CNN-based models or sequence-to-sequence models such as LSTM, GRU, or
Transformer has become prevalent. While sequence-to-sequence models can be more
precise, they are also more computationally intensive and require a larger
amount of training data. To tackle these limitations, we propose a novel
approach that leverages a temporal convolution network with augmentations and
attention, referred to as TCN-AA. Our proposed method is computationally
efficient and exhibits improved accuracy even when the data size is increased
threefold through our augmentation techniques. Our experiments on a publicly
available dataset indicate that our approach outperforms existing
state-of-the-art methods, with a final accuracy of 99.42%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19604">Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sicen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xianbing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>Medication recommendation is a fundamental yet crucial branch of healthcare,
which provides opportunities to support clinical physicians with more accurate
medication prescriptions for patients with complex health conditions. Learning
from electronic health records (EHR) to recommend medications is the most
common way in previous studies. However, most of them neglect incorporating
domain knowledge according to the clinical manifestations in the EHR of the
patient. To address these issues, we propose a novel \textbf{D}omain
\textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate
domain knowledge with observable clinical manifestations of the patient, which
is the first dynamic domain knowledge informed framework toward medication
recommendation. In particular, we first design a knowledge-driven encoder to
capture the domain information and then develop a data-driven encoder to
integrate domain knowledge into the observable EHR. To endow the model with the
capability of temporal decision, we design an explicit medication encoder for
learning the longitudinal dependence of the patient. Extensive experiments on
three publicly available datasets verify the superiority of our method. The
code will be public upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01424">Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. (arXiv:2306.01424v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Melnychuk_V/0/1/0/all/0/1">Valentyn Melnychuk</a>, <a href="http://arxiv.org/find/stat/1/au:+Frauen_D/0/1/0/all/0/1">Dennis Frauen</a>, <a href="http://arxiv.org/find/stat/1/au:+Feuerriegel_S/0/1/0/all/0/1">Stefan Feuerriegel</a></p>
<p>Counterfactual inference aims to answer retrospective "what if" questions and
thus belongs to the most fine-grained type of inference in Pearl's causality
ladder. Existing methods for counterfactual inference with continuous outcomes
aim at point identification and thus make strong and unnatural assumptions
about the underlying structural causal model. In this paper, we relax these
assumptions and aim at partial counterfactual identification of continuous
outcomes, i.e., when the counterfactual query resides in an ignorance interval
with informative bounds. We prove that, in general, the ignorance interval of
the counterfactual queries has non-informative bounds, already when functions
of structural causal models are continuously differentiable. As a remedy, we
propose a novel sensitivity model called Curvature Sensitivity Model. This
allows us to obtain informative bounds by bounding the curvature of level sets
of the functions. We further show that existing point counterfactual
identification methods are special cases of our Curvature Sensitivity Model
when the bound of the curvature is set to zero. We then propose an
implementation of our Curvature Sensitivity Model in the form of a novel deep
generative model, which we call Augmented Pseudo-Invertible Decoder. Our
implementation employs (i) residual normalizing flows with (ii) variational
augmentations. We empirically demonstrate the effectiveness of our Augmented
Pseudo-Invertible Decoder. To the best of our knowledge, ours is the first
partial identification model for Markovian structural causal models with
continuous outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01684">Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1">Alexey Kurakin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponomareva_N/0/1/0/all/0/1">Natalia Ponomareva</a>, <a href="http://arxiv.org/find/cs/1/au:+Syed_U/0/1/0/all/0/1">Umar Syed</a>, <a href="http://arxiv.org/find/cs/1/au:+MacDermed_L/0/1/0/all/0/1">Liam MacDermed</a>, <a href="http://arxiv.org/find/cs/1/au:+Terzis_A/0/1/0/all/0/1">Andreas Terzis</a></p>
<p>Differentially private training algorithms like DP-SGD protect sensitive
training data by ensuring that trained models do not reveal private
information. An alternative approach, which this paper studies, is to use a
sensitive dataset to generate synthetic data that is differentially private
with respect to the original data, and then non-privately training a model on
the synthetic data. Doing so has several advantages: synthetic data can be
reused for other tasks (including for hyper parameter tuning), retained
indefinitely, and shared with third parties without sacrificing privacy.
However, generating private synthetic data is much harder than training a
private model. To improve performance on text data, recent work has utilized
public data by starting with a pre-trained generative language model and
privately fine-tuning it on sensitive data. This model can be used to sample a
DP synthetic dataset. While this strategy seems straightforward, executing it
has proven problematic. Previous approaches either show significant performance
loss, or have, as we show, critical design flaws. In this paper we demonstrate
that a proper training objective along with tuning fewer parameters results in
excellent DP synthetic data quality. Our approach is competitive with direct
DP-training of downstream classifiers in terms of performance on downstream
tasks. Further, we demonstrate that our DP synthetic data is not only useful
for downstream classifier training, but also to tune those same models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02426">Resilient Constrained Learning. (arXiv:2306.02426v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hounie_I/0/1/0/all/0/1">Ignacio Hounie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1">Alejandro Ribeiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Chamon_L/0/1/0/all/0/1">Luiz F. O. Chamon</a></p>
<p>When deploying machine learning solutions, they must satisfy multiple
requirements beyond accuracy, such as fairness, robustness, or safety. These
requirements are imposed during training either implicitly, using penalties, or
explicitly, using constrained optimization methods based on Lagrangian duality.
Either way, specifying requirements is hindered by the presence of compromises
and limited prior knowledge about the data. Furthermore, their impact on
performance can often only be evaluated by actually solving the learning
problem. This paper presents a constrained learning approach that adapts the
requirements while simultaneously solving the learning task. To do so, it
relaxes the learning constraints in a way that contemplates how much they
affect the task at hand by balancing the performance gains obtained from the
relaxation against a user-defined cost of that relaxation. We call this
approach resilient constrained learning after the term used to describe
ecological systems that adapt to disruptions by modifying their operation. We
show conditions under which this balance can be achieved and introduce a
practical algorithm to compute it, for which we derive approximation and
generalization guarantees. We showcase the advantages of this resilient
learning method in image classification tasks involving multiple potential
invariances and in heterogeneous federated learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02729">Gibbs Sampling the Posterior of Neural Networks. (arXiv:2306.02729v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piccioli_G/0/1/0/all/0/1">Giovanni Piccioli</a>, <a href="http://arxiv.org/find/cs/1/au:+Troiani_E/0/1/0/all/0/1">Emanuele Troiani</a>, <a href="http://arxiv.org/find/cs/1/au:+Zdeborova_L/0/1/0/all/0/1">Lenka Zdeborov&#xe1;</a></p>
<p>In this paper, we study sampling from a posterior derived from a neural
network. We propose a new probabilistic model consisting of adding noise at
every pre- and post-activation in the network, arguing that the resulting
posterior can be sampled using an efficient Gibbs sampler. For small models,
the Gibbs sampler attains similar performances as the state-of-the-art Markov
chain Monte Carlo (MCMC) methods, such as the Hamiltonian Monte Carlo (HMC) or
the Metropolis adjusted Langevin algorithm (MALA), both on real and synthetic
data. By framing our analysis in the teacher-student setting, we introduce a
thermalization criterion that allows us to detect when an algorithm, when run
on data with synthetic labels, fails to sample from the posterior. The
criterion is based on the fact that in the teacher-student setting we can
initialize an algorithm directly at equilibrium.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13853">A Unified Approach to Controlling Implicit Regularization via Mirror Descent. (arXiv:2306.13853v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haoyuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Gatmiry_K/0/1/0/all/0/1">Khashayar Gatmiry</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_K/0/1/0/all/0/1">Kwangjun Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizan_N/0/1/0/all/0/1">Navid Azizan</a></p>
<p>Inspired by the remarkable success of large neural networks, there has been
significant interest in understanding the generalization performance of
over-parameterized models. Substantial efforts have been invested in
characterizing how optimization algorithms impact generalization through their
"preferred" solutions, a phenomenon commonly referred to as implicit
regularization. In particular, it has been argued that gradient descent (GD)
induces an implicit $\ell_2$-norm regularization in regression and
classification problems. However, the implicit regularization of different
algorithms are confined to either a specific geometry or a particular class of
learning problems, indicating a gap in a general approach for controlling the
implicit regularization. To address this, we present a unified approach using
mirror descent (MD), a notable generalization of GD, to control implicit
regularization in both regression and classification settings. More
specifically, we show that MD with the general class of homogeneous potential
functions converges in direction to a generalized maximum-margin solution for
linear classification problems, thereby answering a long-standing question in
the classification setting. Further, we show that MD can be implemented
efficiently and enjoys fast convergence under suitable conditions. Through
comprehensive experiments, we demonstrate that MD is a versatile method to
produce learned models with different regularizers, which in turn have
different generalization performances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01849">Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Belagali_V/0/1/0/all/0/1">Varun Belagali</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jinghuan Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1">Michael S. Ryoo</a></p>
<p>Sequence modeling approaches have shown promising results in robot imitation
learning. Recently, diffusion models have been adopted for behavioral cloning
in a sequence modeling fashion, benefiting from their exceptional capabilities
in modeling complex data distributions. The standard diffusion-based policy
iteratively generates action sequences from random noise conditioned on the
input states. Nonetheless, the model for diffusion policy can be further
improved in terms of visual representations. In this work, we propose Crossway
Diffusion, a simple yet effective method to enhance diffusion-based visuomotor
policy learning via a carefully designed state decoder and an auxiliary
self-supervised learning (SSL) objective. The state decoder reconstructs raw
image pixels and other state information from the intermediate representations
of the reverse diffusion process. The whole model is jointly optimized by the
SSL objective and the original diffusion loss. Our experiments demonstrate the
effectiveness of Crossway Diffusion in various simulated and real-world robot
tasks, confirming its consistent advantages over the standard diffusion-based
policy and substantial improvements over the baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05638">A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1">Peng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdulkadir_A/0/1/0/all/0/1">Ahmed Abdulkadir</a>, <a href="http://arxiv.org/find/cs/1/au:+Luley_P/0/1/0/all/0/1">Paul-Philipp Luley</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenthal_M/0/1/0/all/0/1">Matthias Rosenthal</a>, <a href="http://arxiv.org/find/cs/1/au:+Schatte_G/0/1/0/all/0/1">Gerrit A. Schatte</a>, <a href="http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1">Benjamin F. Grewe</a>, <a href="http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1">Thilo Stadelmann</a></p>
<p>Automating the monitoring of industrial processes has the potential to
enhance efficiency and optimize quality by promptly detecting abnormal events
and thus facilitating timely interventions. Deep learning, with its capacity to
discern non-trivial patterns within large datasets, plays a pivotal role in
this process. Standard deep learning methods are suitable to solve a specific
task given a specific type of data. During training, deep learning demands
large volumes of labeled data. However, due to the dynamic nature of the
industrial processes and environment, it is impractical to acquire large-scale
labeled data for standard deep learning training for every slightly different
case anew. Deep transfer learning offers a solution to this problem. By
leveraging knowledge from related tasks and accounting for variations in data
distributions, the transfer learning framework solves new tasks with little or
even no additional labeled data. The approach bypasses the need to retrain a
model from scratch for every new setup and dramatically reduces the labeled
data requirement. This survey first provides an in-depth review of deep
transfer learning, examining the problem settings of transfer learning and
classifying the prevailing deep transfer learning methods. Moreover, we delve
into applications of deep transfer learning in the context of a broad spectrum
of time series anomaly detection tasks prevalent in primary industrial domains,
e.g., manufacturing process monitoring, predictive maintenance, energy
management, and infrastructure facility monitoring. We discuss the challenges
and limitations of deep transfer learning in industrial contexts and conclude
the survey with practical directions and actionable suggestions to address the
need to leverage diverse time series data for anomaly detection in an
increasingly dynamic production environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13621">Scaling up machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points. (arXiv:2307.13621v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Esders_M/0/1/0/all/0/1">Malte Esders</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramirez_G/0/1/0/all/0/1">Gimmy Alex Fernandez Ramirez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gastegger_M/0/1/0/all/0/1">Michael Gastegger</a>, <a href="http://arxiv.org/find/cs/1/au:+Samal_S/0/1/0/all/0/1">Satya Swarup Samal</a></p>
<p>Idealized first-principles models of chemical plants can be inaccurate. An
alternative is to fit a Machine Learning (ML) model directly to plant sensor
data. We use a structured approach: Each unit within the plant gets represented
by one ML model. After fitting the models to the data, the models are connected
into a flowsheet-like directed graph. We find that for smaller plants, this
approach works well, but for larger plants, the complex dynamics arising from
large and nested cycles in the flowsheet lead to instabilities in the solver
during model initialization. We show that a high accuracy of the single-unit
models is not enough: The gradient can point in unexpected directions, which
prevents the solver from converging to the correct stationary state. To address
this problem, we present a way to fine-tune ML models such that initialization,
even with very simple solvers, becomes robust.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.00031">Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1">Giorgio Franceschelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1">Mirco Musolesi</a></p>
<p>Generative Artificial Intelligence (AI) is one of the most exciting
developments in Computer Science of the last decade. At the same time,
Reinforcement Learning (RL) has emerged as a very successful paradigm for a
variety of machine learning tasks. In this survey, we discuss the state of the
art, opportunities and open research questions in applying RL to generative AI.
In particular, we will discuss three types of applications, namely, RL as an
alternative way for generation without specified objectives; as a way for
generating outputs while concurrently maximizing an objective function; and,
finally, as a way of embedding desired characteristics, which cannot be easily
captured by means of an objective function, into the generative process. We
conclude the survey with an in-depth discussion of the opportunities and
challenges in this fascinating emerging area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11339">ProAgent: Building Proactive Cooperative Agents with Large Language Models. (arXiv:2308.11339v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ceyao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kaijie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Siyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanghe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yihang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anji Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xiaojun Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Feng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yitao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>Building agents with adaptive behavior in cooperative tasks stands as a
paramount goal in the realm of multi-agent systems. Current approaches to
developing cooperative agents rely primarily on learning-based methods, whose
policy generalization depends heavily on the diversity of teammates they
interact with during the training phase. Such reliance, however, constrains the
agents' capacity for strategic adaptation when cooperating with unfamiliar
teammates, which becomes a significant challenge in zero-shot coordination
scenarios. To address this challenge, we propose ProAgent, a novel framework
that harnesses large language models (LLMs) to create proactive agents capable
of dynamically adapting their behavior to enhance cooperation with teammates.
ProAgent can analyze the present state, and infer the intentions of teammates
from observations. It then updates its beliefs in alignment with the teammates'
subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of
modularity and interpretability, making it easily integrated into various of
coordination scenarios. Experimental evaluations conducted within the
Overcooked-AI environment unveil the remarkable performance superiority of
ProAgent, outperforming five methods based on self-play and population-based
training when cooperating with AI agents. Furthermore, in partnered with human
proxy models, its performance exhibits an average improvement exceeding 10%
compared to the current state-of-the-art method. For more information about our
project, please visit~\url{https://pku-proagent.github.io}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14647">Edge Generation Scheduling for DAG Tasks Using Deep Reinforcement Learning. (arXiv:2308.14647v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1">Binqi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Theile_M/0/1/0/all/0/1">Mirco Theile</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Ziyuan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernardini_D/0/1/0/all/0/1">Daniele Bernardini</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1">Debayan Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastoni_A/0/1/0/all/0/1">Andrea Bastoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Caccamo_M/0/1/0/all/0/1">Marco Caccamo</a></p>
<p>Directed acyclic graph (DAG) tasks are currently adopted in the real-time
domain to model complex applications from the automotive, avionics, and
industrial domains that implement their functionalities through chains of
intercommunicating tasks. This paper studies the problem of scheduling
real-time DAG tasks by presenting a novel schedulability test based on the
concept of trivial schedulability. Using this schedulability test, we propose a
new DAG scheduling framework (edge generation scheduling -- EGS) that attempts
to minimize the DAG width by iteratively generating edges while guaranteeing
the deadline constraint. We study how to efficiently solve the problem of
generating edges by developing a deep reinforcement learning algorithm combined
with a graph representation neural network to learn an efficient edge
generation policy for EGS. We evaluate the effectiveness of the proposed
algorithm by comparing it with state-of-the-art DAG scheduling heuristics and
an optimal mixed-integer linear programming baseline. Experimental results show
that the proposed algorithm outperforms the state-of-the-art by requiring fewer
processors to schedule the same DAG tasks. The code is available at
https://github.com/binqi-sun/egs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03561">Trinary Decision Trees for handling missing data. (arXiv:2309.03561v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zakrisson_H/0/1/0/all/0/1">Henning Zakrisson</a></p>
<p>This paper introduces the Trinary decision tree, an algorithm designed to
improve the handling of missing data in decision tree regressors and
classifiers. Unlike other approaches, the Trinary decision tree does not assume
that missing values contain any information about the response. Both
theoretical calculations on estimator bias and numerical illustrations using
real data sets are presented to compare its performance with established
algorithms in different missing data scenarios (Missing Completely at Random
(MCAR), and Informative Missingness (IM)). Notably, the Trinary tree
outperforms its peers in MCAR settings, especially when data is only missing
out-of-sample, while lacking behind in IM settings. A hybrid model, the
TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes
(MIA) approach, shows robust performance in all types of missingness. Despite
the potential drawback of slower training speed, the Trinary tree offers a
promising and more accurate method of handling missing data in decision tree
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.03581">Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giovanelli_J/0/1/0/all/0/1">Joseph Giovanelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1">Alexander Tornede</a>, <a href="http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1">Tanja Tornede</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1">Marius Lindauer</a></p>
<p>Hyperparameter optimization (HPO) is important to leverage the full potential
of machine learning (ML). In practice, users are often interested in
multi-objective (MO) problems, i.e., optimizing potentially conflicting
objectives, like accuracy and energy consumption. To tackle this, the vast
majority of MO-ML algorithms return a Pareto front of non-dominated machine
learning models to the user. Optimizing the hyperparameters of such algorithms
is non-trivial as evaluating a hyperparameter configuration entails evaluating
the quality of the resulting Pareto front. In literature, there are known
indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by
quantifying different properties (e.g., volume, proximity to a reference
point). However, choosing the indicator that leads to the desired Pareto front
might be a hard task for a user. In this paper, we propose a human-centered
interactive HPO approach tailored towards multi-objective ML leveraging
preference learning to extract desiderata from users that guide the
optimization. Instead of relying on the user guessing the most suitable
indicator for their needs, our approach automatically learns an appropriate
indicator. Concretely, we leverage pairwise comparisons of distinct Pareto
fronts to learn such an appropriate quality indicator. Then, we optimize the
hyperparameters of the underlying MO-ML algorithm towards this learned
indicator using a state-of-the-art HPO approach. In an experimental study
targeting the environmental impact of ML, we demonstrate that our approach
leads to substantially better Pareto fronts compared to optimizing based on a
wrong indicator pre-selected by the user, and performs comparable in the case
of an advanced user knowing which indicator to pick.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04941">Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. (arXiv:2309.04941v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Junru Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiarui Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a></p>
<p>The ability of graph neural networks (GNNs) to count certain graph
substructures, especially cycles, is important for the success of GNNs on a
wide range of tasks. It has been recently used as a popular metric for
evaluating the expressive power of GNNs. Many of the proposed GNN models with
provable cycle counting power are based on subgraph GNNs, i.e., extracting a
bag of subgraphs from the input graph, generating representations for each
subgraph, and using them to augment the representation of the input graph.
However, those methods require heavy preprocessing, and suffer from high time
and memory costs. In this paper, we overcome the aforementioned limitations of
subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted
FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose
mutual distances are at most $d$ as the units for message passing to balance
the expressive power and complexity. By performing message passing among
distance-restricted node pairs in the original graph, $d$-DRFWL(2) GNNs avoid
the expensive subgraph extraction operations in subgraph GNNs, making both the
time and space complexity lower. We theoretically show that the discriminative
power of $d$-DRFWL(2) GNNs strictly increases as $d$ increases. More
importantly, $d$-DRFWL(2) GNNs have provably strong cycle counting power even
with $d=2$: they can count all 3, 4, 5, 6-cycles. Since 6-cycles (e.g., benzene
rings) are ubiquitous in organic molecules, being able to detect and count them
is crucial for achieving robust and generalizable performance on molecular
tasks. Experiments on both synthetic datasets and molecular datasets verify our
theory. To the best of our knowledge, our model is the most efficient GNN model
to date (both theoretically and empirically) that can count up to 6-cycles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07156">Transparency in Sleep Staging: Deep Learning Method for EEG Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1">Shivam Sharma</a>, <a href="http://arxiv.org/find/eess/1/au:+Maiti_S/0/1/0/all/0/1">Suvadeep Maiti</a>, <a href="http://arxiv.org/find/eess/1/au:+Mythirayee_S/0/1/0/all/0/1">S. Mythirayee</a>, <a href="http://arxiv.org/find/eess/1/au:+Rajendran_S/0/1/0/all/0/1">Srijithesh Rajendran</a>, <a href="http://arxiv.org/find/eess/1/au:+Bapi_R/0/1/0/all/0/1">Raju Surampudi Bapi</a></p>
<p>Automated Sleep stage classification using raw single channel EEG is a
critical tool for sleep quality assessment and disorder diagnosis. However,
modelling the complexity and variability inherent in this signal is a
challenging task, limiting their practicality and effectiveness in clinical
settings. To mitigate these challenges, this study presents an end-to-end deep
learning (DL) model which integrates squeeze and excitation blocks within the
residual network to extract features and stacked Bi-LSTM to understand complex
temporal dependencies. A distinctive aspect of this study is the adaptation of
GradCam for sleep staging, marking the first instance of an explainable DL
model in this domain with alignment of its decision-making with sleep expert's
insights. We evaluated our model on the publically available datasets
(SleepEDF-20, SleepEDF-78, and SHHS), achieving Macro-F1 scores of 82.5, 78.9,
and 81.9, respectively. Additionally, a novel training efficiency enhancement
strategy was implemented by increasing stride size, leading to 8x faster
training times with minimal impact on performance. Comparative analyses
underscore our model outperforms all existing baselines, indicating its
potential for clinical usage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.07207">EarthPT: a time series foundation model for Earth Observation. (arXiv:2309.07207v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1">Michael J. Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Fleming_L/0/1/0/all/0/1">Luke Fleming</a>, <a href="http://arxiv.org/find/cs/1/au:+Geach_J/0/1/0/all/0/1">James E. Geach</a></p>
<p>We introduce EarthPT -- an Earth Observation (EO) pretrained transformer.
EarthPT is a 700 million parameter decoding transformer foundation model
trained in an autoregressive self-supervised manner and developed specifically
with EO use-cases in mind. We demonstrate that EarthPT is an effective
forecaster that can accurately predict future pixel-level surface reflectances
across the 400-2300 nm range well into the future. For example, forecasts of
the evolution of the Normalised Difference Vegetation Index (NDVI) have a
typical error of approximately 0.05 (over a natural range of -1 -&gt; 1) at the
pixel level over a five month test set horizon, out-performing simple
phase-folded models based on historical averaging. We also demonstrate that
embeddings learnt by EarthPT hold semantically meaningful information and could
be exploited for downstream tasks such as highly granular, dynamic land use
classification. Excitingly, we note that the abundance of EO data provides us
with -- in theory -- quadrillions of training tokens. Therefore, if we assume
that EarthPT follows neural scaling laws akin to those derived for Large
Language Models (LLMs), there is currently no data-imposed limit to scaling
EarthPT and other similar `Large Observation Models.'
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11526">Likelihood-based Sensor Calibration using Affine Transformation. (arXiv:2309.11526v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Machhamer_R/0/1/0/all/0/1">R&#xfc;diger Machhamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazlic_L/0/1/0/all/0/1">Lejla Begic Fazlic</a>, <a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1">Eray Guven</a>, <a href="http://arxiv.org/find/cs/1/au:+Junk_D/0/1/0/all/0/1">David Junk</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1">Gunes Karabulut Kurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Naumann_S/0/1/0/all/0/1">Stefan Naumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Didas_S/0/1/0/all/0/1">Stephan Didas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollmer_K/0/1/0/all/0/1">Klaus-Uwe Gollmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergmann_R/0/1/0/all/0/1">Ralph Bergmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Timm_I/0/1/0/all/0/1">Ingo J. Timm</a>, <a href="http://arxiv.org/find/cs/1/au:+Dartmann_G/0/1/0/all/0/1">Guido Dartmann</a></p>
<p>An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. The results demonstrate the
adaptability of this solution for various applications, including software
calibration of sensors, implementation of expert-based adaptation, and paving
the way for future advancements such as distributed learning methods. One idea
here is to use the knowledge of experts for estimating an affine transformation
between different systems. We evaluate our research with simulations and also
with real measured data of a multi-sensor board with 8 identical sensors. Both
data set and evaluation script are provided for download. The results show an
improvement for both the simulation and the experiments with real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06059">Early Warning Prediction with Automatic Labeling in Epilepsy Patients. (arXiv:2310.06059v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Ting Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jin Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jinqiao Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1">Sergey Nikolenko</a></p>
<p>Early warning for epilepsy patients is crucial for their safety and
well-being, in particular to prevent or minimize the severity of seizures.
Through the patients' EEG data, we propose a meta learning framework to improve
the prediction of early ictal signals. The proposed bi-level optimization
framework can help automatically label noisy data at the early ictal stage, as
well as optimize the training accuracy of the backbone model. To validate our
approach, we conduct a series of experiments to predict seizure onset in
various long-term windows, with LSTM and ResNet implemented as the baseline
models. Our study demonstrates that not only the ictal prediction accuracy
obtained by meta learning is significantly improved, but also the resulting
model captures some intrinsic patterns of the noisy data that a single backbone
model could not learn. As a result, the predicted probability generated by the
meta network serves as a highly effective early warning indicator.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06266">CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Di_P/0/1/0/all/0/1">Peng Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Wenting Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1">Gang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1">Jie Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zi Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1">Tingting Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1">Zhichao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Ting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1">Ming Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Cong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingchang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiachen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shaojun Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1">Min Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangpei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhaogui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiawei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qing Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Gehao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zelin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xunjin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hailian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lifu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xianying Zhu</a></p>
<p>Code Large Language Models (Code LLMs) have gained significant attention in
the industry due to their wide applications in the full lifecycle of software
engineering. However, the effectiveness of existing models in understanding
non-English inputs for multi-lingual code-related tasks is still far from well
studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code
LLM. It is specifically designed for code-related tasks with both English and
Chinese prompts and supports over 40 programming languages. CodeFuse achieves
its effectiveness by utilizing a high quality pre-training dataset that is
carefully filtered by program analyzers and optimized during the training
process. Extensive experiments are conducted using real-world usage scenarios,
the industry-standard benchmark HumanEval-x, and the specially designed
CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we
actively collected valuable human feedback from the AntGroup's software
development process where CodeFuse has been successfully deployed. The results
demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%,
positioning it as one of the top multi-lingual code LLMs with similar parameter
sizes. In practical scenarios, such as code generation, code translation, code
comments, and testcase generation, CodeFuse performs better than other models
when confronted with Chinese prompts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10898">Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection. (arXiv:2310.10898v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aref_S/0/1/0/all/0/1">Samin Aref</a>, <a href="http://arxiv.org/find/cs/1/au:+Mostajabdaveh_M/0/1/0/all/0/1">Mahdi Mostajabdaveh</a></p>
<p>Community detection, which involves partitioning nodes within a network, has
widespread applications across computational sciences. Modularity-based
algorithms identify communities by attempting to maximize the modularity
function across network node partitions. Our study assesses the performance of
various modularity-based algorithms in obtaining optimal partitions. Our
analysis utilizes 104 networks, including both real-world instances from
diverse contexts and modular graphs from two families of synthetic benchmarks.
We analyze ten inexact modularity-based algorithms against the exact integer
programming baseline that globally optimizes modularity. Our comparative
analysis includes eight heuristics, two variants of a graph neural network
algorithm, and nine variations of the Bayan approximation algorithm.
</p>
<p>Our findings reveal that the average modularity-based heuristic yields
optimal partitions in only 43.9% of the 104 networks analyzed. Graph neural
networks and approximate Bayan, on average, achieve optimality on 68.7% and
82.3% of the networks respectively. Additionally, our analysis of three
partition similarity metrics exposes substantial dissimilarities between
high-modularity sub-optimal partitions and any optimal partition of the
networks. We observe that near-optimal partitions are often disproportionately
dissimilar to any optimal partition. Taken together, our analysis points to a
crucial limitation of the commonly used modularity-based methods: they rarely
produce an optimal partition or a partition resembling an optimal partition
even on networks with modular structures. If modularity is to be used for
detecting communities, we recommend approximate optimization algorithms for a
more methodologically sound usage of modularity within its applicability
limits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18716">Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding. (arXiv:2310.18716v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jiangyan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yisen Wang</a></p>
<p>Spectral embedding is a powerful graph embedding technique that has received
a lot of attention recently due to its effectiveness on Graph Transformers.
However, from a theoretical perspective, the universal expressive power of
spectral embedding comes at the price of losing two important invariance
properties of graphs, sign and basis invariance, which also limits its
effectiveness on graph data. To remedy this issue, many previous methods
developed costly approaches to learn new invariants and suffer from high
computation complexity. In this work, we explore a minimal approach that
resolves the ambiguity issues by directly finding canonical directions for the
eigenvectors, named Laplacian Canonization (LC). As a pure pre-processing
method, LC is light-weighted and can be applied to any existing GNNs. We
provide a thorough investigation, from theory to algorithm, on this approach,
and discover an efficient algorithm named Maximal Axis Projection (MAP) that
works for both sign and basis invariance and successfully canonizes more than
90% of all eigenvectors. Experiments on real-world benchmark datasets like
ZINC, MOLTOX21, and MOLPCBA show that MAP consistently outperforms existing
methods while bringing minimal computation overhead. Code is available at
https://github.com/PKU-ML/LaplacianCanonization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.00636">Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures. (arXiv:2311.00636v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eschenhagen_R/0/1/0/all/0/1">Runa Eschenhagen</a>, <a href="http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1">Alexander Immer</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1">Richard E. Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_F/0/1/0/all/0/1">Frank Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1">Philipp Hennig</a></p>
<p>The core components of many modern neural network architectures, such as
transformers, convolutional, or graph neural networks, can be expressed as
linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate
Curvature (K-FAC), a second-order optimisation method, has shown promise to
speed up neural network training and thereby reduce computational costs.
However, there is currently no framework to apply it to generic architectures,
specifically ones with linear weight-sharing layers. In this work, we identify
two different settings of linear weight-sharing layers which motivate two
flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they
are exact for deep linear networks with weight-sharing in their respective
setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we
leverage to speed up automatic hyperparameter selection via optimising the
marginal likelihood for a Wide ResNet. Finally, we observe little difference
between these two K-FAC variations when using them to train both a graph neural
network and a vision transformer. However, both variations are able to reach a
fixed validation metric target in $50$-$75\%$ of the number of steps of a
first-order reference run, which translates into a comparable improvement in
wall-clock time. This highlights the potential of applying K-FAC to modern
neural network architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02790">CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_I/0/1/0/all/0/1">Ishan Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1">Zhijing Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1">Ehsan Mokhtarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Siyuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>Evaluating the significance of a paper is pivotal yet challenging for the
scientific community. While the citation count is the most commonly used proxy
for this purpose, they are widely criticized for failing to accurately reflect
a paper's true impact. In this work, we propose a causal inference method,
TextMatch, which adapts the traditional matching framework to high-dimensional
text embeddings. Specifically, we encode each paper using the text embeddings
by large language models (LLMs), extract similar samples by cosine similarity,
and synthesize a counterfactual sample by the weighted average of similar
papers according to their similarity values. We apply the resulting metric,
called CausalCite, as a causal formulation of paper citations. We show its
effectiveness on various criteria, such as high correlation with paper impact
as reported by scientific experts on a previous dataset of 1K papers,
(test-of-time) awards for past papers, and its stability across various
sub-fields of AI. We also provide a set of findings that can serve as suggested
ways for future researchers to use our metric for a better understanding of a
paper's quality. Our code and data are at
https://github.com/causalNLP/causal-cite.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09229">Developing a Novel Holistic, Personalized Dementia Risk Prediction Model via Integration of Machine Learning and Network Systems Biology Approaches. (arXiv:2311.09229v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Mamidala_S/0/1/0/all/0/1">Srilekha Mamidala</a></p>
<p>The prevalence of dementia has increased over time as global life expectancy
improves and populations age. An individual's risk of developing dementia is
influenced by various genetic, lifestyle, and environmental factors, among
others. Predicting dementia risk may enable individuals to employ mitigation
strategies or lifestyle changes to delay dementia onset. Current computational
approaches to dementia prediction only return risk upon narrow categories of
variables and do not account for interactions between different risk variables.
The proposed framework utilizes a novel holistic approach to dementia risk
prediction and is the first to incorporate various sources of tabular
environmental pollution and lifestyle factor data with network systems
biology-based genetic data. LightGBM gradient boosting was employed to ensure
validity of included factors. This approach successfully models interactions
between variables through an original weighted integration method coined
Sysable. Multiple machine learning models trained the algorithm to reduce
reliance on a single model. The developed approach surpassed all existing
dementia risk prediction approaches, with a sensitivity of 85%, specificity of
99%, geometric accuracy of 92%, and AUROC of 91.7%. A transfer learning model
was implemented as well. De-biasing algorithms were run on the model via the AI
Fairness 360 Library. Effects of demographic disparities on dementia prevalence
were analyzed to potentially highlight areas in need and promote equitable and
accessible care. The resulting model was additionally integrated into a
user-friendly app providing holistic predictions and personalized risk
mitigation strategies. The developed model successfully employs holistic
computational dementia risk prediction for clinical use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13326">Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1">Woosung Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1">Insu Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yuntae Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Gimin Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woo Chang Kim</a></p>
<p>Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.15816">Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale. (arXiv:2311.15816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Soyed Tuhin Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Danouchi_K/0/1/0/all/0/1">Kamal Danouchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hefenbrock_M/0/1/0/all/0/1">Michael Hefenbrock</a>, <a href="http://arxiv.org/find/cs/1/au:+Prenat_G/0/1/0/all/0/1">Guillaume Prenat</a>, <a href="http://arxiv.org/find/cs/1/au:+Anghel_L/0/1/0/all/0/1">Lorena Anghel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tahoori_M/0/1/0/all/0/1">Mehdi B. Tahoori</a></p>
<p>Uncertainty estimation in Neural Networks (NNs) is vital in improving
reliability and confidence in predictions, particularly in safety-critical
applications. Bayesian Neural Networks (BayNNs) with Dropout as an
approximation offer a systematic approach to quantifying uncertainty, but they
inherently suffer from high hardware overhead in terms of power, memory, and
computation. Thus, the applicability of BayNNs to edge devices with limited
resources or to high-performance applications is challenging. Some of the
inherent costs of BayNNs can be reduced by accelerating them in hardware on a
Computation-In-Memory (CIM) architecture with spintronic memories and
binarizing their parameters. However, numerous stochastic units are required to
implement conventional dropout-based BayNN. In this paper, we propose the Scale
Dropout, a novel regularization technique for Binary Neural Networks (BNNs),
and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient
uncertainty estimation. Our approach requires only one stochastic unit for the
entire model, irrespective of the model size, leading to a highly scalable
Bayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM
architecture for the proposed BayNN that achieves more than $100\times$ energy
savings compared to the state-of-the-art. We validated our method to show up to
a $1\%$ improvement in predictive performance and superior uncertainty
estimates compared to related works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17929">New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance. (arXiv:2311.17929v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DuPont_Q/0/1/0/all/0/1">Quinn DuPont</a></p>
<p>This research examines the polycentric governance of digital assets in
blockchain-based Decentralized Autonomous Organizations (DAOs). It offers a
theoretical framework and addresses a critical challenge facing decentralized
governance by developing a method to identify sybils, or spurious identities.
The method uses graph deep learning techniques to identify sybil activity in a
DAO governance dataset (snapshot.org). Specifically, a Graph Convolutional
Neural Network (GCNN) learned voting behaviours and a fast k-means vector
clustering algorithm (FAISS) used the high dimensional embeddings to identify
similar nodes in a graph. The results reveal that deep learning can effectively
identify sybils, reducing the voting graph by 2-5%. This research underscores
the importance of sybil resistance in DAOs and offers a novel perspective on
decentralized governance, informing future policy, regulation, and governance
practices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.18252">Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dawen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1">Boming Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1">Thong Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhenchang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1">Mark Staples</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qinghua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Liming Zhu</a></p>
<p>The advent of Generative AI has marked a significant milestone in artificial
intelligence, demonstrating remarkable capabilities in generating realistic
images, texts, and data patterns. However, these advancements come with
heightened concerns over data privacy and copyright infringement, primarily due
to the reliance on vast datasets for model training. Traditional approaches
like differential privacy, machine unlearning, and data poisoning only offer
fragmented solutions to these complex issues. Our paper delves into the
multifaceted challenges of privacy and copyright protection within the data
lifecycle. We advocate for integrated approaches that combines technical
innovation with ethical foresight, holistically addressing these concerns by
investigating and devising solutions that are informed by the lifecycle
perspective. This work aims to catalyze a broader discussion and inspire
concerted efforts towards data privacy and copyright integrity in Generative
AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.00840">Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Cheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">LianLi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Heng Tao Shen</a></p>
<p>Catastrophic Forgetting (CF) is a prominent issue in continual learning.
Parameter isolation addresses this challenge by masking a sub-network for each
task to mitigate interference with old tasks. However, these sub-networks are
constructed relying on weight magnitude, which does not necessarily correspond
to the importance of weights, resulting in maintaining unimportant weights and
constructing redundant sub-networks. To overcome this limitation, inspired by
information bottleneck, which removes redundancy between adjacent network
layers, we propose \textbf{\underline{I}nformation \underline{B}ottleneck
\underline{M}asked sub-network (IBM)} to eliminate redundancy within
sub-networks. Specifically, IBM accumulates valuable information into essential
weights to construct redundancy-free sub-networks, not only effectively
mitigating CF by freezing the sub-networks but also facilitating new tasks
training through the transfer of valuable knowledge. Additionally, IBM
decomposes hidden representations to automate the construction process and make
it flexible. Extensive experiments demonstrate that IBM consistently
outperforms state-of-the-art methods. Notably, IBM surpasses the
state-of-the-art parameter isolation method with a 70\% reduction in the number
of parameters within sub-networks and an 80\% decrease in training time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02133">Style Aligned Image Generation via Shared Attention. (arXiv:2312.02133v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1">Amir Hertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1">Shlomi Fruchter</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a></p>
<p>Large-scale Text-to-Image (T2I) models have rapidly gained prominence across
creative fields, generating visually compelling outputs from textual prompts.
However, controlling these models to ensure consistent style remains
challenging, with existing methods necessitating fine-tuning and manual
intervention to disentangle content and style. In this paper, we introduce
StyleAligned, a novel technique designed to establish style alignment among a
series of generated images. By employing minimal `attention sharing' during the
diffusion process, our method maintains style consistency across images within
T2I models. This approach allows for the creation of style-consistent images
using a reference style through a straightforward inversion operation. Our
method's evaluation across diverse styles and text prompts demonstrates
high-quality synthesis and fidelity, underscoring its efficacy in achieving
consistent style across various inputs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05720">Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a></p>
<p>Federated learning (FL) emphasizes decentralized training by storing data
locally and sending only model updates, underlining user privacy. Recently, a
line of works on privacy attacks impairs user privacy by extracting sensitive
training text from language models in the context of FL. Yet, these attack
techniques face distinct hurdles: some work chiefly with limited batch sizes
(e.g., batch size of 1), and others are easily detectable. This paper
introduces an innovative approach that is challenging to detect, significantly
enhancing the recovery rate of text in various batch-size settings. Building on
fundamental gradient matching and domain prior knowledge, we enhance the attack
by recovering the input of the Pooler layer of language models, which enables
us to provide additional supervised signals at the feature level. Unlike
gradient data, these signals do not average across sentences and tokens,
thereby offering more nuanced and effective insights. We benchmark our method
using text classification tasks on datasets such as CoLA, SST-2, and Rotten
Tomatoes. Across different batch sizes and models, our approach consistently
outperforms previous state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08567">ConFormer: A Novel Collection of Deep Learning Models to Assist Cardiologists in the Assessment of Cardiac Function. (arXiv:2312.08567v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Thomas_E/0/1/0/all/0/1">Ethan Thomas</a>, <a href="http://arxiv.org/find/eess/1/au:+Aslam_S/0/1/0/all/0/1">Salman Aslam</a></p>
<p>Cardiovascular diseases, particularly heart failure, are a leading cause of
death globally. The early detection of heart failure through routine
echocardiogram screenings is often impeded by the high cost and labor-intensive
nature of these procedures, a barrier that can mean the difference between life
and death. This paper presents ConFormer, a novel deep learning model designed
to automate the estimation of Ejection Fraction (EF) and Left Ventricular Wall
Thickness from echocardiograms. The implementation of ConFormer has the
potential to enhance preventative cardiology by enabling cost-effective,
accessible, and comprehensive heart health monitoring, thereby saving countless
lives. The source code is available at https://github.com/Aether111/ConFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10813">Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1">Tianxiang Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1">Mengyao Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Sicheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jungong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1">Guiguang Ding</a></p>
<p>With the development of large pre-trained vision-language models, how to
effectively transfer the knowledge of such foundational models to downstream
tasks becomes a hot topic, especially in a data-deficient scenario. Recently,
prompt tuning has become a popular solution. When adapting the vision-language
models, researchers freeze the parameters in the backbone and only design and
tune the prompts. On the one hand, the delicate design of prompt tuning
exhibits strong performance. On the other hand, complicated structures and
update rules largely increase the computation and storage cost. Motivated by
the observation that the evolution pattern of the generalization capability in
visual-language models aligns harmoniously with the trend of rank variations in
the prompt matrix during adaptation, we design a new type of prompt,
Re-parameterized Low-rank Prompt (RLP), for both efficient and effective
adaptation. Our method could largely reduce the number of tunable parameters
and storage space, which is quite beneficial in resource-limited scenarios.
Extensive experiments further demonstrate the superiority of RLP. In
particular, RLP shows comparable or even stronger performance than the latest
state-of-the-art methods with an extremely small number of parameters. On a
series of tasks over 11 datasets, RLP significantly increases the average
downstream accuracy of classic prompt tuning by up to 5.25% using merely 0.5K
parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11747">Robust Stochastic Graph Generator for Counterfactual Explanations. (arXiv:2312.11747v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prado_Romero_M/0/1/0/all/0/1">Mario Alfonso Prado-Romero</a>, <a href="http://arxiv.org/find/cs/1/au:+Prenkaj_B/0/1/0/all/0/1">Bardh Prenkaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Stilo_G/0/1/0/all/0/1">Giovanni Stilo</a></p>
<p>Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16483">Expressivity and Approximation Properties of Deep Neural Networks with ReLU$^k$ Activation. (arXiv:2312.16483v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Juncai He</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1">Tong Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jinchao Xu</a></p>
<p>In this paper, we investigate the expressivity and approximation properties
of deep neural networks employing the ReLU$^k$ activation function for $k \geq
2$. Although deep ReLU networks can approximate polynomials effectively, deep
ReLU$^k$ networks have the capability to represent higher-degree polynomials
precisely. Our initial contribution is a comprehensive, constructive proof for
polynomial representation using deep ReLU$^k$ networks. This allows us to
establish an upper bound on both the size and count of network parameters.
Consequently, we are able to demonstrate a suboptimal approximation rate for
functions from Sobolev spaces as well as for analytic functions. Additionally,
through an exploration of the representation power of deep ReLU$^k$ networks
for shallow networks, we reveal that deep ReLU$^k$ networks can approximate
functions from a range of variation spaces, extending beyond those generated
solely by the ReLU$^k$ activation function. This finding demonstrates the
adaptability of deep ReLU$^k$ networks in approximating functions within
various variation spaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02501">The cell signaling structure function. (arXiv:2401.02501v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aho_L/0/1/0/all/0/1">Layton Aho</a>, <a href="http://arxiv.org/find/cs/1/au:+Winter_M/0/1/0/all/0/1">Mark Winter</a>, <a href="http://arxiv.org/find/cs/1/au:+DeCarlo_M/0/1/0/all/0/1">Marc DeCarlo</a>, <a href="http://arxiv.org/find/cs/1/au:+Frismantiene_A/0/1/0/all/0/1">Agne Frismantiene</a>, <a href="http://arxiv.org/find/cs/1/au:+Blum_Y/0/1/0/all/0/1">Yannick Blum</a>, <a href="http://arxiv.org/find/cs/1/au:+Gagliardi_P/0/1/0/all/0/1">Paolo Armando Gagliardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pertz_O/0/1/0/all/0/1">Olivier Pertz</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1">Andrew R. Cohen</a></p>
<p>Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here an approach
to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell
microscopy movies unique in requiring no a priori knowledge of expected pattern
dynamics, and no training data. The proposed cell signaling structure function
(SSF) is a Kolmogorov structure function that optimally measures cell signaling
state as nuclear intensity w.r.t. surrounding cytoplasm, a significant
improvement compared to the current state-of-the-art cytonuclear ratio. SSF
kymographs store at each spatiotemporal cell centroid the SSF value, or a
functional output such as velocity. Patterns of similarity are identified via
the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input SSF kymographs as points
in a low dimensional embedding that optimally captures the pattern similarity
identified by the NCD throughout the space. The only parameter is the expected
cell radii ($\mu m$). A new formulation of the cluster structure function
optimally estimates how meaningful an embedding from the RKHS representation.
Results are presented quantifying the impact of ERK and AKT signaling between
different oncogenic mutations, and by the relation between ERK signaling and
cellular velocity patterns for movies of 2-D monolayers of human breast
epithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation
of ERK, and human induced pluripotent stem cells .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03302">Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hashemi_S/0/1/0/all/0/1">Seyed Mohammad Hossein Hashemi</a>, <a href="http://arxiv.org/find/eess/1/au:+Safari_L/0/1/0/all/0/1">Leila Safari</a>, <a href="http://arxiv.org/find/eess/1/au:+Taromi_A/0/1/0/all/0/1">Amirhossein Dadashzade Taromi</a></p>
<p>In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03621">Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI. (arXiv:2401.03621v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ellethy_H/0/1/0/all/0/1">Hanem Ellethy</a>, <a href="http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1">Shekhar S. Chandra</a>, <a href="http://arxiv.org/find/eess/1/au:+Vegh_V/0/1/0/all/0/1">Viktor Vegh</a></p>
<p>Traumatic Brain Injury (TBI) poses a significant global public health
challenge, contributing to high morbidity and mortality rates and placing a
substantial economic burden on healthcare systems worldwide. The diagnosis of
TBI relies on clinical information along with Computed Tomography (CT) scans.
Addressing the multifaceted challenges posed by TBI has seen the development of
innovative, data-driven approaches, for this complex condition. Particularly
noteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority
of TBI cases where conventional methods often fall short. As such, we review
the state-of-the-art Machine Learning (ML) techniques applied to clinical
information and CT scans in TBI, with a particular focus on mTBI. We categorize
ML applications based on their data sources, and there is a spectrum of ML
techniques used to date. Most of these techniques have primarily focused on
diagnosis, with relatively few attempts at predicting the prognosis. This
review may serve as a source of inspiration for future research studies aimed
at improving the diagnosis of TBI using data-driven approaches and standard
diagnostic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03786">Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wachi_A/0/1/0/all/0/1">Akifumi Wachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_W/0/1/0/all/0/1">Wataru Hashimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1">Kazumune Hashimoto</a></p>
<p>Safety is an indispensable requirement for applying reinforcement learning
(RL) to real problems. Although there has been a surge of safe RL algorithms
proposed in recent years, most existing work typically 1) relies on receiving
numeric safety feedback; 2) does not guarantee safety during the learning
process; 3) limits the problem to a priori known, deterministic transition
dynamics; and/or 4) assume the existence of a known safe policy for any states.
Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback
Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision
processes (CMDPs) with binary safety feedback and an unknown, stochastic state
transition function. LoBiSaRL optimizes a policy to maximize rewards while
guaranteeing a long-term safety that an agent executes only safe state-action
pairs throughout each episode with high probability. Specifically, LoBiSaRL
models the binary safety function via a generalized linear model (GLM) and
conservatively takes only a safe action at every time step while inferring its
effect on future safety under proper assumptions. Our theoretical results show
that LoBiSaRL guarantees the long-term safety constraint, with high
probability. Finally, our empirical results demonstrate that our algorithm is
safer than existing methods without significantly compromising performance in
terms of reward.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.03955">Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ekambaram_V/0/1/0/all/0/1">Vijay Ekambaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Jati_A/0/1/0/all/0/1">Arindam Jati</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nam H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dayama_P/0/1/0/all/0/1">Pankaj Dayama</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1">Chandra Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gifford_W/0/1/0/all/0/1">Wesley M. Gifford</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalagnanam_J/0/1/0/all/0/1">Jayant Kalagnanam</a></p>
<p>Large Pretrained models for zero/few-shot learning excel in language and
vision domains but encounter challenges in multivariate time series (TS) due to
the diverse nature and scarcity of publicly available pretraining data.
Consequently, there has been a recent surge in utilizing pretrained large
language models (LLMs) with various adaptations for time series forecasting.
These approaches employ cross-domain transfer learning and surprisingly yield
impressive results. However, these models are typically very slow and large
($\sim$billion parameters) and do not consider cross-channel correlations. To
address this, we present Multi-level Tiny Time Mixers (TTM), a significantly
small model based on the lightweight TSMixer architecture. TTM marks the first
success in developing tiny general-pretrained models ($\le$1 million
parameters), exclusively trained on public TS datasets in a flash of just 4-8
hrs with effective transfer learning capabilities for forecasting. To tackle
the complexity of pretraining on multiple datasets with varied temporal
resolutions, we introduce several novel enhancements such as adaptive patching,
dataset augmentation via downsampling, and resolution prefix tuning. Moreover,
we employ a multi-level modeling strategy to effectively model channel
correlations and incorporate exogenous signals during fine-tuning, a crucial
capability lacking in existing benchmarks. TTM excels in few/zero-shot
forecasting, demonstrating significant accuracy gains (12-38%) over existing
benchmarks. Further, it achieves a remarkable 14-106X reduction in model
parameters, enabling 54-65X faster finetuning/inference as compared to the
LLM-TS benchmarks. In fact, TTM's zero-shot often surpasses the few-shot
results in many popular benchmarks, highlighting the efficacy of our approach.
Code and Pretrained Models will be open-sourced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04679">RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1">Mahdi Nikdan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1">Soroush Tabesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a></p>
<p>We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
$\href{https://github.com/IST-DASLab/RoSA}{\text{our github page}}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.04847">On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm. (arXiv:2401.04847v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Won_J/0/1/0/all/0/1">Joong-Ho Won</a>, <a href="http://arxiv.org/find/stat/1/au:+Jung_J/0/1/0/all/0/1">Jihan Jung</a></p>
<p>This paper presents an in-depth analysis of the generalized isotonic
recursive partitioning (GIRP) algorithm for fitting isotonic models under
separable convex losses, proposed by Luss and Rosset [J. Comput. Graph.
Statist., 23 (2014), pp. 192--201] for differentiable losses and extended by
Painsky and Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp.
308-321] for nondifferentiable losses. The GIRP algorithm poseses an attractive
feature that in each step of the algorithm, the intermediate solution satisfies
the isotonicity constraint. The paper begins with an example showing that the
GIRP algorithm as described in the literature may fail to produce an isotonic
model, suggesting that the existence and uniqueness of the solution to the
isotonic regression problem must be carefully addressed. It proceeds with
showing that, among possibly many solutions, there indeed exists a solution
that can be found by recursive binary partitioning of the set of observed data.
A small modification of the GIRP algorithm suffices to obtain a correct
solution and preserve the desired property that all the intermediate solutions
are isotonic. This proposed modification includes a proper choice of
intermediate solutions and a simplification of the partitioning step from
ternary to binary.
</p>
</p>
</div>

    </div>
    </body>
    