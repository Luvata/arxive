<!DOCTYPE html>
<html>
<head>
<title>2024-01-27-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.13697">Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xianbing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1">Soujanya Poria</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuejiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yixin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1">Buzhou Tang</a></p>
<p>Existing multimodal sentiment analysis tasks are highly rely on the
assumption that the training and test sets are complete multimodal data, while
this assumption can be difficult to hold: the multimodal data are often
incomplete in real-world scenarios. Therefore, a robust multimodal model in
scenarios with randomly missing modalities is highly preferred. Recently,
CLIP-based multimodal foundational models have demonstrated impressive
performance on numerous multimodal tasks by learning the aligned cross-modal
semantics of image and text pairs, but the multimodal foundational models are
also unable to directly address scenarios involving modality absence. To
alleviate this issue, we propose a simple and effective framework, namely TRML,
Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML
employs generated virtual modalities to replace missing modalities, and aligns
the semantic spaces between the generated and missing modalities. Concretely,
we design a missing modality inference module to generate virtual modaliites
and replace missing modalities. We also design a semantic matching learning
module to align semantic spaces generated and missing modalities. Under the
prompt of complete modality, our model captures the semantics of missing
modalities by leveraging the aligned cross-modal semantic space. Experiments
demonstrate the superiority of our approach on three multimodal sentiment
analysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13782">Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weissburg_I/0/1/0/all/0/1">Iain Xie Weissburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_M/0/1/0/all/0/1">Mehir Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Liangming Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a></p>
<p>As the number of accepted papers at AI and ML conferences reaches into the
thousands, it has become unclear how researchers access and read research
publications. In this paper, we investigate the role of social media
influencers in enhancing the visibility of machine learning research,
particularly the citation counts of papers they share. We have compiled a
comprehensive dataset of over 8,000 papers, spanning tweets from December 2018
to October 2023, alongside 1:1 matched controls based on publication year,
venue, and abstract topics. Our analysis reveals a significant increase in
citations for papers endorsed by these influencers, with median citation counts
2-3 times higher than those of the control group. Additionally, the study
delves into the geographic, gender, and institutional diversity of highlighted
authors. These findings highlight the expanding influence of social media in
scholarly communication and underscore the importance of an evolving ecosystem
in today's digital academic landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13789">A Unified Approach to Emotion Detection and Task-Oriented Dialogue Modeling. (arXiv:2401.13789v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stricker_A/0/1/0/all/0/1">Armand Stricker</a>, <a href="http://arxiv.org/find/cs/1/au:+Paroubek_P/0/1/0/all/0/1">Patrick Paroubek</a></p>
<p>In current text-based task-oriented dialogue (TOD) systems, user emotion
detection (ED) is often overlooked or is typically treated as a separate and
independent task, requiring additional training. In contrast, our work
demonstrates that seamlessly unifying ED and TOD modeling brings about mutual
benefits, and is therefore an alternative to be considered. Our method consists
in augmenting SimpleToD, an end-to-end TOD system, by extending belief state
tracking to include ED, relying on a single language model. We evaluate our
approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ
annotated with emotions. Our results reveal a general increase in performance
for ED and task results. Our findings also indicate that user emotions provide
useful contextual conditioning for system responses, and can be leveraged to
further refine responses in terms of empathy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13802">Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khajezade_M/0/1/0/all/0/1">Mohamad Khajezade</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1">Fatemeh Hendijani Fard</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Perez_G/0/1/0/all/0/1">Gema Rodr&#xed;guez-P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1">Mohamed Sami Shehata</a></p>
<p>Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. %\textbf{Goal:}
GPT-based models are one of the popular ones studied for tasks such as code
comment generation or test generation. These tasks are `generative' tasks.
However, there is limited research on the usage of LLMs for `non-generative'
tasks such as classification using the prompt-based paradigm. In this
preliminary exploratory study, we investigated the applicability of LLMs for
Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By
building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we
first investigated two different prompts using ChatGPT to detect
\textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a
zero-shot setting. We \textcolor{black}{then} conducted an analysis to
understand the strengths and weaknesses of ChatGPT in CCD. %\textbf{Results:}
ChatGPT surpasses the baselines in cross-language CCD
\textcolor{black}{attaining an F1-score of 0.877 } and achieves comparable
performance to fully fine-tuned models for mono-lingual CCD,
\textcolor{black}{with an F1-score of 0.878}. Also, the
\textcolor{black}{prompt and the} difficulty level of the problems has an
impact on the performance of ChatGPT. \textcolor{black}{Finally,} we provide
insights and future directions based on our initial analysis
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13810">Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4. (arXiv:2401.13810v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuchao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Supriyo Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_C/0/1/0/all/0/1">Chetan Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rujia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Minghua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yu Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a></p>
<p>Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis
process for cloud services, requiring on-call engineers to identify the primary
issues and implement corrective actions to prevent future recurrences.
Improving the incident RCA process is vital for minimizing service downtime,
customer impact and manual toil. Recent advances in artificial intelligence
have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which
have proven effective in tackling various AIOps problems, ranging from code
authoring to incident management. Nonetheless, the GPT-4 model's immense size
presents challenges when trying to fine-tune it on user data because of the
significant GPU resource demand and the necessity for continuous model
fine-tuning with the emergence of new data. To address the high cost of
fine-tuning LLM, we propose an in-context learning approach for automated root
causing, which eliminates the need for fine-tuning. We conduct extensive study
over 100,000 production incidents, comparing several large language models
using multiple metrics. The results reveal that our in-context learning
approach outperforms the previous fine-tuned large language models such as
GPT-3 by an average of 24.8\% across all metrics, with an impressive 49.7\%
improvement over the zero-shot model. Moreover, human evaluation involving
actual incident owners demonstrates its superiority over the fine-tuned model,
achieving a 43.5\% improvement in correctness and an 8.7\% enhancement in
readability. The impressive results demonstrate the viability of utilizing a
vanilla GPT model for the RCA task, thereby avoiding the high computational and
maintenance costs associated with a fine-tuned model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13835">The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Steyvers_M/0/1/0/all/0/1">Mark Steyvers</a>, <a href="http://arxiv.org/find/cs/1/au:+Tejeda_H/0/1/0/all/0/1">Heliodoro Tejeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Aakriti Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Belem_C/0/1/0/all/0/1">Catarina Belem</a>, <a href="http://arxiv.org/find/cs/1/au:+Karny_S/0/1/0/all/0/1">Sheer Karny</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinyue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayer_L/0/1/0/all/0/1">Lukas Mayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1">Padhraic Smyth</a></p>
<p>For large language models (LLMs) to be trusted by humans they need to be
well-calibrated in the sense that they can accurately assess and communicate
how likely it is that their predictions are correct. Recent work has focused on
the quality of internal LLM confidence assessments, but the question remains of
how well LLMs can communicate this internal model confidence to human users.
This paper explores the disparity between external human confidence in an LLM's
responses and the internal confidence of the model. Through experiments
involving multiple-choice questions, we systematically examine human users'
ability to discern the reliability of LLM outputs. Our study focuses on two key
areas: (1) assessing users' perception of true LLM confidence and (2)
investigating the impact of tailored explanations on this perception. The
research highlights that default explanations from LLMs often lead to user
overestimation of both the model's confidence and its' accuracy. By modifying
the explanations to more accurately reflect the LLM's internal confidence, we
observe a significant shift in user perception, aligning it more closely with
the model's actual confidence levels. This adjustment in explanatory approach
demonstrates potential for enhancing user trust and accuracy in assessing LLM
outputs. The findings underscore the importance of transparent communication of
confidence levels in LLMs, particularly in high-stakes applications where
understanding the reliability of AI-generated information is essential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13849">TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance. (arXiv:2401.13849v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haorui Wang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongzhi Zhang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinghao Li</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1">Lingkai Kong</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yuchen Zhuang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiusi Chen</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a> (1) ((1) College of Computing, Georgia Institute of Technology, (2) Department of Computer Science, University of California, Los Angeles)</p>
<p>Large Language Models (LLMs) have recently showcased remarkable reasoning
abilities. However, larger models often surpass their smaller counterparts in
reasoning tasks, posing the challenge of effectively transferring these
capabilities from larger models. Existing approaches heavily rely on extensive
fine-tuning data or continuous interactions with a superior teacher LLM during
inference. We introduce a principle-based teacher-student framework called
``Teaching via Principle Discovery'' (TPD) to address these limitations.
Inspired by human learning mechanisms, TPD mimics the interaction between a
teacher and a student using a principle-based approach. The teacher LLM
generates problem-solving instructions and corrective principles based on the
student LLM's errors. These principles guide the refinement of instructions and
the selection of instructive examples from a validation set. This enables the
student model to learn from both the teacher's guidance and its own mistakes.
Once the student model begins making inferences, TPD requires no further
intervention from the teacher LLM or humans. Through extensive experiments
across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared
to standard chain-of-thought prompting, TPD significantly improves the student
model's performance, achieving $6.2\%$ improvement on average.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13867">Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation. (arXiv:2401.13867v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a></p>
<p>Large language models like GPT-3.5-turbo and GPT-4 hold promise for
healthcare professionals, but they may inadvertently inherit biases during
their training, potentially affecting their utility in medical applications.
Despite few attempts in the past, the precise impact and extent of these biases
remain uncertain. Through both qualitative and quantitative analyses, we find
that these models tend to project higher costs and longer hospitalizations for
White populations and exhibit optimistic views in challenging medical scenarios
with much higher survival rates. These biases, which mirror real-world
healthcare disparities, are evident in the generation of patient backgrounds,
the association of specific diseases with certain races, and disparities in
treatment recommendations, etc. Our findings underscore the critical need for
future research to address and mitigate biases in language models, especially
in critical healthcare applications, to ensure fair and accurate outcomes for
all patients.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13887">A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification. (arXiv:2401.13887v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1">Madhumita Sushil</a>, <a href="http://arxiv.org/find/cs/1/au:+Zack_T/0/1/0/all/0/1">Travis Zack</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandair_D/0/1/0/all/0/1">Divneet Mandair</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhiwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wali_A/0/1/0/all/0/1">Ahmed Wali</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yan-Ning Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1">Yuwei Quan</a>, <a href="http://arxiv.org/find/cs/1/au:+Butte_A/0/1/0/all/0/1">Atul J. Butte</a></p>
<p>Although supervised machine learning is popular for information extraction
from clinical notes, creating large annotated datasets requires extensive
domain expertise and is time-consuming. Meanwhile, large language models (LLMs)
have demonstrated promising transfer learning capability. In this study, we
explored whether recent LLMs can reduce the need for large-scale data
annotations. We curated a manually-labeled dataset of 769 breast cancer
pathology reports, labeled with 13 categories, to compare zero-shot
classification capability of the GPT-4 model and the GPT-3.5 model with
supervised classification performance of three model architectures: random
forests classifier, long short-term memory networks with attention (LSTM-Att),
and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either
significantly better than or as well as the best supervised model, the LSTM-Att
model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance
between labels, the differences were more prominent. Frequent sources of GPT-4
errors included inferences from multiple samples and complex task design. On
complex tasks where large annotated datasets cannot be easily collected, LLMs
can reduce the burden of large-scale data labeling. However, if the use of LLMs
is prohibitive, the use of simpler supervised models with large annotated
datasets can provide comparable results. LLMs demonstrated the potential to
speed up the execution of clinical NLP studies by reducing the need for
curating large annotated datasets. This may result in an increase in the
utilization of NLP-based variables and outcomes in observational clinical
studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13905">Dynamic embedded topic models and change-point detection for exploring literary-historical hypotheses. (arXiv:2401.13905v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sirin_H/0/1/0/all/0/1">Hale Sirin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lippincott_T/0/1/0/all/0/1">Tom Lippincott</a></p>
<p>We present a novel combination of dynamic embedded topic models and
change-point detection to explore diachronic change of lexical semantic
modality in classical and early Christian Latin. We demonstrate several methods
for finding and characterizing patterns in the output, and relating them to
traditional scholarship in Comparative Literature and Classics. This simple
approach to unsupervised models of semantic change can be applied to any
suitable corpus, and we conclude with future directions and refinements aiming
to allow noisier, less-curated materials to meet that threshold.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13907">No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts. (arXiv:2401.13907v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Han Chen</a></p>
<p>Researchers recently found out that sometimes language models achieve high
accuracy on benchmark data set, but they can not generalize very well with even
little changes to the original data set. This is sometimes due to data
artifacts, model is learning the spurious correlation between tokens and
labels, instead of the semantics and logic. In this work, we analyzed SNLI data
and visualized such spurious correlations. We proposed an adaptive up-sampling
algorithm to correct the data artifacts, which is simple and effective, and
does not need human edits or annotation. We did an experiment applying the
algorithm to fix the data artifacts in SNLI data and the model trained with
corrected data performed significantly better than the model trained with raw
SNLI data, overall, as well as on the subset we corrected.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13919">WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">Hongliang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wenlin Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kaixin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>The advancement of large language models (LLMs) leads to a new era marked by
the development of autonomous applications in the real world, which drives
innovation in the creation of advanced web-based agents. Existing web agents
typically only handle one input modality and are evaluated only in simplified
web simulators or static web snapshots, greatly limiting their applicability in
real-world scenarios. To bridge this gap, we introduce WebVoyager, an
innovative Large Multimodal Model (LMM) powered web agent that can complete
user instructions end-to-end by interacting with real-world websites. Moreover,
we propose a new evaluation protocol for web agents to address the challenges
of automatic evaluation of open-ended web agent tasks, leveraging the robust
multimodal comprehension capabilities of GPT-4V. We create a new benchmark by
gathering real-world tasks from 15 widely used websites to evaluate our agents.
We show that WebVoyager achieves a 55.7% task success rate, significantly
surpassing the performance of both GPT-4 (All Tools) and the WebVoyager
(text-only) setups, underscoring the exceptional capability of WebVoyager in
practical applications. We found that our proposed automatic evaluation
achieves 85.3% agreement with human judgment, paving the way for further
development of web agents in a real-world setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13920">LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zhijie Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Li Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1">Entong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Binfan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rongqian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a></p>
<p>The Mixtures-of-Experts (MoE) model is a widespread distributed and
integrated learning method for large language models (LLM), which is favored
due to its ability to sparsify and expand models efficiently. However, the
performance of MoE is limited by load imbalance and high latency of All-To-All
communication, along with relatively redundant computation owing to large
expert capacity. Load imbalance may result from existing routing policies that
consistently tend to select certain experts. The frequent inter-node
communication in the All-To-All procedure also significantly prolongs the
training time. To alleviate the above performance problems, we propose a novel
routing strategy that combines load balance and locality by converting partial
inter-node communication to that of intra-node. Notably, we elucidate that
there is a minimum threshold for expert capacity, calculated through the
maximal angular deviation between the gating weights of the experts and the
assigned tokens. We port these modifications on the PanGu-Sigma model based on
the MindSpore framework with multi-level routing and conduct experiments on
Ascend clusters. The experiment results demonstrate that the proposed LocMoE
reduces training time per epoch by 12.68% to 22.24% compared to classical
routers, such as hash router and switch router, without impacting the model
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13927">Adaptive Text Watermark for Large Language Models. (arXiv:2401.13927v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yepeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1">Yuheng Bu</a></p>
<p>The advancement of Large Language Models (LLMs) has led to increasing
concerns about the misuse of AI-generated text, and watermarking for
LLM-generated text has emerged as a potential solution. However, it is
challenging to generate high-quality watermarked text while maintaining strong
security, robustness, and the ability to detect watermarks without prior
knowledge of the prompt or model. This paper proposes an adaptive watermarking
strategy to address this problem. To improve the text quality and maintain
robustness, we adaptively add watermarking to token distributions with high
entropy measured using an auxiliary model and keep the low entropy token
distributions untouched. For the sake of security and to further minimize the
watermark's impact on text quality, instead of using a fixed green/red list
generated from a random secret key, which can be vulnerable to decryption and
forgery, we adaptively scale up the output logits in proportion based on the
semantic embedding of previously generated text using a well designed semantic
mapping model. Our experiments involving various LLMs demonstrate that our
approach achieves comparable robustness performance to existing watermark
methods. Additionally, the text generated by our method has perplexity
comparable to that of \emph{un-watermarked} LLMs while maintaining security
even under various attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13979">Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1">Alireza Mohammadshahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1">Ali Shaikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1">Majid Yazdani</a></p>
<p>In this paper, we propose an architecture to harness the collective knowledge
of multiple trained LLMs to create a new state-of-the-art. At the core of this
framework is a LLM-based orchestrator that is adept at picking the right
underlying LLM experts for optimal task execution. Inspired by self-play in
reinforcement learning, we created a loop of query generation, orchestration,
and evaluation to generate training data for the orchestrator. Our evaluation
focused on the MMLU benchmark, employing models with 7B, 13B, and 34B
parameters available on Hugging Face. The results demonstrate new
state-of-the-art open-source models: Our Leeroo orchestrator achieves
performance on par with the Mixtral model while incurring only two-thirds of
its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by
over 5% at the same cost level, reaching an accuracy of 75.9%. Further
enhancements were observed when integrating GPT4 into the underlying model
pool. The Leeroo orchestrator nearly matches GPT4's performance at half the
cost and even exceeds GPT4's results with a 25% cost reduction. These findings
illustrate the potential of our architecture in creating state-of-the-art and
cost-effective LLMs by optimizing the synergy between multiple LLMs to achieve
superior performance outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13986">Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanda Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1">Chandan Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaodong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1">Simiao Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1">He He</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a></p>
<p>Large language models (LLMs) often generate convincing, fluent explanations.
However, different from humans, they often generate inconsistent explanations
on different inputs. For example, an LLM may generate the explanation "all
birds can fly" when answering the question "Can sparrows fly?" but meanwhile
answer "no" to the related question "Can penguins fly?". Explanations should be
consistent across related examples so that they allow a human to simulate the
LLM's decision process on multiple examples. We propose explanation-consistency
finetuning (EC-finetuning), a method that adapts LLMs to generate more
consistent natural-language explanations on related examples. EC-finetuning
involves finetuning LLMs on synthetic data that is carefully constructed to
contain consistent explanations. Across a variety of question-answering
datasets in various domains, EC-finetuning yields a 10.0% relative explanation
consistency improvement on four finetuning datasets, and generalizes to seven
out-of-distribution datasets not seen during finetuning (+4.5% relative). Code
is available at https://github.com/yandachen/explanation-consistency-finetuning .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13996">Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution. (arXiv:2401.13996v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Cheng Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1">Shihao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yujia Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yining Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1">Xin Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yankai Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yesai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy
for enhancing the adaptability and flexibility of AI agents through inter-task
self-evolution. Unlike existing methods focused on intra-task learning, ICE
promotes the transfer of knowledge between tasks for genuine self-evolution,
similar to human experience learning. The strategy dynamically investigates
planning and execution trajectories, consolidates them into simplified
workflows and pipelines, and exploits them for improved task execution. Our
experiments on the XAgent framework demonstrate ICE's effectiveness, reducing
API calls by as much as 80% and significantly decreasing the demand for the
model's capability. Specifically, when combined with GPT-3.5, ICE's performance
matches that of raw GPT-4 across various agent tasks. We argue that this
self-evolution approach represents a paradigm shift in agent design,
contributing to a more robust AI community and ecosystem, and moving a step
closer to full autonomy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14003">ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases. (arXiv:2401.14003v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1">Quyet V. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1">Tianqing Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1">Shizhe Diao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has
been explored as a way to acquire new commonsense knowledge based on reference
knowledge in the original CSKBs and external prior knowledge. Despite the
advancement of Large Language Models (LLM) and prompt engineering techniques in
various reasoning tasks, they still struggle to deal with CSKB reasoning. One
of the problems is that it is hard for them to acquire explicit relational
constraints in CSKBs from only in-context exemplars, due to a lack of symbolic
reasoning capabilities (Bengio et al., 2021). To this end, we proposed
**ConstraintChecker**, a plugin over prompting techniques to provide and check
explicit constraints. When considering a new knowledge instance,
ConstraintChecker employs a rule-based module to produce a list of constraints,
then it uses a zero-shot learning module to check whether this knowledge
instance satisfies all constraints. The acquired constraint-checking result is
then aggregated with the output of the main prompting technique to produce the
final output. Experimental results on CSKB Reasoning benchmarks demonstrate the
effectiveness of our method by bringing consistent improvements over all
prompting methods. Codes and data are available at
\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14011">CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zheqi He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xinya Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pengfei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuan_R/0/1/0/all/0/1">Richeng Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qiannan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hua Huang</a></p>
<p>Multi-modal large language models(MLLMs) have achieved remarkable progress
and demonstrated powerful knowledge comprehension and reasoning abilities.
However, the mastery of domain-specific knowledge, which is essential for
evaluating the intelligence of MLLMs, continues to be a challenge. Current
multi-modal benchmarks for domain-specific knowledge concentrate on
multiple-choice questions and are predominantly available in English, which
imposes limitations on the comprehensiveness of the evaluation. To this end, we
introduce CMMU, a novel benchmark for multi-modal and multi-type question
understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7
subjects, covering knowledge from primary to high school. The questions can be
categorized into 3 types: multiple-choice, multiple-response, and
fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we
propose a rigorous evaluation strategy called ShiftCheck for assessing
multiple-choice questions. The strategy aims to reduce position bias, minimize
the influence of randomness on correctness, and perform a quantitative analysis
of position bias. We evaluate seven open-source MLLMs along with GPT4-V,
Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a
significant challenge to the recent MLLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14016">Towards Uncertainty-Aware Language Agent. (arXiv:2401.14016v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiuzhou Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1">Wray Buntine</a>, <a href="http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1">Ehsan Shareghi</a></p>
<p>While Language Agents have achieved promising success by placing Large
Language Models at the core of a more versatile design that dynamically
interacts with the external world, the existing approaches neglect the notion
of uncertainty during these interactions. We present the Uncertainty-Aware
Language Agent (UALA), a framework that orchestrates the interaction between
the agent and the external world using uncertainty quantification. Compared
with other well-known counterparts like ReAct, our extensive experiments across
3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes
demonstrates that UALA brings a significant improvement of performance, while
having a substantially lower reliance on the external world (i.e., reduced
number of tool calls and tokens). Our analyses provide various insights
including the great potential of UALA compared with agent fine-tuning, and
underscoring the unreliably of verbalised confidence of LLMs as a proxy for
uncertainty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14019">Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI. (arXiv:2401.14019v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bandel_E/0/1/0/all/0/1">Elron Bandel</a>, <a href="http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1">Yotam Perlitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1">Elad Venezian</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedman_Melamed_R/0/1/0/all/0/1">Roni Friedman-Melamed</a>, <a href="http://arxiv.org/find/cs/1/au:+Arviv_O/0/1/0/all/0/1">Ofir Arviv</a>, <a href="http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1">Matan Orbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Don_Yehyia_S/0/1/0/all/0/1">Shachar Don-Yehyia</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheinwald_D/0/1/0/all/0/1">Dafna Sheinwald</a>, <a href="http://arxiv.org/find/cs/1/au:+Gera_A/0/1/0/all/0/1">Ariel Gera</a>, <a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1">Leshem Choshen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1">Michal Shmueli-Scheuer</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1">Yoav Katz</a></p>
<p>In the dynamic landscape of generative NLP, traditional text processing
pipelines limit research flexibility and reproducibility, as they are tailored
to specific dataset, task, and model combinations. The escalating complexity,
involving system prompts, model-specific formats, instructions, and more, calls
for a shift to a structured, modular, and customizable solution. Addressing
this need, we present Unitxt, an innovative library for customizable textual
data preparation and evaluation tailored to generative language models. Unitxt
natively integrates with common libraries like HuggingFace and LM-eval-harness
and deconstructs processing flows into modular components, enabling easy
customization and sharing between practitioners. These components encompass
model-specific formats, task prompts, and many other comprehensive dataset
processing definitions. The Unitxt-Catalog centralizes these components,
fostering collaboration and exploration in modern textual data workflows.
Beyond being a tool, Unitxt is a community-driven platform, empowering users to
build, share, and advance their pipelines collaboratively. Join the Unitxt
community at https://github.com/IBM/unitxt!
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14021">Accelerating Retrieval-Augmented Language Model Serving with Speculation. (arXiv:2401.14021v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhihao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1">Alan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lijie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yihua Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lanting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Phothilimthana_P/0/1/0/all/0/1">Phitchaya Mangpo Phothilimthana</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1">Zhihao Jia</a></p>
<p>Retrieval-augmented language models (RaLM) have demonstrated the potential to
solve knowledge-intensive natural language processing (NLP) tasks by combining
a non-parametric knowledge base with a parametric language model. Instead of
fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to
the latest data and better source attribution mechanisms. Among various RaLM
approaches, iterative RaLM delivers a better generation quality due to a more
frequent interaction between the retriever and the language model. Despite the
benefits, iterative RaLM usually encounters high overheads due to the frequent
retrieval step. To this end, we propose RaLMSpec, a speculation-inspired
framework that provides generic speed-up over iterative RaLM while preserving
the same model outputs through speculative retrieval and batched verification.
By further incorporating prefetching, optimal speculation stride scheduler, and
asynchronous verification, RaLMSpec can automatically exploit the acceleration
potential to the fullest. For naive iterative RaLM serving, extensive
evaluations over three language models on four downstream QA datasets
demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,
1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,
approximate dense retriever, and sparse retriever respectively compared with
the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to
7.59x and 2.45x when the retriever is an exact dense retriever and approximate
dense retriever, respectively, compared with the baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14040">(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection. (arXiv:2401.14040v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Periti_F/0/1/0/all/0/1">Francesco Periti</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubossarsky_H/0/1/0/all/0/1">Haim Dubossarsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Tahmasebi_N/0/1/0/all/0/1">Nina Tahmasebi</a></p>
<p>In the universe of Natural Language Processing, Transformer-based language
models like BERT and (Chat)GPT have emerged as lexical superheroes with great
power to solve open research problems. In this paper, we specifically focus on
the temporal problem of semantic change, and evaluate their ability to solve
two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and
HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf
technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a
family of models that currently stand as the state-of-the-art for modeling
semantic change. Our experiments represent the first attempt to assess the use
of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT
performs significantly worse than the foundational GPT version. Furthermore,
our results demonstrate that (Chat)GPT achieves slightly lower performance than
BERT in detecting long-term changes but performs significantly worse in
detecting short-term changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14043">Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haochen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1">Jonathan Leung</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhiqi Shen</a></p>
<p>Large Language Models (LLMs) have shown prominent performance in various
downstream tasks in which prompt engineering plays a pivotal role in optimizing
LLMs' performance. This paper, not as an overview of current prompt engineering
methods, aims to highlight the limitation of designing prompts while holding an
anthropomorphic assumption that expects LLMs to think like humans. From our
review of 35 representative studies, we demonstrate that a goal-oriented prompt
formulation, which guides LLMs to follow established human logical thinking,
significantly improves the performance of LLMs. Furthermore, We introduce a
novel taxonomy that categorizes goal-oriented prompting methods into five
interconnected stages and we demonstrate the broad applicability of our
framework by summarizing ten applicable tasks. With four future directions
proposed, we hope to further emphasize and promote goal-oriented prompt
engineering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14067">Ta&#x27;keed: The First Generative Fact-Checking System for Arabic Claims. (arXiv:2401.14067v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Althabiti_S/0/1/0/all/0/1">Saud Althabiti</a>, <a href="http://arxiv.org/find/cs/1/au:+Alsalka_M/0/1/0/all/0/1">Mohammad Ammar Alsalka</a>, <a href="http://arxiv.org/find/cs/1/au:+Atwell_E/0/1/0/all/0/1">Eric Atwell</a></p>
<p>This paper introduces Ta'keed, an explainable Arabic automatic fact-checking
system. While existing research often focuses on classifying claims as "True"
or "False," there is a limited exploration of generating explanations for claim
credibility, particularly in Arabic. Ta'keed addresses this gap by assessing
claim truthfulness based on retrieved snippets, utilizing two main components:
information retrieval and LLM-based claim verification. We compiled the
ArFactEx, a testing gold-labelled dataset with manually justified references,
to evaluate the system. The initial model achieved a promising F1 score of 0.72
in the classification task. Meanwhile, the system's generated explanations are
compared with gold-standard explanations syntactically and semantically. The
study recommends evaluating using semantic similarities, resulting in an
average cosine similarity score of 0.76. Additionally, we explored the impact
of varying snippet quantities on claim classification accuracy, revealing a
potential correlation, with the model using the top seven hits outperforming
others with an F1 score of 0.77.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14109">CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tomut_A/0/1/0/all/0/1">Andrei Tomut</a>, <a href="http://arxiv.org/find/cs/1/au:+Jahromi_S/0/1/0/all/0/1">Saeed S. Jahromi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sukhbinder Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishtiaq_F/0/1/0/all/0/1">Faysal Ishtiaq</a>, <a href="http://arxiv.org/find/cs/1/au:+Munoz_C/0/1/0/all/0/1">Cesar Mu&#xf1;oz</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1">Prabdeep Singh Bajaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Elborady_A/0/1/0/all/0/1">Ali Elborady</a>, <a href="http://arxiv.org/find/cs/1/au:+Bimbo_G/0/1/0/all/0/1">Gianni del Bimbo</a>, <a href="http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1">Mehrazin Alizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Montero_D/0/1/0/all/0/1">David Montero</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Ramiro_P/0/1/0/all/0/1">Pablo Martin-Ramiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Muhammad Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaoui_O/0/1/0/all/0/1">Oussama Tahiri Alaoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Malcolm_J/0/1/0/all/0/1">John Malcolm</a>, <a href="http://arxiv.org/find/cs/1/au:+Mugel_S/0/1/0/all/0/1">Samuel Mugel</a>, <a href="http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1">Roman Orus</a></p>
<p>Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly
in generative Artificial Intelligence (AI), but their immense size poses
significant challenges, such as huge training and inference costs, substantial
energy demands, and limitations for on-site deployment. Traditional compression
methods such as pruning, distillation, and low-rank approximation focus on
reducing the effective number of neurons in the network, while quantization
focuses on reducing the numerical precision of individual weights to reduce the
model size while keeping the number of neurons fixed. While these compression
methods have been relatively successful in practice, there's no compelling
reason to believe that truncating the number of neurons is an optimal strategy.
In this context, this paper introduces CompactifAI, an innovative LLM
compression approach using quantum-inspired Tensor Networks that focuses on the
model's correlation space instead, allowing for a more controlled, refined and
interpretable model compression. Our method is versatile and can be implemented
with - or on top of - other compression techniques. As a benchmark, we
demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model
to only $30\%$ of its original size while recovering over $90\%$ of the
original accuracy after a brief distributed retraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14113">On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling. (arXiv:2401.14113v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaobao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1">Fengjun Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yichao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chaoqun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cong-Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1">Anh Tuan Luu</a></p>
<p>Hierarchical topic modeling aims to discover latent topics from a corpus and
organize them into a hierarchy to understand documents with desirable semantic
granularity. However, existing work struggles with producing topic hierarchies
of low affinity, rationality, and diversity, which hampers document
understanding. To overcome these challenges, we in this paper propose Transport
Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early
simple topic dependencies, we propose a transport plan dependency method. It
constrains dependencies to ensure their sparsity and balance, and also
regularizes topic hierarchy building with them. This improves affinity and
diversity of hierarchies. We further propose a context-aware disentangled
decoder. Rather than previously entangled decoding, it distributes different
semantic granularity to topics at different levels by disentangled decoding.
This facilitates the rationality of hierarchies. Experiments on benchmark
datasets demonstrate that our method surpasses state-of-the-art baselines,
effectively improving the affinity, rationality, and diversity of hierarchical
topic modeling with better performance on downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14135">Convolutional Neural Networks can achieve binary bail judgement classification. (arXiv:2401.14135v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barman_A/0/1/0/all/0/1">Amit Barman</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1">Devangan Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1">Debapriya Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_I/0/1/0/all/0/1">Indranil Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Guha_S/0/1/0/all/0/1">Shouvik Kumar Guha</a>, <a href="http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1">Samir Karmakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1">Sudip Kumar Naskar</a></p>
<p>There is an evident lack of implementation of Machine Learning (ML) in the
legal domain in India, and any research that does take place in this domain is
usually based on data from the higher courts of law and works with English
data. The lower courts and data from the different regional languages of India
are often overlooked. In this paper, we deploy a Convolutional Neural Network
(CNN) architecture on a corpus of Hindi legal documents. We perform a bail
Prediction task with the help of a CNN model and achieve an overall accuracy of
93\% which is an improvement on the benchmark accuracy, set by Kapoor et al.
(2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14151">True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Weihao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shanqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Longtao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinrun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bo An</a></p>
<p>Despite the impressive performance across numerous tasks, large language
models (LLMs) often fail in solving simple decision-making tasks due to the
misalignment of the knowledge in LLMs with environments. On the contrary,
reinforcement learning (RL) agents learn policies from scratch, which makes
them always align with environments but difficult to incorporate prior
knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a
novel general online framework that deploys LLMs as decision-making agents to
efficiently interact and align with embodied environments via RL without
requiring any prepared datasets or prior knowledge of the environments.
Firstly, we query the joint probabilities of each valid action with LLMs to
form behavior policies. Then, to enhance the stability and robustness of the
policies, we propose two normalization methods and summarize four prompt design
principles. Finally, we design a novel parameter-efficient training
architecture where the actor and critic share one frozen LLM equipped with
low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to
evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency
and performance compared to the conventional RL method, PPO, and prompt tuning
method, SayCan, in both classical decision-making environment, Overcooked, and
simulated household environment, VirtualHome. ii) Benefiting from LLMs'
open-vocabulary feature, TWOSOME shows superior generalization ability to
unseen tasks. iii) Under our framework, there is no significant loss of the
LLMs' original ability during online PPO finetuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14166">BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangmeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1">Fei Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yifan Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a></p>
<p>As a novel and effective fine-tuning paradigm based on large-scale
pre-trained language models (PLMs), prompt-tuning aims to reduce the gap
between downstream tasks and pre-training objectives. While prompt-tuning has
yielded continuous advancements in various tasks, such an approach still
remains a persistent defect: prompt-tuning methods fail to generalize to
specific few-shot patterns. From the perspective of distribution analyses, we
disclose that the intrinsic issues behind the phenomenon are the
over-multitudinous conceptual knowledge contained in PLMs and the abridged
knowledge for target downstream domains, which jointly result in that PLMs
mis-locate the knowledge distributions corresponding to the target domains in
the universal knowledge embedding space. To this end, we intuitively explore to
approximate the unabridged target domains of downstream tasks in a debiased
manner, and then abstract such domains to generate discriminative prompts,
thereby providing the de-ambiguous guidance for PLMs. Guided by such an
intuition, we propose a simple yet effective approach, namely BayesPrompt, to
learn prompts that contain the domain discriminative information against the
interference from domain-irrelevant knowledge. BayesPrompt primitively
leverages known distributions to approximate the debiased factual distributions
of target domains and further uniformly samples certain representative features
from the approximated distributions to generate the ultimate prompts for PLMs.
We provide theoretical insights with the connection to domain adaptation.
Empirically, our method achieves state-of-the-art performance on benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14192">How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shuo Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Runze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhenxun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yanming Shen</a></p>
<p>While Large Language Models (LLMs) dominate tasks like natural language
processing and computer vision, harnessing their power for spatial-temporal
forecasting remains challenging. The disparity between sequential text and
complex spatial-temporal data hinders this application. To address this issue,
this paper introduces STG-LLM, an innovative approach empowering LLMs for
spatial-temporal forecasting. We tackle the data mismatch by proposing: 1)
STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph
data into concise tokens capturing both spatial and temporal relationships; 2)
STG-Adapter: This minimalistic adapter, consisting of linear encoding and
decoding layers, bridges the gap between tokenized data and LLM comprehension.
By fine-tuning only a small set of parameters, it can effectively grasp the
semantics of tokens generated by STG-Tokenizer, while preserving the original
natural language understanding capabilities of LLMs. Extensive experiments on
diverse spatial-temporal benchmark datasets show that STG-LLM successfully
unlocks LLM potential for spatial-temporal forecasting. Remarkably, our
approach achieves competitive performance on par with dedicated SOTA methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14194">Parameter-Efficient Conversational Recommender System as a Language Processing Task. (arXiv:2401.14194v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1">Mathieu Ravaut</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1">Aixin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a></p>
<p>Conversational recommender systems (CRS) aim to recommend relevant items to
users by eliciting user preference through natural language conversation. Prior
work often utilizes external knowledge graphs for items' semantic information,
a language model for dialogue generation, and a recommendation module for
ranking relevant items. This combination of multiple components suffers from a
cumbersome training process, and leads to semantic misalignment issues between
dialogue generation and item recommendation. In this paper, we represent items
in natural language and formulate CRS as a natural language processing task.
Accordingly, we leverage the power of pre-trained language models to encode
items, understand user intent via conversation, perform item recommendation
through semantic matching, and generate dialogues. As a unified model, our
PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without
relying on non-textual metadata such as a knowledge graph. Experiments on two
benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of
PECRS on recommendation and conversation. Our code is available at:
https://github.com/Ravoxsg/efficient_unified_crs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14196">DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daya Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qihao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Dejian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhenda Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_K/0/1/0/all/0/1">Kai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wentao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1">Xiao Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Y. Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Y.K. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fuli Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yingfei Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1">Wenfeng Liang</a></p>
<p>The rapid development of large language models has revolutionized code
intelligence in software development. However, the predominance of
closed-source models has restricted extensive research and development. To
address this, we introduce the DeepSeek-Coder series, a range of open-source
code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion
tokens. These models are pre-trained on a high-quality project-level code
corpus and employ a fill-in-the-blank task with a 16K window to enhance code
generation and infilling. Our extensive evaluations demonstrate that
DeepSeek-Coder not only achieves state-of-the-art performance among open-source
code models across multiple benchmarks but also surpasses existing
closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models
are under a permissive license that allows for both research and unrestricted
commercial use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14212">Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations. (arXiv:2401.14212v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nuyts_W/0/1/0/all/0/1">Wolf Nuyts</a>, <a href="http://arxiv.org/find/cs/1/au:+Cartuyvels_R/0/1/0/all/0/1">Ruben Cartuyvels</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a></p>
<p>Recognizing visual entities in a natural language sentence and arranging them
in a 2D spatial layout require a compositional understanding of language and
space. This task of layout prediction is valuable in text-to-image synthesis as
it allows localized and controlled in-painting of the image. In this
comparative study it is shown that we can predict layouts from language
representations that implicitly or explicitly encode sentence syntax, if the
sentences mention similar entity-relationships to the ones seen during
training. To test compositional understanding, we collect a test set of
grammatically correct sentences and layouts describing compositions of entities
and relations that unlikely have been seen during training. Performance on this
test set substantially drops, showing that current models rely on correlations
in the training data and have difficulties in understanding the structure of
the input sentences. We propose a novel structural loss function that better
enforces the syntactic structure of the input sentence and show large
performance gains in the task of 2D spatial layout prediction conditioned on
text. The loss has the potential to be used in other generation tasks where a
tree-like structure underlies the conditioning modality. Code, trained models
and the USCOCO evaluation set will be made available via github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14215">Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hana Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1">Kai Tzu-iunn Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seoyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1">Jinyoung Yeo</a></p>
<p>Memorizing and utilizing speakers' personas is a common practice for response
generation in long-term conversations. Yet, human-authored datasets often
provide uninformative persona sentences that hinder response quality. This
paper presents a novel framework that leverages commonsense-based persona
expansion to address such issues in long-term conversation. While prior work
focuses on not producing personas that contradict others, we focus on
transforming contradictory personas into sentences that contain rich speaker
information, by refining them based on their contextual backgrounds with
designed strategies. As the pioneer of persona expansion in multi-session
settings, our framework facilitates better response generation via human-like
persona refinement. The supplementary video of our work is available at
https://caffeine-15bbf.web.app/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14228">Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sabry_M/0/1/0/all/0/1">Mohammed Sabry</a>, <a href="http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1">Anya Belz</a></p>
<p>As the cost of training ever larger language models has grown, so has the
interest in reusing previously learnt knowledge. Transfer learning methods have
shown how reusing non-task-specific knowledge can help in subsequent
task-specific learning. In this paper, we investigate the inverse: porting
whole functional modules that encode task-specific knowledge from one model to
another. We designed a study comprising 1,440 training/testing runs to test the
portability of modules trained by parameter-efficient finetuning (PEFT)
techniques, using sentiment analysis as an example task. We test portability in
a wide range of scenarios, involving different PEFT techniques and different
pretrained host models, among other dimensions. We compare the performance of
ported modules with that of equivalent modules trained (i) from scratch, and
(ii) from parameters sampled from the same distribution as the ported module.
We find that the ported modules far outperform the two alternatives tested, but
that there are interesting performance differences between the four PEFT
techniques. We conclude that task-specific knowledge in the form of
structurally modular sets of parameters as produced by PEFT techniques is
highly portable, but that degree of success depends on type of PEFT and on
differences between originating and receiving pretrained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14240">Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer Models for Classifying Depression Severity in English and Luganda. (arXiv:2401.14240v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kimera_R/0/1/0/all/0/1">Richard Kimera</a>, <a href="http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1">Daniela N. Rim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirabira_J/0/1/0/all/0/1">Joseph Kirabira</a>, <a href="http://arxiv.org/find/cs/1/au:+Udomah_U/0/1/0/all/0/1">Ubong Godwin Udomah</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1">Heeyoul Choi</a></p>
<p>Depression is a global burden and one of the most challenging mental health
conditions to control. Experts can detect its severity early using the Beck
Depression Inventory (BDI) questionnaire, administer appropriate medication to
patients, and impede its progression. Due to the fear of potential
stigmatization, many patients turn to social media platforms like Reddit for
advice and assistance at various stages of their journey. This research
extracts text from Reddit to facilitate the diagnostic process. It employs a
proposed labeling approach to categorize the text and subsequently fine-tunes
the Longformer model. The model's performance is compared against baseline
models, including Naive Bayes, Random Forest, Support Vector Machines, and
Gradient Boosting. Our findings reveal that the Longformer model outperforms
the baseline models in both English (48%) and Luganda (45%) languages on a
custom-made dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14242">Improving Natural Language Capability of Code Large Language Model. (arXiv:2401.14242v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1">Daoguang Zan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1">Bei Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1">Ailun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaolin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongji Wang</a></p>
<p>Code large language models (Code LLMs) have demonstrated remarkable
performance in code generation. Nonetheless, most existing works focus on
boosting code LLMs from the perspective of programming capabilities, while
their natural language capabilities receive less attention. To fill this gap,
we thus propose a novel framework, comprising two modules: AttentionExtractor,
which is responsible for extracting key phrases from the user's natural
language requirements, and AttentionCoder, which leverages these extracted
phrases to generate target code to solve the requirement. This framework
pioneers an innovative idea by seamlessly integrating code LLMs with
traditional natural language processing tools. To validate the effectiveness of
the framework, we craft a new code generation benchmark, called MultiNL-H,
covering five natural languages. Extensive experimental results demonstrate the
effectiveness of our proposed framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14267">Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1">Lyle Muller</a>, <a href="http://arxiv.org/find/cs/1/au:+Churchland_P/0/1/0/all/0/1">Patricia S. Churchland</a>, <a href="http://arxiv.org/find/cs/1/au:+Sejnowski_T/0/1/0/all/0/1">Terrence J. Sejnowski</a></p>
<p>The capabilities of transformer networks such as ChatGPT and other Large
Language Models (LLMs) have captured the world's attention. The crucial
computational mechanism underlying their performance relies on transforming a
complete input sequence - for example, all the words in a sentence into a long
"encoding vector" - that allows transformers to learn long-range temporal
dependencies in naturalistic sequences. Specifically, "self-attention" applied
to this encoding vector enhances temporal context in transformers by computing
associations between pairs of words in the input sequence. We suggest that
waves of neural activity, traveling across single cortical regions or across
multiple regions at the whole-brain scale, could implement a similar encoding
principle. By encapsulating recent input history into a single spatial pattern
at each moment in time, cortical waves may enable temporal context to be
extracted from sequences of sensory inputs, the same computational principle
used in transformers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14280">RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Husain_J/0/1/0/all/0/1">Jaavid Aktar Husain</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1">Raj Dabre</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Aswanth Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1">Ratish Puduppully</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1">Anoop Kunchukuttan</a></p>
<p>This study addresses the challenge of extending Large Language Models (LLMs)
to non-English languages, specifically those using non-Latin scripts. We
propose an innovative approach that utilizes the romanized form of text as an
interface for LLMs, hypothesizing that its frequent informal use and shared
tokens with English enhance cross-lingual alignment. Focusing on Hindi, we
demonstrate through Hindi-to-English translation and sentiment analysis tasks
that romanized text not only significantly improves inference efficiency due to
its lower fertility compared to native text but also achieves competitive
performance with limited pre-training. Additionally, our novel multi-script
prompting approach, which combines romanized and native texts, shows promise in
further enhancing task performance. These findings suggest the potential of
romanization in bridging the language gap for LLM applications, with future
work aimed at expanding this approach to more languages and tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14295">Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1">Maciej Besta</a>, <a href="http://arxiv.org/find/cs/1/au:+Memedi_F/0/1/0/all/0/1">Florim Memedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstenberger_R/0/1/0/all/0/1">Robert Gerstenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Blach_N/0/1/0/all/0/1">Nils Blach</a>, <a href="http://arxiv.org/find/cs/1/au:+Nyczyk_P/0/1/0/all/0/1">Piotr Nyczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Copik_M/0/1/0/all/0/1">Marcin Copik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwasniewski_G/0/1/0/all/0/1">Grzegorz Kwa&#x15b;niewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1">J&#xfc;rgen M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1">Lukas Gianinazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kubicek_A/0/1/0/all/0/1">Ales Kubicek</a>, <a href="http://arxiv.org/find/cs/1/au:+Niewiadomski_H/0/1/0/all/0/1">Hubert Niewiadomski</a>, <a href="http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1">Onur Mutlu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1">Torsten Hoefler</a></p>
<p>The field of natural language processing (NLP) has witnessed significant
progress in recent years, with a notable focus on improving large language
models' (LLM) performance through innovative prompting techniques. Among these,
prompt engineering coupled with structures has emerged as a promising paradigm,
with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,
in which the overall LLM reasoning is guided by a structure such as a graph. As
illustrated with numerous examples, this paradigm significantly enhances the
LLM's capability to solve numerous tasks, ranging from logical or mathematical
reasoning to planning or creative writing. To facilitate the understanding of
this growing field and pave the way for future developments, we devise a
general blueprint for effective and efficient LLM reasoning schemes. For this,
we conduct an in-depth analysis of the prompt execution pipeline, clarifying
and clearly defining different concepts. We then build the first taxonomy of
structure-enhanced LLM reasoning schemes. We focus on identifying fundamental
classes of harnessed structures, and we analyze the representations of these
structures, algorithms executed with these structures, and many others. We
refer to these structures as reasoning topologies, because their representation
becomes to a degree spatial, as they are contained within the LLM context. Our
study compares existing prompting schemes using the proposed taxonomy,
discussing how certain design choices lead to different patterns in performance
and cost. We also outline theoretical underpinnings, relationships between
prompting and others parts of the LLM ecosystem such as knowledge bases, and
the associated research challenges. Our work will help to advance future prompt
engineering techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14360">A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts. (arXiv:2401.14360v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elahi_K/0/1/0/all/0/1">Kazi Toufique Elahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1">Tasnuva Binte Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1">Shakil Shahriar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1">Samir Sarker</a>, <a href="http://arxiv.org/find/cs/1/au:+Shawon_M/0/1/0/all/0/1">Md. Tanvir Rouf Shawon</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1">G. M. Shahariar</a></p>
<p>While Bengali is considered a language with limited resources, sentiment
analysis has been a subject of extensive research in the literature.
Nevertheless, there is a scarcity of exploration into sentiment analysis
specifically in the realm of noisy Bengali texts. In this paper, we introduce a
dataset (NC-SentNoB) that we annotated manually to identify ten different types
of noise found in a pre-existing sentiment analysis dataset comprising of
around 15K noisy Bengali texts. At first, given an input noisy text, we
identify the noise type, addressing this as a multi-label classification task.
Then, we introduce baseline noise reduction methods to alleviate noise prior to
conducting sentiment analysis. Finally, we assess the performance of fine-tuned
sentiment analysis models with both noisy and noise-reduced texts to make
comparisons. The experimental findings indicate that the noise reduction
methods utilized are not satisfactory, highlighting the need for more suitable
noise reduction methods in future research endeavors. We have made the
implementation and dataset presented in this paper publicly available at
https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14367">Genie: Achieving Human Parity in Content-Grounded Datasets Generation. (arXiv:2401.14367v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1">Asaf Yehudai</a>, <a href="http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1">Boaz Carmeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Mass_Y/0/1/0/all/0/1">Yosi Mass</a>, <a href="http://arxiv.org/find/cs/1/au:+Arviv_O/0/1/0/all/0/1">Ofir Arviv</a>, <a href="http://arxiv.org/find/cs/1/au:+Mills_N/0/1/0/all/0/1">Nathaniel Mills</a>, <a href="http://arxiv.org/find/cs/1/au:+Toledo_A/0/1/0/all/0/1">Assaf Toledo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1">Eyal Shnarch</a>, <a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1">Leshem Choshen</a></p>
<p>The lack of high-quality data for content-grounded generation tasks has been
identified as a major obstacle to advancing these tasks. To address this gap,
we propose Genie, a novel method for automatically generating high-quality
content-grounded data. It consists of three stages: (a) Content Preparation,
(b) Generation: creating task-specific examples from the content (e.g.,
question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure
the quality and faithfulness of the generated data. We showcase this
methodology by generating three large-scale synthetic data, making wishes, for
Long-Form Question-Answering (LFQA), summarization, and information extraction.
In a human evaluation, our generated data was found to be natural and of high
quality. Furthermore, we compare models trained on our data with models trained
on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for
Summarization. We show that our models are on par with or outperforming models
trained on human-generated data and consistently outperforming them in
faithfulness. Finally, we applied our method to create LFQA data within the
medical domain and compared a model trained on it with models trained on other
domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14373">TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation. (arXiv:2401.14373v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Uludogan_G/0/1/0/all/0/1">G&#xf6;k&#xe7;e Uludo&#x11f;an</a>, <a href="http://arxiv.org/find/cs/1/au:+Balal_Z/0/1/0/all/0/1">Zeynep Yirmibe&#x15f;o&#x11f;lu Balal</a>, <a href="http://arxiv.org/find/cs/1/au:+Akkurt_F/0/1/0/all/0/1">Furkan Akkurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Turker_M/0/1/0/all/0/1">Melik&#x15f;ah T&#xfc;rker</a>, <a href="http://arxiv.org/find/cs/1/au:+Gungor_O/0/1/0/all/0/1">Onur G&#xfc;ng&#xf6;r</a>, <a href="http://arxiv.org/find/cs/1/au:+Uskudarli_S/0/1/0/all/0/1">Susan &#xdc;sk&#xfc;darl&#x131;</a></p>
<p>The recent advances in natural language processing have predominantly favored
well-resourced English-centric models, resulting in a significant gap with
low-resource languages. In this work, we introduce the language model TURNA,
which is developed for the low-resource language Turkish and is capable of both
natural language understanding and generation tasks. TURNA is pretrained with
an encoder-decoder architecture based on the unified framework UL2 with a
diverse corpus that we specifically curated for this purpose. We evaluated
TURNA with three generation tasks and five understanding tasks for Turkish. The
results show that TURNA outperforms several multilingual models in both
understanding and generation tasks, and competes with monolingual Turkish
models in understanding tasks. TURNA is made available at
https://huggingface.co/boun-tabi-LMG/TURNA .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.14400">Modular Adaptation of Multilingual Encoders to Written Swiss German Dialect. (arXiv:2401.14400v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1">Jannis Vamvas</a>, <a href="http://arxiv.org/find/cs/1/au:+Aepli_N/0/1/0/all/0/1">No&#xeb;mi Aepli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1">Rico Sennrich</a></p>
<p>Creating neural text encoders for written Swiss German is challenging due to
a dearth of training data combined with dialectal variation. In this paper, we
build on several existing multilingual encoders and adapt them to Swiss German
using continued pre-training. Evaluation on three diverse downstream tasks
shows that simply adding a Swiss German adapter to a modular encoder achieves
97.5% of fully monolithic adaptation performance. We further find that for the
task of retrieving Swiss German sentences given Standard German queries,
adapting a character-level model is more effective than the other adaptation
strategies. We release our code and the models trained for our experiments at
https://github.com/ZurichNLP/swiss-german-text-encoders
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.03818">Retrieval augmentation of large language models for lay language generation. (arXiv:2211.03818v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yue Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1">Wei Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1">Gondy Leroy</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1">Trevor Cohen</a></p>
<p>Recent lay language generation systems have used Transformer models trained
on a parallel corpus to increase health information accessibility. However, the
applicability of these models is constrained by the limited size and topical
breadth of available corpora. We introduce CELLS, the largest (63k pairs) and
broadest-ranging (12 journals) parallel corpus for lay language generation. The
abstract and the corresponding lay language summary are written by domain
experts, assuring the quality of our dataset. Furthermore, qualitative
evaluation of expert-authored plain language summaries has revealed background
explanation as a key strategy to increase accessibility. Such explanation is
challenging for neural models to generate because it goes beyond simplification
by adding content absent from the source. We derive two specialized paired
corpora from CELLS to address key challenges in lay language generation:
generating background explanations and simplifying the original abstract. We
adopt retrieval-augmented models as an intuitive fit for the task of background
explanation generation, and show improvements in summary quality and simplicity
while maintaining factual correctness. Taken together, this work presents the
first comprehensive study of background explanation for lay language
generation, paving the path for disseminating scientific knowledge to a broader
audience. CELLS is publicly available at:
https://github.com/LinguisticAnomalies/pls_retrieval.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00509">CultureBERT: Measuring Corporate Culture With Transformer-Based Language Models. (arXiv:2212.00509v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1">Sebastian Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasch_S/0/1/0/all/0/1">Stefan Pasch</a></p>
<p>This paper introduces transformer-based language models to the literature
measuring corporate culture from text documents. We compile a unique data set
of employee reviews that were labeled by human evaluators with respect to the
information the reviews reveal about the firms' corporate culture. Using this
data set, we fine-tune state-of-the-art transformer-based language models to
perform the same classification task. In out-of-sample predictions, our
language models classify 17 to 30 percentage points more of employee reviews in
line with human evaluators than traditional approaches of text classification.
We make our models publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16416">Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering. (arXiv:2303.16416v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jingcheng Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xueqing Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Keloth_V/0/1/0/all/0/1">Vipina Kuttichi Keloth</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1">Xu Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yujia Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zehan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaoqian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiyong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_K/0/1/0/all/0/1">Kirk Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hua Xu</a></p>
<p>Objective: This study quantifies the capabilities of GPT-3.5 and GPT-4 for
clinical named entity recognition (NER) tasks and proposes task-specific
prompts to improve their performance. Materials and Methods: We evaluated these
models on two clinical NER tasks: (1) to extract medical problems, treatments,
and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2
concept extraction shared task, and (2) identifying nervous system
disorder-related adverse events from safety reports in the vaccine adverse
event reporting system (VAERS). To improve the GPT models' performance, we
developed a clinical task-specific prompt framework that includes (1) baseline
prompts with task description and format specification, (2) annotation
guideline-based prompts, (3) error analysis-based instructions, and (4)
annotated samples for few-shot learning. We assessed each prompt's
effectiveness and compared the models to BioClinicalBERT. Results: Using
baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804
for MTSamples, and 0.301, 0.593 for VAERS. Additional prompt components
consistently improved model performance. When all four components were used,
GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and
0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt
framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the
MTSamples dataset and 0.802 for the VAERS), it is very promising considering
few training samples are needed. Conclusion: While direct application of GPT
models to clinical NER tasks falls short of optimal performance, our
task-specific prompt framework, incorporating medical knowledge and training
samples, significantly enhances GPT models' feasibility for potential clinical
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04928">From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts. (arXiv:2305.04928v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kosprdic_M/0/1/0/all/0/1">Milo&#x161; Ko&#x161;prdi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Prodanovic_N/0/1/0/all/0/1">Nikola Prodanovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ljajic_A/0/1/0/all/0/1">Adela Ljaji&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Basaragin_B/0/1/0/all/0/1">Bojana Ba&#x161;aragin</a>, <a href="http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1">Nikola Milo&#x161;evi&#x107;</a></p>
<p>Supervised named entity recognition (NER) in the biomedical domain depends on
large sets of annotated texts with the given named entities. The creation of
such datasets can be time-consuming and expensive, while extraction of new
entities requires additional annotation tasks and retraining the model. To
address these challenges, this paper proposes a method for zero- and few-shot
NER in the biomedical domain. The method is based on transforming the task of
multi-class token classification into binary token classification and
pre-training on a large amount of datasets and biomedical entities, which allow
the model to learn semantic relations between the given and potentially novel
named entity labels. We have achieved average F1 scores of 35.44% for zero-shot
NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot
NER on 9 diverse evaluated biomedical entities with fine-tuned PubMedBERT-based
model. The results demonstrate the effectiveness of the proposed method for
recognizing new biomedical entities with no or limited number of examples,
outperforming previous transformer-based methods, and being comparable to
GPT3-based models using models with over 1000 times fewer parameters. We make
models and developed code publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10818">Diffusion Language Models Generation Can Be Halted Early. (arXiv:2305.10818v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vaina_S/0/1/0/all/0/1">Sofia Maria Lo Cicero Vaina</a>, <a href="http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1">Nikita Balagansky</a>, <a href="http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1">Daniil Gavrilov</a></p>
<p>Diffusion Language models (DLMs) are a promising avenue for text generation
due to their practical properties on tractable controllable generation. They
also have the advantage of not having to predict text autoregressively.
However, despite these notable features, DLMs have not yet reached the
performance levels of their autoregressive counterparts. One of the ways to
reduce the performance gap between these two types of language models is to
speed up the generation of DLMs. Therefore, we propose a novel methodology to
address this issue in this work. It enables the execution of more generation
steps within a given time frame, leading to higher-quality outputs.
Specifically, our methods estimate DLMs completeness of text generation and
allow adaptive halting of the generation process. We evaluate our methods on
Plaid, SSD, and CDCD DLMs and create a cohesive perspective on their generation
workflows. Finally, we confirm that our methods allow halting these models and
decrease the generation time by $10$-$40$\% without a drop in the quality of
model samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14603">OpenPI2.0: An Improved Dataset for Entity Tracking in Texts. (arXiv:2305.14603v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hainiu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kommula_A/0/1/0/all/0/1">Abhinav Kommula</a>, <a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1">Chris Callison-Burch</a>, <a href="http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1">Niket Tandon</a></p>
<p>Much text describes a changing world (e.g., procedures, stories, newswires),
and understanding them requires tracking how entities change. An earlier
dataset, OpenPI, provided crowdsourced annotations of entity state changes in
text. However, a major limitation was that those annotations were free-form and
did not identify salient changes, hampering model evaluation. To overcome these
limitations, we present an improved dataset, OpenPI2.0, where entities and
attributes are fully canonicalized and additional entity salience annotations
are added. On our fairer evaluation setting, we find that current
state-of-the-art language models are far from competent. We also show that
using state changes of salient entities as a chain-of-thought prompt,
downstream performance is improved on tasks such as question answering and
classical planning, outperforming the setting involving all related entities
indiscriminately. We offer OpenPI2.0 for the continued development of models
that can understand the dynamics of entities in text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00789">Improved Cross-Lingual Transfer Learning For Automatic Speech Translation. (arXiv:2306.00789v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1">Sameer Khurana</a>, <a href="http://arxiv.org/find/cs/1/au:+Dawalatabad_N/0/1/0/all/0/1">Nauman Dawalatabad</a>, <a href="http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1">Antoine Laurent</a>, <a href="http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1">Luis Vicente</a>, <a href="http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1">Pablo Gimeno</a>, <a href="http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1">Victoria Mingote</a>, <a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1">James Glass</a></p>
<p>Research in multilingual speech-to-text translation is topical. Having a
single model that supports multiple translation tasks is desirable. The goal of
this work it to improve cross-lingual transfer learning in multilingual
speech-to-text translation via semantic knowledge distillation. We show that by
initializing the encoder of the encoder-decoder sequence-to-sequence
translation model with SAMU-XLS-R, a multilingual speech transformer encoder
trained using multi-modal (speech-text) semantic knowledge distillation, we
achieve significantly better cross-lingual task knowledge transfer than the
baseline XLS-R, a multilingual speech transformer encoder trained via
self-supervised learning. We demonstrate the effectiveness of our approach on
two popular datasets, namely, CoVoST-2 and Europarl. On the 21 translation
tasks of the CoVoST-2 benchmark, we achieve an average improvement of 12.8 BLEU
points over the baselines. In the zero-shot translation scenario, we achieve an
average gain of 18.8 and 11.9 average BLEU points on unseen medium and
low-resource languages. We make similar observations on Europarl speech
translation benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08302">Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Shirui Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Linhao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yufei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiapu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xindong Wu</a></p>
<p>Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13588">System-Level Natural Language Feedback. (arXiv:2306.13588v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Weizhe Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1">Jason Weston</a></p>
<p>Natural language (NL) feedback offers rich insights into user experience.
While existing studies focus on an instance-level approach, where feedback is
used to refine specific examples, we introduce a framework for system-level use
of NL feedback. We show how to use feedback to formalize system-level design
decisions in a human-in-the-loop-process -- in order to produce better models.
In particular this is done through: (i) metric design for tasks; and (ii)
language model prompt design for refining model responses. We conduct two case
studies of this approach for improving search query and dialog response
generation, demonstrating the effectiveness of system-level feedback. We show
the combination of system-level and instance-level feedback brings further
gains, and that human written instance-level feedback results in more grounded
refinements than GPT-3.5 written ones, underlying the importance of human
feedback for building systems. We release our code and data at
https://github.com/yyy-Apple/Sys-NL-Feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14806">A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling. (arXiv:2306.14806v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Ye Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Huazheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Wen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenxin Hu</a></p>
<p>The goal of document-level relation extraction (RE) is to identify relations
between entities that span multiple sentences. Recently, incomplete labeling in
document-level RE has received increasing attention, and some studies have used
methods such as positive-unlabeled learning to tackle this issue, but there is
still a lot of room for improvement. Motivated by this, we propose a
positive-augmentation and positive-mixup positive-unlabeled metric learning
framework (P3M). Specifically, we formulate document-level RE as a metric
learning problem. We aim to pull the distance closer between entity pair
embedding and their corresponding relation embedding, while pushing it farther
away from the none-class relation embedding. Additionally, we adapt the
positive-unlabeled learning to this loss objective. In order to improve the
generalizability of the model, we use dropout to augment positive samples and
propose a positive-none-class mixup method. Extensive experiments show that P3M
improves the F1 score by approximately 4-10 points in document-level RE with
incomplete labeling, and achieves state-of-the-art results in fully labeled
scenarios. Furthermore, P3M has also demonstrated robustness to prior
estimation bias in incomplete labeled scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00162">What do self-supervised speech models know about words?. (arXiv:2307.00162v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1">Ankita Pasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Chien_C/0/1/0/all/0/1">Chung-Ming Chien</a>, <a href="http://arxiv.org/find/cs/1/au:+Settle_S/0/1/0/all/0/1">Shane Settle</a>, <a href="http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1">Karen Livescu</a></p>
<p>Many self-supervised speech models (S3Ms) have been introduced over the last
few years, improving performance and data efficiency on various speech tasks.
However, these empirical successes alone do not give a complete picture of what
is learned during pre-training. Recent work has begun analyzing how S3Ms encode
certain properties, such as phonetic and speaker information, but we still lack
a proper understanding of knowledge encoded at the word level and beyond. In
this work, we use lightweight analysis methods to study segment-level
linguistic properties -- word identity, boundaries, pronunciation, syntactic
features, and semantic features -- encoded in S3Ms. We present a comparative
study of layer-wise representations from ten S3Ms and find that (i) the
frame-level representations within each word segment are not all equally
informative, and (ii) the pre-training objective and model size heavily
influence the accessibility and distribution of linguistic information across
layers. We also find that on several tasks -- word discrimination, word
segmentation, and semantic sentence similarity -- S3Ms trained with visual
grounding outperform their speech-only counterparts. Finally, our task-based
analyses demonstrate an improved performance on word segmentation and acoustic
word discrimination while using simpler methods than prior work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01673">Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Baas_M/0/1/0/all/0/1">Matthew Baas</a>, <a href="http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1">Herman Kamper</a></p>
<p>Can we develop a model that can synthesize realistic speech directly from a
latent space, without explicit conditioning? Despite several efforts over the
last decade, previous adversarial and diffusion-based approaches still struggle
to achieve this, even on small-vocabulary datasets. To address this, we propose
AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional
speech synthesis tailored to learn a disentangled latent space. Building upon
the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a
disentangled latent vector which is then mapped to a sequence of audio features
so that signal aliasing is suppressed at every layer. To successfully train
ASGAN, we introduce a number of new techniques, including a modification to
adaptive discriminator augmentation which probabilistically skips discriminator
updates. We apply it on the small-vocabulary Google Speech Commands digits
dataset, where it achieves state-of-the-art results in unconditional speech
synthesis. It is also substantially faster than existing top-performing
diffusion models. We confirm that ASGAN's latent space is disentangled: we
demonstrate how simple linear operations in the space can be used to perform
several tasks unseen during training. Specifically, we perform evaluations in
voice conversion, speech enhancement, speaker verification, and keyword
classification. Our work indicates that GANs are still highly competitive in
the unconditional speech synthesis landscape, and that disentangled latent
spaces can be used to aid generalization to unseen tasks. Code, models,
samples: https://github.com/RF5/simple-asgan/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.16778">KoBBQ: Korean Bias Benchmark for Question Answering. (arXiv:2307.16778v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Jiho Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jiseon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Nayeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1">Haneul Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1">Alice Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hwaran Lee</a></p>
<p>The Bias Benchmark for Question Answering (BBQ) is designed to evaluate
social biases of language models (LMs), but it is not simple to adapt this
benchmark to cultural contexts other than the US because social biases depend
heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias
benchmark dataset, and we propose a general framework that addresses
considerations for cultural adaptation of a dataset. Our framework includes
partitioning the BBQ dataset into three classes--Simply-Transferred (can be
used directly after cultural translation), Target-Modified (requires
localization in target groups), and Sample-Removed (does not fit Korean
culture)-- and adding four new categories of bias specific to Korean culture.
We conduct a large-scale survey to collect and validate the social biases and
the targets of the biases that reflect the stereotypes in Korean culture. The
resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12
categories of social bias. We use KoBBQ to measure the accuracy and bias scores
of several state-of-the-art multilingual LMs. The results clearly show
differences in the bias of LMs as measured by KoBBQ and a machine-translated
version of BBQ, demonstrating the need for and utility of a well-constructed,
culturally-aware social bias benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.01154">Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1">Davide Maltoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrara_M/0/1/0/all/0/1">Matteo Ferrara</a></p>
<p>A better understanding of the emergent computation and problem-solving
capabilities of recent large language models is of paramount importance to
further improve them and broaden their applicability. This work investigates
how a language model, trained to predict the next token, can perform arithmetic
computations generalizing beyond training data. Binary addition and
multiplication constitute a good testbed for this purpose, since they require a
very small vocabulary and exhibit relevant input/output discontinuities making
smooth input interpolation ineffective for novel data. We successfully trained
a light language model to learn these tasks and ran a number of experiments to
investigate the extrapolation capabilities and internal information processing.
Our findings support the hypothesis that the language model works as an
Encoding-Regression-Decoding machine where the computation takes place in the
value space once the input token representation is mapped to an appropriate
internal representation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06450">ERNetCL: A novel emotion recognition network in textual conversation based on curriculum learning strategy. (arXiv:2308.06450v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yingjian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhigang Zeng</a></p>
<p>Emotion recognition in conversation (ERC) has emerged as a research hotspot
in domains such as conversational robots and question-answer systems. How to
efficiently and adequately retrieve contextual emotional cues has been one of
the key challenges in the ERC task. Existing efforts do not fully model the
context and employ complex network structures, resulting in limited performance
gains. In this paper, we propose a novel emotion recognition network based on
curriculum learning strategy (ERNetCL). The proposed ERNetCL primarily consists
of temporal encoder (TE), spatial encoder (SE), and curriculum learning (CL)
loss. We utilize TE and SE to combine the strengths of previous methods in a
simplistic manner to efficiently capture temporal and spatial contextual
information in the conversation. To ease the harmful influence resulting from
emotion shift and simulate the way humans learn curriculum from easy to hard,
we apply the idea of CL to the ERC task to progressively optimize the network
parameters. At the beginning of training, we assign lower learning weights to
difficult samples. As the epoch increases, the learning weights for these
samples are gradually raised. Extensive experiments on four datasets exhibit
that our proposed method is effective and dramatically beats other baseline
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04661">Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chenmien Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a></p>
<p>While large language models (LLMs) have enabled learning knowledge from the
pre-training corpora, the acquired knowledge may be fundamentally incorrect or
outdated over time, which necessitates rectifying the knowledge of the language
model (LM) after the training. A promising approach involves employing a
hyper-network to generate parameter shift, whereas existing hyper-networks
suffer from inferior scalability in synchronous editing operation amount. To
mitigate the problem, we propose the MAssive Language Model Editing Network
(MALMEN), which formulates the parameter shift aggregation as the least square
problem, subsequently updating the LM parameters using the normal equation. To
accommodate editing multiple facts simultaneously with limited memory budgets,
we separate the computation on the hyper-network and LM, enabling arbitrary
batch size on both neural networks. Our method is evaluated by editing up to
thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,
T5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,
i.e., closed book fact-checking and question answering. Remarkably, MALMEN is
capable of editing hundreds of times more facts than strong baselines with the
identical hyper-network architecture and outperforms editor specifically
designed for GPT. Our code is available at
https://github.com/ChenmienTan/malmen.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04928">Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1">Marios Papachristou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Longqi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1">Chin-Chia Hsu</a></p>
<p>In various work contexts, such as meeting scheduling, collaborating, and
project planning, collective decision-making is essential but often challenging
due to diverse individual preferences, varying work focuses, and power dynamics
among members. To address this, we propose a system leveraging Large Language
Models (LLMs) to facilitate group decision-making by managing conversations and
balancing preferences among individuals. Our system aims to extract individual
preferences from conversations and suggest options that satisfy the preferences
of the members. We specifically apply this system to corporate meeting
scheduling. We create synthetic employee profiles and simulate conversations at
scale, leveraging LLMs to evaluate the system performance as a novel approach
to conducting a user study. Our results indicate efficient coordination with
reduced interactions between the members and the LLM-based system. The system
refines and improves its proposed options over time, ensuring that many of the
members' individual preferences are satisfied in an equitable way. Finally, we
conduct a survey study involving human participants to assess our system's
ability to aggregate preferences and reasoning about them. Our findings show
that the system exhibits strong performance in both dimensions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.11482">Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a></p>
<p>This paper presents a comprehensive study of Meta Prompting, an innovative
technique reshaping the utilization of large language models (LLMs),
multi-modal foundation models, and AI systems in problem-solving and data
interpretation. Grounded in type theory and category theory, Meta Prompting
emphasizes the structure and syntax of information over traditional
content-centric methods. The paper explores the formal definitions of Meta
Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its
effectiveness in various AI applications. A key focus is on extending Meta
Prompting to complex reasoning tasks, showing how it effectively deconstructs
intricate problems into simpler sub-problems, enhancing token efficiency and
enabling more equitable problem-solving comparisons, especially against
few-shot example methods. Additionally, the paper introduces Meta Prompting for
Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative,
metaprogramming-like manner. This innovative approach marks a significant leap
in AI's autonomous and adaptive capabilities. The paper also pioneers the
integration of Meta Prompting into multi-modal foundation model settings,
tackling the challenges and opportunities of incorporating varied data types
such as images, audio, and video within the structured Meta Prompting
framework. (The code is available at
https://github.com/meta-prompting/meta-prompting)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13892">General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Bingkang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaodan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1">Dehan Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yulei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zongzhen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1">Honglei Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Longtao Huang</a></p>
<p>The social biases and unwelcome stereotypes revealed by pretrained language
models are becoming obstacles to their application. Compared to numerous
debiasing methods targeting word level, there has been relatively less
attention on biases present at phrase level, limiting the performance of
debiasing in discipline domains. In this paper, we propose an automatic
multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which
is capable of mitigating phrase-level biases in masked language models.
Specifically, our method consists of a \textit{phrase filter stage} that
generates stereotypical phrases from Wikipedia pages as well as a \textit{model
debias stage} that can debias models at the multi-token level to tackle bias
challenges on phrases. The latter searches for prompts that trigger model's
bias, and then uses them for debiasing. State-of-the-art results on standard
datasets and metrics show that our approach can significantly reduce gender
biases on both career and multiple disciplines, across models with varying
parameter sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14067">Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study Based on Lexical Diversity and Divergence. (arXiv:2311.14067v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stricker_A/0/1/0/all/0/1">Armand Stricker</a>, <a href="http://arxiv.org/find/cs/1/au:+Paroubek_P/0/1/0/all/0/1">Patrick Paroubek</a></p>
<p>As a recent development, task-oriented dialogues (TODs) have been enriched
with chitchat in an effort to make dialogues more diverse and engaging. This
enhancement is particularly valuable as TODs are often confined to narrow
domains, making the mitigation of repetitive and predictable responses a
significant challenge. This paper presents a comparative analysis of three
chitchat enhancements, aiming to identify the most effective approach in terms
of diversity. Additionally, we quantify the divergence between the added
chitchat, the original task-oriented language, and chitchat typically found in
chitchat datasets, highlighting the top 20 divergent keywords for each
comparison. Our findings drive a discussion on future enhancements for
augmenting TODs, emphasizing the importance of grounding dialogues beyond the
task to achieve more diverse and natural exchanges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05934">Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ovadia_O/0/1/0/all/0/1">Oded Ovadia</a>, <a href="http://arxiv.org/find/cs/1/au:+Brief_M/0/1/0/all/0/1">Menachem Brief</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishaeli_M/0/1/0/all/0/1">Moshik Mishaeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Elisha_O/0/1/0/all/0/1">Oren Elisha</a></p>
<p>Large language models (LLMs) encapsulate a vast amount of factual information
within their pre-trained weights, as evidenced by their ability to answer
diverse questions across different domains. However, this knowledge is
inherently limited, relying heavily on the characteristics of the training
data. Consequently, using external datasets to incorporate new information or
refine the capabilities of LLMs on previously seen information poses a
significant challenge. In this study, we compare two common approaches:
unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate
both approaches on a variety of knowledge-intensive tasks across different
topics. Our findings reveal that while unsupervised fine-tuning offers some
improvement, RAG consistently outperforms it, both for existing knowledge
encountered during training and entirely new knowledge. Moreover, we find that
LLMs struggle to learn new factual information through unsupervised
fine-tuning, and that exposing them to numerous variations of the same fact
during training could alleviate this problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09043">Topic Bias in Emotion Classification. (arXiv:2312.09043v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1">Maximilian Wegge</a>, <a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1">Roman Klinger</a></p>
<p>Emotion corpora are typically sampled based on keyword/hashtag search or by
asking study participants to generate textual instances. In any case, these
corpora are not uniform samples representing the entirety of a domain. We
hypothesize that this practice of data acquisition leads to unrealistic
correlations between overrepresented topics in these corpora that harm the
generalizability of models. Such topic bias could lead to wrong predictions for
instances like "I organized the service for my aunt's funeral." when funeral
events are over-represented for instances labeled with sadness, despite the
emotion of pride being more appropriate here. In this paper, we study this
topic bias both from the data and the modeling perspective. We first label a
set of emotion corpora automatically via topic modeling and show that emotions
in fact correlate with specific topics. Further, we see that emotion
classifiers are confounded by such topics. Finally, we show that the
established debiasing method of adversarial correction via gradient reversal
mitigates the issue. Our work points out issues with existing emotion corpora
and that more representative resources are required for fair evaluation of
models predicting affective concepts from text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11562">A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chuanyang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1">Ruihang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jianing Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1">Mengzhe Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yue Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junxian He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xihui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1">Hao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Ming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jifeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1">Hui Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a></p>
<p>Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, e.g., Large Language Models (LLMs), there is a growing
interest in exploring their abilities in reasoning tasks. In this paper, we
introduce seminal foundation models proposed or adaptable for reasoning,
highlighting the latest advancements in various reasoning tasks, methods, and
benchmarks. We then delve into the potential future directions behind the
emergence of reasoning abilities within foundation models. We also discuss the
relevance of multimodal learning, autonomous agents, and super alignment in the
context of reasoning. By discussing these future research directions, we hope
to inspire researchers in their exploration of this field, stimulate further
advancements in reasoning with foundation models, and contribute to the
development of AGI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11819">An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Youshao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weichang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhenglei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1">Fagui Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shangchun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1">Lin Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1">Lei Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaolu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a></p>
<p>Recently, ChatGPT or InstructGPT like large language models (LLM) has made a
significant impact in the AI world. Many works have attempted to reproduce the
complex InstructGPT's training pipeline, namely Reinforcement Learning with
Human Feedback (RLHF). However, the mainstream distributed RLHF training
methods typically adopt a fixed model placement strategy, referred to as the
Flattening strategy. This strategy treats all four interdependent models
involved in RLHF as a single entity, distributing them across all devices and
applying parallelism techniques designed for a single model, regardless of the
different workloads inherent to each model. As a result, this strategy
exacerbates the generation bottlenecks in the RLHF training and degrades the
overall training efficiency. To address these issues, we propose an adaptive
model placement framework that offers two flexible model placement strategies.
The Interleaving strategy helps reduce memory redundancy and communication
costs of RLHF training by placing models without dependencies on exclusive
devices with careful orchestration. On the other hand, the Separation strategy
improves the throughput of model training by separating the training and
inference runtime of the RLHF pipeline with additional shadow models.
Furthermore, our framework provides a simple user interface and allows for the
agile allocation of models across devices in a fine-grained manner for various
training scenarios, involving models of varying sizes and devices of different
scales. Extensive experiments have demonstrated that our Interleaving and
Separation strategies can achieve notable improvements up to 11X, compared to
the current SOTA approaches. The results highlight the effectiveness and
adaptability of our approaches in accelerating the training of distributed
RLHF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15643">Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jiaxin Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yicheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tianshi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yue Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>Abductive reasoning is the process of making educated guesses to provide
explanations for observations. Although many applications require the use of
knowledge for explanations, the utilization of abductive reasoning in
conjunction with structured knowledge, such as a knowledge graph, remains
largely unexplored. To fill this gap, this paper introduces the task of complex
logical hypothesis generation, as an initial step towards abductive logical
reasoning with KG. In this task, we aim to generate a complex logical
hypothesis so that it can explain a set of observations. We find that the
supervised trained generative model can generate logical hypotheses that are
structurally closer to the reference hypothesis. However, when generalized to
unseen observations, this training objective does not guarantee better
hypothesis generation. To address this, we introduce the Reinforcement Learning
from Knowledge Graph (RLF-KG) method, which minimizes differences between
observations and conclusions drawn from generated hypotheses according to the
KG. Experiments show that, with RLF-KG's assistance, the generated hypotheses
provide better explanations, and achieve state-of-the-art results on three
widely used KGs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01623">Can AI Be as Creative as Humans?. (arXiv:2401.01623v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1">Michael Mozer</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1">Alex Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Linjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Weijie J Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Michael Qizhe Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_H/0/1/0/all/0/1">Hannah Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>Creativity serves as a cornerstone for societal progress and innovation. With
the rise of advanced generative AI models capable of tasks once reserved for
human creativity, the study of AI's creative potential becomes imperative for
its responsible development and application. In this paper, we prove in theory
that AI can be as creative as humans under the condition that it can properly
fit the data generated by human creators. Therefore, the debate on AI's
creativity is reduced into the question of its ability to fit a sufficient
amount of data. To arrive at this conclusion, this paper first addresses the
complexities in defining creativity by introducing a new concept called
Relative Creativity. Rather than attempting to define creativity universally,
we shift the focus to whether AI can match the creative abilities of a
hypothetical human. The methodological shift leads to a statistically
quantifiable assessment of AI's creativity, term Statistical Creativity. This
concept, statistically comparing the creative abilities of AI with those of
specific human groups, facilitates theoretical exploration of AI's creative
potential. Our analysis reveals that by fitting extensive conditional data
without marginalizing out the generative conditions, AI can emerge as a
hypothetical new creator. The creator possesses the same creative abilities on
par with the human creators it was trained on. Building on theoretical
findings, we discuss the application in prompt-conditioned autoregressive
models, providing a practical means for evaluating creative abilities of
generative AI models, such as Large Language Models (LLMs). Additionally, this
study provides an actionable training guideline, bridging the theoretical
quantification of creativity with practical model training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.05561">TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lichao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Siyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qihui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chujie Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yixin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1">Wenhan Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiner Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yixin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yijue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhikun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1">Bhavya Kailkhura</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1">Eric Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1">Heng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kellis_M/0/1/0/all/0/1">Manolis Kellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1">Marinka Zitnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Meng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jian Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jieyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1">John Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kaidi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lifang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lifu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1">Michael Backes</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1">Neil Zhenqiang Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ran Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1">Rex Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shuiwang Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1">Suman Jana</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yanfang Ye</a>, et al. (3 additional authors not shown)</p>
<p>Large language models (LLMs), exemplified by ChatGPT, have gained
considerable attention for their excellent natural language processing
capabilities. Nonetheless, these LLMs present many challenges, particularly in
the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs
emerges as an important topic. This paper introduces TrustLLM, a comprehensive
study of trustworthiness in LLMs, including principles for different dimensions
of trustworthiness, established benchmark, evaluation, and analysis of
trustworthiness for mainstream LLMs, and discussion of open challenges and
future directions. Specifically, we first propose a set of principles for
trustworthy LLMs that span eight different dimensions. Based on these
principles, we further establish a benchmark across six dimensions including
truthfulness, safety, fairness, robustness, privacy, and machine ethics. We
then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of
over 30 datasets. Our findings firstly show that in general trustworthiness and
utility (i.e., functional effectiveness) are positively related. Secondly, our
observations reveal that proprietary LLMs generally outperform most open-source
counterparts in terms of trustworthiness, raising concerns about the potential
risks of widely accessible open-source LLMs. However, a few open-source LLMs
come very close to proprietary ones. Thirdly, it is important to note that some
LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent
that they compromise their utility by mistakenly treating benign prompts as
harmful and consequently not responding. Finally, we emphasize the importance
of ensuring transparency not only in the models themselves but also in the
technologies that underpin trustworthiness. Knowing the specific trustworthy
technologies that have been employed is crucial for analyzing their
effectiveness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.08491">Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models. (arXiv:2401.08491v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1">Tassilo Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1">Moin Nabi</a></p>
<p>The generation of undesirable and factually incorrect content of large
language models poses a significant challenge and remains largely an unsolved
issue. This paper studies the integration of a contrastive learning objective
for fine-tuning LLMs for implicit knowledge editing and controlled text
generation. Optimizing the training objective entails aligning text
perplexities in a contrastive fashion. To facilitate training the model in a
self-supervised fashion, we leverage an off-the-shelf LLM for training data
generation. We showcase applicability in the domain of detoxification. Herein,
the proposed approach leads to a significant decrease in the generation of
toxic content while preserving general utility for downstream tasks such as
commonsense reasoning and reading comprehension. The proposed approach is
conceptually simple but empirically powerful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10286">Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Linghan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xiaojun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiayuan Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1">Yue Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1">Gang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongwei Chen</a></p>
<p>While the alignment between tasks and training corpora is a fundamental
consensus in the application of language models, our series of experiments and
the metrics we designed reveal that code-based Large Language Models (LLMs)
significantly outperform models trained on data that is closely matched to the
tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to
Chinese hallucinations, models exhibiting fewer linguistic features of the
Chinese language achieve better performance. Our experimental results can be
easily replicated in Chinese data processing tasks, such as preparing data for
Retrieval-Augmented Generation (RAG), by simply replacing the base model with a
code-based model. Additionally, our research offers a distinct perspective for
discussion on the philosophical "Chinese Room" thought experiment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.10529">Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hongjin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuancheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1">Feihong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Taixi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1">Gedas Bertasius</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated proficiency in
handling a variety of visual-language tasks. However, current MLLM benchmarks
are predominantly designed to evaluate reasoning based on static information
about a single image, and the ability of modern MLLMs to extrapolate from image
sequences, which is essential for understanding our ever-changing world, has
been less investigated. To address this challenge, this paper introduces
Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning
abilities. Mementos features 4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning
performance. Through a careful evaluation of nine recent MLLMs on Mementos,
including GPT-4V and Gemini, we find that they struggle to accurately describe
dynamic information about given image sequences, often leading to
hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting
MLLMs' sequential image reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the
compounding impact of behavioral hallucinations. Our dataset is available at
https://github.com/umd-huang-lab/Mementos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12522">BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Feng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1">Hanling Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiaotian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guangming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1">Rong Xiao</a></p>
<p>Large language models (LLMs) commonly employ autoregressive generation during
inference, leading to high memory bandwidth demand and consequently extended
latency. To mitigate this inefficiency, we present Bi-directional Tuning for
lossless Acceleration (BiTA), an innovative method expediting LLMs via
streamlined semi-autoregressive generation and draft verification. Inspired by
the concept of prompt tuning, we enhance LLMs with a parameter-efficient design
called bi-directional tuning for the capability in semi-autoregressive
generation. Employing efficient tree-based decoding, the models perform draft
candidate generation and verification in parallel, ensuring outputs identical
to their autoregressive counterparts under greedy sampling. BiTA serves as a
lightweight plug-in module, seamlessly boosting the inference efficiency of
existing LLMs without requiring additional assistance models or incurring
significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat
achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments
confirm our method surpasses state-of-the-art acceleration techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12689">Energy-based Automated Model Evaluation. (arXiv:2401.12689v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1">Ru Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1">Heming Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haobo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yawen Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zenan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junbo Zhao</a></p>
<p>The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE's validity, together with its
superiority compared with prior approaches. We also prove MDE's versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels. Code and data are
available: https://github.com/pengr/Energy_AutoEval
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12756">What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holtermann_C/0/1/0/all/0/1">Carolin Holtermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Frohmann_M/0/1/0/all/0/1">Markus Frohmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1">Navid Rekabsaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1">Anne Lauscher</a></p>
<p>The knowledge encapsulated in a model is the core factor determining its
final performance on downstream tasks. Much research in NLP has focused on
efficient methods for storing and adapting different types of knowledge, e.g.,
in dedicated modularized structures, and on how to effectively combine these,
e.g., by learning additional parameters. However, given the many possible
options, a thorough understanding of the mechanisms involved in these
compositions is missing, and hence it remains unclear which strategies to
utilize. To address this research gap, we propose a novel framework for
zero-shot module composition, which encompasses existing and some novel
variations for selecting, weighting, and combining parameter modules under a
single unified notion. Focusing on the scenario of domain knowledge and adapter
layers, our framework provides a systematic unification of concepts, allowing
us to conduct the first comprehensive benchmarking study of various zero-shot
knowledge composition strategies. In particular, we test two module combination
methods and five selection and weighting strategies for their effectiveness and
efficiency in an extensive experimental setup. Our results highlight the
efficacy of ensembling but also hint at the power of simple though
often-ignored weighting methods. Further in-depth analyses allow us to
understand the role of weighting vs. top-k selection, and show that, to a
certain extent, the performance of adapter composition can even be predicted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13527">SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation. (arXiv:2401.13527v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1">Jun Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shimin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yaqian Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Benefiting from effective speech modeling, current Speech Large Language
Models (SLLMs) have demonstrated exceptional capabilities in in-context speech
generation and efficient generalization to unseen speakers. However, the
prevailing information modeling process is encumbered by certain redundancies,
leading to inefficiencies in speech generation. We propose Chain-of-Information
Generation (CoIG), a method for decoupling semantic and perceptual information
in large-scale speech generation. Building on this, we develop SpeechGPT-Gen,
an 8-billion-parameter SLLM efficient in semantic and perceptual information
modeling. It comprises an autoregressive model based on LLM for semantic
information modeling and a non-autoregressive model employing flow matching for
perceptual information modeling. Additionally, we introduce the novel approach
of infusing semantic information into the prior distribution to enhance the
efficiency of flow matching. Extensive experimental results demonstrate that
SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice
conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable
proficiency in capturing and modeling speech's semantic and perceptual
dimensions. Code and models are available at
https://github.com/0nutation/SpeechGPT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.13601">MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Duzhen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yahan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenxing Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiahua Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1">Dan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1">Chenhui Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>In the past year, MultiModal Large Language Models (MM-LLMs) have undergone
substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or
outputs via cost-effective training strategies. The resulting models not only
preserve the inherent reasoning and decision-making capabilities of LLMs but
also empower a diverse range of MM tasks. In this paper, we provide a
comprehensive survey aimed at facilitating further research of MM-LLMs.
Specifically, we first outline general design formulations for model
architecture and training pipeline. Subsequently, we provide brief
introductions of $26$ existing MM-LLMs, each characterized by its specific
formulations. Additionally, we review the performance of MM-LLMs on mainstream
benchmarks and summarize key training recipes to enhance the potency of
MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently
maintaining a real-time tracking website for the latest developments in the
field. We hope that this survey contributes to the ongoing advancement of the
MM-LLMs domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.12255">Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiashu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Mingyu Derek Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1">Pang Wei Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>The exorbitant cost of training Large language models (LLMs) from scratch
makes it essential to fingerprint the models to protect intellectual property
via ownership authentication and to ensure downstream users and developers
comply with their license terms (e.g. restricting commercial use). In this
study, we present a pilot study on LLM fingerprinting as a form of very
lightweight instruction tuning. Model publisher specifies a confidential
private key and implants it as an instruction backdoor that causes the LLM to
generate specific text when the key is present. Results on 11 popularly-used
LLMs showed that this approach is lightweight and does not affect the normal
behavior of the model. It also prevents publisher overclaim, maintains
robustness against fingerprint guessing and parameter-efficient training, and
supports multi-stage fingerprinting akin to MIT License. Code is available in
https://cnut1648.github.io/Model-Fingerprint/.
</p>
</p>
</div>

    </div>
    </body>
    