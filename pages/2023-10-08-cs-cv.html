<!DOCTYPE html>
<html>
<head>
<title>2023-10-08-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2310.03033">Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition. (arXiv:2310.03033v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Postovan_A/0/1/0/all/0/1">Andreea Postovan</a>, <a href="http://arxiv.org/find/cs/1/au:+Erascu_M/0/1/0/all/0/1">M&#x103;d&#x103;lina Era&#x15f;cu</a></p>
<p>Traffic signs play a critical role in road safety and traffic management for
autonomous driving systems. Accurate traffic sign classification is essential
but challenging due to real-world complexities like adversarial examples and
occlusions. To address these issues, binary neural networks offer promise in
constructing classifiers suitable for resource-constrained devices.
</p>
<p>In our previous work, we proposed high-accuracy BNN models for traffic sign
recognition, focusing on compact size for limited computation and energy
resources. To evaluate their local robustness, this paper introduces a set of
benchmark problems featuring layers that challenge state-of-the-art
verification tools. These layers include binarized convolutions, max pooling,
batch normalization, fully connected. The difficulty of the verification
problem is given by the high number of network parameters (905k - 1.7 M), of
the input dimension (2.7k-12k), and of the number of regions (43) as well by
the fact that the neural networks are not sparse.
</p>
<p>The proposed BNN models and local robustness properties can be checked at
https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/traffic_signs_recognition.
</p>
<p>The results of the 4th International Verification of Neural Networks
Competition (VNN-COMP'23) revealed the fact that 4, out of 7, solvers can
handle many of our benchmarks randomly selected (minimum is 6, maximum is 36,
out of 45). Surprisingly, tools output also wrong results or missing
counterexample (ranging from 1 to 4). Currently, our focus lies in exploring
the possibility of achieving a greater count of solved instances by extending
the allotted time (previously set at 8 minutes). Furthermore, we are intrigued
by the reasons behind the erroneous outcomes provided by the tools for certain
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03037">Quantum image edge detection based on eight-direction Sobel operator for NEQR. (arXiv:2310.03037v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a></p>
<p>Quantum Sobel edge detection (QSED) is a kind of algorithm for image edge
detection using quantum mechanism, which can solve the real-time problem
encountered by classical algorithms. However, the existing QSED algorithms only
consider two- or four-direction Sobel operator, which leads to a certain loss
of edge detail information in some high-definition images. In this paper, a
novel QSED algorithm based on eight-direction Sobel operator is proposed, which
not only reduces the loss of edge information, but also simultaneously
calculates eight directions' gradient values of all pixel in a quantum image.
In addition, the concrete quantum circuits, which consist of gradient
calculation, non-maximum suppression, double threshold detection and edge
tracking units, are designed in details. For a 2^n x 2^n image with q gray
scale, the complexity of our algorithm can be reduced to O(n^2 + q^2), which is
lower than other existing classical or quantum algorithms. And the simulation
experiment demonstrates that our algorithm can detect more edge information,
especially diagonal edges, than the two- and four-direction QSED algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03038">A quantum moving target segmentation algorithm for grayscale video. (arXiv:2310.03038v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenjie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingshan Wu</a></p>
<p>The moving target segmentation (MTS) aims to segment out moving targets in
the video, however, the classical algorithm faces the huge challenge of
real-time processing in the current video era. Some scholars have successfully
demonstrated the quantum advantages in some video processing tasks, but not
concerning moving target segmentation. In this paper, a quantum moving target
segmentation algorithm for grayscale video is proposed, which can use quantum
mechanism to simultaneously calculate the difference of all pixels in all
adjacent frames and then quickly segment out the moving target. In addition, a
feasible quantum comparator is designed to distinguish the grayscale values
with the threshold. Then several quantum circuit units, including three-frame
difference, binarization and AND operation, are designed in detail, and then
are combined together to construct the complete quantum circuits for segmenting
the moving target. For a quantum video with $2^m$ frames (every frame is a
$2^n\times 2^n$ image with $q$ grayscale levels), the complexity of our
algorithm can be reduced to O$(n^2 + q)$. Compared with the classic
counterpart, it is an exponential speedup, while its complexity is also
superior to the existing quantum algorithms. Finally, the experiment is
conducted on IBM Q to show the feasibility of our algorithm in the noisy
intermediate-scale quantum (NISQ) era.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1">Ivan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Eric Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1">Ray Gu</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/EvenJoker/Point-PEFT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03085">Batch-less stochastic gradient descent for compressive learning of deep regularization for image denoising. (arXiv:2310.03085v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Hui Shi</a> (IMB), <a href="http://arxiv.org/find/cs/1/au:+Traonmilin_Y/0/1/0/all/0/1">Yann Traonmilin</a> (IMB), <a href="http://arxiv.org/find/cs/1/au:+Aujol_J/0/1/0/all/0/1">J-F Aujol</a> (IMB)</p>
<p>We consider the problem of denoising with the help of prior information taken
from a database of clean signals or images. Denoising with variational methods
is very efficient if a regularizer well adapted to the nature of the data is
available. Thanks to the maximum a posteriori Bayesian framework, such
regularizer can be systematically linked with the distribution of the data.
With deep neural networks (DNN), complex distributions can be recovered from a
large training database.To reduce the computational burden of this task, we
adapt the compressive learning framework to the learning of regularizers
parametrized by DNN. We propose two variants of stochastic gradient descent
(SGD) for the recovery of deep regularization parameters from a heavily
compressed database. These algorithms outperform the initially proposed method
that was limited to low-dimensional signals, each iteration using information
from the whole database. They also benefit from classical SGD convergence
guarantees. Thanks to these improvements we show that this method can be
applied for patch based image denoising.}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03091">Privacy-preserving Multi-biometric Indexing based on Frequent Binary Patterns. (arXiv:2310.03091v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Osorio_Roig_D/0/1/0/all/0/1">Daile Osorio-Roig</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Soler_L/0/1/0/all/0/1">Lazaro J. Gonzalez-Soler</a>, <a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1">Christian Rathgeb</a>, <a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1">Christoph Busch</a></p>
<p>The development of large-scale identification systems that ensure the privacy
protection of enrolled subjects represents a major challenge. Biometric
deployments that provide interoperability and usability by including efficient
multi-biometric solutions are a recent requirement. In the context of privacy
protection, several template protection schemes have been proposed in the past.
However, these schemes seem inadequate for indexing (workload reduction) in
biometric identification systems. More specifically, they have been used in
identification systems that perform exhaustive searches, leading to a
degradation of computational efficiency. To overcome these limitations, we
propose an efficient privacy-preserving multi-biometric identification system
that retrieves protected deep cancelable templates and is agnostic with respect
to biometric characteristics and biometric template protection schemes. To this
end, a multi-biometric binning scheme is designed to exploit the low
intra-class variation properties contained in the frequent binary patterns
extracted from different types of biometric characteristics. Experimental
results reported on publicly available databases using state-of-the-art Deep
Neural Network (DNN)-based embedding extractors show that the protected
multi-biometric identification system can reduce the computational workload to
approximately 57\% (indexing up to three types of biometric characteristics)
and 53% (indexing up to two types of biometric characteristics), while
simultaneously improving the biometric performance of the baseline biometric
system at the high-security thresholds. The source code of the proposed
multi-biometric indexing approach together with the composed multi-biometric
dataset, will be made available to the research community once the article is
accepted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03106">Creating an Atlas of Normal Tissue for Pruning WSI Patching Through Anomaly Detection. (arXiv:2310.03106v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Nejat_P/0/1/0/all/0/1">Peyman Nejat</a>, <a href="http://arxiv.org/find/eess/1/au:+Alsaafin_A/0/1/0/all/0/1">Areej Alsaafin</a>, <a href="http://arxiv.org/find/eess/1/au:+Alabtah_G/0/1/0/all/0/1">Ghazal Alabtah</a>, <a href="http://arxiv.org/find/eess/1/au:+Comfere_N/0/1/0/all/0/1">Nneka Comfere</a>, <a href="http://arxiv.org/find/eess/1/au:+Mangold_A/0/1/0/all/0/1">Aaron Mangold</a>, <a href="http://arxiv.org/find/eess/1/au:+Murphree_D/0/1/0/all/0/1">Dennis Murphree</a>, <a href="http://arxiv.org/find/eess/1/au:+Zot_P/0/1/0/all/0/1">Patricija Zot</a>, <a href="http://arxiv.org/find/eess/1/au:+Yasir_S/0/1/0/all/0/1">Saba Yasir</a>, <a href="http://arxiv.org/find/eess/1/au:+Garcia_J/0/1/0/all/0/1">Joaquin J. Garcia</a>, <a href="http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1">H.R. Tizhoosh</a></p>
<p>Patching gigapixel whole slide images (WSIs) is an important task in
computational pathology. Some methods have been proposed to select a subset of
patches as WSI representation for downstream tasks. While most of the
computational pathology tasks are designed to classify or detect the presence
of pathological lesions in each WSI, the confounding role and redundant nature
of normal histology in tissue samples are generally overlooked in WSI
representations. In this paper, we propose and validate the concept of an
"atlas of normal tissue" solely using samples of WSIs obtained from normal
tissue biopsies. Such atlases can be employed to eliminate normal fragments of
tissue samples and hence increase the representativeness collection of patches.
We tested our proposed method by establishing a normal atlas using 107 normal
skin WSIs and demonstrated how established indexes and search engines like
Yottixel can be improved. We used 553 WSIs of cutaneous squamous cell carcinoma
(cSCC) to show the advantage. We also validated our method applied to an
external dataset of 451 breast WSIs. The number of selected WSI patches was
reduced by 30% to 50% after utilizing the proposed normal atlas while
maintaining the same indexing and search performance in leave-one-patinet-out
validation for both datasets. We show that the proposed normal atlas shows
promise for unsupervised selection of the most representative patches of the
abnormal/malignant WSI lesions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03108">Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition. (arXiv:2310.03108v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1">Hamid Mohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazerfard_E/0/1/0/all/0/1">Ehsan Nazerfard</a>, <a href="http://arxiv.org/find/cs/1/au:+Firoozi_T/0/1/0/all/0/1">Tahereh Firoozi</a></p>
<p>Video violence recognition based on deep learning concerns accurate yet
scalable human violence recognition. Currently, most state-of-the-art video
violence recognition studies use CNN-based models to represent and categorize
videos. However, recent studies suggest that pre-trained transformers are more
accurate than CNN-based models on various video analysis benchmarks. Yet these
models are not thoroughly evaluated for video violence recognition. This paper
introduces a novel transformer-based Mixture of Experts (MoE) video violence
recognition system. Through an intelligent combination of large vision
transformers and efficient transformer architectures, the proposed system not
only takes advantage of the vision transformer architecture but also reduces
the cost of utilizing large vision transformers. The proposed architecture
maximizes violence recognition system accuracy while actively reducing
computational costs through a reinforcement learning-based router. The
empirical results show the proposed MoE architecture's superiority over
CNN-based models by achieving 92.4% accuracy on the RWF dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03118">Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator. (arXiv:2310.03118v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1">Yongyi Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1">Wenjun Xia</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1">Ge Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Mou_X/0/1/0/all/0/1">Xuanqin Mou</a></p>
<p>Lowering radiation dose per view and utilizing sparse views per scan are two
common CT scan modes, albeit often leading to distorted images characterized by
noise and streak artifacts. Blind image quality assessment (BIQA) strives to
evaluate perceptual quality in alignment with what radiologists perceive, which
plays an important role in advancing low-dose CT reconstruction techniques. An
intriguing direction involves developing BIQA methods that mimic the
operational characteristic of the human visual system (HVS). The internal
generative mechanism (IGM) theory reveals that the HVS actively deduces primary
content to enhance comprehension. In this study, we introduce an innovative
BIQA metric that emulates the active inference process of IGM. Initially, an
active inference module, implemented as a denoising diffusion probabilistic
model (DDPM), is constructed to anticipate the primary content. Then, the
dissimilarity map is derived by assessing the interrelation between the
distorted image and its primary content. Subsequently, the distorted image and
dissimilarity map are combined into a multi-channel image, which is inputted
into a transformer-based image quality evaluator. Remarkably, by exclusively
utilizing this transformer-based quality evaluator, we won the second place in
the MICCAI 2023 low-dose computed tomography perceptual image quality
assessment grand challenge. Leveraging the DDPM-derived primary content, our
approach further improves the performance on the challenge dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03125">Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation. (arXiv:2310.03125v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yihan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1">Brandon Y. Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heng Huang</a></p>
<p>In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03140">ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time Measurements. (arXiv:2310.03140v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1">Bryan Bo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Alali_A/0/1/0/all/0/1">Abrar Alali</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hansi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meegan_N/0/1/0/all/0/1">Nicholas Meegan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gruteser_M/0/1/0/all/0/1">Marco Gruteser</a>, <a href="http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1">Kristin Dana</a>, <a href="http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1">Ashwin Ashok</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Shubham Jain</a></p>
<p>Tracking subjects in videos is one of the most widely used functions in
camera-based IoT applications such as security surveillance, smart city traffic
safety enhancement, vehicle to pedestrian communication and so on. In the
computer vision domain, tracking is usually achieved by first detecting
subjects with bounding boxes, then associating detected bounding boxes across
video frames. For many IoT systems, images captured by cameras are usually sent
over the network to be processed at a different site that has more powerful
computing resources than edge devices. However, sending entire frames through
the network causes significant bandwidth consumption that may exceed the system
bandwidth constraints. To tackle this problem, we propose ViFiT, a
transformer-based model that reconstructs vision bounding box trajectories from
phone data (IMU and Fine Time Measurements). It leverages a transformer ability
of better modeling long-term time series data. ViFiT is evaluated on Vi-Fi
Dataset, a large-scale multimodal dataset in 5 diverse real world scenes,
including indoor and outdoor environments. To fill the gap of proper metrics of
jointly capturing the system characteristics of both tracking quality and video
bandwidth reduction, we propose a novel evaluation framework dubbed Minimum
Required Frames (MRF) and Minimum Required Frames Ratio (MRFR). ViFiT achieves
an MRFR of 0.65 that outperforms the state-of-the-art approach for cross-modal
reconstruction in LSTM Encoder-Decoder architecture X-Translator of 0.98,
resulting in a high frame reduction rate as 97.76%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03149">Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1">Nicholas Konz</a>, <a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1">Charles Godfrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1">Madelyn Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jonathan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1">Henry Kvinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a></p>
<p>By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03182">Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1">An Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yiwu Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zexue He</a>, <a href="http://arxiv.org/find/cs/1/au:+Karypis_P/0/1/0/all/0/1">Petros Karypis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zihan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1">Chengyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gentili_A/0/1/0/all/0/1">Amilcare Gentili</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1">Chun-Nan Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1">Julian McAuley</a></p>
<p>Medical image classification is a critical problem for healthcare, with the
potential to alleviate the workload of doctors and facilitate diagnoses of
patients. However, two challenges arise when deploying deep learning models to
real-world healthcare applications. First, neural models tend to learn spurious
correlations instead of desired features, which could fall short when
generalizing to new domains (e.g., patients with different ages). Second, these
black-box models lack interpretability. When making diagnostic predictions, it
is important to understand why a model makes a decision for trustworthy and
safety considerations. In this paper, to address these two limitations, we
propose a new paradigm to build robust and interpretable medical image
classifiers with natural language concepts. Specifically, we first query
clinical concepts from GPT-4, then transform latent image features into
explicit concepts with a vision-language model. We systematically evaluate our
method on eight medical image classification datasets to verify its
effectiveness. On challenging datasets with strong confounding factors, our
method can mitigate spurious correlations thus substantially outperform
standard visual encoders and other baselines. Finally, we show how
classification with a small number of concepts brings a level of
interpretability for understanding model decisions through case studies in real
medical data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03205">A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization. (arXiv:2310.03205v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1">Kim Youwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hyun_L/0/1/0/all/0/1">Lee Hyun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Bin_K/0/1/0/all/0/1">Kim Sung-Bin</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1">Suekyeong Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1">Janghoon Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1">Tae-Hyun Oh</a></p>
<p>We propose NeuFace, a 3D face mesh pseudo annotation method on videos via
neural re-parameterized optimization. Despite the huge progress in 3D face
reconstruction methods, generating reliable 3D face labels for in-the-wild
dynamic videos remains challenging. Using NeuFace optimization, we annotate the
per-view/-frame accurate and consistent face meshes on large-scale face videos,
called the NeuFace-dataset. We investigate how neural re-parameterization helps
to reconstruct image-aligned facial details on 3D meshes via gradient analysis.
By exploiting the naturalness and diversity of 3D faces in our dataset, we
demonstrate the usefulness of our dataset for 3D face-related tasks: improving
the reconstruction accuracy of an existing 3D face reconstruction model and
learning 3D facial motion prior. Code and datasets will be available at
https://neuface-dataset.github.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03211">On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1">Utsav Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1">Erhan Bas</a></p>
<p>Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03270">EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models. (arXiv:2310.03270v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yefei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Diffusion models have demonstrated remarkable capabilities in image synthesis
and related generative tasks. Nevertheless, their practicality for low-latency
real-world applications is constrained by substantial computational costs and
latency issues. Quantization is a dominant way to compress and accelerate
diffusion models, where post-training quantization (PTQ) and quantization-aware
training (QAT) are two main approaches, each bearing its own properties. While
PTQ exhibits efficiency in terms of both time and data usage, it may lead to
diminished performance in low bit-width. On the other hand, QAT can alleviate
performance degradation but comes with substantial demands on computational and
data resources. To capitalize on the advantages while avoiding their respective
drawbacks, we introduce a data-free and parameter-efficient fine-tuning
framework for low-bit diffusion models, dubbed EfficientDM, to achieve
QAT-level performance with PTQ-like efficiency. Specifically, we propose a
quantization-aware variant of the low-rank adapter (QALoRA) that can be merged
with model weights and jointly quantized to low bit-width. The fine-tuning
process distills the denoising capabilities of the full-precision model into
its quantized counterpart, eliminating the requirement for training data. We
also introduce scale-aware optimization and employ temporal learned step-size
quantization to further enhance performance. Extensive experimental results
demonstrate that our method significantly outperforms previous PTQ-based
diffusion models while maintaining similar time and data efficiency.
Specifically, there is only a marginal 0.05 sFID increase when quantizing both
weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to
QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization
speed with comparable generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03273">Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning. (arXiv:2310.03273v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Komatsu_T/0/1/0/all/0/1">Takayuki Komatsu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohmura_Y/0/1/0/all/0/1">Yoshiyuki Ohmura</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1">Yasuo Kuniyoshi</a></p>
<p>Multi-object representation learning aims to represent complex real-world
visual input using the composition of multiple objects. Representation learning
methods have often used unsupervised learning to segment an input image into
individual objects and encode these objects into each latent vector. However,
it is not clear how previous methods have achieved the appropriate segmentation
of individual objects. Additionally, most of the previous methods regularize
the latent vectors using a Variational Autoencoder (VAE). Therefore, it is not
clear whether VAE regularization contributes to appropriate object
segmentation. To elucidate the mechanism of object segmentation in multi-object
representation learning, we conducted an ablation study on MONet, which is a
typical method. MONet represents multiple objects using pairs that consist of
an attention mask and the latent vector corresponding to the attention mask.
Each latent vector is encoded from the input image and attention mask. Then,
the component image and attention mask are decoded from each latent vector. The
loss function of MONet consists of 1) the sum of reconstruction losses between
the input image and decoded component image, 2) the VAE regularization loss of
the latent vector, and 3) the reconstruction loss of the attention mask to
explicitly encode shape information. We conducted an ablation study on these
three loss functions to investigate the effect on segmentation performance. Our
results showed that the VAE regularization loss did not affect segmentation
performance and the others losses did affect it. Based on this result, we
hypothesize that it is important to maximize the attention mask of the image
region best represented by a single latent vector corresponding to the
attention mask. We confirmed this hypothesis by evaluating a new loss function
with the same mechanism as the hypothesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03279">Classifying Whole Slide Images: What Matters?. (arXiv:2310.03279v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Long Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1">Aiden Nibali</a>, <a href="http://arxiv.org/find/cs/1/au:+Millward_J/0/1/0/all/0/1">Joshua Millward</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhen He</a></p>
<p>Recently there have been many algorithms proposed for the classification of
very high resolution whole slide images (WSIs). These new algorithms are mostly
focused on finding novel ways to combine the information from small local
patches extracted from the slide, with an emphasis on effectively aggregating
more global information for the final predictor. In this paper we thoroughly
explore different key design choices for WSI classification algorithms to
investigate what matters most for achieving high accuracy. Surprisingly, we
found that capturing global context information does not necessarily mean
better performance. A model that captures the most global information
consistently performs worse than a model that captures less global information.
In addition, a very simple multi-instance learning method that captures no
global information performs almost as well as models that capture a lot of
global information. These results suggest that the most important features for
effective WSI classification are captured at the local small patch level, where
cell and tissue micro-environment detail is most pronounced. Another surprising
finding was that unsupervised pre-training on a larger set of 33 cancers gives
significantly worse performance compared to pre-training on a smaller dataset
of 7 cancers (including the target cancer). We posit that pre-training on a
smaller, more focused dataset allows the feature extractor to make better use
of the limited feature space to better discriminate between subtle differences
in the input patch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03288">PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches. (arXiv:2310.03288v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zherui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeow_R/0/1/0/all/0/1">Raye Chen-Hua Yeow</a></p>
<p>Real-time intelligent detection and prediction of subjects' behavior
particularly their movements or actions is critical in the ward. This approach
offers the advantage of reducing in-hospital care costs and improving the
efficiency of healthcare workers, which is especially true for scenarios at
night or during peak admission periods. Therefore, in this work, we propose
using computer vision (CV) and deep learning (DL) methods for detecting
subjects and recognizing their actions. We utilize OpenPose as an accurate
subject detector for recognizing the positions of human subjects in the video
stream. Additionally, we employ AlphAction's Asynchronous Interaction
Aggregation (AIA) network to predict the actions of detected subjects. This
integrated model, referred to as PoseAction, is proposed. At the same time, the
proposed model is further trained to predict 12 common actions in ward areas,
such as staggering, chest pain, and falling down, using medical-related video
clips from the NTU RGB+D and NTU RGB+D 120 datasets. The results demonstrate
that PoseAction achieves the highest classification mAP of 98.72% (IoU@0.5).
Additionally, this study develops an online real-time mode for action
recognition, which strongly supports the clinical translation of PoseAction.
Furthermore, using OpenPose's function for recognizing face key points, we also
implement face blurring, which is a practical solution to address the privacy
protection concerns of patients and healthcare workers. Nevertheless, the
training data for PoseAction is currently limited, particularly in terms of
label diversity. Consequently, the subsequent step involves utilizing a more
diverse dataset (including general actions) to train the model's parameters for
improved generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03291">SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models. (arXiv:2310.03291v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1">Yiren Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tingkai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1">Yunzhe Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1">Soroush Vosoughi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">HX Yang</a></p>
<p>In this paper, we propose ``SimVLG'', a streamlined framework for the
pre-training of computationally intensive vision-language generative models,
leveraging frozen pre-trained large language models (LLMs). The prevailing
paradigm in vision-language pre-training (VLP) typically involves a two-stage
optimization process: an initial resource-intensive phase dedicated to
general-purpose vision-language representation learning, aimed at extracting
and consolidating pertinent visual features, followed by a subsequent phase
focusing on end-to-end alignment between visual and linguistic modalities. Our
one-stage, single-loss framework circumvents the aforementioned computationally
demanding first stage of training by gradually merging similar visual tokens
during training. This gradual merging process effectively compacts the visual
information while preserving the richness of semantic content, leading to fast
convergence without sacrificing performance. Our experiments show that our
approach can speed up the training of vision-language models by a factor
$\times 5$ without noticeable impact on the overall performance. Additionally,
we show that our models can achieve comparable performance to current
vision-language models with only $1/10$ of the data. Finally, we demonstrate
how our image-text models can be easily adapted to video-language generative
tasks through a novel soft attentive temporal token merging modules.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03295">Can pre-trained models assist in dataset distillation?. (arXiv:2310.03295v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuguang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuchen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jianyang Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianle Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaoniu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1">Qi Xuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yang You</a></p>
<p>Dataset Distillation (DD) is a prominent technique that encapsulates
knowledge from a large-scale original dataset into a small synthetic dataset
for efficient training. Meanwhile, Pre-trained Models (PTMs) function as
knowledge repositories, containing extensive information from the original
dataset. This naturally raises a question: Can PTMs effectively transfer
knowledge to synthetic datasets, guiding DD accurately? To this end, we conduct
preliminary experiments, confirming the contribution of PTMs to DD. Afterwards,
we systematically study different options in PTMs, including initialization
parameters, model architecture, training epoch and domain knowledge, revealing
that: 1) Increasing model diversity enhances the performance of synthetic
datasets; 2) Sub-optimal models can also assist in DD and outperform
well-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory
for DD, but a reasonable domain match is crucial. Finally, by selecting optimal
options, we significantly improve the cross-architecture generalization over
baseline DD methods. We hope our work will facilitate researchers to develop
better DD techniques. Our code is available at
https://github.com/yaolu-zjut/DDInterpreter.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03314">Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction. (arXiv:2310.03314v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1">Aadi Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tohme_T/0/1/0/all/0/1">Tony Tohme</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaotong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Youcef_Toumi_K/0/1/0/all/0/1">Kamal Youcef-Toumi</a></p>
<p>Human motion prediction is an essential step for efficient and safe
human-robot collaboration. Current methods either purely rely on representing
the human joints in some form of neural network-based architecture or use
regression models offline to fit hyper-parameters in the hope of capturing a
model encompassing human motion. While these methods provide good initial
results, they are missing out on leveraging well-studied human body kinematic
models as well as body and scene constraints which can help boost the efficacy
of these prediction frameworks while also explicitly avoiding implausible human
joint configurations. We propose a novel human motion prediction framework that
incorporates human joint constraints and scene constraints in a Gaussian
Process Regression (GPR) model to predict human motion over a set time horizon.
This formulation is combined with an online context-aware constraints model to
leverage task-dependent motions. It is tested on a human arm kinematic model
and implemented on a human-robot collaborative setup with a UR5 robot arm to
demonstrate the real-time capability of our approach. Simulations were also
performed on datasets like HA4M and ANDY. The simulation and experimental
results demonstrate considerable improvements in a Gaussian Process framework
when these constraints are explicitly considered.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03324">Investigating the Limitation of CLIP Models: The Worst-Performing Categories. (arXiv:2310.03324v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1">Jie-Jing Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jiang-Xin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao-Wen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Lan-Zhe Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu-Feng Li</a></p>
<p>Contrastive Language-Image Pre-training (CLIP) provides a foundation model by
integrating natural language into visual concepts, enabling zero-shot
recognition on downstream tasks. It is usually expected that satisfactory
overall accuracy can be achieved across numerous domains through well-designed
textual prompts. However, we found that their performance in the worst
categories is significantly inferior to the overall performance. For example,
on ImageNet, there are a total of 10 categories with class-wise accuracy as low
as 0\%, even though the overall performance has achieved 64.1\%. This
phenomenon reveals the potential risks associated with using CLIP models,
particularly in risk-sensitive applications where specific categories hold
significant importance. To address this issue, we investigate the alignment
between the two modalities in the CLIP model and propose the Class-wise
Matching Margin (\cmm) to measure the inference confusion. \cmm\ can
effectively identify the worst-performing categories and estimate the potential
performance of the candidate prompts. We further query large language models to
enrich descriptions of worst-performing categories and build a weighted
ensemble to highlight the efficient prompts. Experimental results clearly
verify the effectiveness of our proposal, where the accuracy on the worst-10
categories on ImageNet is boosted to 5.2\%, without manual prompt engineering,
laborious optimization, or access to labeled validation data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03325">Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yilue Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Peiyu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lifeng Fan</a></p>
<p>Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories and unseen object categories.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03333">Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring. (arXiv:2310.03333v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1">Jia Syuen Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiajun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Khamis_A/0/1/0/all/0/1">Abdelwahed Khamis</a>, <a href="http://arxiv.org/find/cs/1/au:+Arablouei_R/0/1/0/all/0/1">Reza Arablouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Barlow_R/0/1/0/all/0/1">Robert Barlow</a>, <a href="http://arxiv.org/find/cs/1/au:+McAllister_R/0/1/0/all/0/1">Ryan McAllister</a></p>
<p>Regulatory compliance auditing across diverse industrial domains requires
heightened quality assurance and traceability. Present manual and intermittent
approaches to such auditing yield significant challenges, potentially leading
to oversights in the monitoring process. To address these issues, we introduce
a real-time, multi-modal sensing system employing 3D time-of-flight and RGB
cameras, coupled with unsupervised learning techniques on edge AI devices. This
enables continuous object tracking thereby enhancing efficiency in
record-keeping and minimizing manual interventions. While we validate the
system in a knife sanitization context within agrifood facilities, emphasizing
its prowess against occlusion and low-light issues with RGB cameras, its
potential spans various industrial monitoring settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03335">Continual Test-time Domain Adaptation via Dynamic Sample Selection. (arXiv:2310.03335v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanshuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Jie Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1">Ali Cheraghian</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1">Shafin Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1">David Ahmedt-Aristizabal</a>, <a href="http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1">Lars Petersson</a>, <a href="http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1">Mehrtash Harandi</a></p>
<p>The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually
adapt a pre-trained model to a sequence of target domains without accessing the
source data. This paper proposes a Dynamic Sample Selection (DSS) method for
CTDA. DSS consists of dynamic thresholding, positive learning, and negative
learning processes. Traditionally, models learn from unlabeled unknown
environment data and equally rely on all samples' pseudo-labels to update their
parameters through self-training. However, noisy predictions exist in these
pseudo-labels, so all samples are not equally trustworthy. Therefore, in our
method, a dynamic thresholding module is first designed to select suspected
low-quality from high-quality samples. The selected low-quality samples are
more likely to be wrongly predicted. Therefore, we apply joint positive and
negative learning on both high- and low-quality samples to reduce the risk of
using wrong information. We conduct extensive experiments that demonstrate the
effectiveness of our proposed method for CTDA in the image domain,
outperforming the state-of-the-art results. Furthermore, our approach is also
evaluated in the 3D point cloud domain, showcasing its versatility and
potential for broader applicability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03337">Denoising Diffusion Step-aware Models. (arXiv:2310.03337v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuai Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yukang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Luozhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingcong Chen</a></p>
<p>Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for
data generation across various domains. However, a significant bottleneck is
the necessity for whole-network computation during every step of the generative
process, leading to high computational overheads. This paper presents a novel
framework, Denoising Diffusion Step-aware Models (DDSM), to address this
challenge. Unlike conventional approaches, DDSM employs a spectrum of neural
networks whose sizes are adapted according to the importance of each generative
step, as determined through evolutionary search. This step-wise network
variation effectively circumvents redundant computational efforts, particularly
in less critical steps, thereby enhancing the efficiency of the diffusion
model. Furthermore, the step-aware design can be seamlessly integrated with
other efficiency-geared diffusion models such as DDIMs and latent diffusion,
thus broadening the scope of computational savings. Empirical evaluations
demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%
for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all
without compromising the generation quality. Our code and models will be
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03346">Combining Datasets with Different Label Sets for Improved Nucleus Segmentation and Classification. (arXiv:2310.03346v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1">Amruta Parulekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanwat_U/0/1/0/all/0/1">Utkarsh Kanwat</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Ravi Kant Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Chippa_M/0/1/0/all/0/1">Medha Chippa</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacob_T/0/1/0/all/0/1">Thomas Jacob</a>, <a href="http://arxiv.org/find/cs/1/au:+Bameta_T/0/1/0/all/0/1">Tripti Bameta</a>, <a href="http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1">Swapnil Rane</a>, <a href="http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1">Amit Sethi</a></p>
<p>Segmentation and classification of cell nuclei in histopathology images using
deep neural networks (DNNs) can save pathologists' time for diagnosing various
diseases, including cancers, by automating cell counting and morphometric
assessments. It is now well-known that the accuracy of DNNs increases with the
sizes of annotated datasets available for training. Although multiple datasets
of histopathology images with nuclear annotations and class labels have been
made publicly available, the set of class labels differ across these datasets.
We propose a method to train DNNs for instance segmentation and classification
on multiple datasets where the set of classes across the datasets are related
but not the same. Specifically, our method is designed to utilize a
coarse-to-fine class hierarchy, where the set of classes labeled and annotated
in a dataset can be at any level of the hierarchy, as long as the classes are
mutually exclusive. Within a dataset, the set of classes need not even be at
the same level of the class hierarchy tree. Our results demonstrate that
segmentation and classification metrics for the class set used by the test
split of a dataset can improve by pre-training on another dataset that may even
have a different set of classes due to the expansion of the training set
enabled by our method. Furthermore, generalization to previously unseen
datasets also improves by combining multiple other datasets with different sets
of classes for training. The improvement is both qualitative and quantitative.
The proposed method can be adapted for various loss functions, DNN
architectures, and application domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03358">Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1">Nuoyan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Decheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dawei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a></p>
<p>Deep neural networks are vulnerable to adversarial noise. Adversarial
training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two characteristics of robust
representation: (1) $\bf{exclusion}$: the feature of natural examples keeps
away from that of other classes; (2) $\bf{alignment}$: the feature of natural
and corresponding adversarial examples is close to each other. These motivate
us to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance. Code is
available at &lt;https://github.com/changzhang777/ANCRA&gt;.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03360">CSI: Enhancing the Robustness of 3D Point Cloud Recognition against Corruption. (arXiv:2310.03360v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhuoyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiachen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a></p>
<p>Despite recent advancements in deep neural networks for point cloud
recognition, real-world safety-critical applications present challenges due to
unavoidable data corruption. Current models often fall short in generalizing to
unforeseen distribution shifts. In this study, we harness the inherent set
property of point cloud data to introduce a novel critical subset
identification (CSI) method, aiming to bolster recognition robustness in the
face of data corruption. Our CSI framework integrates two pivotal components:
density-aware sampling (DAS) and self-entropy minimization (SEM), which cater
to static and dynamic CSI, respectively. DAS ensures efficient robust anchor
point sampling by factoring in local density, while SEM is employed during
training to accentuate the most salient point-to-point attention. Evaluations
reveal that our CSI approach yields error rates of 18.4\% and 16.3\% on
ModelNet40-C and PointCloud-C, respectively, marking a notable improvement over
state-of-the-art methods by margins of 5.2\% and 4.2\% on the respective
benchmarks. Code is available at
\href{https://github.com/masterwu2115/CSI/tree/main}{https://github.com/masterwu2115/CSI/tree/main}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03363">Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior. (arXiv:2310.03363v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinting Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Li Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hei Victor Cheng</a></p>
<p>Speech-to-face generation is an intriguing area of research that focuses on
generating realistic facial images based on a speaker's audio speech. However,
state-of-the-art methods employing GAN-based architectures lack stability and
cannot generate realistic face images. To fill this gap, we propose a novel
speech-to-face generation framework, which leverages a Speech-Conditioned
Latent Diffusion Model, called SCLDM. To the best of our knowledge, this is the
first work to harness the exceptional modeling capabilities of diffusion models
for speech-to-face generation. Preserving the shared identity information
between speech and face is crucial in generating realistic results. Therefore,
we employ contrastive pre-training for both the speech encoder and the face
encoder. This pre-training strategy facilitates effective alignment between the
attributes of speech, such as age and gender, and the corresponding facial
characteristics in the face images. Furthermore, we tackle the challenge posed
by excessive diversity in the synthesis process caused by the diffusion model.
To overcome this challenge, we introduce the concept of residuals by
integrating a statistical face prior to the diffusion process. This addition
helps to eliminate the shared component across the faces and enhances the
subtle variations captured by the speech condition. Extensive quantitative,
qualitative, and user study experiments demonstrate that our method can produce
more realistic face images while preserving the identity of the speaker better
than state-of-the-art methods. Highlighting the notable enhancements, our
method demonstrates significant gains in all metrics on the AVSpeech dataset
and Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and
32.72 on the cosine distance metric for the two datasets, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03365">Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jafari_H/0/1/0/all/0/1">Hossein Jafari</a>, <a href="http://arxiv.org/find/eess/1/au:+Faez_K/0/1/0/all/0/1">Karim Faez</a>, <a href="http://arxiv.org/find/eess/1/au:+Amindavar_H/0/1/0/all/0/1">Hamidreza Amindavar</a></p>
<p>Lung cancer is highly lethal, emphasizing the critical need for early
detection. However, identifying lung nodules poses significant challenges for
radiologists, who rely heavily on their expertise and experience for accurate
diagnosis. To address this issue, computer-aided diagnosis systems based on
machine learning techniques have emerged to assist doctors in identifying lung
nodules from computed tomography (CT) scans. Unfortunately, existing networks
in this domain often suffer from computational complexity, leading to high
rates of false negatives and false positives, limiting their effectiveness. To
address these challenges, we present an innovative model that harnesses the
strengths of both convolutional neural networks and vision transformers.
Inspired by object detection in videos, we treat each 3D CT image as a video,
individual slices as frames, and lung nodules as objects, enabling a
time-series application. The primary objective of our work is to overcome
hardware limitations during model training, allowing for efficient processing
of 2D data while utilizing inter-slice information for accurate identification
based on 3D image context. We validated the proposed network by applying a
10-fold cross-validation technique to the publicly available Lung Nodule
Analysis 2016 dataset. Our proposed architecture achieves an average
sensitivity criterion of 97.84% and a competition performance metrics (CPM) of
96.0% with few parameters. Comparative analysis with state-of-the-art
advancements in lung nodule identification demonstrates the significant
accuracy achieved by our proposed model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03375">Point-Based Radiance Fields for Controllable Human Motion Synthesis. (arXiv:2310.03375v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Haitao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Deheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Peiyuan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyi Zhang</a></p>
<p>This paper proposes a novel controllable human motion synthesis method for
fine-level deformation based on static point-based radiance fields. Although
previous editable neural radiance field methods can generate impressive results
on novel-view synthesis and allow naive deformation, few algorithms can achieve
complex 3D human editing such as forward kinematics. Our method exploits the
explicit point cloud to train the static 3D scene and apply the deformation by
encoding the point cloud translation using a deformation MLP. To make sure the
rendering result is consistent with the canonical space training, we estimate
the local rotation using SVD and interpolate the per-point rotation to the
query view direction of the pre-trained radiance field. Extensive experiments
show that our approach can significantly outperform the state-of-the-art on
fine-level complex deformation which can be generalized to other 3D characters
besides humans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03377">ACT-Net: Anchor-context Action Detection in Surgery Videos. (arXiv:2310.03377v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1">Luoying Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Wenjun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Heng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jinming Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a></p>
<p>Recognition and localization of surgical detailed actions is an essential
component of developing a context-aware decision support system. However, most
existing detection algorithms fail to provide high-accuracy action classes even
having their locations, as they do not consider the surgery procedure's
regularity in the whole video. This limitation hinders their application.
Moreover, implementing the predictions in clinical applications seriously needs
to convey model confidence to earn entrustment, which is unexplored in surgical
action prediction. In this paper, to accurately detect fine-grained actions
that happen at every moment, we propose an anchor-context action detection
network (ACTNet), including an anchor-context detection (ACD) module and a
class conditional diffusion (CCD) module, to answer the following questions: 1)
where the actions happen; 2) what actions are; 3) how confidence predictions
are. Specifically, the proposed ACD module spatially and temporally highlights
the regions interacting with the extracted anchor in surgery video, which
outputs action location and its class distribution based on anchor-context
interactions. Considering the full distribution of action classes in videos,
the CCD module adopts a denoising diffusion-based generative model conditioned
on our ACD estimator to further reconstruct accurately the action predictions.
Moreover, we utilize the stochastic nature of the diffusion model outputs to
access model confidence for each prediction. Our method reports the
state-of-the-art performance, with improvements of 4.0% mAP against baseline on
the surgical video dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03388">OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rabino_P/0/1/0/all/0/1">Paolo Rabino</a>, <a href="http://arxiv.org/find/cs/1/au:+Alliegro_A/0/1/0/all/0/1">Antonio Alliegro</a>, <a href="http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1">Francesco Cappio Borlino</a>, <a href="http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1">Tatiana Tommasi</a></p>
<p>Moving deep learning models from the laboratory setting to the open world
entails preparing them to handle unforeseen conditions. In several applications
the occurrence of novel classes during deployment poses a significant threat,
thus it is crucial to effectively detect them. Ideally, this skill should be
used when needed without requiring any further computational training effort at
every new task. Out-of-distribution detection has attracted significant
attention in the last years, however the majority of the studies deal with 2D
images ignoring the inherent 3D nature of the real-world and often confusing
between domain and semantic novelty. In this work, we focus on the latter,
considering the objects geometric structure captured by 3D point clouds
regardless of the specific domain. We advance the field by introducing
OpenPatch that builds on a large pre-trained model and simply extracts from its
intermediate features a set of patch representations that describe each known
class. For any new sample, we obtain a novelty score by evaluating whether it
can be recomposed mainly by patches of a single known class or rather via the
contribution of multiple classes. We present an extensive experimental
evaluation of our approach for the task of semantic novelty detection on
real-world point cloud samples when the reference known data are synthetic. We
demonstrate that OpenPatch excels in both the full and few-shot known sample
scenarios, showcasing its robustness across varying pre-training objectives and
network backbones. The inherent training-free nature of our method allows for
its immediate application to a wide array of real-world tasks, offering a
compelling advantage over approaches that need expensive retraining efforts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03396">Learning to Simplify Spatial-Temporal Graphs in Gait Analysis. (arXiv:2310.03396v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1">Adrian Cosma</a>, <a href="http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1">Emilian Radoi</a></p>
<p>Gait analysis leverages unique walking patterns for person identification and
assessment across multiple domains. Among the methods used for gait analysis,
skeleton-based approaches have shown promise due to their robust and
interpretable features. However, these methods often rely on hand-crafted
spatial-temporal graphs that are based on human anatomy disregarding the
particularities of the dataset and task. This paper proposes a novel method to
simplify the spatial-temporal graph representation for gait-based gender
estimation, improving interpretability without losing performance. Our approach
employs two models, an upstream and a downstream model, that can adjust the
adjacency matrix for each walking instance, thereby removing the fixed nature
of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model
is trainable end-to-end. We demonstrate the effectiveness of our approach on
the CASIA-B dataset for gait-based gender estimation. The resulting graphs are
interpretable and differ qualitatively from fixed graphs used in existing
models. Our research contributes to enhancing the explainability and
task-specific adaptability of gait recognition, promoting more efficient and
reliable gait-based biometrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03402">A Complementary Global and Local Knowledge Network for Ultrasound denoising with Fine-grained Refinement. (arXiv:2310.03402v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1">Zhenyu Bu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai-Ni Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1">Fuxing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1">Guang-Quan Zhou</a></p>
<p>Ultrasound imaging serves as an effective and non-invasive diagnostic tool
commonly employed in clinical examinations. However, the presence of speckle
noise in ultrasound images invariably degrades image quality, impeding the
performance of subsequent tasks, such as segmentation and classification.
Existing methods for speckle noise reduction frequently induce excessive image
smoothing or fail to preserve detailed information adequately. In this paper,
we propose a complementary global and local knowledge network for ultrasound
denoising with fine-grained refinement. Initially, the proposed architecture
employs the L-CSwinTransformer as encoder to capture global information,
incorporating CNN as decoder to fuse local features. We expand the resolution
of the feature at different stages to extract more global information compared
to the original CSwinTransformer. Subsequently, we integrate Fine-grained
Refinement Block (FRB) within the skip-connection stage to further augment
features. We validate our model on two public datasets, HC18 and BUSI.
Experimental results demonstrate that our model can achieve competitive
performance in both quantitative metrics and visual performance. Our code will
be available at https://github.com/AAlkaid/USDenoising.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03420">FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators. (arXiv:2310.03420v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haiping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yujing Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bisheng Yang</a></p>
<p>Matching cross-modality features between images and point clouds is a
fundamental problem for image-to-point cloud registration. However, due to the
modality difference between images and points, it is difficult to learn robust
and discriminative cross-modality features by existing metric learning methods
for feature matching. Instead of applying metric learning on cross-modality
data, we propose to unify the modality between images and point clouds by
pretrained large-scale models first, and then establish robust correspondence
within the same modality. We show that the intermediate features, called
diffusion features, extracted by depth-to-image diffusion models are
semantically consistent between images and point clouds, which enables the
building of coarse but robust cross-modality correspondences. We further
extract geometric features on depth maps produced by the monocular depth
estimator. By matching such geometric features, we significantly improve the
accuracy of the coarse correspondences produced by diffusion features.
Extensive experiments demonstrate that without any task-specific training,
direct utilization of both features produces accurate image-to-point cloud
registration. On three public indoor and outdoor benchmarks, the proposed
method averagely achieves a 20.6 percent improvement in Inlier Ratio, a
three-fold higher Inlier Number, and a 48.6 percent improvement in Registration
Recall than existing state-of-the-arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03431">Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on Double Covering. (arXiv:2310.03431v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1">Fei Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuhui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wencheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1">Hong Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Ying He</a></p>
<p>In this paper, we propose a new method, called DoubleCoverUDF, for extracting
the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a
learned UDF and a user-specified parameter $r$ (a small positive real number)
as input and extracts an iso-surface with an iso-value $r$ using the
conventional marching cubes algorithm. We show that the computed iso-surface is
the boundary of the $r$-offset volume of the target zero level-set $S$, which
is an orientable manifold, regardless of the topology of $S$. Next, the
algorithm computes a covering map to project the boundary mesh onto $S$,
preserving the mesh's topology and avoiding folding. If $S$ is an orientable
manifold surface, our algorithm separates the double-layered mesh into a single
layer using a robust minimum-cut post-processing step. Otherwise, it keeps the
double-layered mesh as the output. We validate our algorithm by reconstructing
3D surfaces of open models and demonstrate its efficacy and effectiveness on
synthetic models and benchmark datasets. Our experimental results confirm that
our method is robust and produces meshes with better quality in terms of both
visual evaluation and quantitative measures than existing UDF-based methods.
The source code is available at https://github.com/jjjkkyz/DCUDF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03432">Mitigating the Influence of Domain Shift in Skin Lesion Classification: A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic Images. (arXiv:2310.03432v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chamarthi_S/0/1/0/all/0/1">Sireesha Chamarthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fogelberg_K/0/1/0/all/0/1">Katharina Fogelberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Maron_R/0/1/0/all/0/1">Roman C. Maron</a>, <a href="http://arxiv.org/find/cs/1/au:+Brinker_T/0/1/0/all/0/1">Titus J. Brinker</a>, <a href="http://arxiv.org/find/cs/1/au:+Niebling_J/0/1/0/all/0/1">Julia Niebling</a></p>
<p>The potential of deep neural networks in skin lesion classification has
already been demonstrated to be on-par if not superior to the dermatologists
diagnosis. However, the performance of these models usually deteriorates when
the test data differs significantly from the training data (i.e. domain shift).
This concerning limitation for models intended to be used in real-world skin
lesion classification tasks poses a risk to patients. For example, different
image acquisition systems or previously unseen anatomical sites on the patient
can suffice to cause such domain shifts. Mitigating the negative effect of such
shifts is therefore crucial, but developing effective methods to address domain
shift has proven to be challenging. In this study, we carry out an in-depth
analysis of eight different unsupervised domain adaptation methods to analyze
their effectiveness in improving generalization for dermoscopic datasets. To
ensure robustness of our findings, we test each method on a total of ten
distinct datasets, thereby covering a variety of possible domain shifts. In
addition, we investigated which factors in the domain shifted datasets have an
impact on the effectiveness of domain adaptation methods. Our findings show
that all of the eight domain adaptation methods result in improved AUPRC for
the majority of analyzed datasets. Altogether, these results indicate that
unsupervised domain adaptations generally lead to performance improvements for
the binary melanoma-nevus classification task regardless of the nature of the
domain shift. However, small or heavily imbalanced datasets lead to a reduced
conformity of the results due to the influence of these factors on the methods
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03456">Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization. (arXiv:2310.03456v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fish_E/0/1/0/all/0/1">Edward Fish</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinbren_J/0/1/0/all/0/1">Jon Weinbren</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1">Andrew Gilbert</a></p>
<p>Temporal Action Localization (TAL) aims to identify actions' start, end, and
class labels in untrimmed videos. While recent advancements using transformer
networks and Feature Pyramid Networks (FPN) have enhanced visual feature
recognition in TAL tasks, less progress has been made in the integration of
audio features into such frameworks. This paper introduces the Multi-Resolution
Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge
audio-visual data across different temporal resolutions. Central to our
approach is a hierarchical gated cross-attention mechanism, which discerningly
weighs the importance of audio information at diverse temporal scales. Such a
technique not only refines the precision of regression boundaries but also
bolsters classification confidence. Importantly, MRAV-FF is versatile, making
it compatible with existing FPN TAL architectures and offering a significant
enhancement in performance when audio data is available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03472">Ammonia-Net: A Multi-task Joint Learning Model for Multi-class Segmentation and Classification in Tooth-marked Tongue Diagnosis. (arXiv:2310.03472v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shunkai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qihui Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yiming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1">Muhammad Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Melliou_A/0/1/0/all/0/1">Aikaterini Melliou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dongmei Yu</a></p>
<p>In Traditional Chinese Medicine, the tooth marks on the tongue, stemming from
prolonged dental pressure, serve as a crucial indicator for assessing qi (yang)
deficiency, which is intrinsically linked to visceral health. Manual diagnosis
of tooth-marked tongue solely relies on experience. Nonetheless, the diversity
in shape, color, and type of tooth marks poses a challenge to diagnostic
accuracy and consistency. To address these problems, herein we propose a
multi-task joint learning model named Ammonia-Net. This model employs a
convolutional neural network-based architecture, specifically designed for
multi-class segmentation and classification of tongue images. Ammonia-Net
performs semantic segmentation of tongue images to identify tongue and tooth
marks. With the assistance of segmentation output, it classifies the images
into the desired number of classes: healthy tongue, light tongue, moderate
tongue, and severe tongue. As far as we know, this is the first attempt to
apply the semantic segmentation results of tooth marks for tooth-marked tongue
classification. To train Ammonia-Net, we collect 856 tongue images from 856
subjects. After a number of extensive experiments, the experimental results
show that the proposed model achieves 99.06% accuracy in the two-class
classification task of tooth-marked tongue identification and 80.02%. As for
the segmentation task, mIoU for tongue and tooth marks amounts to 71.65%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03485">BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic Classification. (arXiv:2310.03485v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kollias_D/0/1/0/all/0/1">Dimitrios Kollias</a>, <a href="http://arxiv.org/find/eess/1/au:+Vendal_K/0/1/0/all/0/1">Karanjot Vendal</a>, <a href="http://arxiv.org/find/eess/1/au:+Gadhavi_P/0/1/0/all/0/1">Priyanka Gadhavi</a>, <a href="http://arxiv.org/find/eess/1/au:+Russom_S/0/1/0/all/0/1">Solomon Russom</a></p>
<p>Brain tumors pose significant health challenges worldwide, with glioblastoma
being one of the most aggressive forms. Accurate determination of the
O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is
crucial for personalized treatment strategies. However, traditional methods are
labor-intensive and time-consuming. This paper proposes a novel multi-modal
approach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w,
T1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet
addresses two main challenges: the variable volume lengths (i.e., each volume
consists of a different number of slices) and the volume-level annotations
(i.e., the whole 3D volume is annotated and not the independent slices that it
consists of). BTDNet consists of four components: i) the data augmentation one
(that performs geometric transformations, convex combinations of data pairs and
test-time data augmentation); ii) the 3D analysis one (that performs global
analysis through a CNN-RNN); iii) the routing one (that contains a mask layer
that handles variable input feature lengths), and iv) the modality fusion one
(that effectively enhances data representation, reduces ambiguities and
mitigates data scarcity). The proposed method outperforms by large margins the
state-of-the-art methods in the RSNA-ASNR-MICCAI BraTS 2021 Challenge, offering
a promising avenue for enhancing brain tumor diagnosis and treatment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03499">IceCloudNet: Cirrus and mixed-phase cloud prediction from SEVIRI input learned from sparse supervision. (arXiv:2310.03499v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Jeggle_K/0/1/0/all/0/1">Kai Jeggle</a>, <a href="http://arxiv.org/find/physics/1/au:+Czerkawski_M/0/1/0/all/0/1">Mikolaj Czerkawski</a>, <a href="http://arxiv.org/find/physics/1/au:+Serva_F/0/1/0/all/0/1">Federico Serva</a>, <a href="http://arxiv.org/find/physics/1/au:+Saux_B/0/1/0/all/0/1">Bertrand Le Saux</a>, <a href="http://arxiv.org/find/physics/1/au:+Neubauer_D/0/1/0/all/0/1">David Neubauer</a>, <a href="http://arxiv.org/find/physics/1/au:+Lohmann_U/0/1/0/all/0/1">Ulrike Lohmann</a></p>
<p>Clouds containing ice particles play a crucial role in the climate system.
Yet they remain a source of great uncertainty in climate models and future
climate projections. In this work, we create a new observational constraint of
regime-dependent ice microphysical properties at the spatio-temporal coverage
of geostationary satellite instruments and the quality of active satellite
retrievals. We achieve this by training a convolutional neural network on three
years of SEVIRI and DARDAR data sets. This work will enable novel research to
improve ice cloud process understanding and hence, reduce uncertainties in a
changing climate and help assess geoengineering methods for cirrus clouds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03502">Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion. (arXiv:2310.03502v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1">Anton Razzhigaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakhmatov_A/0/1/0/all/0/1">Arseniy Shakhmatov</a>, <a href="http://arxiv.org/find/cs/1/au:+Maltseva_A/0/1/0/all/0/1">Anastasia Maltseva</a>, <a href="http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1">Vladimir Arkhipkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavlov_I/0/1/0/all/0/1">Igor Pavlov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryabov_I/0/1/0/all/0/1">Ilya Ryabov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuts_A/0/1/0/all/0/1">Angelina Kuts</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1">Alexander Panchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1">Andrey Kuznetsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1">Denis Dimitrov</a></p>
<p>Text-to-image generation is a significant domain in modern computer vision
and has achieved substantial improvements through the evolution of generative
architectures. Among these, there are diffusion-based models that have
demonstrated essential quality enhancements. These models are generally split
into two categories: pixel-level and latent-level approaches. We present
Kandinsky1, a novel exploration of latent diffusion architecture, combining the
principles of the image prior models with latent diffusion techniques. The
image prior model is trained separately to map text embeddings to image
embeddings of CLIP. Another distinct feature of the proposed model is the
modified MoVQ implementation, which serves as the image autoencoder component.
Overall, the designed model contains 3.3B parameters. We also deployed a
user-friendly demo system that supports diverse generative modes such as
text-to-image generation, image fusion, text and image fusion, image variations
generation, and text-guided inpainting/outpainting. Additionally, we released
the source code and checkpoints for the Kandinsky models. Experimental
evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking
our model as the top open-source performer in terms of measurable image
generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03507">RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time Path Tracing. (arXiv:2310.03507v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scardigli_A/0/1/0/all/0/1">Antoine Scardigli</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavigelli_L/0/1/0/all/0/1">Lukas Cavigelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1">Lorenz K. M&#xfc;ller</a></p>
<p>Monte-Carlo path tracing is a powerful technique for realistic image
synthesis but suffers from high levels of noise at low sample counts, limiting
its use in real-time applications. To address this, we propose a framework with
end-to-end training of a sampling importance network, a latent space encoder
network, and a denoiser network. Our approach uses reinforcement learning to
optimize the sampling importance network, thus avoiding explicit numerically
approximated gradients. Our method does not aggregate the sampled values per
pixel by averaging but keeps all sampled values which are then fed into the
latent space encoder. The encoder replaces handcrafted spatiotemporal
heuristics by learned representations in a latent space. Finally, a neural
denoiser is trained to refine the output image. Our approach increases visual
quality on several challenging datasets and reduces rendering times for equal
quality by a factor of 1.6x compared to the previous state-of-the-art, making
it a promising solution for real-time applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03513">Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery. (arXiv:2310.03513v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gallego_Mejia_J/0/1/0/all/0/1">Joseph A. Gallego-Mejia</a>, <a href="http://arxiv.org/find/cs/1/au:+Jungbluth_A/0/1/0/all/0/1">Anna Jungbluth</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_Ferrer_L/0/1/0/all/0/1">Laura Mart&#xed;nez-Ferrer</a>, <a href="http://arxiv.org/find/cs/1/au:+Allen_M/0/1/0/all/0/1">Matt Allen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dorr_F/0/1/0/all/0/1">Francisco Dorr</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalaitzis_F/0/1/0/all/0/1">Freddie Kalaitzis</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_Pollan_R/0/1/0/all/0/1">Ra&#xfa;l Ramos-Poll&#xe1;n</a></p>
<p>Self-supervised learning (SSL) models have recently demonstrated remarkable
performance across various tasks, including image segmentation. This study
delves into the emergent characteristics of the Self-Distillation with No
Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)
imagery. We pre-train a vision transformer (ViT)-based DINO model using
unlabeled SAR data, and later fine-tune the model to predict high-resolution
land cover maps. We rigorously evaluate the utility of attention maps generated
by the ViT backbone, and compare them with the model's token embedding space.
We observe a small improvement in model performance with pre-training compared
to training from scratch, and discuss the limitations and opportunities of SSL
for remote sensing and land cover segmentation. Beyond small performance
increases, we show that ViT attention maps hold great intrinsic value for
remote sensing, and could provide useful inputs to other algorithms. With this,
our work lays the ground-work for bigger and better SSL models for Earth
Observation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03517">PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification. (arXiv:2310.03517v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1">Feihong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1">Lingyu Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1">Leilei Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1">Fanzhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a></p>
<p>Few-shot image classification has received considerable attention for
addressing the challenge of poor classification performance with limited
samples in novel classes. However, numerous studies have employed sophisticated
learning strategies and diversified feature extraction methods to address this
issue. In this paper, we propose our method called PrototypeFormer, which aims
to significantly advance traditional few-shot image classification approaches
by exploring prototype relationships. Specifically, we utilize a transformer
architecture to build a prototype extraction module, aiming to extract class
representations that are more discriminative for few-shot classification.
Additionally, during the model training process, we propose a contrastive
learning-based optimization approach to optimize prototype features in few-shot
learning scenarios. Despite its simplicity, the method performs remarkably
well, with no bells and whistles. We have experimented with our approach on
several popular few-shot image classification benchmark datasets, which shows
that our method outperforms all current state-of-the-art methods. In
particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way
1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with
accuracy of 7.27% and 8.72%, respectively. The code will be released later.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03525">V2X Cooperative Perception for Autonomous Driving: Recent Advances and Challenges. (arXiv:2310.03525v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dinh C. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1">Mostafa Rahimi Azghadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yuxuan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1">Qing-Long Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Sumei Sun</a></p>
<p>Accurate perception is essential for advancing autonomous driving and
addressing safety challenges in modern transportation systems. Despite
significant advancements in computer vision for object recognition, current
perception methods still face difficulties in complex real-world traffic
environments. Challenges such as physical occlusion and limited sensor field of
view persist for individual vehicle systems. Cooperative Perception (CP) with
Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome
these obstacles and enhance driving automation systems. While some research has
explored CP's fundamental architecture and critical components, there remains a
lack of comprehensive summaries of the latest innovations, particularly in the
context of V2X communication technologies. To address this gap, this paper
provides a comprehensive overview of the evolution of CP technologies, spanning
from early explorations to recent developments, including advancements in V2X
communication technologies. Additionally, a contemporary generic framework is
proposed to illustrate the V2X-based CP workflow, aiding in the structured
understanding of CP system components. Furthermore, this paper categorizes
prevailing V2X-based CP methodologies based on the critical issues they
address. An extensive literature review is conducted within this taxonomy,
evaluating existing datasets and simulators. Finally, open challenges and
future directions in CP for autonomous driving are discussed by considering
both perception and V2X communication advancements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03534">3D-Aware Hypothesis &amp; Verification for Generalizable Relative Object Pose Estimation. (arXiv:2310.03534v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a></p>
<p>Prior methods that tackle the problem of generalizable object pose estimation
highly rely on having dense views of the unseen object. By contrast, we address
the scenario where only a single reference view of the object is available. Our
goal then is to estimate the relative object pose between this reference view
and a query image that depicts the object in a different pose. In this
scenario, robust generalization is imperative due to the presence of unseen
objects during testing and the large-scale object pose variation between the
reference and the query. To this end, we present a new
hypothesis-and-verification framework, in which we generate and evaluate
multiple pose hypotheses, ultimately selecting the most reliable one as the
relative object pose. To measure reliability, we introduce a 3D-aware
verification that explicitly applies 3D transformations to the 3D object
representations learned from the two input images. Our comprehensive
experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior
accuracy of our approach in relative pose estimation and its robustness in
large-scale pose variations, when dealing with unseen objects.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03535">Towards Unified Deep Image Deraining: A Survey and A New Benchmark. (arXiv:2310.03535v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jinshan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jiangxin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jinhui Tang</a></p>
<p>Recent years have witnessed significant advances in image deraining due to
the kinds of effective image priors and deep learning models. As each deraining
approach has individual settings (e.g., training and test datasets, evaluation
criteria), how to fairly evaluate existing approaches comprehensively is not a
trivial task. Although existing surveys aim to review of image deraining
approaches comprehensively, few of them focus on providing unify evaluation
settings to examine the deraining capability and practicality evaluation. In
this paper, we provide a comprehensive review of existing image deraining
method and provide a unify evaluation setting to evaluate the performance of
image deraining methods. We construct a new high-quality benchmark named
HQ-RAIN to further conduct extensive evaluation, consisting of 5,000 paired
high-resolution synthetic images with higher harmony and realism. We also
discuss the existing challenges and highlight several future research
opportunities worth exploring. To facilitate the reproduction and tracking of
the latest deraining technologies for general users, we build an online
platform to provide the off-the-shelf toolkit, involving the large-scale
performance evaluation. This online platform and the proposed new benchmark are
publicly available and will be regularly updated at <a href="http://www.deraining.tech/.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03559">MedSynV1: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images. (arXiv:2310.03559v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1">Li Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Visweswaran_S/0/1/0/all/0/1">Shyam Visweswaran</a>, <a href="http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1">Kayhan Batmanghelich</a></p>
<p>This paper introduces an innovative methodology for producing high-quality 3D
lung CT images guided by textual information. While diffusion-based generative
models are increasingly used in medical imaging, current state-of-the-art
approaches are limited to low-resolution outputs and underutilize radiology
reports' abundant information. The radiology reports can enhance the generation
process by providing additional guidance and offering fine-grained control over
the synthesis of images. Nevertheless, expanding text-guided generation to
high-resolution 3D images poses significant memory and anatomical
detail-preserving challenges. Addressing the memory issue, we introduce a
hierarchical scheme that uses a modified UNet architecture. We start by
synthesizing low-resolution images conditioned on the text, serving as a
foundation for subsequent generators for complete volumetric data. To ensure
the anatomical plausibility of the generated samples, we provide further
guidance by generating vascular, airway, and lobular segmentation masks in
conjunction with the CT images. The model demonstrates the capability to use
textual input and segmentation tasks to generate synthesized images. The
results of comparative assessments indicate that our approach exhibits superior
performance compared to the most advanced models based on GAN and diffusion
techniques, especially in accurately retaining crucial anatomical features such
as fissure lines, airways, and vascular structures. This innovation introduces
novel possibilities. This study focuses on two main objectives: (1) the
development of a method for creating images based on textual prompts and
anatomical components, and (2) the capability to generate new images
conditioning on anatomical elements. The advancements in image generation can
be applied to enhance numerous downstream tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03563">BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields. (arXiv:2310.03563v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Csehi_A/0/1/0/all/0/1">&#xc1;goston Istv&#xe1;n Csehi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jozsa_C/0/1/0/all/0/1">Csaba M&#xe1;t&#xe9; J&#xf3;zsa</a></p>
<p>We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which
defines the image pose estimation problem as a NeRF based iterative linear
optimization. NeRFs are novel neural space representation models that can
synthesize photorealistic novel views of real-world scenes or objects. Our
contributions are as follows: we extend the localization optimization objective
with a depth-based loss function, we introduce a multi-image based loss
function where a sequence of images with known relative poses are used without
increasing the computational complexity, we omit hierarchical sampling during
volumetric rendering, meaning only the coarse model is used for pose
estimation, and we how that by extending the sampling interval convergence can
be achieved even or higher initial pose estimate errors. With the proposed
modifications the convergence speed is significantly improved, and the basin of
convergence is substantially extended.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03602">Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints. (arXiv:2310.03602v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Chuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaotao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1">Kunming Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1">Ping Tan</a></p>
<p>Text-driven 3D indoor scene generation could be useful for gaming, film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which is
able to generate convincing 3D rooms with designer-style layouts and
high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables
versatile interactive editing operations such as resizing or moving individual
furniture items. Our key insight is to separate the modeling of layouts and
appearance. %how to model the room that takes into account both scene texture
and geometry at the same time. To this end, Our proposed method consists of two
stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The
`Layout Generation Stage' trains a text-conditional diffusion model to learn
the layout distribution with our holistic scene code parameterization. Next,
the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a
vivid panoramic image of the room guided by the 3D scene layout and text
prompt. In this way, we achieve a high-quality 3D room with convincing layouts
and lively textures. Benefiting from the scene code parameterization, we can
easily edit the generated room model through our mask-guided editing module,
without expensive editing-specific training. Extensive experiments on the
Structured3D dataset demonstrate that our method outperforms existing methods
in producing more reasonable, view-consistent, and editable 3D rooms from
natural language prompts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03608">How Good Are Synthetic Medical Images? An Empirical Study with Lung Ultrasound. (arXiv:2310.03608v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yu_M/0/1/0/all/0/1">Menghan Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Kulhare_S/0/1/0/all/0/1">Sourabh Kulhare</a>, <a href="http://arxiv.org/find/eess/1/au:+Mehanian_C/0/1/0/all/0/1">Courosh Mehanian</a>, <a href="http://arxiv.org/find/eess/1/au:+Delahunt_C/0/1/0/all/0/1">Charles B Delahunt</a>, <a href="http://arxiv.org/find/eess/1/au:+Shea_D/0/1/0/all/0/1">Daniel E Shea</a>, <a href="http://arxiv.org/find/eess/1/au:+Laverriere_Z/0/1/0/all/0/1">Zohreh Laverriere</a>, <a href="http://arxiv.org/find/eess/1/au:+Shah_I/0/1/0/all/0/1">Ishan Shah</a>, <a href="http://arxiv.org/find/eess/1/au:+Horning_M/0/1/0/all/0/1">Matthew P Horning</a></p>
<p>Acquiring large quantities of data and annotations is known to be effective
for developing high-performing deep learning models, but is difficult and
expensive to do in the healthcare context. Adding synthetic training data using
generative models offers a low-cost method to deal effectively with the data
scarcity challenge, and can also address data imbalance and patient privacy
issues. In this study, we propose a comprehensive framework that fits
seamlessly into model development workflows for medical image analysis. We
demonstrate, with datasets of varying size, (i) the benefits of generative
models as a data augmentation method; (ii) how adversarial methods can protect
patient privacy via data substitution; (iii) novel performance metrics for
these use cases by testing models on real holdout data. We show that training
with both synthetic and real data outperforms training with real data alone,
and that models trained solely with synthetic data approach their real-only
counterparts. Code is available at
https://github.com/Global-Health-Labs/US-DCGAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03615">Animatable Virtual Humans: Learning pose-dependent human representations in UV space for interactive performance synthesis. (arXiv:2310.03615v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morgenstern_W/0/1/0/all/0/1">Wieland Morgenstern</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagdasarian_M/0/1/0/all/0/1">Milena T. Bagdasarian</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1">Anna Hilsmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1">Peter Eisert</a></p>
<p>We propose a novel representation of virtual humans for highly realistic
real-time animation and rendering in 3D applications. We learn pose dependent
appearance and geometry from highly accurate dynamic mesh sequences obtained
from state-of-the-art multiview-video reconstruction. Learning pose-dependent
appearance and geometry from mesh sequences poses significant challenges, as it
requires the network to learn the intricate shape and articulated motion of a
human body. However, statistical body models like SMPL provide valuable
a-priori knowledge which we leverage in order to constrain the dimension of the
search space enabling more efficient and targeted learning and define
pose-dependency. Instead of directly learning absolute pose-dependent geometry,
we learn the difference between the observed geometry and the fitted SMPL
model. This allows us to encode both pose-dependent appearance and geometry in
the consistent UV space of the SMPL model. This approach not only ensures a
high level of realism but also facilitates streamlined processing and rendering
of virtual humans in real-time scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03624">High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning. (arXiv:2310.03624v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schulze_L/0/1/0/all/0/1">Lennart Schulze</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1">Hod Lipson</a></p>
<p>A robot self-model is a task-agnostic representation of the robot's physical
morphology that can be used for motion planning tasks in absence of classical
geometric kinematic models. In particular, when the latter are hard to engineer
or the robot's kinematics change unexpectedly, human-free self-modeling is a
necessary feature of truly autonomous agents. In this work, we leverage neural
fields to allow a robot to self-model its kinematics as a neural-implicit query
model learned only from 2D images annotated with camera poses and
configurations. This enables significantly greater applicability than existing
approaches which have been dependent on depth images or geometry knowledge. To
this end, alongside a curricular data sampling strategy, we propose a new
encoder-based neural density field architecture for dynamic object-centric
scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF
robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%
of the robot's workspace dimension. We demonstrate the capabilities of this
model on a motion planning task as an exemplary downstream application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03629">Wasserstein Distortion: Unifying Fidelity and Realism. (arXiv:2310.03629v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yang Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1">Aaron B. Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1">Johannes Ball&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Theis_L/0/1/0/all/0/1">Lucas Theis</a></p>
<p>We introduce a distortion measure for images, Wasserstein distortion, that
simultaneously generalizes pixel-level fidelity on the one hand and realism on
the other. We show how Wasserstein distortion reduces mathematically to a pure
fidelity constraint or a pure realism constraint under different parameter
choices. Pairs of images that are close under Wasserstein distortion illustrate
its utility. In particular, we generate random textures that have high fidelity
to a reference texture in one location of the image and smoothly transition to
an independent realization of the texture as one moves away from this point.
Connections between Wasserstein distortion and models of the human visual
system are noted.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03635">CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jiayuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xuelin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xikun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah D. Goodman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a></p>
<p>Building machines that can reason about physical events and their causal
relationships is crucial for flexible interaction with the physical world.
However, most existing physical and causal reasoning benchmarks are exclusively
based on synthetically generated events and synthetic natural language
descriptions of causal relationships. This design brings up two issues. First,
there is a lack of diversity in both event types and natural language
descriptions; second, causal relationships based on manually-defined heuristics
are different from human judgments. To address both shortcomings, we present
the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of
physical events with human labels. We employ two techniques to improve data
collection efficiency: first, a novel iterative event cloze task to elicit a
new representation of events in videos, which we term Causal Event Graphs
(CEGs); second, a data augmentation technique based on neural language
generative models. We convert the collected CEGs into questions and answers to
be consistent with prior work. Finally, we study a collection of baseline
approaches for CLEVRER-Humans question-answering, highlighting the great
challenges set forth by our benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03658">Visual inspection for illicit items in X-ray images using Deep Learning. (arXiv:2310.03658v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1">Ioannis Mademlis</a>, <a href="http://arxiv.org/find/cs/1/au:+Batsis_G/0/1/0/all/0/1">Georgios Batsis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chrysochoou_A/0/1/0/all/0/1">Adamantia Anna Rebolledo Chrysochoou</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_G/0/1/0/all/0/1">Georgios Th. Papadopoulos</a></p>
<p>Automated detection of contraband items in X-ray images can significantly
increase public safety, by enhancing the productivity and alleviating the
mental load of security officers in airports, subways, customs/post offices,
etc. The large volume and high throughput of passengers, mailed parcels, etc.,
during rush hours practically make it a Big Data problem. Modern computer
vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of
undertaking this task even under resource-constrained and embedded execution
scenarios, e.g., as is the case with fast, single-stage object detectors.
However, no comparative experimental assessment of the various relevant DNN
components/methods has been performed under a common evaluation protocol, which
means that reliable cross-method comparisons are missing. This paper presents
exactly such a comparative assessment, utilizing a public relevant dataset and
a well-defined methodology for selecting the specific DNN components/modules
that are being evaluated. The results indicate the superiority of Transformer
detectors, the obsolete nature of auxiliary neural modules that have been
developed in the past few years for security applications and the efficiency of
the CSP-DarkNet backbone CNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03661">Robustness-Guided Image Synthesis for Data-Free Quantization. (arXiv:2310.03661v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jianhong Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1">Huanpeng Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hualiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zuozhu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Ruizhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaoxuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_L/0/1/0/all/0/1">Lianrui Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1">Chengfei Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Haoji Hu</a></p>
<p>Quantization has emerged as a promising direction for model compression.
Recently, data-free quantization has been widely studied as a promising method
to avoid privacy concerns, which synthesizes images as an alternative to real
training data. Existing methods use classification loss to ensure the
reliability of the synthesized images. Unfortunately, even if these images are
well-classified by the pre-trained model, they still suffer from low semantics
and homogenization issues. Intuitively, these low-semantic images are sensitive
to perturbations, and the pre-trained model tends to have inconsistent output
when the generator synthesizes an image with poor semantics. To this end, we
propose Robustness-Guided Image Synthesis (RIS), a simple but effective method
to enrich the semantics of synthetic images and improve image diversity,
further boosting the performance of downstream data-free compression tasks.
Concretely, we first introduce perturbations on input and model weight, then
define the inconsistency metrics at feature and prediction levels before and
after perturbations. On the basis of inconsistency on two levels, we design a
robustness optimization objective to enhance the semantics of synthetic images.
Moreover, we also make our approach diversity-aware by forcing the generator to
synthesize images with small correlations in the label space. With RIS, we
achieve state-of-the-art performance for various settings on data-free
quantization and can be extended to other data-free compression tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03664">Certification of Deep Learning Models for Medical Image Segmentation. (arXiv:2310.03664v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Laousy_O/0/1/0/all/0/1">Othmane Laousy</a>, <a href="http://arxiv.org/find/eess/1/au:+Araujo_A/0/1/0/all/0/1">Alexandre Araujo</a>, <a href="http://arxiv.org/find/eess/1/au:+Chassagnon_G/0/1/0/all/0/1">Guillaume Chassagnon</a>, <a href="http://arxiv.org/find/eess/1/au:+Paragios_N/0/1/0/all/0/1">Nikos Paragios</a>, <a href="http://arxiv.org/find/eess/1/au:+Revel_M/0/1/0/all/0/1">Marie-Pierre Revel</a>, <a href="http://arxiv.org/find/eess/1/au:+Vakalopoulou_M/0/1/0/all/0/1">Maria Vakalopoulou</a></p>
<p>In medical imaging, segmentation models have known a significant improvement
in the past decade and are now used daily in clinical practice. However,
similar to classification models, segmentation models are affected by
adversarial attacks. In a safety-critical field like healthcare, certifying
model predictions is of the utmost importance. Randomized smoothing has been
introduced lately and provides a framework to certify models and obtain
theoretical guarantees. In this paper, we present for the first time a
certified segmentation baseline for medical imaging based on randomized
smoothing and diffusion models. Our results show that leveraging the power of
denoising diffusion probabilistic models helps us overcome the limits of
randomized smoothing. We conduct extensive experiments on five public datasets
of chest X-rays, skin lesions, and colonoscopies, and empirically show that we
are able to maintain high certified Dice scores even for highly perturbed
images. Our work represents the first attempt to certify medical image
segmentation models, and we aspire for it to set a foundation for future
benchmarks in this crucial and largely uncharted area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03669">LumiNet: The Bright Side of Perceptual Knowledge Distillation. (arXiv:2310.03669v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1">Md. Ismail Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1">M M Lutfe Elahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1">Sameera Ramasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1">Ali Cheraghian</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_F/0/1/0/all/0/1">Fuad Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1">Nabeel Mohammed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1">Shafin Rahman</a></p>
<p>In knowledge distillation research, feature-based methods have dominated due
to their ability to effectively tap into extensive teacher models. In contrast,
logit-based approaches are considered to be less adept at extracting hidden
'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel
knowledge-transfer algorithm designed to enhance logit-based distillation. We
introduce a perception matrix that aims to recalibrate logits through
adjustments based on the model's representation capability. By meticulously
analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class
relationships, enabling the student model to learn a richer breadth of
knowledge. Both teacher and student models are mapped onto this refined matrix,
with the student's goal being to minimize representational discrepancies.
Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)
attests to LumiNet's efficacy, revealing its competitive edge over leading
feature-based methods. Moreover, in exploring the realm of transfer learning,
we assess how effectively the student model, trained using our method, adapts
to downstream tasks. Notably, when applied to Tiny ImageNet, the transferred
features exhibit remarkable performance, further underscoring LumiNet's
versatility and robustness in diverse settings. With LumiNet, we hope to steer
the research discourse towards a renewed interest in the latent capabilities of
logit-based knowledge distillation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03670">Regress Before Construct: Regress Autoencoder for Point Cloud Self-supervised Learning. (arXiv:2310.03670v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Can Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+King_X/0/1/0/all/0/1">Xulin King</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a></p>
<p>Masked Autoencoders (MAE) have demonstrated promising performance in
self-supervised learning for both 2D and 3D computer vision. Nevertheless,
existing MAE-based methods still have certain drawbacks. Firstly, the
functional decoupling between the encoder and decoder is incomplete, which
limits the encoder's representation learning ability. Secondly, downstream
tasks solely utilize the encoder, failing to fully leverage the knowledge
acquired through the encoder-decoder architecture in the pre-text task. In this
paper, we propose Point Regress AutoEncoder (Point-RAE), a new scheme for
regressive autoencoders for point cloud self-supervised learning. The proposed
method decouples functions between the decoder and the encoder by introducing a
mask regressor, which predicts the masked patch representation from the visible
patch representation encoded by the encoder and the decoder reconstructs the
target from the predicted masked patch representation. By doing so, we minimize
the impact of decoder updates on the representation space of the encoder.
Moreover, we introduce an alignment constraint to ensure that the
representations for masked patches, predicted from the encoded representations
of visible patches, are aligned with the masked patch presentations computed
from the encoder. To make full use of the knowledge learned in the pre-training
stage, we design a new finetune mode for the proposed Point-RAE. Extensive
experiments demonstrate that our approach is efficient during pre-training and
generalizes well on various downstream tasks. Specifically, our pre-trained
models achieve a high accuracy of \textbf{90.28\%} on the ScanObjectNN hardest
split and \textbf{94.1\%} accuracy on ModelNet40, surpassing all the other
self-supervised learning methods. Our code and pretrained model are public
available at: \url{https://github.com/liuyyy111/Point-RAE}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03704">Drag View: Generalizable Novel View Synthesis with Unposed Imagery. (arXiv:2310.03704v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhiwen Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_P/0/1/0/all/0/1">Panwang Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peihao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hanwen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zehao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>We introduce DragView, a novel and interactive framework for generating novel
views of unseen scenes. DragView initializes the new view from a single source
image, and the rendering is supported by a sparse set of unposed multi-view
images, all seamlessly executed within a single feed-forward pass. Our approach
begins with users dragging a source view through a local relative coordinate
system. Pixel-aligned features are obtained by projecting the sampled 3D points
along the target ray onto the source view. We then incorporate a view-dependent
modulation layer to effectively handle occlusion during the projection.
Additionally, we broaden the epipolar attention mechanism to encompass all
source pixels, facilitating the aggregation of initialized coordinate-aligned
point features from other unposed views. Finally, we employ another transformer
to decode ray features into final pixel intensities. Crucially, our framework
does not rely on either 2D prior models or the explicit estimation of camera
poses. During testing, DragView showcases the capability to generalize to new
scenes unseen during training, also utilizing only unposed support images,
enabling the generation of photo-realistic new views characterized by flexible
camera trajectories. In our experiments, we conduct a comprehensive comparison
of the performance of DragView with recent scene representation networks
operating under pose-free conditions, as well as with generalizable NeRFs
subject to noisy test camera poses. DragView consistently demonstrates its
superior performance in view synthesis quality, while also being more
user-friendly. Project page: https://zhiwenfan.github.io/DragView/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03707">OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks. (arXiv:2310.03707v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tal_O/0/1/0/all/0/1">Ofir Bar Tal</a>, <a href="http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1">Adi Haviv</a>, <a href="http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1">Amit H. Bermano</a></p>
<p>Evasion Attacks (EA) are used to test the robustness of trained neural
networks by distorting input data to misguide the model into incorrect
classifications. Creating these attacks is a challenging task, especially with
the ever-increasing complexity of models and datasets. In this work, we
introduce a self-supervised, computationally economical method for generating
adversarial examples, designed for the unseen black-box setting. Adapting
techniques from representation learning, our method generates on-manifold EAs
that are encouraged to resemble the data distribution. These attacks are
comparable in effectiveness compared to the state-of-the-art when attacking the
model trained on, but are significantly more effective when attacking unseen
models, as the attacks are more related to the data rather than the model
itself. Our experiments consistently demonstrate the method is effective across
various models, unseen data categories, and even defended models, suggesting a
significant role for on-manifold EAs when targeting unseen models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03731">MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Houxing Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Aojun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zimu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Sichun Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weikang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Renrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Linqi Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1">Mingjie Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a></p>
<p>The recently released GPT-4 Code Interpreter has demonstrated remarkable
proficiency in solving challenging math problems, primarily attributed to its
ability to seamlessly reason with natural language, generate code, execute
code, and continue reasoning based on the execution output. In this paper, we
present a method to fine-tune open-source language models, enabling them to use
code for modeling and deriving math equations and, consequently, enhancing
their mathematical reasoning abilities. We propose a method of generating novel
and high-quality datasets with math problems and their code-based solutions,
referred to as MathCodeInstruct. Each solution interleaves natural language,
code, and execution results. We also introduce a customized supervised
fine-tuning and inference approach. This approach yields the MathCoder models,
a family of models capable of generating code-based solutions for solving
challenging math problems. Impressively, the MathCoder models achieve
state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K
(83.9%) datasets, substantially outperforming other open-source alternatives.
Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K
and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The
dataset and models will be released at https://github.com/mathllm/MathCoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.00696">Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms. (arXiv:2102.00696v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tekin_S/0/1/0/all/0/1">Selim Furkan Tekin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazla_A/0/1/0/all/0/1">Arda Fazla</a>, <a href="http://arxiv.org/find/cs/1/au:+Kozat_S/0/1/0/all/0/1">Suleyman Serdar Kozat</a></p>
<p>Numerical weather forecasting using high-resolution physical models often
requires extensive computational resources on supercomputers, which diminishes
their wide usage in most real-life applications. As a remedy, applying deep
learning methods has revealed innovative solutions within this field. To this
end, we introduce a novel deep learning architecture for forecasting
high-resolution spatio-temporal weather data. Our approach extends the
conventional encoder-decoder structure by integrating Convolutional Long-short
Term Memory and Convolutional Neural Networks. In addition, we incorporate
attention and context matcher mechanisms into the model architecture. Our
Weather Model achieves significant performance improvements compared to
baseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our
experimental evaluation involves high-scale, real-world benchmark numerical
weather datasets, namely the ERA5 hourly dataset on pressure levels and
WeatherBench. Our results demonstrate substantial improvements in identifying
spatial and temporal correlations with attention matrices focusing on distinct
parts of the input series to model atmospheric circulations. We also compare
our model with high-resolution physical models using the benchmark metrics and
show that our Weather Model is accurate and easy to interpret.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.14883">Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shenggui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1">Zhengda Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jiarui Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haichen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1">Yang You</a></p>
<p>The success of Transformer models has pushed the deep learning model scale to
billions of parameters. Due to the limited memory resource of a single GPU,
However, the best practice for choosing the optimal parallel strategy is still
lacking, since it requires domain expertise in both deep learning and parallel
computing.
</p>
<p>The Colossal-AI system addressed the above challenge by introducing a unified
interface to scale your sequential code of model training to distributed
environments. It supports parallel training methods such as data, pipeline,
tensor, and sequence parallelism, as well as heterogeneous training methods
integrated with zero redundancy optimizer. Compared to the baseline system,
Colossal-AI can achieve up to 2.76 times training speedup on large-scale
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.15497">Unsupervised Foreground Extraction via Deep Region Competition. (arXiv:2110.15497v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Peiyu Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Sirui Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yixin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a></p>
<p>We present Deep Region Competition (DRC), an algorithm designed to extract
foreground objects from images in a fully unsupervised manner. Foreground
extraction can be viewed as a special case of generic image segmentation that
focuses on identifying and disentangling objects from the background. In this
work, we rethink the foreground extraction by reconciling energy-based prior
with generative image modeling in the form of Mixture of Experts (MoE), where
we further introduce the learned pixel re-assignment as the essential inductive
bias to capture the regularities of background regions. With this modeling, the
foreground-background partition can be naturally found through
Expectation-Maximization (EM). We show that the proposed method effectively
exploits the interaction between the mixture components during the partitioning
process, which closely connects to region competition, a seminal approach for
generic image segmentation. Experiments demonstrate that DRC exhibits more
competitive performances on complex real-world data and challenging
multi-object scenes compared with prior methods. Moreover, we show empirically
that DRC can potentially generalize to novel foreground objects even from
categories unseen during training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.12232">PMSSC: Parallelizable multi-subset based self-expressive model for subspace clustering. (arXiv:2111.12232v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hotta_K/0/1/0/all/0/1">Katsuya Hotta</a>, <a href="http://arxiv.org/find/cs/1/au:+Akashi_T/0/1/0/all/0/1">Takuya Akashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tokai_S/0/1/0/all/0/1">Shogo Tokai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a></p>
<p>Subspace clustering methods which embrace a self-expressive model that
represents each data point as a linear combination of other data points in the
dataset provide powerful unsupervised learning techniques. However, when
dealing with large datasets, representation of each data point by referring to
all data points via a dictionary suffers from high computational complexity. To
alleviate this issue, we introduce a parallelizable multi-subset based
self-expressive model (PMS) which represents each data point by combining
multiple subsets, with each consisting of only a small proportion of the
samples. The adoption of PMS in subspace clustering (PMSSC) leads to
computational advantages because the optimization problems decomposed over each
subset are small, and can be solved efficiently in parallel. Furthermore, PMSSC
is able to combine multiple self-expressive coefficient vectors obtained from
subsets, which contributes to an improvement in self-expressiveness. Extensive
experiments on synthetic and real-world datasets show the efficiency and
effectiveness of our approach in comparison to other methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.09573">Diversity in deep generative models and generative AI. (arXiv:2202.09573v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Turinici_G/0/1/0/all/0/1">Gabriel Turinici</a></p>
<p>The decoder-based machine learning generative algorithms such as Generative
Adversarial Networks (GAN), Variational Auto-Encoders (VAE), Transformers show
impressive results when constructing objects similar to those in a training
ensemble. However, the generation of new objects builds mainly on the
understanding of the hidden structure of the training dataset followed by a
sampling from a multi-dimensional normal variable. In particular each sample is
independent from the others and can repeatedly propose same type of objects. To
cure this drawback we introduce a kernel-based measure quantization method that
can produce new objects from a given target measure by approximating it as a
whole and even staying away from elements already drawn from that distribution.
This ensures a better diversity of the produced objects. The method is tested
on classic machine learning benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.06184">GSDA: Generative Adversarial Network-based Semi-Supervised Data Augmentation for Ultrasound Image Classification. (arXiv:2203.06184v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1">Zhaoshan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lv_Q/0/1/0/all/0/1">Qiujie Lv</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1">Chau Hung Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1">Lei Shen</a></p>
<p>Medical Ultrasound (US) is one of the most widely used imaging modalities in
clinical practice, but its usage presents unique challenges such as variable
imaging quality. Deep Learning (DL) models can serve as advanced medical US
image analysis tools, but their performance is greatly limited by the scarcity
of large datasets. To solve the common data shortage, we develop GSDA, a
Generative Adversarial Network (GAN)-based semi-supervised data augmentation
method. GSDA consists of the GAN and Convolutional Neural Network (CNN). The
GAN synthesizes and pseudo-labels high-resolution, high-quality US images, and
both real and synthesized images are then leveraged to train the CNN. To
address the training challenges of both GAN and CNN with limited data, we
employ transfer learning techniques during their training. We also introduce a
novel evaluation standard that balances classification accuracy with
computational time. We evaluate our method on the BUSI dataset and GSDA
outperforms existing state-of-the-art methods. With the high-resolution and
high-quality images synthesized, GSDA achieves a 97.9% accuracy using merely
780 images. Given these promising results, we believe that GSDA holds potential
as an auxiliary tool for medical US analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.03519">Self-supervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1">Peizhou Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoliang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaojuan Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Dong_L/0/1/0/all/0/1">Liang Dong</a>, <a href="http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1">Leslie Ying</a></p>
<p>Deep learning methods have been successfully used in various computer vision
tasks. Inspired by that success, deep learning has been explored in magnetic
resonance imaging (MRI) reconstruction. In particular, integrating deep
learning and model-based optimization methods has shown considerable
advantages. However, a large amount of labeled training data is typically
needed for high reconstruction quality, which is challenging for some MRI
applications. In this paper, we propose a novel reconstruction method, named
DURED-Net, that enables interpretable self-supervised learning for MR image
reconstruction by combining a self-supervised denoising network and a
plug-and-play method. We aim to boost the reconstruction performance of
Noise2Noise in MR reconstruction by adding an explicit prior that utilizes
imaging physics. Specifically, the leverage of a denoising network for MRI
reconstruction is achieved using Regularization by Denoising (RED). Experiment
results demonstrate that the proposed method requires a reduced amount of
training data to achieve high reconstruction quality among the state-of-art of
MR reconstruction utilizing the Noise2Noise method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.05241">CANet: Channel Extending and Axial Attention Catching Network for Multi-structure Kidney Segmentation. (arXiv:2208.05241v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bu_Z/0/1/0/all/0/1">Zhenyu Bu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1">Kai-Ni Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_G/0/1/0/all/0/1">Guang-Quan Zhou</a></p>
<p>Renal cancer is one of the most prevalent cancers worldwide. Clinical signs
of kidney cancer include hematuria and low back discomfort, which are quite
distressing to the patient. Some surgery-based renal cancer treatments like
laparoscopic partial nephrectomy relys on the 3D kidney parsing on computed
tomography angiography (CTA) images. Many automatic segmentation techniques
have been put forward to make multi-structure segmentation of the kidneys more
accurate. The 3D visual model of kidney anatomy will help clinicians plan
operations accurately before surgery. However, due to the diversity of the
internal structure of the kidney and the low grey level of the edge. It is
still challenging to separate the different parts of the kidney in a clear and
accurate way. In this paper, we propose a channel extending and axial attention
catching Network(CANet) for multi-structure kidney segmentation. Our solution
is founded based on the thriving nn-UNet architecture. Firstly, by extending
the channel size, we propose a larger network, which can provide a broader
perspective, facilitating the extraction of complex structural information.
Secondly, we include an axial attention catching(AAC) module in the decoder,
which can obtain detailed information for refining the edges. We evaluate our
CANet on the KiPA2022 dataset, achieving the dice scores of 95.8%, 89.1%, 87.5%
and 84.9% for kidney, tumor, artery and vein, respectively, which helps us get
fourth place in the challenge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.12148">Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection. (arXiv:2209.12148v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1">Neelu Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1">Nicolae-Catalin Ristea</a>, <a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1">Radu Tudor Ionescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1">Kamal Nasrollahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1">Thomas B. Moeslund</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1">Mubarak Shah</a></p>
<p>Anomaly detection has recently gained increasing attention in the field of
computer vision, likely due to its broad set of applications ranging from
product fault detection on industrial production lines and impending event
detection in video surveillance to finding lesions in medical scans. Regardless
of the domain, anomaly detection is typically framed as a one-class
classification task, where the learning is conducted on normal examples only.
An entire family of successful anomaly detection methods is based on learning
to reconstruct masked normal inputs (e.g. patches, future frames, etc.) and
exerting the magnitude of the reconstruction error as an indicator for the
abnormality level. Unlike other reconstruction-based methods, we present a
novel self-supervised masked convolutional transformer block (SSMCTB) that
comprises the reconstruction-based functionality at a core architectural level.
The proposed self-supervised block is extremely flexible, enabling information
masking at any layer of a neural network and being compatible with a wide range
of neural architectures. In this work, we extend our previous self-supervised
predictive convolutional attentive block (SSPCAB) with a 3D masked
convolutional layer, a transformer for channel-wise attention, as well as a
novel self-supervised objective based on Huber loss. Furthermore, we show that
our block is applicable to a wider variety of tasks, adding anomaly detection
in medical images and thermal videos to the previously considered tasks based
on RGB images and surveillance videos. We exhibit the generality and
flexibility of SSMCTB by integrating it into multiple state-of-the-art neural
models for anomaly detection, bringing forth empirical results that confirm
considerable performance improvements on five benchmarks. We release our code
and data as open source at: https://github.com/ristea/ssmctb.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.03660">SC-DepthV3: Robust Self-supervised Monocular Depth Estimation for Dynamic Scenes. (arXiv:2211.03660v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Libo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jia-Wang Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1">Huangying Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1">Wei Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1">Ian Reid</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a></p>
<p>Self-supervised monocular depth estimation has shown impressive results in
static scenes. It relies on the multi-view consistency assumption for training
networks, however, that is violated in dynamic object regions and occlusions.
Consequently, existing methods show poor accuracy in dynamic scenes, and the
estimated depth map is blurred at object boundaries because they are usually
occluded in other training views. In this paper, we propose SC-DepthV3 for
addressing the challenges. Specifically, we introduce an external pretrained
monocular depth estimation model for generating single-image depth prior,
namely pseudo-depth, based on which we propose novel losses to boost
self-supervised training. As a result, our model can predict sharp and accurate
depth maps, even when training from monocular videos of highly-dynamic scenes.
We demonstrate the significantly superior performance of our method over
previous methods on six challenging datasets, and we provide detailed ablation
studies for the proposed terms. Source code and data will be released at
https://github.com/JiawangBian/sc_depth_pl
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.07091">BiViT: Extremely Compressed Binary Vision Transformer. (arXiv:2211.07091v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yefei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Z/0/1/0/all/0/1">Zhenyu Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Luoming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Model binarization can significantly compress model size, reduce energy
consumption, and accelerate inference through efficient bit-wise operations.
Although binarizing convolutional neural networks have been extensively
studied, there is little work on exploring binarization of vision Transformers
which underpin most recent breakthroughs in visual recognition. To this end, we
propose to solve two fundamental challenges to push the horizon of Binary
Vision Transformers (BiViT). First, the traditional binary method does not take
the long-tailed distribution of softmax attention into consideration, bringing
large binarization errors in the attention module. To solve this, we propose
Softmax-aware Binarization, which dynamically adapts to the data distribution
and reduces the error caused by binarization. Second, to better preserve the
information of the pretrained model and restore accuracy, we propose a
Cross-layer Binarization scheme that decouples the binarization of
self-attention and multi-layer perceptrons (MLPs), and Parameterized Weight
Scales which introduce learnable scaling factors for weight binarization.
Overall, our method performs favorably against state-of-the-arts by 19.8% on
the TinyImageNet dataset. On ImageNet, our BiViT achieves a competitive 75.6%
Top-1 accuracy over Swin-S model. Additionally, on COCO object detection, our
method achieves an mAP of 40.8 with a Swin-T backbone over Cascade Mask R-CNN
framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02648">Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moayeri_M/0/1/0/all/0/1">Mazda Moayeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a></p>
<p>We present a simple but effective method to measure and mitigate model biases
caused by reliance on spurious cues. Instead of requiring costly changes to
one's data or model training, our method better utilizes the data one already
has by sorting them. Specifically, we rank images within their classes based on
spuriosity (the degree to which common spurious cues are present), proxied via
deep neural features of an interpretable network. With spuriosity rankings, it
is easy to identify minority subpopulations (i.e. low spuriosity images) and
assess model bias as the gap in accuracy between high and low spuriosity
images. One can even efficiently remove a model's bias at little cost to
accuracy by finetuning its classification head on low spuriosity images,
resulting in fairer treatment of samples regardless of spuriosity. We
demonstrate our method on ImageNet, annotating $5000$ class-feature
dependencies ($630$ of which we find to be spurious) and generating a dataset
of $325k$ soft segmentations for these features along the way. Having computed
spuriosity rankings via the identified spurious neural features, we assess
biases for $89$ diverse models and find that class-wise biases are highly
correlated across models. Our results suggest that model bias due to spurious
feature reliance is influenced far more by what the model is trained on than
how it is trained.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04494">Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains. (arXiv:2301.04494v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1">Indel Pal Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghorbel_E/0/1/0/all/0/1">Enjie Ghorbel</a>, <a href="http://arxiv.org/find/cs/1/au:+Oyedotun_O/0/1/0/all/0/1">Oyebade Oyedotun</a>, <a href="http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1">Djamila Aouada</a></p>
<p>This paper proposes an adaptive graph-based approach for multi-label image
classification. Graph-based methods have been largely exploited in the field of
multi-label classification, given their ability to model label correlations.
Specifically, their effectiveness has been proven not only when considering a
single domain but also when taking into account multiple domains. However, the
topology of the used graph is not optimal as it is pre-defined heuristically.
In addition, consecutive Graph Convolutional Network (GCN) aggregations tend to
destroy the feature similarity. To overcome these issues, an architecture for
learning the graph connectivity in an end-to-end fashion is introduced. This is
done by integrating an attention-based mechanism and a similarity-preserving
strategy. The proposed framework is then extended to multiple domains using an
adversarial training scheme. Numerous experiments are reported on well-known
single-domain and multi-domain benchmarks. The results demonstrate that our
approach achieves competitive results in terms of mean Average Precision (mAP)
and model size as compared to the state-of-the-art. The code will be made
publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04554">Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis. (arXiv:2301.04554v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1">Benedetta Tondi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1">Mauro Barni</a></p>
<p>We propose a Universal Defence against backdoor attacks based on Clustering
and Centroids Analysis (CCA-UD). The goal of the defence is to reveal whether a
Deep Neural Network model is subject to a backdoor attack by inspecting the
training dataset. CCA-UD first clusters the samples of the training set by
means of density-based clustering. Then, it applies a novel strategy to detect
the presence of poisoned clusters. The proposed strategy is based on a general
misclassification behaviour observed when the features of a representative
example of the analysed cluster are added to benign samples. The capability of
inducing a misclassification error is a general characteristic of poisoned
samples, hence the proposed defence is attack-agnostic. This marks a
significant difference with respect to existing defences, that, either can
defend against only some types of backdoor attacks, or are effective only when
some conditions on the poisoning ratio or the kind of triggering signal used by
the attacker are satisfied.
</p>
<p>Experiments carried out on several classification tasks and network
architectures, considering different types of backdoor attacks (with either
clean or corrupted labels), and triggering signals, including both global and
local triggering signals, as well as sample-specific and source-specific
triggers, reveal that the proposed method is very effective to defend against
backdoor attacks in all the cases, always outperforming the state of the art
techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02394">Eliminating Contextual Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion. (arXiv:2302.02394v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zuopeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Tianshu Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1">Erdun Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Daqing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoyue Wang</a></p>
<p>The recent success of text-to-image generation diffusion models has also
revolutionized semantic image editing, enabling the manipulation of images
based on query/target texts. Despite these advancements, a significant
challenge lies in the potential introduction of contextual prior bias in
pre-trained models during image editing, e.g., making unexpected modifications
to inappropriate regions. To address this issue, we present a novel approach
called Dual-Cycle Diffusion, which generates an unbiased mask to guide image
editing. The proposed model incorporates a Bias Elimination Cycle that consists
of both a forward path and an inverted path, each featuring a Structural
Consistency Cycle to ensure the preservation of image content during the
editing process. The forward path utilizes the pre-trained model to produce the
edited image, while the inverted path converts the result back to the source
image. The unbiased mask is generated by comparing differences between the
processed source image and the edited image to ensure that both conform to the
same distribution. Our experiments demonstrate the effectiveness of the
proposed method, as it significantly improves the D-CLIP score from 0.272 to
0.283. The code will be available at
https://github.com/JohnDreamer/DualCycleDiffsion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02936">Private GANs, Revisited. (arXiv:2302.02936v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bie_A/0/1/0/all/0/1">Alex Bie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1">Gautam Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guojun Zhang</a></p>
<p>We show that the canonical approach for training differentially private GANs
-- updating the discriminator with differentially private stochastic gradient
descent (DPSGD) -- can yield significantly improved results after modifications
to training. Specifically, we propose that existing instantiations of this
approach neglect to consider how adding noise only to discriminator updates
inhibits discriminator training, disrupting the balance between the generator
and discriminator necessary for successful GAN training. We show that a simple
fix -- taking more discriminator steps between generator steps -- restores
parity between the generator and discriminator and improves results.
</p>
<p>Additionally, with the goal of restoring parity, we experiment with other
modifications -- namely, large batch sizes and adaptive discriminator update
frequency -- to improve discriminator training and see further improvements in
generation quality. Our results demonstrate that on standard image synthesis
benchmarks, DPSGD outperforms all alternative GAN privatization schemes. Code:
https://github.com/alexbie98/dpgan-revisit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01338">AdvRain: Adversarial Raindrops to Attack Camera-based Smart Vision Systems. (arXiv:2303.01338v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1">Amira Guesmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1">Muhammad Abdullah Hanif</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1">Muhammad Shafique</a></p>
<p>Vision-based perception modules are increasingly deployed in many
applications, especially autonomous vehicles and intelligent robots. These
modules are being used to acquire information about the surroundings and
identify obstacles. Hence, accurate detection and classification are essential
to reach appropriate decisions and take appropriate and safe actions at all
times. Current studies have demonstrated that "printed adversarial attacks",
known as physical adversarial attacks, can successfully mislead perception
models such as object detectors and image classifiers. However, most of these
physical attacks are based on noticeable and eye-catching patterns for
generated perturbations making them identifiable/detectable by human eye or in
test drives. In this paper, we propose a camera-based inconspicuous adversarial
attack (\textbf{AdvRain}) capable of fooling camera-based perception systems
over all objects of the same class. Unlike mask based fake-weather attacks that
require access to the underlying computing hardware or image memory, our attack
is based on emulating the effects of a natural weather condition (i.e.,
Raindrops) that can be printed on a translucent sticker, which is externally
placed over the lens of a camera. To accomplish this, we provide an iterative
process based on performing a random search aiming to identify critical
positions to make sure that the performed transformation is adversarial for a
target classifier. Our transformation is based on blurring predefined parts of
the captured image corresponding to the areas covered by the raindrop. We
achieve a drop in average model accuracy of more than $45\%$ and $40\%$ on
VGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only $20$
raindrops.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06088">Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization. (arXiv:2303.06088v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1">Marin Scalbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1">Maria Vakalopoulou</a>, <a href="http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1">Florent Couzini&#xe9;-Devy</a></p>
<p>In Self-Supervised Learning (SSL), models are typically pretrained,
fine-tuned, and evaluated on the same domains. However, they tend to perform
poorly when evaluated on unseen domains, a challenge that Unsupervised Domain
Generalization (UDG) seeks to address. Current UDG methods rely on domain
labels, which are often challenging to collect, and domain-specific
architectures that lack scalability when confronted with numerous domains,
making the current methodology impractical and rigid. Inspired by
contrastive-based UDG methods that mitigate spurious correlations by
restricting comparisons to examples from the same domain, we hypothesize that
eliminating style variability within a batch could provide a more convenient
and flexible way to reduce spurious correlations without requiring domain
labels. To verify this hypothesis, we introduce Batch Styles Standardization
(BSS), a relatively simple yet powerful Fourier-based method to standardize the
style of images in a batch specifically designed for integration with SSL
methods to tackle UDG. Combining BSS with existing SSL methods offers serious
advantages over prior UDG methods: (1) It eliminates the need for domain labels
or domain-specific network components to enhance domain-invariance in SSL
representations, and (2) offers flexibility as BSS can be seamlessly integrated
with diverse contrastive-based but also non-contrastive-based SSL methods.
Experiments on several UDG datasets demonstrate that it significantly improves
downstream task performances on unseen domains, often outperforming or rivaling
with UDG methods. Finally, this work clarifies the underlying mechanisms
contributing to BSS's effectiveness in improving domain-invariance in SSL
representations and performances on unseen domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09234">NAISR: A 3D Neural Additive Model for Interpretable Shape Representation. (arXiv:2303.09234v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1">Yining Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zdanski_C/0/1/0/all/0/1">Carlton Zdanski</a>, <a href="http://arxiv.org/find/cs/1/au:+Kimbell_J/0/1/0/all/0/1">Julia Kimbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Prince_A/0/1/0/all/0/1">Andrew Prince</a>, <a href="http://arxiv.org/find/cs/1/au:+Worden_C/0/1/0/all/0/1">Cameron Worden</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirse_S/0/1/0/all/0/1">Samuel Kirse</a>, <a href="http://arxiv.org/find/cs/1/au:+Rutter_C/0/1/0/all/0/1">Christopher Rutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Shields_B/0/1/0/all/0/1">Benjamin Shields</a>, <a href="http://arxiv.org/find/cs/1/au:+Dunn_W/0/1/0/all/0/1">William Dunn</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1">Jisan Mahmud</a>, <a href="http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1">Marc Niethammer</a></p>
<p>Deep implicit functions (DIFs) have emerged as a powerful paradigm for many
computer vision tasks such as 3D shape reconstruction, generation,
registration, completion, editing, and understanding. However, given a set of
3D shapes with associated covariates there is at present no shape
representation method which allows to precisely represent the shapes while
capturing the individual dependencies on each covariate. Such a method would be
of high utility to researchers to discover knowledge hidden in a population of
shapes. For scientific shape discovery, we propose a 3D Neural Additive Model
for Interpretable Shape Representation ($\texttt{NAISR}$) which describes
individual shapes by deforming a shape atlas in accordance to the effect of
disentangled covariates. Our approach captures shape population trends and
allows for patient-specific predictions through shape transfer.
$\texttt{NAISR}$ is the first approach to combine the benefits of deep implicit
shape representations with an atlas deforming according to specified
covariates. We evaluate $\texttt{NAISR}$ with respect to shape reconstruction,
shape disentanglement, shape evolution, and shape transfer on three datasets:
1) $\textit{Starman}$, a simulated 2D shape dataset; 2) the ADNI hippocampus 3D
shape dataset; and 3) a pediatric airway 3D shape dataset. Our experiments
demonstrate that $\textit{Starman}$ achieves excellent shape reconstruction
performance while retaining interpretability. Our code is available at
$\href{https://github.com/uncbiag/NAISR}{https://github.com/uncbiag/NAISR}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09874">Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1">Alexander Hepburn</a>, <a href="http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1">Valero Laparra</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1">Ra&#xfa;l Santos-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1">Jes&#xfa;s Malo</a></p>
<p>In the 1950s, Barlow and Attneave hypothesised a link between biological
vision and information maximisation. Following Shannon, information was defined
using the probability of natural images. A number of physiological and
psychophysical phenomena have been derived ever since from principles like
info-max, efficient coding, or optimal denoising. However, it remains unclear
how this link is expressed in mathematical terms from image probability. First,
classical derivations were subjected to strong assumptions on the probability
models and on the behaviour of the sensors. Moreover, the direct evaluation of
the hypothesis was limited by the inability of the classical image models to
deliver accurate estimates of the probability. In this work we directly
evaluate image probabilities using an advanced generative model for natural
images, and we analyse how probability-related factors can be combined to
predict human perception via sensitivity of state-of-the-art subjective image
quality metrics. We use information theory and regression analysis to find a
combination of just two probability-related factors that achieves 0.8
correlation with subjective metrics. This probability-based sensitivity is
psychophysically validated by reproducing the basic trends of the Contrast
Sensitivity Function, its suprathreshold variation, and trends of the Weber-law
and masking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12214">Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific Prompt Tuning. (arXiv:2303.12214v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapse_S/0/1/0/all/0/1">Saarthak Kapse</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Ke Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1">Prateek Prasanna</a>, <a href="http://arxiv.org/find/cs/1/au:+Saltz_J/0/1/0/all/0/1">Joel Saltz</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1">Maria Vakalopoulou</a>, <a href="http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1">Dimitris Samaras</a></p>
<p>Whole slide image (WSI) classification is a critical task in computational
pathology, requiring the processing of gigapixel-sized images, which is
challenging for current deep-learning methods. Current state of the art methods
are based on multi-instance learning schemes (MIL), which usually rely on
pretrained features to represent the instances. Due to the lack of
task-specific annotated data, these features are either obtained from
well-established backbones on natural images, or, more recently from
self-supervised models pretrained on histopathology. However, both approaches
yield task-agnostic features, resulting in performance loss compared to the
appropriate task-related supervision, if available. In this paper, we show that
when task-specific annotations are limited, we can inject such supervision into
downstream task training, to reduce the gap between fully task-tuned and task
agnostic features. We propose Prompt-MIL, an MIL framework that integrates
prompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism,
where only a small fraction of parameters calibrates the pretrained features to
encode task-specific information, rather than the conventional full fine-tuning
approaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC,
and BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL
methods, achieving a relative improvement of 1.49%-4.03% in accuracy and
0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters.
Compared to conventional full fine-tuning approaches, we fine-tune less than
1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in
accuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45%
while training 21%-27% faster. Our code is available at
https://github.com/cvlab-stonybrook/PromptMIL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14655">GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation. (arXiv:2303.14655v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Ji Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1">Teng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kunyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1">Xinyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaozhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Bin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Lei Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Juanzi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Weidong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yu Xu</a></p>
<p>Despite the recent emergence of video captioning models, how to generate
vivid, fine-grained video descriptions based on the background knowledge (i.e.,
long and informative commentary about the domain-specific scenes with
appropriate reasoning) is still far from being solved, which however has great
applications such as automatic sports narrative. In this paper, we present
GOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k
knowledge triples for proposing a challenging new task setting as
Knowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental
adaption of existing methods to show the difficulty and potential directions
for solving this valuable and applicable task. Our data and code are available
at https://github.com/THU-KEG/goal.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.16887">Towards Understanding the Effect of Pretraining Label Granularity. (arXiv:2303.16887v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_G/0/1/0/all/0/1">Guan Zhe Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1">Yin Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuxman_A/0/1/0/all/0/1">Ariel Fuxman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1">Stanley H. Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_E/0/1/0/all/0/1">Enming Luo</a></p>
<p>In this paper, we study how the granularity of pretraining labels affects the
generalization of deep neural networks in image classification tasks. We focus
on the "fine-to-coarse" transfer learning setting, where the pretraining label
space is more fine-grained than that of the target problem. Empirically, we
show that pretraining on the leaf labels of ImageNet21k produces better
transfer results on ImageNet1k than pretraining on other coarser granularity
levels, which supports the common practice used in the community.
Theoretically, we explain the benefit of fine-grained pretraining by proving
that, for a data distribution satisfying certain hierarchy conditions, 1)
coarse-grained pretraining only allows a neural network to learn the "common"
or "easy-to-learn" features well, while 2) fine-grained pretraining helps the
network learn the "rarer" or "fine-grained" features in addition to the common
ones, thus improving its accuracy on hard downstream test samples in which
common features are missing or weak in strength. Furthermore, we perform
comprehensive experiments using the label hierarchies of iNaturalist 2021 and
observe that the following conditions, in addition to proper choice of label
granularity, enable the transfer to work well in practice: 1) the pretraining
dataset needs to have a meaningful label hierarchy, and 2) the pretraining and
target label functions need to align well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03752">V3Det: Vast Vocabulary Visual Detection Dataset. (arXiv:2304.03752v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1">Tao Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuhang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yujie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Conghui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a></p>
<p>Recent advances in detecting arbitrary objects in the real world are trained
and evaluated on object detection datasets with a relatively restricted
vocabulary. To facilitate the development of more general visual object
detection, we propose V3Det, a vast vocabulary visual detection dataset with
precisely annotated bounding boxes on massive images. V3Det has several
appealing properties: 1) Vast Vocabulary: It contains bounding boxes of objects
from 13,204 categories on real-world images, which is 10 times larger than the
existing large vocabulary object detection dataset, e.g., LVIS. 2) Hierarchical
Category Organization: The vast vocabulary of V3Det is organized by a
hierarchical category tree which annotates the inclusion relationship among
categories, encouraging the exploration of category relationships in vast and
open vocabulary object detection. 3) Rich Annotations: V3Det comprises
precisely annotated objects in 243k images and professional descriptions of
each category written by human experts and a powerful chatbot. By offering a
vast exploration space, V3Det enables extensive benchmarks on both vast and
open vocabulary object detection, leading to new observations, practices, and
insights for future research. It has the potential to serve as a cornerstone
dataset for developing more general visual perception systems. V3Det is
available at https://v3det.openxlab.org.cn/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14979">Assessment of the Reliablity of a Model&#x27;s Decision by Generalizing Attribution to the Wavelet Domain. (arXiv:2305.14979v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1">Gabriel Kasmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1">Laurent Dubus</a>, <a href="http://arxiv.org/find/cs/1/au:+Drenan_Y/0/1/0/all/0/1">Yves-Marie Saint Drenan</a>, <a href="http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1">Philippe Blanc</a></p>
<p>Neural networks have shown remarkable performance in computer vision, but
their deployment in numerous scientific and technical fields is challenging due
to their black-box nature. Scientists and practitioners need to evaluate the
reliability of a decision, i.e., to know simultaneously if a model relies on
the relevant features and whether these features are robust to image
corruptions. Existing attribution methods aim to provide human-understandable
explanations by highlighting important regions in the image domain, but fail to
fully characterize a decision process's reliability. To bridge this gap, we
introduce the Wavelet sCale Attribution Method (WCAM), a generalization of
attribution from the pixel domain to the space-scale domain using wavelet
transforms. Attribution in the wavelet domain reveals where {\it and} on what
scales the model focuses, thus enabling us to assess whether a decision is
reliable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15086">Unpaired Image-to-Image Translation via Neural Schr\&quot;odinger Bridge. (arXiv:2305.15086v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1">Beomsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1">Gihyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kwanyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jong Chul Ye</a></p>
<p>Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. Although
diffusion models have achieved remarkable progress in recent years, they have
limitations in the unpaired image-to-image translation tasks due to the
Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to
translate between two arbitrary distributions, have risen as an attractive
solution to this problem. However, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which
expresses SB problem as a sequence of adversarial learning problems. This
allows us to incorporate advanced discriminators and regularization to learn a
SB between unpaired data. We demonstrate that UNSB is scalable and successfully
solves various unpaired image-to-image translation tasks. Code:
\url{https://github.com/cyclomon/UNSB}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15357">Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1">Yiyang Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1">Huan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1">Wenhan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1">Jianlong Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiaying Liu</a></p>
<p>Diffusion models, as a kind of powerful generative model, have given
impressive results on image super-resolution (SR) tasks. However, due to the
randomness introduced in the reverse process of diffusion models, the
performances of diffusion-based SR models are fluctuating at every time of
sampling, especially for samplers with few resampled steps. This inherent
randomness of diffusion models results in ineffectiveness and instability,
making it challenging for users to guarantee the quality of SR results.
However, our work takes this randomness as an opportunity: fully analyzing and
leveraging it leads to the construction of an effective plug-and-play sampling
method that owns the potential to benefit a series of diffusion-based SR
methods. More in detail, we propose to steadily sample high-quality SR images
from pre-trained diffusion-based SR models by solving diffusion ordinary
differential equations (diffusion ODEs) with optimal boundary conditions (BCs)
and analyze the characteristics between the choices of BCs and their
corresponding SR results. Our analysis shows the route to obtain an
approximately optimal BC via an efficient exploration in the whole space. The
quality of SR results sampled by the proposed method with fewer steps
outperforms the quality of results sampled by current methods with randomness
from the same pre-trained diffusion-based SR model, which means that our
sampling method "boosts" current diffusion-based SR models without any
additional training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17455">CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers. (arXiv:2305.17455v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1">Dachuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1">Chaofan Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1">Anyi Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhendong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1">Chun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaqi Wang</a></p>
<p>Recent vision-language models have achieved tremendous progress far beyond
what we ever expected. However, their computational costs are also dramatically
growing with rapid development, especially for the large models. It makes model
acceleration exceedingly critical in a scenario of limited resources. Although
extensively studied for unimodal models, the acceleration for multimodal
models, especially the vision-language Transformers, is relatively
under-explored. To pursue more efficient and accessible vision-language
Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided
\textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal
acceleration framework for vision-language Transformers. This framework
adaptively combines tokens through real-time, cross-modal guidance, thereby
achieving substantial acceleration while keeping high performance.
\textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and
Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and
ensemble to exploit cross-modal information effectively, only introducing
cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph
Soft Matching}. In contrast to the existing bipartite soft matching approach,
\textit{CrossGET} introduces a complete-graph soft matching policy to achieve
more reliable token-matching results while maintaining parallelizability and
high efficiency. Extensive experiments are conducted on various vision-language
tasks, including image-text retrieval, visual reasoning, image captioning, and
visual question answering. Performance on both classic multimodal architectures
and emerging multimodal LLMs demonstrate the effectiveness and versatility of
the proposed \textit{CrossGET} framework. The code will be at
\url{https://github.com/sdc17/CrossGET}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.20062">Chatting Makes Perfect: Chat-based Image Retrieval. (arXiv:2305.20062v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1">Matan Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1">Rami Ben-Ari</a>, <a href="http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1">Nir Darshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1">Dani Lischinski</a></p>
<p>Chats emerge as an effective user-friendly approach for information
retrieval, and are successfully employed in many domains, such as customer
service, healthcare, and finance. However, existing image retrieval approaches
typically address the case of a single query-to-image round, and the use of
chats for image retrieval has been mostly overlooked. In this work, we
introduce ChatIR: a chat-based image retrieval system that engages in a
conversation with the user to elicit information, in addition to an initial
query, in order to clarify the user's search intent. Motivated by the
capabilities of today's foundation models, we leverage Large Language Models to
generate follow-up questions to an initial image description. These questions
form a dialog with the user in order to retrieve the desired image from a large
corpus. In this study, we explore the capabilities of such a system tested on a
large dataset and reveal that engaging in a dialog yields significant gains in
image retrieval. We start by building an evaluation pipeline from an existing
manually generated dataset and explore different modules and training
strategies for ChatIR. Our comparison includes strong baselines derived from
related applications trained with Reinforcement Learning. Our system is capable
of retrieving the target image from a pool of 50K images with over 78% success
rate after 5 dialogue rounds, compared to 75% when questions are asked by
humans, and 64% for a single shot text-to-image retrieval. Extensive
evaluations reveal the strong capabilities and examine the limitations of
CharIR under different settings. Project repository is available at
https://github.com/levymsn/ChatIR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00966">The Hidden Language of Diffusion Models. (arXiv:2306.00966v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1">Hila Chefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_O/0/1/0/all/0/1">Oran Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1">Mor Geva</a>, <a href="http://arxiv.org/find/cs/1/au:+Polosukhin_V/0/1/0/all/0/1">Volodymyr Polosukhin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shocher_A/0/1/0/all/0/1">Assaf Shocher</a>, <a href="http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1">Michal Irani</a>, <a href="http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1">Inbar Mosseri</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lior Wolf</a></p>
<p>Text-to-image diffusion models have demonstrated an unparalleled ability to
generate high-quality, diverse images from a textual prompt. However, the
internal representations learned by these models remain an enigma. In this
work, we present Conceptor, a novel method to interpret the internal
representation of a textual concept by a diffusion model. This interpretation
is obtained by decomposing the concept into a small set of human-interpretable
textual elements. Applied over the state-of-the-art Stable Diffusion model,
Conceptor reveals non-trivial structures in the representations of concepts.
For example, we find surprising visual connections between concepts, that
transcend their textual semantics. We additionally discover concepts that rely
on mixtures of exemplars, biases, renowned artistic styles, or a simultaneous
fusion of multiple meanings of the concept. Through a large battery of
experiments, we demonstrate Conceptor's ability to provide meaningful, robust,
and faithful decompositions for a wide variety of abstract, concrete, and
complex textual concepts, while allowing to naturally connect each
decomposition element to its corresponding visual impact on the generated
images. Our code will be available at: https://hila-chefer.github.io/Conceptor/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01879">Revisiting the Role of Language Priors in Vision-Language Models. (arXiv:2306.01879v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhiqiu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinyue Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1">Deepak Pathak</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengchuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1">Deva Ramanan</a></p>
<p>Vision-language models (VLMs) are impactful in part because they can be
applied to a variety of visual understanding tasks in a zero-shot fashion,
without any fine-tuning. We study $\textit{generative VLMs}$ that are trained
for next-word generation given an image. We explore their zero-shot performance
on the illustrative task of image-text retrieval across 8 popular
vision-language benchmarks. Our first observation is that they can be
repurposed for discriminative tasks (such as image-text retrieval) by simply
computing the match score of generating a particular text string given an
image. We call this probabilistic score the $\textit{Visual Generative
Pre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces
near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on
others. We analyze this behavior through a probabilistic lens, pointing out
that some benchmarks inadvertently capture unnatural language distributions by
creating adversarial but unlikely text captions. In fact, we demonstrate that
even a "blind" language model that ignores any image evidence can sometimes
outperform all prior art, reminiscent of similar challenges faced by the
visual-question answering (VQA) community many years ago. We derive a
probabilistic post-processing scheme that controls for the amount of linguistic
bias in generative VLMs at test time without having to retrain or fine-tune the
model. We show that the VisualGPTScore, when appropriately debiased, is a
strong zero-shot baseline for vision-language understanding, oftentimes
producing state-of-the-art accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03364">Learning Representations on the Unit Sphere: Investigating Angular Gaussian and von Mises-Fisher Distributions for Online Continual Learning. (arXiv:2306.03364v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Michel_N/0/1/0/all/0/1">Nicolas Michel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chierchia_G/0/1/0/all/0/1">Giovanni Chierchia</a>, <a href="http://arxiv.org/find/cs/1/au:+Negrel_R/0/1/0/all/0/1">Romain Negrel</a>, <a href="http://arxiv.org/find/cs/1/au:+Bercher_J/0/1/0/all/0/1">Jean-Fran&#xe7;ois Bercher</a></p>
<p>We use the maximum a posteriori estimation principle for learning
representations distributed on the unit sphere. We propose to use the angular
Gaussian distribution, which corresponds to a Gaussian projected on the
unit-sphere and derive the associated loss function. We also consider the von
Mises-Fisher distribution, which is the conditional of a Gaussian in the
unit-sphere. The learned representations are pushed toward fixed directions,
which are the prior means of the Gaussians; allowing for a learning strategy
that is resilient to data drift. This makes it suitable for online continual
learning, which is the problem of training neural networks on a continuous data
stream, where multiple classification tasks are presented sequentially so that
data from past tasks are no longer accessible, and data from the current task
can be seen only once. To address this challenging scenario, we propose a
memory-based representation learning technique equipped with our new loss
functions. Our approach does not require negative data or knowledge of task
boundaries and performs well with smaller batch sizes while being
computationally efficient. We demonstrate with extensive experiments that the
proposed method outperforms the current state-of-the-art methods on both
standard evaluation scenarios and realistic scenarios with blurry task
boundaries. For reproducibility, we use the same training pipeline for every
compared method and share the code at https://t.ly/SQTj.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10840">RedMotion: Motion Prediction via Redundancy Reduction. (arXiv:2306.10840v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wagner_R/0/1/0/all/0/1">Royden Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Tas_O/0/1/0/all/0/1">Omer Sahin Tas</a>, <a href="http://arxiv.org/find/cs/1/au:+Klemp_M/0/1/0/all/0/1">Marvin Klemp</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1">Carlos Fernandez Lopez</a></p>
<p>Predicting the future motion of traffic agents is vital for self-driving
vehicles to ensure their safe operation. We introduce RedMotion, a transformer
model for motion prediction that incorporates two types of redundancy
reduction. The first type of redundancy reduction is induced by an internal
transformer decoder and reduces a variable-sized set of road environment
tokens, such as road graphs with agent data, to a fixed-sized embedding. The
second type of redundancy reduction is a self-supervised learning objective and
applies the redundancy reduction principle to embeddings generated from
augmented views of road environments. Our experiments reveal that our
representation learning approach can outperform PreTraM, Traj-MAE, and
GraphDINO in a semi-supervised setting. Our RedMotion model achieves results
that are competitive with those of Scene Transformer or MTR++. We provide an
open source implementation that is accessible via GitHub
(https://github.com/kit-mrt/red-motion) and Colab
(https://colab.research.google.com/drive/1Q-Z9VdiqvfPfctNG8oqzPcgm0lP3y1il).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13933">Boost Video Frame Interpolation via Motion Adaptation. (arXiv:2306.13933v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoning Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weidi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Ya Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanfeng Wang</a></p>
<p>Video frame interpolation (VFI) is a challenging task that aims to generate
intermediate frames between two consecutive frames in a video. Existing
learning-based VFI methods have achieved great success, but they still suffer
from limited generalization ability due to the limited motion distribution of
training datasets. In this paper, we propose a novel optimization-based VFI
method that can adapt to unseen motions at test time. Our method is based on a
cycle-consistency adaptation strategy that leverages the motion characteristics
among video frames. We also introduce a lightweight adapter that can be
inserted into the motion estimation module of existing pre-trained VFI models
to improve the efficiency of adaptation. Extensive experiments on various
benchmarks demonstrate that our method can boost the performance of two-frame
VFI models, outperforming the existing state-of-the-art methods, even those
that use extra input.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14840">Building Flyweight FLIM-based CNNs with Adaptive Decoding for Object Detection. (arXiv:2306.14840v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Joao_L/0/1/0/all/0/1">Leonardo de Melo Joao</a>, <a href="http://arxiv.org/find/eess/1/au:+Sousa_A/0/1/0/all/0/1">Azael de Melo e Sousa</a>, <a href="http://arxiv.org/find/eess/1/au:+Santos_B/0/1/0/all/0/1">Bianca Martins dos Santos</a>, <a href="http://arxiv.org/find/eess/1/au:+Guimaraes_S/0/1/0/all/0/1">Silvio Jamil Ferzoli Guimaraes</a>, <a href="http://arxiv.org/find/eess/1/au:+Gomes_J/0/1/0/all/0/1">Jancarlo Ferreira Gomes</a>, <a href="http://arxiv.org/find/eess/1/au:+Kijak_E/0/1/0/all/0/1">Ewa Kijak</a>, <a href="http://arxiv.org/find/eess/1/au:+Falcao_A/0/1/0/all/0/1">Alexandre Xavier Falcao</a></p>
<p>State-of-the-art (SOTA) object detection methods have succeeded in several
applications at the price of relying on heavyweight neural networks, which
makes them inefficient and inviable for many applications with computational
resource constraints. This work presents a method to build a Convolutional
Neural Network (CNN) layer by layer for object detection from user-drawn
markers on discriminative regions of representative images. We address the
detection of Schistosomiasis mansoni eggs in microscopy images of fecal
samples, and the detection of ships in satellite images as application
examples. We could create a flyweight CNN without backpropagation from very few
input images. Our method explores a recent methodology, Feature Learning from
Image Markers (FLIM), to build convolutional feature extractors (encoders) from
marker pixels. We extend FLIM to include a single-layer adaptive decoder, whose
weights vary with the input image -- a concept never explored in CNNs. Our CNN
weighs thousands of times less than SOTA object detectors, being suitable for
CPU execution and showing superior or equivalent performance to three methods
in five measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10784">SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with 4D Imaging Radar. (arXiv:2307.10784v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jianan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qiuchi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1">Weiyi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Tao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1">Qing-Long Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Bing Zhu</a></p>
<p>The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle
sensing due to its cost-effectiveness and operability in adverse weather
conditions. However, the adoption of this technology has been hindered by
sparsity and noise issues in radar point cloud data. This paper introduces
spatial multi-representation fusion (SMURF), a novel approach to 3D object
detection using a single 4D imaging radar. SMURF leverages multiple
representations of radar detection points, including pillarization and density
features of a multi-dimensional Gaussian mixture distribution through kernel
density estimation (KDE). KDE effectively mitigates measurement inaccuracy
caused by limited angular resolution and multi-path propagation of radar
signals. Additionally, KDE helps alleviate point cloud sparsity by capturing
density features. Experimental evaluations on View-of-Delft (VoD) and
TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of
SMURF, outperforming recently proposed 4D imaging radar-based
single-representation models. Moreover, while using 4D imaging radar only,
SMURF still achieves comparable performance to the state-of-the-art 4D imaging
radar and camera fusion-based method, with an increase of 1.22% in the mean
average precision on bird's-eye view of TJ4DRadSet dataset and 1.32% in the 3D
mean average precision on the entire annotated area of VoD dataset. Our
proposed method demonstrates impressive inference time and addresses the
challenges of real-time detection, with the inference time no more than 0.05
seconds for most scans on both datasets. This research highlights the benefits
of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D
object detection with 4D imaging radar.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11932">RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction. (arXiv:2307.11932v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kasahara_I/0/1/0/all/0/1">Isaac Kasahara</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Shubham Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Engin_S/0/1/0/all/0/1">Selim Engin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chavan_Dafle_N/0/1/0/all/0/1">Nikhil Chavan-Dafle</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shuran Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Isler_V/0/1/0/all/0/1">Volkan Isler</a></p>
<p>General scene reconstruction refers to the task of estimating the full 3D
geometry and texture of a scene containing previously unseen objects. In many
practical applications such as AR/VR, autonomous navigation, and robotics, only
a single view of the scene may be available, making the scene reconstruction
task challenging. In this paper, we present a method for scene reconstruction
by structurally breaking the problem into two steps: rendering novel views via
inpainting and 2D to 3D scene lifting. Specifically, we leverage the
generalization capability of large visual language models (Dalle-2) to inpaint
the missing areas of scene color images rendered from different views. Next, we
lift these inpainted images to 3D by predicting normals of the inpainted image
and solving for the missing depth values. By predicting for normals instead of
depth directly, our method allows for robustness to changes in depth
distributions and scale. With rigorous quantitative evaluation, we show that
our method outperforms multiple baselines while providing generalization to
novel objects and scenes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08303">Leveraging Next-Active Objects for Context-Aware Anticipation in Egocentric Videos. (arXiv:2308.08303v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thakur_S/0/1/0/all/0/1">Sanket Thakur</a>, <a href="http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1">Cigdem Beyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Morerio_P/0/1/0/all/0/1">Pietro Morerio</a>, <a href="http://arxiv.org/find/cs/1/au:+Murino_V/0/1/0/all/0/1">Vittorio Murino</a>, <a href="http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1">Alessio Del Bue</a></p>
<p>Objects are crucial for understanding human-object interactions. By
identifying the relevant objects, one can also predict potential future
interactions or actions that may occur with these objects. In this paper, we
study the problem of Short-Term Object interaction anticipation (STA) and
propose NAOGAT (Next-Active-Object Guided Anticipation Transformer), a
multi-modal end-to-end transformer network, that attends to objects in observed
frames in order to anticipate the next-active-object (NAO) and, eventually, to
guide the model to predict context-aware future actions. The task is
challenging since it requires anticipating future action along with the object
with which the action occurs and the time after which the interaction will
begin, a.k.a. the time to contact (TTC). Compared to existing video modeling
architectures for action anticipation, NAOGAT captures the relationship between
objects and the global scene context in order to predict detections for the
next active object and anticipate relevant future actions given these
detections, leveraging the objects' dynamics to improve accuracy. One of the
key strengths of our approach, in fact, is its ability to exploit the motion
dynamics of objects within a given clip, which is often ignored by other
models, and separately decoding the object-centric and motion-centric
information. Through our experiments, we show that our model outperforms
existing methods on two separate datasets, Ego4D and EpicKitchens-100 ("Unseen
Set"), as measured by several additional metrics, such as time to contact, and
next-active-object localization. The code will be available upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12439">BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tinghao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiangyu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1">Ping He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a></p>
<p>We present a novel defense, against backdoor attacks on Deep Neural Networks
(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)
into DNNs. Our defense falls within the category of post-development defenses
that operate independently of how the model was generated. The proposed defense
is built upon a novel reverse engineering approach that can directly extract
backdoor functionality of a given backdoored model to a backdoor expert model.
The approach is straightforward -- finetuning the backdoored model over a small
set of intentionally mislabeled clean samples, such that it unlearns the normal
functionality while still preserving the backdoor functionality, and thus
resulting in a model (dubbed a backdoor expert model) that can only recognize
backdoor inputs. Based on the extracted backdoor expert model, we show the
feasibility of devising highly accurate backdoor input detectors that filter
out the backdoor inputs during model inference. Further augmented by an
ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert
(Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA
backdoor attacks while minimally impacting clean utility. The effectiveness of
BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)
across various model architectures (ResNet, VGG, MobileNetV2 and Vision
Transformer).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12960">Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment. (arXiv:2308.12960v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1">Muzammal Naseer</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guangyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhiqiang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Khan</a></p>
<p>Large-scale pre-trained Vision Language Models (VLMs) have proven effective
for zero-shot classification. Despite the success, most traditional VLMs-based
methods are restricted by the assumption of partial source supervision or ideal
vocabularies, which rarely satisfy the open-world scenario. In this paper, we
aim at a more challenging setting, Realistic Zero-Shot Classification, which
assumes no annotation but instead a broad vocabulary. To address this
challenge, we propose the Self Structural Semantic Alignment (S^3A) framework,
which extracts the structural semantic information from unlabeled data while
simultaneously self-learning. Our S^3A framework adopts a unique
Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups
unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR
process includes iterative clustering on images, voting within each cluster to
identify initial class candidates from the vocabulary, generating
discriminative prompts with large language models to discern confusing
candidates, and realigning images and the vocabulary as structural semantic
alignment. Finally, we propose to self-learn the CLIP image encoder with both
individual and structural semantic alignment through a teacher-student learning
strategy. Our comprehensive experiments across various generic and fine-grained
benchmarks demonstrate that the S^3A method offers substantial improvements
over existing VLMs-based approaches, achieving a more than 15% accuracy
improvement over CLIP on average. Our codes, models, and prompts are publicly
released at https://github.com/sheng-eatamath/S3A.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16150">Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liang_Z/0/1/0/all/0/1">Ziyun Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Anthony_H/0/1/0/all/0/1">Harry Anthony</a>, <a href="http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1">Felix Wagner</a>, <a href="http://arxiv.org/find/eess/1/au:+Kamnitsas_K/0/1/0/all/0/1">Konstantinos Kamnitsas</a></p>
<p>Unsupervised anomaly segmentation aims to detect patterns that are distinct
from any patterns processed during training, commonly called abnormal or
out-of-distribution patterns, without providing any associated manual
segmentations. Since anomalies during deployment can lead to model failure,
detecting the anomaly can enhance the reliability of models, which is valuable
in high-risk domains like medical imaging. This paper introduces Masked
Modality Cycles with Conditional Diffusion (MMCCD), a method that enables
segmentation of anomalies across diverse patterns in multimodal MRI. The method
is based on two fundamental ideas. First, we propose the use of cyclic modality
translation as a mechanism for enabling abnormality detection.
Image-translation models learn tissue-specific modality mappings, which are
characteristic of tissue physiology. Thus, these learned mappings fail to
translate tissues or image patterns that have never been encountered during
training, and the error enables their segmentation. Furthermore, we combine
image translation with a masked conditional diffusion model, which attempts to
`imagine' what tissue exists under a masked area, further exposing unknown
patterns as the generative model fails to recreate them. We evaluate our method
on a proxy task by training on healthy-looking slices of BraTS2021
multi-modality MRIs and testing on slices with tumors. We show that our method
compares favorably to previous unsupervised approaches based on image
reconstruction and denoising with autoencoders and diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.16738">SFUSNet: A Spatial-Frequency domain-based Multi-branch Network for diagnosis of Cervical Lymph Node Lesions in Ultrasound Images. (arXiv:2308.16738v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yue_Y/0/1/0/all/0/1">Yubiao Yue</a>, <a href="http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1">Jun Xue</a>, <a href="http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1">Haihua Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_B/0/1/0/all/0/1">Bingchun Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Zhenzhang Li</a></p>
<p>Booming deep learning has substantially improved the diagnosis for diverse
lesions in ultrasound images, but a conspicuous research gap concerning
cervical lymph node lesions still remains. The objective of this work is to
diagnose cervical lymph node lesions in ultrasound images by leveraging a deep
learning model. To this end, we first collected 3392 cervical ultrasound images
containing normal lymph nodes, benign lymph node lesions, malignant primary
lymph node lesions, and malignant metastatic lymph node lesions. Given that
ultrasound images are generated by the reflection and scattering of sound waves
across varied bodily tissues, we proposed the Conv-FFT Block. It integrates
convolutional operations with the fast Fourier transform to more astutely model
the images. Building upon this foundation, we designed a novel architecture,
named SFUSNet. SFUSNet not only discerns variances in ultrasound images from
the spatial domain but also adeptly captures micro-structural alterations
across various lesions in the frequency domain. To ascertain the potential of
SFUSNet, we benchmarked it against 12 popular architectures through five-fold
cross-validation. The results show that SFUSNet is the state-of-the-art model
and can achieve 92.89% accuracy. Moreover, its average precision, average
sensitivity and average specificity for four types of lesions achieve 90.46%,
89.95% and 97.49%, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00616">OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation. (arXiv:2309.00616v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhening Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoyang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hengshuang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1">Joan Lasenby</a></p>
<p>Current 3D open-vocabulary scene understanding methods mostly utilize
well-aligned 2D images as the bridge to learn 3D features with language.
However, applying these approaches becomes challenging in scenarios where 2D
images are absent. In this work, we introduce a new pipeline, namely,
OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary scene
understanding at the instance level. The OpenIns3D framework employs a
"Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic mask
proposals in 3D point clouds. The "Snap" module generates synthetic scene-level
images at multiple scales and leverages 2D vision language models to extract
interesting objects. The "Lookup" module searches through the outcomes of
"Snap" with the help of Mask2Pixel maps, which contain the precise
correspondence between 3D masks and synthetic images, to assign category names
to the proposed masks. This 2D input-free and flexible approach achieves
state-of-the-art results on a wide range of indoor and outdoor datasets by a
large margin. Moreover, OpenIns3D allows for effortless switching of 2D
detectors without re-training. When integrated with powerful 2D open-world
models such as ODISE and GroundingDINO, excellent results were observed on
open-vocabulary instance segmentation. When integrated with LLM-powered 2D
models like LISA, it demonstrates a remarkable capacity to process highly
complex text queries which require intricate reasoning and world knowledge.
Project page: https://zheninghuang.github.io/OpenIns3D/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06262">Modality Unifying Network for Visible-Infrared Person Re-Identification. (arXiv:2309.06262v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weihao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Guoying Zhao</a></p>
<p>Visible-infrared person re-identification (VI-ReID) is a challenging task due
to large cross-modality discrepancies and intra-class variations. Existing
methods mainly focus on learning modality-shared representations by embedding
different modalities into the same feature space. As a result, the learned
feature emphasizes the common patterns across modalities while suppressing
modality-specific and identity-aware information that is valuable for Re-ID. To
address these issues, we propose a novel Modality Unifying Network (MUN) to
explore a robust auxiliary modality for VI-ReID. First, the auxiliary modality
is generated by combining the proposed cross-modality learner and
intra-modality learner, which can dynamically model the modality-specific and
modality-shared representations to alleviate both cross-modality and
intra-modality variations. Second, by aligning identity centres across the
three modalities, an identity alignment loss function is proposed to discover
the discriminative feature representations. Third, a modality alignment loss is
introduced to consistently reduce the distribution distance of visible and
infrared images by modality prototype modeling. Extensive experiments on
multiple public datasets demonstrate that the proposed method surpasses the
current state-of-the-art methods by a significant margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09472">Reconstructing Existing Levels through Level Inpainting. (arXiv:2309.09472v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Johor Jara Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1">Matthew Guzdial</a></p>
<p>Procedural Content Generation (PCG) and Procedural Content Generation via
Machine Learning (PCGML) have been used in prior work for generating levels in
various games. This paper introduces Content Augmentation and focuses on the
subproblem of level inpainting, which involves reconstructing and extending
video game levels. Drawing inspiration from image inpainting, we adapt two
techniques from this domain to address our specific use case. We present two
approaches for level inpainting: an Autoencoder and a U-net. Through a
comprehensive case study, we demonstrate their superior performance compared to
a baseline method and discuss their relative merits. Furthermore, we provide a
practical demonstration of both approaches for the level inpainting task and
offer insights into potential directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11745">PIE: Simulating Disease Progression via Progressive Image Editing. (arXiv:2309.11745v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1">Xu Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_K/0/1/0/all/0/1">Kuei-Da Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1">Tianren Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Ye_W/0/1/0/all/0/1">Wenqian Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1">Zhengyu Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Cao_J/0/1/0/all/0/1">Jianguo Cao</a>, <a href="http://arxiv.org/find/eess/1/au:+Nama_T/0/1/0/all/0/1">Tejas Nama</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Disease progression simulation is a crucial area of research that has
significant implications for clinical diagnosis, prognosis, and treatment. One
major challenge in this field is the lack of continuous medical imaging
monitoring of individual patients over time. To address this issue, we develop
a novel framework termed Progressive Image Editing (PIE) that enables
controlled manipulation of disease-related image features, facilitating precise
and realistic disease progression simulation. Specifically, we leverage recent
advancements in text-to-image generative models to simulate disease progression
accurately and personalize it for each patient. We theoretically analyze the
iterative refining process in our framework as a gradient descent with an
exponentially decayed learning rate. To validate our framework, we conduct
experiments in three medical imaging domains. Our results demonstrate the
superiority of PIE over existing methods such as Stable Diffusion Walk and
Style-Based Manifold Extrapolation based on CLIP score (Realism) and Disease
Classification Confidence (Alignment). Our user study collected feedback from
35 veteran physicians to assess the generated progressions. Remarkably, 76.2%
of the feedback agrees with the fidelity of the generated progressions. To our
best knowledge, PIE is the first of its kind to generate disease progression
images meeting real-world standards. It is a promising tool for medical
research and clinical practice, potentially allowing healthcare providers to
model disease trajectories over time, predict future treatment responses, and
improve patient outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13777">Diffeomorphic Multi-Resolution Deep Learning Registration for Applications in Breast MRI. (arXiv:2309.13777v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+French_M/0/1/0/all/0/1">Matthew G. French</a>, <a href="http://arxiv.org/find/eess/1/au:+Talou_G/0/1/0/all/0/1">Gonzalo D. Maso Talou</a>, <a href="http://arxiv.org/find/eess/1/au:+Gamage_T/0/1/0/all/0/1">Thiranja P. Babarenda Gamage</a>, <a href="http://arxiv.org/find/eess/1/au:+Nash_M/0/1/0/all/0/1">Martyn P. Nash</a>, <a href="http://arxiv.org/find/eess/1/au:+Nielsen_P/0/1/0/all/0/1">Poul M. Nielsen</a>, <a href="http://arxiv.org/find/eess/1/au:+Doyle_A/0/1/0/all/0/1">Anthony J. Doyle</a>, <a href="http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1">Juan Eugenio Iglesias</a>, <a href="http://arxiv.org/find/eess/1/au:+Balbastre_Y/0/1/0/all/0/1">Ya&#xeb;l Balbastre</a>, <a href="http://arxiv.org/find/eess/1/au:+Young_S/0/1/0/all/0/1">Sean I. Young</a></p>
<p>In breast surgical planning, accurate registration of MR images across
patient positions has the potential to improve the localisation of tumours
during breast cancer treatment. While learning-based registration methods have
recently become the state-of-the-art approach for most medical image
registration tasks, these methods have yet to make inroads into breast image
registration due to certain difficulties-the lack of rich texture information
in breast MR images and the need for the deformations to be diffeomophic. In
this work, we propose learning strategies for breast MR image registration that
are amenable to diffeomorphic constraints, together with early experimental
results from in-silico and in-vivo experiments. One key contribution of this
work is a registration network which produces superior registration outcomes
for breast images in addition to providing diffeomorphic guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15523">Improving Facade Parsing with Vision Transformers and Line Integration. (arXiv:2309.15523v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ran Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunqin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liangzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1">Yuta Nakashima</a></p>
<p>Facade parsing stands as a pivotal computer vision task with far-reaching
applications in areas like architecture, urban planning, and energy efficiency.
Despite the recent success of deep learning-based methods in yielding
impressive results on certain open-source datasets, their viability for
real-world applications remains uncertain. Real-world scenarios are
considerably more intricate, demanding greater computational efficiency.
Existing datasets often fall short in representing these settings, and previous
methods frequently rely on extra models to enhance accuracy, which requires
much computation cost. In this paper, we introduce Comprehensive Facade Parsing
(CFP), a dataset meticulously designed to encompass the intricacies of
real-world facade parsing tasks. Comprising a total of 602 high-resolution
street-view images, this dataset captures a diverse array of challenging
scenarios, including sloping angles and densely clustered buildings, with
painstakingly curated annotations for each image. We introduce a new pipeline
known as Revision-based Transformer Facade Parsing (RTFP). This marks the
pioneering utilization of Vision Transformers (ViT) in facade parsing, and our
experimental results definitively substantiate its merit. We also design Line
Acquisition, Filtering, and Revision (LAFR), an efficient yet accurate revision
algorithm that can improve the segment result solely from simple line detection
using prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP,
we evaluate the superiority of our method. The dataset and code are available
at https://github.com/wbw520/RTFP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16206">Alzheimer&#x27;s Disease Prediction via Brain Structural-Functional Deep Fusing Network. (arXiv:2309.16206v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zuo_Q/0/1/0/all/0/1">Qiankun Zuo</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1">Junren Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Shuqiang Wang</a></p>
<p>Fusing structural-functional images of the brain has shown great potential to
analyze the deterioration of Alzheimer's disease (AD). However, it is a big
challenge to effectively fuse the correlated and complementary information from
multimodal neuroimages. In this paper, a novel model termed cross-modal
transformer generative adversarial network (CT-GAN) is proposed to effectively
fuse the functional and structural information contained in functional magnetic
resonance imaging (fMRI) and diffusion tensor imaging (DTI). The CT-GAN can
learn topological features and generate multimodal connectivity from multimodal
imaging data in an efficient end-to-end manner. Moreover, the swapping
bi-attention mechanism is designed to gradually align common features and
effectively enhance the complementary features between modalities. By analyzing
the generated connectivity features, the proposed model can identify AD-related
brain connections. Evaluations on the public ADNI dataset show that the
proposed CT-GAN can dramatically improve prediction performance and detect
AD-related brain regions effectively. The proposed model also provides new
insights for detecting AD-related abnormal neural circuits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.17329">Efficient Anatomical Labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kangxian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiancheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1">Donglai Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1">Ziqiao Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1">Pascal Fua</a></p>
<p>Pulmonary diseases rank prominently among the principal causes of death
worldwide. Curing them will require, among other things, a better understanding
of the many complex 3D tree-shaped structures within the pulmonary system, such
as airways, arteries, and veins. In theory, they can be modeled using
high-resolution image stacks. Unfortunately, standard CNN approaches operating
on dense voxel grids are prohibitively expensive. To remedy this, we introduce
a point-based approach that preserves graph connectivity of tree skeleton and
incorporates an implicit surface representation. It delivers SOTA accuracy at a
low computational cost and the resulting models have usable surfaces. Due to
the scarcity of publicly accessible data, we have also curated an extensive
dataset to evaluate our approach and will make it public.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00031">Text-image Alignment for Diffusion-based Perception. (arXiv:2310.00031v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kondapaneni_N/0/1/0/all/0/1">Neehar Kondapaneni</a>, <a href="http://arxiv.org/find/cs/1/au:+Marks_M/0/1/0/all/0/1">Markus Marks</a>, <a href="http://arxiv.org/find/cs/1/au:+Knott_M/0/1/0/all/0/1">Manuel Knott</a>, <a href="http://arxiv.org/find/cs/1/au:+Guimaraes_R/0/1/0/all/0/1">Rog&#xe9;rio Guimar&#xe3;es</a>, <a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1">Pietro Perona</a></p>
<p>Diffusion models are generative models with impressive text-to-image
synthesis capabilities and have spurred a new wave of creative methods for
classical machine learning tasks. However, the best way to harness the
perceptual knowledge of these generative models for visual tasks is still an
open question. Specifically, it is unclear how to use the prompting interface
when applying diffusion backbones to vision tasks. We find that automatically
generated captions can improve text-image alignment and significantly enhance a
model's cross-attention maps, leading to better perceptual performance. Our
approach improves upon the current SOTA in diffusion-based semantic
segmentation on ADE20K and the current overall SOTA in depth estimation on
NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use
model personalization and caption modifications to align our model to the
target domain and find improvements over unaligned baselines. Our object
detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.
Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark
Zurich-val and Nighttime Driving. Project page:
https://www.vision.caltech.edu/tadp/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00944">Towards Robust 3D Object Detection In Rainy Conditions. (arXiv:2310.00944v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piroli_A/0/1/0/all/0/1">Aldi Piroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1">Vinzenz Dallabetta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kopp_J/0/1/0/all/0/1">Johannes Kopp</a>, <a href="http://arxiv.org/find/cs/1/au:+Walessa_M/0/1/0/all/0/1">Marc Walessa</a>, <a href="http://arxiv.org/find/cs/1/au:+Meissner_D/0/1/0/all/0/1">Daniel Meissner</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1">Klaus Dietmayer</a></p>
<p>LiDAR sensors are used in autonomous driving applications to accurately
perceive the environment. However, they are affected by adverse weather
conditions such as snow, fog, and rain. These everyday phenomena introduce
unwanted noise into the measurements, severely degrading the performance of
LiDAR-based perception systems. In this work, we propose a framework for
improving the robustness of LiDAR-based 3D object detectors against road spray.
Our approach uses a state-of-the-art adverse weather detection network to
filter out spray from the LiDAR point cloud, which is then used as input for
the object detector. In this way, the detected objects are less affected by the
adverse weather in the scene, resulting in a more accurate perception of the
environment. In addition to adverse weather filtering, we explore the use of
radar targets to further filter false positive detections. Tests on real-world
data show that our approach improves the robustness to road spray of several
popular 3D object detectors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01662">SYRAC: Synthesize, Rank, and Count. (arXiv:2310.01662v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DAlessandro_A/0/1/0/all/0/1">Adriano D&#x27;Alessandro</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1">Ali Mahdavi-Amiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1">Ghassan Hamarneh</a></p>
<p>Crowd counting is a critical task in computer vision, with several important
applications. However, existing counting methods rely on labor-intensive
density map annotations, necessitating the manual localization of each
individual pedestrian. While recent efforts have attempted to alleviate the
annotation burden through weakly or semi-supervised learning, these approaches
fall short of significantly reducing the workload. We propose a novel approach
to eliminate the annotation burden by leveraging latent diffusion models to
generate synthetic data. However, these models struggle to reliably understand
object quantities, leading to noisy annotations when prompted to produce images
with a specific quantity of objects. To address this, we use latent diffusion
models to create two types of synthetic data: one by removing pedestrians from
real images, which generates ranked image pairs with a weak but reliable object
quantity signal, and the other by generating synthetic images with a
predetermined number of objects, offering a strong but noisy counting signal.
Our method utilizes the ranking image pairs for pre-training and then fits a
linear layer to the noisy synthetic images using these crowd quantity features.
We report state-of-the-art results for unsupervised crowd counting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02601">MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiyuan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lanqing Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1">Dit-Yan Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Qiang Xu</a></p>
<p>Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird's-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02676">PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting. (arXiv:2310.02676v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yujin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiaming Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xiang Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zeying Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Junwei Liang</a></p>
<p>Accurate precipitation forecasting is a vital challenge of both scientific
and societal importance. Data-driven approaches have emerged as a widely used
solution for addressing this challenge. However, solely relying on data-driven
approaches has limitations in modeling the underlying physics, making accurate
predictions difficult. Coupling AI-based post-processing techniques with
traditional Numerical Weather Prediction (NWP) methods offers a more effective
solution for improving forecasting accuracy. Despite previous post-processing
efforts, accurately predicting heavy rainfall remains challenging due to the
imbalanced precipitation data across locations and complex relationships
between multiple meteorological variables. To address these limitations, we
introduce the PostRainBench, a comprehensive multi-variable NWP post-processing
benchmark consisting of three datasets for NWP post-processing-based
precipitation forecasting. We propose CAMT, a simple yet effective Channel
Attention Enhanced Multi-task Learning framework with a specially designed
weighted loss function. Its flexible design allows for easy plug-and-play
integration with various backbones. Extensive experimental results on the
proposed benchmark show that our method outperforms state-of-the-art methods by
6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most
notably, our model is the first deep learning-based method to outperform
traditional Numerical Weather Prediction (NWP) approaches in extreme
precipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over
NWP predictions in heavy rain CSI on respective datasets. These results
highlight the potential impact of our model in reducing the severe consequences
of extreme weather events.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03006">COOLer: Class-Incremental Learning for Appearance-Based Multiple Object Tracking. (arXiv:2310.03006v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhizheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1">Mattia Segu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fisher Yu</a></p>
<p>Continual learning allows a model to learn multiple tasks sequentially while
retaining the old knowledge without the training data of the preceding tasks.
This paper extends the scope of continual learning research to
class-incremental learning for multiple object tracking (MOT), which is
desirable to accommodate the continuously evolving needs of autonomous systems.
Previous solutions for continual learning of object detectors do not address
the data association stage of appearance-based trackers, leading to
catastrophic forgetting of previous classes' re-identification features. We
introduce COOLer, a COntrastive- and cOntinual-Learning-based tracker, which
incrementally learns to track new categories while preserving past knowledge by
training on a combination of currently available ground truth labels and
pseudo-labels generated by the past tracker. To further exacerbate the
disentanglement of instance representations, we introduce a novel contrastive
class-incremental instance representation learning technique. Finally, we
propose a practical evaluation protocol for continual learning for MOT and
conduct experiments on the BDD100K and SHIFT datasets. Experimental results
demonstrate that COOLer continually learns while effectively addressing
catastrophic forgetting of both tracking and detection. The code is available
at https://github.com/BoSmallEar/COOLer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15724">REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v3 [cs.RO] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zeyi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bahety_A/0/1/0/all/0/1">Arpit Bahety</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shuran Song</a></p>
<p>The ability to detect and analyze failed executions automatically is crucial
for an explainable and robust robotic system. Recently, Large Language Models
(LLMs) have demonstrated strong reasoning abilities on textual inputs. To
leverage the power of LLMs for robot failure explanation, we introduce REFLECT,
a framework which queries LLM for failure reasoning based on a hierarchical
summary of robot past experiences generated from multisensory observations. The
failure explanation can further guide a language-based planner to correct the
failure and complete the task. To systematically evaluate the framework, we
create the RoboFail dataset with a variety of tasks and failure scenarios. We
demonstrate that the LLM-based framework is able to generate informative
failure explanations that assist successful correction planning.
</p>
</p>
</div>

    </div>
    </body>
    