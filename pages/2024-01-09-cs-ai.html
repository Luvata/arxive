<!DOCTYPE html>
<html>
<head>
<title>2024-01-09-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2401.02421">Neuronal Auditory Machine Intelligence (NEURO-AMI) In Perspective. (arXiv:2401.02421v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Osegi_E/0/1/0/all/0/1">Emmanuel Ndidi Osegi</a></p>
<p>The recent developments in soft computing cannot be complete without noting
the contributions of artificial neural machine learning systems that draw
inspiration from real cortical tissue or processes that occur in human brain.
The universal approximability of such neural systems has led to its wide spread
use, and novel developments in this evolving technology has shown that there is
a bright future for such Artificial Intelligent (AI) techniques in the soft
computing field. Indeed, the proliferation of large and very deep networks of
artificial neural systems and the corresponding enhancement and development of
neural machine learning algorithms have contributed immensely to the
development of the modern field of Deep Learning as may be found in the well
documented research works of Lecun, Bengio and Hinton. However, the key
requirements of end user affordability in addition to reduced complexity and
reduced data learning size requirement means there still remains a need for the
synthesis of more cost-efficient and less data-hungry artificial neural
systems. In this report, we present an overview of a new competing bio-inspired
continual learning neural tool Neuronal Auditory Machine Intelligence
(Neuro-AMI) as a predictor detailing its functional and structural details,
important aspects on right applicability, some recent application use cases and
future research directions for current and prospective machine learning experts
and data scientists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02424">Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning. (arXiv:2401.02424v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kunwar_S/0/1/0/all/0/1">Suman Kunwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferdush_J/0/1/0/all/0/1">Jannatul Ferdush</a></p>
<p>As the global population continues to expand, the demand for natural
resources increases. Unfortunately, human activities account for 23% of
greenhouse gas emissions. On a positive note, remote sensing technologies have
emerged as a valuable tool in managing our environment. These technologies
allow us to monitor land use, plan urban areas, and drive advancements in areas
such as agriculture, climate change mitigation, disaster recovery, and
environmental monitoring. Recent advances in AI, computer vision, and earth
observation data have enabled unprecedented accuracy in land use mapping. By
using transfer learning and fine-tuning with RGB bands, we achieved an
impressive 99.19% accuracy in land use analysis. Such findings can be used to
inform conservation and urban planning policies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02425">UAV Trajectory Planning for AoI-Minimal Data Collection in UAV-Aided IoT Networks by Transformer. (arXiv:2401.02425v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Botao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bedeer_E/0/1/0/all/0/1">Ebrahim Bedeer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Ha H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Barton_R/0/1/0/all/0/1">Robert Barton</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zhen Gao</a></p>
<p>Maintaining freshness of data collection in Internet-of-Things (IoT) networks
has attracted increasing attention. By taking into account age-of-information
(AoI), we investigate the trajectory planning problem of an unmanned aerial
vehicle (UAV) that is used to aid a cluster-based IoT network. An optimization
problem is formulated to minimize the total AoI of the collected data by the
UAV from the ground IoT network. Since the total AoI of the IoT network depends
on the flight time of the UAV and the data collection time at hovering points,
we jointly optimize the selection of hovering points and the visiting order to
these points. We exploit the state-of-the-art transformer and the weighted A*,
which is a path search algorithm, to design a machine learning algorithm to
solve the formulated problem. The whole UAV-IoT system is fed into the encoder
network of the proposed algorithm, and the algorithm's decoder network outputs
the visiting order to ground clusters. Then, the weighted A* is used to find
the hovering point for each cluster in the ground IoT network. Simulation
results show that the trained model by the proposed algorithm has a good
generalization ability to generate solutions for IoT networks with different
numbers of ground clusters, without the need to retrain the model. Furthermore,
results show that our proposed algorithm can find better UAV trajectories with
the minimum total AoI when compared to other algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02427">5G Positioning Advancements with AI/ML. (arXiv:2401.02427v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alawieh_M/0/1/0/all/0/1">Mohammad Alawieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kontes_G/0/1/0/all/0/1">Georgios Kontes</a></p>
<p>This paper provides a comprehensive review of AI/ML-based direct positioning
within 5G systems, focusing on its potential in challenging scenarios and
conditions where conventional methods often fall short. Building upon the
insights from the technical report TR38.843, we examine the Life Cycle
Management (LCM) with a focus on to the aspects associated direct positioning
process. We highlight significant simulation results and key observations from
the report on the direct positioning under the various challenging conditions.
Additionally, we discuss selected solutions that address measurement reporting,
data collection, and model management, emphasizing their importance for
advancing direct positioning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02429">Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yan-Fu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gryllias_K/0/1/0/all/0/1">Konstantinos Gryllias</a></p>
<p>In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial
discipline concerned with detecting and gathering vital information about
industrial equipment's health condition, thereby facilitating the
identification of failure types and severities. The pursuit of precise and
effective fault recognition has garnered substantial attention, culminating in
a focus on automating equipment monitoring to preclude safety accidents and
reduce reliance on human labor. The advent of artificial neural networks (ANNs)
has been instrumental in augmenting intelligent IFD algorithms, particularly in
the context of big data. Despite these advancements, ANNs, being a simplified
biomimetic neural network model, exhibit inherent limitations such as resource
and data dependencies and restricted cognitive capabilities. To address these
limitations, the third-generation Spiking Neural Network (SNN), founded on
principles of Brain-inspired computing, has surfaced as a promising
alternative. The SNN, characterized by its biological neuron dynamics and
spiking information encoding, demonstrates exceptional potential in
representing spatiotemporal features. Consequently, developing SNN-based IFD
models has gained momentum, displaying encouraging performance. Nevertheless,
this field lacks systematic surveys to illustrate the current situation,
challenges, and future directions. Therefore, this paper systematically reviews
the theoretical progress of SNN-based models to answer the question of what SNN
is. Subsequently, it reviews and analyzes existing SNN-based IFD models to
explain why SNN needs to be used and how to use it. More importantly, this
paper systematically answers the challenges, solutions, and opportunities of
SNN in IFD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02430">Automated Classification of Model Errors on ImageNet. (arXiv:2401.02430v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peychev_M/0/1/0/all/0/1">Momchil Peychev</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Mark Niklas M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1">Marc Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a></p>
<p>While the ImageNet dataset has been driving computer vision research over the
past decade, significant label noise and ambiguity have made top-1 accuracy an
insufficient measure of further progress. To address this, new label-sets and
evaluation protocols have been proposed for ImageNet showing that
state-of-the-art models already achieve over 95% accuracy and shifting the
focus on investigating why the remaining errors persist.
</p>
<p>Recent work in this direction employed a panel of experts to manually
categorize all remaining classification errors for two selected models.
However, this process is time-consuming, prone to inconsistencies, and requires
trained experts, making it unsuitable for regular model evaluation thus
limiting its utility. To overcome these limitations, we propose the first
automated error classification framework, a valuable tool to study how modeling
choices affect error distributions. We use our framework to comprehensively
evaluate the error distribution of over 900 models. Perhaps surprisingly, we
find that across model architectures, scales, and pre-training corpora, top-1
accuracy is a strong predictor for the portion of all error types. In
particular, we observe that the portion of severe errors drops significantly
with top-1 accuracy indicating that, while it underreports a model's true
performance, it remains a valuable performance metric.
</p>
<p>We release all our code at
https://github.com/eth-sri/automated-error-analysis .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02433">FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">DaiXun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1">Weiying Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">ZiXuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">YiBing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunsong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1">Leyuan Fang</a></p>
<p>With the rapid development of imaging sensor technology in the field of
remote sensing, multi-modal remote sensing data fusion has emerged as a crucial
research direction for land cover classification tasks. While diffusion models
have made great progress in generative models and image classification tasks,
existing models primarily focus on single-modality and single-client control,
that is, the diffusion process is driven by a single modal in a single
computing node. To facilitate the secure fusion of heterogeneous data from
clients, it is necessary to enable distributed multi-modal control, such as
merging the hyperspectral data of organization A and the LiDAR data of
organization B privately on each base station client. In this study, we propose
a multi-modal collaborative diffusion federated learning framework called
FedDiff. Our framework establishes a dual-branch diffusion model feature
extraction setup, where the two modal data are inputted into separate branches
of the encoder. Our key insight is that diffusion models driven by different
modalities are inherently complementary in terms of potential denoising steps
on which bilateral connections can be built. Considering the challenge of
private and efficient communication between multiple clients, we embed the
diffusion model into the federated learning communication structure, and
introduce a lightweight communication module. Qualitative and quantitative
experiments validate the superiority of our framework in terms of image quality
and conditional consistency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02452">The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?. (arXiv:2401.02452v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Besiroglu_T/0/1/0/all/0/1">Tamay Besiroglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergerson_S/0/1/0/all/0/1">Sage Andrus Bergerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Michael_A/0/1/0/all/0/1">Amelia Michael</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xueyun Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Thompson_N/0/1/0/all/0/1">Neil Thompson</a></p>
<p>There are pronounced differences in the extent to which industrial and
academic AI labs use computing resources. We provide a data-driven survey of
the role of the compute divide in shaping machine learning research. We show
that a compute divide has coincided with a reduced representation of
academic-only research teams in compute intensive research topics, especially
foundation models. We argue that, academia will likely play a smaller role in
advancing the associated techniques, providing critical evaluation and
scrutiny, and in the diffusion of such models. Concurrent with this change in
research focus, there is a noticeable shift in academic research towards
embracing open source, pre-trained models developed within the industry. To
address the challenges arising from this trend, especially reduced scrutiny of
influential models, we recommend approaches aimed at thoughtfully expanding
academic insights. Nationally-sponsored computing infrastructure coupled with
open science initiatives could judiciously boost academic compute access,
prioritizing research on interpretability, safety and security. Structured
access programs and third-party auditing may also allow measured external
evaluation of industry systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02456">A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management. (arXiv:2401.02456v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Boroujeni_S/0/1/0/all/0/1">Sayed Pedram Haeri Boroujeni</a>, <a href="http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1">Abolfazl Razi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoshdel_S/0/1/0/all/0/1">Sahand Khoshdel</a>, <a href="http://arxiv.org/find/cs/1/au:+Afghah_F/0/1/0/all/0/1">Fatemeh Afghah</a>, <a href="http://arxiv.org/find/cs/1/au:+Coen_J/0/1/0/all/0/1">Janice L. Coen</a>, <a href="http://arxiv.org/find/cs/1/au:+ONeill_L/0/1/0/all/0/1">Leo ONeill</a>, <a href="http://arxiv.org/find/cs/1/au:+Fule_P/0/1/0/all/0/1">Peter Z. Fule</a>, <a href="http://arxiv.org/find/cs/1/au:+Watts_A/0/1/0/all/0/1">Adam Watts</a>, <a href="http://arxiv.org/find/cs/1/au:+Kokolakis_N/0/1/0/all/0/1">Nick-Marios T. Kokolakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvoudakis_K/0/1/0/all/0/1">Kyriakos G. Vamvoudakis</a></p>
<p>Wildfires have emerged as one of the most destructive natural disasters
worldwide, causing catastrophic losses in both human lives and forest wildlife.
Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by
the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models,
has created an unprecedented momentum to implement and develop more effective
wildfire management. Although some of the existing survey papers have explored
various learning-based approaches, a comprehensive review emphasizing the
application of AI-enabled UAV systems and their subsequent impact on
multi-stage wildfire management is notably lacking. This survey aims to bridge
these gaps by offering a systematic review of the recent state-of-the-art
technologies, highlighting the advancements of UAV systems and AI models from
pre-fire, through the active-fire stage, to post-fire management. To this aim,
we provide an extensive analysis of the existing remote sensing systems with a
particular focus on the UAV advancements, device specifications, and sensor
technologies relevant to wildfire management. We also examine the pre-fire and
post-fire management approaches, including fuel monitoring, prevention
strategies, as well as evacuation planning, damage assessment, and operation
strategies. Additionally, we review and summarize a wide range of computer
vision techniques in active-fire management, with an emphasis on Machine
Learning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms
for wildfire classification, segmentation, detection, and monitoring tasks.
Ultimately, we underscore the substantial advancement in wildfire modeling
through the integration of cutting-edge AI techniques and UAV-based data,
providing novel insights and enhanced predictive capabilities to understand
dynamic wildfire behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02457">eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning. (arXiv:2401.02457v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zuo_Z/0/1/0/all/0/1">Zhiwei Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhuo Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kenli Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1">Anwitaman Datta</a></p>
<p>New categories may be introduced over time, or existing categories may need
to be reclassified. Class incremental learning (CIL) is employed for the
gradual acquisition of knowledge about new categories while preserving
information about previously learned ones in such dynamic environments. It
might also be necessary to also eliminate the influence of related categories
on the model to adapt to reclassification. We thus introduce class-level
machine unlearning (MU) within CIL. Typically, MU methods tend to be
time-consuming and can potentially harm the model's performance. A continuous
stream of unlearning requests could lead to catastrophic forgetting. To address
these issues, we propose a non-destructive eCIL-MU framework based on embedding
techniques to map data into vectors and then be stored in vector databases. Our
approach exploits the overlap between CIL and MU tasks for acceleration.
Experiments demonstrate the capability of achieving unlearning effectiveness
and orders of magnitude (upto $\sim 278\times$) of acceleration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02458">Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunkun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zheling Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Lingfeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1">Kexin Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaoting Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dequan Wang</a></p>
<p>The advent of foundation models (FMs) as an emerging suite of AI techniques
has struck a wave of opportunities in computational healthcare. The interactive
nature of these models, guided by pre-training data and human instructions, has
ignited a data-centric AI paradigm that emphasizes better data
characterization, quality, and scale. In healthcare AI, obtaining and
processing high-quality clinical data records has been a longstanding
challenge, ranging from data quantity, annotation, patient privacy, and ethics.
In this survey, we investigate a wide range of data-centric approaches in the
FM era (from model pre-training to inference) towards improving the healthcare
workflow. We discuss key perspectives in AI security, assessment, and alignment
with human values. Finally, we offer a promising outlook of FM-based analytics
to enhance the performance of patient outcome and clinical workflow in the
evolving landscape of healthcare and medicine. We provide an up-to-date list of
healthcare-related foundation models and datasets at
https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02465">Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows. (arXiv:2401.02465v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiaburu_T/0/1/0/all/0/1">Teodor Chiaburu</a>, <a href="http://arxiv.org/find/cs/1/au:+Biessmann_F/0/1/0/all/0/1">Felix Biessmann</a></p>
<p>Climate change poses increasingly complex challenges to our society. Extreme
weather events such as floods, wild fires or droughts are becoming more
frequent, spontaneous and difficult to foresee or counteract. In this work we
specifically address the problem of sewage water polluting surface water bodies
after spilling over from rain tanks as a consequence of heavy rain events. We
investigate to what extent state-of-the-art interpretable time series models
can help predict such critical water level points, so that the excess can
promptly be redistributed across the sewage network. Our results indicate that
modern time series models can contribute to better waste water management and
prevention of environmental pollution from sewer systems. All the code and
experiments can be found in our repository:
https://github.com/TeodorChiaburu/RIWWER_TimeSeries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02500">On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS). (arXiv:2401.02500v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pallagani_V/0/1/0/all/0/1">Vishal Pallagani</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Muppasani_B/0/1/0/all/0/1">Bharath Muppasani</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabiano_F/0/1/0/all/0/1">Francesco Fabiano</a>, <a href="http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1">Andrea Loreggia</a>, <a href="http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1">Keerthiram Murugesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1">Biplav Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1">Francesca Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1">Lior Horesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1">Amit Sheth</a></p>
<p>Automated Planning and Scheduling is among the growing areas in Artificial
Intelligence (AI) where mention of LLMs has gained popularity. Based on a
comprehensive review of 126 papers, this paper investigates eight categories
based on the unique applications of LLMs in addressing various aspects of
planning problems: language translation, plan generation, model construction,
multi-agent planning, interactive planning, heuristics optimization, tool
integration, and brain-inspired planning. For each category, we articulate the
issues considered and existing gaps. A critical insight resulting from our
review is that the true potential of LLMs unfolds when they are integrated with
traditional symbolic planners, pointing towards a promising neuro-symbolic
approach. This approach effectively combines the generative aspects of LLMs
with the precision of classical planning methods. By synthesizing insights from
existing literature, we underline the potential of this integration to address
complex planning challenges. Our goal is to encourage the ICAPS community to
recognize the complementary strengths of LLMs and symbolic planners, advocating
for a direction in automated planning that leverages these synergistic
capabilities to develop more advanced and intelligent planning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02509">Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Li_J/0/1/0/all/0/1">Jitang Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_J/0/1/0/all/0/1">Jinzheng Li</a></p>
<p>With the development in cognitive science and Large Language Models (LLMs),
increasing connections have come to light between these two distinct fields.
Building upon these connections, we propose a conjecture suggesting the
existence of a duality between LLMs and Tulving's theory of memory. We identify
a potential correspondence between Tulving's synergistic ecphory model (SEM) of
retrieval and the emergent abilities observed in LLMs, serving as supporting
evidence for our conjecture. Furthermore, we speculate that consciousness may
be considered a form of emergent ability based on this duality. We also discuss
how other theories of consciousness intersect with our research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02511">Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation. (arXiv:2401.02511v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Lamarque_M/0/1/0/all/0/1">Maxence Lamarque</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhan_L/0/1/0/all/0/1">Luke Bhan</a>, <a href="http://arxiv.org/find/eess/1/au:+Vazquez_R/0/1/0/all/0/1">Rafael Vazquez</a>, <a href="http://arxiv.org/find/eess/1/au:+Krstic_M/0/1/0/all/0/1">Miroslav Krstic</a></p>
<p>To stabilize PDE models, control laws require space-dependent functional
gains mapped by nonlinear operators from the PDE functional coefficients. When
a PDE is nonlinear and its "pseudo-coefficient" functions are state-dependent,
a gain-scheduling (GS) nonlinear design is the simplest approach to the design
of nonlinear feedback. The GS version of PDE backstepping employs gains
obtained by solving a PDE at each value of the state. Performing such PDE
computations in real time may be prohibitive. The recently introduced neural
operators (NO) can be trained to produce the gain functions, rapidly in real
time, for each state value, without requiring a PDE solution. In this paper we
introduce NOs for GS-PDE backstepping. GS controllers act on the premise that
the state change is slow and, as a result, guarantee only local stability, even
for ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear
recirculation using both a "full-kernel" approach and the "gain-only" approach
to gain operator approximation. Numerical simulations illustrate stabilization
and demonstrate speedup by three orders of magnitude over traditional PDE
gain-scheduling. Code (Github) for the numerical implementation is published to
enable exploration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02516">Moving-Horizon Estimators for Hyperbolic and Parabolic PDEs in 1-D. (arXiv:2401.02516v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bhan_L/0/1/0/all/0/1">Luke Bhan</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1">Yuanyuan Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Karafyllis_I/0/1/0/all/0/1">Iasson Karafyllis</a>, <a href="http://arxiv.org/find/eess/1/au:+Krstic_M/0/1/0/all/0/1">Miroslav Krstic</a>, <a href="http://arxiv.org/find/eess/1/au:+Rawlings_J/0/1/0/all/0/1">James B. Rawlings</a></p>
<p>Observers for PDEs are themselves PDEs. Therefore, producing real time
estimates with such observers is computationally burdensome. For both
finite-dimensional and ODE systems, moving-horizon estimators (MHE) are
operators whose output is the state estimate, while their inputs are the
initial state estimate at the beginning of the horizon as well as the measured
output and input signals over the moving time horizon. In this paper we
introduce MHEs for PDEs which remove the need for a numerical solution of an
observer PDE in real time. We accomplish this using the PDE backstepping method
which, for certain classes of both hyperbolic and parabolic PDEs, produces
moving-horizon state estimates explicitly. Precisely, to explicitly produce the
state estimates, we employ a backstepping transformation of a hard-to-solve
observer PDE into a target observer PDE, which is explicitly solvable. The MHEs
we propose are not new observer designs but simply the explicit MHE
realizations, over a moving horizon of arbitrary length, of the existing
backstepping observers. Our PDE MHEs lack the optimality of the MHEs that arose
as duals of MPC, but they are given explicitly, even for PDEs. In the paper we
provide explicit formulae for MHEs for both hyperbolic and parabolic PDEs, as
well as simulation results that illustrate theoretically guaranteed convergence
of the MHEs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02523">Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1">Md Ruman Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramaniam_M/0/1/0/all/0/1">Mahadevan Subramaniam</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Pei-Chi Huang</a> (Department of Computer Science, University of Nebraska at Omaha, Omaha, NE, USA)</p>
<p>Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02524">Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bauer_A/0/1/0/all/0/1">Andr&#xe9; Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Trapp_S/0/1/0/all/0/1">Simon Trapp</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenger_M/0/1/0/all/0/1">Michael Stenger</a>, <a href="http://arxiv.org/find/cs/1/au:+Leppich_R/0/1/0/all/0/1">Robert Leppich</a>, <a href="http://arxiv.org/find/cs/1/au:+Kounev_S/0/1/0/all/0/1">Samuel Kounev</a>, <a href="http://arxiv.org/find/cs/1/au:+Leznik_M/0/1/0/all/0/1">Mark Leznik</a>, <a href="http://arxiv.org/find/cs/1/au:+Chard_K/0/1/0/all/0/1">Kyle Chard</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1">Ian Foster</a></p>
<p>Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02540">DISO: A Domain Ontology for Modeling Dislocations in Crystalline Materials. (arXiv:2401.02540v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Ihsan_A/0/1/0/all/0/1">Ahmad Zainul Ihsan</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fathalla_S/0/1/0/all/0/1">Said Fathalla</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sandfeld_S/0/1/0/all/0/1">Stefan Sandfeld</a></p>
<p>Crystalline materials, such as metals and semiconductors, nearly always
contain a special defect type called dislocation. This defect decisively
determines many important material properties, e.g., strength, fracture
toughness, or ductility. Over the past years, significant effort has been put
into understanding dislocation behavior across different length scales via
experimental characterization techniques and simulations. This paper introduces
the dislocation ontology (DISO), which defines the concepts and relationships
related to linear defects in crystalline materials. We developed DISO using a
top-down approach in which we start defining the most general concepts in the
dislocation domain and subsequent specialization of them. DISO is published
through a persistent URL following W3C best practices for publishing Linked
Data. Two potential use cases for DISO are presented to illustrate its
usefulness in the dislocation dynamics domain. The evaluation of the ontology
is performed in two directions, evaluating the success of the ontology in
modeling a real-world domain and the richness of the ontology.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02542">A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature. (arXiv:2401.02542v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chunjiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yikun Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haiyun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shihan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kaidi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yongye Su</a></p>
<p>This study introduces an innovative approach that integrates community
detection algorithms with Graph Neural Network (GNN) models to enhance link
prediction in scientific literature networks. We specifically focus on the
utilization of the Louvain community detection algorithm to uncover latent
community structures within these networks, which are then incorporated into
GNN architectures to predict potential links. Our methodology demonstrates the
importance of understanding community dynamics in complex networks and
leverages the strengths of both community detection and GNNs to improve
predictive accuracy. Through extensive experiments on bipartite graphs
representing scientific collaborations and citations, our approach not only
highlights the synergy between community detection and GNNs but also addresses
some of the prevalent challenges in link prediction, such as scalability and
resolution limits. The results suggest that incorporating community-level
information can significantly enhance the performance of GNNs in link
prediction tasks. This work contributes to the evolving field of network
science by offering a novel perspective on integrating advanced machine
learning techniques with traditional network analysis methods to better
understand and predict the intricate patterns of scientific collaborations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02549">Quantitative Technology Forecasting: a Review of Trend Extrapolation Methods. (arXiv:2401.02549v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsai_P/0/1/0/all/0/1">Peng-Hung Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1">Daniel Berleant</a>, <a href="http://arxiv.org/find/cs/1/au:+Segall_R/0/1/0/all/0/1">Richard S. Segall</a>, <a href="http://arxiv.org/find/cs/1/au:+Aboudja_H/0/1/0/all/0/1">Hyacinthe Aboudja</a>, <a href="http://arxiv.org/find/cs/1/au:+Batthula_V/0/1/0/all/0/1">Venkata Jaipal R. Batthula</a>, <a href="http://arxiv.org/find/cs/1/au:+Duggirala_S/0/1/0/all/0/1">Sheela Duggirala</a>, <a href="http://arxiv.org/find/cs/1/au:+Howell_M/0/1/0/all/0/1">Michael Howell</a></p>
<p>Quantitative technology forecasting uses quantitative methods to understand
and project technological changes. It is a broad field encompassing many
different techniques and has been applied to a vast range of technologies. A
widely used approach in this field is trend extrapolation. Based on the
publications available to us, there has been little or no attempt made to
systematically review the empirical evidence on quantitative trend
extrapolation techniques. This study attempts to close this gap by conducting a
systematic review of technology forecasting literature addressing the
application of quantitative trend extrapolation techniques. We identified 25
studies relevant to the objective of this research and classified the
techniques used in the studies into different categories, among which growth
curves and time series methods were shown to remain popular over the past
decade, while newer methods, such as machine learning-based hybrid models, have
emerged in recent years. As more effort and evidence are needed to determine if
hybrid models are superior to traditional methods, we expect to see a growing
trend in the development and application of hybrid models to technology
forecasting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02575">Large Language Models for Social Networks: Applications, Challenges, and Solutions. (arXiv:2401.02575v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jingying Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Richard Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_W/0/1/0/all/0/1">Waleed Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1">Langxuan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Babic_B/0/1/0/all/0/1">Bojan Babic</a>, <a href="http://arxiv.org/find/cs/1/au:+Shacham_D/0/1/0/all/0/1">Danny Shacham</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xiao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jaewon Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1">Qi He</a></p>
<p>Large Language Models (LLMs) are transforming the way people generate,
explore, and engage with content. We study how we can develop LLM applications
for online social networks. Despite LLMs' successes in other domains, it is
challenging to develop LLM-based products for social networks for numerous
reasons, and it has been relatively under-reported in the research community.
We categorize LLM applications for social networks into three categories. First
is knowledge tasks where users want to find new knowledge and information, such
as search and question-answering. Second is entertainment tasks where users
want to consume interesting content, such as getting entertaining notification
content. Third is foundational tasks that need to be done to moderate and
operate the social networks, such as content annotation and LLM monitoring. For
each task, we share the challenges we found, solutions we developed, and
lessons we learned. To the best of our knowledge, this is the first
comprehensive paper about developing LLM applications for social networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02576">t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yue_W/0/1/0/all/0/1">William Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1">Peter Stone</a></p>
<p>Deep generative replay has emerged as a promising approach for continual
learning in decision-making tasks. This approach addresses the problem of
catastrophic forgetting by leveraging the generation of trajectories from
previously encountered tasks to augment the current dataset. However, existing
deep generative replay methods for continual learning rely on autoregressive
models, which suffer from compounding errors in the generated trajectories. In
this paper, we propose a simple, scalable, and non-autoregressive method for
continual learning in decision-making tasks using a generative model that
generates task samples conditioned on the trajectory timestep. We evaluate our
method on Continual World benchmarks and find that our approach achieves
state-of-the-art performance on the average success rate metric among continual
learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02586">Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1">Peiyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Morris Chang</a></p>
<p>One of the most challenging issues in federated learning is that the data is
often not independent and identically distributed (nonIID). Clients are
expected to contribute the same type of data and drawn from one global
distribution. However, data are often collected in different ways from
different resources. Thus, the data distributions among clients might be
different from the underlying global distribution. This creates a weight
divergence issue and reduces federated learning performance. This work focuses
on improving federated learning performance for skewed data distribution across
clients. The main idea is to adjust the client distribution closer to the
global distribution using sample weights. Thus, the machine learning model
converges faster with higher accuracy. We start from the fundamental concept of
empirical risk minimization and theoretically derive a solution for adjusting
the distribution skewness using sample weights. To determine sample weights, we
implicitly exchange density information by leveraging a neural network-based
density estimation model, MADE. The clients data distribution can then be
adjusted without exposing their raw data. Our experiment results on three
real-world datasets show that the proposed method not only improves federated
learning accuracy but also significantly reduces communication costs compared
to the other experimental methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02588">Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting. (arXiv:2401.02588v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Van Minh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandidge_E/0/1/0/all/0/1">Emma Sandidge</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahendrakar_T/0/1/0/all/0/1">Trupti Mahendrakar</a>, <a href="http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1">Ryan T. White</a></p>
<p>The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target's geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02589">Identification of 4FGL uncertain sources at Higher Resolutions with Inverse Discrete Wavelet Transform. (arXiv:2401.02589v1 [astro-ph.HE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Cao_H/0/1/0/all/0/1">Haitao Cao</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Xiao_H/0/1/0/all/0/1">Hubing Xiao</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Luo_Z/0/1/0/all/0/1">Zhijian Luo</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Zeng_X/0/1/0/all/0/1">Xiangtao Zeng</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Fan_J/0/1/0/all/0/1">Junhui Fan</a></p>
<p>In the forthcoming era of big astronomical data, it is a burden to find out
target sources from ground-based and space-based telescopes. Although Machine
Learning (ML) methods have been extensively utilized to address this issue, the
incorporation of in-depth data analysis can significantly enhance the
efficiency of identifying target sources when dealing with massive volumes of
astronomical data. In this work, we focused on the task of finding AGN
candidates and identifying BL Lac/FSRQ candidates from the 4FGL DR3 uncertain
sources. We studied the correlations among the attributes of the 4FGL DR3
catalogue and proposed a novel method, named FDIDWT, to transform the original
data. The transformed dataset is characterized as low-dimensional and
feature-highlighted, with the estimation of correlation features by Fractal
Dimension (FD) theory and the multi-resolution analysis by Inverse Discrete
Wavelet Transform (IDWT). Combining the FDIDWT method with an improved
lightweight MatchboxConv1D model, we accomplished two missions: (1) to
distinguish the Active Galactic Nuclei (AGNs) from others (Non-AGNs) in the
4FGL DR3 uncertain sources with an accuracy of 96.65%, namely, Mission A; (2)
to classify blazar candidates of uncertain type (BCUs) into BL Lacertae objects
(BL Lacs) or Flat Spectrum Radio Quasars (FSRQs) with an accuracy of 92.03%,
namely, Mission B. There are 1354 AGN candidates in Mission A, 482 BL Lacs
candidates and 128 FSRQ candidates in Mission B were found. The results show a
high consistency of greater than 98% with the results in previous works. In
addition, our method has the advantage of finding less variable and relatively
faint sources than ordinary methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02591">Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data. (arXiv:2401.02591v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Hung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Morris Chang</a></p>
<p>This study examines the impact of class-imbalanced data on deep learning
models and proposes a technique for data balancing by generating synthetic data
for the minority class. Unlike random-based oversampling, our method
prioritizes balancing the informative regions by identifying high entropy
samples. Generating well-placed synthetic data can enhance machine learning
algorithms accuracy and efficiency, whereas poorly-placed ones may lead to
higher misclassification rates. We introduce an algorithm that maximizes the
probability of generating a synthetic sample in the correct region of its class
by optimizing the class posterior ratio. Additionally, to maintain data
topology, synthetic data are generated within each minority sample's
neighborhood. Our experimental results on forty-one datasets demonstrate the
superior performance of our technique in enhancing deep-learning models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02600">Object-oriented backdoor attack against image captioning. (arXiv:2401.02600v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Meiling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_N/0/1/0/all/0/1">Nan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1">Zhenxing Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a></p>
<p>Backdoor attack against image classification task has been widely studied and
proven to be successful, while there exist little research on the backdoor
attack against vision-language models. In this paper, we explore backdoor
attack towards image captioning models by poisoning training data. Assuming the
attacker has total access to the training dataset, and cannot intervene in
model construction or training process. Specifically, a portion of benign
training samples is randomly selected to be poisoned. Afterwards, considering
that the captions are usually unfolded around objects in an image, we design an
object-oriented method to craft poisons, which aims to modify pixel values by a
slight range with the modification number proportional to the scale of the
current detected object region. After training with the poisoned data, the
attacked model behaves normally on benign images, but for poisoned images, the
model will generate some sentences irrelevant to the given image. The attack
controls the model behavior on specific test images without sacrificing the
generation performance on benign test images. Our method proves the weakness of
image captioning models to backdoor attack and we hope this work can raise the
awareness of defending against backdoor attack in the image captioning field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02602">Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1">Kevin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1">Elias Bareinboim</a></p>
<p>The abilities of humans to understand the world in terms of cause and effect
relationships, as well as to compress information into abstract concepts, are
two hallmark features of human intelligence. These two topics have been studied
in tandem in the literature under the rubric of causal abstractions theory. In
practice, it remains an open problem how to best leverage abstraction theory in
real-world causal inference tasks, where the true mechanisms are unknown and
only limited data is available. In this paper, we develop a new family of
causal abstractions by clustering variables and their domains. This approach
refines and generalizes previous notions of abstractions to better accommodate
individual causal distributions that are spawned by Pearl's causal hierarchy.
We show that such abstractions are learnable in practical settings through
Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning
toolkit to solve various challenging causal inference tasks -- identification,
estimation, sampling -- at different levels of granularity. Finally, we
integrate these results with representation learning to create more flexible
abstractions, moving these results closer to practical applications. Our
experiments support the theory and illustrate how to scale causal inferences to
high-dimensional settings involving image data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02620">Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human. (arXiv:2401.02620v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Song Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jie Li</a></p>
<p>While AI-generated text and 2D images continue to expand its territory, 3D
generation has gradually emerged as a trend that cannot be ignored. Since the
year 2023 an abundant amount of research papers has emerged in the domain of 3D
generation. This growth encompasses not just the creation of 3D objects, but
also the rapid development of 3D character and motion generation. Several key
factors contribute to this progress. The enhanced fidelity in stable diffusion,
coupled with control methods that ensure multi-view consistency, and realistic
human models like SMPL-X, contribute synergistically to the production of 3D
models with remarkable consistency and near-realistic appearances. The
advancements in neural network-based 3D storing and rendering models, such as
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have
accelerated the efficiency and realism of neural rendered models. Furthermore,
the multimodality capabilities of large language models have enabled language
inputs to transcend into human motion outputs. This paper aims to provide a
comprehensive overview and summary of the relevant papers published mostly
during the latter half year of 2023. It will begin by discussing the AI
generated object models in 3D, followed by the generated 3D human models, and
finally, the generated 3D human motions, culminating in a conclusive summary
and a vision for the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02627">Characteristics and prevalence of fake social media profiles with AI-generated faces. (arXiv:2401.02627v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kai-Cheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1">Danishjeet Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Menczer_F/0/1/0/all/0/1">Filippo Menczer</a></p>
<p>Recent advancements in generative artificial intelligence (AI) have raised
concerns about their potential to create convincing fake social media accounts,
but empirical evidence is lacking. In this paper, we present a systematic
analysis of Twitter(X) accounts using human faces generated by Generative
Adversarial Networks (GANs) for their profile pictures. We present a dataset of
1,353 such accounts and show that they are used to spread scams, spam, and
amplify coordinated messages, among other inauthentic activities. Leveraging a
feature of GAN-generated faces -- consistent eye placement -- and supplementing
it with human annotation, we devise an effective method for identifying
GAN-generated profiles in the wild. Applying this method to a random sample of
active Twitter users, we estimate a lower bound for the prevalence of profiles
using GAN-generated faces between 0.021% and 0.044% -- around 10K daily active
accounts. These findings underscore the emerging threats posed by multimodal
generative AI. We release the source code of our detection method and the data
we collect to facilitate further investigation. Additionally, we provide
practical heuristics to assist social media users in recognizing such accounts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02643">Training and Serving System of Foundation Models: A Comprehensive Survey. (arXiv:2401.02643v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiahang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1">Zicong Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wuhui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yue Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chuanfu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zibin Zheng</a></p>
<p>Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-$\Sigma$)
have demonstrated extraordinary performance in key technological areas, such as
natural language processing and visual recognition, and have become the
mainstream trend of artificial general intelligence. This has led more and more
major technology giants to dedicate significant human and financial resources
to actively develop their foundation model systems, which drives continuous
growth of these models' parameters. As a result, the training and serving of
these models have posed significant challenges, including substantial computing
power, memory consumption, bandwidth demands, etc. Therefore, employing
efficient training and serving strategies becomes particularly crucial. Many
researchers have actively explored and proposed effective methods. So, a
comprehensive survey of them is essential for system developers and
researchers. This paper extensively explores the methods employed in training
and serving foundation models from various perspectives. It provides a detailed
categorization of these state-of-the-art methods, including finer aspects such
as network, computing, and storage. Additionally, the paper summarizes the
challenges and presents a perspective on the future development direction of
foundation model systems. Through comprehensive discussion and analysis, it
hopes to provide a solid theoretical basis and practical guidance for future
research and applications, promoting continuous innovation and development in
foundation model systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02644">Simple Hierarchical Planning with Diffusion. (arXiv:2401.02644v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1">Fei Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1">Caglar Gulcehre</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Sungjin Ahn</a></p>
<p>Diffusion-based generative methods have proven effective in modeling
trajectories with offline datasets. However, they often face computational
challenges and can falter in generalization, especially in capturing temporal
abstractions for long-horizon tasks. To overcome this, we introduce the
Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning
method combining the advantages of hierarchical and diffusion-based planning.
Our model adopts a "jumpy" planning strategy at the higher level, which allows
it to have a larger receptive field but at a lower computational cost -- a
crucial factor for diffusion-based planning methods, as we have empirically
verified. Additionally, the jumpy sub-goals guide our low-level planner,
facilitating a fine-tuning stage and further improving our approach's
effectiveness. We conducted empirical evaluations on standard offline
reinforcement learning benchmarks, demonstrating our method's superior
performance and efficiency in terms of training and planning speed compared to
the non-hierarchical Diffuser as well as other hierarchical planning methods.
Moreover, we explore our model's generalization capability, particularly on how
our method improves generalization capabilities on compositional
out-of-distribution tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02652">Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bector_R/0/1/0/all/0/1">Ridhima Bector</a>, <a href="http://arxiv.org/find/cs/1/au:+Aradhya_A/0/1/0/all/0/1">Abhay Aradhya</a>, <a href="http://arxiv.org/find/cs/1/au:+Quek_C/0/1/0/all/0/1">Chai Quek</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabinovich_Z/0/1/0/all/0/1">Zinovi Rabinovich</a></p>
<p>Among the most insidious attacks on Reinforcement Learning (RL) solutions are
training-time attacks (TTAs) that create loopholes and backdoors in the learned
behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are
now available, where the attacker forces a specific, target behaviour upon a
training RL agent (victim). However, even state-of-the-art C-TTAs focus on
target behaviours that could be naturally adopted by the victim if not for a
particular feature of the environment dynamics, which C-TTAs exploit. In this
work, we show that a C-TTA is possible even when the target behaviour is
un-adoptable due to both environment dynamics as well as non-optimality with
respect to the victim objective(s). To find efficient attacks in this context,
we develop a specialised flavour of the DDPG algorithm, which we term
gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically
alters the attack policy planning horizon based on the victim's current
behaviour. This improves effort distribution throughout the attack timeline and
reduces the effect of uncertainty the attacker has about the victim. To
demonstrate the features of our method and better relate the results to prior
research, we borrow a 3D grid domain from a state-of-the-art C-TTA for our
experiments. Code is available at "bit.ly/github-rb-gDDPG".
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02653">A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids. (arXiv:2401.02653v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chifu_V/0/1/0/all/0/1">Viorica Rozina Chifu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cioara_T/0/1/0/all/0/1">Tudor Cioara</a>, <a href="http://arxiv.org/find/cs/1/au:+Pop_C/0/1/0/all/0/1">Cristina Bianca Pop</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_H/0/1/0/all/0/1">Horia Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Anghel_I/0/1/0/all/0/1">Ionut Anghel</a></p>
<p>Economic and policy factors are driving the continuous increase in the
adoption and usage of electrical vehicles (EVs). However, despite being a
cleaner alternative to combustion engine vehicles, EVs have negative impacts on
the lifespan of microgrid equipment and energy balance due to increased power
demand and the timing of their usage. In our view grid management should
leverage on EVs scheduling flexibility to support local network balancing
through active participation in demand response programs. In this paper, we
propose a model-free solution, leveraging Deep Q-Learning to schedule the
charging and discharging activities of EVs within a microgrid to align with a
target energy profile provided by the distribution system operator. We adapted
the Bellman Equation to assess the value of a state based on specific rewards
for EV scheduling actions and used a neural network to estimate Q-values for
available actions and the epsilon-greedy algorithm to balance exploitation and
exploration to meet the target energy profile. The results are promising
showing that the proposed solution can effectively schedule the EVs charging
and discharging actions to align with the target profile with a Person
coefficient of 0.99, handling effective EVs scheduling situations that involve
dynamicity given by the e-mobility features, relying only on data with no
knowledge of EVs and microgrid dynamics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02661">Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin. (arXiv:2401.02661v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Faruqui_S/0/1/0/all/0/1">Syed Hasib Akhter Faruqui</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaeddini_A/0/1/0/all/0/1">Adel Alaeddini</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1">Kumar Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a></p>
<p>Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a
significant risk of serious health complications and negative impacts on the
quality of life. Given the impact of individual characteristics and lifestyle
on the treatment plan and patient outcomes, it is crucial to develop precise
and personalized management strategies. Artificial intelligence (AI) provides
great promise in combining patterns from various data sources with nurses'
expertise to achieve optimal care. Methods: This is a 6-month ancillary study
among T2D patients (n = 20, age = 57 +- 10). Participants were randomly
assigned to an intervention (AI, n=10) group to receive daily AI-generated
individualized feedback or a control group without receiving the daily feedback
(non-AI, n=10) in the last three months. The study developed an online
nurse-in-the-loop predictive control (ONLC) model that utilizes a predictive
digital twin (PDT). The PDT was developed using a transfer-learning-based
Artificial Neural Network. The PDT was trained on participants self-monitoring
data (weight, food logs, physical activity, glucose) from the first three
months, and the online control algorithm applied particle swarm optimization to
identify impactful behavioral changes for maintaining the patient's glucose and
weight levels for the next three months. The ONLC provided the intervention
group with individualized feedback and recommendations via text messages. The
PDT was re-trained weekly to improve its performance. Findings: The trained
ONLC model achieved &gt;=80% prediction accuracy across all patients while the
model was tuned online. Participants in the intervention group exhibited a
trend of improved daily steps and stable or improved total caloric and total
carb intake as recommended.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02663">A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1">Jiazhu Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haoyu Sun</a></p>
<p>Graph Neural Networks (GNNs) are a class of deep learning models capable of
processing graph-structured data, and they have demonstrated significant
performance in a variety of real-world applications. Recent studies have found
that GNN models are vulnerable to backdoor attacks. When specific patterns
(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input
data, the backdoor embedded in the GNN models is activated, which misclassifies
the input data into the target class label specified by the attacker, whereas
when there are no backdoor triggers in the input, the backdoor embedded in the
GNN models is not activated, and the models work normally. Backdoor attacks are
highly stealthy and expose GNN models to serious security risks. Currently,
research on backdoor attacks against GNNs mainly focus on tasks such as graph
classification and node classification, and backdoor attacks against link
prediction tasks are rarely studied. In this paper, we propose a backdoor
attack against the link prediction tasks based on GNNs and reveal the existence
of such security vulnerability in GNN models, which make the backdoored GNN
models to incorrectly predict unlinked two nodes as having a link relationship
when a trigger appear. The method uses a single node as the trigger and poison
selected node pairs in the training graph, and then the backdoor will be
embedded in the GNN models through the training process. In the inference
stage, the backdoor in the GNN models can be activated by simply linking the
trigger node to the two end nodes of the unlinked node pairs in the input data,
causing the GNN models to produce incorrect link prediction results for the
target node pairs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02665">Zero-shot Microclimate Prediction with Deep Learning. (arXiv:2401.02665v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deznabi_I/0/1/0/all/0/1">Iman Deznabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1">Peeyush Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiterau_M/0/1/0/all/0/1">Madalina Fiterau</a></p>
<p>Weather station data is a valuable resource for climate prediction, however,
its reliability can be limited in remote locations. To compound the issue,
making local predictions often relies on sensor data that may not be accessible
for a new, previously unmonitored location. In response to these challenges, we
propose a novel zero-shot learning approach designed to forecast various
climate measurements at new and unmonitored locations. Our method surpasses
conventional weather forecasting techniques in predicting microclimate
variables by leveraging knowledge extracted from other geographic locations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02673">A unified multichannel far-field speech recognition system: combining neural beamforming with attention based end-to-end model. (arXiv:2401.02673v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1">Dongdi Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1">Jianbo Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1">Lu Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Jinke Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Ji_X/0/1/0/all/0/1">Xuan Ji</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1">Lei Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_F/0/1/0/all/0/1">Fuming Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_F/0/1/0/all/0/1">Feijun Jiang</a></p>
<p>Far-field speech recognition is a challenging task that conventionally uses
signal processing beamforming to attack noise and interference problem. But the
performance has been found usually limited due to heavy reliance on
environmental assumption. In this paper, we propose a unified multichannel
far-field speech recognition system that combines the neural beamforming and
transformer-based Listen, Spell, Attend (LAS) speech recognition system, which
extends the end-to-end speech recognition system further to include speech
enhancement. Such framework is then jointly trained to optimize the final
objective of interest. Specifically, factored complex linear projection (fCLP)
has been adopted to form the neural beamforming. Several pooling strategies to
combine look directions are then compared in order to find the optimal
approach. Moreover, information of the source direction is also integrated in
the beamforming to explore the usefulness of source direction as a prior, which
is usually available especially in multi-modality scenario. Experiments on
different microphone array geometry are conducted to evaluate the robustness
against spacing variance of microphone array. Large in-house databases are used
to evaluate the effectiveness of the proposed framework and the proposed method
achieve 19.26\% improvement when compared with a strong baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02677">Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1">Yatharth Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaddipal_V/0/1/0/all/0/1">Vishnu V. Jaddipal</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabhala_H/0/1/0/all/0/1">Harish Prabhala</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1">Sayak Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1">Patrick Von Platen</a></p>
<p>Stable Diffusion XL (SDXL) has become the best open source text-to-image
model (T2I) for its versatility and top-notch image quality. Efficiently
addressing the computational demands of SDXL models is crucial for wider reach
and applicability. In this work, we introduce two scaled-down variants, Segmind
Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter
UNets, respectively, achieved through progressive removal using layer-level
losses focusing on reducing the model size while preserving generative quality.
We release these models weights at https://hf.co/Segmind. Our methodology
involves the elimination of residual networks and transformer blocks from the
U-Net structure of SDXL, resulting in significant reductions in parameters, and
latency. Our compact models effectively emulate the original SDXL by
capitalizing on transferred knowledge, achieving competitive results against
larger multi-billion parameter SDXL. Our work underscores the efficacy of
knowledge distillation coupled with layer-level losses in reducing model size
while preserving the high-quality generative capabilities of SDXL, thus
facilitating more accessible deployment in resource-constrained environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02683">Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Can Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haosen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weigang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1">Pengfei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hongyang Chen</a></p>
<p>Denoising diffusion models have shown great potential in multiple research
areas. Existing diffusion-based generative methods on de novo 3D molecule
generation face two major challenges. Since majority heavy atoms in molecules
allow connections to multiple atoms through single bonds, solely using
pair-wise distance to model molecule geometries is insufficient. Therefore, the
first one involves proposing an effective neural network as the denoising
kernel that is capable to capture complex multi-body interatomic relationships
and learn high-quality features. Due to the discrete nature of graphs,
mainstream diffusion-based methods for molecules heavily rely on predefined
rules and generate edges in an indirect manner. The second challenge involves
accommodating molecule generation to diffusion and accurately predicting the
existence of bonds. In our research, we view the iterative way of updating
molecule conformations in diffusion process is consistent with molecular
dynamics and introduce a novel molecule generation method named
Geometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,
we introduce a Dual-Track Transformer Network (DTN) to fully excevate global
spatial relationships and learn high quality representations which contribute
to accurate predictions of features and geometries. As for the second
challenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the
formation of bonds during the training period, instead of directly embedding
edges into the latent space. Comprehensive experiments on current benchmarks
demonstrate the superiority of GFMDiff.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02703">Verifying Relational Explanations: A Probabilistic Approach. (arXiv:2401.02703v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Magar_A/0/1/0/all/0/1">Abisha Thapa Magar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakya_A/0/1/0/all/0/1">Anup Shakya</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkhel_S/0/1/0/all/0/1">Somdeb Sarkhel</a>, <a href="http://arxiv.org/find/cs/1/au:+Venugopal_D/0/1/0/all/0/1">Deepak Venugopal</a></p>
<p>Explanations on relational data are hard to verify since the explanation
structures are more complex (e.g. graphs). To verify interpretable explanations
(e.g. explanations of predictions made in images, text, etc.), typically human
subjects are used since it does not necessarily require a lot of expertise.
However, to verify the quality of a relational explanation requires expertise
and is hard to scale-up. GNNExplainer is arguably one of the most popular
explanation methods for Graph Neural Networks. In this paper, we develop an
approach where we assess the uncertainty in explanations generated by
GNNExplainer. Specifically, we ask the explainer to generate explanations for
several counterfactual examples. We generate these examples as symmetric
approximations of the relational structure in the original data. From these
explanations, we learn a factor graph model to quantify uncertainty in an
explanation. Our results on several datasets show that our approach can help
verify explanations from GNNExplainer by reliably estimating the uncertainty of
a relation specified in the explanation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02705">XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model. (arXiv:2401.02705v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhitao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zirao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Long Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1">Can Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xinjie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Luyang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hanjing Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shouzhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a></p>
<p>In past years, we have been dedicated to automating user acceptance testing
(UAT) process of WeChat Pay, one of the most influential mobile payment
applications in China. A system titled XUAT has been developed for this
purpose. However, there is still a human-labor-intensive stage, i.e, test
scripts generation, in the current system. Therefore, in this paper, we
concentrate on methods of boosting the automation level of the current system,
particularly the stage of test scripts generation. With recent notable
successes, large language models (LLMs) demonstrate significant potential in
attaining human-like intelligence and there has been a growing research area
that employs LLMs as autonomous agents to obtain human-like decision-making
capabilities. Inspired by these works, we propose an LLM-powered multi-agent
collaborative system, named XUAT-Copilot, for automated UAT. The proposed
system mainly consists of three LLM-based agents responsible for action
planning, state checking and parameter selecting, respectively, and two
additional modules for state sensing and case rewriting. The agents interact
with testing device, make human-like decision and generate action command in a
collaborative way. The proposed multi-agent system achieves a close
effectiveness to human testers in our experimental studies and gains a
significant improvement of Pass@1 accuracy compared with single-agent
architecture. More importantly, the proposed system has launched in the formal
testing environment of WeChat Pay mobile app, which saves a considerable amount
of manpower in the daily development work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02708">TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis. (arXiv:2401.02708v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1">Lianzhen Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1">Di Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_H/0/1/0/all/0/1">Hui Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1">Jie Tian</a></p>
<p>A core challenge in survival analysis is to model the distribution of
censored time-to-event data, where the event of interest may be a death,
failure, or occurrence of a specific event. Previous studies have showed that
ranking and maximum likelihood estimation (MLE)loss functions are widely-used
for survival analysis. However, ranking loss only focus on the ranking of
survival time and does not consider potential effect of samples for exact
survival time values. Furthermore, the MLE is unbounded and easily subject to
outliers (e.g., censored data), which may cause poor performance of modeling.
To handle the complexities of learning process and exploit valuable survival
time values, we propose a time-adaptive coordinate loss function, TripleSurv,
to achieve adaptive adjustments by introducing the differences in the survival
time between sample pairs into the ranking, which can encourage the model to
quantitatively rank relative risk of pairs, ultimately enhancing the accuracy
of predictions. Most importantly, the TripleSurv is proficient in quantifying
the relative risk between samples by ranking ordering of pairs, and consider
the time interval as a trade-off to calibrate the robustness of model over
sample distribution. Our TripleSurv is evaluated on three real-world survival
datasets and a public synthetic dataset. The results show that our method
outperforms the state-of-the-art methods and exhibits good model performance
and robustness on modeling various sophisticated data distributions with
different censor rates. Our code will be available upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02709">German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wehrli_S/0/1/0/all/0/1">Silvan Wehrli</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnrich_B/0/1/0/all/0/1">Bert Arnrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Irrgang_C/0/1/0/all/0/1">Christopher Irrgang</a></p>
<p>This work introduces a benchmark assessing the performance of clustering
German text embeddings in different domains. This benchmark is driven by the
increasing use of clustering neural text embeddings in tasks that require the
grouping of texts (such as topic modeling) and the need for German resources in
existing benchmarks. We provide an initial analysis for a range of pre-trained
mono- and multilingual models evaluated on the outcome of different clustering
algorithms. Results include strong performing mono- and multilingual models.
Reducing the dimensions of embeddings can further improve clustering.
Additionally, we conduct experiments with continued pre-training for German
BERT models to estimate the benefits of this additional training. Our
experiments suggest that significant performance improvements are possible for
short text. All code and datasets are publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02710">Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning. (arXiv:2401.02710v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1">Hong-Gi Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Sukhyun Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1">Eui-Yeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Sungho Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1">Young-Jin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yong-Hoon Choi</a></p>
<p>Mining of formulaic alpha factors refers to the process of discovering and
developing specific factors or indicators (referred to as alpha factors) for
quantitative trading in stock market. To efficiently discover alpha factors in
vast search space, reinforcement learning (RL) is commonly employed. This paper
proposes a method to enhance existing alpha factor mining approaches by
expanding a search space and utilizing pretrained formulaic alpha set as
initial seed values to generate synergistic formulaic alpha. We employ
information coefficient (IC) and rank information coefficient (Rank IC) as
performance evaluation metrics for the model. Using CSI300 market data, we
conducted real investment simulations and observed significant performance
improvement compared to existing techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02713">Graph-level Protein Representation Learning by Structure Knowledge Refinement. (arXiv:2401.02713v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Ge Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1">Zelin Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jiangbin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1">Jun Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Stan Z. Li</a></p>
<p>This paper focuses on learning representation on the whole graph level in an
unsupervised manner. Learning graph-level representation plays an important
role in a variety of real-world issues such as molecule property prediction,
protein structure feature extraction, and social network analysis. The
mainstream method is utilizing contrastive learning to facilitate graph feature
extraction, known as Graph Contrastive Learning (GCL). GCL, although effective,
suffers from some complications in contrastive learning, such as the effect of
false negative pairs. Moreover, augmentation strategies in GCL are weakly
adaptive to diverse graph datasets. Motivated by these problems, we propose a
novel framework called Structure Knowledge Refinement (SKR) which uses data
structure to determine the probability of whether a pair is positive or
negative. Meanwhile, we propose an augmentation strategy that naturally
preserves the semantic meaning of the original data and is compatible with our
SKR framework. Furthermore, we illustrate the effectiveness of our SKR
framework through intuition and experiments. The experimental results on the
tasks of graph-level classification demonstrate that our SKR framework is
superior to most state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02717">Complementary Information Mutual Learning for Multimodality Medical Image Segmentation. (arXiv:2401.02717v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chuyun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoqing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1">Fengping Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1">Bo Jin</a></p>
<p>Radiologists must utilize multiple modal images for tumor segmentation and
diagnosis due to the limitations of medical imaging and the diversity of tumor
signals. This leads to the development of multimodal learning in segmentation.
However, the redundancy among modalities creates challenges for existing
subtraction-based joint learning methods, such as misjudging the importance of
modalities, ignoring specific modal information, and increasing cognitive load.
These thorny issues ultimately decrease segmentation accuracy and increase the
risk of overfitting. This paper presents the complementary information mutual
learning (CIML) framework, which can mathematically model and address the
negative impact of inter-modal redundant information. CIML adopts the idea of
addition and removes inter-modal redundant information through inductive
bias-driven task decomposition and message passing-based redundancy filtering.
CIML first decomposes the multimodal segmentation task into multiple subtasks
based on expert prior knowledge, minimizing the information dependence between
modalities. Furthermore, CIML introduces a scheme in which each modality can
extract information from other modalities additively through message passing.
To achieve non-redundancy of extracted information, the redundant filtering is
transformed into complementary information learning inspired by the variational
information bottleneck. The complementary information learning procedure can be
efficiently solved by variational inference and cross-modal spatial attention.
Numerical results from the verification task and standard benchmarks indicate
that CIML efficiently removes redundant information between modalities,
outperforming SOTA methods regarding validation accuracy and segmentation
effect.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02719">Learning Image Demoireing from Unpaired Real Data. (arXiv:2401.02719v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yunshan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuyao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1">Fei Chao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1">Rongrong Ji</a></p>
<p>This paper focuses on addressing the issue of image demoireing. Unlike the
large volume of existing studies that rely on learning from paired real data,
we attempt to learn a demoireing model from unpaired real data, i.e., moire
images associated with irrelevant clean images. The proposed method, referred
to as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from
unpaired datasets, generating pairs with clean images for training demoireing
models. To achieve this, we divide real moire images into patches and group
them in compliance with their moire complexity. We introduce a novel moire
generation framework to synthesize moire images with diverse moire features,
resembling real moire patches, and details akin to real moire-free images.
Additionally, we introduce an adaptive denoise method to eliminate the
low-quality pseudo moire images that adversely impact the learning of
demoireing models. We conduct extensive experiments on the commonly-used FHDMi
and UHDM datasets. Results manifest that our UnDeM performs better than
existing methods when using existing demoireing models such as MBCNN and
ESDNet-L. Code: https://github.com/zysxmu/UnDeM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02726">Une ontologie pour les syst{\`e}mes multi-agents ambiants dans les villes intelligentes. (arXiv:2401.02726v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aky_N/0/1/0/all/0/1">Nathan Aky</a> (LIM), <a href="http://arxiv.org/find/cs/1/au:+Payet_D/0/1/0/all/0/1">Denis Payet</a> (LIM), <a href="http://arxiv.org/find/cs/1/au:+Giroux_S/0/1/0/all/0/1">Sylvain Giroux</a> (UdeS), <a href="http://arxiv.org/find/cs/1/au:+Courdier_R/0/1/0/all/0/1">R&#xe9;my Courdier</a> (LIM)</p>
<p>Towns and cities are currently equipping themselves with a host of connected
devices, with a view to transforming themselves into ''smart cities''. To
manage this mass of connected objects, autonomous software entities, known as
agents, can be attached to them to cooperate and use these devices to offer
personalized services. However, this object infrastructure needs to be
semantically structured in order to be exploited. This is why the proposal of
this article is an ontology, formatted in OWL, describing the object
infrastructures, their links with the organization of the multi-agent system
and the services to be delivered according to the users of the system. The
ontology is applied to smart mobility for people with reduced mobility, and
could be adapted to other smart city axes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02727">Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1">Hui Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Biwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1">Anjie Peng</a></p>
<p>Adversarial examples (AEs) have been extensively studied due to their
potential for privacy protection and inspiring robust neural networks. However,
making a targeted AE transferable across unknown models remains challenging. In
this paper, to alleviate the overfitting dilemma common in an AE crafted by
existing simple iterative attacks, we propose fine-tuning it in the feature
space. Specifically, starting with an AE generated by a baseline attack, we
encourage the features that contribute to the target class and discourage the
features that contribute to the original class in a middle layer of the source
model. Extensive experiments demonstrate that only a few iterations of
fine-tuning can boost existing attacks in terms of targeted transferability
nontrivially and universally. Our results also verify that the simple iterative
attacks can yield comparable or even better transferability than the
resource-intensive methods, which rely on training target-specific classifiers
or generators with additional data. The code is available at:
github.com/zengh5/TA_feature_FT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02731">Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoyuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Haisheng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Bei Yu</a></p>
<p>Large Language Models (LLMs) have demonstrated considerable proficiency in
general natural language processing (NLP) tasks. Instruction tuning, a
successful paradigm, enhances the ability of LLMs to follow natural language
instructions and exhibit robust generalization across a wide range of tasks.
However, these models often encounter performance limitations across multiple
tasks due to constrained model capacity. Expanding this capacity during the
instruction tuning phase poses significant challenges. To address this issue,
we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC),
which transitions dense models to sparse models using a Mixture of Experts
(MoE) architecture. PESC integrates adapters into the MoE layers of sparse
models, differentiating experts without altering the individual weights within
these layers. This method significantly reduces computational costs and GPU
memory requirements, facilitating model capacity expansion through a minimal
increase in parameters via the inserted adapters. Our empirical evaluation
demonstrates the effectiveness of the PESC method. Using PESC during
instruction tuning, our sparse models, dubbed Camelidae outperform all other
opensource sparse models and exhibit superior general capabilities compared to
GPT3.5.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02740">Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuxin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a></p>
<p>Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to
collaboratively train machine learning models without disclosing sensitive
private data. Existing FL research mostly focuses on the monopoly scenario in
which a single FL server selects a subset of FL clients to update their local
models in each round of training. In practice, there can be multiple FL servers
simultaneously trying to select clients from the same pool. In this paper, we
propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)
approach to bridge this gap. Based on Lyapunov optimization, it ensures fair
allocation of high-demand FL client datasets to FL jobs in need of them, by
jointly considering the current demand and the job payment bids, in order to
prevent prolonged waiting. Extensive experiments comparing FairFedJS against
four state-of-the-art approaches on two datasets demonstrate its significant
advantages. It outperforms the best baseline by 31.9% and 1.0% on average in
terms of scheduling fairness and convergence time, respectively, while
achieving comparable test accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02744">MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning. (arXiv:2401.02744v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fauzulhaq_A/0/1/0/all/0/1">Alfirsa Damasyifa Fauzulhaq</a>, <a href="http://arxiv.org/find/cs/1/au:+Parwitayasa_W/0/1/0/all/0/1">Wahyu Parwitayasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugihdharma_J/0/1/0/all/0/1">Joseph Ananda Sugihdharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ridhani_M/0/1/0/all/0/1">M. Fadli Ridhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1">Novanto Yudistira</a></p>
<p>Neuron labeling is an approach to visualize the behaviour and respond of a
certain neuron to a certain pattern that activates the neuron. Neuron labeling
extract information about the features captured by certain neurons in a deep
neural network, one of which uses the encoder-decoder image captioning
approach. The encoder used can be a pretrained CNN-based model and the decoder
is an RNN-based model for text generation. Previous work, namely MILAN (Mutual
Information-guided Linguistic Annotation of Neuron), has tried to visualize the
neuron behaviour using modified Show, Attend, and Tell (SAT) model in the
encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show
great result on short sequence neuron captioning, but it does not show great
result on long sequence neuron captioning, so in this work, we would like to
improve the performance of MILAN even more by utilizing different kind of
attention mechanism and additionally adding several attention result into one,
in order to combine all the advantages from several attention mechanism. Using
our compound dataset, we obtained higher BLEU and F1-Score on our proposed
model, achieving 17.742 and 0.4811 respectively. At some point where the model
converges at the peak, our model obtained BLEU of 21.2262 and BERTScore
F1-Score of 0.4870.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02749">Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jinnai_Y/0/1/0/all/0/1">Yuu Jinnai</a>, <a href="http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1">Kaito Ariu</a></p>
<p>Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to
beam search decoding for a wide range of text generation tasks. However, MBR
requires a huge amount of time for inference to compute the MBR objective,
which makes the method infeasible in many situations where response time is
critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently
been proposed to reduce the inference time in machine translation tasks.
Although it is shown to significantly reduce the amount of computation, it
requires hyperparameter tuning using a development set to be effective. To this
end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a
hyperparameter-free method to run MBR decoding approximately. AMBR is derived
from the observation that the problem of computing the sample-based MBR
objective is the medoid identification problem. AMBR uses the Correlated
Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best
approximation algorithm to date for the medoid identification problem, to
compute the sample-based MBR objective. We evaluate AMBR on machine
translation, text summarization, and image captioning tasks. The results show
that AMBR achieves on par with CBP, with CBP selecting hyperparameters through
an Oracle for each given computation budget.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02773">Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets. (arXiv:2401.02773v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pereira_J/0/1/0/all/0/1">Joao Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Chalatsis_D/0/1/0/all/0/1">Dimitrios Chalatsis</a>, <a href="http://arxiv.org/find/cs/1/au:+Hodossy_B/0/1/0/all/0/1">Balint Hodossy</a>, <a href="http://arxiv.org/find/cs/1/au:+Farina_D/0/1/0/all/0/1">Dario Farina</a></p>
<p>sEMG pattern recognition algorithms have been explored extensively in
decoding movement intent, yet are known to be vulnerable to changing recording
conditions, exhibiting significant drops in performance across subjects, and
even across sessions. Multi-channel surface EMG, also referred to as
high-density sEMG (HD-sEMG) systems, have been used to improve performance with
the information collected through the use of additional electrodes. However, a
lack of robustness is ever present due to limited datasets and the difficulties
in addressing sources of variability, such as electrode placement. In this
study, we propose training on a collection of input channel subsets and
augmenting our training distribution with data from different electrode
locations, simultaneously targeting electrode shift and reducing input
dimensionality. Our method increases robustness against electrode shift and
results in significantly higher intersession performance across subjects and
classification algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02777">From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Na Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liangyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1">Xiaoyu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1">Wei Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kaijiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Ming Cui</a></p>
<p>This paper introduces RAISE (Reasoning and Acting through Scratchpad and
Examples), an advanced architecture enhancing the integration of Large Language
Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of
the ReAct framework, incorporates a dual-component memory system, mirroring
human short-term and long-term memory, to maintain context and continuity in
conversations. It entails a comprehensive agent construction scenario,
including phases like Conversation Selection, Scene Extraction, CoT Completion,
and Scene Augmentation, leading to the LLMs Training phase. This approach
appears to enhance agent controllability and adaptability in complex,
multi-turn dialogues. Our preliminary evaluations in a real estate sales
context suggest that RAISE has some advantages over traditional agents,
indicating its potential for broader applications. This work contributes to the
AI field by providing a robust framework for developing more context-aware and
versatile conversational agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02797">PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jinlong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pengfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zixu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1">Shenjun Zhong</a></p>
<p>Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs to
predict free-form answers as a generative task to solve medical visual question
answering (Med-VQA) tasks. In this paper, we propose a parameter efficient
framework for fine-tuning MLLM specifically tailored to Med-VQA applications,
and empirically validate it on a public benchmark dataset. To accurately
measure the performance, we employ human evaluation and the results reveal that
our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v
model by a significant margin of 26% absolute accuracy on closed-ended
questions. The code will be available here: https://github.com/jinlHe/PeFoMed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02810">Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mustajab_A/0/1/0/all/0/1">Abdul Hannan Mustajab</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1">Hao Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizvi_Z/0/1/0/all/0/1">Zarghaam Rizvi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wuttke_F/0/1/0/all/0/1">Frank Wuttke</a></p>
<p>Physics-informed neural network (PINN) is a data-driven solver for partial
and ordinary differential equations(ODEs/PDEs). It provides a unified framework
to address both forward and inverse problems. However, the complexity of the
objective function often leads to training failures. This issue is particularly
prominent when solving high-frequency and multi-scale problems. We proposed
using transfer learning to boost the robustness and convergence of training
PINN, starting training from low-frequency problems and gradually approaching
high-frequency problems. Through two case studies, we discovered that transfer
learning can effectively train PINN to approximate solutions from low-frequency
problems to high-frequency problems without increasing network parameters.
Furthermore, it requires fewer data points and less training time. We
elaborately described our training strategy, including optimizer selection, and
suggested guidelines for using transfer learning to train neural networks for
solving more complex problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02838">CrisisViT: A Robust Vision Transformer for Crisis Image Classification. (arXiv:2401.02838v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1">Zijun Long</a>, <a href="http://arxiv.org/find/cs/1/au:+McCreadie_R/0/1/0/all/0/1">Richard McCreadie</a>, <a href="http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1">Muhammad Imran</a></p>
<p>In times of emergency, crisis response agencies need to quickly and
accurately assess the situation on the ground in order to deploy relevant
services and resources. However, authorities often have to make decisions based
on limited information, as data on affected regions can be scarce until local
response services can provide first-hand reports. Fortunately, the widespread
availability of smartphones with high-quality cameras has made citizen
journalism through social media a valuable source of information for crisis
responders. However, analyzing the large volume of images posted by citizens
requires more time and effort than is typically available. To address this
issue, this paper proposes the use of state-of-the-art deep neural models for
automatic image classification/tagging, specifically by adapting
transformer-based architectures for crisis image classification (CrisisViT). We
leverage the new Incidents1M crisis image dataset to develop a range of new
transformer-based image classification models. Through experimentation over the
standard Crisis image benchmark dataset, we demonstrate that the CrisisViT
models significantly outperform previous approaches in emergency type, image
relevance, humanitarian category, and damage severity classification.
Additionally, we show that the new Incidents1M dataset can further augment the
CrisisViT models resulting in an additional 1.25% absolute accuracy gain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02839">Pheme: Efficient and Conversational Speech Generation. (arXiv:2401.02839v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Budzianowski_P/0/1/0/all/0/1">Pawe&#x142; Budzianowski</a>, <a href="http://arxiv.org/find/eess/1/au:+Sereda_T/0/1/0/all/0/1">Taras Sereda</a>, <a href="http://arxiv.org/find/eess/1/au:+Cichy_T/0/1/0/all/0/1">Tomasz Cichy</a>, <a href="http://arxiv.org/find/eess/1/au:+Vulic_I/0/1/0/all/0/1">Ivan Vuli&#x107;</a></p>
<p>In recent years, speech generation has seen remarkable progress, now
achieving one-shot generation capability that is often virtually
indistinguishable from real human voice. Integrating such advancements in
speech generation with large language models might revolutionize a wide range
of applications. However, certain applications, such as assistive
conversational systems, require natural and conversational speech generation
tools that also operate efficiently in real time. Current state-of-the-art
models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,
require large neural components and extensive training data to work well. In
contrast, MQTTS aims to build more compact conversational TTS models while
capitalizing on smaller-scale real-life conversational speech data. However,
its autoregressive nature yields high inference latency and thus limits its
real-time usage. In order to mitigate the current limitations of the
state-of-the-art TTS models while capitalizing on their strengths, in this work
we introduce the Pheme model series that 1) offers compact yet high-performing
models, 2) allows for parallel speech generation of 3) natural conversational
speech, and 4) it can be trained efficiently on smaller-scale conversational
data, cutting data demands by more than 10x but still matching the quality of
the autoregressive TTS models. We also show that through simple teacher-student
distillation we can meet significant improvements in voice quality for
single-speaker setups on top of pretrained Pheme checkpoints, relying solely on
synthetic speech generated by much larger teacher models. Audio samples and
pretrained models are available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02843">Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grace_K/0/1/0/all/0/1">Katja Grace</a>, <a href="http://arxiv.org/find/cs/1/au:+Stewart_H/0/1/0/all/0/1">Harlan Stewart</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandkuhler_J/0/1/0/all/0/1">Julia Fabienne Sandk&#xfc;hler</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1">Stephen Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinstein_Raun_B/0/1/0/all/0/1">Ben Weinstein-Raun</a>, <a href="http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1">Jan Brauner</a></p>
<p>In the largest survey of its kind, 2,778 researchers who had published in
top-tier artificial intelligence (AI) venues gave predictions on the pace of AI
progress and the nature and impacts of advanced AI systems The aggregate
forecasts give at least a 50% chance of AI systems achieving several milestones
by 2028, including autonomously constructing a payment processing site from
scratch, creating a song indistinguishable from a new song by a popular
musician, and autonomously downloading and fine-tuning a large language model.
If science continues undisrupted, the chance of unaided machines outperforming
humans in every possible task was estimated at 10% by 2027, and 50% by 2047.
The latter estimate is 13 years earlier than that reached in a similar survey
we conducted only one year earlier [Grace et al., 2022]. However, the chance of
all human occupations becoming fully automatable was forecast to reach 10% by
2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).
</p>
<p>Most respondents expressed substantial uncertainty about the long-term value
of AI progress: While 68.3% thought good outcomes from superhuman AI are more
likely than bad, of these net optimists 48% gave at least a 5% chance of
extremely bad outcomes such as human extinction, and 59% of net pessimists gave
5% or more to extremely good outcomes. Between 38% and 51% of respondents gave
at least a 10% chance to advanced AI leading to outcomes as bad as human
extinction. More than half suggested that "substantial" or "extreme" concern is
warranted about six different AI-related scenarios, including misinformation,
authoritarian control, and inequality. There was disagreement about whether
faster or slower AI progress would be better for the future of humanity.
However, there was broad agreement that research aimed at minimizing potential
risks from AI systems ought to be prioritized more.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02851">Generative Large Language Models are autonomous practitioners of evidence-based medicine. (arXiv:2401.02851v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vaid_A/0/1/0/all/0/1">Akhil Vaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampert_J/0/1/0/all/0/1">Joshua Lampert</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Juhee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawant_A/0/1/0/all/0/1">Ashwin Sawant</a>, <a href="http://arxiv.org/find/cs/1/au:+Apakama_D/0/1/0/all/0/1">Donald Apakama</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakhuja_A/0/1/0/all/0/1">Ankit Sakhuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Soroush_A/0/1/0/all/0/1">Ali Soroush</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Denise Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Landi_I/0/1/0/all/0/1">Isotta Landi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bussola_N/0/1/0/all/0/1">Nicole Bussola</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabeel_I/0/1/0/all/0/1">Ismail Nabeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Freeman_R/0/1/0/all/0/1">Robbie Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovatch_P/0/1/0/all/0/1">Patricia Kovatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Carr_B/0/1/0/all/0/1">Brendan Carr</a>, <a href="http://arxiv.org/find/cs/1/au:+Glicksberg_B/0/1/0/all/0/1">Benjamin Glicksberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Argulian_E/0/1/0/all/0/1">Edgar Argulian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lerakis_S/0/1/0/all/0/1">Stamatios Lerakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kraft_M/0/1/0/all/0/1">Monica Kraft</a>, <a href="http://arxiv.org/find/cs/1/au:+Charney_A/0/1/0/all/0/1">Alexander Charney</a>, <a href="http://arxiv.org/find/cs/1/au:+Nadkarni_G/0/1/0/all/0/1">Girish Nadkarni</a></p>
<p>Background: Evidence-based medicine (EBM) is fundamental to modern clinical
practice, requiring clinicians to continually update their knowledge and apply
the best clinical evidence in patient care. The practice of EBM faces
challenges due to rapid advancements in medical research, leading to
information overload for clinicians. The integration of artificial intelligence
(AI), specifically Generative Large Language Models (LLMs), offers a promising
solution towards managing this complexity.
</p>
<p>Methods: This study involved the curation of real-world clinical cases across
various specialties, converting them into .json files for analysis. LLMs,
including proprietary models like ChatGPT 3.5 and 4, Gemini Pro, and
open-source models like LLaMA v2 and Mixtral-8x7B, were employed. These models
were equipped with tools to retrieve information from case files and make
clinical decisions similar to how clinicians must operate in the real world.
Model performance was evaluated based on correctness of final answer, judicious
use of tools, conformity to guidelines, and resistance to hallucinations.
</p>
<p>Results: GPT-4 was most capable of autonomous operation in a clinical
setting, being generally more effective in ordering relevant investigations and
conforming to clinical guidelines. Limitations were observed in terms of model
ability to handle complex guidelines and diagnostic nuances. Retrieval
Augmented Generation made recommendations more tailored to patients and
healthcare systems.
</p>
<p>Conclusions: LLMs can be made to function as autonomous practitioners of
evidence-based medicine. Their ability to utilize tooling can be harnessed to
interact with the infrastructure of a real-world healthcare system and perform
the tasks of patient management in a guideline directed manner. Prompt
engineering may help to further enhance this potential and transform healthcare
for the clinician and the patient.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02860">Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chinpattanakarn_N/0/1/0/all/0/1">Naaek Chinpattanakarn</a>, <a href="http://arxiv.org/find/cs/1/au:+Amornbunchornvej_C/0/1/0/all/0/1">Chainarong Amornbunchornvej</a></p>
<p>Knowing who follows whom and what patterns they are following are crucial
steps to understand collective behaviors (e.g. a group of human, a school of
fish, or a stock market). Time series is one of resources that can be used to
get insight regarding following relations. However, the concept of following
patterns or motifs and the solution to find them in time series are not
obvious. In this work, we formalize a concept of following motifs between two
time series and present a framework to infer following patterns between two
time series. The framework utilizes one of efficient and scalable methods to
retrieve motifs from time series called the Matrix Profile Method. We compare
our proposed framework with several baselines. The framework performs better
than baselines in the simulation datasets. In the dataset of sound recording,
the framework is able to retrieve the following motifs within a pair of time
series that two singers sing following each other. In the cryptocurrency
dataset, the framework is capable of capturing the following motifs within a
pair of time series from two digital currencies, which implies that the values
of one currency follow the values of another currency patterns. Our framework
can be utilized in any field of time series to get insight regarding following
patterns between time series.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02863">A Customizable Generator for Comic-Style Visual Narrative. (arXiv:2401.02863v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jhala_A/0/1/0/all/0/1">Arnav Jhala</a></p>
<p>We present a theory-inspired visual narrative generator that incorporates
comic-authoring idioms, which transfers the conceptual principles of comics
into system layers that integrate the theories to create comic content. The
generator creates comics through sequential decision-making across layers from
panel composition, object positions, panel transitions, and narrative elements.
Each layer's decisions are based on narrative goals and follow the respective
layer idioms of the medium. Cohn's narrative grammar provides the overall story
arc. Photographic compositions inspired by the rule of thirds is used to
provide panel compositions. McCloud's proposed panel transitions based on focus
shifts between scene, character, and temporal changes are encoded in the
transition layer. Finally, common overlay symbols (such as the exclamation) are
added based on analyzing action verbs using an action-verb ontology. We
demonstrate the variety of generated comics through various settings with
example outputs. The generator and associated modules could be a useful system
for visual narrative authoring and for further research into computational
models of visual narrative understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02870">AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models. (arXiv:2401.02870v1 [cs.MA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zihong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changwang Zhang</a></p>
<p>The evolution of Large Language Models (LLMs) has introduced a new paradigm
for investigating human behavior emulation. Recent research has employed
LLM-based Agents to create a sociological research environment, in which agents
exhibit behavior based on the unfiltered characteristics of large language
models. However, these studies overlook the iterative development within a
human-like setting - Human preferences and personalities are complex, shaped by
various factors and subject to ongoing change as a result of environmental and
subjective influences. In light of this observation, we propose Agent Framework
for Shaping Preference and Personality (AFSPP), exploring the multifaceted
impact of social networks and subjective consciousness on LLM-based Agents'
preference and personality formation. With AFSPP, we have, for the first time,
successfully replicated several key findings from human personality
experiments. And other AFSPP-based experimental results indicate that plan
making, sensory perceptions and social networking with subjective information,
wield the most pronounced influence on preference shaping. AFSPP can
significantly enhance the efficiency and scope of psychological experiments,
while yielding valuable insights for Trustworthy Artificial Intelligence
research for strategies to prevent undesirable preference and personality
development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02873">Optimal Chaining of Vehicle Plans with Time Windows. (arXiv:2401.02873v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Fiedler_D/0/1/0/all/0/1">David Fiedler</a>, <a href="http://arxiv.org/find/math/1/au:+Difonzo_F/0/1/0/all/0/1">Fabio V. Difonzo</a>, <a href="http://arxiv.org/find/math/1/au:+Mrkos_J/0/1/0/all/0/1">Jan Mrkos</a></p>
<p>For solving problems from the domain of vehicle routing with time windows, we
often need to connect vehicle plans into sequences spanning a longer time
horizon or, in other words, we need to perform a plan chaining. Recently, a
network-based solution has been proposed to solve the fleet-sizing problem. The
method, however, does not consider the time flexibility of the plans, an
essential property of all vehicle routing problems with time windows. Instead,
plans have fixed times and cannot be delayed. This work presents a new problem
formulation that considers delays in line with the given time windows and a
method that can be used to solve it. Moreover, we prove that the method is
optimal, and we analyze its complexity. Finally, we list some practical
applications and perform a demonstration for one of them: the method for
solving the static Dial-a-ride problem. The demonstration results show that for
a significant number of instances, the proposed method provides a better
solution than the other two heuristic baseline methods we have evaluated, while
not having the largest computational time requirements.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02884">MsDC-DEQ-Net: Deep Equilibrium Model (DEQ) with Multi-scale Dilated Convolution for Image Compressive Sensing (CS). (arXiv:2401.02884v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1">Youhao Yu</a>, <a href="http://arxiv.org/find/eess/1/au:+Dansereau_R/0/1/0/all/0/1">Richard M. Dansereau</a></p>
<p>Compressive sensing (CS) is a technique that enables the recovery of sparse
signals using fewer measurements than traditional sampling methods. To address
the computational challenges of CS reconstruction, our objective is to develop
an interpretable and concise neural network model for reconstructing natural
images using CS. We achieve this by mapping one step of the iterative shrinkage
thresholding algorithm (ISTA) to a deep network block, representing one
iteration of ISTA. To enhance learning ability and incorporate structural
diversity, we integrate aggregated residual transformations (ResNeXt) and
squeeze-and-excitation (SE) mechanisms into the ISTA block. This block serves
as a deep equilibrium layer, connected to a semi-tensor product network
(STP-Net) for convenient sampling and providing an initial reconstruction. The
resulting model, called MsDC-DEQ-Net, exhibits competitive performance compared
to state-of-the-art network-based methods. It significantly reduces storage
requirements compared to deep unrolling methods, using only one iteration block
instead of multiple iterations. Unlike deep unrolling models, MsDC-DEQ-Net can
be iteratively used, gradually improving reconstruction accuracy while
considering computation trade-offs. Additionally, the model benefits from
multi-scale dilated convolutions, further enhancing performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02905">H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Haidong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaw_N/0/1/0/all/0/1">Nathan Gaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yinan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnstone_C/0/1/0/all/0/1">Chancellor Johnstone</a>, <a href="http://arxiv.org/find/cs/1/au:+Beauchene_C/0/1/0/all/0/1">Christine Beauchene</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuditskaya_S/0/1/0/all/0/1">Sophia Yuditskaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1">Hrishikesh Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1">Chun-An Chou</a></p>
<p>Discovering human cognitive and emotional states using multi-modal
physiological signals draws attention across various research applications.
Physiological responses of the human body are influenced by human cognition and
commonly used to analyze cognitive states. From a network science perspective,
the interactions of these heterogeneous physiological modalities in a graph
structure may provide insightful information to support prediction of cognitive
states. However, there is no clue to derive exact connectivity between
heterogeneous modalities and there exists a hierarchical structure of
sub-modalities. Existing graph neural networks are designed to learn on
non-hierarchical homogeneous graphs with pre-defined graph structures; they
failed to learn from hierarchical, multi-modal physiological data without a
pre-defined graph structure. To this end, we propose a hierarchical
heterogeneous graph generative network (H2G2-Net) that automatically learns a
graph structure without domain knowledge, as well as a powerful representation
on the hierarchical heterogeneous graph in an end-to-end fashion. We validate
the proposed method on the CogPilot dataset that consists of multi-modal
physiological signals. Extensive experiments demonstrate that our proposed
method outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02920">Analytically-Driven Resource Management for Cloud-Native Microservices. (arXiv:2401.02920v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhuangzhuang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Elnikety_S/0/1/0/all/0/1">Sameh Elnikety</a>, <a href="http://arxiv.org/find/cs/1/au:+Delimitrou_C/0/1/0/all/0/1">Christina Delimitrou</a></p>
<p>Resource management for cloud-native microservices has attracted a lot of
recent attention. Previous work has shown that machine learning (ML)-driven
approaches outperform traditional techniques, such as autoscaling, in terms of
both SLA maintenance and resource efficiency. However, ML-driven approaches
also face challenges including lengthy data collection processes and limited
scalability. We present Ursa, a lightweight resource management system for
cloud-native microservices that addresses these challenges. Ursa uses an
analytical model that decomposes the end-to-end SLA into per-service SLA, and
maps per-service SLA to individual resource allocations per microservice tier.
To speed up the exploration process and avoid prolonged SLA violations, Ursa
explores each microservice individually, and swiftly stops exploration if
latency exceeds its SLA.
</p>
<p>We evaluate Ursa on a set of representative and end-to-end microservice
topologies, including a social network, media service and video processing
pipeline, each consisting of multiple classes and priorities of requests with
different SLAs, and compare it against two representative ML-driven systems,
Sinan and Firm. Compared to these ML-driven approaches, Ursa provides
significant advantages: It shortens the data collection process by more than
128x, and its control plane is 43x faster than ML-driven approaches. At the
same time, Ursa does not sacrifice resource efficiency or SLAs. During online
deployment, Ursa reduces the SLA violation rate by 9.0% up to 49.9%, and
reduces CPU allocation by up to 86.2% compared to ML-driven approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02941">Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nananukul_N/0/1/0/all/0/1">Navapat Nananukul</a>, <a href="http://arxiv.org/find/cs/1/au:+Soltanian_zadeh_H/0/1/0/all/0/1">Hamid Soltanian-zadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a></p>
<p>Automatic semantic segmentation of magnetic resonance imaging (MRI) images
using deep neural networks greatly assists in evaluating and planning
treatments for various clinical applications. However, training these models is
conditioned on the availability of abundant annotated data to implement the
end-to-end supervised learning procedure. Even if we annotate enough data, MRI
images display considerable variability due to factors such as differences in
patients, MRI scanners, and imaging protocols. This variability necessitates
retraining neural networks for each specific application domain, which, in
turn, requires manual annotation by expert radiologists for all new domains. To
relax the need for persistent data annotation, we develop a method for
unsupervised federated domain adaptation using multiple annotated source
domains. Our approach enables the transfer of knowledge from several annotated
source domains to adapt a model for effective use in an unannotated target
domain. Initially, we ensure that the target domain data shares similar
representations with each source domain in a latent embedding space, modeled as
the output of a deep encoder, by minimizing the pair-wise distances of the
distributions for the target domain and the source domains. We then employ an
ensemble approach to leverage the knowledge obtained from all domains. We
provide theoretical analysis and perform experiments on the MICCAI 2016
multi-site dataset to demonstrate our method is effective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02949">Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rute_J/0/1/0/all/0/1">Jason Rute</a>, <a href="http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1">Miroslav Ol&#x161;&#xe1;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaauwbroek_L/0/1/0/all/0/1">Lasse Blaauwbroek</a>, <a href="http://arxiv.org/find/cs/1/au:+Massolo_F/0/1/0/all/0/1">Fidel Ivan Schaposnik Massolo</a>, <a href="http://arxiv.org/find/cs/1/au:+Piepenbrock_J/0/1/0/all/0/1">Jelle Piepenbrock</a>, <a href="http://arxiv.org/find/cs/1/au:+Pestun_V/0/1/0/all/0/1">Vasily Pestun</a></p>
<p>Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users' workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.02954">DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DeepSeek-AI/0/1/0/all/0/1">DeepSeek-AI</a>: <a href="http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1">Xiao Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Deli Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guanting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shanhuang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Damai Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chengqi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Honghui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_K/0/1/0/all/0/1">Kai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1">Qiushi Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zhe Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Huazuo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1">Kaige Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Wenjun Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1">Ruiqi Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_K/0/1/0/all/0/1">Kang Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daya Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jianzhong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1">Guangbo Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1">Zhewen Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Ying He</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenjie Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Panpan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1">Erhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiashi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Y.K. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1">Wenfeng Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Fangyun Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">A.X. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaodong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Haoyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shanghao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fuli Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shirong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1">Xiaotao Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_T/0/1/0/all/0/1">Tian Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Piao_Y/0/1/0/all/0/1">Yishi Piao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Junjie Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1">Hui Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1">Tongzheng Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zehui Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_C/0/1/0/all/0/1">Chong Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_Z/0/1/0/all/0/1">Zhangli Sha</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhihong Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Junxiao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xuecheng Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jingxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yaofeng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Minghui Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bingxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaohui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongji Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Y. Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhenda Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Ziwei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yiliang Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hanwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">R.X. Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yanhong Xu</a>, et al. (18 additional authors not shown)</p>
<p>The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.04987">AutoGL: A Library for Automated Graph Learning. (arXiv:2104.04987v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yijian Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1">Chaoyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jie Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Heng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jiyan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1">Zixin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1">Beini Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a></p>
<p>Recent years have witnessed an upsurge in research interests and applications
of machine learning on graphs. However, manually designing the optimal machine
learning algorithms for different graph datasets and tasks is inflexible,
labor-intensive, and requires expert knowledge, limiting its adaptivity and
applicability. Automated machine learning (AutoML) on graphs, aiming to
automatically design the optimal machine learning algorithm for a given graph
dataset and task, has received considerable attention. However, none of the
existing libraries can fully support AutoML on graphs. To fill this gap, we
present Automated Graph Learning (AutoGL), the first dedicated library for
automated machine learning on graphs. AutoGL is open-source, easy to use, and
flexible to be extended. Specifically, we propose a three-layer architecture,
consisting of backends to interface with devices, a complete automated graph
learning pipeline, and supported graph applications. The automated machine
learning pipeline further contains five functional modules: auto feature
engineering, neural architecture search, hyper-parameter optimization, model
training, and auto ensemble, covering the majority of existing AutoML methods
on graphs. For each module, we provide numerous state-of-the-art methods and
flexible base classes and APIs, which allow easy usage and customization. We
further provide experimental results to showcase the usage of our AutoGL
library. We also present AutoGL-light, a lightweight version of AutoGL to
facilitate customizing pipelines and enriching applications, as well as
benchmarks for graph neural architecture search. The codes of AutoGL are
publicly available at https://github.com/THUMNLab/AutoGL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.00282">Stabilizing Spiking Neuron Training. (arXiv:2202.00282v4 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herranz_Celotti_L/0/1/0/all/0/1">Luca Herranz-Celotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1">Jean Rouat</a></p>
<p>Stability arguments are often used to prevent learning algorithms from having
ever increasing activity and weights that hinder generalization. However,
stability conditions can clash with the sparsity required to augment the energy
efficiency of spiking neurons. Nonetheless it can also provide solutions. In
fact, spiking Neuromorphic Computing uses binary activity to improve Artificial
Intelligence energy efficiency. However, its non-smoothness requires
approximate gradients, known as Surrogate Gradients (SG), to close the
performance gap with Deep Learning. Several SG have been proposed in the
literature, but it remains unclear how to determine the best SG for a given
task and network. Thus, we aim at theoretically define the best SG, through
stability arguments, to reduce the need for grid search. In fact, we show that
more complex tasks and networks need more careful choice of SG, even if overall
the derivative of the fast sigmoid tends to outperform the other, for a wide
range of learning rates. We therefore design a stability based theoretical
method to choose initialization and SG shape before training on the most common
spiking neuron, the Leaky Integrate and Fire (LIF). Since our stability method
suggests the use of high firing rates at initialization, which is non-standard
in the neuromorphic literature, we show that high initial firing rates,
combined with a sparsity encouraging loss term introduced gradually, can lead
to better generalization, depending on the SG shape. Our stability based
theoretical solution, finds a SG and initialization that experimentally result
in improved accuracy. We show how it can be used to reduce the need of
extensive grid-search of dampening, sharpness and tail-fatness of the SG. We
also show that our stability concepts can be extended to be applicable on
different LIF variants, such as DECOLLE and fluctuations-driven
initializations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.09893">Variational Quantum and Quantum-Inspired Clustering. (arXiv:2206.09893v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Bermejo_P/0/1/0/all/0/1">Pablo Bermejo</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Orus_R/0/1/0/all/0/1">Roman Orus</a></p>
<p>Here we present a quantum algorithm for clustering data based on a
variational quantum circuit. The algorithm allows to classify data into many
clusters, and can easily be implemented in few-qubit Noisy Intermediate-Scale
Quantum (NISQ) devices. The idea of the algorithm relies on reducing the
clustering problem to an optimization, and then solving it via a Variational
Quantum Eigensolver (VQE) combined with non-orthogonal qubit states. In
practice, the method uses maximally-orthogonal states of the target Hilbert
space instead of the usual computational basis, allowing for a large number of
clusters to be considered even with few qubits. We benchmark the algorithm with
numerical simulations using real datasets, showing excellent performance even
with one single qubit. Moreover, a tensor network simulation of the algorithm
implements, by construction, a quantum-inspired clustering algorithm that can
run on current classical hardware.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.12735">Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration. (arXiv:2211.12735v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yunjie Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lingxi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1">Jihao Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jianbin Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1">Qi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1">Qixiang Ye</a></p>
<p>We propose integrally pre-trained transformer pyramid network (iTPN), towards
jointly optimizing the network backbone and the neck, so that transfer gap
between representation models and downstream tasks is minimal. iTPN is born
with two elaborated designs: 1) The first pre-trained feature pyramid upon
vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid
using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing
computational memory overhead and accelerating inference through two flexible
designs. 1) Token migration: dropping redundant tokens of the backbone while
replenishing them in the feature pyramid without attention operations. 2) Token
gathering: reducing computation cost caused by global attention by introducing
few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1
accuracy on ImageNet-1K. With 1x training schedule using DINO, the
base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object
detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using
MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with
negligible performance loss, demonstrating the potential to be a powerful
backbone for downstream vision tasks. The code is available at:
github.com/sunsmarterjie/iTPN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.06683">Surgical Aggregation: Federated Class-Heterogeneous Learning. (arXiv:2301.06683v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1">Pranav Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanhere_A/0/1/0/all/0/1">Adway Kanhere</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1">Paul H. Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1">Vishwa S. Parekh</a></p>
<p>The release of numerous chest x-ray datasets has spearheaded the development
of deep learning models with expert-level performance. However, they have
limited interoperability due to class-heterogeneity -- a result of inconsistent
labeling schemes and partial annotations. Therefore, it is challenging to
leverage these datasets in aggregate to train models with a complete
representation of abnormalities that may occur within the thorax. In this work,
we propose surgical aggregation, a federated learning framework for aggregating
knowledge from class-heterogeneous datasets and learn a model that can
simultaneously predict the presence of all disease labels present across the
datasets. We evaluate our method using simulated and real-world
class-heterogeneous datasets across both independent and identically
distributed (iid) and non-iid settings. Our results show that surgical
aggregation outperforms current methods, has better generalizability, and is a
crucial first step towards tackling class-heterogeneity in federated learning
to facilitate the development of clinically-useful models using previously
non-interoperable chest x-ray datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05292">MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive Impairment in older adults using facial videos. (arXiv:2304.05292v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dodge_H/0/1/0/all/0/1">Hiroko H. Dodge</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahoor_M/0/1/0/all/0/1">Mohammad H. Mahoor</a></p>
<p>Deep machine learning models including Convolutional Neural Networks (CNN)
have been successful in the detection of Mild Cognitive Impairment (MCI) using
medical images, questionnaires, and videos. This paper proposes a novel
Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to
distinguish MCI from those with normal cognition by analyzing facial features.
The data comes from the I-CONECT, a behavioral intervention trial aimed at
improving cognitive function by providing frequent video chats. MC-ViViT
extracts spatiotemporal features of videos in one branch and augments
representations by the MC module. The I-CONECT dataset is challenging as the
dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which
impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy
and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE
loss to address the imbalanced problem. Our experimental results on the
I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a
high accuracy of 90.63% accuracy on some of the interview videos.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12711">Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement. (arXiv:2305.12711v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">De Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaojian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lingfeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhihui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a></p>
<p>Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13301">Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Black_K/0/1/0/all/0/1">Kevin Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1">Michael Janner</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yilun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostrikov_I/0/1/0/all/0/1">Ilya Kostrikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>Diffusion models are a class of flexible generative models trained with an
approximation to the log-likelihood objective. However, most use cases of
diffusion models are not concerned with likelihoods, but instead with
downstream objectives such as human-perceived image quality or drug
effectiveness. In this paper, we investigate reinforcement learning methods for
directly optimizing diffusion models for such objectives. We describe how
posing denoising as a multi-step decision-making problem enables a class of
policy gradient algorithms, which we refer to as denoising diffusion policy
optimization (DDPO), that are more effective than alternative reward-weighted
likelihood approaches. Empirically, DDPO is able to adapt text-to-image
diffusion models to objectives that are difficult to express via prompting,
such as image compressibility, and those derived from human feedback, such as
aesthetic quality. Finally, we show that DDPO can improve prompt-image
alignment using feedback from a vision-language model without the need for
additional data collection or human annotation. The project's website can be
found at <a href="http://rl-diffusion.github.io">this http URL</a> .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19787">DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation. (arXiv:2305.19787v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1">Xianwei Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Persello_C/0/1/0/all/0/1">Claudio Persello</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wangbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ming_D/0/1/0/all/0/1">Dongping Ming</a>, <a href="http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1">Alfred Stein</a></p>
<p>Image segmentation aims to partition an image according to the objects in the
scene and is a fundamental step in analysing very high spatial-resolution (VHR)
remote sensing imagery. Current methods struggle to effectively consider land
objects with diverse shapes and sizes. Additionally, the determination of
segmentation scale parameters frequently adheres to a static and empirical
doctrine, posing limitations on the segmentation of large-scale remote sensing
images and yielding algorithms with limited interpretability. To address the
above challenges, we propose a deep-learning-based region merging method dubbed
DeepMerge to handle the segmentation of complete objects in large VHR images by
integrating deep learning and region adjacency graph (RAG). This is the first
method to use deep learning to learn the similarity and merge similar adjacent
super-pixels in RAG. We propose a modified binary tree sampling method to
generate shift-scale data, serving as inputs for transformer-based deep
learning networks, a shift-scale attention with 3-Dimension relative position
embedding to learn features across scales, and an embedding to fuse learned
features with hand-crafted features. DeepMerge can achieve high segmentation
accuracy in a supervised manner from large-scale remotely sensed images and
provides an interpretable optimal scale parameter, which is validated using a
remote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The
experimental results show that DeepMerge achieves the highest F value (0.9550)
and the lowest total error TE (0.0895), correctly segmenting objects of
different sizes and outperforming all competing segmentation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01774">RE-centric Recommendations for the Development of Trustworthy(er) Autonomous Systems. (arXiv:2306.01774v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ronanki_K/0/1/0/all/0/1">Krishna Ronanki</a>, <a href="http://arxiv.org/find/cs/1/au:+Cabrero_Daniel_B/0/1/0/all/0/1">Beatriz Cabrero-Daniel</a>, <a href="http://arxiv.org/find/cs/1/au:+Horkoff_J/0/1/0/all/0/1">Jennifer Horkoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1">Christian Berger</a></p>
<p>Complying with the EU AI Act (AIA) guidelines while developing and
implementing AI systems will soon be mandatory within the EU. However,
practitioners lack actionable instructions to operationalise ethics during AI
systems development. A literature review of different ethical guidelines
revealed inconsistencies in the principles addressed and the terminology used
to describe them. Furthermore, requirements engineering (RE), which is
identified to foster trustworthiness in the AI development process from the
early stages was observed to be absent in a lot of frameworks that support the
development of ethical and trustworthy AI. This incongruous phrasing combined
with a lack of concrete development practices makes trustworthy AI development
harder. To address this concern, we formulated a comparison table for the
terminology used and the coverage of the ethical AI principles in major ethical
AI guidelines. We then examined the applicability of ethical AI development
frameworks for performing effective RE during the development of trustworthy AI
systems. A tertiary review and meta-analysis of literature discussing ethical
AI frameworks revealed their limitations when developing trustworthy AI. Based
on our findings, we propose recommendations to address such limitations during
the development of trustworthy AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11698">DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weixin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1">Hengzhi Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chulin Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Mintong Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chenhui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chejian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1">Zidi Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_R/0/1/0/all/0/1">Ritik Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1">Rylan Schaeffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1">Sang T. Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1">Simran Arora</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1">Mantas Mazeika</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1">Dan Hendrycks</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zinan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1">Sanmi Koyejo</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1">Dawn Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a></p>
<p>Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/; our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of
this work is at https://openreview.net/pdf?id=kaHpo8OZw2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09564">Reinforcement Learning and Data-Generation for Syntax-Guided Synthesis. (arXiv:2307.09564v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Parsert_J/0/1/0/all/0/1">Julian Parsert</a>, <a href="http://arxiv.org/find/cs/1/au:+Polgreen_E/0/1/0/all/0/1">Elizabeth Polgreen</a></p>
<p>Program synthesis is the task of automatically generating code based on a
specification. In Syntax-Guided Synthesis (SyGuS) this specification is a
combination of a syntactic template and a logical formula, and the result is
guaranteed to satisfy both.
</p>
<p>We present a reinforcement-learning guided algorithm for SyGuS which uses
Monte-Carlo Tree Search (MCTS) to search the space of candidate solutions. Our
algorithm learns policy and value functions which, combined with the upper
confidence bound for trees, allow it to balance exploration and exploitation. A
common challenge in applying machine learning approaches to syntax-guided
synthesis is the scarcity of training data. To address this, we present a
method for automatically generating training data for SyGuS based on
anti-unification of existing first-order satisfiability problems, which we use
to train our MCTS policy. We implement and evaluate this setup and demonstrate
that learned policy and value improve the synthesis performance over a baseline
by over 26 percentage points in the training and testing sets. Our tool
outperforms state-of-the-art tool cvc5 on the training set and performs
comparably in terms of the total number of problems solved on the testing set
(solving 23% of the benchmarks on which cvc5 fails). We make our data set
publicly available, to enable further application of machine learning methods
to the SyGuS problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12547">Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1">Palash Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolay_S/0/1/0/all/0/1">Sudeshna Kolay</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sipra Singh</a></p>
<p>We study the knapsack problem with graph theoretic constraints. That is, we
assume that there exists a graph structure on the set of items of knapsack and
the solution also needs to satisfy certain graph theoretic properties on top of
knapsack constraints. In particular, we need to compute in the connected
knapsack problem a connected subset of items which has maximum value subject to
the size of knapsack constraint. We show that this problem is strongly
NP-complete even for graphs of maximum degree four and NP-complete even for
star graphs. On the other hand, we develop an algorithm running in time
$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$
are respectively treewidth of the graph, size, and target value of the
knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm
running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for
every $\epsilon&gt;0$. We show similar results for several other graph theoretic
properties, namely path and shortest-path under the problem names path-knapsack
and shortestpath-knapsack. Our results seems to indicate that
connected-knapsack is computationally hardest followed by path-knapsack and
shortestpath-knapsack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.04586">Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs. (arXiv:2308.04586v15 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1">Mark Stefik</a>, <a href="http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1">Robert Price</a></p>
<p>Mainstream approaches for creating AIs include deep learning and generative
approaches (e.g., large language models) and manually constructed symbolic
approaches. These approaches have led to valuable AI systems and impressive
feats, but they can create risks when their operations affect people. Manually
constructed AIs are brittle even in circumscribed domains. Generative AIs can
make strange mistakes and not notice them. Today, these AIs cannot be
instructed easily, fail to use common sense, lack curiosity, and lack social
alignment. Developmental AI is a bootstrapping approach that uses embodied AIs.
The AIs start with innate competences and learn by interacting with their
environment. The AIs develop abilities in small steps along a bio-inspired
trajectory. Developmental AIs have shown capabilities for multimodal
perception, object recognition, and manipulation. Computational models for
hierarchical planning, abstraction discovery, curiosity, and language
acquisition exist but need to be adapted to an embodied approach. This research
aims to produce AIs that learn to communicate, establish common ground, read
critically, consider the provenance of information, test hypotheses, and
collaborate. However, developmental AI systems have not yet passed the
abilities of young children. They need to bridge competence gaps involving
nonverbal communication, speech, reading, and writing. Scaling to practical
applications also requires reducing hardware costs. This position paper lays
out prospects, gaps, and challenges for this approach. The ambition is to
create data-rich experientially based foundation models for human-compatible,
resilient, and trustworthy AIs. The AIs would learn, share what they learn, and
collaborate to achieve high standards. The approach would make AI technology
more democratic and enable more people to train, test, build on, and replicate
AIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12075">Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herranz_Celotti_L/0/1/0/all/0/1">Luca Herranz-Celotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1">Jean Rouat</a></p>
<p>Numerous theories of learning propose to prevent the gradient from
exponential growth with depth or time, to stabilize and improve training.
Typically, these analyses are conducted on feed-forward fully-connected neural
networks or simple single-layer recurrent neural networks, given their
mathematical tractability. In contrast, this study demonstrates that
pre-training the network to local stability can be effective whenever the
architectures are too complex for an analytical initialization. Furthermore, we
extend known stability theories to encompass a broader family of deep recurrent
networks, requiring minimal assumptions on data and parameter distribution, a
theory we call the Local Stability Condition (LSC). Our investigation reveals
that the classical Glorot, He, and Orthogonal initialization schemes satisfy
the LSC when applied to feed-forward fully-connected neural networks. However,
analysing deep recurrent networks, we identify a new additive source of
exponential explosion that emerges from counting gradient paths in a
rectangular grid in depth and time. We propose a new approach to mitigate this
issue, that consists on giving a weight of a half to the time and depth
contributions to the gradient, instead of the classical weight of one. Our
empirical results confirm that pre-training both feed-forward and recurrent
networks, for differentiable, neuromorphic and state-space models to fulfill
the LSC, often results in improved final performance. This study contributes to
the field by providing a means to stabilize networks of any complexity. Our
approach can be implemented as an additional step before pre-training on large
augmented datasets, and as an alternative to finding stable initializations
analytically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00201">Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wanyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cummings_M/0/1/0/all/0/1">Mary L. Cummings</a></p>
<p>Model selection is a necessary step in unsupervised machine learning. Despite
numerous criteria and metrics, model selection remains subjective. A high
degree of subjectivity may lead to questions about repeatability and
reproducibility of various machine learning studies and doubts about the
robustness of models deployed in the real world. Yet, the impact of modelers'
preferences on model selection outcomes remains largely unexplored. This study
uses the Hidden Markov Model as an example to investigate the subjectivity
involved in model selection. We asked 33 participants and three Large Language
Models (LLMs) to make model selections in three scenarios. Results revealed
variability and inconsistencies in both the participants' and the LLMs'
choices, especially when different criteria and metrics disagree. Sources of
subjectivity include varying opinions on the importance of different criteria
and metrics, differing views on how parsimonious a model should be, and how the
size of a dataset should influence model selection. The results underscore the
importance of developing a more standardized way to document subjective choices
made in model selection processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04695">Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1">Zhijie Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Richong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xudong Liu</a></p>
<p>Current methods for Knowledge-Based Question Answering (KBQA) usually rely on
complex training techniques and model frameworks, leading to many limitations
in practical applications. Recently, the emergence of In-Context Learning (ICL)
capabilities in Large Language Models (LLMs) provides a simple and
training-free semantic parsing paradigm for KBQA: Given a small number of
questions and their labeled logical forms as demo examples, LLMs can understand
the task intent and generate the logic form for a new question. However,
current powerful LLMs have little exposure to logic forms during pre-training,
resulting in a high format error rate. To solve this problem, we propose a
code-style in-context learning method for KBQA, which converts the generation
process of unfamiliar logical form into the more familiar code generation
process for LLMs. Experimental results on three mainstream datasets show that
our method dramatically mitigated the formatting error problem in generating
logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the
few-shot setting. The code and supplementary files are released at
https://github.com/Arthurizijar/KB-Coder .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08051">Retrieval-Augmented Text-to-Audio Generation. (arXiv:2309.08051v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yi Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haohe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xubo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qiushi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1">Mark D. Plumbley</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenwu Wang</a></p>
<p>Despite recent progress in text-to-audio (TTA) generation, we show that the
state-of-the-art models, such as AudioLDM, trained on datasets with an
imbalanced class distribution, such as AudioCaps, are biased in their
generation performance. Specifically, they excel in generating common audio
classes while underperforming in the rare ones, thus degrading the overall
generation performance. We refer to this problem as long-tailed text-to-audio
generation. To address this issue, we propose a simple retrieval-augmented
approach for TTA models. Specifically, given an input text prompt, we first
leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve
relevant text-audio pairs. The features of the retrieved audio-text data are
then used as additional conditions to guide the learning of TTA models. We
enhance AudioLDM with our proposed approach and denote the resulting augmented
system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a
state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the
existing approaches by a large margin. Furthermore, we show that Re-AudioLDM
can generate realistic audio for complex scenes, rare audio classes, and even
unseen audio types, indicating its potential in TTA tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.09404">Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Valluru_S/0/1/0/all/0/1">Siva Likitha Valluru</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1">Biplav Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Paladi_S/0/1/0/all/0/1">Sai Teja Paladi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Siwen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1">Sriraam Natarajan</a></p>
<p>Building teams and promoting collaboration are two very common business
activities. An example of these are seen in the TeamingForFunding problem,
where research institutions and researchers are interested to identify
collaborative opportunities when applying to funding agencies in response to
latter's calls for proposals. We describe a novel system to recommend teams
using a variety of AI methods, such that (1) each team achieves the highest
possible skill coverage that is demanded by the opportunity, and (2) the
workload of distributing the opportunities is balanced amongst the candidate
members. We address these questions by extracting skills latent in open data of
proposal calls (demand) and researcher profiles (supply), normalizing them
using taxonomies, and creating efficient algorithms that match demand to
supply. We create teams to maximize goodness along a novel metric balancing
short- and long-term objectives. We validate the success of our algorithms (1)
quantitatively, by evaluating the recommended teams using a goodness score and
find that more informed methods lead to recommendations of smaller number of
teams but higher goodness, and (2) qualitatively, by conducting a large-scale
user study at a college-wide level, and demonstrate that users overall found
the tool very useful and relevant. Lastly, we evaluate our system in two
diverse settings in US and India (of researchers and proposal calls) to
establish generality of our approach, and deploy it at a major US university
for routine use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16221">Hierarchical Randomized Smoothing. (arXiv:2310.16221v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1">Yan Scholten</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1">Jan Schuchardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1">Aleksandar Bojchevski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18446">A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v4 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaoyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Hu Ding</a></p>
<p>Optimal transport is a fundamental topic that has attracted a great amount of
attention from the optimization community in the past decades. In this paper,
we consider an interesting discrete dynamic optimal transport problem: can we
efficiently update the optimal transport plan when the weights or the locations
of the data points change? This problem is naturally motivated by several
applications in machine learning. For example, we often need to compute the
optimal transport cost between two different data sets; if some changes happen
to a few data points, should we re-compute the high complexity cost function or
update the cost by some efficient dynamic data structure? We are aware that
several dynamic maximum flow algorithms have been proposed before, however, the
research on dynamic minimum cost flow problem is still quite limited, to the
best of our knowledge. We propose a novel 2D Skip Orthogonal List together with
some dynamic tree techniques. Although our algorithm is based on the
conventional simplex method, it can efficiently find the variable to pivot
within expected $O(1)$ time, and complete each pivoting operation within
expected $O(|V|)$ time where $V$ is the set of all supply and demand nodes.
Since dynamic modifications typically do not introduce significant changes, our
algorithm requires only a few simplex iterations in practice. So our algorithm
is more efficient than re-computing the optimal transport cost that needs at
least one traversal over all $|E| = O(|V|^2)$ variables, where $|E|$ denotes
the number of edges in the network. Our experiments demonstrate that our
algorithm significantly outperforms existing algorithms in the dynamic
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05197">Deep learning in computed tomography pulmonary angiography imaging: a dual-pronged approach for pulmonary embolism detection. (arXiv:2311.05197v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bushra_F/0/1/0/all/0/1">Fabiha Bushra</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1">Muhammad E. H. Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarmun_R/0/1/0/all/0/1">Rusab Sarmun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1">Saidul Kabir</a>, <a href="http://arxiv.org/find/cs/1/au:+Said_M/0/1/0/all/0/1">Menatalla Said</a>, <a href="http://arxiv.org/find/cs/1/au:+Zoghoul_S/0/1/0/all/0/1">Sohaib Bassam Zoghoul</a>, <a href="http://arxiv.org/find/cs/1/au:+Mushtak_A/0/1/0/all/0/1">Adam Mushtak</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Hashimi_I/0/1/0/all/0/1">Israa Al-Hashimi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1">Abdulrahman Alqahtani</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1">Anwarul Hasan</a></p>
<p>The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA)
for Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need
for improved diagnostic solutions. The primary objective of this study is to
leverage deep learning techniques to enhance the Computer Assisted Diagnosis
(CAD) of PE. With this aim, we propose a classifier-guided detection approach
that effectively leverages the classifier's probabilistic inference to direct
the detection predictions, marking a novel contribution in the domain of
automated PE diagnosis. Our classification system includes an Attention-Guided
Convolutional Neural Network (AG-CNN) that uses local context by employing an
attention mechanism. This approach emulates a human expert's attention by
looking at both global appearances and local lesion regions before making a
decision. The classifier demonstrates robust performance on the FUMPE dataset,
achieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and an
F1-score of 0.805 with the Inception-v3 backbone architecture. Moreover, AG-CNN
outperforms the baseline DenseNet-121 model, achieving an 8.1% AUROC gain.
While previous research has mostly focused on finding PE in the main arteries,
our use of cutting-edge object detection models and ensembling techniques
greatly improves the accuracy of detecting small embolisms in the peripheral
arteries. Finally, our proposed classifier-guided detection approach further
refines the detection metrics, contributing new state-of-the-art to the
community: mAP$_{50}$, sensitivity, and F1-score of 0.846, 0.901, and 0.779,
respectively, outperforming the former benchmark with a significant 3.7%
improvement in mAP$_{50}$. Our research aims to elevate PE patient care by
integrating AI solutions into clinical workflows, highlighting the potential of
human-AI collaboration in medical diagnostics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09441">Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joohyung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Seif_M/0/1/0/all/0/1">Mohamed Seif</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jungchan Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1">H. Vincent Poor</a></p>
<p>Split Federated Learning (SFL) has recently emerged as a promising
distributed learning technology, leveraging the strengths of both federated
learning and split learning. It emphasizes the advantages of rapid convergence
while addressing privacy concerns. As a result, this innovation has received
significant attention from both industry and academia. However, since the model
is split at a specific layer, known as a cut layer, into both client-side and
server-side models for the SFL, the choice of the cut layer in SFL can have a
substantial impact on the energy consumption of clients and their privacy, as
it influences the training burden and the output of the client-side models.
Moreover, the design challenge of determining the cut layer is highly
intricate, primarily due to the inherent heterogeneity in the computing and
networking capabilities of clients. In this article, we provide a comprehensive
overview of the SFL process and conduct a thorough analysis of energy
consumption and privacy. This analysis takes into account the influence of
various system parameters on the cut layer selection strategy. Additionally, we
provide an illustrative example of the cut layer selection, aiming to minimize
the risk of clients from reconstructing the raw data at the server while
sustaining energy consumption within the required energy budget, which involve
trade-offs. Finally, we address open challenges in this field. These directions
represent promising avenues for future research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.09651">&quot;It&#x27;s not like Jarvis, but it&#x27;s pretty close!&quot; -- Examining ChatGPT&#x27;s Usage among Undergraduate Students in Computer Science. (arXiv:2311.09651v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1">Ishika Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Budhiraja_R/0/1/0/all/0/1">Ritvik Budhiraja</a>, <a href="http://arxiv.org/find/cs/1/au:+Akolekar_H/0/1/0/all/0/1">Harshal D Akolekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Challa_J/0/1/0/all/0/1">Jagat Sesh Challa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1">Dhruv Kumar</a></p>
<p>Large language models (LLMs) such as ChatGPT and Google Bard have garnered
significant attention in the academic community. Previous research has
evaluated these LLMs for various applications such as generating programming
exercises and solutions. However, these evaluations have predominantly been
conducted by instructors and researchers, not considering the actual usage of
LLMs by students. This study adopts a student-first approach to comprehensively
understand how undergraduate computer science students utilize ChatGPT, a
popular LLM, released by OpenAI. We employ a combination of student surveys and
interviews to obtain valuable insights into the benefits, challenges, and
suggested improvements related to ChatGPT. Our findings suggest that a majority
of students (over 57%) have a convincingly positive outlook towards adopting
ChatGPT as an aid in coursework-related tasks. However, our research also
highlights various challenges that must be resolved for long-term acceptance of
ChatGPT amongst students. The findings from this investigation have broader
implications and may be applicable to other LLMs and their role in computing
education.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13018">GeoLocator: a location-integrated large multimodal model for inferring geo-privacy. (arXiv:2311.13018v3 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siqin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Daoyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shuju Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junzhou He</a></p>
<p>Geographic privacy or geo-privacy refers to the keeping private of one's
geographic location, especially the restriction of geographical data maintained
by personal electronic devices. Geo-privacy is a crucial aspect of personal
security; however, it often goes unnoticed in daily activities. With the surge
in the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source
Intelligence (OSINT), the potential risks associated with geo-privacy breaches
have intensified. This study develops a location-integrated GPT-4 based model
named GeoLocator and designs four-dimensional experiments to demonstrate its
capability in inferring the locational information of input imageries and/or
social media contents. Our experiments reveal that GeoLocator generates
specific geographic details with high accuracy and consequently embeds the risk
of the model users exposing geospatial information to the public
unintentionally, highlighting the thread of online data sharing, information
gathering technologies and LLMs on geo-privacy. We conclude with the broader
implications of GeoLocator and our findings for individuals and the community
at large, by emphasizing the urgency for enhanced awareness and protective
measures against geo-privacy leakage in the era of advanced AI and widespread
social media usage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17431">Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaojin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.04889">KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Haojie Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1">Zepeng Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Hao Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1">Yaojia Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1">Ruiji Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1">Bing Qin</a></p>
<p>Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user's query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system's performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07910">PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1">Kaijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qinlin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a></p>
<p>The evaluation of large language models (LLMs) is crucial to assess their
performance and mitigate potential security risks. In this paper, we introduce
PromptBench, a unified library to evaluate LLMs. It consists of several key
components that are easily used and extended by researchers: prompt
construction, prompt engineering, dataset and model loading, adversarial prompt
attack, dynamic evaluation protocols, and analysis tools. PromptBench is
designed to be an open, general, and flexible codebase for research purposes
that can facilitate original study in creating new benchmarks, deploying
downstream applications, and designing new evaluation protocols. The code is
available at: https://github.com/microsoft/promptbench and will be continuously
supported.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yunfan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yun Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1">Kangxiang Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jinliu Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1">Yuxi Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yi Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiawei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1">Qianyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haofen Wang</a></p>
<p>Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11514">LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alizadeh_K/0/1/0/all/0/1">Keivan Alizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirzadeh_I/0/1/0/all/0/1">Iman Mirzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Belenko_D/0/1/0/all/0/1">Dmitry Belenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Khatamifard_K/0/1/0/all/0/1">Karen Khatamifard</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1">Minsik Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Mundo_C/0/1/0/all/0/1">Carlo C Del Mundo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1">Mohammad Rastegari</a>, <a href="http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1">Mehrdad Farajtabar</a></p>
<p>Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
substantial computational and memory requirements present challenges,
especially for devices with limited DRAM capacity. This paper tackles the
challenge of efficiently running LLMs that exceed the available DRAM capacity
by storing the model parameters in flash memory, but bringing them on demand to
DRAM. Our method involves constructing an inference cost model that takes into
account the characteristics of flash memory, guiding us to optimize in two
critical areas: reducing the volume of data transferred from flash and reading
data in larger, more contiguous chunks. Within this hardware-informed
framework, we introduce two principal techniques. First, "windowing"
strategically reduces data transfer by reusing previously activated neurons,
and second, "row-column bundling", tailored to the sequential data access
strengths of flash memory, increases the size of data chunks read from flash
memory. These methods collectively enable running models up to twice the size
of the available DRAM, with a 4-5x and 20-25x increase in inference speed
compared to naive loading approaches in CPU and GPU, respectively. Our
integration of sparsity awareness, context-adaptive loading, and a
hardware-oriented design paves the way for effective inference of LLMs on
devices with limited memory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.17163">FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liman Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1">Hanyang Zhong</a></p>
<p>Inspired by human driving focus, this research pioneers networks augmented
with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN
architecture and Directional IoU Loss - targeted innovations addressing
obstacles to precise lane detection for autonomous driving. Experiments
demonstrate our Focusing Sampling strategy, emphasizing vital distant details
unlike uniform approaches, significantly boosts both benchmark and practical
curved/distant lane recognition accuracy essential for safety. While FENetV1
achieves state-of-the-art conventional metric performance via enhancements
isolating perspective-aware contexts mimicking driver vision, FENetV2 proves
most reliable on the proposed Partial Field analysis. Hence we specifically
recommend V2 for practical lane navigation despite fractional degradation on
standard entire-image measures. Future directions include collecting on-road
data and integrating complementary dual frameworks to further breakthroughs
guided by human perception principles. The Code is available at
https://github.com/HanyangZhong/FENet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00031">Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges. (arXiv:2401.00031v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoqian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jianbin Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junge Zhang</a></p>
<p>Decision-making is a dynamic process requiring perception, memory, and
reasoning to make choices and find optimal policies. Traditional approaches to
decision-making suffer from sample efficiency and generalization, while
large-scale self-supervised pretraining has enabled fast adaptation with
fine-tuning or few-shot learning in language and vision. We thus argue to
integrate knowledge acquired from generic large-scale self-supervised
pretraining into downstream decision-making problems. We propose
Pretrain-Then-Adapt pipeline and survey recent work on data collection,
pretraining objectives and adaptation strategies for decision-making
pretraining and downstream inference. Finally, we identify critical challenges
and future directions for developing decision foundation model with the help of
generic and flexible self-supervised pretraining.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.00867">Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aizpurua_B/0/1/0/all/0/1">Borja Aizpurua</a>, <a href="http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1">Roman Orus</a></p>
<p>In this paper we show how tensor networks help in developing explainability
of machine learning algorithms. Specifically, we develop an unsupervised
clustering algorithm based on Matrix Product States (MPS) and apply it in the
context of a real use-case of adversary-generated threat intelligence. Our
investigation proves that MPS rival traditional deep learning models such as
autoencoders and GANs in terms of performance, while providing much richer
model interpretability. Our approach naturally facilitates the extraction of
feature-wise probabilities, Von Neumann Entropy, and mutual information,
offering a compelling narrative for classification of anomalies and fostering
an unprecedented level of transparency and interpretability, something
fundamental to understand the rationale behind artificial intelligence
decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2401.01623">Can AI Be as Creative as Humans?. (arXiv:2401.01623v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haonan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1">Michael Mozer</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1">Alex Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Linjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Weijie J Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1">Zhun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Michael Qizhe Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_H/0/1/0/all/0/1">Hannah Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI's creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI's creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI's creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.11092">INVIGORATE: Interactive Visual Grounding and Grasping in Clutter. (arXiv:2108.11092v1 [cs.RO] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yunfan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Cunjun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1">David Hsu</a>, <a href="http://arxiv.org/find/cs/1/au:+La_X/0/1/0/all/0/1">Xuguang La</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a></p>
<p>This paper presents INVIGORATE, a robot system that interacts with human
through natural language and grasps a specified object in clutter. The objects
may occlude, obstruct, or even stack on top of one another. INVIGORATE embodies
several challenges: (i) infer the target object among other occluding objects,
from input language expressions and RGB images, (ii) infer object blocking
relationships (OBRs) from the images, and (iii) synthesize a multi-step plan to
ask questions that disambiguate the target object and to grasp it successfully.
We train separate neural networks for object detection, for visual grounding,
for question generation, and for OBR detection and grasping. They allow for
unrestricted object categories and language expressions, subject to the
training datasets. However, errors in visual perception and ambiguity in human
languages are inevitable and negatively impact the robot's performance. To
overcome these uncertainties, we build a partially observable Markov decision
process (POMDP) that integrates the learned neural network modules. Through
approximate POMDP planning, the robot tracks the history of observations and
asks disambiguation questions in order to achieve a near-optimal sequence of
actions that identify and grasp the target object. INVIGORATE combines the
benefits of model-based POMDP planning and data-driven deep learning.
Preliminary experiments with INVIGORATE on a Fetch robot show significant
benefits of this integrated approach to object grasping in clutter with natural
language interactions. A demonstration video is available at
https://youtu.be/zYakh80SGcU.
</p>
</p>
</div>

    </div>
    </body>
    