<!DOCTYPE html>
<html>
<head>
<title>2023-07-30-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.14343">Pruning Distorted Images in MNIST Handwritten Digits. (arXiv:2307.14343v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+R_A/0/1/0/all/0/1">Amarnath R</a>, <a href="http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1">Vinay Kumar V</a></p>
<p>Recognizing handwritten digits is a challenging task primarily due to the
diversity of writing styles and the presence of noisy images. The widely used
MNIST dataset, which is commonly employed as a benchmark for this task,
includes distorted digits with irregular shapes, incomplete strokes, and
varying skew in both the training and testing datasets. Consequently, these
factors contribute to reduced accuracy in digit recognition. To overcome this
challenge, we propose a two-stage deep learning approach. In the first stage,
we create a simple neural network to identify distorted digits within the
training set. This model serves to detect and filter out such distorted and
ambiguous images. In the second stage, we exclude these identified images from
the training dataset and proceed to retrain the model using the filtered
dataset. This process aims to improve the classification accuracy and
confidence levels while mitigating issues of underfitting and overfitting. Our
experimental results demonstrate the effectiveness of the proposed approach,
achieving an accuracy rate of over 99.5% on the testing dataset. This
significant improvement showcases the potential of our method in enhancing
digit classification accuracy. In our future work, we intend to explore the
scalability of this approach and investigate techniques to further enhance
accuracy by reducing the size of the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14346">Multi-objective Deep Reinforcement Learning for Mobile Edge Computing. (arXiv:2307.14346v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1">Ning Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Junrui Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Meng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Ming Tang</a></p>
<p>Mobile edge computing (MEC) is essential for next-generation mobile network
applications that prioritize various performance metrics, including delays and
energy consumption. However, conventional single-objective scheduling solutions
cannot be directly applied to practical systems in which the preferences of
these applications (i.e., the weights of different objectives) are often
unknown or challenging to specify in advance. In this study, we address this
issue by formulating a multi-objective offloading problem for MEC with multiple
edges to minimize expected long-term energy consumption and transmission delay
while considering unknown preferences as parameters. To address the challenge
of unknown preferences, we design a multi-objective (deep) reinforcement
learning (MORL)-based resource scheduling scheme with proximal policy
optimization (PPO). In addition, we introduce a well-designed state encoding
method for constructing features for multiple edges in MEC systems, a
sophisticated reward function for accurately computing the utilities of delay
and energy consumption. Simulation results demonstrate that our proposed MORL
scheme enhances the hypervolume of the Pareto front by up to 233.1% compared to
benchmarks. Our full framework is available at
https://github.com/gracefulning/mec_morl_multipolicy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14354">Learned Gridification for Efficient Point Cloud Processing. (arXiv:2307.14354v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Linden_P/0/1/0/all/0/1">Putri A. van der Linden</a>, <a href="http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1">David W. Romero</a>, <a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1">Erik J. Bekkers</a></p>
<p>Neural operations that rely on neighborhood information are much more
expensive when deployed on point clouds than on grid data due to the irregular
distances between points in a point cloud. In a grid, on the other hand, we can
compute the kernel only once and reuse it for all query positions. As a result,
operations that rely on neighborhood information scale much worse for point
clouds than for grid data, specially for large inputs and large neighborhoods.
</p>
<p>In this work, we address the scalability issue of point cloud methods by
tackling its root cause: the irregularity of the data. We propose learnable
gridification as the first step in a point cloud processing pipeline to
transform the point cloud into a compact, regular grid. Thanks to
gridification, subsequent layers can use operations defined on regular grids,
e.g., Conv3D, which scale much better than native point cloud methods. We then
extend gridification to point cloud to point cloud tasks, e.g., segmentation,
by adding a learnable de-gridification step at the end of the point cloud
processing pipeline to map the compact, regular grid back to its original point
cloud form. Through theoretical and empirical analysis, we show that gridified
networks scale better in terms of memory and time than networks directly
applied on raw point cloud data, while being able to achieve competitive
results. Our code is publicly available at
https://github.com/computri/gridifier.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14359">A new derivative-free optimization method: Gaussian Crunching Search. (arXiv:2307.14359v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wong_B/0/1/0/all/0/1">Benny Wong</a></p>
<p>Optimization methods are essential in solving complex problems across various
domains. In this research paper, we introduce a novel optimization method
called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles
in a Gaussian distribution, GCS aims to efficiently explore the solution space
and converge towards the global optimum. We present a comprehensive analysis of
GCS, including its working mechanism, and potential applications. Through
experimental evaluations and comparisons with existing optimization methods, we
highlight the advantages and strengths of GCS. This research paper serves as a
valuable resource for researchers, practitioners, and students interested in
optimization, providing insights into the development and potential of Gaussian
Crunching Search as a new and promising approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14361">A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Aburass_S/0/1/0/all/0/1">Sanad Aburass</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dorgham_O/0/1/0/all/0/1">Osama Dorgham</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Shaqsi_J/0/1/0/all/0/1">Jamil Al Shaqsi</a></p>
<p>This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and
GloVe to classify gene mutations using Kaggle's Personalized Medicine:
Redefining Cancer Treatment dataset. The results were compared against
well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and
their LSTM ensembles. Our model outperformed all other models in terms of
accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it
also needed less training time, resulting in a perfect combination of
performance and efficiency. This study demonstrates the utility of ensemble
models for difficult tasks such as gene mutation classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14362">Learnable wavelet neural networks for cosmological inference. (arXiv:2307.14362v1 [astro-ph.IM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Pedersen_C/0/1/0/all/0/1">Christian Pedersen</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Eickenberg_M/0/1/0/all/0/1">Michael Eickenberg</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1">Shirley Ho</a></p>
<p>Convolutional neural networks (CNNs) have been shown to both extract more
information than the traditional two-point statistics from cosmological fields,
and marginalise over astrophysical effects extremely well. However, CNNs
require large amounts of training data, which is potentially problematic in the
domain of expensive cosmological simulations, and it is difficult to interpret
the network. In this work we apply the learnable scattering transform, a kind
of convolutional neural network that uses trainable wavelets as filters, to the
problem of cosmological inference and marginalisation over astrophysical
effects. We present two models based on the scattering transform, one
constructed for performance, and one constructed for interpretability, and
perform a comparison with a CNN. We find that scattering architectures are able
to outperform a CNN, significantly in the case of small training data samples.
Additionally we present a lightweight scattering network that is highly
interpretable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14363">Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields. (arXiv:2307.14363v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Catalan_T/0/1/0/all/0/1">Tabita Catal&#xe1;n</a>, <a href="http://arxiv.org/find/eess/1/au:+Courdurier_M/0/1/0/all/0/1">Mat&#xed;as Courdurier</a>, <a href="http://arxiv.org/find/eess/1/au:+Osses_A/0/1/0/all/0/1">Axel Osses</a>, <a href="http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1">Ren&#xe9; Botnar</a>, <a href="http://arxiv.org/find/eess/1/au:+Costabal_F/0/1/0/all/0/1">Francisco Sahli Costabal</a>, <a href="http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1">Claudia Prieto</a></p>
<p>Cardiac cine MRI is the gold standard for cardiac functional assessment, but
the inherently slow acquisition process creates the necessity of reconstruction
approaches for accelerated undersampled acquisitions. Several regularization
approaches that exploit spatial-temporal redundancy have been proposed to
reconstruct undersampled cardiac cine MRI. More recently, methods based on
supervised deep learning have been also proposed to further accelerate
acquisition and reconstruction. However, these techniques rely on usually large
dataset for training, which are not always available. In this work, we propose
an unsupervised approach based on implicit neural field representations for
cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in
in-vivo undersampled golden-angle radial multi-coil acquisitions for
undersampling factors of 26x and 52x, achieving good image quality, and
comparable spatial and improved temporal depiction than a state-of-the-art
reconstruction technique.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14364">Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis. (arXiv:2307.14364v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Jiao_Y/0/1/0/all/0/1">Yang Jiao</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Song_D/0/1/0/all/0/1">Dongjin Song</a></p>
<p>Distributionally Robust Optimization (DRO), which aims to find an optimal
decision that minimizes the worst case cost over the ambiguity set of
probability distribution, has been widely applied in diverse applications,
e.g., network behavior analysis, risk management, etc. However, existing DRO
techniques face three key challenges: 1) how to deal with the asynchronous
updating in a distributed environment; 2) how to leverage the prior
distribution effectively; 3) how to properly adjust the degree of robustness
according to different scenarios. To this end, we propose an asynchronous
distributed algorithm, named Asynchronous Single-looP alternatIve gRadient
projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to
tackle the federated distributionally robust optimization (FDRO) problem.
Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set,
is developed to effectively leverage the prior distribution and flexibly
control the degree of robustness. Finally, our theoretical analysis elucidates
that the proposed algorithm is guaranteed to converge and the iteration
complexity is also analyzed. Extensive empirical studies on real-world datasets
demonstrate that the proposed method can not only achieve fast convergence, and
remain robust against data heterogeneity as well as malicious attacks, but also
tradeoff robustness with performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14366">Explainable Disparity Compensation for Efficient Fair Ranking. (arXiv:2307.14366v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gale_A/0/1/0/all/0/1">Abraham Gale</a>, <a href="http://arxiv.org/find/cs/1/au:+Marian_A/0/1/0/all/0/1">Am&#xc9;lie Marian</a></p>
<p>Ranking functions that are used in decision systems often produce disparate
results for different populations because of bias in the underlying data.
Addressing, and compensating for, these disparate outcomes is a critical
problem for fair decision-making. Recent compensatory measures have mostly
focused on opaque transformations of the ranking functions to satisfy fairness
guarantees or on the use of quotas or set-asides to guarantee a minimum number
of positive outcomes to members of underrepresented groups. In this paper we
propose easily explainable data-driven compensatory measures for ranking
functions. Our measures rely on the generation of bonus points given to members
of underrepresented groups to address disparity in the ranking function. The
bonus points can be set in advance, and can be combined, allowing for
considering the intersections of representations and giving better transparency
to stakeholders. We propose efficient sampling-based algorithms to calculate
the number of bonus points to minimize disparity. We validate our algorithms
using real-world school admissions and recidivism datasets, and compare our
results with that of existing fair ranking algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14367">Prot2Text: Multimodal Protein&#x27;s Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Abdine_H/0/1/0/all/0/1">Hadi Abdine</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chatzianastasis_M/0/1/0/all/0/1">Michail Chatzianastasis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bouyioukos_C/0/1/0/all/0/1">Costas Bouyioukos</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a></p>
<p>The complex nature of big biological systems pushed some scientists to
classify its understanding under the inconceivable missions. Different leveled
challenges complicated this task, one of is the prediction of a protein's
function. In recent years, significant progress has been made in this field
through the development of various machine learning approaches. However, most
existing methods formulate the task as a multi-classification problem, i.e
assigning predefined labels to proteins. In this work, we propose a novel
approach, \textbf{Prot2Text}, which predicts a protein function's in a free
text style, moving beyond the conventional binary or categorical
classifications. By combining Graph Neural Networks(GNNs) and Large Language
Models(LLMs), in an encoder-decoder framework, our model effectively integrates
diverse data types including proteins' sequences, structures, and textual
annotations. This multimodal approach allows for a holistic representation of
proteins' functions, enabling the generation of detailed and accurate
descriptions. To evaluate our model, we extracted a multimodal protein dataset
from SwissProt, and demonstrate empirically the effectiveness of Prot2Text.
These results highlight the transformative impact of multimodal models,
specifically the fusion of GNNs and LLMs, empowering researchers with powerful
tools for more accurate prediction of proteins' functions. The code, the models
and a demo will be publicly released.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14371">Prediction of depression status in college students using a Naive Bayes classifier based machine learning model. (arXiv:2307.14371v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cruz_F/0/1/0/all/0/1">Fred Torres Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Flores_E/0/1/0/all/0/1">Evelyn Eliana Coaquira Flores</a>, <a href="http://arxiv.org/find/cs/1/au:+Quispe_S/0/1/0/all/0/1">Sebastian Jarom Condori Quispe</a></p>
<p>This study presents a machine learning model based on the Naive Bayes
classifier for predicting the level of depression in university students, the
objective was to improve prediction accuracy using a machine learning model
involving 70% training data and 30% validation data based on the Naive Bayes
classifier, the collected data includes factors associated with depression from
519 university students, the results showed an accuracy of 78.03%, high
sensitivity in detecting positive cases of depression, especially at moderate
and severe levels, and significant specificity in correctly classifying
negative cases, these findings highlight the effectiveness of the model in
early detection and treatment of depression, benefiting vulnerable sectors and
contributing to the improvement of mental health in the student population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14373">Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks. (arXiv:2307.14373v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McCarty_S/0/1/0/all/0/1">Sarah McCarty</a></p>
<p>This paper analyzes representations of continuous piecewise linear functions
with infinite width, finite cost shallow neural networks using the rectified
linear unit (ReLU) as an activation function. Through its integral
representation, a shallow neural network can be identified by the corresponding
signed, finite measure on an appropriate parameter space. We map these measures
on the parameter space to measures on the projective $n$-sphere cross
$\mathbb{R}$, allowing points in the parameter space to be bijectively mapped
to hyperplanes in the domain of the function. We prove a conjecture of Ongie et
al. that every continuous piecewise linear function expressible with this kind
of infinite width neural network is expressible as a finite width shallow ReLU
neural network.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14374">Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design. (arXiv:2307.14374v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sadhukhan_S/0/1/0/all/0/1">Suchetana Sadhukhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1">Vivek Kumar Yadav</a></p>
<p>This study provides a comprehensive time series analysis of daily
industry-specific, country-wise CO$_2$ emissions from January 2019 to February
2023. The research focuses on the Power, Industry, Ground Transport, Domestic
Aviation, and International Aviation sectors in European countries (EU27 &amp; UK,
Italy, Germany, Spain) and India, utilizing near-real-time activity data from
the Carbon Monitor research initiative. To identify regular emission patterns,
the data from the year 2020 is excluded due to the disruptive effects caused by
the COVID-19 pandemic. The study then performs a principal component analysis
(PCA) to determine the key contributors to CO$_2$ emissions. The analysis
reveals that the Power, Industry, and Ground Transport sectors account for a
significant portion of the variance in the dataset. A 7-day moving averaged
dataset is employed for further analysis to facilitate robust predictions. This
dataset captures both short-term and long-term trends and enhances the quality
of the data for prediction purposes. The study utilizes Long Short-Term Memory
(LSTM) models on the 7-day moving averaged dataset to effectively predict
emissions and provide insights for policy decisions, mitigation strategies, and
climate change efforts. During the training phase, the stability and
convergence of the LSTM models are ensured, which guarantees their reliability
in the testing phase. The evaluation of the loss function indicates this
reliability. The model achieves high efficiency, as demonstrated by $R^2$
values ranging from 0.8242 to 0.995 for various countries and sectors.
Furthermore, there is a proposal for utilizing scandium and
boron/aluminium-based thin films as exceptionally efficient materials for
capturing CO$_2$ (with a binding energy range from -3.0 to -3.5 eV). These
materials are shown to surpass the affinity of graphene and boron nitride
sheets in this regard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14375">DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm. (arXiv:2307.14375v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Ying Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hou-biao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu-pu Zhang</a></p>
<p>With the development of Big data technology, data analysis has become
increasingly important. Traditional clustering algorithms such as K-means are
highly sensitive to the initial centroid selection and perform poorly on
non-convex datasets. In this paper, we address these problems by proposing a
data-driven Bregman divergence parameter optimization clustering algorithm
(DBGSA), which combines the Universal Gravitational Algorithm to bring similar
points closer in the dataset. We construct a gravitational coefficient equation
with a special property that gradually reduces the influence factor as the
iteration progresses. Furthermore, we introduce the Bregman divergence
generalized power mean information loss minimization to identify cluster
centers and build a hyperparameter identification optimization model, which
effectively solves the problems of manual adjustment and uncertainty in the
improved dataset. Extensive experiments are conducted on four simulated
datasets and six real datasets. The results demonstrate that DBGSA
significantly improves the accuracy of various clustering algorithms by an
average of 63.8\% compared to other similar approaches like enhanced clustering
algorithms and improved datasets. Additionally, a three-dimensional grid search
was established to compare the effects of different parameter values within
threshold conditions, and it was discovered the parameter set provided by our
model is optimal. This finding provides strong evidence of the high accuracy
and robustness of the algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14380">Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations. (arXiv:2307.14380v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaluza_D/0/1/0/all/0/1">Daniel Ka&#x142;u&#x17c;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Janusz_A/0/1/0/all/0/1">Andrzej Janusz</a>, <a href="http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1">Dominik &#x15a;l&#x119;zak</a></p>
<p>Supervised classification algorithms are used to solve a growing number of
real-life problems around the globe. Their performance is strictly connected
with the quality of labels used in training. Unfortunately, acquiring
good-quality annotations for many tasks is infeasible or too expensive to be
done in practice. To tackle this challenge, active learning algorithms are
commonly employed to select only the most relevant data for labeling. However,
this is possible only when the quality and quantity of labels acquired from
experts are sufficient. Unfortunately, in many applications, a trade-off
between annotating individual samples by multiple annotators to increase label
quality vs. annotating new samples to increase the total number of labeled
instances is necessary. In this paper, we address the issue of faulty data
annotations in the context of active learning. In particular, we propose two
novel annotation unification algorithms that utilize unlabeled parts of the
sample space. The proposed methods require little to no intersection between
samples annotated by different experts. Our experiments on four public datasets
indicate the robustness and superiority of the proposed methods in both, the
estimation of the annotator's reliability, and the assignment of actual labels,
against the state-of-the-art algorithms and the simple majority voting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14381">EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence. (arXiv:2307.14381v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sikdokur_I/0/1/0/all/0/1">Ilkay Sikdokur</a>, <a href="http://arxiv.org/find/cs/1/au:+Baytas_I/0/1/0/all/0/1">&#x130;nci M. Bayta&#x15f;</a>, <a href="http://arxiv.org/find/cs/1/au:+Yurdakul_A/0/1/0/all/0/1">Arda Yurdakul</a></p>
<p>Deep edge intelligence aims to deploy deep learning models that demand
computationally expensive training in the edge network with limited
computational power. Moreover, many deep edge intelligence applications require
handling distributed data that cannot be transferred to a central server due to
privacy concerns. Decentralized learning methods, such as federated learning,
offer solutions where models are learned collectively by exchanging learned
weights. However, they often require complex models that edge devices may not
handle and multiple rounds of network communication to achieve state-of-the-art
performances. This study proposes a convolutional ensemble learning approach,
coined EdgeConvEns, that facilitates training heterogeneous weak models on edge
and learning to ensemble them where data on edge are heterogeneously
distributed. Edge models are implemented and trained independently on
Field-Programmable Gate Array (FPGA) devices with various computational
capacities. Learned data representations are transferred to a central server
where the ensemble model is trained with the learned features received from the
edge devices to boost the overall prediction performance. Extensive experiments
demonstrate that the EdgeConvEns can outperform the state-of-the-art
performance with fewer communications and less data in various training
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14382">When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fontana_M/0/1/0/all/0/1">Maxime Fontana</a>, <a href="http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1">Michael Spratling</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1">Miaojing Shi</a></p>
<p>Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while
exploiting their mutual relationships. By using shared resources to
simultaneously calculate multiple outputs, this learning paradigm has the
potential to have lower memory requirements and inference times compared to the
traditional approach of using separate methods for each task. Previous work in
MTL has mainly focused on fully-supervised methods, as task relationships can
not only be leveraged to lower the level of data-dependency of those methods
but they can also improve performance. However, MTL introduces a set of
challenges due to a complex optimisation scheme and a higher labeling
requirement. This review focuses on how MTL could be utilised under different
partial supervision settings to address these challenges. First, this review
analyses how MTL traditionally uses different parameter sharing techniques to
transfer knowledge in between tasks. Second, it presents the different
challenges arising from such a multi-objective optimisation scheme. Third, it
introduces how task groupings can be achieved by analysing task relationships.
Fourth, it focuses on how partially supervised methods applied to MTL can
tackle the aforementioned challenges. Lastly, this review presents the
available datasets, tools and benchmarking results of such methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14384">HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning. (arXiv:2307.14384v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1">Xinting Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weiming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaochao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pengyang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Huabin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yanchao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yue Qi</a></p>
<p>Federated learning (FL) collaboratively models user data in a decentralized
way. However, in the real world, non-identical and independent data
distributions (non-IID) among clients hinder the performance of FL due to three
issues, i.e., (1) the class statistics shifting, (2) the insufficient
hierarchical information utilization, and (3) the inconsistency in aggregating
clients. To address the above issues, we propose HyperFed which contains three
main modules, i.e., hyperbolic prototype Tammes initialization (HPTI),
hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly,
HPTI in the server constructs uniformly distributed and fixed class prototypes,
and shares them with clients to match class statistics, further guiding
consistent feature representation for local clients. Secondly, HPL in each
client captures the hierarchical information in local data with the supervision
of shared class prototypes in the hyperbolic model space. Additionally, CA in
the server mitigates the impact of the inconsistent deviations from clients to
server. Extensive studies of four datasets prove that HyperFed is effective in
enhancing the performance of FL under the non-IID set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14389">Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Soowon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1">Young-Eun Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seo-Hyun Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Decoding EEG signals for imagined speech is a challenging task due to the
high-dimensional nature of the data and low signal-to-noise ratio. In recent
years, denoising diffusion probabilistic models (DDPMs) have emerged as
promising approaches for representation learning in various domains. Our study
proposes a novel method for decoding EEG signals for imagined speech using
DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E
significantly improves the accuracy of decoding EEG signals for imagined speech
compared to traditional machine learning techniques and baseline models. Our
findings suggest that DDPMs can be an effective tool for EEG signal decoding,
with potential implications for the development of brain-computer interfaces
that enable communication through imagined speech.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14394">Hypergraph Isomorphism Computation. (arXiv:2307.14394v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yifan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiashu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1">Shihui Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yue Gao</a></p>
<p>The isomorphism problem is a fundamental problem in network analysis, which
involves capturing both low-order and high-order structural information. In
terms of extracting low-order structural information, graph isomorphism
algorithms analyze the structural equivalence to reduce the solver space
dimension, which demonstrates its power in many applications, such as protein
design, chemical pathways, and community detection. For the more commonly
occurring high-order relationships in real-life scenarios, the problem of
hypergraph isomorphism, which effectively captures these high-order structural
relationships, cannot be straightforwardly addressed using graph isomorphism
methods. Besides, the existing hypergraph kernel methods may suffer from high
memory consumption or inaccurate sub-structure identification, thus yielding
sub-optimal performance. In this paper, to address the abovementioned problems,
we first propose the hypergraph Weisfiler-Lehman test algorithm for the
hypergraph isomorphism test problem by generalizing the Weisfiler-Lehman test
algorithm from graphs to hypergraphs. Secondly, based on the presented
algorithm, we propose a general hypergraph Weisfieler-Lehman kernel framework
and implement two instances, which are Hypergraph Weisfeiler-Lehamn Subtree
Kernel and Hypergraph Weisfeiler-Lehamn Hyperedge Kernel. In order to fulfill
our research objectives, a comprehensive set of experiments was meticulously
designed, including seven graph classification datasets and 12 hypergraph
classification datasets. Results on hypergraph classification datasets show
significant improvements compared to other typical kernel-based methods, which
demonstrates the effectiveness of the proposed methods. In our evaluation, we
found that our proposed methods outperform the second-best method in terms of
runtime, running over 80 times faster when handling complex hypergraph
structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14395">Learning to simulate partially known spatio-temporal dynamics with trainable difference operators. (arXiv:2307.14395v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuoyuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongsheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hongye Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1">Bin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1">Bei Hua</a></p>
<p>Recently, using neural networks to simulate spatio-temporal dynamics has
received a lot of attention. However, most existing methods adopt pure
data-driven black-box models, which have limited accuracy and interpretability.
By combining trainable difference operators with black-box models, we propose a
new hybrid architecture explicitly embedded with partial prior knowledge of the
underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options
called the trainable flipping difference layer (TFDL) and the trainable dynamic
difference layer (TDDL) for the difference operators. Numerous numerical
experiments have demonstrated that PDE-Net++ has superior prediction accuracy
and better extrapolation performance than black-box models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14397">A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot. (arXiv:2307.14397v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdollahzadeh_M/0/1/0/all/0/1">Milad Abdollahzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_T/0/1/0/all/0/1">Touba Malekzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1">Christopher T. H. Teo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandrasegaran_K/0/1/0/all/0/1">Keshigeyan Chandrasegaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guimeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1">Ngai-Man Cheung</a></p>
<p>In machine learning, generative modeling aims to learn to generate new data
statistically similar to the training data distribution. In this paper, we
survey learning generative models under limited data, few shots and zero shot,
referred to as Generative Modeling under Data Constraint (GM-DC). This is an
important topic when data acquisition is challenging, e.g. healthcare
applications. We discuss background, challenges, and propose two taxonomies:
one on GM-DC tasks and another on GM-DC approaches. Importantly, we study
interactions between different GM-DC tasks and approaches. Furthermore, we
highlight research gaps, research trends, and potential avenues for future
exploration. Project website: https://gmdc-survey.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14403">Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity. (arXiv:2307.14403v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ciotola_M/0/1/0/all/0/1">Matteo Ciotola</a>, <a href="http://arxiv.org/find/eess/1/au:+Poggi_G/0/1/0/all/0/1">Giovanni Poggi</a>, <a href="http://arxiv.org/find/eess/1/au:+Scarpa_G/0/1/0/all/0/1">Giuseppe Scarpa</a></p>
<p>In latest years, deep learning has gained a leading role in the pansharpening
of multiresolution images. Given the lack of ground truth data, most deep
learning-based methods carry out supervised training in a reduced-resolution
domain. However, models trained on downsized images tend to perform poorly on
high-resolution target images. For this reason, several research groups are now
turning to unsupervised training in the full-resolution domain, through the
definition of appropriate loss functions and training paradigms. In this
context, we have recently proposed a full-resolution training framework which
can be applied to many existing architectures.
</p>
<p>Here, we propose a new deep learning-based pansharpening model that fully
exploits the potential of this approach and provides cutting-edge performance.
Besides architectural improvements with respect to previous work, such as the
use of residual attention modules, the proposed model features a novel loss
function that jointly promotes the spectral and spatial quality of the
pansharpened data. In addition, thanks to a new fine-tuning strategy, it
improves inference-time adaptation to target images. Experiments on a large
variety of test images, performed in challenging scenarios, demonstrate that
the proposed method compares favorably with the state of the art both in terms
of numerical results and visual output. Code is available online at
https://github.com/matciotola/Lambda-PNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14439">Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kortvelesy_R/0/1/0/all/0/1">Ryan Kortvelesy</a></p>
<p>It is often useful to perform integration over learned functions represented
by neural networks. However, this integration is usually performed numerically,
as analytical integration over learned functions (especially neural networks)
is generally viewed as intractable. In this work, we present a method for
representing the analytical integral of a learned function $f$. This allows the
exact integral of a neural network to be computed, and enables constrained
neural networks to be parametrised by applying constraints directly to the
integral. Crucially, we also introduce a method to constrain $f$ to be
positive, a necessary condition for many applications (e.g. probability
distributions, distance metrics, etc). Finally, we introduce several
applications where our fixed-integral neural network (FINN) can be utilised.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14448">VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions. (arXiv:2307.14448v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teng_X/0/1/0/all/0/1">Xian Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1">Yongsu Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yu-Ru Lin</a></p>
<p>Big data and machine learning tools have jointly empowered humans in making
data-driven decisions. However, many of them capture empirical associations
that might be spurious due to confounding factors and subgroup heterogeneity.
The famous Simpson's paradox is such a phenomenon where aggregated and
subgroup-level associations contradict with each other, causing cognitive
confusions and difficulty in making adequate interpretations and decisions.
Existing tools provide little insights for humans to locate, reason about, and
prevent pitfalls of spurious association in practice. We propose VISPUR, a
visual analytic system that provides a causal analysis framework and a
human-centric workflow for tackling spurious associations. These include a
CONFOUNDER DASHBOARD, which can automatically identify possible confounding
factors, and a SUBGROUP VIEWER, which allows for the visualization and
comparison of diverse subgroup patterns that likely or potentially result in a
misinterpretation of causality. Additionally, we propose a REASONING
STORYBOARD, which uses a flow-based approach to illustrate paradoxical
phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure
accountable decision-making. Through an expert interview and a controlled user
experiment, our qualitative and quantitative results demonstrate that the
proposed "de-paradox" workflow and the designed visual analytic system are
effective in helping human users to identify and understand spurious
associations, as well as to make accountable causal decisions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14453">Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches. (arXiv:2307.14453v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sengupta_P/0/1/0/all/0/1">Prajit Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1">Anant Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Rana_P/0/1/0/all/0/1">Prashant Singh Rana</a></p>
<p>Armoured vehicles are specialized and complex pieces of machinery designed to
operate in high-stress environments, often in combat or tactical situations.
This study proposes a predictive maintenance-based ensemble system that aids in
predicting potential maintenance needs based on sensor data collected from
these vehicles. The proposed model's architecture involves various models such
as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier
and Gradient Boosting to predict the maintenance requirements of the vehicles
accurately. In addition, K-fold cross validation, along with TOPSIS analysis,
is employed to evaluate the proposed ensemble model's stability. The results
indicate that the proposed system achieves an accuracy of 98.93%, precision of
99.80% and recall of 99.03%. The algorithm can effectively predict maintenance
needs, thereby reducing vehicle downtime and improving operational efficiency.
Through comparisons between various algorithms and the suggested ensemble, this
study highlights the potential of machine learning-based predictive maintenance
solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14459">Training Quantum Boltzmann Machines with Coresets. (arXiv:2307.14459v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Viszlai_J/0/1/0/all/0/1">Joshua Viszlai</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tomesh_T/0/1/0/all/0/1">Teague Tomesh</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gokhale_P/0/1/0/all/0/1">Pranav Gokhale</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Anschuetz_E/0/1/0/all/0/1">Eric Anschuetz</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1">Frederic T. Chong</a></p>
<p>Recent work has proposed and explored using coreset techniques for quantum
algorithms that operate on classical data sets to accelerate the applicability
of these algorithms on near-term quantum devices. We apply these ideas to
Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs
state sampling are the main computational bottleneck during training. By using
a coreset in place of the full data set, we try to minimize the number of steps
needed and accelerate the overall training time. In a regime where
computational time on quantum computers is a precious resource, we propose this
might lead to substantial practical savings. We evaluate this approach on 6x6
binary images from an augmented bars and stripes data set using a QBM with 36
visible units and 8 hidden units. Using an Inception score inspired metric, we
compare QBM training times with and without using coresets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14465">What Kinds of Contracts Do ML APIs Need?. (arXiv:2307.14465v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khairunnesa_S/0/1/0/all/0/1">Samantha Syeda Khairunnesa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1">Shibbir Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Imtiaz_S/0/1/0/all/0/1">Sayem Mohammad Imtiaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1">Hridesh Rajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Leavens_G/0/1/0/all/0/1">Gary T. Leavens</a></p>
<p>Recent work has shown that Machine Learning (ML) programs are error-prone and
called for contracts for ML code. Contracts, as in the design by contract
methodology, help document APIs and aid API users in writing correct code. The
question is: what kinds of contracts would provide the most help to API users?
We are especially interested in what kinds of contracts help API users catch
errors at earlier stages in the ML pipeline. We describe an empirical study of
posts on Stack Overflow of the four most often-discussed ML libraries:
TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, our study
extracted 413 informal (English) API specifications. We used these
specifications to understand the following questions. What are the root causes
and effects behind ML contract violations? Are there common patterns of ML
contract violations? When does understanding ML contracts require an advanced
level of ML software expertise? Could checking contracts at the API level help
detect the violations in early ML pipeline stages? Our key findings are that
the most commonly needed contracts for ML APIs are either checking constraints
on single arguments of an API or on the order of API calls. The software
engineering community could employ existing contract mining approaches to mine
these contracts to promote an increased understanding of ML APIs. We also noted
a need to combine behavioral and temporal contract mining approaches. We report
on categories of required ML contracts, which may help designers of contract
languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14474">Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Polloreno_A/0/1/0/all/0/1">Anthony M. Polloreno</a></p>
<p>In this work, we bound a machine's ability to learn based on computational
limitations implied by physicality. We start by considering the information
processing capacity (IPC), a normalized measure of the expected squared error
of a collection of signals to a complete basis of functions. We use the IPC to
measure the degradation under noise of the performance of reservoir computers,
a particular kind of recurrent network, when constrained by physical
considerations. First, we show that the IPC is at most a polynomial in the
system size $n$, even when considering the collection of $2^n$ possible
pointwise products of the $n$ output signals. Next, we argue that this
degradation implies that the family of functions represented by the reservoir
requires an exponential number of samples to learn in the presence of the
reservoir's noise. Finally, we conclude with a discussion of the performance of
the same collection of $2^n$ functions without noise when being used for binary
classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14482">Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization. (arXiv:2307.14482v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kline_T/0/1/0/all/0/1">Timothy L. Kline</a>, <a href="http://arxiv.org/find/eess/1/au:+Ramanathan_S/0/1/0/all/0/1">Sumana Ramanathan</a>, <a href="http://arxiv.org/find/eess/1/au:+Gottlich_H/0/1/0/all/0/1">Harrison C. Gottlich</a>, <a href="http://arxiv.org/find/eess/1/au:+Korfiatis_P/0/1/0/all/0/1">Panagiotis Korfiatis</a>, <a href="http://arxiv.org/find/eess/1/au:+Gregory_A/0/1/0/all/0/1">Adriana V. Gregory</a></p>
<p>Purpose: This study evaluated the out-of-domain performance and
generalization capabilities of automated medical image segmentation models,
with a particular focus on adaptation to new image acquisitions and disease
type.
</p>
<p>Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT
scans of healthy patients and those with polycystic kidney disease (PKD) were
used. A total of 400 images (100 non-contrast controls, 100 contrast controls,
100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation
of models to segment kidneys, livers, and spleens, and the final models were
then tested on 100 non-contrast CT images of patients affected by PKD.
Performance was evaluated using Dice, Jaccard, TPR, and Precision.
</p>
<p>Results: Models trained on a diverse range of data showed no worse
performance than models trained exclusively on in-domain data when tested on
in-domain data. For instance, the Dice similarity of the model trained on 25%
from each dataset was found to be non-inferior to the model trained purely on
in-domain data.
</p>
<p>Conclusions: The results indicate that broader training examples
significantly enhances model generalization and out-of-domain performance,
thereby improving automated segmentation tools' applicability in clinical
settings. The study's findings provide a roadmap for future research to adopt a
data-centric approach in medical image AI model development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14490">HUGE: Huge Unsupervised Graph Embeddings with TPUs. (arXiv:2307.14490v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mayer_B/0/1/0/all/0/1">Brandon Mayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1">Anton Tsitsulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fichtenberger_H/0/1/0/all/0/1">Hendrik Fichtenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Halcrow_J/0/1/0/all/0/1">Jonathan Halcrow</a>, <a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1">Bryan Perozzi</a></p>
<p>Graphs are a representation of structured data that captures the
relationships between sets of objects. With the ubiquity of available network
data, there is increasing industrial and academic need to quickly analyze
graphs with billions of nodes and trillions of edges. A common first step for
network understanding is Graph Embedding, the process of creating a continuous
representation of nodes in a graph. A continuous representation is often more
amenable, especially at scale, for solving downstream machine learning tasks
such as classification, link prediction, and clustering. A high-performance
graph embedding architecture leveraging Tensor Processing Units (TPUs) with
configurable amounts of high-bandwidth memory is presented that simplifies the
graph embedding problem and can scale to graphs with billions of nodes and
trillions of edges. We verify the embedding space quality on real and synthetic
large-scale datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14500">A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1">Nimrod Dvir</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1">Elaine Friedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1">Suraj Commuri</a>, <a href="http://arxiv.org/find/cs/1/au:+yang_F/0/1/0/all/0/1">Fan yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1">Jennifer Romano</a></p>
<p>This study introduces and empirically tests a novel predictive model for
digital information engagement (IE) - the READ model, an acronym for the four
pivotal attributes of engaging information: Representativeness, Ease-of-use,
Affect, and Distribution. Conceptualized within the theoretical framework of
Cumulative Prospect Theory, the model integrates key cognitive biases with
computational linguistics and natural language processing to develop a
multidimensional perspective on information engagement. A rigorous testing
protocol was implemented, involving 50 randomly selected pairs of synonymous
words (100 words in total) from the WordNet database. These words' engagement
levels were evaluated through a large-scale online survey (n = 80,500) to
derive empirical IE metrics. The READ attributes for each word were then
computed and their predictive efficacy examined. The findings affirm the READ
model's robustness, accurately predicting a word's IE level and distinguishing
the more engaging word from a pair of synonyms with an 84% accuracy rate. The
READ model's potential extends across various domains, including business,
education, government, and healthcare, where it could enhance content
engagement and inform AI language model development and generative text work.
Future research should address the model's scalability and adaptability across
different domains and languages, thereby broadening its applicability and
efficacy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14502">The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Close_G/0/1/0/all/0/1">George Close</a>, <a href="http://arxiv.org/find/eess/1/au:+Hain_T/0/1/0/all/0/1">Thomas Hain</a>, <a href="http://arxiv.org/find/eess/1/au:+Goetze_S/0/1/0/all/0/1">Stefan Goetze</a></p>
<p>Recent work in the field of speech enhancement (SE) has involved the use of
self-supervised speech representations (SSSRs) as feature transformations in
loss functions. However, in prior work, very little attention has been paid to
the relationship between the language of the audio used to train the
self-supervised representation and that used to train the SE system.
Enhancement models trained using a loss function which incorporates a
self-supervised representation that shares exactly the language of the noisy
data used to train the SE system show better performance than those which do
not match exactly. This may lead to enhancement systems which are language
specific and as such do not generalise well to unseen languages, unlike models
trained using traditional spectrogram or time domain loss functions. In this
work, SE models are trained and tested on a number of different languages, with
self-supervised representations which themselves are trained using different
language combinations and with differing network structures as loss function
representations. These models are then tested across unseen languages and their
performances are analysed. It is found that the training language of the
self-supervised representation appears to have a minor effect on enhancement
performance, the amount of training data of a particular language, however,
greatly affects performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14512">Bug Characterization in Machine Learning-based Systems. (arXiv:2307.14512v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morovati_M/0/1/0/all/0/1">Mohammad Mehdi Morovati</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1">Amin Nikanjam</a>, <a href="http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1">Florian Tambon</a>, <a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1">Foutse Khomh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1">Zhen Ming</a> (Jack) <a href="http://arxiv.org/find/cs/1/au:+Jiang/0/1/0/all/0/1">Jiang</a></p>
<p>Rapid growth of applying Machine Learning (ML) in different domains,
especially in safety-critical areas, increases the need for reliable ML
components, i.e., a software component operating based on ML. Understanding the
bugs characteristics and maintenance challenges in ML-based systems can help
developers of these systems to identify where to focus maintenance and testing
efforts, by giving insights into the most error-prone components, most common
bugs, etc. In this paper, we investigate the characteristics of bugs in
ML-based software systems and the difference between ML and non-ML bugs from
the maintenance viewpoint. We extracted 447,948 GitHub repositories that used
one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and
PyTorch. After multiple filtering steps, we select the top 300 repositories
with the highest number of closed issues. We manually investigate the extracted
repositories to exclude non-ML-based systems. Our investigation involved a
manual inspection of 386 sampled reported issues in the identified ML-based
systems to indicate whether they affect ML components or not. Our analysis
shows that nearly half of the real issues reported in ML-based systems are ML
bugs, indicating that ML components are more error-prone than non-ML
components. Next, we thoroughly examined 109 identified ML bugs to identify
their root causes, symptoms, and calculate their required fixing time. The
results also revealed that ML bugs have significantly different characteristics
compared to non-ML bugs, in terms of the complexity of bug-fixing (number of
commits, changed files, and changed lines of code). Based on our results,
fixing ML bugs are more costly and ML components are more error-prone, compared
to non-ML bugs and non-ML components respectively. Hence, paying a significant
attention to the reliability of the ML components is crucial in ML-based
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14527">Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manzini_T/0/1/0/all/0/1">Thomas Manzini</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1">Robin Murphy</a></p>
<p>This paper details the challenges in applying two computer vision systems, an
EfficientDET supervised learning model and the unsupervised RX spectral
classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and
rescue (WSAR) effort in Japan and identifies 3 directions for future research.
There have been at least 19 proposed approaches and 3 datasets aimed at
locating missing persons in drone imagery, but only 3 approaches (2
unsupervised and 1 of an unknown structure) are referenced in the literature as
having been used in an actual WSAR operation. Of these proposed approaches, the
EfficientDET architecture and the unsupervised spectral RX classifier were
selected as the most appropriate for this setting. The EfficientDET model was
applied to the HERIDAL dataset and despite achieving performance that is
statistically equivalent to the state-of-the-art, the model fails to translate
to the real world in terms of false positives (e.g., identifying tree limbs and
rocks as people), and false negatives (e.g., failing to identify members of the
search team). The poor results in practice for algorithms that showed good
results on datasets suggest 3 areas of future research: more realistic datasets
for wilderness SAR, computer vision models that are capable of seamlessly
handling the variety of imagery that can be collected during actual WSAR
operations, and better alignment on performance measures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14528">Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM. (arXiv:2307.14528v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garrigos_G/0/1/0/all/0/1">Guillaume Garrigos</a>, <a href="http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1">Robert M. Gower</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaipp_F/0/1/0/all/0/1">Fabian Schaipp</a></p>
<p>Here we develop variants of SGD (stochastic gradient descent) with an
adaptive step size that make use of the sampled loss values. In particular, we
focus on solving a finite sum-of-terms problem, also known as empirical risk
minimization. We first detail an idealized adaptive method called
$\texttt{SPS}_+$ that makes use of the sampled loss values and assumes
knowledge of the sampled loss at optimality. This $\texttt{SPS}_+$ is a minor
modification of the SPS (Stochastic Polyak Stepsize) method, where the step
size is enforced to be positive. We then show that $\texttt{SPS}_+$ achieves
the best known rates of convergence for SGD in the Lipschitz non-smooth. We
then move onto to develop $\texttt{FUVAL}$, a variant of $\texttt{SPS}_+$ where
the loss values at optimality are gradually learned, as opposed to being given.
We give three viewpoints of $\texttt{FUVAL}$, as a projection based method, as
a variant of the prox-linear method, and then as a particular online SGD
method. We then present a convergence analysis of $\texttt{FUVAL}$ and
experimental results. The shortcomings of our work is that the convergence
analysis of $\texttt{FUVAL}$ shows no advantage over SGD. Another shortcomming
is that currently only the full batch version of $\texttt{FUVAL}$ shows a minor
advantages of GD (Gradient Descent) in terms of sensitivity to the step size.
The stochastic version shows no clear advantage over SGD. We conjecture that
large mini-batches are required to make $\texttt{FUVAL}$ competitive.
</p>
<p>Currently the new $\texttt{FUVAL}$ method studied in this paper does not
offer any clear theoretical or practical advantage. We have chosen to make this
draft available online nonetheless because of some of the analysis techniques
we use, such as the non-smooth analysis of $\texttt{SPS}_+$, and also to show
an apparently interesting approach that currently does not work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14530">Optimal Estimation in Mixed-Membership Stochastic Block Models. (arXiv:2307.14530v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Noskov_F/0/1/0/all/0/1">Fedor Noskov</a>, <a href="http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1">Maxim Panov</a></p>
<p>Community detection is one of the most critical problems in modern network
science. Its applications can be found in various fields, from protein modeling
to social network analysis. Recently, many papers appeared studying the problem
of overlapping community detection, where each node of a network may belong to
several communities. In this work, we consider Mixed-Membership Stochastic
Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite
a general setting for modeling overlapping community structure in graphs. The
central question of this paper is to reconstruct relations between communities
given an observed network. We compare different approaches and establish the
minimax lower bound on the estimation error. Then, we propose a new estimator
that matches this lower bound. Theoretical results are proved under fairly
general conditions on the considered model. Finally, we illustrate the theory
in a series of experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14531">Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel&#x27;s Spectrum. (arXiv:2307.14531v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geifman_A/0/1/0/all/0/1">Amnon Geifman</a>, <a href="http://arxiv.org/find/cs/1/au:+Barzilai_D/0/1/0/all/0/1">Daniel Barzilai</a>, <a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1">Ronen Basri</a>, <a href="http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1">Meirav Galun</a></p>
<p>Wide neural networks are biased towards learning certain functions,
influencing both the rate of convergence of gradient descent (GD) and the
functions that are reachable with GD in finite training time. As such, there is
a great need for methods that can modify this bias according to the task at
hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel
family of constructed kernels that can be used to approximate kernels with
desired eigenvalues for which no closed form is known. We leverage the duality
between wide neural networks and Neural Tangent Kernels and propose a
preconditioned gradient descent method, which alters the trajectory of GD. As a
result, this allows for a polynomial and, in some cases, exponential training
speedup without changing the final solution. Our method is both computationally
efficient and simple to implement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14549">Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application. (arXiv:2307.14549v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jianjun Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Woon_W/0/1/0/all/0/1">Wei Lee Woon</a>, <a href="http://arxiv.org/find/cs/1/au:+Coba_L/0/1/0/all/0/1">Ludovik Coba</a></p>
<p>This paper presents an efficient algorithm to solve the sleeping bandit with
multiple plays problem in the context of an online recommendation system. The
problem involves bounded, adversarial loss and unknown i.i.d. distributions for
arm availability. The proposed algorithm extends the sleeping bandit algorithm
for single arm selection and is guaranteed to achieve theoretical performance
with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the
number of arms selected per time step, $N$ is the total number of arms, and $T$
is the time horizon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14565">Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples. (arXiv:2307.14565v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yeye He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1">Cong Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chauduri_S/0/1/0/all/0/1">Surajit Chauduri</a></p>
<p>Relational tables, where each row corresponds to an entity and each column
corresponds to an attribute, have been the standard for tables in relational
databases. However, such a standard cannot be taken for granted when dealing
with tables "in the wild". Our survey of real spreadsheet-tables and web-tables
shows that over 30% of such tables do not conform to the relational standard,
for which complex table-restructuring transformations are needed before these
tables can be queried easily using SQL-based analytics tools. Unfortunately,
the required transformations are non-trivial to program, which has become a
substantial pain point for technical and non-technical users alike, as
evidenced by large numbers of forum questions in places like StackOverflow and
Excel/Tableau forums.
</p>
<p>We develop an Auto-Tables system that can automatically synthesize pipelines
with multi-step transformations (in Python or other languages), to transform
non-relational tables into standard relational forms for downstream analytics,
obviating the need for users to manually program transformations. We compile an
extensive benchmark for this new task, by collecting 194 real test cases from
user spreadsheets and online forums. Our evaluation suggests that Auto-Tables
can successfully synthesize transformations for over 70% of test cases at
interactive speeds, without requiring any input from users, making this an
effective tool for both technical and non-technical users to prepare data for
analytics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14568">Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning. (arXiv:2307.14568v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Angulo_B/0/1/0/all/0/1">Brian Angulo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorbov_G/0/1/0/all/0/1">Gregory Gorbov</a>, <a href="http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1">Aleksandr Panov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1">Konstantin Yakovlev</a></p>
<p>While reinforcement learning algorithms have had great success in the field
of autonomous navigation, they cannot be straightforwardly applied to the real
autonomous systems without considering the safety constraints. The later are
crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To
highlight the importance of these constraints, in this study, we compare two
learnable navigation policies: safe and unsafe. The safe policy takes the
constraints into account, while the other does not. We show that the safe
policy is able to generate trajectories with more clearance (distance to the
obstacles) and makes less collisions while training without sacrificing the
overall performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14588">MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation. (arXiv:2307.14588v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1">Liang Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1">Mingxiao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1">Yi Cheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1">Pengfei Shao</a>, <a href="http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1">Shuwei Shen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1">Peng Yao</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1">Ronald X.Xu</a></p>
<p>The UNet architecture, based on Convolutional Neural Networks (CNN), has
demonstrated its remarkable performance in medical image analysis. However, it
faces challenges in capturing long-range dependencies due to the limited
receptive fields and inherent bias of convolutional operations. Recently,
numerous transformer-based techniques have been incorporated into the UNet
architecture to overcome this limitation by effectively capturing global
feature correlations. However, the integration of the Transformer modules may
result in the loss of local contextual information during the global feature
fusion process. To overcome these challenges, we propose a 2D medical image
segmentation model called Multi-scale Cross Perceptron Attention Network
(MCPA). The MCPA consists of three main components: an encoder, a decoder, and
a Cross Perceptron. The Cross Perceptron first captures the local correlations
using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of
features across scales. The resulting multi-scale feature vectors are then
spatially unfolded, concatenated, and fed through a Global Perceptron module to
model global dependencies. Furthermore, we introduce a Progressive Dual-branch
Structure to address the semantic segmentation of the image involving finer
tissue structures. This structure gradually shifts the segmentation focus of
MCPA network training from large-scale structural features to more
sophisticated pixel-level features. We evaluate our proposed MCPA model on
several publicly available medical image datasets from different tasks and
devices, including the open large-scale dataset of CT (Synapse), MRI (ACDC),
fundus camera (DRIVE, CHASE_DB1, HRF), and OCTA (ROSE). The experimental
results show that our MCPA model achieves state-of-the-art performance. The
code is available at
https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14596">HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting. (arXiv:2307.14596v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zezhi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yuchen Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1">Guangyin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yongjun Xu</a></p>
<p>Traffic forecasting, which aims to predict traffic conditions based on
historical observations, has been an enduring research topic and is widely
recognized as an essential component of intelligent transportation. Recent
proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made
significant progress by combining sequential models with graph convolution
networks. However, due to high complexity issues, STGNNs only focus on
short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more
practical long-term forecasting. In this paper, we make the first attempt to
explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we
first reveal its unique challenges in exploiting multi-scale representations.
Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address
the issues of long-term traffic forecasting. HUTFormer consists of a
hierarchical encoder and decoder to jointly generate and utilize multi-scale
representations of traffic data. Specifically, for the encoder, we propose
window self-attention and segment merging to extract multi-scale
representations from long-term traffic data. For the decoder, we design a
cross-scale attention mechanism to effectively incorporate multi-scale
representations. In addition, HUTFormer employs an efficient input embedding
strategy to address the complexity issues. Extensive experiments on four
traffic datasets show that the proposed HUTFormer significantly outperforms
state-of-the-art traffic forecasting and long time series forecasting
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14609">Complete and separate: Conditional separation with missing target source attribute completion. (arXiv:2307.14609v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bralios_D/0/1/0/all/0/1">Dimitrios Bralios</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1">Efthymios Tzinis</a>, <a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1">Paris Smaragdis</a></p>
<p>Recent approaches in source separation leverage semantic information about
their input mixtures and constituent sources that when used in conditional
separation models can achieve impressive performance. Most approaches along
these lines have focused on simple descriptions, which are not always useful
for varying types of input mixtures. In this work, we present an approach in
which a model, given an input mixture and partial semantic information about a
target source, is trained to extract additional semantic data. We then leverage
this pre-trained model to improve the separation performance of an uncoupled
multi-conditional separation network. Our experiments demonstrate that the
separation performance of this multi-conditional model is significantly
improved, approaching the performance of an oracle model with complete semantic
information. Furthermore, our approach achieves performance levels that are
comparable to those of the best performing specialized single conditional
models, thus providing an easier to use alternative.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14613">Self-Contrastive Graph Diffusion Network. (arXiv:2307.14613v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yixian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1">Kun Zhan</a></p>
<p>Augmentation techniques and sampling strategies are crucial in contrastive
learning, but in most existing works, augmentation techniques require careful
design, and their sampling strategies can only capture a small amount of
intrinsic supervision information. Additionally, the existing methods require
complex designs to obtain two different representations of the data. To
overcome these limitations, we propose a novel framework called the
Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two
main components: the Attentional Module (AttM) and the Diffusion Module (DiFM).
AttM aggregates higher-order structure and feature information to get an
excellent embedding, while DiFM balances the state of each node in the graph
through Laplacian diffusion learning and allows the cooperative evolution of
adjacency and feature information in the graph. Unlike existing methodologies,
SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic
drift, without the need for pre-training. We conduct a high-quality sampling of
samples based on structure and feature information. If two nodes are neighbors,
they are considered positive samples of each other. If two disconnected nodes
are also unrelated on $k$NN graph, they are considered negative samples for
each other. The contrastive objective reasonably uses our proposed sampling
strategies, and the redundancy reduction term minimizes redundant information
in the embedding and can well retain more discriminative information. In this
novel framework, the graph self-contrastive learning paradigm gives expression
to a powerful force. SCGDN effectively balances between preserving high-order
structure information and avoiding overfitting. The results manifest that SCGDN
can consistently generate outperformance over both the contrastive methods and
the classical methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14619">Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Block_A/0/1/0/all/0/1">Adam Block</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfrommer_D/0/1/0/all/0/1">Daniel Pfrommer</a>, <a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1">Max Simchowitz</a></p>
<p>We propose a theoretical framework for studying the imitation of stochastic,
non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations
in nonlinear dynamical systems. Our framework invokes low-level controllers -
either learned or implicit in position-command control - to stabilize imitation
policies around expert demonstrations. We show that with (a) a suitable
low-level stability guarantee and (b) a stochastic continuity property of the
learned policy we call "total variation continuity" (TVC), an imitator that
accurately estimates actions on the demonstrator's state distribution closely
matches the demonstrator's distribution over entire trajectories. We then show
that TVC can be ensured with minimal degradation of accuracy by combining a
popular data-augmentation regimen with a novel algorithmic trick: adding
augmentation noise at execution time. We instantiate our guarantees for
policies parameterized by diffusion models and prove that if the learner
accurately estimates the score of the (noise-augmented) expert policy, then the
distribution of imitator trajectories is close to the demonstrator distribution
in a natural optimal transport distance. Our analysis constructs intricate
couplings between noise-augmented trajectories, a technique that may be of
independent interest. We conclude by empirically validating our algorithmic
recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14623">BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1">Sheikh Md Shakeel Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Feeney_A/0/1/0/all/0/1">Arthur Feeney</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhruv_A/0/1/0/all/0/1">Akash Dhruv</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jihoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Suh_Y/0/1/0/all/0/1">Youngjoon Suh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1">Jaiyoung Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Won_Y/0/1/0/all/0/1">Yoonjin Won</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandramowlishwaran_A/0/1/0/all/0/1">Aparna Chandramowlishwaran</a></p>
<p>In the field of phase change phenomena, the lack of accessible and diverse
datasets suitable for machine learning (ML) training poses a significant
challenge. Existing experimental datasets are often restricted, with limited
availability and sparse ground truth data, impeding our understanding of this
complex multi-physics phenomena. To bridge this gap, we present the BubbleML
Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven
simulations to provide accurate ground truth information for various boiling
scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled
boiling. This extensive dataset covers a wide range of parameters, including
varying gravity conditions, flow rates, sub-cooling levels, and wall superheat,
comprising 51 simulations. BubbleML is validated against experimental
observations and trends, establishing it as an invaluable resource for ML
research. Furthermore, we showcase its potential to facilitate exploration of
diverse downstream tasks by introducing two benchmarks: (a) optical flow
analysis to capture bubble dynamics, and (b) operator networks for learning
temperature dynamics. The BubbleML dataset and its benchmarks serve as a
catalyst for advancements in ML-driven research on multi-physics phase change
phenomena, enabling the development and comparison of state-of-the-art
techniques and models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14628">Rapid and Scalable Bayesian AB Testing. (arXiv:2307.14628v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chennu_S/0/1/0/all/0/1">Srivas Chennu</a>, <a href="http://arxiv.org/find/cs/1/au:+Maher_A/0/1/0/all/0/1">Andrew Maher</a>, <a href="http://arxiv.org/find/cs/1/au:+Pangerl_C/0/1/0/all/0/1">Christian Pangerl</a>, <a href="http://arxiv.org/find/cs/1/au:+Prabanantham_S/0/1/0/all/0/1">Subash Prabanantham</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1">Jae Hyeon Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1">Jamie Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Goswami_B/0/1/0/all/0/1">Bud Goswami</a></p>
<p>AB testing aids business operators with their decision making, and is
considered the gold standard method for learning from data to improve digital
user experiences. However, there is usually a gap between the requirements of
practitioners, and the constraints imposed by the statistical hypothesis
testing methodologies commonly used for analysis of AB tests. These include the
lack of statistical power in multivariate designs with many factors,
correlations between these factors, the need of sequential testing for early
stopping, and the inability to pool knowledge from past tests. Here, we propose
a solution that applies hierarchical Bayesian estimation to address the above
limitations. In comparison to current sequential AB testing methodology, we
increase statistical power by exploiting correlations between factors, enabling
sequential testing and progressive early stopping, without incurring excessive
false positive risk. We also demonstrate how this methodology can be extended
to enable the extraction of composite global learnings from past AB tests, to
accelerate future tests. We underpin our work with a solid theoretical
framework that articulates the value of hierarchical estimation. We demonstrate
its utility using both numerical simulations and a large set of real-world AB
tests. Together, these results highlight the practical value of our approach
for statistical inference in the technology industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14634">Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1">Razi Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Ge Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalra_M/0/1/0/all/0/1">Mannudeep Kalra</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1">Pingkun Yan</a></p>
<p>With advances in generative artificial intelligence (AI), it is now possible
to produce realistic-looking automated reports for preliminary reads of
radiology images. This can expedite clinical workflows, improve accuracy and
reduce overall costs. However, it is also well-known that such models often
hallucinate, leading to false findings in the generated reports. In this paper,
we propose a new method of fact-checking of AI-generated reports using their
associated images. Specifically, the developed examiner differentiates real and
fake sentences in reports by learning the association between an image and
sentences describing real or potentially fake findings. To train such an
examiner, we first created a new dataset of fake reports by perturbing the
findings in the original ground truth radiology reports associated with images.
Text encodings of real and fake sentences drawn from these reports are then
paired with image encodings to learn the mapping to real/fake labels. The
utility of such an examiner is demonstrated for verifying automatically
generated reports by detecting and removing fake sentences. Future generative
AI approaches can use the resulting tool to validate their reports leading to a
more responsible use of AI in expediting clinical workflows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14642">Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Kim_K/0/1/0/all/0/1">Kyurae Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1">Yian Ma</a>, <a href="http://arxiv.org/find/stat/1/au:+Gardner_J/0/1/0/all/0/1">Jacob R. Gardner</a></p>
<p>We prove that black-box variational inference (BBVI) with control variates,
particularly the sticking-the-landing (STL) estimator, converges at a geometric
(traditionally called "linear") rate under perfect variational family
specification. In particular, we prove a quadratic bound on the gradient
variance of the STL estimator, one which encompasses misspecified variational
families. Combined with previous works on the quadratic variance condition,
this directly implies convergence of BBVI with the use of projected stochastic
gradient descent. We also improve existing analysis on the regular closed-form
entropy gradient estimators, which enables comparison against the STL estimator
and provides explicit non-asymptotic complexity guarantees for both.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14643">MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nie_H/0/1/0/all/0/1">Haitao Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1">Bin Xie</a></p>
<p>How to accurately measure the relevance and redundancy of features is an
age-old challenge in the field of feature selection. However, existing
filter-based feature selection methods cannot directly measure redundancy for
continuous data. In addition, most methods rely on manually specifying the
number of features, which may introduce errors in the absence of expert
knowledge. In this paper, we propose a non-parametric feature selection
algorithm based on maximum inter-class variation and minimum redundancy,
abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel
density estimation on the features to capture their similarities and
differences in inter-class and overall distributions. Subsequently, we present
the criteria for maximum inter-class variation and minimum redundancy (MVMR),
wherein the inter-class probability distributions are employed to reflect
feature relevance and the distances between overall probability distributions
are used to quantify redundancy. Finally, we employ an AGA to search for the
feature subset that minimizes the MVMR. Compared with ten state-of-the-art
methods, MVMR-FS achieves the highest average accuracy and improves the
accuracy by 5% to 11%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14648">Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models. (arXiv:2307.14648v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1">Kevin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a></p>
<p>In this paper, we study the denoising diffusion probabilistic model (DDPM) in
wavelet space, instead of pixel space, for visual synthesis. Considering the
wavelet transform represents the image in spatial and frequency domains, we
carefully design a novel architecture SFUNet to effectively capture the
correlation for both domains. Specifically, in the standard denoising U-Net for
pixel data, we supplement the 2D convolutions and spatial-only attention layers
with our spatial frequency-aware convolution and attention modules to jointly
model the complementary information from spatial and frequency domains in
wavelet data. Our new architecture can be used as a drop-in replacement to the
pixel-based network and is compatible with the vanilla DDPM training process.
By explicitly modeling the wavelet signals, we find our model is able to
generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and
LSUN-Church datasets, than the pixel-based counterpart.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14653">Speed Limits for Deep Learning. (arXiv:2307.14653v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Seroussi_I/0/1/0/all/0/1">Inbar Seroussi</a>, <a href="http://arxiv.org/find/stat/1/au:+Alemi_A/0/1/0/all/0/1">Alexander A. Alemi</a>, <a href="http://arxiv.org/find/stat/1/au:+Helias_M/0/1/0/all/0/1">Moritz Helias</a>, <a href="http://arxiv.org/find/stat/1/au:+Ringel_Z/0/1/0/all/0/1">Zohar Ringel</a></p>
<p>State-of-the-art neural networks require extreme computational power to
train. It is therefore natural to wonder whether they are optimally trained.
Here we apply a recent advancement in stochastic thermodynamics which allows
bounding the speed at which one can go from the initial weight distribution to
the final distribution of the fully trained network, based on the ratio of
their Wasserstein-2 distance and the entropy production rate of the dynamical
process connecting them. Considering both gradient-flow and Langevin training
dynamics, we provide analytical expressions for these speed limits for linear
and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably,
given some plausible scaling assumptions on the NTK spectra and spectral
decomposition of the labels -- learning is optimal in a scaling sense. Our
results are consistent with small-scale experiments with Convolutional Neural
Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing
a short highly non-optimal regime followed by a longer optimal regime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14654">Machine Learning based Parameter Sensitivity of Regional Climate Models -- A Case Study of the WRF Model for Heat Extremes over Southeast Australia. (arXiv:2307.14654v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Reddy_P/0/1/0/all/0/1">P. Jyoteeshkumar Reddy</a>, <a href="http://arxiv.org/find/physics/1/au:+Chinta_S/0/1/0/all/0/1">Sandeep Chinta</a>, <a href="http://arxiv.org/find/physics/1/au:+Matear_R/0/1/0/all/0/1">Richard Matear</a>, <a href="http://arxiv.org/find/physics/1/au:+Taylor_J/0/1/0/all/0/1">John Taylor</a>, <a href="http://arxiv.org/find/physics/1/au:+Baki_H/0/1/0/all/0/1">Harish Baki</a>, <a href="http://arxiv.org/find/physics/1/au:+Thatcher_M/0/1/0/all/0/1">Marcus Thatcher</a>, <a href="http://arxiv.org/find/physics/1/au:+Kala_J/0/1/0/all/0/1">Jatin Kala</a>, <a href="http://arxiv.org/find/physics/1/au:+Sharples_J/0/1/0/all/0/1">Jason Sharples</a></p>
<p>Heatwaves and bushfires cause substantial impacts on society and ecosystems
across the globe. Accurate information of heat extremes is needed to support
the development of actionable mitigation and adaptation strategies. Regional
climate models are commonly used to better understand the dynamics of these
events. These models have very large input parameter sets, and the parameters
within the physics schemes substantially influence the model's performance.
However, parameter sensitivity analysis (SA) of regional models for heat
extremes is largely unexplored. Here, we focus on the southeast Australian
region, one of the global hotspots of heat extremes. In southeast Australia
Weather Research and Forecasting (WRF) model is the widely used regional model
to simulate extreme weather events across the region. Hence in this study, we
focus on the sensitivity of WRF model parameters to surface meteorological
variables such as temperature, relative humidity, and wind speed during two
extreme heat events over southeast Australia. Due to the presence of multiple
parameters and their complex relationship with output variables, a machine
learning (ML) surrogate-based global sensitivity analysis method is considered
for the SA. The ML surrogate-based Sobol SA is used to identify the sensitivity
of 24 adjustable parameters in seven different physics schemes of the WRF
model. Results show that out of these 24, only three parameters, namely the
scattering tuning parameter, multiplier of saturated soil water content, and
profile shape exponent in the momentum diffusivity coefficient, are important
for the considered meteorological variables. These SA results are consistent
for the two different extreme heat events. Further, we investigated the
physical significance of sensitive parameters. This study's results will help
in further optimising WRF parameters to improve model simulation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14657">Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance. (arXiv:2307.14657v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dambra_S/0/1/0/all/0/1">Savino Dambra</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yufei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Aonzo_S/0/1/0/all/0/1">Simone Aonzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotzias_P/0/1/0/all/0/1">Platon Kotzias</a>, <a href="http://arxiv.org/find/cs/1/au:+Vitale_A/0/1/0/all/0/1">Antonino Vitale</a>, <a href="http://arxiv.org/find/cs/1/au:+Caballero_J/0/1/0/all/0/1">Juan Caballero</a>, <a href="http://arxiv.org/find/cs/1/au:+Balzarotti_D/0/1/0/all/0/1">Davide Balzarotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilge_L/0/1/0/all/0/1">Leyla Bilge</a></p>
<p>Many studies have proposed machine-learning (ML) models for malware detection
and classification, reporting an almost-perfect performance. However, they
assemble ground-truth in different ways, use diverse static- and
dynamic-analysis techniques for feature extraction, and even differ on what
they consider a malware family. As a consequence, our community still lacks an
understanding of malware classification results: whether they are tied to the
nature and distribution of the collected dataset, to what extent the number of
families and samples in the training dataset influence performance, and how
well static and dynamic features complement each other.
</p>
<p>This work sheds light on those open questions. by investigating the key
factors influencing ML-based malware detection and classification. For this, we
collect the largest balanced malware dataset so far with 67K samples from 670
families (100 samples each), and train state-of-the-art models for malware
detection and family classification using our dataset. Our results reveal that
static features perform better than dynamic features, and that combining both
only provides marginal improvement over static features. We discover no
correlation between packing and classification accuracy, and that missing
behaviors in dynamically-extracted features highly penalize their performance.
We also demonstrate how a larger number of families to classify make the
classification harder, while a higher number of samples per family increases
accuracy. Finally, we find that models trained on a uniform distribution of
samples per family better generalize on unseen data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14668">Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment. (arXiv:2307.14668v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Sen Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1">Weishen Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changshui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a></p>
<p>Algorithmic fairness has been a serious concern and received lots of interest
in machine learning community. In this paper, we focus on the bipartite ranking
scenario, where the instances come from either the positive or negative class
and the goal is to learn a ranking function that ranks positive instances
higher than negative ones. While there could be a trade-off between fairness
and performance, we propose a model agnostic post-processing framework xOrder
for achieving fairness in bipartite ranking and maintaining the algorithm
classification performance. In particular, we optimize a weighted sum of the
utility as identifying an optimal warping path across different protected
groups and solve it through a dynamic programming process. xOrder is compatible
with various classification models and ranking fairness metrics, including
supervised and unsupervised fairness metrics. In addition to binary groups,
xOrder can be applied to multiple protected groups. We evaluate our proposed
algorithm on four benchmark data sets and two real-world patient electronic
health record repositories. xOrder consistently achieves a better balance
between the algorithm utility and ranking fairness on a variety of datasets
with different metrics. From the visualization of the calibrated ranking
scores, xOrder mitigates the score distribution shifts of different groups
compared with baselines. Moreover, additional analytical results verify that
xOrder achieves a robust performance when faced with fewer samples and a bigger
difference between training and testing ranking score distributions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14675">Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification. (arXiv:2307.14675v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gijon_A/0/1/0/all/0/1">Alfonso Gij&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Pujana_Goitia_A/0/1/0/all/0/1">Ainhoa Pujana-Goitia</a>, <a href="http://arxiv.org/find/cs/1/au:+Perea_E/0/1/0/all/0/1">Eugenio Perea</a>, <a href="http://arxiv.org/find/cs/1/au:+Molina_Solana_M/0/1/0/all/0/1">Miguel Molina-Solana</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_Romero_J/0/1/0/all/0/1">Juan G&#xf3;mez-Romero</a></p>
<p>The ever-growing use of wind energy makes necessary the optimization of
turbine operations through pitch angle controllers and their maintenance with
early fault detection. It is crucial to have accurate and robust models
imitating the behavior of wind turbines, especially to predict the generated
power as a function of the wind speed. Existing empirical and physics-based
models have limitations in capturing the complex relations between the input
variables and the power, aggravated by wind variability. Data-driven methods
offer new opportunities to enhance wind turbine modeling of large datasets by
improving accuracy and efficiency. In this study, we used physics-informed
neural networks to reproduce historical data coming from 4 turbines in a wind
farm, while imposing certain physical constraints to the model. The developed
models for regression of the power, torque, and power coefficient as output
variables showed great accuracy for both real data and physical equations
governing the system. Lastly, introducing an efficient evidential layer
provided uncertainty estimations of the predictions, proved to be consistent
with the absolute error, and made possible the definition of a confidence
interval in the power curve.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14680">TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting. (arXiv:2307.14680v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nancy Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosma_C/0/1/0/all/0/1">Chrysoula Kosma</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a></p>
<p>Time series forecasting lies at the core of important real-world applications
in many fields of science and engineering. The abundance of large time series
datasets that consist of complex patterns and long-term dependencies has led to
the development of various neural network architectures. Graph neural network
approaches, which jointly learn a graph structure based on the correlation of
raw values of multivariate time series while forecasting, have recently seen
great success. However, such solutions are often costly to train and difficult
to scale. In this paper, we propose TimeGNN, a method that learns dynamic
temporal graph representations that can capture the evolution of inter-series
patterns along with the correlations of multiple series. TimeGNN achieves
inference times 4 to 80 times faster than other state-of-the-art graph-based
methods while achieving comparable forecasting performance
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14729">Understanding Silent Failures in Medical Image Classification. (arXiv:2307.14729v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bungert_T/0/1/0/all/0/1">Till J. Bungert</a>, <a href="http://arxiv.org/find/eess/1/au:+Kobelke_L/0/1/0/all/0/1">Levin Kobelke</a>, <a href="http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1">Paul F. Jaeger</a></p>
<p>To ensure the reliable use of classification systems in medical applications,
it is crucial to prevent silent failures. This can be achieved by either
designing classifiers that are robust enough to avoid failures in the first
place, or by detecting remaining failures using confidence scoring functions
(CSFs). A predominant source of failures in image classification is
distribution shifts between training data and deployment data. To understand
the current state of silent failure prevention in medical imaging, we conduct
the first comprehensive analysis comparing various CSFs in four biomedical
tasks and a diverse range of distribution shifts. Based on the result that none
of the benchmarked CSFs can reliably prevent silent failures, we conclude that
a deeper understanding of the root causes of failures in the data is required.
To facilitate this, we introduce SF-Visuals, an interactive analysis tool that
uses latent space clustering to visualize shifts and failures. On the basis of
various examples, we demonstrate how this tool can help researchers gain
insight into the requirements for safe application of classification systems in
the medical domain. The open-source benchmark and tool are at:
https://github.com/IML-DKFZ/sf-visuals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14732">A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory. (arXiv:2307.14732v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yeung_C/0/1/0/all/0/1">Calvin C. K. Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1">Keisuke Fujii</a></p>
<p>Complex interactions between two opposing agents frequently occur in domains
of machine learning, game theory, and other application domains. Quantitatively
analyzing the strategies involved can provide an objective basis for
decision-making. One such critical scenario is shot-taking in football, where
decisions, such as whether the attacker should shoot or pass the ball and
whether the defender should attempt to block the shot, play a crucial role in
the outcome of the game. However, there are currently no effective data-driven
and/or theory-based approaches to analyzing such situations. To address this
issue, we proposed a novel framework to analyze such scenarios based on game
theory, where we estimate the expected payoff with machine learning (ML)
models, and additional features for ML models were extracted with a
theory-based shot block model. Conventionally, successes or failures (1 or 0)
are used as payoffs, while a success shot (goal) is extremely rare in football.
Therefore, we proposed the Expected Probability of Shot On Target (xSOT) metric
to evaluate players' actions even if the shot results in no goal; this allows
for effective differentiation and comparison between different shots and even
enables counterfactual shot situation analysis. In our experiments, we have
validated the framework by comparing it with baseline and ablated models.
Furthermore, we have observed a high correlation between the xSOT and existing
metrics. This alignment of information suggests that xSOT provides valuable
insights. Lastly, as an illustration, we studied optimal strategies in the
World Cup 2022 and analyzed a shot situation in EURO 2020.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14748">Semantic Image Completion and Enhancement using GANs. (arXiv:2307.14748v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1">Priyansh Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1">Raahat Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1">Akshat Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1">Saumil Maheshwari</a></p>
<p>Semantic inpainting or image completion alludes to the task of inferring
arbitrary large missing regions in images based on image semantics. Since the
prediction of image pixels requires an indication of high-level context, this
makes it significantly tougher than image completion, which is often more
concerned with correcting data corruption and removing entire objects from the
input image. On the other hand, image enhancement attempts to eliminate
unwanted noise and blur from the image, along with sustaining most of the image
details. Efficient image completion and enhancement model should be able to
recover the corrupted and masked regions in images and then refine the image
further to increase the quality of the output image. Generative Adversarial
Networks (GAN), have turned out to be helpful in picture completion tasks. In
this chapter, we will discuss the underlying GAN architecture and how they can
be used used for image completion tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14751">FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tekgul_B/0/1/0/all/0/1">Buse G. A. Tekgul</a>, <a href="http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1">N. Asokan</a></p>
<p>We propose FLARE, the first fingerprinting mechanism to verify whether a
suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of
another (victim) policy. We first show that it is possible to find
non-transferable, universal adversarial masks, i.e., perturbations, to generate
adversarial examples that can successfully transfer from a victim policy to its
modified versions but not to independently trained policies. FLARE employs
these masks as fingerprints to verify the true ownership of stolen DRL policies
by measuring an action agreement value over states perturbed via such masks.
Our empirical evaluations show that FLARE is effective (100% action agreement
on stolen copies) and does not falsely accuse independent policies (no false
positives). FLARE is also robust to model modification attacks and cannot be
easily evaded by more informed adversaries without negatively impacting agent
performance. We also show that not all universal adversarial masks are suitable
candidates for fingerprints due to the inherent characteristics of DRL
policies. The spatio-temporal dynamics of DRL problems and sequential
decision-making process make characterizing the decision boundary of DRL
policies more difficult, as well as searching for universal masks that capture
the geometry of it.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14754">Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oesterling_A/0/1/0/all/0/1">Alex Oesterling</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jiaqi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1">Flavio P. Calmon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Hima Lakkaraju</a></p>
<p>As public consciousness regarding the collection and use of personal
information by corporations grows, it is of increasing importance that
consumers be active participants in the curation of corporate datasets. In
light of this, data governance frameworks such as the General Data Protection
Regulation (GDPR) have outlined the right to be forgotten as a key principle
allowing individuals to request that their personal data be deleted from the
databases and models used by organizations. To achieve forgetting in practice,
several machine unlearning methods have been proposed to address the
computational inefficiencies of retraining a model from scratch with each
unlearning request. While efficient online alternatives to retraining, it is
unclear how these methods impact other properties critical to real-world
applications, such as fairness. In this work, we propose the first fair machine
unlearning method that can provably and efficiently unlearn data instances
while preserving group fairness. We derive theoretical results which
demonstrate that our method can provably unlearn data instances while
maintaining fairness objectives. Extensive experimentation with real-world
datasets highlight the efficacy of our method at unlearning data instances
while preserving fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14758">Towards Practicable Sequential Shift Detectors. (arXiv:2307.14758v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cobb_O/0/1/0/all/0/1">Oliver Cobb</a>, <a href="http://arxiv.org/find/cs/1/au:+Looveren_A/0/1/0/all/0/1">Arnaud Van Looveren</a></p>
<p>There is a growing awareness of the harmful effects of distribution shift on
the performance of deployed machine learning models. Consequently, there is a
growing interest in detecting these shifts before associated costs have time to
accumulate. However, desiderata of crucial importance to the practicable
deployment of sequential shift detectors are typically overlooked by existing
works, precluding their widespread adoption. We identify three such desiderata,
highlight existing works relevant to their satisfaction, and recommend
impactful directions for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14778">MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data. (arXiv:2307.14778v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1">Jing Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1">Tianqi Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dongbo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yu Zhang</a></p>
<p>Non-intrusive load monitoring (NILM) identifies the status and power
consumption of various household appliances by disaggregating the total power
usage signal of an entire house. Efficient and accurate load monitoring
facilitates user profile establishment, intelligent household energy
management, and peak load shifting. This is beneficial for both the end-users
and utilities by improving the overall efficiency of a power distribution
network. Existing approaches mainly focus on developing an individual model for
each appliance. Those approaches typically rely on a large amount of
household-labeled data which is hard to collect. In this paper, we propose a
multi-appliance-task framework with a training-efficient sample augmentation
(SA) scheme that boosts the disaggregation performance with limited labeled
data. For each appliance, we develop a shared-hierarchical split structure for
its regression and classification tasks. In addition, we also propose a
two-dimensional attention mechanism in order to capture spatio-temporal
correlations among all appliances. With only one-day training data and limited
appliance operation profiles, the proposed SA algorithm can achieve comparable
test performance to the case of training with the full dataset. Finally,
simulation results show that our proposed approach features a significantly
improved performance over many baseline models. The relative errors can be
reduced by more than 50\% on average. The codes of this work are available at
https://github.com/jxiong22/MATNilm
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14783">Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sulun_S/0/1/0/all/0/1">Serkan Sulun</a>, <a href="http://arxiv.org/find/eess/1/au:+Oliveira_P/0/1/0/all/0/1">Pedro Oliveira</a>, <a href="http://arxiv.org/find/eess/1/au:+Viana_P/0/1/0/all/0/1">Paula Viana</a></p>
<p>We present a new large-scale emotion-labeled symbolic music dataset
consisting of 12k MIDI songs. To create this dataset, we first trained emotion
classification models on the GoEmotions dataset, achieving state-of-the-art
results with a model half the size of the baseline. We then applied these
models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide
range of fine-grained emotions, providing a valuable resource to explore the
connection between music and emotions and, especially, to develop models that
can generate music based on specific emotions. Our code for inference, trained
models, and datasets are available online.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14788">Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction. (arXiv:2307.14788v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1">Tiago Rodrigues de Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozos_O/0/1/0/all/0/1">Oscar Martinez Mozos</a></p>
<p>Autonomous systems in the road transportation network require intelligent
mechanisms that cope with uncertainty to foresee the future. In this paper, we
propose a multi-stage probabilistic approach for trajectory forecasting:
trajectory transformation to displacement space, clustering of displacement
time series, trajectory proposals, and ranking proposals. We introduce a new
deep feature clustering method, underlying self-conditioned GAN, which copes
better with distribution shifts than traditional methods. Additionally, we
propose novel distance-based ranking proposals to assign probabilities to the
generated trajectories that are more efficient yet accurate than an auxiliary
neural network. The overall system surpasses context-free deep generative
models in human and road agents trajectory data while performing similarly to
point estimators when comparing the most probable trajectory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14823">Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dubinin_I/0/1/0/all/0/1">Igor Dubinin</a>, <a href="http://arxiv.org/find/cs/1/au:+Effenberger_F/0/1/0/all/0/1">Felix Effenberger</a></p>
<p>Residual connections have been proposed as architecture-based inductive bias
to mitigate the problem of exploding and vanishing gradients and increase task
performance in both feed-forward and recurrent networks (RNNs) when trained
with the backpropagation algorithm. Yet, little is known about how residual
connections in RNNs influence their dynamics and fading memory properties.
Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which
residual connections result in well-defined Lyapunov exponents and allow for
studying properties of fading memory. We investigate how the residual
connections of WCRNNs influence their performance, network dynamics, and memory
properties on a set of benchmark tasks. We show that several distinct forms of
residual connections yield effective inductive biases that result in increased
network expressivity. In particular, residual connections that (i) result in
network dynamics at the proximity of the edge of chaos, (ii) allow networks to
capitalize on characteristic spectral properties of the data, and (iii) result
in heterogeneous memory properties are shown to increase practical
expressivity. In addition, we demonstrate how our results can be extended to
non-linear residuals and introduce a weakly coupled residual initialization
scheme that can be used for Elman RNNs
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14839">Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+English_E/0/1/0/all/0/1">Eshant English</a>, <a href="http://arxiv.org/find/stat/1/au:+Kirchler_M/0/1/0/all/0/1">Matthias Kirchler</a>, <a href="http://arxiv.org/find/stat/1/au:+Lippert_C/0/1/0/all/0/1">Christoph Lippert</a></p>
<p>Normalising Flows are generative models characterised by their invertible
architecture. However, the requirement of invertibility imposes constraints on
their expressiveness, necessitating a large number of parameters and innovative
architectural designs to achieve satisfactory outcomes. Whilst flow-based
models predominantly rely on neural-network-based transformations for
expressive designs, alternative transformation methods have received limited
attention. In this work, we present Ferumal flow, a novel kernelised
normalising flow paradigm that integrates kernels into the framework. Our
results demonstrate that a kernelised flow can yield competitive or superior
results compared to neural network-based flows whilst maintaining parameter
efficiency. Kernelised flows excel especially in the low-data regime, enabling
flexible non-parametric density estimation in applications with sparse data
availability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14849">Counterfactual Explanations for Graph Classification Through the Lenses of Density. (arXiv:2307.14849v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1">Carlo Abrate</a>, <a href="http://arxiv.org/find/cs/1/au:+Preti_G/0/1/0/all/0/1">Giulia Preti</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1">Francesco Bonchi</a></p>
<p>Counterfactual examples have emerged as an effective approach to produce
simple and understandable post-hoc explanations. In the context of graph
classification, previous work has focused on generating counterfactual
explanations by manipulating the most elementary units of a graph, i.e.,
removing an existing edge, or adding a non-existing one. In this paper, we
claim that such language of explanation might be too fine-grained, and turn our
attention to some of the main characterizing features of real-world complex
networks, such as the tendency to close triangles, the existence of recurring
motifs, and the organization into dense modules. We thus define a general
density-based counterfactual search framework to generate instance-level
counterfactual explanations for graph classifiers, which can be instantiated
with different notions of dense substructures. In particular, we show two
specific instantiations of this general framework: a method that searches for
counterfactual graphs by opening or closing triangles, and a method driven by
maximal cliques. We also discuss how the general method can be instantiated to
exploit any other notion of dense substructures, including, for instance, a
given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7
brain network datasets and compare the counterfactual statements generated
according to several widely-used metrics. Results confirm that adopting a
semantic-relevant unit of change like density is essential to define versatile
and interpretable counterfactual explanation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14857">Generative convective parametrization of dry atmospheric boundary layer. (arXiv:2307.14857v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Heyder_F/0/1/0/all/0/1">Florian Heyder</a>, <a href="http://arxiv.org/find/physics/1/au:+Mellado_J/0/1/0/all/0/1">Juan Pedro Mellado</a>, <a href="http://arxiv.org/find/physics/1/au:+Schumacher_J/0/1/0/all/0/1">J&#xf6;rg Schumacher</a></p>
<p>Turbulence parametrizations will remain a necessary building block in
kilometer-scale Earth system models. In convective boundary layers, where the
mean vertical gradients of conserved properties such as potential temperature
and moisture are approximately zero, the standard ansatz which relates
turbulent fluxes to mean vertical gradients via an eddy diffusivity has to be
extended by mass flux parametrizations for the typically asymmetric up- and
downdrafts in the atmospheric boundary layer. In this work, we present a
parametrization for a dry convective boundary layer based on a generative
adversarial network. The model incorporates the physics of self-similar layer
growth following from the classical mixed layer theory by Deardorff. This
enhances the training data base of the generative machine learning algorithm
and thus significantly improves the predicted statistics of the synthetically
generated turbulence fields at different heights inside the boundary layer. The
algorithm training is based on fully three-dimensional direct numerical
simulation data. Differently to stochastic parametrizations, our model is able
to predict the highly non-Gaussian transient statistics of buoyancy
fluctuations, vertical velocity, and buoyancy flux at different heights thus
also capturing the fastest thermals penetrating into the stabilized top region.
The results of our generative algorithm agree with standard two-equation or
multi-plume stochastic mass-flux schemes. The present parametrization provides
additionally the granule-type horizontal organization of the turbulent
convection which cannot be obtained in any of the other model closures. Our
work paves the way to efficient data-driven convective parametrizations in
other natural flows, such as moist convection, upper ocean mixing, or
convection in stellar interiors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14902">CodeLens: An Interactive Tool for Visualizing Code Representations. (arXiv:2307.14902v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuejun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bettaieb_S/0/1/0/all/0/1">Seifeddine Bettaieb</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Traon_Y/0/1/0/all/0/1">Yves Le Traon</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1">Qiang Tang</a></p>
<p>Representing source code in a generic input format is crucial to automate
software engineering tasks, e.g., applying machine learning algorithms to
extract information. Visualizing code representations can further enable human
experts to gain an intuitive insight into the code. Unfortunately, as of today,
there is no universal tool that can simultaneously visualise different types of
code representations. In this paper, we introduce a tool, CodeLens, which
provides a visual interaction environment that supports various representation
methods and helps developers understand and explore them. CodeLens is designed
to support multiple programming languages, such as Java, Python, and
JavaScript, and four types of code representations, including sequence of
tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow
graph (CFG). By using CodeLens, developers can quickly visualize the specific
code representation and also obtain the represented inputs for models of code.
The Web-based interface of CodeLens is available at <a href="http://www.codelens.org.">this http URL</a>
The demonstration video can be found at <a href="http://www.codelens.org/demo.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14906">Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilm_T/0/1/0/all/0/1">Timo Wilm</a>, <a href="http://arxiv.org/find/cs/1/au:+Normann_P/0/1/0/all/0/1">Philipp Normann</a>, <a href="http://arxiv.org/find/cs/1/au:+Baumeister_S/0/1/0/all/0/1">Sophie Baumeister</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobow_P/0/1/0/all/0/1">Paul-Vincent Kobow</a></p>
<p>This work introduces TRON, a scalable session-based Transformer Recommender
using Optimized Negative-sampling. Motivated by the scalability and performance
limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates
top-k negative sampling and listwise loss functions to enhance its
recommendation accuracy. Evaluations on relevant large-scale e-commerce
datasets show that TRON improves upon the recommendation quality of current
methods while maintaining training speeds similar to SASRec. A live A/B test
yielded an 18.14% increase in click-through rate over SASRec, highlighting the
potential of TRON in practical settings. For further research, we provide
access to our source code at https://github.com/otto-de/TRON and an anonymized
dataset at https://github.com/otto-de/recsys-dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14917">NSA: Naturalistic Support Artifact to Boost Network Confidence. (arXiv:2307.14917v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Abhijith Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Munz_P/0/1/0/all/0/1">Phil Munz</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1">Apurva Narayan</a></p>
<p>Visual AI systems are vulnerable to natural and synthetic physical corruption
in the real-world. Such corruption often arises unexpectedly and alters the
model's performance. In recent years, the primary focus has been on adversarial
attacks. However, natural corruptions (e.g., snow, fog, dust) are an
omnipresent threat to visual AI systems and should be considered equally
important. Many existing works propose interesting solutions to train robust
models against natural corruption. These works either leverage image
augmentations, which come with the additional cost of model training, or place
suspicious patches in the scene to design unadversarial examples. In this work,
we propose the idea of naturalistic support artifacts (NSA) for robust
prediction. The NSAs are shown to be beneficial in scenarios where model
parameters are inaccessible and adding artifacts in the scene is feasible. The
NSAs are natural looking objects generated through artifact training using
DC-GAN to have high visual fidelity in the scene. We test against natural
corruptions on the Imagenette dataset and observe the improvement in prediction
confidence score by four times. We also demonstrate NSA's capability to
increase adversarial accuracy by 8\% on average. Lastly, we qualitatively
analyze NSAs using saliency maps to understand how they help improve prediction
confidence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14921">Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems. (arXiv:2307.14921v1 [cs.PF])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Williams_W/0/1/0/all/0/1">Warren R. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Glandon_S/0/1/0/all/0/1">S. Ross Glandon</a>, <a href="http://arxiv.org/find/cs/1/au:+Morris_L/0/1/0/all/0/1">Luke L. Morris</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jing-Ru C. Cheng</a></p>
<p>Performance Benchmarking of HPC systems is an ongoing effort that seeks to
provide information that will allow for increased performance and improve the
job schedulers that manage these systems. We develop a benchmarking tool that
utilizes machine learning models and gathers performance data on
GPU-accelerated nodes while they perform material segmentation analysis. The
benchmark uses a ML model that has been converted from Caffe to PyTorch using
the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on
two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while
Vulcanite has faster model times in a large number of benchmarks, and it is
also more subject to some environmental factors that can cause performances
slower than Onyx. In contrast the model times from Onyx are consistent across
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14928">Graph-based Polyphonic Multitrack Music Generation. (arXiv:2307.14928v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cosenza_E/0/1/0/all/0/1">Emanuele Cosenza</a>, <a href="http://arxiv.org/find/cs/1/au:+Valenti_A/0/1/0/all/0/1">Andrea Valenti</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1">Davide Bacciu</a></p>
<p>Graphs can be leveraged to model polyphonic multitrack symbolic music, where
notes, chords and entire sections may be linked at different levels of the
musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a
lack of works that consider graph representations in the context of deep
learning systems for music generation. This paper bridges this gap by
introducing a novel graph representation for music and a deep Variational
Autoencoder that generates the structure and the content of musical graphs
separately, one after the other, with a hierarchical architecture that matches
the structural priors of music. By separating the structure and content of
musical graphs, it is possible to condition generation by specifying which
instruments are played at certain times. This opens the door to a new form of
human-computer interaction in the context of music co-creation. After training
the model on existing MIDI datasets, the experiments show that the model is
able to generate appealing short and long musical sequences and to
realistically interpolate between them, producing music that is tonally and
rhythmically consistent. Finally, the visualization of the embeddings shows
that the model is able to organize its latent space in accordance with known
musical concepts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14935">Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1">George Chernishev</a>, <a href="http://arxiv.org/find/cs/1/au:+Polyntsov_M/0/1/0/all/0/1">Michael Polyntsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Chizhov_A/0/1/0/all/0/1">Anton Chizhov</a>, <a href="http://arxiv.org/find/cs/1/au:+Stupakov_K/0/1/0/all/0/1">Kirill Stupakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Shchuckin_I/0/1/0/all/0/1">Ilya Shchuckin</a>, <a href="http://arxiv.org/find/cs/1/au:+Smirnov_A/0/1/0/all/0/1">Alexander Smirnov</a>, <a href="http://arxiv.org/find/cs/1/au:+Strutovsky_M/0/1/0/all/0/1">Maxim Strutovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlyonskikh_A/0/1/0/all/0/1">Alexey Shlyonskikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Firsov_M/0/1/0/all/0/1">Mikhail Firsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Manannikov_S/0/1/0/all/0/1">Stepan Manannikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Bobrov_N/0/1/0/all/0/1">Nikita Bobrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Goncharov_D/0/1/0/all/0/1">Daniil Goncharov</a>, <a href="http://arxiv.org/find/cs/1/au:+Barutkin_I/0/1/0/all/0/1">Ilia Barutkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalnev_V/0/1/0/all/0/1">Vladislav Shalnev</a>, <a href="http://arxiv.org/find/cs/1/au:+Muraviev_K/0/1/0/all/0/1">Kirill Muraviev</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakhmukova_A/0/1/0/all/0/1">Anna Rakhmukova</a>, <a href="http://arxiv.org/find/cs/1/au:+Shcheka_D/0/1/0/all/0/1">Dmitriy Shcheka</a>, <a href="http://arxiv.org/find/cs/1/au:+Chernikov_A/0/1/0/all/0/1">Anton Chernikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Vyrodov_M/0/1/0/all/0/1">Mikhail Vyrodov</a>, <a href="http://arxiv.org/find/cs/1/au:+Yaroslav_K/0/1/0/all/0/1">Kurbatov Yaroslav</a>, <a href="http://arxiv.org/find/cs/1/au:+Fofanov_M/0/1/0/all/0/1">Maxim Fofanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Sergei_B/0/1/0/all/0/1">Belokonnyi Sergei</a>, <a href="http://arxiv.org/find/cs/1/au:+Pavel_A/0/1/0/all/0/1">Anosov Pavel</a>, <a href="http://arxiv.org/find/cs/1/au:+Saliou_A/0/1/0/all/0/1">Arthur Saliou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaisin_E/0/1/0/all/0/1">Eduard Gaisin</a>, <a href="http://arxiv.org/find/cs/1/au:+Smirnov_K/0/1/0/all/0/1">Kirill Smirnov</a></p>
<p>Data profiling is an essential process in modern data-driven industries. One
of its critical components is the discovery and validation of complex
statistics, including functional dependencies, data constraints, association
rules, and others.
</p>
<p>However, most existing data profiling systems that focus on complex
statistics do not provide proper integration with the tools used by
contemporary data scientists. This creates a significant barrier to the
adoption of these tools in the industry. Moreover, existing systems were not
created with industrial-grade workloads in mind. Finally, they do not aim to
provide descriptive explanations, i.e. why a given pattern is not found. It is
a significant issue as it is essential to understand the underlying reasons for
a specific pattern's absence to make informed decisions based on the data.
</p>
<p>Because of that, these patterns are effectively rest in thin air: their
application scope is rather limited, they are rarely used by the broader
public. At the same time, as we are going to demonstrate in this presentation,
complex statistics can be efficiently used to solve many classic data quality
problems.
</p>
<p>Desbordante is an open-source data profiler that aims to close this gap. It
is built with emphasis on industrial application: it is efficient, scalable,
resilient to crashes, and provides explanations. Furthermore, it provides
seamless Python integration by offloading various costly operations to the C++
core, not only mining.
</p>
<p>In this demonstration, we show several scenarios that allow end users to
solve different data quality problems. Namely, we showcase typo detection, data
deduplication, and data anomaly detection scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14936">PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Bo Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Taihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1">Daoguang Zan</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1">Bing Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1">An Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Muhan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1">Ailun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jichuan Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jingyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuenan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianxiang Wang</a></p>
<p>Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14938">Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops. (arXiv:2307.14938v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jafarpour_S/0/1/0/all/0/1">Saber Jafarpour</a>, <a href="http://arxiv.org/find/eess/1/au:+Harapanahalli_A/0/1/0/all/0/1">Akash Harapanahalli</a>, <a href="http://arxiv.org/find/eess/1/au:+Coogan_S/0/1/0/all/0/1">Samuel Coogan</a></p>
<p>In this paper, we propose a computationally efficient framework for interval
reachability of neural network controlled systems. Our approach builds upon
inclusion functions for the neural network controller and the open-loop system.
We observe that many state-of-the-art neural network verifiers can produce
inclusion functions for neural networks. We introduce and analyze a new class
of inclusion functions for the open-loop dynamics based on bounds of the
function Jacobian that is particularly suitable for capturing the interactions
between systems and neural network controllers. Next, for any dynamical system,
we use inclusion functions to construct an embedding system with twice the
number of states as the original system. We show that a single trajectory of
this embedding system provides hyper-rectangular over-approximations of
reachable sets. We then propose two approaches for constructing a closed-loop
embedding system for a neural network controlled dynamical system that accounts
for the interaction between the system and the controller in different ways.
The interconnection-based approach accounts for the worst-case evolution of
each coordinate separately by substituting the neural network inclusion
function into the open-loop embedding system. The interaction-based approach
uses the newly introduced class of Jacobian-based inclusion functions to fully
capture first-order interactions between the system and the controller.
Finally, we implement our approach in a Python framework called
\texttt{ReachMM} and show that on several existing benchmarks, our methods
outperform the existing approaches in the literature. We also demonstrate the
scalability of our method on a vehicle platooning example with up to $200$
states.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14940">A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1">C. Coelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_M/0/1/0/all/0/1">M. Fernanda P. Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferras_L/0/1/0/all/0/1">L. L. Ferr&#xe1;s</a></p>
<p>The continuous dynamics of natural systems has been effectively modelled
using Neural Ordinary Differential Equations (Neural ODEs). However, for
accurate and meaningful predictions, it is crucial that the models follow the
underlying rules or laws that govern these systems. In this work, we propose a
self-adaptive penalty algorithm for Neural ODEs to enable modelling of
constrained natural systems. The proposed self-adaptive penalty function can
dynamically adjust the penalty parameters. The explicit introduction of prior
knowledge helps to increase the interpretability of Neural ODE -based models.
We validate the proposed approach by modelling three natural systems with prior
knowledge constraints: population growth, chemical reaction evolution, and
damped harmonic oscillator motion. The numerical experiments and a comparison
with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE,
demonstrate the effectiveness of the proposed self-adaptive penalty algorithm
for Neural ODEs in modelling constrained natural systems. Moreover, the
self-adaptive penalty approach provides more accurate and robust models with
reliable and meaningful predictions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14952">Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning. (arXiv:2307.14952v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mclaughlin_C/0/1/0/all/0/1">Connor Mclaughlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Matthew Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Edogmus_D/0/1/0/all/0/1">Denis Edogmus</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1">Lili Su</a></p>
<p>As the network scale increases, existing fully distributed solutions start to
lag behind the real-world challenges such as (1) slow information propagation,
(2) network communication failures, and (3) external adversarial attacks. In
this paper, we focus on hierarchical system architecture and address the
problem of non-Bayesian learning over networks that are vulnerable to
communication failures and adversarial attacks. On network communication, we
consider packet-dropping link failures.
</p>
<p>We first propose a hierarchical robust push-sum algorithm that can achieve
average consensus despite frequent packet-dropping link failures. We provide a
sparse information fusion rule between the parameter server and arbitrarily
selected network representatives. Then, interleaving the consensus update step
with a dual averaging update with Kullback-Leibler (KL) divergence as the
proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian
learning algorithm with provable convergence guarantees.
</p>
<p>On external adversarial attacks, we consider Byzantine attacks in which the
compromised agents can send maliciously calibrated messages to others
(including both the agents and the parameter server). To avoid the curse of
dimensionality of Byzantine consensus, we solve the non-Bayesian learning
problem via running multiple dynamics, each of which only involves Byzantine
consensus with scalar inputs. To facilitate resilient information propagation
across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at
the parameter server.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14953">Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1">Eduardo Fernandes Montesuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1">Fred Ngol&#xe8; Mboula</a>, <a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1">Antoine Souloumiac</a></p>
<p>This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims
to mitigate data distribution shifts when transferring knowledge from multiple
labeled source domains to an unlabeled target domain. We propose a novel MSDA
framework based on dictionary learning and optimal transport. We interpret each
domain in MSDA as an empirical distribution. As such, we express each domain as
a Wasserstein barycenter of dictionary atoms, which are empirical
distributions. We propose a novel algorithm, DaDiL, for learning via
mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.
Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based
on the reconstruction of labeled samples in the target domain, and DaDiL-E,
based on the ensembling of classifiers learned on atom distributions. We
evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU,
where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in
classification performance. Finally, we show that interpolations in the
Wasserstein hull of learned atoms provide data that can generalize to the
target domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14959">Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Elbatel_M/0/1/0/all/0/1">Marawan Elbatel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hualiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Marti_R/0/1/0/all/0/1">Robert Mart&#xed;</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaomeng Li</a></p>
<p>In the medical field, federated learning commonly deals with highly
imbalanced datasets, including skin lesions and gastrointestinal images.
Existing federated methods under highly imbalanced datasets primarily focus on
optimizing a global model without incorporating the intra-class variations that
can arise in medical imaging due to different populations, findings, and
scanners. In this paper, we study the inter-client intra-class variations with
publicly available self-supervised auxiliary networks. Specifically, we find
that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on
every client yields consistent divergence measurements. Based on these
findings, we derive a dynamic balanced model aggregation via self-supervised
priors (MAS) to guide the global model optimization. Fed-MAS can be utilized
with different local learning methods for effective model aggregation toward a
highly robust and unbiased global model. Our code is available at
\url{https://github.com/xmed-lab/Fed-MAS}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14970">Learning locally dominant force balances in active particle systems. (arXiv:2307.14970v1 [cond-mat.soft])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Sturm_D/0/1/0/all/0/1">Dominik Sturm</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Maddu_S/0/1/0/all/0/1">Suryanarayana Maddu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sbalzarini_I/0/1/0/all/0/1">Ivo F. Sbalzarini</a></p>
<p>We use a combination of unsupervised clustering and sparsity-promoting
inference algorithms to learn locally dominant force balances that explain
macroscopic pattern formation in self-organized active particle systems. The
self-organized emergence of macroscopic patterns from microscopic interactions
between self-propelled particles can be widely observed nature. Although
hydrodynamic theories help us better understand the physical basis of this
phenomenon, identifying a sufficient set of local interactions that shape,
regulate, and sustain self-organized structures in active particle systems
remains challenging. We investigate a classic hydrodynamic model of
self-propelled particles that produces a wide variety of patterns, like asters
and moving density bands. Our data-driven analysis shows that propagating bands
are formed by local alignment interactions driven by density gradients, while
steady-state asters are shaped by a mechanism of splay-induced negative
compressibility arising from strong particle interactions. Our method also
reveals analogous physical principles of pattern formation in a system where
the speed of the particle is influenced by local density. This demonstrates the
ability of our method to reveal physical commonalities across models. The
physical mechanisms inferred from the data are in excellent agreement with
analytical scaling arguments and experimental observations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14971">Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xumin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1">Yongming Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiwen Lu</a></p>
<p>With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14988">Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1">Or Sharir</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>Deep learning often faces the challenge of efficiently processing dynamic
inputs, such as sensor data or user inputs. For example, an AI writing
assistant is required to update its suggestions in real time as a document is
edited. Re-running the model each time is expensive, even with compression
techniques like knowledge distillation, pruning, or quantization. Instead, we
take an incremental computing approach, looking to reuse calculations as the
inputs change. However, the dense connectivity of conventional architectures
poses a major obstacle to incremental computation, as even minor input changes
cascade through the network and restrict information reuse. To address this, we
use vector quantization to discretize intermediate values in the network, which
filters out noisy and unnecessary modifications to hidden neurons, facilitating
the reuse of their values. We apply this approach to the transformers
architecture, creating an efficient incremental inference algorithm with
complexity proportional to the fraction of the modified inputs. Our experiments
with adapting the OPT-125M pre-trained language model demonstrate comparable
accuracy on document classification while requiring 12.1X (median) fewer
operations for processing sequences of atomic edits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14993">Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1">Stephen Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Anokhin_I/0/1/0/all/0/1">Ivan Anokhin</a>, <a href="http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1">David Krueger</a></p>
<p>We propose the Thinker algorithm, a novel approach that enables reinforcement
learning agents to autonomously interact with and utilize a learned world
model. The Thinker algorithm wraps the environment with a world model and
introduces new actions designed for interacting with the world model. These
model-interaction actions enable agents to perform planning by proposing
alternative plans to the world model before selecting a final action to execute
in the environment. This approach eliminates the need for hand-crafted planning
algorithms by enabling the agent to learn how to plan autonomously and allows
for easy interpretation of the agent's plan with visualization. We demonstrate
the algorithm's effectiveness through experimental results in the game of
Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves
state-of-the-art performance and competitive results, respectively.
Visualizations of agents trained with the Thinker algorithm demonstrate that
they have learned to plan effectively with the world model to select better
actions. The algorithm's generality opens a new research direction on how a
world model can be used in reinforcement learning and how planning can be
seamlessly integrated into an agent's decision-making process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15007">Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhalla_U/0/1/0/all/0/1">Usha Bhalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1">Suraj Srinivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Himabindu Lakkaraju</a></p>
<p>With the increased deployment of machine learning models in various
real-world applications, researchers and practitioners alike have emphasized
the need for explanations of model behaviour. To this end, two broad strategies
have been outlined in prior literature to explain models. Post hoc explanation
methods explain the behaviour of complex black-box models by highlighting
features that are critical to model predictions; however, prior work has shown
that these explanations may not be faithful, and even more concerning is our
inability to verify them. Specifically, it is nontrivial to evaluate if a given
attribution is correct with respect to the underlying model. Inherently
interpretable models, on the other hand, circumvent these issues by explicitly
encoding explanations into model architecture, meaning their explanations are
naturally faithful and verifiable, but they often exhibit poor predictive
performance due to their limited expressive power. In this work, we aim to
bridge the gap between the aforementioned strategies by proposing Verifiability
Tuning (VerT), a method that transforms black-box models into models that
naturally yield faithful and verifiable feature attributions. We begin by
introducing a formal theoretical framework to understand verifiability and show
that attributions produced by standard models cannot be verified. We then
leverage this framework to propose a method to build verifiable models and
feature attributions out of fully trained black-box models. Finally, we perform
extensive experiments on semi-synthetic and real-world datasets, and show that
VerT produces models that (1) yield explanations that are correct and
verifiable and (2) are faithful to the original black-box models they are meant
to explain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15008">A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1">Nicholas Carlini</a></p>
<p>Large language models (LLMs) are now highly capable at a diverse range of
tasks. This paper studies whether or not GPT-4, one such LLM, is capable of
assisting researchers in the field of adversarial machine learning. As a case
study, we evaluate the robustness of AI-Guardian, a recent defense to
adversarial examples published at IEEE S&amp;P 2023, a top computer security
conference. We completely break this defense: the proposed scheme does not
increase robustness compared to an undefended baseline.
</p>
<p>We write none of the code to attack this model, and instead prompt GPT-4 to
implement all attack algorithms following our instructions and guidance. This
process was surprisingly effective and efficient, with the language model at
times producing code from ambiguous instructions faster than the author of this
paper could have done. We conclude by discussing (1) the warning signs present
in the evaluation that suggested to us AI-Guardian would be broken, and (2) our
experience with designing attacks and performing novel research using the most
recent advances in language modeling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15010">Harnessing Synthetic Active Particles for Physical Reservoir Computing. (arXiv:2307.15010v1 [cond-mat.soft])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_X/0/1/0/all/0/1">Xiangzun Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Cichos_F/0/1/0/all/0/1">Frank Cichos</a></p>
<p>The processing of information is an indispensable property of living systems
realized by networks of active processes with enormous complexity. They have
inspired many variants of modern machine learning one of them being reservoir
computing, in which stimulating a network of nodes with fading memory enables
computations and complex predictions. Reservoirs are implemented on computer
hardware, but also on unconventional physical substrates such as mechanical
oscillators, spins, or bacteria often summarized as physical reservoir
computing. Here we demonstrate physical reservoir computing with a synthetic
active microparticle system that self-organizes from an active and passive
component into inherently noisy nonlinear dynamical units. The
self-organization and dynamical response of the unit is the result of a delayed
propulsion of the microswimmer to a passive target. A reservoir of such units
with a self-coupling via the delayed response can perform predictive tasks
despite the strong noise resulting from Brownian motion of the microswimmers.
To achieve efficient noise suppression, we introduce a special architecture
that uses historical reservoir states for output. Our results pave the way for
the study of information processing in synthetic self-organized active particle
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15016">How Good is Google Bard&#x27;s Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1">Haotong Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1">Ge-Peng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1">Deng-Ping Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1">Fahad Shahbaz Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a></p>
<p>Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in
the field of conversational AI. Notably, Bard has recently been updated to
handle visual inputs alongside text prompts during conversations. Given Bard's
impressive track record in handling textual inputs, we explore its capabilities
in understanding and interpreting visual data (images) conditioned by text
questions. This exploration holds the potential to unveil new insights and
challenges for Bard and other forthcoming multi-modal Generative models,
especially in addressing complex computer vision problems that demand accurate
visual and language understanding. Specifically, in this study, we focus on 15
diverse task scenarios encompassing regular, camouflaged, medical, under-water
and remote sensing data to comprehensively evaluate Bard's performance. Our
primary finding indicates that Bard still struggles in these vision scenarios,
highlighting the significant gap in vision-based understanding that needs to be
bridged in future developments. We expect that this empirical study will prove
valuable in advancing future models, leading to enhanced capabilities in
comprehending and interpreting fine-grained visual data. Our project is
released on https://github.com/htqin/GoogleBard-VisUnderstand
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15017">Samplable Anonymous Aggregation for Private Federated Data Analysis. (arXiv:2307.15017v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1">Kunal Talwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1">Audra McMillan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jina_V/0/1/0/all/0/1">Vojta Jina</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1">Vitaly Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Basile_B/0/1/0/all/0/1">Bailey Basile</a>, <a href="http://arxiv.org/find/cs/1/au:+Cahill_A/0/1/0/all/0/1">Aine Cahill</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_Y/0/1/0/all/0/1">Yi Sheng Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatzidakis_M/0/1/0/all/0/1">Mike Chatzidakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junye Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chick_O/0/1/0/all/0/1">Oliver Chick</a>, <a href="http://arxiv.org/find/cs/1/au:+Chitnis_M/0/1/0/all/0/1">Mona Chitnis</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganta_S/0/1/0/all/0/1">Suman Ganta</a>, <a href="http://arxiv.org/find/cs/1/au:+Goren_Y/0/1/0/all/0/1">Yusuf Goren</a>, <a href="http://arxiv.org/find/cs/1/au:+Granqvist_F/0/1/0/all/0/1">Filip Granqvist</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1">Kristine Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_F/0/1/0/all/0/1">Frederic Jacobs</a>, <a href="http://arxiv.org/find/cs/1/au:+Javidbakht_O/0/1/0/all/0/1">Omid Javidbakht</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Albert Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Low_R/0/1/0/all/0/1">Richard Low</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascenik_D/0/1/0/all/0/1">Dan Mascenik</a>, <a href="http://arxiv.org/find/cs/1/au:+Myers_S/0/1/0/all/0/1">Steve Myers</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1">David Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1">Wonhee Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Parsa_G/0/1/0/all/0/1">Gianni Parsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Pauly_T/0/1/0/all/0/1">Tommy Pauly</a>, <a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1">Christian Priebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Rishi_R/0/1/0/all/0/1">Rehan Rishi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rothblum_G/0/1/0/all/0/1">Guy Rothblum</a>, <a href="http://arxiv.org/find/cs/1/au:+Scaria_M/0/1/0/all/0/1">Michael Scaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1">Linmao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Congzheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarbe_K/0/1/0/all/0/1">Karl Tarbe</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_S/0/1/0/all/0/1">Sebastian Vogt</a>, <a href="http://arxiv.org/find/cs/1/au:+Winstrom_L/0/1/0/all/0/1">Luke Winstrom</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shundong Zhou</a></p>
<p>We revisit the problem of designing scalable protocols for private statistics
and private federated learning when each device holds its private data. Our
first contribution is to propose a simple primitive that allows for efficient
implementation of several commonly used algorithms, and allows for privacy
accounting that is close to that in the central setting without requiring the
strong trust assumptions it entails. Second, we propose a system architecture
that implements this primitive and perform a security analysis of the proposed
system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15019">Self-Supervised Graph Transformer for Deepfake Detection. (arXiv:2307.15019v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khormali_A/0/1/0/all/0/1">Aminollah Khormali</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">Jiann-Shiun Yuan</a></p>
<p>Deepfake detection methods have shown promising results in recognizing
forgeries within a given dataset, where training and testing take place on the
in-distribution dataset. However, their performance deteriorates significantly
when presented with unseen samples. As a result, a reliable deepfake detection
system must remain impartial to forgery types, appearance, and quality for
guaranteed generalizable detection performance. Despite various attempts to
enhance cross-dataset generalization, the problem remains challenging,
particularly when testing against common post-processing perturbations, such as
video compression or blur. Hence, this study introduces a deepfake detection
framework, leveraging a self-supervised pre-training model that delivers
exceptional generalization ability, withstanding common corruptions and
enabling feature explainability. The framework comprises three key components:
a feature extractor based on vision Transformer architecture that is
pre-trained via self-supervised contrastive learning methodology, a graph
convolution network coupled with a Transformer discriminator, and a graph
Transformer relevancy map that provides a better understanding of manipulated
regions and further explains the model's decision. To assess the effectiveness
of the proposed framework, several challenging experiments are conducted,
including in-data distribution performance, cross-dataset, cross-manipulation
generalization, and robustness against common post-production perturbations.
The results achieved demonstrate the remarkable effectiveness of the proposed
deepfake detection framework, surpassing the current state-of-the-art
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15034">Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1">Colin White</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1">Renbo Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1">Jean Kossaifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1">Gennady Pekhimenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1">Kamyar Azizzadenesheli</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a></p>
<p>The Fourier neural operator (FNO) is a powerful technique for learning
surrogate maps for partial differential equation (PDE) solution operators. For
many real-world applications, which often require high-resolution data points,
training time and memory usage are significant bottlenecks. While there are
mixed-precision training techniques for standard neural networks, those work
for real-valued datatypes on finite dimensions and therefore cannot be directly
applied to FNO, which crucially operates in the (complex-valued) Fourier domain
and in function spaces. On the other hand, since the Fourier transform is
already an approximation (due to discretization error), we do not need to
perform the operation at full precision. In this work, we (i) profile memory
and runtime for FNO with full and mixed-precision training, (ii) conduct a
study on the numerical stability of mixed-precision training of FNO, and (iii)
devise a training routine which substantially decreases training time and
memory usage (up to 34%), with little or no reduction in accuracy, on the
Navier-Stokes and Darcy flow equations. Combined with the recently proposed
tensorized FNO (Kossaifi et al., 2023), the resulting model has far better
performance while also being significantly faster than the original FNO.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1">Andy Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1">Matt Fredrikson</a></p>
<p>Because "out-of-the-box" large language models are capable of generating a
great deal of objectionable content, recent work has focused on aligning these
models in an attempt to prevent undesirable generation. While there has been
some success at circumventing these measures -- so-called "jailbreaks" against
LLMs -- these attacks have required significant human ingenuity and are brittle
in practice. In this paper, we propose a simple and effective attack method
that causes aligned language models to generate objectionable behaviors.
Specifically, our approach finds a suffix that, when attached to a wide range
of queries for an LLM to produce objectionable content, aims to maximize the
probability that the model produces an affirmative response (rather than
refusing to answer). However, instead of relying on manual engineering, our
approach automatically produces these adversarial suffixes by a combination of
greedy and gradient-based search techniques, and also improves over past
automatic prompt generation methods.
</p>
<p>Surprisingly, we find that the adversarial prompts generated by our approach
are quite transferable, including to black-box, publicly released LLMs.
Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
queries asking for many different types of objectionable content), as well as
multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
attack suffix is able to induce objectionable content in the public interfaces
to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
Pythia, Falcon, and others. In total, this work significantly advances the
state-of-the-art in adversarial attacks against aligned language models,
raising important questions about how such systems can be prevented from
producing objectionable information. Code is available at
github.com/llm-attacks/llm-attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15045">A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Momeni_S/0/1/0/all/0/1">Saleh Momeni</a>, <a href="http://arxiv.org/find/cs/1/au:+BabaAli_B/0/1/0/all/0/1">Bagher BabaAli</a></p>
<p>Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15053">On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation. (arXiv:2307.15053v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeunen_O/0/1/0/all/0/1">Olivier Jeunen</a>, <a href="http://arxiv.org/find/cs/1/au:+Potapov_I/0/1/0/all/0/1">Ivan Potapov</a>, <a href="http://arxiv.org/find/cs/1/au:+Ustimenko_A/0/1/0/all/0/1">Aleksei Ustimenko</a></p>
<p>Approaches to recommendation are typically evaluated in one of two ways: (1)
via a (simulated) online experiment, often seen as the gold standard, or (2)
via some offline evaluation procedure, where the goal is to approximate the
outcome of an online experiment. Several offline evaluation metrics have been
adopted in the literature, inspired by ranking metrics prevalent in the field
of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one
such metric that has seen widespread adoption in empirical studies, and higher
(n)DCG values have been used to present new methods as the state-of-the-art in
top-$n$ recommendation for many years.
</p>
<p>Our work takes a critical look at this approach, and investigates when we can
expect such metrics to approximate the gold standard outcome of an online
experiment. We formally present the assumptions that are necessary to consider
DCG an unbiased estimator of online reward and provide a derivation for this
metric from first principles, highlighting where we deviate from its
traditional uses in IR. Importantly, we show that normalising the metric
renders it inconsistent, in that even when DCG is unbiased, ranking competing
methods by their normalised DCG can invert their relative order. Through a
correlation analysis between off- and on-line experiments conducted on a
large-scale recommendation platform, we show that our unbiased DCG estimates
strongly correlate with online reward, even when some of the metric's inherent
assumptions are violated. This statement no longer holds for its normalised
variant, suggesting that nDCG's practical utility may be limited.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1912.13122">Towards Regulated Deep Learning. (arXiv:1912.13122v7 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcia_Camino_A/0/1/0/all/0/1">Andr&#xe9;s Garc&#xed;a-Camino</a></p>
<p>Regulation of Multi-Agent Systems (MAS) and Declarative Electronic
Institutions (DEIs) was a multidisciplinary research topic of the past decade
involving (Physical and Software) Agents and Law since the beginning, but
recently evolved towards News-claimed Robot Lawyer since 2016. One of these
first proposals of restricting the behaviour of Software Agents was Electronic
Institutions.However, with the recent reformulation of Artificial Neural
Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal
issues regarding the use of DL has raised concerns in the Artificial
Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly
addressed, we propose the Regulation of Artificial Neural Networks as
Agent-based Training of a special type of regulated Artificial Neural Network
that we call Institutional Neural Network (INN).The main purpose of this paper
is to bring attention to Artificial Teaching (AT) and to give a tentative
answer showing a proof-of-concept implementation of Regulated Deep Learning
(RDL). This paper introduces the former concept and provide $I^*$, a language
previously used to model declaratively and extend Electronic Institutions, as a
means to regulate the execution of Artificial Neural Networks and their
interactions with Artificial Teachers (ATs)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2005.00695">On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Sen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyang R. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1">Gregory Valiant</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1">Christopher R&#xe9;</a></p>
<p>Data augmentation is a powerful technique to improve performance in
applications such as image and text classification tasks. Yet, there is little
rigorous understanding of why and how various augmentations work. In this work,
we consider a family of linear transformations and study their effects on the
ridge estimator in an over-parametrized linear regression setting. First, we
show that transformations that preserve the labels of the data can improve
estimation by enlarging the span of the training data. Second, we show that
transformations that mix data can improve estimation by playing a
regularization effect. Finally, we validate our theoretical insights on MNIST.
Based on the insights, we propose an augmentation scheme that searches over the
space of transformations by how uncertain the model is about the transformed
data. We validate our proposed scheme on image and text datasets. For example,
our method outperforms random sampling methods by 1.24% on CIFAR-100 using
Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA
Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.09246">Experimental Study on Reinforcement Learning-based Control of an Acrobot. (arXiv:2011.09246v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dostal_L/0/1/0/all/0/1">Leo Dostal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bespalko_A/0/1/0/all/0/1">Alexej Bespalko</a>, <a href="http://arxiv.org/find/cs/1/au:+Duecker_D/0/1/0/all/0/1">Daniel A. Duecker</a></p>
<p>We present computational and experimental results on how artificial
intelligence (AI) learns to control an Acrobot using reinforcement learning
(RL). Thereby the experimental setup is designed as an embedded system, which
is of interest for robotics and energy harvesting applications. Specifically,
we study the control of angular velocity of the Acrobot, as well as control of
its total energy, which is the sum of the kinetic and the potential energy. By
this means the RL algorithm is designed to drive the angular velocity or the
energy of the first pendulum of the Acrobot towards a desired value. With this,
libration or full rotation of the unactuated pendulum of the Acrobot is
achieved. Moreover, investigations of the Acrobot control are carried out,
which lead to insights about the influence of the state space discretization,
the episode length, the action space or the mass of the driven pendulum on the
RL control. By further numerous simulations and experiments the effects of
parameter variations are evaluated.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.02626">Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Bena_G/0/1/0/all/0/1">Gabriel B&#xe9;na</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Goodman_D/0/1/0/all/0/1">Dan F. M. Goodman</a></p>
<p>It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold except
at extreme levels. We then systematically tested which features of the
environment and network do lead to the emergence of specialization. We used a
simple toy environment, task and network, allowing us precise control, and show
that in this setup, several distinct measures of specialization give
qualitatively similar results. We further find that (1) specialization can only
emerge in environments where features of that environment are meaningfully
separable, (2) specialization preferentially emerges when the network is
strongly resource-constrained, and (3) these findings are qualitatively similar
across different network architectures, but the quantitative relationships
depends on the architecture type. Finally, we show that functional
specialization varies dynamically across time, and demonstrate that these
dynamics depend on both the timing and bandwidth of information flow in the
network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligent systems in situations of real-world complexity. We propose that
thoroughly stress testing candidate definitions of functional modularity in
simplified scenarios before extending to more complex data, network models and
electrophysiological recordings is likely to be a fruitful approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.03328">Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning. (arXiv:2106.03328v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1">Jinhyun So</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1">Ramy E. Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Guler_B/0/1/0/all/0/1">Basak Guler</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1">Jiantao Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1">Salman Avestimehr</a></p>
<p>Secure aggregation is a critical component in federated learning (FL), which
enables the server to learn the aggregate model of the users without observing
their local models. Conventionally, secure aggregation algorithms focus only on
ensuring the privacy of individual users in a single training round. We contend
that such designs can lead to significant privacy leakages over multiple
training rounds, due to partial user selection/participation at each round of
FL. In fact, we show that the conventional random user selection strategies in
FL lead to leaking users' individual models within number of rounds that is
linear in the number of users. To address this challenge, we introduce a secure
aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees.
In particular, we introduce a new metric to quantify the privacy guarantees of
FL over multiple training rounds, and develop a structured user selection
strategy that guarantees the long-term privacy of each user (over any number of
training rounds). Our framework also carefully accounts for the fairness and
the average number of participating users at each round. Our experiments on
MNIST and CIFAR-10 datasets in the IID and the non-IID settings demonstrate the
performance improvement over the baselines, both in terms of privacy protection
and test accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.11264">Compositional federated learning: Applications in distributionally robust averaging and meta learning. (arXiv:2106.11264v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Feihu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junyi Li</a></p>
<p>In the paper, we propose an effective and efficient Compositional Federated
Learning (ComFedL) algorithm for solving a new compositional Federated Learning
(FL) framework, which frequently appears in many data mining and machine
learning problems with a hierarchical structure such as distributionally robust
FL and model-agnostic meta learning (MAML). Moreover, we study the convergence
analysis of our ComFedL algorithm under some mild conditions, and prove that it
achieves a convergence rate of $O(\frac{1}{\sqrt{T}})$, where $T$ denotes the
number of iteration. To the best of our knowledge, our new Compositional FL
framework is the first work to bridge federated learning with composition
stochastic optimization. In particular, we first transform the distributionally
robust FL (i.e., a minimax optimization problem) into a simple composition
optimization problem by using KL divergence regularization. At the same time,
we also first transform the distribution-agnostic MAML problem (i.e., a minimax
optimization problem) into a simple yet effective composition optimization
problem. Finally, we apply two popular machine learning tasks, i.e.,
distributionally robust FL and MAML to demonstrate the effectiveness of our
algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.11277">Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hendrickx_K/0/1/0/all/0/1">Kilian Hendrickx</a>, <a href="http://arxiv.org/find/cs/1/au:+Perini_L/0/1/0/all/0/1">Lorenzo Perini</a>, <a href="http://arxiv.org/find/cs/1/au:+Plas_D/0/1/0/all/0/1">Dries Van der Plas</a>, <a href="http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1">Wannes Meert</a>, <a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1">Jesse Davis</a></p>
<p>Machine learning models always make a prediction, even when it is likely to
be inaccurate. This behavior should be avoided in many decision support
applications, where mistakes can have severe consequences. Albeit already
studied in 1970, machine learning with rejection recently gained interest. This
machine learning subfield enables machine learning models to abstain from
making a prediction when likely to make a mistake.
</p>
<p>This survey aims to provide an overview on machine learning with rejection.
We introduce the conditions leading to two types of rejection, ambiguity and
novelty rejection, which we carefully formalize. Moreover, we review and
categorize strategies to evaluate a model's predictive and rejective quality.
Additionally, we define the existing architectures for models with rejection
and describe the standard techniques for learning such models. Finally, we
provide examples of relevant application domains and show how machine learning
with rejection relates to other machine learning research areas.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.13624">Towards Out-Of-Distribution Generalization: A Survey. (arXiv:2108.13624v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiashuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zheyan Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yue He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Renzhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1">Peng Cui</a></p>
<p>Traditional machine learning paradigms are based on the assumption that both
training and test data follow the same statistical pattern, which is
mathematically referred to as Independent and Identically Distributed
($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often
fails to hold due to unforeseen distributional shifts, leading to considerable
degradation in model performance upon deployment. This observed discrepancy
indicates the significance of investigating the Out-of-Distribution (OOD)
generalization problem. OOD generalization is an emerging topic of machine
learning research that focuses on complex scenarios wherein the distributions
of the test data differ from those of the training data. This paper represents
the first comprehensive, systematic review of OOD generalization, encompassing
a spectrum of aspects from problem definition, methodological development, and
evaluation procedures, to the implications and future directions of the field.
Our discussion begins with a precise, formal characterization of the OOD
generalization problem. Following that, we categorize existing methodologies
into three segments: unsupervised representation learning, supervised model
learning, and optimization, according to their positions within the overarching
learning process. We provide an in-depth discussion on representative
methodologies for each category, further elucidating the theoretical links
between them. Subsequently, we outline the prevailing benchmark datasets
employed in OOD generalization studies. To conclude, we overview the existing
body of work in this domain and suggest potential avenues for future research
on OOD generalization. A summary of the OOD generalization methodologies
surveyed in this paper can be accessed at
<a href="http://out-of-distribution-generalization.com.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.13934">RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes. (arXiv:2112.13934v3 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Habib_S/0/1/0/all/0/1">Salman Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Beemer_A/0/1/0/all/0/1">Allison Beemer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kliewer_J/0/1/0/all/0/1">Joerg Kliewer</a></p>
<p>In this work we propose RELDEC, a novel approach for sequential decoding of
moderate length low-density parity-check (LDPC) codes. The main idea behind
RELDEC is that an optimized decoding policy is subsequently obtained via
reinforcement learning based on a Markov decision process (MDP). In contrast to
our previous work, where an agent learns to schedule only a single check node
(CN) within a group (cluster) of CNs per iteration, in this work we train the
agent to schedule all CNs in a cluster, and all clusters in every iteration.
That is, in each learning step of RELDEC an agent learns to schedule CN
clusters sequentially depending on a reward associated with the outcome of
scheduling a particular cluster. We also modify the state space representation
of the MDP, enabling RELDEC to be suitable for larger block length LDPC codes
than those studied in our previous work. Furthermore, to address decoding under
varying channel conditions, we propose agile meta-RELDEC (AM-RELDEC) that
employs meta-reinforcement learning. The proposed RELDEC scheme significantly
outperforms standard flooding and random sequential decoding for a variety of
LDPC codes, including codes designed for 5G new radio.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.14704">Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaozhuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chuanqi Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1">Luo Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.10291">Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs. (arXiv:2206.10291v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1">Micha&#x142; Derezi&#x144;ski</a></p>
<p>Algorithmic Gaussianization is a phenomenon that can arise when using
randomized sketching or sampling methods to produce smaller representations of
large datasets: For certain tasks, these sketched representations have been
observed to exhibit many robust performance characteristics that are known to
occur when a data sample comes from a sub-gaussian random design, which is a
powerful statistical model of data distributions. However, this phenomenon has
only been studied for specific tasks and metrics, or by relying on
computationally expensive methods. We address this by providing an algorithmic
framework for gaussianizing data distributions via averaging, proving that it
is possible to efficiently construct data sketches that are nearly
indistinguishable (in terms of total variation distance) from sub-gaussian
random designs. In particular, relying on a recently introduced sketching
technique called Leverage Score Sparsified (LESS) embeddings, we show that one
can construct an $n\times d$ sketch of an $N\times d$ matrix $A$, where $n\ll
N$, that is nearly indistinguishable from a sub-gaussian design, in time
$O(\text{nnz}(A)\log N + nd^2)$, where $\text{nnz}(A)$ is the number of
non-zero entries in $A$. As a consequence, strong statistical guarantees and
precise asymptotics available for the estimators produced from sub-gaussian
designs (e.g., for least squares and Lasso regression, covariance estimation,
low-rank approximation, etc.) can be straightforwardly adapted to our sketching
framework. We illustrate this with a new approximation guarantee for sketched
least squares, among other examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.12481">Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1">Zulqarnain Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1">Davin Hill</a>, <a href="http://arxiv.org/find/cs/1/au:+Masoomi_A/0/1/0/all/0/1">Aria Masoomi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bone_J/0/1/0/all/0/1">Joshua Bone</a>, <a href="http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1">Jennifer Dy</a></p>
<p>Machine learning methods have significantly improved in their predictive
capabilities, but at the same time they are becoming more complex and less
transparent. As a result, explainers are often relied on to provide
interpretability to these black-box prediction models. As crucial diagnostics
tools, it is important that these explainers themselves are robust. In this
paper we focus on one particular aspect of robustness, namely that an explainer
should give similar explanations for similar data inputs. We formalize this
notion by introducing and defining explainer astuteness, analogous to
astuteness of prediction functions. Our formalism allows us to connect
explainer robustness to the predictor's probabilistic Lipschitzness, which
captures the probability of local smoothness of a function. We provide lower
bound guarantees on the astuteness of a variety of explainers (e.g., SHAP,
RISE, CXPlain) given the Lipschitzness of the prediction function. These
theoretical results imply that locally smooth prediction functions lend
themselves to locally robust explanations. We evaluate these results
empirically on simulated as well as real datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.12672">Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bogdanov_E/0/1/0/all/0/1">Eli Bogdanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1">Izack Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gal_A/0/1/0/all/0/1">Avigdor Gal</a></p>
<p>In this work we propose an algorithm for trace recovery from stochastically
known logs, a setting that is becoming more common with the increasing number
of sensors and predictive models that generate uncertain data. The suggested
approach calculates the conformance between a process model and a
stochastically known trace and recovers the best alignment within this
stochastic trace as the true trace. The paper offers an analysis of the impact
of various cost models on trace recovery accuracy and makes use of a product
multi-graph to compare alternative trace recovery options. The average accuracy
of our approach, evaluated using two publicly available datasets, is
impressive, with an average recovery accuracy score of 90-97%, significantly
improving a common heuristic that chooses the most likely value for each
uncertain activity. We believe that the effectiveness of the proposed algorithm
in recovering correct traces from stochastically known logs may be a powerful
aid for developing credible decision-making tools in uncertain settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.00052">Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yanwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1">Ching-Yun Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>One powerful paradigm in visual navigation is to predict actions from
observations directly. Training such an end-to-end system allows
representations useful for downstream tasks to emerge automatically. However,
the lack of inductive bias makes this system data inefficient. We hypothesize a
sufficient representation of the current view and the goal view for a
navigation policy can be learned by predicting the location and size of a crop
of the current view that corresponds to the goal. We further show that training
such random crop prediction in a self-supervised fashion purely on synthetic
noise images transfers well to natural home images. The learned representation
can then be bootstrapped to learn a navigation policy efficiently with little
interaction data. The code is available at https://yanweiw.github.io/noise2ptz
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.05776">Neural Networks for Scalar Input and Functional Output. (arXiv:2208.05776v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wu_S/0/1/0/all/0/1">Sidi Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Beaulac_C/0/1/0/all/0/1">C&#xe9;dric Beaulac</a>, <a href="http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1">Jiguo Cao</a></p>
<p>The regression of a functional response on a set of scalar predictors can be
a challenging task, especially if there is a large number of predictors, or the
relationship between those predictors and the response is nonlinear. In this
work, we propose a solution to this problem: a feed-forward neural network (NN)
designed to predict a functional response using scalar inputs. First, we
transform the functional response to a finite-dimensional representation and
construct an NN that outputs this representation. Then, we propose to modify
the output of an NN via the objective function and introduce different
objective functions for network training. The proposed models are suited for
both regularly and irregularly spaced data, and a roughness penalty can be
further applied to control the smoothness of the predicted curve. The
difficulty in implementing both those features lies in the definition of
objective functions that can be back-propagated. In our experiments, we
demonstrate that our model outperforms the conventional function-on-scalar
regression model in multiple scenarios while computationally scaling better
with the dimension of the predictors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10255">On the non-efficient PAC learnability of conjunctive queries. (arXiv:2208.10255v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cate_B/0/1/0/all/0/1">Balder ten Cate</a>, <a href="http://arxiv.org/find/cs/1/au:+Funk_M/0/1/0/all/0/1">Maurice Funk</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1">Jean Christoph Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lutz_C/0/1/0/all/0/1">Carsten Lutz</a></p>
<p>This note serves three purposes: (i) we provide a self-contained exposition
of the fact that conjunctive queries are not efficiently learnable in the
Probably-Approximately-Correct (PAC) model, paying clear attention to the
complicating fact that this concept class lacks the polynomial-size fitting
property, a property that is tacitly assumed in much of the computational
learning theory literature; (ii) we establish a strong negative PAC
learnability result that applies to many restricted classes of conjunctive
queries (CQs), including acyclic CQs for a wide range of notions of
"acyclicity"; (iii) we show that CQs (and UCQs) are efficiently PAC learnable
with membership queries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.11838">Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1">Alessandro Abate</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Almulla_Y/0/1/0/all/0/1">Yousif Almulla</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1">James Fox</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Hyland_D/0/1/0/all/0/1">David Hyland</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1">Michael Wooldridge</a> (1) ((1) University of Oxford)</p>
<p>Training reinforcement learning (RL) agents using scalar reward signals is
often infeasible when an environment has sparse and non-Markovian rewards.
Moreover, handcrafting these reward functions before training is prone to
misspecification, especially when the environment's dynamics are only partially
known. This paper proposes a novel pipeline for learning non-Markovian task
specifications as succinct finite-state `task automata' from episodes of agent
experience within unknown environments. We leverage two key algorithmic
insights. First, we learn a product MDP, a model composed of the
specification's automaton and the environment's MDP (both initially unknown),
by treating the product MDP as a partially observable MDP and using the
well-known Baum-Welch algorithm for learning hidden Markov models. Second, we
propose a novel method for distilling the task automaton (assumed to be a
deterministic finite automaton) from the learnt product MDP. Our learnt task
automaton enables the decomposition of a task into its constituent sub-tasks,
which improves the rate at which an RL agent can later synthesise an optimal
policy. It also provides an interpretable encoding of high-level environmental
and task features, so a human can readily verify that the agent has learnt
coherent tasks with no misspecifications. In addition, we take steps towards
ensuring that the learnt automaton is environment-agnostic, making it
well-suited for use in transfer learning. Finally, we provide experimental
results compared with two baselines to illustrate our algorithm's performance
in different environments and tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.06589">Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungeun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kijung Yoon</a></p>
<p>Graph neural networks (GNNs) have become compelling models designed to
perform learning and inference on graph-structured data. However, little work
has been done to understand the fundamental limitations of GNNs for scaling to
larger graphs and generalizing to out-of-distribution (OOD) inputs. In this
paper, we use a random graph generator to systematically investigate how the
graph size and structural properties affect the predictive performance of GNNs.
We present specific evidence that the average node degree is a key feature in
determining whether GNNs can generalize to unseen graphs, and that the use of
multiple node update functions can improve the generalization performance of
GNNs when dealing with graphs of multimodal degree distributions. Accordingly,
we propose a multi-module GNN framework that allows the network to adapt
flexibly to new graphs by generalizing a single canonical nonlinear
transformation over aggregated inputs. Our results show that the multi-module
GNNs improve the OOD generalization on a variety of inference tasks in the
direction of diverse structural features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.07436">Statistical process monitoring of artificial neural networks. (arXiv:2209.07436v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Malinovskaya_A/0/1/0/all/0/1">Anna Malinovskaya</a>, <a href="http://arxiv.org/find/stat/1/au:+Mozharovskyi_P/0/1/0/all/0/1">Pavlo Mozharovskyi</a>, <a href="http://arxiv.org/find/stat/1/au:+Otto_P/0/1/0/all/0/1">Philipp Otto</a></p>
<p>The rapid advancement of models based on artificial intelligence demands
innovative monitoring techniques which can operate in real time with low
computational costs. In machine learning, especially if we consider artificial
neural networks (ANNs), the models are often trained in a supervised manner.
Consequently, the learned relationship between the input and the output must
remain valid during the model's deployment. If this stationarity assumption
holds, we can conclude that the ANN provides accurate predictions. Otherwise,
the retraining or rebuilding of the model is required. We propose considering
the latent feature representation of the data (called "embedding") generated by
the ANN to determine the time when the data stream starts being nonstationary.
In particular, we monitor embeddings by applying multivariate control charts
based on the data depth calculation and normalized ranks. The performance of
the introduced method is compared with benchmark approaches for various ANN
architectures and different underlying data formats.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.10825">Nonsmooth Nonconvex-Nonconcave Minimax Optimization: Primal-Dual Balancing and Iteration Complexity Analysis. (arXiv:2209.10825v3 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1">Jiajin Li</a>, <a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1">Linglingzhi Zhu</a>, <a href="http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1">Anthony Man-Cho So</a></p>
<p>Nonconvex-nonconcave minimax optimization has gained widespread interest over
the last decade. However, most existing works focus on variants of gradient
descent-ascent (GDA) algorithms, which are only applicable to smooth
nonconvex-concave settings. To address this limitation, we propose a novel
algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which
can effectively handle a broad range of structured nonsmooth
nonconvex-nonconcave minimax problems. Specifically, we consider the setting
where the primal function has a nonsmooth composite structure and the dual
function possesses the Kurdyka-Lojasiewicz (KL) property with exponent $\theta
\in [0,1)$. We introduce a novel convergence analysis framework for smoothed
PLDA, the key components of which are our newly developed nonsmooth primal
error bound and dual error bound. Using this framework, we show that smoothed
PLDA can find both $\epsilon$-game-stationary points and
$\epsilon$-optimization-stationary points of the problems of interest in
$\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ iterations. Furthermore, when
$\theta \in [0,\frac{1}{2}]$, smoothed PLDA achieves the optimal iteration
complexity of $\mathcal{O}(\epsilon^{-2})$. To further demonstrate the
effectiveness and wide applicability of our analysis framework, we show that
certain max-structured problem possesses the KL property with exponent
$\theta=0$ under mild assumptions. As a by-product, we establish
algorithm-independent quantitative relationships among various stationarity
concepts, which may be of independent interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03124">Learning Transfer Operators by Kernel Density Estimation. (arXiv:2210.03124v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Surasinghe_S/0/1/0/all/0/1">Sudam Surasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Fish_J/0/1/0/all/0/1">Jeremie Fish</a>, <a href="http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1">Erik M. Bollt</a></p>
<p>Inference of transfer operators from data is often formulated as a classical
problem that hinges on the Ulam method. The conventional description, known as
the Ulam-Galerkin method, involves projecting onto basis functions represented
as characteristic functions supported over a fine grid of rectangles. From this
perspective, the Ulam-Galerkin approach can be interpreted as density
estimation using the histogram method. In this study, we recast the problem
within the framework of statistical density estimation. This alternative
perspective allows for an explicit and rigorous analysis of bias and variance,
thereby facilitating a discussion on the mean square error. Through
comprehensive examples utilizing the logistic map and a Markov map, we
demonstrate the validity and effectiveness of this approach in estimating the
eigenvectors of the Frobenius-Perron operator. We compare the performance of
Histogram Density Estimation(HDE) and Kernel Density Estimation(KDE) methods
and find that KDE generally outperforms HDE in terms of accuracy. However, it
is important to note that KDE exhibits limitations around boundary points and
jumps. Based on our research findings, we suggest the possibility of
incorporating other density estimation methods into this field and propose
future investigations into the application of KDE-based estimation for
high-dimensional maps. These findings provide valuable insights for researchers
and practitioners working on estimating the Frobenius-Perron operator and
highlight the potential of density estimation techniques in this area of study.
</p>
<p>Keywords: Transfer Operators; Frobenius-Perron operator; probability density
estimation; Ulam-Galerkin method; Kernel Density Estimation; Histogram Density
Estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09924">Predicting Winning Regions in Parity Games via Graph Neural Networks (Extended Abstract). (arXiv:2210.09924v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hecking_T/0/1/0/all/0/1">Tobias Hecking</a>, <a href="http://arxiv.org/find/cs/1/au:+Muthukrishnan_S/0/1/0/all/0/1">Swathy Muthukrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinert_A/0/1/0/all/0/1">Alexander Weinert</a></p>
<p>Solving parity games is a major building block for numerous applications in
reactive program verification and synthesis. While they can be solved
efficiently in practice, no known approach has a polynomial worst-case runtime
complexity. We present a incomplete polynomial-time approach to determining the
winning regions of parity games via graph neural networks.
</p>
<p>Our evaluation on 900 randomly generated parity games shows that this
approach is effective and efficient in practice. It correctly determines the
winning regions of $\sim$60\% of the games in our data set and only incurs
minor errors in the remaining ones. We believe that this approach can be
extended to efficiently solve parity games as well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08411">Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kandpal_N/0/1/0/all/0/1">Nikhil Kandpal</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1">Haikang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1">Adam Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1">Eric Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1">Colin Raffel</a></p>
<p>The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08944">Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality. (arXiv:2211.08944v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ohayon_G/0/1/0/all/0/1">Guy Ohayon</a>, <a href="http://arxiv.org/find/eess/1/au:+Adrai_T/0/1/0/all/0/1">Theo Adrai</a>, <a href="http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1">Michael Elad</a>, <a href="http://arxiv.org/find/eess/1/au:+Michaeli_T/0/1/0/all/0/1">Tomer Michaeli</a></p>
<p>Stochastic restoration algorithms allow to explore the space of solutions
that correspond to the degraded input. In this paper we reveal additional
fundamental advantages of stochastic methods over deterministic ones, which
further motivate their use. First, we prove that any restoration algorithm that
attains perfect perceptual quality and whose outputs are consistent with the
input must be a posterior sampler, and is thus required to be stochastic.
Second, we illustrate that while deterministic restoration algorithms may
attain high perceptual quality, this can be achieved only by filling up the
space of all possible source images using an extremely sensitive mapping, which
makes them highly vulnerable to adversarial attacks. Indeed, we show that
enforcing deterministic models to be robust to such attacks profoundly hinders
their perceptual quality, while robustifying stochastic models hardly
influences their perceptual quality, and improves their output variability.
These findings provide a motivation to foster progress in stochastic
restoration methods, paving the way to better recovery algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01555">Contrastive Domain Adaptation for Time-Series via Temporal Mixup. (arXiv:2212.01555v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1">Emadeldeen Eldele</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1">Mohamed Ragab</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenghua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1">Chee-Keong Kwoh</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoli Li</a></p>
<p>Unsupervised Domain Adaptation (UDA) has emerged as a powerful solution for
the domain shift problem via transferring the knowledge from a labeled source
domain to a shifted unlabeled target domain. Despite the prevalence of UDA for
visual applications, it remains relatively less explored for time-series
applications. In this work, we propose a novel lightweight contrastive domain
adaptation framework called CoTMix for time-series data. Unlike existing
approaches that either use statistical distances or adversarial techniques, we
leverage contrastive learning solely to mitigate the distribution shift across
the different domains. Specifically, we propose a novel temporal mixup strategy
to generate two intermediate augmented views for the source and target domains.
Subsequently, we leverage contrastive learning to maximize the similarity
between each domain and its corresponding augmented view. The generated views
consider the temporal dynamics of time-series data during the adaptation
process while inheriting the semantics among the two domains. Hence, we
gradually push both domains towards a common intermediate space, mitigating the
distribution shift across them. Extensive experiments conducted on five
real-world time-series datasets show that our approach can significantly
outperform all state-of-the-art UDA methods. The implementation code of CoTMix
is available at
\href{https://github.com/emadeldeen24/CoTMix}{github.com/emadeldeen24/CoTMix}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.07959">Scalable Bayesian Uncertainty Quantification for Neural Network Potentials: Promise and Pitfalls. (arXiv:2212.07959v2 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Thaler_S/0/1/0/all/0/1">Stephan Thaler</a>, <a href="http://arxiv.org/find/physics/1/au:+Doehner_G/0/1/0/all/0/1">Gregor Doehner</a>, <a href="http://arxiv.org/find/physics/1/au:+Zavadlav_J/0/1/0/all/0/1">Julija Zavadlav</a></p>
<p>Neural network (NN) potentials promise highly accurate molecular dynamics
(MD) simulations within the computational complexity of classical MD force
fields. However, when applied outside their training domain, NN potential
predictions can be inaccurate, increasing the need for Uncertainty
Quantification (UQ). Bayesian modeling provides the mathematical framework for
UQ, but classical Bayesian methods based on Markov chain Monte Carlo (MCMC) are
computationally intractable for NN potentials. By training graph NN potentials
for coarse-grained systems of liquid water and alanine dipeptide, we
demonstrate here that scalable Bayesian UQ via stochastic gradient MCMC
(SG-MCMC) yields reliable uncertainty estimates for MD observables. We show
that cold posteriors can reduce the required training data size and that for
reliable UQ, multiple Markov chains are needed. Additionally, we find that
SG-MCMC and the Deep Ensemble method achieve comparable results, despite
shorter training and less hyperparameter tuning of the latter. We show that
both methods can capture aleatoric and epistemic uncertainty reliably, but not
systematic uncertainty, which needs to be minimized by adequate modeling to
obtain accurate credible intervals for MD observables. Our results represent a
step towards accurate UQ that is of vital importance for trustworthy NN
potential-based MD simulations required for decision-making in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.13381">MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yingtian Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1">Vikas Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1">Sarthak Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1">Wai Hoh Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hieu Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1">Juho Kannala</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1">Arno Solin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1">Kenji Kawaguchi</a></p>
<p>Mixup is a popular data augmentation technique for training deep neural
networks where additional samples are generated by linearly interpolating pairs
of inputs and their labels. This technique is known to improve the
generalization performance in many learning paradigms and applications. In this
work, we first analyze Mixup and show that it implicitly regularizes infinitely
many directional derivatives of all orders. Based on this new insight, we
propose an improved version of Mixup, theoretically justified to deliver better
generalization performance than the vanilla Mixup. To demonstrate the
effectiveness of the proposed method, we conduct experiments across various
domains such as images, tabular data, speech, and graphs. Our results show that
the proposed method improves Mixup across multiple datasets using a variety of
architectures, for instance, exhibiting an improvement over Mixup by 0.8% in
ImageNet top-1 accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.01913">Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marty_T/0/1/0/all/0/1">Tom Marty</a>, <a href="http://arxiv.org/find/cs/1/au:+Francois_T/0/1/0/all/0/1">Tristan Fran&#xe7;ois</a>, <a href="http://arxiv.org/find/cs/1/au:+Tessier_P/0/1/0/all/0/1">Pierre Tessier</a>, <a href="http://arxiv.org/find/cs/1/au:+Gauthier_L/0/1/0/all/0/1">Louis Gauthier</a>, <a href="http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1">Louis-Martin Rousseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1">Quentin Cappart</a></p>
<p>Constraint programming is known for being an efficient approach for solving
combinatorial problems. Important design choices in a solver are the branching
heuristics, which are designed to lead the search to the best solutions in a
minimum amount of time. However, developing these heuristics is a
time-consuming process that requires problem-specific expertise. This
observation has motivated many efforts to use machine learning to automatically
learn efficient heuristics without expert intervention. To the best of our
knowledge, it is still an open research question. Although several generic
variable-selection heuristics are available in the literature, the options for
a generic value-selection heuristic are more scarce. In this paper, we propose
to tackle this issue by introducing a generic learning procedure that can be
used to obtain a value-selection heuristic inside a constraint programming
solver. This has been achieved thanks to the combination of a deep Q-learning
algorithm, a tailored reward signal, and a heterogeneous graph neural network
architecture. Experiments on graph coloring, maximum independent set, and
maximum cut problems show that our framework is able to find better solutions
close to optimality without requiring a large amounts of backtracks while being
generic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01198">Causal Lifting and Link Prediction. (arXiv:2302.01198v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cotta_L/0/1/0/all/0/1">Leonardo Cotta</a>, <a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1">Beatrice Bevilacqua</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1">Nesreen Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1">Bruno Ribeiro</a></p>
<p>Existing causal models for link prediction assume an underlying set of
inherent node factors -- an innate characteristic defined at the node's birth
-- that governs the causal evolution of links in the graph. In some causal
tasks, however, link formation is path-dependent: The outcome of link
interventions depends on existing links. Unfortunately, these existing causal
methods are not designed for path-dependent link formation, as the cascading
functional dependencies between links (arising from path dependence) are either
unidentifiable or require an impractical number of control variables. To
overcome this, we develop the first causal model capable of dealing with path
dependencies in link prediction. In this work we introduce the concept of
causal lifting, an invariance in causal models of independent interest that, on
graphs, allows the identification of causal link prediction queries using
limited interventional data. Further, we show how structural pairwise
embeddings exhibit lower bias and correctly represent the task's causal
structure, as opposed to existing node embeddings, e.g., graph neural network
node embeddings and matrix factorization. Finally, we validate our theoretical
findings on three scenarios for causal link prediction tasks: knowledge base
completion, covariance matrix estimation and consumer-product recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.01226">Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anpei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zexiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xinyue Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Siyu Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1">Andreas Geiger</a></p>
<p>We present Factor Fields, a novel framework for modeling and representing
signals. Factor Fields decomposes a signal into a product of factors, each
represented by a classical or neural field representation which operates on
transformed input coordinates. This decomposition results in a unified
framework that accommodates several recent signal representations including
NeRF, Plenoxels, EG3D, Instant-NGP, and TensoRF. Additionally, our framework
allows for the creation of powerful new signal representations, such as the
"Dictionary Field" (DiF) which is a second contribution of this paper. Our
experiments show that DiF leads to improvements in approximation quality,
compactness, and training time when compared to previous fast reconstruction
methods. Experimentally, our representation achieves better image approximation
quality on 2D image regression tasks, higher geometric quality when
reconstructing 3D signed distance fields, and higher compactness for radiance
field reconstruction tasks. Furthermore, DiF enables generalization to unseen
images/3D scenes by sharing bases across signals during training which greatly
benefits use cases such as image regression from sparse observations and
few-shot radiance field reconstruction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08424">From Contextual Data to Newsvendor Decisions: On the Actual Performance of Data-Driven Algorithms. (arXiv:2302.08424v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Besbes_O/0/1/0/all/0/1">Omar Besbes</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Will Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouchtaki_O/0/1/0/all/0/1">Omar Mouchtaki</a></p>
<p>In this work, we explore a framework for contextual decision-making to study
how the relevance and quantity of past data affects the performance of a
data-driven policy. We analyze a contextual Newsvendor problem in which a
decision-maker needs to trade-off between an underage and an overage cost in
the face of uncertain demand. We consider a setting in which past demands
observed under ``close by'' contexts come from close by distributions and
analyze the performance of data-driven algorithms through a notion of
context-dependent worst-case expected regret. We analyze the broad class of
Weighted Empirical Risk Minimization (WERM) policies which weigh past data
according to their similarity in the contextual space. This class includes
classical policies such as ERM, k-Nearest Neighbors and kernel-based policies.
Our main methodological contribution is to characterize exactly the worst-case
regret of any WERM policy on any given configuration of contexts. To the best
of our knowledge, this provides the first understanding of tight performance
guarantees in any contextual decision-making problem, with past literature
focusing on upper bounds via concentration inequalities. We instead take an
optimization approach, and isolate a structure in the Newsvendor loss function
that allows to reduce the infinite-dimensional optimization problem over
worst-case distributions to a simple line search.
</p>
<p>This in turn allows us to unveil fundamental insights that were obfuscated by
previous general-purpose bounds. We characterize actual guaranteed performance
as a function of the contexts, as well as granular insights on the learning
curve of algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08720">Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Annau_N/0/1/0/all/0/1">Nicolaas J. Annau</a>, <a href="http://arxiv.org/find/physics/1/au:+Cannon_A/0/1/0/all/0/1">Alex J. Cannon</a>, <a href="http://arxiv.org/find/physics/1/au:+Monahan_A/0/1/0/all/0/1">Adam H. Monahan</a></p>
<p>This paper explores the application of emerging machine learning methods from
image super-resolution (SR) to the task of statistical downscaling. We
specifically focus on convolutional neural network-based Generative Adversarial
Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to
generate high-resolution (HR) surface winds emulating Weather Research and
Forecasting (WRF) model simulations over North America. Unlike traditional SR
models, where LR inputs are idealized coarsened versions of the HR images, WRF
emulation involves using non-idealized LR and HR pairs resulting in
shared-scale mismatches due to internal variability. Our study builds upon
current SR-based statistical downscaling by experimenting with a novel
frequency-separation (FS) approach from the computer vision field. To assess
the skill of SR models, we carefully select evaluation metrics, and focus on
performance measures based on spatial power spectra. Our analyses reveal how
GAN configurations influence spatial structures in the generated fields,
particularly biases in spatial variability spectra. Using power spectra to
evaluate the FS experiments reveals that successful applications of FS in
computer vision do not translate to climate fields. However, the FS experiments
demonstrate the sensitivity of power spectra to a commonly used GAN-based SR
objective function, which helps interpret and understand its role in
determining spatial structures. This result motivates the development of a
novel partial frequency-separation scheme as a promising configuration option.
We also quantify the influence on GAN performance of non-idealized LR fields
resulting from internal variability. Furthermore, we conduct a spectra-based
feature-importance experiment allowing us to explore the dependence of the
spatial structure of generated fields on different physically relevant LR
covariates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12012">Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guharoy_R/0/1/0/all/0/1">Rabel Guharoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Jana_N/0/1/0/all/0/1">Nanda Dulal Jana</a>, <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Suparna Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_L/0/1/0/all/0/1">Lalit Garg</a></p>
<p>An Electroencephalogram (EEG) is a non-invasive exam that records the
electrical activity of the brain. This exam is used to help diagnose conditions
such as different brain problems. EEG signals are taken for the purpose of
epilepsy detection and with Discrete Wavelet Transform (DWT) and machine
learning classifier, they perform epilepsy detection. In Epilepsy seizure
detection, mainly machine learning classifiers and statistical features are
used. The hidden information in the EEG signal is useful for detecting diseases
affecting the brain. Sometimes it is very difficult to identify the minimum
changes in the EEG in the time and frequency domains purpose. The DWT can give
a good decomposition of the signals in different frequency bands and feature
extraction. We use the tri-dimensionality reduction algorithm.; Principal
Component Analysis (PCA), Independent Component Analysis (ICA), and Linear
Discriminant Analysis (LDA). Finally, features are selected by using a fusion
rule and at the last step three different classifiers Support Vector Machine
(SVM), Naive Bayes (NB) and K-Nearest-Neighbor(KNN) have been used individually
for the classification. The proposed framework is tested on the Bonn dataset
and the simulation results provide the accuracy for the combination of LDA and
SVM 89.17%, LDA and KNN 80.42%, PCA and NB 89.92%, PCA and SVM 85.58%, PCA and
KNN 80.42%, ICA and NB 82.33%, ICA and SVM 90.42%, and ICA and KNN 90%, LDA and
NB 100%, accuracy. It shows the sensitivity, specificity, accuracy, Precision,
and Recall of 100%, 100%, 100%, 100%, and 100%. This combination of LDA with NB
method provides the accuracy of 100% outperforming all existing methods. The
results prove the effectiveness of this model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01669">Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems. (arXiv:2303.01669v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1">Yangyang Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1">Anton van den Hengel</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lingqiao Liu</a></p>
<p>Self-supervised learning (SSL) strategies have demonstrated remarkable
performance in various recognition tasks. However, both our preliminary
investigation and recent studies suggest that they may be less effective in
learning representations for fine-grained visual recognition (FGVR) since many
features helpful for optimizing SSL objectives are not suitable for
characterizing the subtle differences in FGVR. To overcome this issue, we
propose learning an additional screening mechanism to identify discriminative
clues commonly seen across instances and classes, dubbed as common rationales
in this paper. Intuitively, common rationales tend to correspond to the
discriminative patterns from the key parts of foreground objects. We show that
a common rationale detector can be learned by simply exploiting the GradCAM
induced from the SSL objective without using any pre-trained object parts or
saliency detectors, making it seamlessly to be integrated with the existing SSL
process. Specifically, we fit the GradCAM with a branch with limited fitting
capacity, which allows the branch to capture the common rationales and discard
the less common discriminative patterns. At the test stage, the branch
generates a set of spatial weights to selectively aggregate features
representing an instance. Extensive experimental results on four visual tasks
demonstrate that the proposed method can lead to a significant improvement in
different evaluation settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02060">Spectral learning of Bernoulli linear dynamical systems models. (arXiv:2303.02060v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Stone_I/0/1/0/all/0/1">Iris R. Stone</a>, <a href="http://arxiv.org/find/stat/1/au:+Sagiv_Y/0/1/0/all/0/1">Yotam Sagiv</a>, <a href="http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1">Il Memming Park</a>, <a href="http://arxiv.org/find/stat/1/au:+Pillow_J/0/1/0/all/0/1">Jonathan W. Pillow</a></p>
<p>Latent linear dynamical systems with Bernoulli observations provide a
powerful modeling framework for identifying the temporal dynamics underlying
binary time series data, which arise in a variety of contexts such as binary
decision-making and discrete stochastic processes (e.g., binned neural spike
trains). Here we develop a spectral learning method for fast, efficient fitting
of probit-Bernoulli latent linear dynamical system (LDS) models. Our approach
extends traditional subspace identification methods to the Bernoulli setting
via a transformation of the first and second sample moments. This results in a
robust, fixed-cost estimator that avoids the hazards of local optima and the
long computation time of iterative fitting procedures like the
expectation-maximization (EM) algorithm. In regimes where data is limited or
assumptions about the statistical structure of the data are not met, we
demonstrate that the spectral estimate provides a good initialization for
Laplace-EM fitting. Finally, we show that the estimator provides substantial
benefits to real world settings by analyzing data from mice performing a
sensory decision-making task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10135">Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Atad_M/0/1/0/all/0/1">Matan Atad</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianxiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1">Ismael Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Durner_M/0/1/0/all/0/1">Maximilian Durner</a>, <a href="http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1">Rudolph Triebel</a></p>
<p>Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve
productivity and resilience in modern manufacturing along with the growing need
for greater product customization. One of the main challenges in realizing such
automation resides in efficiently finding solutions from a growing number of
potential sequences for increasingly complex assemblies. Besides, costly
feasibility checks are always required for the robotic system. To address this,
we propose a holistic graphical approach including a graph representation
called Assembly Graph for product assemblies and a policy architecture, Graph
Assembly Processing Network, dubbed GRACE for assembly sequence generation.
With GRACE, we are able to extract meaningful information from the graph input
and predict assembly sequences in a step-by-step manner. In experiments, we
show that our approach can predict feasible assembly sequences across product
variants of aluminum profiles based on data collected in simulation of a
dual-armed robotic system. We further demonstrate that our method is capable of
detecting infeasible assemblies, substantially alleviating the undesirable
impacts from false predictions, and hence facilitating real-world deployment
soon. Code and training data are available at https://github.com/DLR-RM/GRACE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.00570">FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1">Huidong Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1">Qiong Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiongchao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1">Xueqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_Z/0/1/0/all/0/1">Zhicheng Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1">Biao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Rominger_A/0/1/0/all/0/1">Axel Rominger</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_K/0/1/0/all/0/1">Kuangyu Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1">James S. Duncan</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chi Liu</a></p>
<p>Low-count PET is an efficient way to reduce radiation exposure and
acquisition time, but the reconstructed images often suffer from low
signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream
tasks. Recent advances in deep learning have shown great potential in improving
low-count PET image quality, but acquiring a large, centralized, and diverse
dataset from multiple institutions for training a robust model is difficult due
to privacy and security concerns of patient data. Moreover, low-count PET data
at different institutions may have different data distribution, thus requiring
personalized models. While previous federated learning (FL) algorithms enable
multi-institution collaborative training without the need of aggregating local
data, addressing the large domain shift in the application of
multi-institutional low-count PET denoising remains a challenge and is still
highly under-explored. In this work, we propose FedFTN, a personalized
federated learning strategy that addresses these challenges. FedFTN uses a
local deep feature transformation network (FTN) to modulate the feature outputs
of a globally shared denoising network, enabling personalized low-count PET
denoising for each institution. During the federated learning process, only the
denoising network's weights are communicated and aggregated, while the FTN
remains at the local institutions for feature transformation. We evaluated our
method using a large-scale dataset of multi-institutional low-count PET imaging
data from three medical centers located across three continents, and showed
that FedFTN provides high-quality low-count PET images, outperforming previous
baseline FL reconstruction methods across all low-count levels at all three
institutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06848">CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cannizzaro_R/0/1/0/all/0/1">Ricardo Cannizzaro</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunze_L/0/1/0/all/0/1">Lars Kunze</a></p>
<p>Robots operating in real-world environments must reason about possible
outcomes of stochastic actions and make decisions based on partial observations
of the true world state. A major challenge for making accurate and robust
action predictions is the problem of confounding, which if left untreated can
lead to prediction errors. The partially observable Markov decision process
(POMDP) is a widely-used framework to model these stochastic and
partially-observable decision-making problems. However, due to a lack of
explicit causal semantics, POMDP planning methods are prone to confounding bias
and thus in the presence of unobserved confounders may produce underperforming
policies. This paper presents a novel causally-informed extension of "anytime
regularized determinized sparse partially observable tree" (AR-DESPOT), a
modern anytime online POMDP planner, using causal modelling and inference to
eliminate errors caused by unmeasured confounder variables. We further propose
a method to learn offline the partial parameterisation of the causal model for
planning, from ground truth model data. We evaluate our methods on a toy
problem with an unobserved confounder and show that the learned causal model is
highly accurate, while our planning method is more robust to confounding and
produces overall higher performing policies than AR-DESPOT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13037">VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data. (arXiv:2304.13037v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1">Van-Duc Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Bui_C/0/1/0/all/0/1">Cuong-Tien Bui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wen-Syan Li</a></p>
<p>An end-to-end machine learning (ML) lifecycle consists of many iterative
processes, from data preparation and ML model design to model training and then
deploying the trained model for inference. When building an end-to-end
lifecycle for an ML problem, many ML pipelines must be designed and executed
that produce a huge number of lifecycle versions. Therefore, this paper
introduces VeML, a Version management system dedicated to end-to-end ML
Lifecycle. Our system tackles several crucial problems that other systems have
not solved. First, we address the high cost of building an ML lifecycle,
especially for large-scale and high-dimensional dataset. We solve this problem
by proposing to transfer the lifecycle of similar datasets managed in our
system to the new training data. We design an algorithm based on the core set
to compute similarity for large-scale, high-dimensional data efficiently.
Another critical issue is the model accuracy degradation by the difference
between training data and testing data during the ML lifetime, which leads to
lifecycle rebuild. Our system helps to detect this mismatch without getting
labeled data from testing data and rebuild the ML lifecycle for a new data
version. To demonstrate our contributions, we conduct experiments on
real-world, large-scale datasets of driving images and spatiotemporal sensor
data and show promising results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02109">Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v2 [cs.NI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdisarabshali_P/0/1/0/all/0/1">Payam Abdisarabshali</a>, <a href="http://arxiv.org/find/cs/1/au:+Accurso_N/0/1/0/all/0/1">Nicholas Accurso</a>, <a href="http://arxiv.org/find/cs/1/au:+Malandra_F/0/1/0/all/0/1">Filippo Malandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Weifeng Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1">Seyyedali Hosseinalipour</a></p>
<p>Federated learning (FL) is the most popular distributed machine learning
technique. However, implementation of FL over modern wireless networks faces
key challenges caused by (i) dynamics of the network conditions and (ii) the
coexistence of multiple FL services/tasks and other network services in the
system, which are not jointly considered in prior works. Motivated by these
challenges, we introduce a generic FL paradigm over NextG networks, called
dynamic multi-service FL (DMS-FL). We identify three unexplored design
considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless
resource fragmentation, and (iii) signal strength fluctuations. We take the
first steps towards addressing these design considerations by proposing a novel
distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL
unleashes the full potential of Open RAN (O-RAN) systems and introduces an
elastic resource provisioning methodology to execute FL services. It further
constitutes a multi-time-scale FL management system that introduces three
dimensions into existing FL architectures: (i) virtualization, (ii)
scalability, and (iii) elasticity. Through investigating EV-FL, we reveal a
series of open research directions for future work. We finally simulate EV-FL
to demonstrate its potential in saving wireless resources and increasing
fairness among FL services.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08890">Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1">Tianxiang Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuanpeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yong Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a></p>
<p>Fuzzy time series forecasting (FTSF) is a typical forecasting method with
wide application. Traditional FTSF is regarded as an expert system which leads
to loss of the ability to recognize undefined features. The mentioned is the
main reason for poor forecasting with FTSF. To solve the problem, the proposed
model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes a
convolution neural network to re-implement FTSF with learnable ability. DFCNN
is capable of recognizing potential information and improving forecasting
accuracy. Thanks to the learnable ability of the neural network, the length of
fuzzy rules established in FTSF is expended to an arbitrary length that the
expert is not able to handle by the expert system. At the same time, FTSF
usually cannot achieve satisfactory performance of non-stationary time series
due to the trend of non-stationary time series. The trend of non-stationary
time series causes the fuzzy set established by FTSF to be invalid and causes
the forecasting to fail. DFCNN utilizes the Difference algorithm to weaken the
non-stationary of time series so that DFCNN can forecast the non-stationary
time series with a low error that FTSF cannot forecast in satisfactory
performance. After the mass of experiments, DFCNN has an excellent prediction
effect, which is ahead of the existing FTSF and common time series forecasting
algorithms. Finally, DFCNN provides further ideas for improving FTSF and holds
continued research value.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13106">On Learning the Tail Quantiles of Driving Behavior Distributions via Quantile Regression and Flows. (arXiv:2305.13106v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tee_J/0/1/0/all/0/1">Jia Yu Tee</a>, <a href="http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1">Oliver De Candido</a>, <a href="http://arxiv.org/find/cs/1/au:+Utschick_W/0/1/0/all/0/1">Wolfgang Utschick</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_P/0/1/0/all/0/1">Philipp Geiger</a></p>
<p>Towards safe autonomous driving (AD), we consider the problem of learning
models that accurately capture the diversity and tail quantiles of human driver
behavior probability distributions, in interaction with an AD vehicle. Such
models, which predict drivers' continuous actions from their states, are
particularly relevant for closing the gap between AD agent simulations and
reality. To this end, we adapt two flexible quantile learning frameworks for
this setting that avoid strong distributional assumptions: (1) quantile
regression (based on the titled absolute loss), and (2) autoregressive quantile
flows (a version of normalizing flows). Training happens in a behavior
cloning-fashion. We use the highD dataset consisting of driver trajectories on
several highways. We evaluate our approach in a one-step acceleration
prediction task, and in multi-step driver simulation rollouts. We report
quantitative results using the tilted absolute loss as metric, give qualitative
examples showing that realistic extremal behavior can be learned, and discuss
the main insights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13849">Gaussian Latent Representations for Uncertainty Estimation using Mahalanobis Distance in Deep Classifiers. (arXiv:2305.13849v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Venkataramanan_A/0/1/0/all/0/1">Aishwarya Venkataramanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Benbihi_A/0/1/0/all/0/1">Assia Benbihi</a>, <a href="http://arxiv.org/find/cs/1/au:+Laviale_M/0/1/0/all/0/1">Martin Laviale</a>, <a href="http://arxiv.org/find/cs/1/au:+Pradalier_C/0/1/0/all/0/1">Cedric Pradalier</a></p>
<p>Recent works show that the data distribution in a network's latent space is
useful for estimating classification uncertainty and detecting
Out-of-distribution (OOD) samples. To obtain a well-regularized latent space
that is conducive for uncertainty estimation, existing methods bring in
significant changes to model architectures and training procedures. In this
paper, we present a lightweight, fast, and high-performance regularization
method for Mahalanobis distance-based uncertainty prediction, and that requires
minimal changes to the network's architecture. To derive Gaussian latent
representation favourable for Mahalanobis Distance calculation, we introduce a
self-supervised representation learning method that separates in-class
representations into multiple Gaussians. Classes with non-Gaussian
representations are automatically identified and dynamically clustered into
multiple new classes that are approximately Gaussian. Evaluation on standard
OOD benchmarks shows that our method achieves state-of-the-art results on OOD
detection with minimal inference time, and is very competitive on predictive
probability calibration. Finally, we show the applicability of our method to a
real-life computer vision use case on microorganism classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16573">Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hasegawa_N/0/1/0/all/0/1">Naoya Hasegawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1">Issei Sato</a></p>
<p>Recognition problems in long-tailed data, where the sample size per class is
heavily skewed, have recently gained importance because the distribution of the
sample size per class in a dataset is generally exponential unless the sample
size is intentionally adjusted. Various approaches have been devised to address
these problems. Recently, weight balancing, which combines well-known classical
regularization techniques with two-stage training, has been proposed. Despite
its simplicity, it is known for its high performance against existing methods
devised in various ways. However, there is a lack of understanding as to why
this approach is effective for long-tailed data. In this study, we analyze the
method focusing on neural collapse and cone effect at each training stage and
find that it can be decomposed into the increase in Fisher's discriminant ratio
of the feature extractor caused by weight decay and cross entropy loss and
implicit logit adjustment caused by weight decay and class-balanced loss. Our
analysis shows that the training method can be further simplified by reducing
the number of training stages to one while increasing accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19472">PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1">Faeze Brahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1">Chandra Bhagavatula</a>, <a href="http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1">Valentina Pyatkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Jena D. Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Lorraine Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1">Hirona J. Arai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1">Soumya Sanyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1">Keisuke Sakaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a></p>
<p>Procedural planning, which entails decomposing a high-level goal into a
sequence of temporally ordered steps, is an important yet intricate task for
machines. It involves integrating common-sense knowledge to reason about
complex contextualized situations that are often counterfactual, e.g.
"scheduling a doctor's appointment without a phone". While current approaches
show encouraging results using large language models (LLMs), they are hindered
by drawbacks such as costly API calls and reproducibility issues. In this
paper, we advocate planning using smaller language models. We present PlaSma, a
novel two-pronged approach to endow small language models with procedural
knowledge and (counterfactual) planning capabilities. More concretely, we
develop symbolic procedural knowledge distillation to enhance the implicit
knowledge in small language models and an inference-time algorithm to
facilitate more structured and accurate reasoning. In addition, we introduce a
novel task, Counterfactual Planning, that requires a revision of a plan to cope
with a counterfactual situation. In both the original and counterfactual
setting, we show that orders-of-magnitude smaller models (770M-11B parameters)
can compete and often surpass their larger teacher models' capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04169">Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation. (arXiv:2306.04169v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1">Mingquan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Junze Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lichen Zhang</a></p>
<p>Weighted low rank approximation is a fundamental problem in numerical linear
algebra, and it has many applications in machine learning. Given a matrix $M
\in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n
\times n}$, a parameter $k$, the goal is to output two matrices $U, V \in
\mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V^\top) \|_F$ is
minimized, where $\circ$ denotes the Hadamard product. Such a problem is known
to be NP-hard and even hard to approximate assuming Exponential Time Hypothesis
[GG11, RSW16]. Meanwhile, alternating minimization is a good heuristic solution
for approximating weighted low rank approximation. The work [LLR16] shows that,
under mild assumptions, alternating minimization does provide provable
guarantees. In this work, we develop an efficient and robust framework for
alternating minimization. For weighted low rank approximation, this improves
the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work
framework is a high-accuracy multiple response regression solver together with
a robust analysis of alternating minimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05697">Group Equivariant Fourier Neural Operators for Partial Differential Equations. (arXiv:2306.05697v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Helwig_J/0/1/0/all/0/1">Jacob Helwig</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Cong Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurtin_J/0/1/0/all/0/1">Jerry Kurtin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1">Stephan Wojtowytsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shuiwang Ji</a></p>
<p>We consider solving partial differential equations (PDEs) with Fourier neural
operators (FNOs), which operate in the frequency domain. Since the laws of
physics do not depend on the coordinate system used to describe them, it is
desirable to encode such symmetries in the neural operator architecture for
better performance and easier learning. While encoding symmetries in the
physical domain using group theory has been studied extensively, how to capture
symmetries in the frequency domain is under-explored. In this work, we extend
group convolutions to the frequency domain and design Fourier layers that are
equivariant to rotations, translations, and reflections by leveraging the
equivariance property of the Fourier transform. The resulting $G$-FNO
architecture generalizes well across input resolutions and performs well in
settings with varying levels of symmetry. Our code is publicly available as
part of the AIRS library (https://github.com/divelab/AIRS).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05965">Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Erp_B/0/1/0/all/0/1">Bart van Erp</a>, <a href="http://arxiv.org/find/cs/1/au:+Nuijten_W/0/1/0/all/0/1">Wouter W. L. Nuijten</a>, <a href="http://arxiv.org/find/cs/1/au:+Laar_T/0/1/0/all/0/1">Thijs van de Laar</a>, <a href="http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1">Bert de Vries</a></p>
<p>Bayesian state and parameter estimation have been automated effectively in a
variety of probabilistic programming languages. The process of model comparison
on the other hand, which still requires error-prone and time-consuming manual
derivations, is often overlooked despite its importance. This paper efficiently
automates Bayesian model averaging, selection, and combination by message
passing on a Forney-style factor graph with a custom mixture node. Parameter
and state inference, and model comparison can then be executed simultaneously
using message passing with scale factors. This approach shortens the model
design cycle and allows for the straightforward extension to hierarchical and
temporal model priors to accommodate for modeling complicated time-varying
processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10045">Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v7 [physics.chem-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1">Yuchao Lin</a>, <a href="http://arxiv.org/find/physics/1/au:+Yan_K/0/1/0/all/0/1">Keqiang Yan</a>, <a href="http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1">Youzhi Luo</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Qian_X/0/1/0/all/0/1">Xiaoning Qian</a>, <a href="http://arxiv.org/find/physics/1/au:+Ji_S/0/1/0/all/0/1">Shuiwang Ji</a></p>
<p>We study property prediction for crystal materials. A crystal structure
consists of a minimal unit cell that is repeated infinitely in 3D space. How to
accurately represent such repetitive structures in machine learning models
remains unresolved. Current methods construct graphs by establishing edges only
between nearby nodes, thereby failing to faithfully capture infinite repeating
patterns and distant interatomic interactions. In this work, we propose several
innovations to overcome these limitations. First, we propose to model
physics-principled interatomic potentials directly instead of only using
distances as in many existing methods. These potentials include the Coulomb
potential, London dispersion potential, and Pauli repulsion potential. Second,
we model the complete set of potentials among all atoms, instead of only
between nearby atoms as in existing methods. This is enabled by our
approximations of infinite potential summations with provable error bounds. We
further develop efficient algorithms to compute the approximations. Finally, we
propose to incorporate our computations of complete interatomic potentials into
message passing neural networks for representation learning. We perform
experiments on the JARVIS and Materials Project benchmarks for evaluation.
Results show that the use of interatomic potentials and complete interatomic
potentials leads to consistent performance improvements with reasonable
computational costs. Our code is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10473">2D-Shapley: A Framework for Fragmented Data Valuation. (arXiv:2306.10473v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhihong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Just_H/0/1/0/all/0/1">Hoang Anh Just</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xiangyu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a></p>
<p>Data valuation -- quantifying the contribution of individual data sources to
certain predictive behaviors of a model -- is of great importance to enhancing
the transparency of machine learning and designing incentive systems for data
sharing. Existing work has focused on evaluating data sources with the shared
feature or sample space. How to valuate fragmented data sources of which each
only contains partial features and samples remains an open question. We start
by presenting a method to calculate the counterfactual of removing a fragment
from the aggregated data matrix. Based on the counterfactual calculation, we
further propose 2D-Shapley, a theoretical framework for fragmented data
valuation that uniquely satisfies some appealing axioms in the fragmented data
context. 2D-Shapley empowers a range of new use cases, such as selecting useful
data fragments, providing interpretation for sample-wise data values, and
fine-grained data issue diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00610">Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1">Raphael Frick</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1">Inna Vogel</a></p>
<p>The option of sharing images, videos and audio files on social media opens up
new possibilities for distinguishing between false information and fake news on
the Internet. Due to the vast amount of data shared every second on social
media, not all data can be verified by a computer or a human expert. Here, a
check-worthiness analysis can be used as a first step in the fact-checking
pipeline and as a filtering mechanism to improve efficiency. This paper
proposes a novel way of detecting the check-worthiness in multi-modal tweets.
It takes advantage of two classifiers, each trained on a single modality. For
image data, extracting the embedded text with an OCR analysis has shown to
perform best. By combining the two classifiers, the proposed solution was able
to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297
achieved on the private test set.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01524">Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kakaiya_R/0/1/0/all/0/1">Ravi Kakaiya</a>, <a href="http://arxiv.org/find/cs/1/au:+Sathish_R/0/1/0/all/0/1">Rakshith Sathish</a>, <a href="http://arxiv.org/find/cs/1/au:+Sethuraman_R/0/1/0/all/0/1">Ramanathan Sethuraman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheet_D/0/1/0/all/0/1">Debdoot Sheet</a></p>
<p>Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the
potential to radically change the way we travel. Many such vehicles currently
rely on segmentation and object detection algorithms to detect and track
objects around its surrounding. The data collected from the vehicles are often
sent to cloud servers to facilitate continual/life-long learning of these
algorithms. Considering the bandwidth constraints, the data is compressed
before sending it to servers, where it is typically decompressed for training
and analysis. In this work, we propose the use of a learning-based compression
Codec to reduce the overhead in latency incurred for the decompression
operation in the standard pipeline. We demonstrate that the learned compressed
representation can also be used to perform tasks like semantic segmentation in
addition to decompression to obtain the images. We experimentally validate the
proposed pipeline on the Cityscapes dataset, where we achieve a compression
factor up to $66 \times$ while preserving the information required to perform
segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved
using decompressed images while reducing the overall compute by $11\%$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02377">Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frick_R/0/1/0/all/0/1">Raphael Frick</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_I/0/1/0/all/0/1">Inna Vogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jeong-Eun Choi</a></p>
<p>This paper describes the second-placed approach developed by the Fraunhofer
SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text
snippet from a political debate, the aim of this task is to determine whether
it should be assessed for check-worthiness. Detecting check-worthy statements
aims to facilitate manual fact-checking efforts by prioritizing the claims that
fact-checkers should consider first. It can also be considered as primary step
of a fact-checking system. Our best-performing method took advantage of an
ensemble classification scheme centered on Model Souping. When applied to the
English data set, our submitted model achieved an overall F1 score of 0.878 and
was ranked as the second-best model in the competition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03430">Differential Privacy for Clustering Under Continual Observation. (arXiv:2307.03430v2 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tour_M/0/1/0/all/0/1">Max Dupr&#xe9; la Tour</a>, <a href="http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1">Monika Henzinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Saulpic_D/0/1/0/all/0/1">David Saulpic</a></p>
<p>We consider the problem of clustering privately a dataset in $\mathbb{R}^d$
that undergoes both insertion and deletion of points. Specifically, we give an
$\varepsilon$-differentially private clustering mechanism for the $k$-means
objective under continual observation. This is the first approximation
algorithm for that problem with an additive error that depends only
logarithmically in the number $T$ of updates. The multiplicative error is
almost the same as non privately. To do so we show how to perform dimension
reduction under continual observation and combine it with a differentially
private greedy approximation algorithm for $k$-means. We also partially extend
our results to the $k$-median problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03811">Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v2 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Sharma_V/0/1/0/all/0/1">Vidushi Sharma</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Giammona_M/0/1/0/all/0/1">Maxwell Giammona</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zubarev_D/0/1/0/all/0/1">Dmitry Zubarev</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Tek_A/0/1/0/all/0/1">Andy Tek</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Nugyuen_K/0/1/0/all/0/1">Khanh Nugyuen</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sundberg_L/0/1/0/all/0/1">Linda Sundberg</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Congiu_D/0/1/0/all/0/1">Daniele Congiu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+La_Y/0/1/0/all/0/1">Young-Hye La</a></p>
<p>Advanced computational methods are being actively sought for addressing the
challenges associated with discovery and development of new combinatorial
material such as formulations. A widely adopted approach involves domain
informed high-throughput screening of individual components that can be
combined into a formulation. This manages to accelerate the discovery of new
compounds for a target application but still leave the process of identifying
the right 'formulation' from the shortlisted chemical space largely a
laboratory experiment-driven process. We report a deep learning model,
Formulation Graph Convolution Network (F-GCN), that can map
structure-composition relationship of the individual components to the property
of liquid formulation as whole. Multiple GCNs are assembled in parallel that
featurize formulation constituents domain-intuitively on the fly. The resulting
molecular descriptors are scaled based on respective constituent's molar
percentage in the formulation, followed by formalizing into a combined
descriptor that represents a complete formulation to an external learning
architecture. The use case of proposed formulation learning model is
demonstrated for battery electrolytes by training and testing it on two
exemplary datasets representing electrolyte formulations vs battery performance
-- one dataset is sourced from literature about Li/Cu half-cells, while the
other is obtained by lab-experiments related to lithium-iodide full-cell
chemistry. The model is shown to predict the performance metrics like Coulombic
Efficiency (CE) and specific capacity of new electrolyte formulations with
lowest reported errors. The best performing F-GCN model uses molecular
descriptors derived from molecular graphs that are informed with HOMO-LUMO and
electric moment properties of the molecules using a knowledge transfer
technique.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05946">A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1">Agnimitra Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1">Sudeepta Mondal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Adway Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1">S. Ilgin Guler</a></p>
<p>Deep-learning models for traffic data prediction can have superior
performance in modeling complex functions using a multi-layer architecture.
However, a major drawback of these approaches is that most of these approaches
do not offer forecasts with uncertainty estimates, which are essential for
traffic operations and control. Without uncertainty estimates, it is difficult
to place any level of trust to the model predictions, and operational
strategies relying on overconfident predictions can lead to worsening traffic
conditions. In this study, we propose a Bayesian recurrent neural network
framework for uncertainty quantification in traffic prediction with higher
generalizability by introducing spectral normalization to its hidden layers. In
our paper, we have shown that normalization alters the training process of deep
neural networks by controlling the model's complexity and reducing the risk of
overfitting to the training data. This, in turn, helps improve the
generalization performance of the model on out-of-distribution datasets.
Results demonstrate that spectral normalization improves uncertainty estimates
and significantly outperforms both the layer normalization and model without
normalization in single-step prediction horizons. This improved performance can
be attributed to the ability of spectral normalization to better localize the
feature space of the data under perturbations. Our findings are especially
relevant to traffic management applications, where predicting traffic
conditions across multiple locations is the goal, but the availability of
training data from multiple locations is limited. Spectral normalization,
therefore, provides a more generalizable approach that can effectively capture
the underlying patterns in traffic data without requiring location-specific
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09916">TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v3 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jianing Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1">Qing Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yilin Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1">Wei Zeng</a></p>
<p>Deep learning (DL) approaches are being increasingly used for time-series
forecasting, with many efforts devoted to designing complex DL models. Recent
studies have shown that the DL success is often attributed to effective data
representations, fostering the fields of feature engineering and representation
learning. However, automated approaches for feature learning are typically
limited with respect to incorporating prior knowledge, identifying interactions
among variables, and choosing evaluation metrics to ensure that the models are
reliable. To improve on these limitations, this paper contributes a novel
visual analytics framework, namely TimeTuner, designed to help analysts
understand how model behaviors are associated with localized correlations,
stationarity, and granularity of time-series representations. The system mainly
consists of the following two-stage technique: We first leverage counterfactual
explanations to connect the relationships among time-series representations,
multivariate features and model predictions. Next, we design multiple
coordinated views including a partition-based correlation matrix and juxtaposed
bivariate stripes, and provide a set of interactions that allow users to step
into the transformation selection process, navigate through the feature space,
and reason the model performance. We instantiate TimeTuner with two
transformation methods of smoothing and sampling, and demonstrate its
applicability on real-world time-series forecasting of univariate sunspots and
multivariate air pollutants. Feedback from domain experts indicates that our
system can help characterize time-series representations and guide the feature
engineering processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10705">TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Che_Q/0/1/0/all/0/1">Quang Huy Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dinh Phuc Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1">Minh Quan Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Lam_D/0/1/0/all/0/1">Duc Khai Lam</a></p>
<p>Semantic segmentation is a common task in autonomous driving to understand
the surrounding environment. Driveable Area Segmentation and Lane Detection are
particularly important for safe and efficient navigation on the road. However,
original semantic segmentation models are computationally expensive and require
high-end hardware, which is not feasible for embedded systems in autonomous
vehicles. This paper proposes a lightweight model for the driveable area and
lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate
and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K
dataset and compare it with modern models. Experimental results show that our
TwinLiteNet performs similarly to existing approaches, requiring significantly
fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score
of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task
with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.
Furthermore, TwinLiteNet can run in real-time on embedded devices with limited
computing power, especially since it achieves 60FPS on Jetson Xavier NX, making
it an ideal solution for self-driving vehicles. Code is available:
url{https://github.com/chequanghuy/TwinLiteNet}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11892">On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1">Avrim Blum</a>, <a href="http://arxiv.org/find/cs/1/au:+Okoroafor_P/0/1/0/all/0/1">Princewill Okoroafor</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1">Aadirupa Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Stangl_K/0/1/0/all/0/1">Kevin Stangl</a></p>
<p>We consider the vulnerability of fairness-constrained learning to small
amounts of malicious noise in the training data. Konstantinov and Lampert
(2021) initiated the study of this question and presented negative results
showing there exist data distributions where for several fairness constraints,
any proper learner will exhibit high vulnerability when group sizes are
imbalanced. Here, we present a more optimistic view, showing that if we allow
randomized classifiers, then the landscape is much more nuanced. For example,
for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in
accuracy, where $\alpha$ is the malicious noise rate, matching the best
possible even without fairness constraints. For Equal Opportunity, we show we
can incur an $O(\sqrt{\alpha})$ loss, and give a matching
$\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert
(2021) showed for proper learners the loss in accuracy for both notions is
$\Omega(1)$. The key technical novelty of our work is how randomization can
bypass simple "tricks" an adversary can use to amplify his power. We also
consider additional fairness notions including Equalized Odds and Calibration.
For these fairness notions, the excess accuracy clusters into three natural
regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more
fine-grained view of the sensitivity of fairness-constrained learning to
adversarial noise in training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13423">Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations. (arXiv:2307.13423v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Close_G/0/1/0/all/0/1">George Close</a>, <a href="http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1">Thomas Hain</a>, <a href="http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1">Stefan Goetze</a></p>
<p>Self-supervised speech representations (SSSRs) have been successfully applied
to a number of speech-processing tasks, e.g. as feature extractor for speech
quality (SQ) prediction, which is, in turn, relevant for assessment and
training speech enhancement systems for users with normal or impaired hearing.
However, exact knowledge of why and how quality-related information is encoded
well in such representations remains poorly understood. In this work,
techniques for non-intrusive prediction of SQ ratings are extended to the
prediction of intelligibility for hearing-impaired users. It is found that
self-supervised representations are useful as input features to non-intrusive
prediction models, achieving competitive performance to more complex systems. A
detailed analysis of the performance depending on Clarity Prediction Challenge
1 listeners and enhancement systems indicates that more data might be needed to
allow generalisation to unknown systems and (hearing-impaired) individuals
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13494">Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v3 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaixin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yabin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1">Chang Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Donghua Yang</a></p>
<p>Learned cardinality estimation methods have achieved high precision compared
to traditional methods. Among learned methods, query-driven approaches face the
data and workload drift problem for a long time. Although both query-driven and
hybrid methods are proposed to avoid this problem, even the state-of-art of
them suffer from high training and estimation costs, limited scalability,
instability, and long-tailed distribution problem on high cardinality and high
dimensional tables, which seriously affects the practical application of
learned cardinality estimators. In this paper, we prove that most of these
problems are directly caused by the widely used progressive sampling. We solve
this problem by introducing predicates into the autoregressive model and
propose Duet, a stable, efficient, and scalable hybrid method to estimate
cardinality directly without sampling or any non-differentiable process, which
can not only reduces the inference complexity from $O(n)$ to $O(1)$ compared to
Naru and UAE but also achieve higher accuracy on high cardinality and high
dimensional tables. Experimental results show that Duet can achieve all the
design goals above and be much more practical and even has a lower inference
cost on CPU than that of most learned methods on GPU.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13709">Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fujii_S/0/1/0/all/0/1">Satoru Fujii</a></p>
<p>Many properties in the real world, such as desirability or strength in
competitive environment, can't be directly observed, which makes them difficult
to evaluate. To deal with this challenging problem, prior works have primarily
focused on estimating those properties of known items, especially the strength
of sports players, only of those who appears in paired comparison dataset. In
this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework
to evaluate any properties of unknown items, not necessarily present in the
training data. Our method seamlessly integrates traditional Bradley-Terry model
with a neural network structure. We also generalizes this architecture further
for asymmetric environment with unfairness, which is much more common in real
world settings. In our experimental analysis, DBTR successfully learned desired
quantification of those properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13813">How to Scale Your EMA. (arXiv:2307.13813v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Busbridge_D/0/1/0/all/0/1">Dan Busbridge</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramapuram_J/0/1/0/all/0/1">Jason Ramapuram</a>, <a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1">Pierre Ablin</a>, <a href="http://arxiv.org/find/stat/1/au:+Likhomanenko_T/0/1/0/all/0/1">Tatiana Likhomanenko</a>, <a href="http://arxiv.org/find/stat/1/au:+Dhekane_E/0/1/0/all/0/1">Eeshan Gunesh Dhekane</a>, <a href="http://arxiv.org/find/stat/1/au:+Suau_X/0/1/0/all/0/1">Xavier Suau</a>, <a href="http://arxiv.org/find/stat/1/au:+Webb_R/0/1/0/all/0/1">Russ Webb</a></p>
<p>Preserving training dynamics across batch sizes is an important tool for
practical machine learning as it enables the trade-off between batch size and
wall-clock time. This trade-off is typically enabled by a scaling rule, for
example, in stochastic gradient descent, one should scale the learning rate
linearly with the batch size. Another important tool for practical machine
learning is the model Exponential Moving Average (EMA), which is a model copy
that does not receive gradient information, but instead follows its target
model with some momentum. This model EMA can improve the robustness and
generalization properties of supervised learning, stabilize pseudo-labeling,
and provide a learning signal for Self-Supervised Learning (SSL). Prior works
have treated the model EMA separately from optimization, leading to different
training dynamics across batch sizes and lower model performance. In this work,
we provide a scaling rule for optimization in the presence of model EMAs and
demonstrate its validity across a range of architectures, optimizers, and data
modalities. We also show the rule's validity where the model EMA contributes to
the optimization of the target model, enabling us to train EMA-based
pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we
enable training of BYOL up to batch size 24,576 without sacrificing
performance, optimally a 6$\times$ wall-clock time reduction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.14066">Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Rousseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Alaka_C/0/1/0/all/0/1">Christian Alaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Covili_E/0/1/0/all/0/1">Emma Covili</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayard_H/0/1/0/all/0/1">Hippolyte Mayard</a>, <a href="http://arxiv.org/find/cs/1/au:+Misrachi_L/0/1/0/all/0/1">Laura Misrachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Au_W/0/1/0/all/0/1">Willy Au</a></p>
<p>Medical radiography segmentation, and specifically dental radiography, is
highly limited by the cost of labeling which requires specific expertise and
labor-intensive annotations. In this work, we propose a straightforward
pre-training method for semantic segmentation leveraging Denoising Diffusion
Probabilistic Models (DDPM), which have shown impressive results for generative
modeling. Our straightforward approach achieves remarkable performance in terms
of label efficiency and does not require architectural modifications between
pre-training and downstream tasks. We propose to first pre-train a Unet by
exploiting the DDPM training objective, and then fine-tune the resulting model
on a segmentation task. Our experimental results on the segmentation of dental
radiographs demonstrate that the proposed method is competitive with
state-of-the-art pre-training methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.01462">Deep learning of quantum entanglement from incomplete measurements. (arXiv:2205.01462v6 [quant-ph] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Koutny_D/0/1/0/all/0/1">Dominik Koutn&#xfd;</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gines_L/0/1/0/all/0/1">Laia Gin&#xe9;s</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Moczala_Dusanowska_M/0/1/0/all/0/1">Magdalena Mocza&#x142;a-Dusanowska</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Hofling_S/0/1/0/all/0/1">Sven H&#xf6;fling</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Schneider_C/0/1/0/all/0/1">Christian Schneider</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Predojevic_A/0/1/0/all/0/1">Ana Predojevi&#x107;</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Jezek_M/0/1/0/all/0/1">Miroslav Je&#x17e;ek</a></p>
<p>The quantification of the entanglement present in a physical system is of
para\-mount importance for fundamental research and many cutting-edge
applications. Currently, achieving this goal requires either a priori knowledge
on the system or very demanding experimental procedures such as full state
tomography or collective measurements. Here, we demonstrate that by employing
neural networks we can quantify the degree of entanglement without needing to
know the full description of the quantum state. Our method allows for direct
quantification of the quantum correlations using an incomplete set of local
measurements. Despite using undersampled measurements, we achieve a
quantification error of up to an order of magnitude lower than the
state-of-the-art quantum tomography. Furthermore, we achieve this result
employing networks trained using exclusively simulated data. Finally, we derive
a method based on a convolutional network input that can accept data from
various measurement scenarios and perform, to some extent, independently of the
measurement device.
</p>
</p>
</div>

    </div>
    </body>
    