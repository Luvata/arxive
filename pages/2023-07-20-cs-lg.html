<!DOCTYPE html>
<html>
<head>
<title>2023-07-20-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.09484">MolFM: A Multimodal Molecular Foundation Model. (arXiv:2307.09484v1 [q-bio.BM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Luo_Y/0/1/0/all/0/1">Yizhen Luo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yang_K/0/1/0/all/0/1">Kai Yang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Hong_M/0/1/0/all/0/1">Massimo Hong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1">Xingyi Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nie_Z/0/1/0/all/0/1">Zaiqing Nie</a></p>
<p>Molecular knowledge resides within three different modalities of information
sources: molecular structures, biomedical documents, and knowledge bases.
Effective incorporation of molecular knowledge from these modalities holds
paramount significance in facilitating biomedical research. However, existing
multimodal molecular foundation models exhibit limitations in capturing
intricate connections between molecular structures and texts, and more
importantly, none of them attempt to leverage a wealth of molecular expertise
derived from knowledge graphs. In this study, we introduce MolFM, a multimodal
molecular foundation model designed to facilitate joint representation learning
from molecular structures, biomedical texts, and knowledge graphs. We propose
cross-modal attention between atoms of molecular structures, neighbors of
molecule entities and semantically related texts to facilitate cross-modal
comprehension. We provide theoretical analysis that our cross-modal
pre-training captures local and global molecular knowledge by minimizing the
distance in the feature space between different modalities of the same
molecule, as well as molecules sharing similar structures or functions. MolFM
achieves state-of-the-art performance on various downstream tasks. On
cross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04%
absolute gains under the zero-shot and fine-tuning settings, respectively.
Furthermore, qualitative analysis showcases MolFM's implicit ability to provide
grounding from molecular substructures and knowledge graphs. Code and models
are available on https://github.com/BioFM/OpenBioMed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09487">Submodular Maximization under the Intersection of Matroid and Knapsack Constraints. (arXiv:2307.09487v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yu-Ran Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1">Chao Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chao Qian</a></p>
<p>Submodular maximization arises in many applications, and has attracted a lot
of research attentions from various areas such as artificial intelligence,
finance and operations research. Previous studies mainly consider only one kind
of constraint, while many real-world problems often involve several
constraints. In this paper, we consider the problem of submodular maximization
under the intersection of two commonly used constraints, i.e., $k$-matroid
constraint and $m$-knapsack constraint, and propose a new algorithm SPROUT by
incorporating partial enumeration into the simultaneous greedy framework. We
prove that SPROUT can achieve a polynomial-time approximation guarantee better
than the state-of-the-art algorithms. Then, we introduce the random enumeration
and smooth techniques into SPROUT to improve its efficiency, resulting in the
SPROUT++ algorithm, which can keep a similar approximation guarantee.
Experiments on the applications of movie recommendation and weighted max-cut
demonstrate the superiority of SPROUT++ in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09488">PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization. (arXiv:2307.09488v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1">Daniele Jahier Pagliari</a>, <a href="http://arxiv.org/find/cs/1/au:+Risso_M/0/1/0/all/0/1">Matteo Risso</a>, <a href="http://arxiv.org/find/cs/1/au:+Motetti_B/0/1/0/all/0/1">Beatrice Alessandra Motetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1">Alessio Burrello</a></p>
<p>Accurate yet efficient Deep Neural Networks (DNNs) are in high demand,
especially for applications that require their execution on constrained edge
devices. Finding such DNNs in a reasonable time for new applications requires
automated optimization pipelines since the huge space of hyper-parameter
combinations is impossible to explore extensively by hand. In this work, we
propose PLiNIO, an open-source library implementing a comprehensive set of
state-of-the-art DNN design automation techniques, all based on lightweight
gradient-based optimization, under a unified and user-friendly interface. With
experiments on several edge-relevant tasks, we show that combining the various
optimizations available in PLiNIO leads to rich sets of solutions that
Pareto-dominate the considered baselines in terms of accuracy vs model size.
Noteworthy, PLiNIO achieves up to 94.34% memory reduction for a &lt;1% accuracy
drop compared to a baseline architecture.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09494">Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing. (arXiv:2307.09494v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Swastika Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chergui_H/0/1/0/all/0/1">Hatim Chergui</a>, <a href="http://arxiv.org/find/cs/1/au:+Verikoukis_C/0/1/0/all/0/1">Christos Verikoukis</a></p>
<p>Future zero-touch artificial intelligence (AI)-driven 6G network automation
requires building trust in the AI black boxes via explainable artificial
intelligence (XAI), where it is expected that AI faithfulness would be a
quantifiable service-level agreement (SLA) metric along with telecommunications
key performance indicators (KPIs). This entails exploiting the XAI outputs to
generate transparent and unbiased deep neural networks (DNNs). Motivated by
closed-loop (CL) automation and explanation-guided learning (EGL), we design an
explanation-guided federated learning (EGFL) scheme to ensure trustworthy
predictions by exploiting the model explanation emanating from XAI strategies
during the training run time via Jensen-Shannon (JS) divergence. Specifically,
we predict per-slice RAN dropped traffic probability to exemplify the proposed
concept while respecting fairness goals formulated in terms of the recall
metric which is included as a constraint in the optimization task. Finally, the
comprehensiveness score is adopted to measure and validate the faithfulness of
the explanations quantitatively. Simulation results show that the proposed
EGFL-JS scheme has achieved more than $50\%$ increase in terms of
comprehensiveness compared to different baselines from the literature,
especially the variant EGFL-KL that is based on the Kullback-Leibler
Divergence. It has also improved the recall score with more than $25\%$
relatively to unconstrained-EGFL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09542">Can Neural Network Memorization Be Localized?. (arXiv:2307.09542v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1">Pratyush Maini</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1">Michael C. Mozer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chiyuan Zhang</a></p>
<p>Recent efforts at explaining the interplay of memorization and generalization
in deep overparametrized networks have posited that neural networks
$\textit{memorize}$ "hard" examples in the final few layers of the model.
Memorization refers to the ability to correctly predict on $\textit{atypical}$
examples of the training set. In this work, we show that rather than being
confined to individual layers, memorization is a phenomenon confined to a small
set of neurons in various layers of the model. First, via three experimental
sources of converging evidence, we find that most layers are redundant for the
memorization of examples and the layers that contribute to example memorization
are, in general, not the final layers. The three sources are $\textit{gradient
accounting}$ (measuring the contribution to the gradient norms from memorized
and clean examples), $\textit{layer rewinding}$ (replacing specific model
weights of a converged model with previous training checkpoints), and
$\textit{retraining}$ (training rewound layers only on clean examples). Second,
we ask a more generic question: can memorization be localized
$\textit{anywhere}$ in a model? We discover that memorization is often confined
to a small number of neurons or channels (around 5) of the model. Based on
these insights we propose a new form of dropout -- $\textit{example-tied
dropout}$ that enables us to direct the memorization of examples to an apriori
determined set of neurons. By dropping out these neurons, we are able to reduce
the accuracy on memorized examples from $100\%\to3\%$, while also reducing the
generalization gap.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09547">DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI. (arXiv:2307.09547v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bedel_H/0/1/0/all/0/1">Hasan Atakan Bedel</a>, <a href="http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1">Tolga &#xc7;ukur</a></p>
<p>Deep learning analyses have offered sensitivity leaps in detection of
cognitive states from functional MRI (fMRI) measurements across the brain. Yet,
as deep models perform hierarchical nonlinear transformations on their input,
interpreting the association between brain responses and cognitive states is
challenging. Among common explanation approaches for deep fMRI classifiers,
attribution methods show poor specificity and perturbation methods show limited
plausibility. While counterfactual generation promises to address these
limitations, previous methods use variational or adversarial priors that yield
suboptimal sample fidelity. Here, we introduce the first diffusion-driven
counterfactual method, DreaMR, to enable fMRI interpretation with high
specificity, plausibility and fidelity. DreaMR performs diffusion-based
resampling of an input fMRI sample to alter the decision of a downstream
classifier, and then computes the minimal difference between the original and
counterfactual samples for explanation. Unlike conventional diffusion methods,
DreaMR leverages a novel fractional multi-phase-distilled diffusion prior to
improve sampling efficiency without compromising fidelity, and it employs a
transformer architecture to account for long-range spatiotemporal context in
fMRI scans. Comprehensive experiments on neuroimaging datasets demonstrate the
superior specificity, fidelity and efficiency of DreaMR in sample generation
over state-of-the-art counterfactual methods for fMRI interpretation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09550">The semantic landscape paradigm for neural networks. (arXiv:2307.09550v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gokhale_S/0/1/0/all/0/1">Shreyas Gokhale</a></p>
<p>Deep neural networks exhibit a fascinating spectrum of phenomena ranging from
predictable scaling laws to the unpredictable emergence of new capabilities as
a function of training time, dataset size and network size. Analysis of these
phenomena has revealed the existence of concepts and algorithms encoded within
the learned representations of these networks. While significant strides have
been made in explaining observed phenomena separately, a unified framework for
understanding, dissecting, and predicting the performance of neural networks is
lacking. Here, we introduce the semantic landscape paradigm, a conceptual and
mathematical framework that describes the training dynamics of neural networks
as trajectories on a graph whose nodes correspond to emergent algorithms that
are instrinsic to the learned representations of the networks. This abstraction
enables us to describe a wide range of neural network phenomena in terms of
well studied problems in statistical physics. Specifically, we show that
grokking and emergence with scale are associated with percolation phenomena,
and neural scaling laws are explainable in terms of the statistics of random
walks on graphs. Finally, we discuss how the semantic landscape paradigm
complements existing theoretical and practical approaches aimed at
understanding and interpreting deep neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09552">Self-Compatibility: Evaluating Causal Discovery without Ground Truth. (arXiv:2307.09552v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Faller_P/0/1/0/all/0/1">Philipp M. Faller</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Vankadara_L/0/1/0/all/0/1">Leena Chennuru Vankadara</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Mastakouri_A/0/1/0/all/0/1">Atalanti A. Mastakouri</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Janzing_D/0/1/0/all/0/1">Dominik Janzing</a> (2) ((1) Karlsruhe Institute of Technology, (2) Amazon Research Tuebingen)</p>
<p>As causal ground truth is incredibly rare, causal discovery algorithms are
commonly only evaluated on simulated data. This is concerning, given that
simulations reflect common preconceptions about generating processes regarding
noise distributions, model classes, and more. In this work, we propose a novel
method for falsifying the output of a causal discovery algorithm in the absence
of ground truth. Our key insight is that while statistical learning seeks
stability across subsets of data points, causal learning should seek stability
across subsets of variables. Motivated by this insight, our method relies on a
notion of compatibility between causal graphs learned on different subsets of
variables. We prove that detecting incompatibilities can falsify wrongly
inferred causal relations due to violation of assumptions or errors from finite
sample effects. Although passing such compatibility tests is only a necessary
criterion for good performance, we argue that it provides strong evidence for
the causal models whenever compatibility entails strong implications for the
joint distribution. We also demonstrate experimentally that detection of
incompatibilities can aid in causal model selection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09575">Causal Influences over Social Learning Networks. (arXiv:2307.09575v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kayaalp_M/0/1/0/all/0/1">Mert Kayaalp</a>, <a href="http://arxiv.org/find/cs/1/au:+Sayed_A/0/1/0/all/0/1">Ali H. Sayed</a></p>
<p>This paper investigates causal influences between agents linked by a social
graph and interacting over time. In particular, the work examines the dynamics
of social learning models and distributed decision-making protocols, and
derives expressions that reveal the causal relations between pairs of agents
and explain the flow of influence over the network. The results turn out to be
dependent on the graph topology and the level of information that each agent
has about the inference problem they are trying to solve. Using these
conclusions, the paper proposes an algorithm to rank the overall influence
between agents to discover highly influential agents. It also provides a method
to learn the necessary model parameters from raw observational data. The
results and the proposed algorithm are illustrated by considering both
synthetic data and real Twitter data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09591">Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muzellec_S/0/1/0/all/0/1">Sabine Muzellec</a>, <a href="http://arxiv.org/find/cs/1/au:+Andeol_L/0/1/0/all/0/1">Leo Andeol</a>, <a href="http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1">Thomas Fel</a>, <a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1">Rufin VanRullen</a>, <a href="http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1">Thomas Serre</a></p>
<p>Recent years have witnessed an explosion in the development of novel
prediction-based attribution methods, which have slowly been supplanting older
gradient-based methods to explain the decisions of deep neural networks.
However, it is still not clear why prediction-based methods outperform
gradient-based ones. Here, we start with an empirical observation: these two
approaches yield attribution maps with very different power spectra, with
gradient-based methods revealing more high-frequency content than
prediction-based methods. This observation raises multiple questions: What is
the source of this high-frequency information, and does it truly reflect
decisions made by the system? Lastly, why would the absence of high-frequency
information in prediction-based methods yield better explainability scores
along multiple metrics? We analyze the gradient of three representative visual
classification models and observe that it contains noisy information emanating
from high-frequencies. Furthermore, our analysis reveals that the operations
used in Convolutional Neural Networks (CNNs) for downsampling appear to be a
significant source of this high-frequency content -- suggesting aliasing as a
possible underlying basis. We then apply an optimal low-pass filter for
attribution maps and demonstrate that it improves gradient-based attribution
methods. We show that (i) removing high-frequency noise yields significant
improvements in the explainability scores obtained with gradient-based methods
across multiple models -- leading to (ii) a novel ranking of state-of-the-art
methods with gradient-based methods at the top. We believe that our results
will spur renewed interest in simpler and computationally more efficient
gradient-based methods for explainability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09602">A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation. (arXiv:2307.09602v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perrett_A/0/1/0/all/0/1">Adam Perrett</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_D/0/1/0/all/0/1">Danny Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1">Gavin Brown</a></p>
<p>This work presents a novel algorithm for transforming a neural network into a
spline representation. Unlike previous work that required convex and
piecewise-affine network operators to create a max-affine spline alternate
form, this work relaxes this constraint. The only constraint is that the
function be bounded and possess a well-define second derivative, although this
was shown experimentally to not be strictly necessary. It can also be performed
over the whole network rather than on each layer independently. As in previous
work, this bridges the gap between neural networks and approximation theory but
also enables the visualisation of network feature maps. Mathematical proof and
experimental investigation of the technique is performed with approximation
error and feature maps being extracted from a range of architectures, including
convolutional neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09607">Sequential Monte Carlo Learning for Time Series Structure Discovery. (arXiv:2307.09607v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saad_F/0/1/0/all/0/1">Feras A. Saad</a>, <a href="http://arxiv.org/find/cs/1/au:+Patton_B/0/1/0/all/0/1">Brian J. Patton</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1">Matthew D. Hoffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1">Rif A. Saurous</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1">Vikash K. Mansinghka</a></p>
<p>This paper presents a new approach to automatically discovering accurate
models of complex time series data. Working within a Bayesian nonparametric
prior over a symbolic space of Gaussian process time series models, we present
a novel structure learning algorithm that integrates sequential Monte Carlo
(SMC) and involutive MCMC for highly effective posterior inference. Our method
can be used both in "online" settings, where new data is incorporated
sequentially in time, and in "offline" settings, by using nested subsets of
historical data to anneal the posterior. Empirical measurements on real-world
time series show that our method can deliver 10x--100x runtime speedups over
previous MCMC and greedy-search structure learning algorithms targeting the
same model family. We use our method to perform the first large-scale
evaluation of Gaussian process time series structure learning on a prominent
benchmark of 1,428 econometric datasets. The results show that our method
discovers sensible models that deliver more accurate point forecasts and
interval forecasts over multiple horizons as compared to widely used
statistical and neural baselines that struggle on this challenging data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09613">Retrieving Continuous Time Event Sequences using Neural Temporal Point Processes with Learnable Hashing. (arXiv:2307.09613v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vinayak Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1">Srikanta Bedathur</a>, <a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1">Abir De</a></p>
<p>Temporal sequences have become pervasive in various real-world applications.
Consequently, the volume of data generated in the form of continuous time-event
sequence(s) or CTES(s) has increased exponentially in the past few years. Thus,
a significant fraction of the ongoing research on CTES datasets involves
designing models to address downstream tasks such as next-event prediction,
long-term forecasting, sequence classification etc. The recent developments in
predictive modeling using marked temporal point processes (MTPP) have enabled
an accurate characterization of several real-world applications involving the
CTESs. However, due to the complex nature of these CTES datasets, the task of
large-scale retrieval of temporal sequences has been overlooked by the past
literature. In detail, by CTES retrieval we mean that for an input query
sequence, a retrieval system must return a ranked list of relevant sequences
from a large corpus. To tackle this, we propose NeuroSeqRet, a
first-of-its-kind framework designed specifically for end-to-end CTES
retrieval. Specifically, NeuroSeqRet introduces multiple enhancements over
standard retrieval frameworks and first applies a trainable unwarping function
on the query sequence which makes it comparable with corpus sequences,
especially when a relevant query-corpus pair has individually different
attributes. Next, it feeds the unwarped query sequence and the corpus sequence
into MTPP-guided neural relevance models. We develop four variants of the
relevance model for different kinds of applications based on the trade-off
between accuracy and efficiency. We also propose an optimization framework to
learn binary sequence embeddings from the relevance scores, suitable for the
locality-sensitive hashing. Our experiments show the significant accuracy boost
of NeuroSeqRet as well as the efficacy of our hashing mechanism.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09614">Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Brusch_T/0/1/0/all/0/1">Thea Br&#xfc;sch</a>, <a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1">Mikkel N. Schmidt</a>, <a href="http://arxiv.org/find/stat/1/au:+Alstrom_T/0/1/0/all/0/1">Tommy S. Alstr&#xf8;m</a></p>
<p>Labeling of multivariate biomedical time series data is a laborious and
expensive process. Self-supervised contrastive learning alleviates the need for
large, labeled datasets through pretraining on unlabeled data. However, for
multivariate time series data the set of input channels often varies between
applications, and most existing work does not allow for transfer between
datasets with different sets of input channels. We propose learning one encoder
to operate on all input channels individually. We then use a message passing
neural network to extract a single representation across channels. We
demonstrate the potential of this method by pretraining our network on a
dataset with six EEG channels and finetuning on a dataset with two different
EEG channels. We compare networks with and without the message passing neural
network across different contrastive loss functions. We show that our method
combined with the TS2Vec loss outperforms all other methods in most settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09615">Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Md. Mahfuzur Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1">Vince D. Calhoun</a>, <a href="http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1">Sergey M. Plis</a></p>
<p>Deep learning (DL) models have been popular due to their ability to learn
directly from the raw data in an end-to-end paradigm, alleviating the concern
of a separate error-prone feature extraction phase. Recent DL-based
neuroimaging studies have also witnessed a noticeable performance advancement
over traditional machine learning algorithms. But the challenges of deep
learning models still exist because of the lack of transparency in these models
for their successful deployment in real-world applications. In recent years,
Explainable AI (XAI) has undergone a surge of developments mainly to get
intuitions of how the models reached the decisions, which is essential for
safety-critical domains such as healthcare, finance, and law enforcement
agencies. While the interpretability domain is advancing noticeably,
researchers are still unclear about what aspect of model learning a post hoc
method reveals and how to validate its reliability. This paper comprehensively
reviews interpretable deep learning models in the neuroimaging domain. Firstly,
we summarize the current status of interpretability resources in general,
focusing on the progression of methods, associated challenges, and opinions.
Secondly, we discuss how multiple recent neuroimaging studies leveraged model
interpretability to capture anatomical and functional brain alterations most
relevant to model predictions. Finally, we discuss the limitations of the
current practices and offer some valuable insights and guidance on how we can
steer our future research directions to make deep learning models substantially
interpretable and thus advance scientific understanding of brain disorders.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09619">Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning. (arXiv:2307.09619v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1">Zachary Charles</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_N/0/1/0/all/0/1">Nicole Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1">Krishna Pillutla</a>, <a href="http://arxiv.org/find/cs/1/au:+Reneer_M/0/1/0/all/0/1">Michael Reneer</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1">Zachary Garrett</a></p>
<p>We introduce a library, Dataset Grouper, to create large-scale
group-structured (e.g., federated) datasets, enabling federated learning
simulation at the scale of foundation models. This library allows the creation
of group-structured versions of existing datasets based on user-specified
partitions, and directly leads to a variety of useful heterogeneous datasets
that can be plugged into existing software frameworks. Dataset Grouper offers
three key advantages. First, it scales to settings where even a single group's
dataset is too large to fit in memory. Second, it provides flexibility, both in
choosing the base (non-partitioned) dataset and in defining partitions.
Finally, it is framework-agnostic. We empirically demonstrate that Dataset
Grouper allows for large-scale federated language modeling simulations on
datasets that are orders of magnitude larger than in previous work. Our
experimental results show that algorithms like FedAvg operate more as
meta-learning methods than as empirical risk minimization methods at this
scale, suggesting their utility in downstream personalization and task-specific
adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09631">Deep Reinforcement Learning for ESG financial portfolio management. (arXiv:2307.09631v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Garrido_Merchan_E/0/1/0/all/0/1">Eduardo C. Garrido-Merch&#xe1;n</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Mora_Figueroa_Cruz_Guzman_S/0/1/0/all/0/1">Sol Mora-Figueroa-Cruz-Guzm&#xe1;n</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Coronado_Vaca_M/0/1/0/all/0/1">Mar&#xed;a Coronado-Vaca</a></p>
<p>This paper investigates the application of Deep Reinforcement Learning (DRL)
for Environment, Social, and Governance (ESG) financial portfolio management,
with a specific focus on the potential benefits of ESG score-based market
regulation. We leveraged an Advantage Actor-Critic (A2C) agent and conducted
our experiments using environments encoded within the OpenAI Gym, adapted from
the FinRL platform. The study includes a comparative analysis of DRL agent
performance under standard Dow Jones Industrial Average (DJIA) market
conditions and a scenario where returns are regulated in line with company ESG
scores. In the ESG-regulated market, grants were proportionally allotted to
portfolios based on their returns and ESG scores, while taxes were assigned to
portfolios below the mean ESG score of the index. The results intriguingly
reveal that the DRL agent within the ESG-regulated market outperforms the
standard DJIA market setup. Furthermore, we considered the inclusion of ESG
variables in the agent state space, and compared this with scenarios where such
data were excluded. This comparison adds to the understanding of the role of
ESG factors in portfolio management decision-making. We also analyze the
behaviour of the DRL agent in IBEX 35 and NASDAQ-100 indexes. Both the A2C and
Proximal Policy Optimization (PPO) algorithms were applied to these additional
markets, providing a broader perspective on the generalization of our findings.
This work contributes to the evolving field of ESG investing, suggesting that
market regulation based on ESG scoring can potentially improve DRL-based
portfolio management, with significant implications for sustainable investing
strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09638">Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malviya_P/0/1/0/all/0/1">Pranshu Malviya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordido_G/0/1/0/all/0/1">Gon&#xe7;alo Mordido</a>, <a href="http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1">Aristide Baratin</a>, <a href="http://arxiv.org/find/cs/1/au:+Harikandeh_R/0/1/0/all/0/1">Reza Babanezhad Harikandeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jerry Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1">Simon Lacoste-Julien</a>, <a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1">Razvan Pascanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a></p>
<p>Adaptive gradient-based optimizers, particularly Adam, have left their mark
in training large-scale deep learning models. The strength of such optimizers
is that they exhibit fast convergence while being more robust to hyperparameter
choice. However, they often generalize worse than non-adaptive methods. Recent
studies have tied this performance gap to flat minima selection: adaptive
methods tend to find solutions in sharper basins of the loss landscape, which
in turn hurts generalization. To overcome this issue, we propose a new
memory-augmented version of Adam that promotes exploration towards flatter
minima by using a buffer of critical momentum terms during training.
Intuitively, the use of the buffer makes the optimizer overshoot outside the
basin of attraction if it is not wide enough. We empirically show that our
method improves the performance of several variants of Adam on standard
supervised language modelling and image classification tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09649">Application of BadNets in Spam Filters. (arXiv:2307.09649v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roychoudhury_S/0/1/0/all/0/1">Swagnik Roychoudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Veldanda_A/0/1/0/all/0/1">Akshaj Kumar Veldanda</a></p>
<p>Spam filters are a crucial component of modern email systems, as they help to
protect users from unwanted and potentially harmful emails. However, the
effectiveness of these filters is dependent on the quality of the machine
learning models that power them. In this paper, we design backdoor attacks in
the domain of spam filtering. By demonstrating the potential vulnerabilities in
the machine learning model supply chain, we highlight the need for careful
consideration and evaluation of the models used in spam filters. Our results
show that the backdoor attacks can be effectively used to identify
vulnerabilities in spam filters and suggest the need for ongoing monitoring and
improvement in this area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09653">HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1">Xiaotian Duan</a></p>
<p>Catastrophic forgetting, the phenomenon in which a neural network loses
previously obtained knowledge during the learning of new tasks, poses a
significant challenge in continual learning. The Hard-Attention-to-the-Task
(HAT) mechanism has shown potential in mitigating this problem, but its
practical implementation has been complicated by issues of usability and
compatibility, and a lack of support for existing network reuse. In this paper,
we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT
mechanism. HAT-CL not only automates gradient manipulation but also streamlines
the transformation of PyTorch modules into HAT modules. It achieves this by
providing a comprehensive suite of modules that can be seamlessly integrated
into existing architectures. Additionally, HAT-CL offers ready-to-use HAT
networks that are smoothly integrated with the TIMM library. Beyond the
redesign and reimplementation of HAT, we also introduce novel mask manipulation
techniques for HAT, which have consistently shown improvements across various
experiments. Our work paves the way for a broader application of the HAT
mechanism, opening up new possibilities in continual learning across diverse
models and applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09660">Neural Priority Queues for Graph Neural Networks. (arXiv:2307.09660v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rishabh Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1">Petar Veli&#x10d;kovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a></p>
<p>Graph Neural Networks (GNNs) have shown considerable success in neural
algorithmic reasoning. Many traditional algorithms make use of an explicit
memory in the form of a data structure. However, there has been limited
exploration on augmenting GNNs with external memory. In this paper, we present
Neural Priority Queues, a differentiable analogue to algorithmic priority
queues, for GNNs. We propose and motivate a desiderata for memory modules, and
show that Neural PQs exhibit the desiderata, and reason about their use with
algorithmic reasoning. This is further demonstrated by empirical results on the
CLRS-30 dataset. Furthermore, we find the Neural PQs useful in capturing
long-range interactions, as empirically shown on a dataset from the Long-Range
Graph Benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09661">Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization. (arXiv:2307.09661v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Drakoulas_G/0/1/0/all/0/1">G. I. Drakoulas</a>, <a href="http://arxiv.org/find/cs/1/au:+Gortsas_T/0/1/0/all/0/1">T. V. Gortsas</a>, <a href="http://arxiv.org/find/cs/1/au:+Polyzos_D/0/1/0/all/0/1">D. Polyzos</a></p>
<p>In the context of digital twins, structural health monitoring (SHM)
constitutes the backbone of condition-based maintenance, facilitating the
interconnection between virtual and physical assets. Guided wave propagation
(GWP) is commonly employed for the inspection of structures in SHM. However,
GWP is sensitive to variations in the material properties of the structure,
leading to false alarms. In this direction, uncertainty quantification (UQ) is
regularly applied to improve the reliability of predictions. Computational
mechanics is a useful tool for the simulation of GWP, and is often applied for
UQ. Even so, the application of UQ methods requires numerous simulations, while
large-scale, transient numerical GWP solutions increase the computational cost.
Reduced order models (ROMs) are commonly employed to provide numerical results
in a limited amount of time. In this paper, we propose a machine learning
(ML)-based ROM, mentioned as BO-ML-ROM, to decrease the computational time
related to the simulation of the GWP. The ROM is integrated with a Bayesian
optimization (BO) framework, to adaptively sample the parameters for the ROM
training. The finite element method is used for the simulation of the
high-fidelity models. The formulated ROM is used for forward UQ of the GWP in
an aluminum plate with varying material properties. To determine the influence
of each parameter perturbation, a global, variance-based sensitivity analysis
is implemented based on Sobol' indices. It is shown that Bayesian optimization
outperforms one-shot sampling methods, both in terms of accuracy and speed-up.
The predicted results reveal the efficiency of BO-ML-ROM for GWP and
demonstrate its value for UQ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09665">Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers. (arXiv:2307.09665v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Horawalavithana_S/0/1/0/all/0/1">Sameera Horawalavithana</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayton_E/0/1/0/all/0/1">Ellyn Ayton</a>, <a href="http://arxiv.org/find/cs/1/au:+Usenko_A/0/1/0/all/0/1">Anastasiya Usenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Cosbey_R/0/1/0/all/0/1">Robin Cosbey</a>, <a href="http://arxiv.org/find/cs/1/au:+Volkova_S/0/1/0/all/0/1">Svitlana Volkova</a></p>
<p>The ability to anticipate technical expertise and capability evolution trends
globally is essential for national and global security, especially in
safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging
fields like artificial intelligence (AI). In this work, we extend traditional
statistical relational learning approaches (e.g., link prediction in
collaboration networks) and formulate a problem of anticipating technical
expertise and capability evolution using dynamic heterogeneous graph
representations. We develop novel capabilities to forecast collaboration
patterns, authorship behavior, and technical capability evolution at different
granularities (e.g., scientist and institution levels) in two distinct research
fields. We implement a dynamic graph transformer (DGT) neural architecture,
which pushes the state-of-the-art graph neural network models by (a)
forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b)
relying on both discrete -- and continuous -- time inputs. We demonstrate that
our DGT models predict collaboration, partnership, and expertise patterns with
0.26, 0.73, and 0.53 mean reciprocal rank values for AI and 0.48, 0.93, and
0.22 for NN domains. DGT model performance exceeds the best-performing static
graph baseline models by 30-80% across AI and NN domains. Our findings
demonstrate that DGT models boost inductive task performance, when previously
unseen nodes appear in the test data, for the domains with emerging
collaboration patterns (e.g., AI). Specifically, models accurately predict
which established scientists will collaborate with early career scientists and
vice-versa in the AI domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09668">Towards A Unified Agent with Foundation Models. (arXiv:2307.09668v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1">Norman Di Palo</a>, <a href="http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1">Arunkumar Byravan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1">Leonard Hasenclever</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1">Markus Wulfmeier</a>, <a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1">Nicolas Heess</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1">Martin Riedmiller</a></p>
<p>Language Models and Vision Language Models have recently demonstrated
unprecedented capabilities in terms of understanding human intentions,
reasoning, scene understanding, and planning-like behaviour, in text form,
among many others. In this work, we investigate how to embed and leverage such
abilities in Reinforcement Learning (RL) agents. We design a framework that
uses language as the core reasoning tool, exploring how this enables an agent
to tackle a series of fundamental RL challenges, such as efficient exploration,
reusing experience data, scheduling skills, and learning from observations,
which traditionally require separate, vertically designed algorithms. We test
our method on a sparse-reward simulated robotic manipulation environment, where
a robot needs to stack a set of objects. We demonstrate substantial performance
improvements over baselines in exploration efficiency and ability to reuse data
from offline datasets, and illustrate how to reuse learned skills to solve
novel tasks or imitate videos of human experts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09670">JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting. (arXiv:2307.09670v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Row_E/0/1/0/all/0/1">Eleanor Row</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jingjing Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1">George Fazekas</a></p>
<p>Jazz pianists often uniquely interpret jazz standards. Passages from these
interpretations can be viewed as sections of variation. We manually extracted
such variations from solo jazz piano performances. The JAZZVAR dataset is a
collection of 502 pairs of Variation and Original MIDI segments. Each Variation
in the dataset is accompanied by a corresponding Original segment containing
the melody and chords from the original jazz standard. Our approach differs
from many existing jazz datasets in the music information retrieval (MIR)
community, which often focus on improvisation sections within jazz
performances. In this paper, we outline the curation process for obtaining and
sorting the repertoire, the pipeline for creating the Original and Variation
pairs, and our analysis of the dataset. We also introduce a new generative
music task, Music Overpainting, and present a baseline Transformer model
trained on the JAZZVAR dataset for this task. Other potential applications of
our dataset include expressive performance analysis and performer
identification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09672">Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction. (arXiv:2307.09672v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haider_D/0/1/0/all/0/1">Daniel Haider</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehler_M/0/1/0/all/0/1">Martin Ehler</a>, <a href="http://arxiv.org/find/cs/1/au:+Balazs_P/0/1/0/all/0/1">Peter Balazs</a></p>
<p>The paper uses a frame-theoretic setting to study the injectivity of a
ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part. In
particular, the interplay between the radius of the ball and the bias vector is
emphasized. Together with a perspective from convex geometry, this leads to a
computationally feasible method of verifying the injectivity of a ReLU-layer
under reasonable restrictions in terms of an upper bound of the bias vector.
Explicit reconstruction formulas are provided, inspired by the duality concept
from frame theory. All this gives rise to the possibility of quantifying the
invertibility of a ReLU-layer and a concrete reconstruction algorithm for any
input vector on the ball.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09688">Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Wei Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Haitao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haoming Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chen Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hongzhi Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Haoyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hanqing Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruirui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Monica Xiao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Goutam_R/0/1/0/all/0/1">Rahul Goutam</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Subbian_K/0/1/0/all/0/1">Karthik Subbian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Suhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yizhou Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Bing Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xianfeng Tang</a></p>
<p>Modeling customer shopping intentions is a crucial task for e-commerce, as it
directly impacts user experience and engagement. Thus, accurately understanding
customer preferences is essential for providing personalized recommendations.
Session-based recommendation, which utilizes customer session data to predict
their next interaction, has become increasingly popular. However, existing
session datasets have limitations in terms of item attributes, user diversity,
and dataset scale. As a result, they cannot comprehensively capture the
spectrum of user behaviors and preferences. To bridge this gap, we present the
Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It
is the first multilingual dataset consisting of millions of user sessions from
six different locales, where the major languages of products are English,
German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can
help us enhance personalization and understanding of user preferences, which
can benefit various existing tasks as well as enable new tasks. To test the
potential of the dataset, we introduce three tasks in this work: (1)
next-product recommendation, (2) next-product recommendation with domain
shifts, and (3) next-product title generation. With the above tasks, we
benchmark a range of algorithms on our proposed dataset, drawing new insights
for further research and practice. In addition, based on the proposed dataset
and tasks, we hosted a competition in the KDD CUP 2023 and have attracted
thousands of users and submissions. The winning solutions and the associated
workshop can be accessed at our website https://kddcup23.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09691">Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach. (arXiv:2307.09691v1 [cs.NI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qianqian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haixia Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1">Dongfeng Yuan</a></p>
<p>Meeting the strict Quality of Service (QoS) requirements of terminals has
imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems,
due to the limited multidimensional resources. To address this challenge, we
propose a collaborative MEC framework that facilitates resource sharing between
the edge servers, and with the aim to maximize the long-term QoS and reduce the
cache switching cost through joint optimization of service caching,
collaborative offfoading, and computation and communication resource
allocation. The dual timescale feature and temporal recurrence relationship
between service caching and other resource allocation make solving the problem
even more challenging. To solve it, we propose a deep reinforcement learning
(DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a
short-term genetic algorithm (GA) and a long short-term memory network-based
deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the
optimization problem as a Markov decision process (MDP) where the
small-timescale resource allocation decisions generated by an improved GA are
taken as the states and input into a centralized LSTM-DDPG agent to generate
the service caching decision for the large-timescale. Simulation results
demonstrate that our proposed algorithm outperforms the baseline algorithms in
terms of the average QoS and cache switching cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09692">STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization. (arXiv:2307.09692v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yachen Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Li He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zifeng Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donglin Wang</a></p>
<p>Preference-based reinforcement learning (PbRL) promises to learn a complex
reward function with binary human preference. However, such human-in-the-loop
formulation requires considerable human effort to assign preference labels to
segment pairs, hindering its large-scale applications. Recent approache has
tried to reuse unlabeled segments, which implicitly elucidates the distribution
of segments and thereby alleviates the human effort. And consistency
regularization is further considered to improve the performance of
semi-supervised learning. However, we notice that, unlike general
classification tasks, in PbRL there exits a unique phenomenon that we defined
as similarity trap in this paper. Intuitively, human can have diametrically
opposite preferredness for similar segment pairs, but such similarity may trap
consistency regularization fail in PbRL. Due to the existence of similarity
trap, such consistency regularization improperly enhances the consistency
possiblity of the model's predictions between segment pairs, and thus reduces
the confidence in reward learning, since the augmented distribution does not
match with the original one in PbRL. To overcome such issue, we present a
self-training method along with our proposed peer regularization, which
penalizes the reward model memorizing uninformative labels and acquires
confident predictions. Empirically, we demonstrate that our approach is capable
of learning well a variety of locomotion and robotic manipulation behaviors
using different semi-supervised alternatives and peer regularization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09702">Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Willard_B/0/1/0/all/0/1">Brandon T. Willard</a>, <a href="http://arxiv.org/find/cs/1/au:+Louf_R/0/1/0/all/0/1">R&#xe9;mi Louf</a></p>
<p>In this article we describe an efficient approach to guiding language model
text generation with regular expressions and context-free grammars. Our
approach adds little to no overhead to the token sequence generation process,
and makes guided generation feasible in practice. An implementation is provided
in the open source Python library Outlines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09706">RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap. (arXiv:2307.09706v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tianjian Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1">Phillipe Langlais</a></p>
<p>Taxonomies are an essential knowledge representation, yet most studies on
automatic taxonomy construction (ATC) resort to manual evaluation to score
proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just
as important as taxonomy construction. We propose RaTE, an automatic label-free
taxonomy scoring procedure, which relies on a large pre-trained language model.
We apply our evaluation procedure to three state-of-the-art ATC algorithms with
which we built seven taxonomies from the Yelp domain, and show that 1) RaTE
correlates well with human judgments and 2) artificially degrading a taxonomy
leads to decreasing RaTE score.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09742">Improved Distribution Matching for Dataset Condensation. (arXiv:2307.09742v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1">Ganlong Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yipeng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yizhou Yu</a></p>
<p>Dataset Condensation aims to condense a large dataset into a smaller one
while maintaining its ability to train a well-performing model, thus reducing
the storage cost and training effort in deep learning applications. However,
conventional dataset condensation methods are optimization-oriented and
condense the dataset by performing gradient or parameter matching during model
optimization, which is computationally intensive even on small datasets and
models. In this paper, we propose a novel dataset condensation method based on
distribution matching, which is more efficient and promising. Specifically, we
identify two important shortcomings of naive distribution matching (i.e.,
imbalanced feature numbers and unvalidated embeddings for distance computation)
and address them with three novel techniques (i.e., partitioning and expansion
augmentation, efficient and enriched model sampling, and class-aware
distribution regularization). Our simple yet effective method outperforms most
previous optimization-oriented methods with much fewer computational resources,
thereby scaling data condensation to larger datasets and models. Extensive
experiments demonstrate the effectiveness of our method. Codes are available at
https://github.com/uitrbn/IDM
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09759">Constructing Extreme Learning Machines with zero Spectral Bias. (arXiv:2307.09759v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_K/0/1/0/all/0/1">Kaumudi Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Snigdha_V/0/1/0/all/0/1">Vukka Snigdha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1">Arya Kumar Bhattacharya</a></p>
<p>The phenomena of Spectral Bias, where the higher frequency components of a
function being learnt in a feedforward Artificial Neural Network (ANN) are seen
to converge more slowly than the lower frequencies, is observed ubiquitously
across ANNs. This has created technology challenges in fields where resolution
of higher frequencies is crucial, like in Physics Informed Neural Networks
(PINNs). Extreme Learning Machines (ELMs) that obviate an iterative solution
process which provides the theoretical basis of Spectral Bias (SB), should in
principle be free of the same. This work verifies the reliability of this
assumption, and shows that it is incorrect. However, the structure of ELMs
makes them naturally amenable to implementation of variants of Fourier Feature
Embeddings, which have been shown to mitigate SB in ANNs. This approach is
implemented and verified to completely eliminate SB, thus bringing into
feasibility the application of ELMs for practical problems like PINNs where
resolution of higher frequencies is essential.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09762">Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ajayakumar_A/0/1/0/all/0/1">Abhishek Ajayakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Raha_S/0/1/0/all/0/1">Soumyendu Raha</a></p>
<p>Complex networks are used to model many real-world systems. However, the
dimensionality of these systems can make them challenging to analyze.
Dimensionality reduction techniques like POD can be used in such cases.
However, these models are susceptible to perturbations in the input data. We
propose an algorithmic framework that combines techniques from pattern
recognition (PR) and stochastic filtering theory to enhance the output of such
models. The results of our study show that our method can improve the accuracy
of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are
susceptible to adversarial attacks. However, recent research has revealed that
neural Ordinary Differential Equations (ODEs) exhibit robustness in specific
applications. We benchmark our algorithmic framework with a Neural ODE-based
approach as a reference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09767">Sig-Splines: universal approximation and convex calibration of time series generative models. (arXiv:2307.09767v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wiese_M/0/1/0/all/0/1">Magnus Wiese</a>, <a href="http://arxiv.org/find/cs/1/au:+Murray_P/0/1/0/all/0/1">Phillip Murray</a>, <a href="http://arxiv.org/find/cs/1/au:+Korn_R/0/1/0/all/0/1">Ralf Korn</a></p>
<p>We propose a novel generative model for multivariate discrete-time time
series data. Drawing inspiration from the construction of neural spline flows,
our algorithm incorporates linear transformations and the signature transform
as a seamless substitution for traditional neural networks. This approach
enables us to achieve not only the universality property inherent in neural
networks but also introduces convexity in the model's parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09768">How Curvature Enhance the Adaptation Power of Framelet GCNs. (arXiv:2307.09768v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1">Dai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhiqi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junbin Gao</a></p>
<p>Graph neural network (GNN) has been demonstrated powerful in modeling
graph-structured data. However, despite many successful cases of applying GNNs
to various graph classification and prediction tasks, whether the graph
geometrical information has been fully exploited to enhance the learning
performance of GNNs is not yet well understood. This paper introduces a new
approach to enhance GNN by discrete graph Ricci curvature. Specifically, the
graph Ricci curvature defined on the edges of a graph measures how difficult
the information transits on one edge from one node to another based on their
neighborhoods. Motivated by the geometric analogy of Ricci curvature in the
graph setting, we prove that by inserting the curvature information with
different carefully designed transformation function $\zeta$, several known
computational issues in GNN such as over-smoothing can be alleviated in our
proposed model. Furthermore, we verified that edges with very positive Ricci
curvature (i.e., $\kappa_{i,j} \approx 1$) are preferred to be dropped to
enhance model's adaption to heterophily graph and one curvature based graph
edge drop algorithm is proposed. Comprehensive experiments show that our
curvature-based GNN model outperforms the state-of-the-art baselines in both
homophily and heterophily graph datasets, indicating the effectiveness of
involving graph geometric information in GNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09771">A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices. (arXiv:2307.09771v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Li_J/0/1/0/all/0/1">Jinyang Li</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Z/0/1/0/all/0/1">Zhepeng Wang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Hu_Z/0/1/0/all/0/1">Zhirui Hu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Date_P/0/1/0/all/0/1">Prasanna Date</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Jiang_W/0/1/0/all/0/1">Weiwen Jiang</a></p>
<p>Quantum computing presents a promising approach for machine learning with its
capability for extremely parallel computation in high-dimension through
superposition and entanglement. Despite its potential, existing quantum
learning algorithms, such as Variational Quantum Circuits(VQCs), face
challenges in handling more complex datasets, particularly those that are not
linearly separable. What's more, it encounters the deployability issue, making
the learning models suffer a drastic accuracy drop after deploying them to the
actual quantum devices. To overcome these limitations, this paper proposes a
novel spatial-temporal design, namely ST-VQC, to integrate non-linearity in
quantum learning and improve the robustness of the learning model to noise.
Specifically, ST-VQC can extract spatial features via a novel block-based
encoding quantum sub-circuit coupled with a layer-wise computation quantum
sub-circuit to enable temporal-wise deep learning. Additionally, a SWAP-Free
physical circuit design is devised to improve robustness. These designs bring a
number of hyperparameters. After a systematic analysis of the design space for
each design component, an automated optimization framework is proposed to
generate the ST-VQC quantum circuit. The proposed ST-VQC has been evaluated on
two IBM quantum processors, ibm_cairo with 27 qubits and ibmq_lima with 7
qubits to assess its effectiveness. The results of the evaluation on the
standard dataset for binary classification show that ST-VQC can achieve over
30% accuracy improvement compared with existing VQCs on actual quantum
computers. Moreover, on a non-linear synthetic dataset, the ST-VQC outperforms
a linear classifier by 27.9%, while the linear classifier using classical
computing outperforms the existing VQC by 15.58%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09779">Beyond Single-Feature Importance with ICECREAM. (arXiv:2307.09779v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oesterle_M/0/1/0/all/0/1">Michael Oesterle</a>, <a href="http://arxiv.org/find/cs/1/au:+Blobaum_P/0/1/0/all/0/1">Patrick Bl&#xf6;baum</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastakouri_A/0/1/0/all/0/1">Atalanti A. Mastakouri</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirschbaum_E/0/1/0/all/0/1">Elke Kirschbaum</a></p>
<p>Which set of features was responsible for a certain output of a machine
learning model? Which components caused the failure of a cloud computing
application? These are just two examples of questions we are addressing in this
work by Identifying Coalition-based Explanations for Common and Rare Events in
Any Model (ICECREAM). Specifically, we propose an information-theoretic
quantitative measure for the influence of a coalition of variables on the
distribution of a target variable. This allows us to identify which set of
factors is essential to obtain a certain outcome, as opposed to
well-established explainability and causal contribution analysis methods which
can assign contributions only to individual factors and rank them by their
importance. In experiments with synthetic and real-world data, we show that
ICECREAM outperforms state-of-the-art methods for explainability and root cause
analysis, and achieves impressive accuracy in both tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09781">Text2Layer: Layered Image Generation using Latent Diffusion Model. (arXiv:2307.09781v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wentian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chien_J/0/1/0/all/0/1">Jeff Chien</a></p>
<p>Layer compositing is one of the most popular image editing workflows among
both amateurs and professionals. Motivated by the success of diffusion models,
we explore layer compositing from a layered image generation perspective.
Instead of generating an image, we propose to generate background, foreground,
layer mask, and the composed image simultaneously. To achieve layered image
generation, we train an autoencoder that is able to reconstruct layered images
and train diffusion models on the latent representation. One benefit of the
proposed problem is to enable better compositing workflows in addition to the
high-quality image output. Another benefit is producing higher-quality layer
masks compared to masks produced by a separate step of image segmentation.
Experimental results show that the proposed method is able to generate
high-quality layered images and initiates a benchmark for future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09782">ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiaoxia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zhewei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuxiong He</a></p>
<p>In the complex domain of large language models (LLMs), striking a balance
between computational efficiency and maintaining model quality is a formidable
challenge. Navigating the inherent limitations of uniform quantization,
particularly when dealing with outliers, and motivated by the launch of
NVIDIA's H100 hardware, this study delves into the viability of floating-point
(FP) quantization, particularly focusing on FP8 and FP4, as a potential
solution. Our comprehensive investigation reveals that for LLMs, FP8 activation
consistently outshines its integer (INT8) equivalent, with the performance edge
becoming more noticeable in models possessing parameters beyond one billion.
For weight quantization, our findings indicate that FP4 exhibits comparable, if
not superior, performance to INT4, simplifying deployment on FP-supported
hardware like H100. To mitigate the overhead from precision alignment caused by
the disparity between weights and activations, we propose two scaling
constraints for weight quantization that negligibly impact the performance
compared to the standard W4A8 model. We additionally enhance our quantization
methods by integrating the Low Rank Compensation (LoRC) strategy, yielding
improvements especially in smaller models. The results of our investigation
emphasize the immense potential of FP quantization for LLMs, paving the way for
high-efficiency deployment in resource-limited settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09792">A Note on Hardness of Computing Recursive Teaching Dimension. (arXiv:2307.09792v1 [cs.CC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a></p>
<p>In this short note, we show that the problem of computing the recursive
teaching dimension (RTD) for a concept class (given explicitly as input)
requires $n^{\Omega(\log n)}$-time, assuming the exponential time hypothesis
(ETH). This matches the running time $n^{O(\log n)}$ of the brute-force
algorithm for the problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09795">From West to East: Who can understand the music of the others better?. (arXiv:2307.09795v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Papaioannou_C/0/1/0/all/0/1">Charilaos Papaioannou</a>, <a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1">Emmanouil Benetos</a>, <a href="http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1">Alexandros Potamianos</a></p>
<p>Recent developments in MIR have led to several benchmark deep learning models
whose embeddings can be used for a variety of downstream tasks. At the same
time, the vast majority of these models have been trained on Western pop/rock
music and related styles. This leads to research questions on whether these
models can be used to learn representations for different music cultures and
styles, or whether we can build similar music audio embedding models trained on
data from different cultures or styles. To that end, we leverage transfer
learning methods to derive insights about the similarities between the
different music cultures to which the data belongs to. We use two Western music
datasets, two traditional/folk datasets coming from eastern Mediterranean
cultures, and two datasets belonging to Indian art music. Three deep audio
embedding models are trained and transferred across domains, including two
CNN-based and a Transformer-based architecture, to perform auto-tagging for
each target domain dataset. Experimental results show that competitive
performance is achieved in all domains via transfer learning, while the best
source dataset varies for each music culture. The implementation and the
trained models are both provided in a public repository.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09796">Forecasting Early with Meta Learning. (arXiv:2307.09796v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jawed_S/0/1/0/all/0/1">Shayan Jawed</a>, <a href="http://arxiv.org/find/cs/1/au:+Madhusudhanan_K/0/1/0/all/0/1">Kiran Madhusudhanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yalavarthi_V/0/1/0/all/0/1">Vijaya Krishna Yalavarthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1">Lars Schmidt-Thieme</a></p>
<p>In the early observation period of a time series, there might be only a few
historic observations available to learn a model. However, in cases where an
existing prior set of datasets is available, Meta learning methods can be
applicable. In this paper, we devise a Meta learning method that exploits
samples from additional datasets and learns to augment time series through
adversarial learning as an auxiliary task for the target dataset. Our model
(FEML), is equipped with a shared Convolutional backbone that learns features
for varying length inputs from different datasets and has dataset specific
heads to forecast for different output lengths. We show that FEML can meta
learn across datasets and by additionally learning on adversarial generated
samples as auxiliary samples for the target dataset, it can improve the
forecasting performance compared to single task learning, and various solutions
adapted from Joint learning, Multi-task learning and classic forecasting
baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09797">Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Negiar_G/0/1/0/all/0/1">Geoffrey N&#xe9;giar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1">Ruijun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Meetei_O/0/1/0/all/0/1">O. Nangba Meetei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1">Mengfei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1">Michael W. Mahoney</a></p>
<p>Obtaining accurate probabilistic forecasts while respecting hierarchical
information is an important operational challenge in many applications, perhaps
most obviously in energy management, supply chain planning, and resource
allocation. The basic challenge, especially for multivariate forecasting, is
that forecasts are often required to be coherent with respect to the
hierarchical structure. In this paper, we propose a new model which leverages a
factor model structure to produce coherent forecasts by construction. This is a
consequence of a simple (exchangeability) observation: permuting
\textit{}base-level series in the hierarchy does not change their aggregates.
Our model uses a convolutional neural network to produce parameters for the
factors, their loadings and base-level distributions; it produces samples which
can be differentiated with respect to the model's parameters; and it can
therefore optimize for any sample-based loss function, including the Continuous
Ranked Probability Score and quantile losses. We can choose arbitrary
continuous distributions for the factor and the base-level distributions. We
compare our method to two previous methods which can be optimized end-to-end,
while enforcing coherent aggregation. Our model achieves significant
improvements: between $11.8-41.4\%$ on three hierarchical forecasting datasets.
We also analyze the influence of parameters in our model with respect to
base-level distribution and number of factors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09801">Graph Federated Learning Based on the Decentralized Framework. (arXiv:2307.09801v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peilin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yanni Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingyue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wu Chen</a></p>
<p>Graph learning has a wide range of applications in many scenarios, which
require more need for data privacy. Federated learning is an emerging
distributed machine learning approach that leverages data from individual
devices or data centers to improve the accuracy and generalization of the
model, while also protecting the privacy of user data. Graph-federated learning
is mainly based on the classical federated learning framework i.e., the
Client-Server framework. However, the Client-Server framework faces problems
such as a single point of failure of the central server and poor scalability of
network topology. First, we introduce the decentralized framework to
graph-federated learning. Second, determine the confidence among nodes based on
the similarity of data among nodes, subsequently, the gradient information is
then aggregated by linear weighting based on confidence. Finally, the proposed
method is compared with FedAvg, Fedprox, GCFL, and GCFL+ to verify the
effectiveness of the proposed method. Experiments demonstrate that the proposed
method outperforms other methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09810">GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence. (arXiv:2307.09810v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xia Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_K/0/1/0/all/0/1">Kai Fong Ernest Chong</a></p>
<p>Web image datasets curated online inherently contain ambiguous
in-distribution (ID) instances and out-of-distribution (OOD) instances, which
we collectively call non-conforming (NC) instances. In many recent approaches
for mitigating the negative effects of NC instances, the core implicit
assumption is that the NC instances can be found via entropy maximization. For
"entropy" to be well-defined, we are interpreting the output prediction vector
of an instance as the parameter vector of a multinomial random variable, with
respect to some trained model with a softmax output layer. Hence, entropy
maximization is based on the idealized assumption that NC instances have
predictions that are "almost" uniformly distributed. However, in real-world web
image datasets, there are numerous NC instances whose predictions are far from
being uniformly distributed. To tackle the limitation of entropy maximization,
we propose $(\alpha, \beta)$-generalized KL divergence,
$\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$, which can be used to identify
significantly more NC instances. Theoretical properties of
$\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ are proven, and we also show
empirically that a simple use of $\mathcal{D}_{\text{KL}}^{\alpha,
\beta}(p\|q)$ outperforms all baselines on the NC instance identification task.
Building upon $(\alpha,\beta)$-generalized KL divergence, we also introduce a
new iterative training framework, GenKL, that identifies and relabels NC
instances. When evaluated on three web image datasets, Clothing1M,
Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art
classification accuracies: $81.34\%$, $85.73\%$ and $78.99\%$/$92.54\%$
(top-1/top-5), respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09816">Manifold Learning with Sparse Regularised Optimal Transport. (arXiv:2307.09816v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1">Stephen Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Mordant_G/0/1/0/all/0/1">Gilles Mordant</a>, <a href="http://arxiv.org/find/stat/1/au:+Matsumoto_T/0/1/0/all/0/1">Tetsuya Matsumoto</a>, <a href="http://arxiv.org/find/stat/1/au:+Schiebinger_G/0/1/0/all/0/1">Geoffrey Schiebinger</a></p>
<p>Manifold learning is a central task in modern statistics and data science.
Many datasets (cells, documents, images, molecules) can be represented as point
clouds embedded in a high dimensional ambient space, however the degrees of
freedom intrinsic to the data are usually far fewer than the number of ambient
dimensions. The task of detecting a latent manifold along which the data are
embedded is a prerequisite for a wide family of downstream analyses. Real-world
datasets are subject to noisy observations and sampling, so that distilling
information about the underlying manifold is a major challenge. We propose a
method for manifold learning that utilises a symmetric version of optimal
transport with a quadratic regularisation that constructs a sparse and adaptive
affinity matrix, that can be interpreted as a generalisation of the
bistochastic kernel normalisation. We prove that the resulting kernel is
consistent with a Laplace-type operator in the continuous limit, establish
robustness to heteroskedastic noise and exhibit these results in simulations.
We identify a highly efficient computational scheme for computing this optimal
transport for discrete data and demonstrate that it outperforms competing
methods in a set of examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09818">Deep unrolling Shrinkage Network for Dynamic MR imaging. (arXiv:2307.09818v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1">Yinghao Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaodi Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1">Weihang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1">Yue Hu</a></p>
<p>Deep unrolling networks that utilize sparsity priors have achieved great
success in dynamic magnetic resonance (MR) imaging. The convolutional neural
network (CNN) is usually utilized to extract the transformed domain, and then
the soft thresholding (ST) operator is applied to the CNN-transformed data to
enforce the sparsity priors. However, the ST operator is usually constrained to
be the same across all channels of the CNN-transformed data. In this paper, we
propose a novel operator, called soft thresholding with channel attention
(AST), that learns the threshold for each channel. In particular, we put
forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the
alternating direction method of multipliers (ADMM) for optimizing the
transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on
an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net
outperforms the state-of-the-art methods. The source code is available at
\url{https://github.com/yhao-z/DUS-Net}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09823">Multi-modal Learning based Prediction for Disease. (arXiv:2307.09823v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yaran Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xueyu Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_Y/0/1/0/all/0/1">Yu Han</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Haoran Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1">Dongbin Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Jingzhong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a></p>
<p>Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic
liver disease, which can be predicted accurately to prevent advanced fibrosis
and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is
invasive, expensive, and prone to sampling errors. Therefore, non-invasive
studies are extremely promising, yet they are still in their infancy due to the
lack of comprehensive research data and intelligent methods for multi-modal
data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a
comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD
prediction method (DeepFLD). The dataset includes over 6000 participants
physical examinations, laboratory and imaging studies, extensive
questionnaires, and facial images of partial participants, which is
comprehensive and valuable for clinical studies. From the dataset, we
quantitatively analyze and select clinical metadata that most contribute to
NAFLD prediction. Furthermore, the proposed DeepFLD, a deep neural network
model designed to predict NAFLD using multi-modal input, including metadata and
facial images, outperforms the approach that only uses metadata. Satisfactory
performance is also verified on other unseen datasets. Inspiringly, DeepFLD can
achieve competitive results using only facial images as input rather than
metadata, paving the way for a more robust and simpler non-invasive NAFLD
diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09829">What do neural networks learn in image classification? A frequency shortcut perspective. (arXiv:2307.09829v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shunxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Veldhuis_R/0/1/0/all/0/1">Raymond Veldhuis</a>, <a href="http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1">Christoph Brune</a>, <a href="http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1">Nicola Strisciuglio</a></p>
<p>Frequency analysis is useful for understanding the mechanisms of
representation learning in neural networks (NNs). Most research in this area
focuses on the learning dynamics of NNs for regression tasks, while little for
classification. This study empirically investigates the latter and expands the
understanding of frequency shortcuts. First, we perform experiments on
synthetic datasets, designed to have a bias in different frequency bands. Our
results demonstrate that NNs tend to find simple solutions for classification,
and what they learn first during training depends on the most distinctive
frequency characteristics, which can be either low- or high-frequencies.
Second, we confirm this phenomenon on natural images. We propose a metric to
measure class-wise frequency characteristics and a method to identify frequency
shortcuts. The results show that frequency shortcuts can be texture-based or
shape-based, depending on what best simplifies the objective. Third, we
validate the transferability of frequency shortcuts on out-of-distribution
(OOD) test sets. Our results suggest that frequency shortcuts can be
transferred across datasets and cannot be fully avoided by larger model
capacity and data augmentation. We recommend that future research should focus
on effective training schemes mitigating frequency shortcut learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09835">Deep Operator Network Approximation Rates for Lipschitz Operators. (arXiv:2307.09835v1 [math.NA])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Schwab_C/0/1/0/all/0/1">Christoph Schwab</a>, <a href="http://arxiv.org/find/math/1/au:+Stein_A/0/1/0/all/0/1">Andreas Stein</a>, <a href="http://arxiv.org/find/math/1/au:+Zech_J/0/1/0/all/0/1">Jakob Zech</a></p>
<p>We establish universality and expression rate bounds for a class of neural
Deep Operator Networks (DON) emulating Lipschitz (or H\"older) continuous maps
$\mathcal G:\mathcal X\to\mathcal Y$ between (subsets of) separable Hilbert
spaces $\mathcal X$, $\mathcal Y$. The DON architecture considered uses linear
encoders $\mathcal E$ and decoders $\mathcal D$ via (biorthogonal) Riesz bases
of $\mathcal X$, $\mathcal Y$, and an approximator network of an
infinite-dimensional, parametric coordinate map that is Lipschitz continuous on
the sequence space $\ell^2(\mathbb N)$. Unlike previous works ([Herrmann,
Schwab and Zech: Neural and Spectral operator surrogates: construction and
expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential
Convergence of Deep Operator Networks for Elliptic Partial Differential
Equations, SAM Report, 2022]), which required for example $\mathcal G$ to be
holomorphic, the present expression rate results require mere Lipschitz (or
H\"older) continuity of $\mathcal G$. Key in the proof of the present
expression rate bounds is the use of either super-expressive activations (e.g.
[Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021],
[Shen, Yang and Zhang: Neural network approximation: Three hidden layers are
enough, Neural Networks, 2021], and the references there) which are inspired by
the Kolmogorov superposition theorem, or of nonstandard NN architectures with
standard (ReLU) activations as recently proposed in [Zhang, Shen and Yang:
Neural Network Architecture Beyond Width and Depth, Adv. in Neural Inf. Proc.
Sys., 2022]. We illustrate the abstract results by approximation rate bounds
for emulation of a) solution operators for parametric elliptic variational
inequalities, and b) Lipschitz maps of Hilbert-Schmidt operators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09836">Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders. (arXiv:2307.09836v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1">Guillaume Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Condat_L/0/1/0/all/0/1">Laurent Condat</a>, <a href="http://arxiv.org/find/cs/1/au:+Barlaud_M/0/1/0/all/0/1">Michel Barlaud</a></p>
<p>Looking for sparsity is nowadays crucial to speed up the training of
large-scale neural networks. Projections onto the $\ell_{1,2}$ and
$\ell_{1,\infty}$ are among the most efficient techniques to sparsify and
reduce the overall cost of neural networks. In this paper, we introduce a new
projection algorithm for the $\ell_{1,\infty}$ norm ball. The worst-case time
complexity of this algorithm is $\mathcal{O}\big(nm+J\log(nm)\big)$ for a
matrix in $\mathbb{R}^{n\times m}$. $J$ is a term that tends to 0 when the
sparsity is high, and to $nm$ when the sparsity is low. Its implementation is
easy and it is guaranteed to converge to the exact solution in a finite time.
Moreover, we propose to incorporate the $\ell_{1,\infty}$ ball projection while
training an autoencoder to enforce feature selection and sparsity of the
weights. Sparsification appears in the encoder to primarily do feature
selection due to our application in biology, where only a very small part
($&lt;2\%$) of the data is relevant. We show that both in the biological case and
in the general case of sparsity that our method is the fastest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09844">Reinforcement Learning for Credit Index Option Hedging. (arXiv:2307.09844v1 [q-fin.TR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Mandelli_F/0/1/0/all/0/1">Francesco Mandelli</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Pinciroli_M/0/1/0/all/0/1">Marco Pinciroli</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Trapletti_M/0/1/0/all/0/1">Michele Trapletti</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Vittori_E/0/1/0/all/0/1">Edoardo Vittori</a></p>
<p>In this paper, we focus on finding the optimal hedging strategy of a credit
index option using reinforcement learning. We take a practical approach, where
the focus is on realism i.e. discrete time, transaction costs; even testing our
policy on real market data. We apply a state of the art algorithm, the Trust
Region Volatility Optimization (TRVO) algorithm and show that the derived
hedging strategy outperforms the practitioner's Black &amp; Scholes delta hedge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09862">Towards a population-informed approach to the definition of data-driven models for structural dynamics. (arXiv:2307.09862v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tsialiamanis_G/0/1/0/all/0/1">G. Tsialiamanis</a>, <a href="http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1">N. Dervilis</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagg_D/0/1/0/all/0/1">D.J. Wagg</a>, <a href="http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1">K. Worden</a></p>
<p>Machine learning has affected the way in which many phenomena for various
domains are modelled, one of these domains being that of structural dynamics.
However, because machine-learning algorithms are problem-specific, they often
fail to perform efficiently in cases of data scarcity. To deal with such
issues, combination of physics-based approaches and machine learning algorithms
have been developed. Although such methods are effective, they also require the
analyser's understanding of the underlying physics of the problem. The current
work is aimed at motivating the use of models which learn such relationships
from a population of phenomena, whose underlying physics are similar. The
development of such models is motivated by the way that physics-based models,
and more specifically finite element models, work. Such models are considered
transferrable, explainable and trustworthy, attributes which are not trivially
imposed or achieved for machine-learning models. For this reason,
machine-learning approaches are less trusted by industry and often considered
more difficult to form validated models. To achieve such data-driven models, a
population-based scheme is followed here and two different machine-learning
algorithms from the meta-learning domain are used. The two algorithms are the
model-agnostic meta-learning (MAML) algorithm and the conditional neural
processes (CNP) model. The algorithms seem to perform as intended and
outperform a traditional machine-learning algorithm at approximating the
quantities of interest. Moreover, they exhibit behaviour similar to traditional
machine learning algorithms (e.g. neural networks or Gaussian processes),
concerning their performance as a function of the available structures in the
training population.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09866">Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jinzhu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Liu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chen Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huandong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Hangyu Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1">Depeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a></p>
<p>Understanding and characterizing the vulnerability of urban infrastructures,
which refers to the engineering facilities essential for the regular running of
cities and that exist naturally in the form of networks, is of great value to
us. Potential applications include protecting fragile facilities and designing
robust topologies, etc. Due to the strong correlation between different
topological characteristics and infrastructure vulnerability and their
complicated evolution mechanisms, some heuristic and machine-assisted analysis
fall short in addressing such a scenario. In this paper, we model the
interdependent network as a heterogeneous graph and propose a system based on
graph neural network with reinforcement learning, which can be trained on
real-world data, to characterize the vulnerability of the city system
accurately. The presented system leverages deep learning techniques to
understand and analyze the heterogeneous graph, which enables us to capture the
risk of cascade failure and discover vulnerable infrastructures of cities.
Extensive experiments with various requests demonstrate not only the expressive
power of our system but also transferring ability and necessity of the specific
components.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09882">Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ben_Dov_O/0/1/0/all/0/1">Omri Ben-Dov</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Pravir Singh Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1">Victoria Abrevaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1">Michael J. Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1">Partha Ghosh</a></p>
<p>Generative Adversarial Networks (GANs) can produce high-quality samples, but
do not provide an estimate of the probability density around the samples.
However, it has been noted that maximizing the log-likelihood within an
energy-based setting can lead to an adversarial framework where the
discriminator provides unnormalized density (often called energy). We further
develop this perspective, incorporate importance sampling, and show that 1)
Wasserstein GAN performs a biased estimate of the partition function, and we
propose instead to use an unbiased estimator; 2) when optimizing for
likelihood, one must maximize generator entropy. This is hypothesized to
provide a better mode coverage. Different from previous works, we explicitly
compute the density of the generated samples. This is the key enabler to
designing an unbiased estimator of the partition function and computation of
the generator entropy term. The generator density is obtained via a new type of
flow network, called one-way flow network, that is less constrained in terms of
architecture, as it does not require to have a tractable inverse function. Our
experimental results show that we converge faster, produce comparable sample
quality to GANs with similar architecture, successfully avoid over-fitting to
commonly used datasets and produce smooth low-dimensional latent
representations of the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09883">Symmetric Equilibrium Learning of VAEs. (arXiv:2307.09883v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Flach_B/0/1/0/all/0/1">Boris Flach</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlesinger_D/0/1/0/all/0/1">Dmitrij Schlesinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Shekhovtsov_A/0/1/0/all/0/1">Alexander Shekhovtsov</a></p>
<p>We view variational autoencoders (VAE) as decoder-encoder pairs, which map
distributions in the data space to distributions in the latent space and vice
versa. The standard learning approach for VAEs, i.e. maximisation of the
evidence lower bound (ELBO), has an obvious asymmetry in that respect.
Moreover, it requires a closed form a-priori latent distribution. This limits
the applicability of VAEs in more complex scenarios, such as general
semi-supervised learning and employing complex generative models as priors. We
propose a Nash equilibrium learning approach that relaxes these restrictions
and allows learning VAEs in situations where both the data and the latent
distributions are accessible only by sampling. The flexibility and simplicity
of this approach allows its application to a wide range of learning scenarios
and downstream tasks. We show experimentally that the models learned by this
method are comparable to those obtained by ELBO learning and demonstrate its
applicability for tasks that are not accessible by standard VAE learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09896">Repeated Observations for Classification. (arXiv:2307.09896v1 [cs.IT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Afser_H/0/1/0/all/0/1">H&#xfc;seyin Af&#x15f;er</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyorfi_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; Gy&#xf6;rfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Walk_H/0/1/0/all/0/1">Harro Walk</a></p>
<p>We study the problem nonparametric classification with repeated observations.
Let $\bX$ be the $d$ dimensional feature vector and let $Y$ denote the label
taking values in $\{1,\dots ,M\}$. In contrast to usual setup with large sample
size $n$ and relatively low dimension $d$, this paper deals with the situation,
when instead of observing a single feature vector $\bX$ we are given $t$
repeated feature vectors $\bV_1,\dots ,\bV_t $. Some simple classification
rules are presented such that the conditional error probabilities have
exponential convergence rate of convergence as $t\to\infty$. In the analysis,
we investigate particular models like robust detection by nominal densities,
prototype classification, linear transformation, linear classification,
scaling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09912">Deep projection networks for learning time-homogeneous dynamical systems. (arXiv:2307.09912v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kostic_V/0/1/0/all/0/1">Vladimir R. Kostic</a>, <a href="http://arxiv.org/find/cs/1/au:+Novelli_P/0/1/0/all/0/1">Pietro Novelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Grazzi_R/0/1/0/all/0/1">Riccardo Grazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lounici_K/0/1/0/all/0/1">Karim Lounici</a>, <a href="http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1">Massimiliano Pontil</a></p>
<p>We consider the general class of time-homogeneous dynamical systems, both
discrete and continuous, and study the problem of learning a meaningful
representation of the state from observed data. This is instrumental for the
task of learning a forward transfer operator of the system, that in turn can be
used for forecasting future states or observables. The representation,
typically parametrized via a neural network, is associated with a projection
operator and is learned by optimizing an objective function akin to that of
canonical correlation analysis (CCA). However, unlike CCA, our objective avoids
matrix inversions and therefore is generally more stable and applicable to
challenging scenarios. Our objective is a tight relaxation of CCA and we
further enhance it by proposing two regularization schemes, one encouraging the
orthogonality of the components of the representation while the other
exploiting Chapman-Kolmogorov's equation. We apply our method to challenging
discrete dynamical systems, discussing improvements over previous methods, as
well as to continuous dynamical systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09916">TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jianing Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1">Qing Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yilin Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1">Wei Zeng</a></p>
<p>Deep learning (DL) approaches are being increasingly used for time-series
forecasting, with many efforts devoted to designing complex DL models. Recent
studies have shown that the DL success is often attributed to effective data
representations, fostering the fields of feature engineering and representation
learning. However, automated approaches for feature learning are typically
limited with respect to incorporating prior knowledge, identifying interactions
among variables, and choosing evaluation metrics to ensure that the models are
reliable. To improve on these limitations, this paper contributes a novel
visual analytics framework, namely TimeTuner, designed to help analysts
understand how model behaviors are associated with localized correlations,
stationarity, and granularity of time-series representations. The system mainly
consists of the following two-stage technique: We first leverage counterfactual
explanations to connect the relationships among time-series representations,
multivariate features and model predictions. Next, we design multiple
coordinated views including a partition-based correlation matrix and juxtaposed
bivariate stripes, and provide a set of interactions that allow users to step
into the transformation selection process, navigate through the feature space,
and reason the model performance. We instantiate TimeTuner with two
transformation methods of smoothing and sampling, and demonstrate its
applicability on real-world time-series forecasting of univariate sunspots and
multivariate air pollutants. Feedback from domain experts indicates that our
system can help characterize time-series representations and guide the feature
engineering processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09931">DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration. (arXiv:2307.09931v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ronchetti_M/0/1/0/all/0/1">Matteo Ronchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Wein_W/0/1/0/all/0/1">Wolfgang Wein</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettinig_O/0/1/0/all/0/1">Oliver Zettinig</a>, <a href="http://arxiv.org/find/cs/1/au:+Prevost_R/0/1/0/all/0/1">Raphael Prevost</a></p>
<p>Multimodal image registration is a challenging but essential step for
numerous image-guided procedures. Most registration algorithms rely on the
computation of complex, frequently non-differentiable similarity metrics to
deal with the appearance discrepancy of anatomical structures between imaging
modalities. Recent Machine Learning based approaches are limited to specific
anatomy-modality combinations and do not generalize to new settings. We propose
a generic framework for creating expressive cross-modal descriptors that enable
fast deformable global registration. We achieve this by approximating existing
metrics with a dot-product in the feature space of a small convolutional neural
network (CNN) which is inherently differentiable can be trained without
registered data. Our method is several orders of magnitude faster than local
patch-based metrics and can be directly applied in clinical settings by
replacing the similarity measure with the proposed one. Experiments on three
different datasets demonstrate that our approach generalizes well beyond the
training data, yielding a broad capture range even on unseen anatomies and
modality pairs, without the need for specialized retraining. We make our
training code and data publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09933">Spuriosity Didn&#x27;t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1">Cian Eastwood</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shashank Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1">Andrei Liviu Nicolicioiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1">Marin Vlastelica</a>, <a href="http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1">Julius von K&#xfc;gelgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a></p>
<p>To avoid failures on out-of-distribution data, recent works have sought to
extract features that have a stable or invariant relationship with the label
across domains, discarding the "spurious" or unstable features whose
relationship with the label changes across domains. However, unstable features
often carry complementary information about the label that could boost
performance if used correctly in the test domain. Our main contribution is to
show that it is possible to learn how to use these unstable features in the
test domain without labels. In particular, we prove that pseudo-labels based on
stable features provide sufficient guidance for doing so, provided that stable
and unstable features are conditionally independent given the label. Based on
this theoretical insight, we propose Stable Feature Boosting (SFB), an
algorithm for: (i) learning a predictor that separates stable and
conditionally-independent unstable features; and (ii) using the stable-feature
predictions to adapt the unstable-feature predictions in the test domain.
Theoretically, we prove that SFB can learn an asymptotically-optimal predictor
without test-domain labels. Empirically, we demonstrate the effectiveness of
SFB on real and synthetic data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09942">TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network. (arXiv:2307.09942v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1">Brandon Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Cao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jimeng Sun</a></p>
<p>Clinical trials are critical for drug development but often suffer from
expensive and inefficient patient recruitment. In recent years, machine
learning models have been proposed for speeding up patient recruitment via
automatically matching patients with clinical trials based on longitudinal
patient electronic health records (EHR) data and eligibility criteria of
clinical trials. However, they either depend on trial-specific expert rules
that cannot expand to other trials or perform matching at a very general level
with a black-box model where the lack of interpretability makes the model
results difficult to be adopted.
</p>
<p>To provide accurate and interpretable patient trial matching, we introduce a
personalized dynamic tree-based memory network model named TREEMENT. It
utilizes hierarchical clinical ontologies to expand the personalized patient
representation learned from sequential EHR data, and then uses an attentional
beam-search query learned from eligibility criteria embedding to offer a
granular level of alignment for improved performance and interpretability. We
evaluated TREEMENT against existing models on real-world datasets and
demonstrated that TREEMENT outperforms the best baseline by 7% in terms of
error reduction in criteria-level matching and achieves state-of-the-art
results in its trial-level matching ability. Furthermore, we also show TREEMENT
can offer good interpretability to make the model results easier for adoption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09943">Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1">Thomas McDonald</a>, <a href="http://arxiv.org/find/cs/1/au:+Maystre_L/0/1/0/all/0/1">Lucas Maystre</a>, <a href="http://arxiv.org/find/cs/1/au:+Lalmas_M/0/1/0/all/0/1">Mounia Lalmas</a>, <a href="http://arxiv.org/find/cs/1/au:+Russo_D/0/1/0/all/0/1">Daniel Russo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciosek_K/0/1/0/all/0/1">Kamil Ciosek</a></p>
<p>Recommender systems are a ubiquitous feature of online platforms.
Increasingly, they are explicitly tasked with increasing users' long-term
satisfaction. In this context, we study a content exploration task, which we
formalize as a multi-armed bandit problem with delayed rewards. We observe that
there is an apparent trade-off in choosing the learning signal: Waiting for the
full reward to become available might take several weeks, hurting the rate at
which learning happens, whereas measuring short-term proxy rewards reflects the
actual long-term goal only imperfectly. We address this challenge in two steps.
First, we develop a predictive model of delayed rewards that incorporates all
information obtained to date. Full observations as well as partial (short or
medium-term) outcomes are combined through a Bayesian filter to obtain a
probabilistic belief. Second, we devise a bandit algorithm that takes advantage
of this new predictive model. The algorithm quickly learns to identify content
aligned with long-term success by carefully balancing exploration and
exploitation. We apply our approach to a podcast recommendation problem, where
we seek to identify shows that users engage with repeatedly over two months. We
empirically validate that our approach results in substantially better
performance compared to approaches that either optimize for short-term proxies,
or wait for the long-term outcome to be fully realized.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09955">XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mengda Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhenjia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1">Cheng Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1">Manuela Veloso</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shuran Song</a></p>
<p>Human demonstration videos are a widely available data source for robot
learning and an intuitive user interface for expressing desired behavior.
However, directly extracting reusable robot manipulation skills from
unstructured human videos is challenging due to the big embodiment difference
and unobserved action parameters. To bridge this embodiment gap, this paper
introduces XSkill, an imitation learning framework that 1) discovers a
cross-embodiment representation called skill prototypes purely from unlabeled
human and robot manipulation videos, 2) transfers the skill representation to
robot actions using conditional diffusion policy, and finally, 3) composes the
learned skill to accomplish unseen tasks specified by a human prompt video. Our
experiments in simulation and real-world environments show that the discovered
skill prototypes facilitate both skill transfer and composition for unseen
tasks, resulting in a more general and scalable imitation learning framework.
The performance of XSkill is best understood from the anonymous website:
https://xskillcorl.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09964">Towards green AI-based software systems: an architecture-centric approach (GAISSA). (arXiv:2307.09964v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1">Xavier Franch</a>, <a href="http://arxiv.org/find/cs/1/au:+Duran_F/0/1/0/all/0/1">Francisco Dur&#xe1;n</a></p>
<p>Nowadays, AI-based systems have achieved outstanding results and have
outperformed humans in different domains. However, the processes of training AI
models and inferring from them require high computational resources, which pose
a significant challenge in the current energy efficiency societal demand. To
cope with this challenge, this research project paper describes the main
vision, goals, and expected outcomes of the GAISSA project. The GAISSA project
aims at providing data scientists and software engineers tool-supported,
architecture-centric methods for the modelling and development of green
AI-based systems. Although the project is in an initial stage, we describe the
current research results, which illustrate the potential to achieve GAISSA
objectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09977">Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks. (arXiv:2307.09977v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yulan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1">Ziqiang Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yue Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1">Wei Xiang</a></p>
<p>The paradigm of federated learning (FL) to address data privacy concerns by
locally training parameters on resource-constrained clients in a distributed
manner has garnered significant attention. Nonetheless, FL is not applicable
when not all clients within the coverage of the FL server are registered with
the FL network. To bridge this gap, this paper proposes joint learner referral
aided federated client selection (LRef-FedCS), along with communications and
computing resource scheduling, and local model accuracy optimization (LMAO)
methods. These methods are designed to minimize the cost incurred by the
worst-case participant and ensure the long-term fairness of FL in hierarchical
Internet of Things (HieIoT) networks. Utilizing the Lyapunov optimization
technique, we reformulate the original problem into a stepwise joint
optimization problem (JOP). Subsequently, to tackle the mixed-integer
non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO
through the centralized method and self-adaptive global best harmony search
(SGHS) algorithm, respectively. To enhance scalability, we further propose a
distributed LRef-FedCS approach based on a matching game to replace the
centralized method described above. Numerical simulations and experimental
results on the MNIST/CIFAR-10 datasets demonstrate that our proposed LRef-FedCS
approach could achieve a good balance between pursuing high global accuracy and
reducing cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09988">TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1">Young D. Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1">Stylianos I. Venieris</a>, <a href="http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1">Jagmohan Chauhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1">Nicholas D. Lane</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1">Cecilia Mascolo</a></p>
<p>On-device training is essential for user personalisation and privacy. With
the pervasiveness of IoT devices and microcontroller units (MCU), this task
becomes more challenging due to the constrained memory and compute resources,
and the limited availability of labelled user data. Nonetheless, prior works
neglect the data scarcity issue, require excessively long training time (e.g. a
few hours), or induce substantial accuracy loss ($\geq$10\%). We propose
TinyTrain, an on-device training approach that drastically reduces training
time by selectively updating parts of the model and explicitly coping with data
scarcity. TinyTrain introduces a task-adaptive sparse-update method that
dynamically selects the layer/channel based on a multi-objective criterion that
jointly captures user data, the memory, and the compute capabilities of the
target device, leading to high accuracy on unseen tasks with reduced
computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of
the entire network by 3.6-5.0\% in accuracy, while reducing the backward-pass
memory and computation cost by up to 2,286$\times$ and 7.68$\times$,
respectively. Targeting broadly used real-world edge devices, TinyTrain
achieves 9.5$\times$ faster and 3.5$\times$ more energy-efficient training over
status-quo approaches, and 2.8$\times$ smaller memory footprint than SOTA
approaches, while remaining within the 1 MB memory envelope of MCU-grade
platforms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09989">UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing. (arXiv:2307.09989v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qifang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1">Meng Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qinghui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huan Xu</a></p>
<p>When doing private domain marketing with cloud services, the merchants
usually have to purchase different machine learning models for the multiple
marketing purposes, leading to a very high cost. We present a unified user-item
matching framework to simultaneously conduct item recommendation and user
targeting with just one model. We empirically demonstrate that the above
concurrent modeling is viable via modeling the user-item interaction matrix
with the multinomial distribution, and propose a bidirectional bias-corrected
NCE loss for the implementation. The proposed loss function guides the model to
learn the user-item joint probability $p(u,i)$ instead of the conditional
probability $p(i|u)$ or $p(u|i)$ through correcting both the users and items'
biases caused by the in-batch negative sampling. In addition, our framework is
model-agnostic enabling a flexible adaptation of different model architectures.
Extensive experiments demonstrate that our framework results in significant
performance gains in comparison with the state-of-the-art methods, with greatly
reduced cost on computing resources and daily maintenance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09994">Impact of Disentanglement on Pruning Neural Networks. (arXiv:2307.09994v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shneider_C/0/1/0/all/0/1">Carl Shneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_P/0/1/0/all/0/1">Peyman Rostami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kacem_A/0/1/0/all/0/1">Anis Kacem</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1">Nilotpal Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Shabayek_A/0/1/0/all/0/1">Abd El Rahman Shabayek</a>, <a href="http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1">Djamila Aouada</a></p>
<p>Deploying deep learning neural networks on edge devices, to accomplish task
specific objectives in the real-world, requires a reduction in their memory
footprint, power consumption, and latency. This can be realized via efficient
model compression. Disentangled latent representations produced by variational
autoencoder (VAE) networks are a promising approach for achieving model
compression because they mainly retain task-specific information, discarding
useless information for the task at hand. We make use of the Beta-VAE framework
combined with a standard criterion for pruning to investigate the impact of
forcing the network to learn disentangled representations on the pruning
process for the task of classification. In particular, we perform experiments
on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose
a path forward for future works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10003">TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aminimehr_A/0/1/0/all/0/1">Amirhossein Aminimehr</a>, <a href="http://arxiv.org/find/cs/1/au:+Khani_P/0/1/0/all/0/1">Pouya Khani</a>, <a href="http://arxiv.org/find/cs/1/au:+Molaei_A/0/1/0/all/0/1">Amirali Molaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Kazemeini_A/0/1/0/all/0/1">Amirmohammad Kazemeini</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a></p>
<p>The field of Explainable Artificial Intelligence (XAI) aims to improve the
interpretability of black-box machine learning models. Building a heatmap based
on the importance value of input features is a popular method for explaining
the underlying functions of such models in producing their predictions.
Heatmaps are almost understandable to humans, yet they are not without flaws.
Non-expert users, for example, may not fully understand the logic of heatmaps
(the logic in which relevant pixels to the model's prediction are highlighted
with different intensities or colors). Additionally, objects and regions of the
input image that are relevant to the model prediction are frequently not
entirely differentiated by heatmaps. In this paper, we propose a framework
called TbExplain that employs XAI techniques and a pre-trained object detector
to present text-based explanations of scene classification models. Moreover,
TbExplain incorporates a novel method to correct predictions and textually
explain them based on the statistics of objects in the input image when the
initial prediction is unreliable. To assess the trustworthiness and validity of
the text-based explanations, we conducted a qualitative experiment, and the
findings indicated that these explanations are sufficiently reliable.
Furthermore, our quantitative and qualitative experiments on TbExplain with
scene classification datasets reveal an improvement in classification accuracy
over ResNet variants.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10022">Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK. (arXiv:2307.10022v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pitas_K/0/1/0/all/0/1">Konstantinos Pitas</a></p>
<p>I propose an open dataset of country-level historical opinion polling data
for the European Union and the UK. The dataset aims to fill a gap in available
opinion polling data for the European Union. Some existing datasets are
restricted to the past five years, limiting research opportunities. At the same
time, some larger proprietary datasets exist but are available only in a visual
preprocessed time series format. Finally, while other large datasets for
individual countries might exist, these could be inaccessible due to language
barriers. The data was gathered from Wikipedia, and preprocessed using the
pandas library. Both the raw and the preprocessed data are in the .csv format.
I hope that given the recent advances in LLMs and deep learning in general,
this large dataset will enable researchers to uncover complex interactions
between multimodal data (news articles, economic indicators, social media) and
voting behavior. The raw data, the preprocessed data, and the preprocessing
scripts are available on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10026">Contextual Reliability: When Different Features Matter in Different Contexts. (arXiv:2307.10026v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosal_G/0/1/0/all/0/1">Gaurav Ghosal</a>, <a href="http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1">Amrith Setlur</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Daniel S. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca D. Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1">Aditi Raghunathan</a></p>
<p>Deep neural networks often fail catastrophically by relying on spurious
correlations. Most prior work assumes a clear dichotomy into spurious and
reliable features; however, this is often unrealistic. For example, most of the
time we do not want an autonomous car to simply copy the speed of surrounding
cars -- we don't want our car to run a red light if a neighboring car does so.
However, we cannot simply enforce invariance to next-lane speed, since it could
provide valuable information about an unobservable pedestrian at a crosswalk.
Thus, universally ignoring features that are sometimes (but not always)
reliable can lead to non-robust performance. We formalize a new setting called
contextual reliability which accounts for the fact that the "right" features to
use may vary depending on the context. We propose and analyze a two-stage
framework called Explicit Non-spurious feature Prediction (ENP) which first
identifies the relevant features to use for a given context, then trains a
model to rely exclusively on these features. Our work theoretically and
empirically demonstrates the advantages of ENP over existing methods and
provides new benchmarks for contextual reliability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10053">Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Xiao_N/0/1/0/all/0/1">Nachuan Xiao</a>, <a href="http://arxiv.org/find/math/1/au:+Hu_X/0/1/0/all/0/1">Xiaoyin Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Toh_K/0/1/0/all/0/1">Kim-Chuan Toh</a></p>
<p>In this paper, we investigate the convergence properties of the stochastic
gradient descent (SGD) method and its variants, especially in training neural
networks built from nonsmooth activation functions. We develop a novel
framework that assigns different timescales to stepsizes for updating the
momentum terms and variables, respectively. Under mild conditions, we prove the
global convergence of our proposed framework in both single-timescale and
two-timescale cases. We show that our proposed framework encompasses a wide
range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion,
normalized SGD and clipped SGD. Furthermore, when the objective function adopts
a finite-sum formulation, we prove the convergence properties for these
SGD-type methods based on our proposed framework. In particular, we prove that
these SGD-type methods find the Clarke stationary points of the objective
function with randomly chosen stepsizes and initial points under mild
assumptions. Preliminary numerical experiments demonstrate the high efficiency
of our analyzed SGD-type methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10060">Accurate deep learning sub-grid scale models for large eddy simulations. (arXiv:2307.10060v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Bose_R/0/1/0/all/0/1">Rikhi Bose</a>, <a href="http://arxiv.org/find/physics/1/au:+Roy_A/0/1/0/all/0/1">Arunabha M. Roy</a></p>
<p>We present two families of sub-grid scale (SGS) turbulence models developed
for large-eddy simulation (LES) purposes. Their development required the
formulation of physics-informed robust and efficient Deep Learning (DL)
algorithms which, unlike state-of-the-art analytical modeling techniques can
produce high-order complex non-linear relations between inputs and outputs.
Explicit filtering of data from direct simulations of the canonical channel
flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided
accurate data for training and testing. The two sets of models use different
network architectures. One of the architectures uses tensor basis neural
networks (TBNN) and embeds the simplified analytical model form of the general
effective-viscosity hypothesis, thus incorporating the Galilean, rotational and
reflectional invariances. The other architecture is that of a relatively simple
network, that is able to incorporate the Galilean invariance only. However,
this simpler architecture has better feature extraction capacity owing to its
ability to establish relations between and extract information from
cross-components of the integrity basis tensors and the SGS stresses. Both sets
of models are used to predict the SGS stresses for feature datasets generated
with different filter widths, and at different Reynolds numbers. It is shown
that due to the simpler model's better feature learning capabilities, it
outperforms the invariance embedded model in statistical performance metrics.
In a priori tests, both sets of models provide similar levels of dissipation
and backscatter. Based on the test results, both sets of models should be
usable in a posteriori actual LESs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10062">Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples. (arXiv:2307.10062v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">JoonHo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1">Jae Oh Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1">Hankyu Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kwonho Lee</a></p>
<p>Deploying deep visual models can lead to performance drops due to the
discrepancies between source and target distributions. Several approaches
leverage labeled source data to estimate target domain accuracy, but accessing
labeled source data is often prohibitively difficult due to data
confidentiality or resource limitations on serving devices. Our work proposes a
new framework to estimate model accuracy on unlabeled target data without
access to source data. We investigate the feasibility of using pseudo-labels
for accuracy estimation and evolve this idea into adopting recent advances in
source-free domain adaptation algorithms. Our approach measures the
disagreement rate between the source hypothesis and the target pseudo-labeling
function, adapted from the source hypothesis. We mitigate the impact of
erroneous pseudo-labels that may arise due to a high ideal joint hypothesis
risk by employing adaptive adversarial perturbation on the input of the target
model. Our proposed source-free framework effectively addresses the challenging
distribution shift scenarios and outperforms existing methods requiring source
data and labels for training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10073">Scalable Deep Learning for RNA Secondary Structure Prediction. (arXiv:2307.10073v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franke_J/0/1/0/all/0/1">J&#xf6;rg K.H. Franke</a>, <a href="http://arxiv.org/find/cs/1/au:+Runge_F/0/1/0/all/0/1">Frederic Runge</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a></p>
<p>The field of RNA secondary structure prediction has made significant progress
with the adoption of deep learning techniques. In this work, we present the
RNAformer, a lean deep learning model using axial attention and recycling in
the latent space. We gain performance improvements by designing the
architecture for modeling the adjacency matrix directly in the latent space and
by scaling the size of the model. Our approach achieves state-of-the-art
performance on the popular TS0 benchmark dataset and even outperforms methods
that use external information. Further, we show experimentally that the
RNAformer can learn a biophysical model of the RNA folding process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10078">A Dual Formulation for Probabilistic Principal Component Analysis. (arXiv:2307.10078v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Plaen_H/0/1/0/all/0/1">Henri De Plaen</a>, <a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1">Johan A. K. Suykens</a></p>
<p>In this paper, we characterize Probabilistic Principal Component Analysis in
Hilbert spaces and demonstrate how the optimal solution admits a representation
in dual space. This allows us to develop a generative framework for kernel
methods. Furthermore, we show how it englobes Kernel Principal Component
Analysis and illustrate its working on a toy and a real dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10088">Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rawles_C/0/1/0/all/0/1">Christopher Rawles</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Alice Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_D/0/1/0/all/0/1">Daniel Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Riva_O/0/1/0/all/0/1">Oriana Riva</a>, <a href="http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1">Timothy Lillicrap</a></p>
<p>There is a growing interest in device-control systems that can interpret
human natural language instructions and execute them on a digital device by
directly controlling its user interface. We present a dataset for
device-control research, Android in the Wild (AITW), which is orders of
magnitude larger than current datasets. The dataset contains human
demonstrations of device interactions, including the screens and actions, and
corresponding natural language instructions. It consists of 715k episodes
spanning 30k unique instructions, four versions of Android (v10-13),and eight
device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It
contains multi-step tasks that require semantic understanding of language and
visual context. This dataset poses a new challenge: actions available through
the user interface must be inferred from their visual appearance. And, instead
of simple UI element-based actions, the action space consists of precise
gestures (e.g., horizontal scrolls to operate carousel widgets). We organize
our dataset to encourage robustness analysis of device-control systems, i.e.,
how well a system performs in the presence of new task descriptions, new
applications, or new platform versions. We develop two agents and report
performance across the dataset. The dataset is available at
https://github.com/google-research/google-research/tree/master/android_in_the_wild.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10093">Revisiting invariances and introducing priors in Gromov-Wasserstein distances. (arXiv:2307.10093v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Demetci_P/0/1/0/all/0/1">Pinar Demetci</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1">Quang Huy Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Redko_I/0/1/0/all/0/1">Ievgen Redko</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Ritambhara Singh</a></p>
<p>Gromov-Wasserstein distance has found many applications in machine learning
due to its ability to compare measures across metric spaces and its invariance
to isometric transformations. However, in certain applications, this invariance
property can be too flexible, thus undesirable. Moreover, the
Gromov-Wasserstein distance solely considers pairwise sample similarities in
input datasets, disregarding the raw feature representations. We propose a new
optimal transport-based distance, called Augmented Gromov-Wasserstein, that
allows for some control over the level of rigidity to transformations. It also
incorporates feature alignments, enabling us to better leverage prior knowledge
on the input data for improved performance. We present theoretical insights
into the proposed metric. We then demonstrate its usefulness for single-cell
multi-omic alignment tasks and a transfer learning scenario in machine
learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10098">Gradient Sparsification For Masked Fine-Tuning of Transformers. (arXiv:2307.10098v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Neill_J/0/1/0/all/0/1">James O&#x27; Neill</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1">Sourav Dutta</a></p>
<p>Fine-tuning pretrained self-supervised language models is widely adopted for
transfer learning to downstream tasks. Fine-tuning can be achieved by freezing
gradients of the pretrained network and only updating gradients of a newly
added classification layer, or by performing gradient updates on all
parameters. Gradual unfreezing makes a trade-off between the two by gradually
unfreezing gradients of whole layers during training. This has been an
effective strategy to trade-off between storage and training speed with
generalization performance. However, it is not clear whether gradually
unfreezing layers throughout training is optimal, compared to sparse variants
of gradual unfreezing which may improve fine-tuning performance. In this paper,
we propose to stochastically mask gradients to regularize pretrained language
models for improving overall fine-tuned performance. We introduce GradDrop and
variants thereof, a class of gradient sparsification methods that mask
gradients during the backward pass, acting as gradient noise. GradDrop is
sparse and stochastic unlike gradual freezing. Extensive experiments on the
multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive
against methods that use additional translated data for intermediate
pretraining and outperforms standard fine-tuning and gradual unfreezing. A
post-analysis shows how GradDrop improves performance with languages it was not
trained on, such as under-resourced languages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10112">Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mueller_T/0/1/0/all/0/1">Tamara T. Mueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Starck_S/0/1/0/all/0/1">Sophie Starck</a>, <a href="http://arxiv.org/find/cs/1/au:+Feiner_L/0/1/0/all/0/1">Leonhard F. Feiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1">Kyriaki-Margarita Bintsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1">Georgios Kaissis</a></p>
<p>When re-structuring patient cohorts into so-called population graphs,
initially independent data points can be incorporated into one interconnected
graph structure. This population graph can then be used for medical downstream
tasks using graph neural networks (GNNs). The construction of a suitable graph
structure is a challenging step in the learning pipeline that can have severe
impact on model performance. To this end, different graph assessment metrics
have been introduced to evaluate graph structures. However, these metrics are
limited to classification tasks and discrete adjacency matrices, only covering
a small subset of real-world applications. In this work, we introduce extended
graph assessment metrics (GAMs) for regression tasks and continuous adjacency
matrices. We focus on two GAMs in specific: \textit{homophily} and
\textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of
GAMs to more than one hop, define homophily for regression tasks, as well as
continuous adjacency matrices, and propose a light-weight CCNS distance for
discrete and continuous adjacency matrices. We show the correlation of these
metrics with model performance on different medical population graphs and under
different learning settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10120">Quarl: A Learning-Based Quantum Circuit Optimizer. (arXiv:2307.10120v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1">Zikun Li</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Peng_J/0/1/0/all/0/1">Jinjun Peng</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Mei_Y/0/1/0/all/0/1">Yixuan Mei</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Lin_S/0/1/0/all/0/1">Sina Lin</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Padon_O/0/1/0/all/0/1">Oded Padon</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Jia_Z/0/1/0/all/0/1">Zhihao Jia</a></p>
<p>Optimizing quantum circuits is challenging due to the very large search space
of functionally equivalent circuits and the necessity of applying
transformations that temporarily decrease performance to achieve a final
performance improvement. This paper presents Quarl, a learning-based quantum
circuit optimizer. Applying reinforcement learning (RL) to quantum circuit
optimization raises two main challenges: the large and varying action space and
the non-uniform state representation. Quarl addresses these issues with a novel
neural architecture and RL-training procedure. Our neural architecture
decomposes the action space into two parts and leverages graph neural networks
in its state representation, both of which are guided by the intuition that
optimization decisions can be mostly guided by local reasoning while allowing
global circuit-wide reasoning. Our evaluation shows that Quarl significantly
outperforms existing circuit optimizers on almost all benchmark circuits.
Surprisingly, Quarl can learn to perform rotation merging, a complex, non-local
circuit optimization implemented as a separate pass in existing optimizers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10142">Benchmarking Potential Based Rewards for Learning Humanoid Locomotion. (arXiv:2307.10142v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1">Se Hwan Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Heim_S/0/1/0/all/0/1">Steve Heim</a>, <a href="http://arxiv.org/find/cs/1/au:+Khazoom_C/0/1/0/all/0/1">Charles Khazoom</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangbae Kim</a></p>
<p>The main challenge in developing effective reinforcement learning (RL)
pipelines is often the design and tuning the reward functions. Well-designed
shaping reward can lead to significantly faster learning. Naively formulated
rewards, however, can conflict with the desired behavior and result in
overfitting or even erratic performance if not properly tuned. In theory, the
broad class of potential based reward shaping (PBRS) can help guide the
learning process without affecting the optimal policy. Although several studies
have explored the use of potential based reward shaping to accelerate learning
convergence, most have been limited to grid-worlds and low-dimensional systems,
and RL in robotics has predominantly relied on standard forms of reward
shaping. In this paper, we benchmark standard forms of shaping with PBRS for a
humanoid robot. We find that in this high-dimensional system, PBRS has only
marginal benefits in convergence speed. However, the PBRS reward terms are
significantly more robust to scaling than typical reward shaping approaches,
and thus easier to tune.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10155">Curvature-based Clustering on Graphs. (arXiv:2307.10155v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yu Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lubberts_Z/0/1/0/all/0/1">Zachary Lubberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1">Melanie Weber</a></p>
<p>Unsupervised node clustering (or community detection) is a classical graph
learning task. In this paper, we study algorithms, which exploit the geometry
of the graph to identify densely connected substructures, which form clusters
or communities. Our method implements discrete Ricci curvatures and their
associated geometric flows, under which the edge weights of the graph evolve to
reveal its community structure. We consider several discrete curvature notions
and analyze the utility of the resulting algorithms. In contrast to prior
literature, we study not only single-membership community detection, where each
node belongs to exactly one community, but also mixed-membership community
detection, where communities may overlap. For the latter, we argue that it is
beneficial to perform community detection on the line graph, i.e., the graph's
dual. We provide both theoretical and empirical evidence for the utility of our
curvature-based clustering algorithms. In addition, we give several results on
the relationship between the curvature of a graph and that of its dual, which
enable the efficient implementation of our proposed mixed-membership community
detection approach and which may be of independent interest for curvature-based
network analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10160">Robust Driving Policy Learning with Guided Meta Reinforcement Learning. (arXiv:2307.10160v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kanghoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Isele_D/0/1/0/all/0/1">David Isele</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinkyoo Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujimura_K/0/1/0/all/0/1">Kikuo Fujimura</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a></p>
<p>Although deep reinforcement learning (DRL) has shown promising results for
autonomous navigation in interactive traffic scenarios, existing work typically
adopts a fixed behavior policy to control social vehicles in the training
environment. This may cause the learned driving policy to overfit the
environment, making it difficult to interact well with vehicles with different,
unseen behaviors. In this work, we introduce an efficient method to train
diverse driving policies for social vehicles as a single meta-policy. By
randomizing the interaction-based reward functions of social vehicles, we can
generate diverse objectives and efficiently train the meta-policy through
guiding policies that achieve specific objectives. We further propose a
training strategy to enhance the robustness of the ego vehicle's driving policy
using the environment where social vehicles are controlled by the learned
meta-policy. Our method successfully learns an ego driving policy that
generalizes well to unseen situations with out-of-distribution (OOD) social
agents' behaviors in a challenging uncontrolled T-intersection scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10163">Rethinking Backdoor Attacks. (arXiv:2307.10163v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khaddaj_A/0/1/0/all/0/1">Alaa Khaddaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Leclerc_G/0/1/0/all/0/1">Guillaume Leclerc</a>, <a href="http://arxiv.org/find/cs/1/au:+Makelov_A/0/1/0/all/0/1">Aleksandar Makelov</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgiev_K/0/1/0/all/0/1">Kristian Georgiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1">Hadi Salman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilyas_A/0/1/0/all/0/1">Andrew Ilyas</a>, <a href="http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1">Aleksander Madry</a></p>
<p>In a backdoor attack, an adversary inserts maliciously constructed backdoor
examples into a training set to make the resulting model vulnerable to
manipulation. Defending against such attacks typically involves viewing these
inserted examples as outliers in the training set and using techniques from
robust statistics to detect and remove them.
</p>
<p>In this work, we present a different approach to the backdoor attack problem.
Specifically, we show that without structural information about the training
data distribution, backdoor attacks are indistinguishable from
naturally-occurring features in the data--and thus impossible to "detect" in a
general sense. Then, guided by this observation, we revisit existing defenses
against backdoor attacks and characterize the (often latent) assumptions they
make and on which they depend. Finally, we explore an alternative perspective
on backdoor attacks: one that assumes these attacks correspond to the strongest
feature in the training data. Under this assumption (which we make formal) we
develop a new primitive for detecting backdoor attacks. Our primitive naturally
gives rise to a detection algorithm that comes with theoretical guarantees and
is effective in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10167">VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Clavier_P/0/1/0/all/0/1">Pierre Clavier</a>, <a href="http://arxiv.org/find/stat/1/au:+Huix_T/0/1/0/all/0/1">Tom Huix</a>, <a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1">Alain Durmus</a></p>
<p>In this paper, we introduce and analyze a variant of the Thompson sampling
(TS) algorithm for contextual bandits. At each round, traditional TS requires
samples from the current posterior distribution, which is usually intractable.
To circumvent this issue, approximate inference techniques can be used and
provide samples with distribution close to the posteriors. However, current
approximate techniques yield to either poor estimation (Laplace approximation)
or can be computationally expensive (MCMC methods, Ensemble sampling...). In
this paper, we propose a new algorithm, Varational Inference Thompson sampling
VITS, based on Gaussian Variational Inference. This scheme provides powerful
posterior approximations which are easy to sample from, and is computationally
efficient, making it an ideal choice for TS. In addition, we show that VITS
achieves a sub-linear regret bound of the same order in the dimension and
number of round as traditional TS for linear contextual bandit. Finally, we
demonstrate experimentally the effectiveness of VITS on both synthetic and real
world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10169">Challenges and Applications of Large Language Models. (arXiv:2307.10169v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_J/0/1/0/all/0/1">Joshua Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1">Maximilian Mozes</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1">Herbie Bradley</a>, <a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1">Roberta Raileanu</a>, <a href="http://arxiv.org/find/cs/1/au:+McHardy_R/0/1/0/all/0/1">Robert McHardy</a></p>
<p>Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10171">LightPath: Lightweight and Scalable Path Representation Learning. (arXiv:2307.10171v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sean Bin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jilin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chenjuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1">Christian S. Jensen</a></p>
<p>Movement paths are used widely in intelligent transportation and smart city
applications. To serve such applications, path representation learning aims to
provide compact representations of paths that enable efficient and accurate
operations when used for different downstream tasks such as path ranking and
travel cost estimation. In many cases, it is attractive that the path
representation learning is lightweight and scalable; in resource-limited
environments and under green computing limitations, it is essential. Yet,
existing path representation learning studies focus on accuracy and pay at most
secondary attention to resource consumption and scalability.
</p>
<p>We propose a lightweight and scalable path representation learning framework,
termed LightPath, that aims to reduce resource consumption and achieve
scalability without affecting accuracy, thus enabling broader applicability.
More specifically, we first propose a sparse auto-encoder that ensures that the
framework achieves good scalability with respect to path length. Next, we
propose a relational reasoning framework to enable faster training of more
robust sparse path encoders. We also propose global-local knowledge
distillation to further reduce the size and improve the performance of sparse
path encoders. Finally, we report extensive experiments on two real-world
datasets to offer insight into the efficiency, scalability, and effectiveness
of the proposed framework.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1912.13122">Declarative Mechanism Design. (arXiv:1912.13122v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Garcia_Camino_A/0/1/0/all/0/1">Andr&#xe9;s Garc&#xed;a-Camino</a></p>
<p>Regulation of Multi-Agent Systems (MAS) and Declarative Electronic
Institutions (DEIs) was a multidisciplinary research topic of the past decade
involving (Physical and Software) Agents and Law since the beginning, but
recently evolved towards News-claimed Robot Lawyer since 2016. One of these
first proposals of restricting the behaviour of Software Agentswas Electronic
Institutions.However, with the recent reformulation of Artificial Neural
Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal
issues regarding the use of DL has raised concerns in the Artificial
Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly
addressed, we propose the Regulation of Artificial Neural Networks as
Agent-based Training of a special type of regulated Artificial Neural Network
that we call Institutional Neural Network (INN).The main purpose of this paper
is to bring attention to Artificial Teaching (AT) and to give a tentative
answer showing a proof-of-concept implementation of Regulated Deep Learning
(RDL). This paper introduces the former concept and provide sI, a language
previously used to model declaratively and extend Electronic Institutions, as a
means to regulate the execution of Artificial Neural Networks and their
interactions with Artificial Teachers (ATs)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2001.05887">MixPath: A Unified Approach for One-shot Neural Architecture Search. (arXiv:2001.05887v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shun Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xudong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a></p>
<p>Blending multiple convolutional kernels is proved advantageous in neural
architecture design. However, current two-stage neural architecture search
methods are mainly limited to single-path search spaces. How to efficiently
search models of multi-path structures remains a difficult problem. In this
paper, we are motivated to train a one-shot multi-path supernet to accurately
evaluate the candidate architectures. Specifically, we discover that in the
studied search spaces, feature vectors summed from multiple paths are nearly
multiples of those from a single path. Such disparity perturbs the supernet
training and its ranking ability. Therefore, we propose a novel mechanism
called Shadow Batch Normalization (SBN) to regularize the disparate feature
statistics. Extensive experiments prove that SBNs are capable of stabilizing
the optimization and improving ranking performance. We call our unified
multi-path one-shot approach as MixPath, which generates a series of models
that achieve state-of-the-art results on ImageNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.03328">Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis. (arXiv:2103.03328v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1">Aleksandar Vakanski</a>, <a href="http://arxiv.org/find/cs/1/au:+Xian_M/0/1/0/all/0/1">Min Xian</a></p>
<p>The generalization performance of deep learning models for medical image
analysis often decreases on images collected with different devices for data
acquisition, device settings, or patient population. A better understanding of
the generalization capacity on new images is crucial for clinicians'
trustworthiness in deep learning. Although significant research efforts have
been recently directed toward establishing generalization bounds and complexity
measures, still, there is often a significant discrepancy between the predicted
and actual generalization performance. As well, related large empirical studies
have been primarily based on validation with general-purpose image datasets.
This paper presents an empirical study that investigates the correlation
between 25 complexity measures and the generalization abilities of supervised
deep learning classifiers for breast ultrasound images. The results indicate
that PAC-Bayes flatness-based and path norm-based measures produce the most
consistent explanation for the combination of models and data. We also
investigate the use of multi-task classification and segmentation approach for
breast images, and report that such learning approach acts as an implicit
regularizer and is conducive toward improved generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.15965">Strong Optimal Classification Trees. (arXiv:2103.15965v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aghaei_S/0/1/0/all/0/1">Sina Aghaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1">Andr&#xe9;s G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vayanos_P/0/1/0/all/0/1">Phebe Vayanos</a></p>
<p>Decision trees are among the most popular machine learning models and are
used routinely in applications ranging from revenue management and medicine to
bioinformatics. In this paper, we consider the problem of learning optimal
binary classification trees with univariate splits. Literature on the topic has
burgeoned in recent years, motivated both by the empirical suboptimality of
heuristic approaches and the tremendous improvements in mixed-integer
optimization (MIO) technology. Yet, existing MIO-based approaches from the
literature do not leverage the power of MIO to its full extent: they rely on
weak formulations, resulting in slow convergence and large optimality gaps. To
fill this gap in the literature, we propose an intuitive flow-based MIO
formulation for learning optimal binary classification trees. Our formulation
can accommodate side constraints to enable the design of interpretable and fair
decision trees. Moreover, we show that our formulation has a stronger linear
optimization relaxation than existing methods in the case of binary data. We
exploit the decomposable structure of our formulation and max-flow/min-cut
duality to derive a Benders' decomposition method to speed-up computation. We
propose a tailored procedure for solving each decomposed subproblem that
provably generates facets of the feasible set of the MIO as constraints to add
to the main problem. We conduct extensive computational experiments on standard
benchmark datasets on which we show that our proposed approaches are 29 times
faster than state-of-the-art MIO-based techniques and improve out-of-sample
performance by up to 8%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.07677">Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1">Christine Herlihy</a>, <a href="http://arxiv.org/find/cs/1/au:+Prins_A/0/1/0/all/0/1">Aviva Prins</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1">Aravind Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P. Dickerson</a></p>
<p>Restless and collapsing bandits are often used to model budget-constrained
resource allocation in settings where arms have action-dependent transition
probabilities, such as the allocation of health interventions among patients.
However, state-of-the-art Whittle-index-based approaches to this planning
problem either do not consider fairness among arms, or incentivize fairness
without guaranteeing it. We thus introduce ProbFair, a probabilistically fair
policy that maximizes total expected reward and satisfies the budget constraint
while ensuring a strictly positive lower bound on the probability of being
pulled at each timestep. We evaluate our algorithm on a real-world application,
where interventions support continuous positive airway pressure (CPAP) therapy
adherence among patients, as well as on a broader class of synthetic transition
matrices. We find that ProbFair preserves utility while providing fairness
guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.06960">MAFAT: Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farley_J/0/1/0/all/0/1">Jackson Farley</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerstlauer_A/0/1/0/all/0/1">Andreas Gerstlauer</a></p>
<p>A rising research challenge is running costly machine learning (ML) networks
locally on resource-constrained edge devices. ML networks with large
convolutional layers can easily exceed available memory, increasing latency due
to excessive OS swapping. Previous memory reduction techniques such as pruning
and quantization reduce model accuracy and often require retraining.
Alternatively, distributed methods partition the convolutions into equivalent
smaller sub-computations, but the implementations introduce communication costs
and require a network of devices. Distributed partitioning approaches can,
however, also be used to run in a reduced memory footprint on a single device
by subdividing the network into smaller operations. In this paper, we extend
prior work on distributed partitioning into a memory-aware execution on a
single device. Our approach extends prior fusing strategies to allow for
multiple groups of convolutional layers that are fused and tiled independently.
This enables trading off overhead versus data reuse in order to specifically
reduces memory footprint. We propose a memory usage predictor coupled with a
search algorithm to provide optimized fusing and tiling configurations for an
arbitrary set of convolutional layers. When applied to the YOLOv2 object
detection network, results show that our approach can run in less than half the
memory, and with a speedup of up to 2.78 under severe memory constraints.
Additionally, our algorithm will return a configuration with a latency that is
within 6% of the best latency measured in a manual search.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.09992">Weisfeiler and Leman go Machine Learning: The Story so far. (arXiv:2112.09992v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1">Christopher Morris</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1">Yaron Lipman</a>, <a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1">Haggai Maron</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1">Bastian Rieck</a>, <a href="http://arxiv.org/find/cs/1/au:+Kriege_N/0/1/0/all/0/1">Nils M. Kriege</a>, <a href="http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1">Martin Grohe</a>, <a href="http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1">Matthias Fey</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1">Karsten Borgwardt</a></p>
<p>In recent years, algorithms and neural architectures based on the
Weisfeiler--Leman algorithm, a well-known heuristic for the graph isomorphism
problem, have emerged as a powerful tool for machine learning with graphs and
relational data. Here, we give a comprehensive overview of the algorithm's use
in a machine-learning setting, focusing on the supervised regime. We discuss
the theoretical background, show how to use it for supervised graph and node
representation learning, discuss recent extensions, and outline the algorithm's
connection to (permutation-)equivariant neural architectures. Moreover, we give
an overview of current applications and future directions to stimulate further
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.09753">Finite-Time Analysis of Natural Actor-Critic for POMDPs. (arXiv:2202.09753v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1">Semih Cayci</a>, <a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1">Niao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1">R. Srikant</a></p>
<p>We consider the reinforcement learning problem for partially observed Markov
decision processes (POMDPs) with large or even countably infinite state spaces,
where the controller has access to only noisy observations of the underlying
controlled Markov chain. We consider a natural actor-critic method that employs
a finite internal memory for policy parameterization, and a multi-step temporal
difference learning algorithm for policy evaluation. We establish, to the best
of our knowledge, the first non-asymptotic global convergence of actor-critic
methods for partially observed systems under function approximation. In
particular, in addition to the function approximation and statistical errors
that also arise in MDPs, we explicitly characterize the error due to the use of
finite-state controllers. This additional error is stated in terms of the total
variation distance between the traditional belief state in POMDPs and the
posterior distribution of the hidden state when using a finite-state
controller. Further, we show that this error can be made small in the case of
sliding-block controllers by using larger block sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.11434">Non-linear Embeddings in Hilbert Simplex Geometry. (arXiv:2203.11434v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1">Frank Nielsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Ke Sun</a></p>
<p>A key technique of machine learning and computer vision is to embed discrete
weighted graphs into continuous spaces for further downstream processing.
Embedding discrete hierarchical structures in hyperbolic geometry has proven
very successful since it was shown that any weighted tree can be embedded in
that geometry with arbitrary low distortion. Various optimization methods for
hyperbolic embeddings based on common models of hyperbolic geometry have been
studied. In this paper, we consider Hilbert geometry for the standard simplex
which is isometric to a vector space equipped with the variation polytope norm.
We study the representation power of this Hilbert simplex geometry by embedding
distance matrices of graphs. Our findings demonstrate that Hilbert simplex
geometry is competitive to alternative geometries such as the Poincar\'e
hyperbolic ball or the Euclidean geometry for embedding tasks while being fast
and numerically robust.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.16475">ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1">Haekyu Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seongmin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1">Benjamin Hoover</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_A/0/1/0/all/0/1">Austin Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1">Omar Shaikh</a>, <a href="http://arxiv.org/find/cs/1/au:+Duggal_R/0/1/0/all/0/1">Rahul Duggal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1">Nilaksh Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1">Judy Hoffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1">Duen Horng Chau</a></p>
<p>We present ConceptEvo, a unified interpretation framework for deep neural
networks (DNNs) that reveals the inception and evolution of learned concepts
during training. Our work fills a critical gap in DNN interpretation research,
as existing methods focus on post-hoc interpretation after training. ConceptEvo
presents two novel technical contributions: (1) an algorithm that generates a
unified semantic space that enables side-by-side comparison of different models
during training; and (2) an algorithm that discovers and quantifies important
concept evolutions for class predictions. Through a large-scale human
evaluation with 260 participants and quantitative experiments, we show that
ConceptEvo discovers evolutions across different models that are meaningful to
humans and important for predictions. ConceptEvo works for both modern
(ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.10403">Tackling Provably Hard Representative Selection via Graph Neural Networks. (arXiv:2205.10403v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1">Mehran Kazemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1">Anton Tsitsulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1">Hossein Esfandiari</a>, <a href="http://arxiv.org/find/cs/1/au:+Bateni_M/0/1/0/all/0/1">MohammadHossein Bateni</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramachandran_D/0/1/0/all/0/1">Deepak Ramachandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1">Bryan Perozzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1">Vahab Mirrokni</a></p>
<p>Representative Selection (RS) is the problem of finding a small subset of
exemplars from a dataset that is representative of the dataset. In this paper,
we study RS for attributed graphs, and focus on finding representative nodes
that optimize the accuracy of a model trained on the selected representatives.
Theoretically, we establish a new hardness result forRS (in the absence of a
graph structure) by proving that a particular, highly practical variant of it
(RS for Learning) is hard to approximate in polynomial time within any
reasonable factor, which implies a significant potential gap between the
optimum solution of widely-used surrogate functions and the actual accuracy of
the model. We then study the setting where a (homophilous) graph structure is
available, or can be constructed, between the data points.We show that with an
appropriate modeling approach, the presence of such a structure can turn a hard
RS (for learning) problem into one that can be effectively solved. To this end,
we develop RS-GNN, a representation learning-based RS model based on Graph
Neural Networks. Empirically, we demonstrate the effectiveness of RS-GNN on
problems with predefined graph structures as well as problems with graphs
induced from node feature similarities, by showing that RS-GNN achieves
significant improvements over established baselines on a suite of eight
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.12549">Learning from time-dependent streaming data with online stochastic algorithms. (arXiv:2205.12549v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Godichon_Baggioni_A/0/1/0/all/0/1">Antoine Godichon-Baggioni</a>, <a href="http://arxiv.org/find/cs/1/au:+Werge_N/0/1/0/all/0/1">Nicklas Werge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wintenberger_O/0/1/0/all/0/1">Olivier Wintenberger</a></p>
<p>This paper addresses stochastic optimization in a streaming setting with
time-dependent and biased gradient estimates. We analyze several first-order
methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and
time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our
non-asymptotic analysis establishes novel heuristics that link dependence,
biases, and convexity levels, enabling accelerated convergence. Specifically,
our findings demonstrate that (i) time-varying mini-batch SGD methods have the
capability to break long- and short-range dependence structures, (ii) biased
SGD methods can achieve comparable performance to their unbiased counterparts,
and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence
of the stochastic optimization algorithms. To validate our theoretical
findings, we conduct a series of experiments using both simulated and real-life
time-dependent data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03597">Meta-Learning Parameterized Skills. (arXiv:2206.03597v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Haotian Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangqun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1">Saket Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Littman_M/0/1/0/all/0/1">Michael Littman</a>, <a href="http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1">George Konidaris</a></p>
<p>We propose a novel parameterized skill-learning algorithm that aims to learn
transferable parameterized skills and synthesize them into a new action space
that supports efficient learning in long-horizon tasks. We propose to leverage
off-policy Meta-RL combined with a trajectory-centric smoothness term to learn
a set of parameterized skills. Our agent can use these learned skills to
construct a three-level hierarchical framework that models a
Temporally-extended Parameterized Action Markov Decision Process. We
empirically demonstrate that the proposed algorithms enable an agent to solve a
set of difficult long-horizon (obstacle-course and robot manipulation) tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03638">Alternately Optimized Graph Neural Networks. (arXiv:2206.03638v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Haoyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaorui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Haitao Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Torkamani_M/0/1/0/all/0/1">MohamadAli Torkamani</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1">Feng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1">Victor Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a></p>
<p>Graph Neural Networks (GNNs) have greatly advanced the semi-supervised node
classification task on graphs. The majority of existing GNNs are trained in an
end-to-end manner that can be viewed as tackling a bi-level optimization
problem. This process is often inefficient in computation and memory usage. In
this work, we propose a new optimization framework for semi-supervised learning
on graphs. The proposed framework can be conveniently solved by the alternating
optimization algorithms, resulting in significantly improved efficiency.
Extensive experiments demonstrate that the proposed method can achieve
comparable or better performance with state-of-the-art baselines while it has
significantly better computation and memory efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.09311">Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">John Sun</a></p>
<p>We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM
achieves good performance on imbalanced data sets with a Majority to Minority
Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including
intercept (bias), regularization and parameters affects performance on our
selection of datasets. Although many resort to SMOTE methods, we aim for a less
computationally intensive method. We evaluate the performance by examining the
learning curves. These curves diagnose whether we overfit or underfit or
whether the random sample of data chosen during the process was not random
enough or diverse enough in dependent variable class for the algorithm to
generalized to unseen examples. We will also see the background of the
hyperparameters versus the test and train error in validation curves. We
benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL
method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the
work of Ding by incorporating kernels into SVM. We will use Python rather than
MATLAB as python has dictionaries for storing mixed data types during
multi-parameter cross-validation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.06265">Trustworthy Recommender Systems. (arXiv:2208.06265v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shoujin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiuzhen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricci_F/0/1/0/all/0/1">Francesco Ricci</a></p>
<p>Recommender systems (RSs) aim to help users to effectively retrieve items of
their interests from a large catalogue. For a quite long period of time,
researchers and practitioners have been focusing on developing accurate RSs.
Recent years have witnessed an increasing number of threats to RSs, coming from
attacks, system and user generated noise, system bias. As a result, it has
become clear that a strict focus on RS accuracy is limited and the research
must consider other important factors, e.g., trustworthiness. For end users, a
trustworthy RS (TRS) should not only be accurate, but also transparent,
unbiased and fair as well as robust to noise or attacks. These observations
actually led to a paradigm shift of the research on RSs: from accuracy-oriented
RSs to TRSs. However, researchers lack a systematic overview and discussion of
the literature in this novel and fast developing field of TRSs. To this end, in
this paper, we provide an overview of TRSs, including a discussion of the
motivation and basic concepts of TRSs, a presentation of the challenges in
building TRSs, and a perspective on the future directions in this area. We also
provide a novel conceptual framework to support the construction of TRSs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.07734">Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1">Jaemin Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tiancheng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1">Leman Akoglu</a></p>
<p>Self-supervised learning (SSL) has emerged as a promising alternative to
create supervisory signals to real-world problems, avoiding the extensive cost
of manual labeling. SSL is particularly attractive for unsupervised tasks such
as anomaly detection (AD), where labeled anomalies are rare or often
nonexistent. A large catalog of augmentation functions has been used for
SSL-based AD (SSAD) on image data, and recent works have reported that the type
of augmentation has a significant impact on accuracy. Motivated by those, this
work sets out to put image-based SSAD under a larger lens and investigate the
role of data augmentation in SSAD. Through extensive experiments on 3 different
detector models and across 420 AD tasks, we provide comprehensive numerical and
visual evidences that the alignment between data augmentation and
anomaly-generating mechanism is the key to the success of SSAD, and in the lack
thereof, SSL may even impair accuracy. To the best of our knowledge, this is
the first meta-analysis on the role of data augmentation in SSAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.10967">The Value of Out-of-Distribution Data. (arXiv:2208.10967v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1">Ashwin De Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1">Rahul Ramesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1">Carey E. Priebe</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1">Pratik Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1">Joshua T. Vogelstein</a></p>
<p>We expect the generalization error to improve with more samples from a
similar task, and to deteriorate with more samples from an out-of-distribution
(OOD) task. In this work, we show a counter-intuitive phenomenon: the
generalization error of a task can be a non-monotonic function of the number of
OOD samples. As the number of OOD samples increases, the generalization error
on the target task improves before deteriorating beyond a threshold. In other
words, there is value in training on small amounts of OOD data. We use Fisher's
Linear Discriminant on synthetic datasets and deep networks on computer vision
benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate
and analyze this phenomenon. In the idealistic setting where we know which
samples are OOD, we show that these non-monotonic trends can be exploited using
an appropriately weighted objective of the target and OOD empirical risk. While
its practical utility is limited, this does suggest that if we can detect OOD
samples, then there may be ways to benefit from them. When we do not know which
samples are OOD, we show how a number of go-to strategies such as
data-augmentation, hyper-parameter optimization, and pre-training are not
enough to ensure that the target generalization error does not deteriorate with
the number of OOD samples in the dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08169">Value Summation: A Novel Scoring Function for MPC-based Model-based Reinforcement Learning. (arXiv:2209.08169v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Raisi_M/0/1/0/all/0/1">Mehran Raisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Noohian_A/0/1/0/all/0/1">Amirhossein Noohian</a>, <a href="http://arxiv.org/find/cs/1/au:+Mccutcheon_L/0/1/0/all/0/1">Luc Mccutcheon</a>, <a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1">Saber Fallah</a></p>
<p>This paper proposes a novel scoring function for the planning module of
MPC-based reinforcement learning methods to address the inherent bias of using
the reward function to score trajectories. The proposed method enhances the
learning efficiency of existing MPC-based MBRL methods using the discounted sum
of values. The method utilizes optimal trajectories to guide policy learning
and updates its state-action value function based on real-world and augmented
onboard data. The learning efficiency of the proposed method is evaluated in
selected MuJoCo Gym environments as well as in learning locomotion skills for a
simulated model of the Cassie robot. The results demonstrate that the proposed
method outperforms the current state-of-the-art algorithms in terms of learning
efficiency and average reward return.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.10901">Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning. (arXiv:2209.10901v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goulao_M/0/1/0/all/0/1">Manuel Goul&#xe3;o</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1">Arlindo L. Oliveira</a></p>
<p>The Vision Transformer architecture has shown to be competitive in the
computer vision (CV) space where it has dethroned convolution-based networks in
several benchmarks. Nevertheless, convolutional neural networks (CNN) remain
the preferential architecture for the representation module in reinforcement
learning. In this work, we study pretraining a Vision Transformer using several
state-of-the-art self-supervised methods and assess the quality of the learned
representations. To show the importance of the temporal dimension in this
context we propose an extension of VICReg to better capture temporal relations
between observations by adding a temporal order verification task. Our results
show that all methods are effective in learning useful representations and
avoiding representational collapse for observations from Atari Learning
Environment (ALE) which leads to improvements in data efficiency when we
evaluated in reinforcement learning (RL). Moreover, the encoder pretrained with
the temporal order verification task shows the best results across all
experiments, with richer representations, more focused attention maps and
sparser representation vectors throughout the layers of the encoder, which
shows the importance of exploring such similarity dimension. With this work, we
hope to provide some insights into the representations learned by ViT during a
self-supervised pretraining with observations from RL environments and which
properties arise in the representations that lead to the best-performing
agents. The source code will be available at:
https://github.com/mgoulao/TOV-VICReg
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15514">Cooperation in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders. (arXiv:2209.15514v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kviman_O/0/1/0/all/0/1">Oskar Kviman</a>, <a href="http://arxiv.org/find/cs/1/au:+Molen_R/0/1/0/all/0/1">Ricky Mol&#xe9;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Hotti_A/0/1/0/all/0/1">Alexandra Hotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurt_S/0/1/0/all/0/1">Semih Kurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Elvira_V/0/1/0/all/0/1">V&#xed;ctor Elvira</a>, <a href="http://arxiv.org/find/cs/1/au:+Lagergren_J/0/1/0/all/0/1">Jens Lagergren</a></p>
<p>In this paper, we show how the mixture components cooperate when they jointly
adapt to maximize the ELBO. We build upon recent advances in the multiple and
adaptive importance sampling literature. We then model the mixture components
using separate encoder networks and show empirically that the ELBO is
monotonically non-decreasing as a function of the number of mixture components.
These results hold for a range of different VAE architectures on the MNIST,
FashionMNIST, and CIFAR-10 datasets. In this work, we also demonstrate that
increasing the number of mixture components improves the latent-representation
capabilities of the VAE on both image and single-cell datasets. This
cooperative behavior motivates that using Mixture VAEs should be considered a
standard approach for obtaining more flexible variational approximations.
Finally, Mixture VAEs are here, for the first time, compared and combined with
normalizing flows, hierarchical models and/or the VampPrior in an extensive
ablation study. Multiple of our Mixture VAEs achieve state-of-the-art
log-likelihood results for VAE architectures on the MNIST and FashionMNIST
datasets. The experiments are reproducible using our code, provided here:
https://github.com/lagergren-lab/mixturevaes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04318">Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Grillo_M/0/1/0/all/0/1">Milo Grillo</a>, <a href="http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1">Yunpeng Han</a>, <a href="http://arxiv.org/find/stat/1/au:+Werpachowska_A/0/1/0/all/0/1">Agnieszka Werpachowska</a></p>
<p>We propose a simple and efficient approach to generate a prediction intervals
(PI) for approximated and forecasted trends. Our method leverages a weighted
asymmetric loss function to estimate the lower and upper bounds of the PI, with
the weights determined by its coverage probability. We provide a concise
mathematical proof of the method, show how it can be extended to derive PIs for
parametrised functions and discuss its effectiveness when training deep neural
networks. The presented tests of the method on a real-world forecasting task
using a neural network-based model show that it can produce reliable PIs in
complex machine learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.06226">Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics. (arXiv:2210.06226v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Daudel_K/0/1/0/all/0/1">Kam&#xe9;lia Daudel</a>, <a href="http://arxiv.org/find/stat/1/au:+Benton_J/0/1/0/all/0/1">Joe Benton</a>, <a href="http://arxiv.org/find/stat/1/au:+Shi_Y/0/1/0/all/0/1">Yuyang Shi</a>, <a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1">Arnaud Doucet</a></p>
<p>Several algorithms involving the Variational R\'enyi (VR) bound have been
proposed to minimize an alpha-divergence between a target posterior
distribution and a variational distribution. Despite promising empirical
results, those algorithms resort to biased stochastic gradient descent
procedures and thus lack theoretical guarantees. In this paper, we formalize
and study the VR-IWAE bound, a generalization of the Importance Weighted
Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several
desirable properties and notably leads to the same stochastic gradient descent
procedure as the VR bound in the reparameterized case, but this time by relying
on unbiased gradient estimators. We then provide two complementary theoretical
analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those
analyses shed light on the benefits or lack thereof of these bounds. Lastly, we
illustrate our theoretical claims over toy and real-data examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.12547">SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems. (arXiv:2210.12547v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1">Aaron Ferber</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1">Taoan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1">Martin Schubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1">Benoit Steiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1">Bistra Dilkina</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a></p>
<p>Optimization problems with nonlinear cost functions and combinatorial
constraints appear in many real-world applications but remain challenging to
solve efficiently compared to their linear counterparts. To bridge this gap, we
propose $\textbf{SurCo}$ that learns linear $\underline{\text{Sur}}$rogate
costs which can be used in existing $\underline{\text{Co}}$mbinatorial solvers
to output good solutions to the original nonlinear combinatorial optimization
problem. The surrogate costs are learned end-to-end with nonlinear loss by
differentiating through the linear surrogate solver, combining the flexibility
of gradient-based methods with the structure of linear combinatorial
optimization. We propose three $\texttt{SurCo}$ variants:
$\texttt{SurCo}-\texttt{zero}$ for individual nonlinear problems,
$\texttt{SurCo}-\texttt{prior}$ for problem distributions, and
$\texttt{SurCo}-\texttt{hybrid}$ to combine both distribution and
problem-specific information. We give theoretical intuition motivating
$\texttt{SurCo}$, and evaluate it empirically. Experiments show that
$\texttt{SurCo}$ finds better solutions faster than state-of-the-art and domain
expert approaches in real-world optimization problems such as embedding table
sharding, inverse photonic design, and nonlinear route planning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13601">Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gajjar_A/0/1/0/all/0/1">Aarshvi Gajjar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1">Chinmay Hegde</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Christopher Musco</a></p>
<p>We consider the problem of active learning for single neuron models, also
sometimes called ``ridge functions'', in the agnostic setting (under
adversarial label noise). Such models have been shown to be broadly effective
in modeling physical phenomena, and for constructing surrogate data-driven
models for partial differential equations.
</p>
<p>Surprisingly, we show that for a single neuron model with any Lipschitz
non-linearity (such as the ReLU, sigmoid, absolute value, low-degree
polynomial, among others), strong provable approximation guarantees can be
obtained using a well-known active learning strategy for fitting \emph{linear
functions} in the agnostic setting. % -- i.e. for the case when there is no
non-linearity. Namely, we can collect samples via statistical \emph{leverage
score sampling}, which has been shown to be near-optimal in other active
learning scenarios. We support our theoretical results with empirical
simulations showing that our proposed active learning strategy based on
leverage score sampling outperforms (ordinary) uniform sampling when fitting
single neuron models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14037">Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Holm_A/0/1/0/all/0/1">Andreas Nugaard Holm</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1">Dustin Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a></p>
<p>Uncertainty approximation in text classification is an important area with
applications in domain adaptation and interpretability. One of the most widely
used uncertainty approximation methods is Monte Carlo (MC) Dropout, which is
computationally expensive as it requires multiple forward passes through the
model. A cheaper alternative is to simply use the softmax based on a single
forward pass without dropout to estimate model uncertainty. However, prior work
has indicated that these predictions tend to be overconfident. In this paper,
we perform a thorough empirical analysis of these methods on five datasets with
two base neural architectures in order to identify the trade-offs between the
two. We compare both softmax and an efficient version of MC Dropout on their
uncertainty approximations and downstream text classification performance,
while weighing their runtime (cost) against performance (benefit). We find
that, while MC dropout produces the best uncertainty approximations, using a
simple softmax leads to competitive and in some cases better uncertainty
estimation for text classification at a much lower computational cost,
suggesting that softmax can in fact be a sufficient uncertainty estimate when
computational resources are a concern.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00736">An exponentially-growing family of universal quantum circuits. (arXiv:2212.00736v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Kordzanganeh_M/0/1/0/all/0/1">Mo Kordzanganeh</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sekatski_P/0/1/0/all/0/1">Pavel Sekatski</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pflitsch_M/0/1/0/all/0/1">Markus Pflitsch</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Melnikov_A/0/1/0/all/0/1">Alexey Melnikov</a></p>
<p>Quantum machine learning has become an area of growing interest but has
certain theoretical and hardware-specific limitations. Notably, the problem of
vanishing gradients, or barren plateaus, renders the training impossible for
circuits with high qubit counts, imposing a limit on the number of qubits that
data scientists can use for solving problems. Independently, angle-embedded
supervised quantum neural networks were shown to produce truncated Fourier
series with a degree directly dependent on two factors: the depth of the
encoding and the number of parallel qubits the encoding applied to. The degree
of the Fourier series limits the model expressivity. This work introduces two
new architectures whose Fourier degrees grow exponentially: the sequential and
parallel exponential quantum machine learning architectures. This is done by
efficiently using the available Hilbert space when encoding, increasing the
expressivity of the quantum encoding. Therefore, the exponential growth allows
staying at the low-qubit limit to create highly expressive circuits avoiding
barren plateaus. Practically, parallel exponential architecture was shown to
outperform the existing linear architectures by reducing their final mean
square error value by up to 44.7% in a one-dimensional test problem.
Furthermore, the feasibility of this technique was also shown on a trapped ion
quantum processing unit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01692">Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1">Michal &#x160;tef&#xe1;nik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1">Marek Kadl&#x10d;&#xed;k</a></p>
<p>Language models exhibit an emergent ability to learn a new task from a small
number of input-output demonstrations. However, recent work shows that
in-context learners largely rely on their pre-trained knowledge, such as the
sentiment of the labels, instead of learning new associations from the input.
We argue that the commonly-used few-shot evaluation using a random selection of
in-context demonstrations can not disentangle models' reliance on such biases,
as most of the randomly-selected demonstrations do not present relations
informative for prediction beyond exposing the task's input-output
distribution.
</p>
<p>Therefore, to evaluate models' in-context learning ability independent of
models' memory, we introduce a Concept-sharing few-shot learning method
choosing the demonstrations that share an underlying concept with the predicted
sample. We extract a set of such concepts from available human explanations and
measure how much models can benefit from presenting these concepts in few-shot
demonstrations.
</p>
<p>We find that most of the recent in-context learners can not consistently
benefit from the demonstrated concepts, irrespective of the model size.
However, we note that T0 models are more sensitive to exhibited concepts,
benefiting from concept-sharing demonstrations in 7 out of 8 evaluation
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.07383">Sequential Kernelized Independence Testing. (arXiv:2212.07383v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1">Aleksandr Podkopaev</a>, <a href="http://arxiv.org/find/stat/1/au:+Blobaum_P/0/1/0/all/0/1">Patrick Bl&#xf6;baum</a>, <a href="http://arxiv.org/find/stat/1/au:+Kasiviswanathan_S/0/1/0/all/0/1">Shiva Prasad Kasiviswanathan</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a></p>
<p>Independence testing is a classical statistical problem that has been
extensively studied in the batch setting when one fixes the sample size before
collecting data. However, practitioners often prefer procedures that adapt to
the complexity of a problem at hand instead of setting sample size in advance.
Ideally, such procedures should (a) stop earlier on easy tasks (and later on
harder tasks), hence making better use of available resources, and (b)
continuously monitor the data and efficiently incorporate statistical evidence
after collecting new data, while controlling the false alarm rate. Classical
batch tests are not tailored for streaming data: valid inference after data
peeking requires correcting for multiple testing which results in low power.
Following the principle of testing by betting, we design sequential kernelized
independence tests that overcome such shortcomings. We exemplify our broad
framework using bets inspired by kernelized dependence measures, e.g., the
Hilbert-Schmidt independence criterion. Our test is also valid under
non-i.i.d., time-varying settings. We demonstrate the power of our approaches
on both simulated and real data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05086">Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (arXiv:2302.05086v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qizhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yiwen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1">Wangmeng Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao Chen</a></p>
<p>The transferability of adversarial examples across deep neural networks
(DNNs) is the crux of many black-box attacks. Many prior efforts have been
devoted to improving the transferability via increasing the diversity in inputs
of some substitute models. In this paper, by contrast, we opt for the diversity
in substitute models and advocate to attack a Bayesian model for achieving
desirable transferability. Deriving from the Bayesian formulation, we develop a
principled strategy for possible finetuning, which can be combined with many
off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive
experiments have been conducted to verify the effectiveness of our method, on
common benchmark datasets, and the results demonstrate that our method
outperforms recent state-of-the-arts by large margins (roughly 19% absolute
increase in average attack success rate on ImageNet), and, by combining with
these recent methods, further performance gain can be obtained. Our code:
https://github.com/qizhangli/MoreBayesian-attack.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.05783">ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. (arXiv:2302.05783v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1">Tsui-Wei Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Subhro Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Megretski_A/0/1/0/all/0/1">Alexandre Megretski</a>, <a href="http://arxiv.org/find/cs/1/au:+Daniel_L/0/1/0/all/0/1">Luca Daniel</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Lam M. Nguyen</a></p>
<p>Deep neural networks (DNN) have shown great capacity of modeling a dynamical
system; nevertheless, they usually do not obey physics constraints such as
conservation laws. This paper proposes a new learning framework named ConCerNet
to improve the trustworthiness of the DNN based dynamics modeling to endow the
invariant properties. ConCerNet consists of two steps: (i) a contrastive
learning method to automatically capture the system invariants (i.e.
conservation properties) along the trajectory observations; (ii) a neural
projection layer to guarantee that the learned dynamics models preserve the
learned invariants. We theoretically prove the functional relationship between
the learned latent representation and the unknown system invariant function.
Experiments show that our method consistently outperforms the baseline neural
networks in both coordinate error and conservation metrics by a large margin.
With neural network based parameterization and no dependence on prior
knowledge, our method can be extended to complex and large-scale dynamics by
leveraging an autoencoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07265">The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus. (arXiv:2302.07265v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1">Anna Hedstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Bommer_P/0/1/0/all/0/1">Philine Bommer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wickstrom_K/0/1/0/all/0/1">Kristoffer K. Wickstr&#xf8;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1">Sebastian Lapuschkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a></p>
<p>One of the unsolved challenges in the field of Explainable AI (XAI) is
determining how to most reliably estimate the quality of an explanation method
in the absence of ground truth explanation labels. Resolving this issue is of
utmost importance as the evaluation outcomes generated by competing evaluation
methods (or ''quality estimators''), which aim at measuring the same property
of an explanation method, frequently present conflicting rankings. Such
disagreements can be challenging for practitioners to interpret, thereby
complicating their ability to select the best-performing explanation method. We
address this problem through a meta-evaluation of different quality estimators
in XAI, which we define as ''the process of evaluating the evaluation method''.
Our novel framework, MetaQuantus, analyses two complementary performance
characteristics of a quality estimator: its resilience to noise and reactivity
to randomness, thus circumventing the need for ground truth labels. We
demonstrate the effectiveness of our framework through a series of experiments,
targeting various open questions in XAI such as the selection and
hyperparameter optimisation of quality estimators. Our work is released under
an open-source license (https://github.com/annahedstroem/MetaQuantus) to serve
as a development tool for XAI- and Machine Learning (ML) practitioners to
verify and benchmark newly constructed quality estimators in a given
explainability context. With this work, we provide the community with clear and
theoretically-grounded guidance for identifying reliable evaluation methods,
thus facilitating reproducibility in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11665">AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving. (arXiv:2302.11665v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuohan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lianmin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yinmin Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1">Vincent Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1">Ying Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yanping Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhifeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E. Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a></p>
<p>Model parallelism is conventionally viewed as a method to scale a single
large deep learning model beyond the memory limits of a single device. In this
paper, we demonstrate that model parallelism can be additionally used for the
statistical multiplexing of multiple devices when serving multiple models, even
when a single model can fit into a single device. Our work reveals a
fundamental trade-off between the overhead introduced by model parallelism and
the opportunity to exploit statistical multiplexing to reduce serving latency
in the presence of bursty workloads. We explore the new trade-off space and
present a novel serving system, AlpaServe, that determines an efficient
strategy for placing and parallelizing collections of large deep learning
models across a distributed cluster. Evaluation results on production workloads
show that AlpaServe can process requests at up to 10x higher rates or 6x more
burstiness while staying within latency constraints for more than 99% of
requests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14015">CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design. (arXiv:2302.14015v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ivanova_D/0/1/0/all/0/1">Desi R. Ivanova</a>, <a href="http://arxiv.org/find/stat/1/au:+Jennings_J/0/1/0/all/0/1">Joel Jennings</a>, <a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1">Cheng Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1">Adam Foster</a></p>
<p>We formalize the problem of contextual optimization through the lens of
Bayesian experimental design and propose CO-BED -- a general, model-agnostic
framework for designing contextual experiments using information-theoretic
principles. After formulating a suitable information-based objective, we employ
black-box variational methods to simultaneously estimate it and optimize the
designs in a single stochastic gradient scheme. In addition, to accommodate
discrete actions within our framework, we propose leveraging continuous
relaxation schemes, which can naturally be integrated into our variational
objective. As a result, CO-BED provides a general and automated solution to a
wide range of contextual optimization problems. We illustrate its effectiveness
in a number of experiments, where CO-BED demonstrates competitive performance
even when compared to bespoke, model-specific alternatives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14101">Robust Field-level Likelihood-free Inference with Galaxies. (arXiv:2302.14101v2 [astro-ph.CO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Santi_N/0/1/0/all/0/1">Natal&#xed; S. M. de Santi</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Shao_H/0/1/0/all/0/1">Helen Shao</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1">Francisco Villaescusa-Navarro</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Abramo_L/0/1/0/all/0/1">L. Raul Abramo</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Teyssier_R/0/1/0/all/0/1">Romain Teyssier</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Villanueva_Domingo_P/0/1/0/all/0/1">Pablo Villanueva-Domingo</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ni_Y/0/1/0/all/0/1">Yueying Ni</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Angles_Alcazar_D/0/1/0/all/0/1">Daniel Angl&#xe9;s-Alc&#xe1;zar</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Genel_S/0/1/0/all/0/1">Shy Genel</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hernandez_Martinez_E/0/1/0/all/0/1">Elena Hernandez-Martinez</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Steinwandel_U/0/1/0/all/0/1">Ulrich P. Steinwandel</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Lovell_C/0/1/0/all/0/1">Christopher C. Lovell</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Dolag_K/0/1/0/all/0/1">Klaus Dolag</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Castro_T/0/1/0/all/0/1">Tiago Castro</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Vogelsberger_M/0/1/0/all/0/1">Mark Vogelsberger</a></p>
<p>We train graph neural networks to perform field-level likelihood-free
inference using galaxy catalogs from state-of-the-art hydrodynamic simulations
of the CAMELS project. Our models are rotational, translational, and
permutation invariant and do not impose any cut on scale. From galaxy catalogs
that only contain $3$D positions and radial velocities of $\sim 1, 000$
galaxies in tiny $(25~h^{-1}{\rm Mpc})^3$ volumes our models can infer the
value of $\Omega_{\rm m}$ with approximately $12$ % precision. More
importantly, by testing the models on galaxy catalogs from thousands of
hydrodynamic simulations, each having a different efficiency of supernova and
AGN feedback, run with five different codes and subgrid models - IllustrisTNG,
SIMBA, Astrid, Magneticum, SWIFT-EAGLE -, we find that our models are robust to
changes in astrophysics, subgrid physics, and subhalo/galaxy finder.
Furthermore, we test our models on $1,024$ simulations that cover a vast region
in parameter space - variations in $5$ cosmological and $23$ astrophysical
parameters - finding that the model extrapolates really well. Our results
indicate that the key to building a robust model is the use of both galaxy
positions and velocities, suggesting that the network have likely learned an
underlying physical relation that does not depend on galaxy formation and is
valid on scales larger than $\sim10~h^{-1}{\rm kpc}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.14518">Generalization Error Bounds for Noisy, Iterative Algorithms via Maximal Leakage. (arXiv:2302.14518v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Issa_I/0/1/0/all/0/1">Ibrahim Issa</a>, <a href="http://arxiv.org/find/cs/1/au:+Esposito_A/0/1/0/all/0/1">Amedeo Roberto Esposito</a>, <a href="http://arxiv.org/find/cs/1/au:+Gastpar_M/0/1/0/all/0/1">Michael Gastpar</a></p>
<p>We adopt an information-theoretic framework to analyze the generalization
behavior of the class of iterative, noisy learning algorithms. This class is
particularly suitable for study under information-theoretic metrics as the
algorithms are inherently randomized, and it includes commonly used algorithms
such as Stochastic Gradient Langevin Dynamics (SGLD). Herein, we use the
maximal leakage (equivalently, the Sibson mutual information of order infinity)
metric, as it is simple to analyze, and it implies both bounds on the
probability of having a large generalization error and on its expected value.
We show that, if the update function (e.g., gradient) is bounded in $L_2$-norm
and the additive noise is isotropic Gaussian noise, then one can obtain an
upper-bound on maximal leakage in semi-closed form. Furthermore, we demonstrate
how the assumptions on the update function affect the optimal (in the sense of
minimizing the induced maximal leakage) choice of the noise. Finally, we
compute explicit tight upper bounds on the induced maximal leakage for other
scenarios of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02918">Graph Positional Encoding via Random Feature Propagation. (arXiv:2303.02918v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1">Moshe Eliasof</a>, <a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1">Fabrizio Frasca</a>, <a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1">Beatrice Bevilacqua</a>, <a href="http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1">Eran Treister</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a>, <a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1">Haggai Maron</a></p>
<p>Two main families of node feature augmentation schemes have been explored for
enhancing GNNs: random features and spectral positional encoding. Surprisingly,
however, there is still no clear understanding of the relation between these
two augmentation schemes. Here we propose a novel family of positional encoding
schemes which draws a link between the above two approaches and improves over
both. The new approach, named Random Feature Propagation (RFP), is inspired by
the power iteration method and its generalizations. It concatenates several
intermediate steps of an iterative algorithm for computing the dominant
eigenvectors of a propagation matrix, starting from random node features.
Notably, these propagation steps are based on graph-dependent propagation
operators that can be either predefined or learned. We explore the theoretical
and empirical benefits of RFP. First, we provide theoretical justifications for
using random features, for incorporating early propagation steps, and for using
multiple random initializations. Then, we empirically demonstrate that RFP
significantly outperforms both spectral PE and random features in multiple node
classification and graph classification benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03027">Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss. (arXiv:2303.03027v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Brechet_P/0/1/0/all/0/1">Pierre Br&#xe9;chet</a>, <a href="http://arxiv.org/find/stat/1/au:+Papagiannouli_K/0/1/0/all/0/1">Katerina Papagiannouli</a>, <a href="http://arxiv.org/find/stat/1/au:+An_J/0/1/0/all/0/1">Jing An</a>, <a href="http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1">Guido Mont&#xfa;far</a></p>
<p>We consider a deep matrix factorization model of covariance matrices trained
with the Bures-Wasserstein distance. While recent works have made advances in
the study of the optimization problem for overparametrized low-rank matrix
approximation, much emphasis has been placed on discriminative settings and the
square loss. In contrast, our model considers another type of loss and connects
with the generative setting. We characterize the critical points and minimizers
of the Bures-Wasserstein distance over the space of rank-bounded matrices. The
Hessian of this loss at low-rank matrices can theoretically blow up, which
creates challenges to analyze convergence of gradient optimization methods. We
establish convergence results for gradient flow using a smooth perturbative
version of the loss as well as convergence results for finite step size
gradient descent under certain assumptions on the initial weights.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09340">Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Thalhammer_J/0/1/0/all/0/1">Johannes Thalhammer</a>, <a href="http://arxiv.org/find/eess/1/au:+Schultheiss_M/0/1/0/all/0/1">Manuel Schultheiss</a>, <a href="http://arxiv.org/find/eess/1/au:+Dorosti_T/0/1/0/all/0/1">Tina Dorosti</a>, <a href="http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1">Tobias Lasser</a>, <a href="http://arxiv.org/find/eess/1/au:+Pfeiffer_F/0/1/0/all/0/1">Franz Pfeiffer</a>, <a href="http://arxiv.org/find/eess/1/au:+Pfeiffer_D/0/1/0/all/0/1">Daniela Pfeiffer</a>, <a href="http://arxiv.org/find/eess/1/au:+Schaff_F/0/1/0/all/0/1">Florian Schaff</a></p>
<p>Purpose: Sparse-view computed tomography (CT) is an effective way to reduce
dose by lowering the total number of views acquired, albeit at the expense of
image quality, which, in turn, can impact the ability to detect diseases. We
explore deep learning-based artifact reduction in sparse-view cranial CT scans
and its impact on automated hemorrhage detection. Methods: We trained a U-Net
for artefact reduction on simulated sparse-view cranial CT scans from 3000
patients obtained from a public dataset and reconstructed with varying levels
of sub-sampling. Additionally, we trained a convolutional neural network on
fully sampled CT data from 17,545 patients for automated hemorrhage detection.
We evaluated the classification performance using the area under the receiver
operator characteristic curves (AUC-ROCs) with corresponding 95% confidence
intervals (CIs) and the DeLong test, along with confusion matrices. The
performance of the U-Net was compared to an analytical approach based on total
variation (TV). Results: The U-Net performed superior compared to unprocessed
and TV-processed images with respect to image quality and automated hemorrhage
diagnosis. With U-Net post-processing, the number of views can be reduced from
4096 (AUC-ROC: 0.974; 95% CI: 0.972-0.976) views to 512 views (0.973;
0.971-0.975) with minimal decrease in hemorrhage detection (P&lt;.001) and to 256
views (0.967; 0.964-0.969) with a slight performance decrease (P&lt;.001).
Conclusion: The results suggest that U-Net based artifact reduction
substantially enhances automated hemorrhage detection in sparse-view cranial
CTs. Our findings highlight that appropriate post-processing is crucial for
optimal image quality and diagnostic accuracy while minimizing radiation dose.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.11103">Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling. (arXiv:2303.11103v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1">Jakob Hoydis</a>, <a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1">Fay&#xe7;al A&#xef;t Aoudia</a>, <a href="http://arxiv.org/find/cs/1/au:+Cammerer_S/0/1/0/all/0/1">Sebastian Cammerer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nimier_David_M/0/1/0/all/0/1">Merlin Nimier-David</a>, <a href="http://arxiv.org/find/cs/1/au:+Binder_N/0/1/0/all/0/1">Nikolaus Binder</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1">Guillermo Marcus</a>, <a href="http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1">Alexander Keller</a></p>
<p>Sionna is a GPU-accelerated open-source library for link-level simulations
based on TensorFlow. Since release v0.14 it integrates a differentiable ray
tracer (RT) for the simulation of radio wave propagation. This unique feature
allows for the computation of gradients of the channel impulse response and
other related quantities with respect to many system and environment
parameters, such as material properties, antenna patterns, array geometries, as
well as transmitter and receiver orientations and positions. In this paper, we
outline the key components of Sionna RT and showcase example applications such
as learning radio materials and optimizing transmitter orientations by gradient
descent. While classic ray tracing is a crucial tool for 6G research topics
like reconfigurable intelligent surfaces, integrated sensing and
communications, as well as user localization, differentiable ray tracing is a
key enabler for many novel and exciting research directions, for example,
digital twins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15585">Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. (arXiv:2303.15585v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yfantidou_S/0/1/0/all/0/1">Sofia Yfantidou</a>, <a href="http://arxiv.org/find/cs/1/au:+Constantinides_M/0/1/0/all/0/1">Marios Constantinides</a>, <a href="http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1">Dimitris Spathis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakali_A/0/1/0/all/0/1">Athena Vakali</a>, <a href="http://arxiv.org/find/cs/1/au:+Quercia_D/0/1/0/all/0/1">Daniele Quercia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1">Fahim Kawsar</a></p>
<p>The field of mobile and wearable computing is undergoing a revolutionary
integration of machine learning. Devices can now diagnose diseases, predict
heart irregularities, and unlock the full potential of human cognition.
However, the underlying algorithms powering these predictions are not immune to
biases with respect to sensitive attributes (e.g., gender, race), leading to
discriminatory outcomes. The goal of this work is to explore the extent to
which the mobile and wearable computing community has adopted ways of reporting
information about datasets and models to surface and, eventually, counter
biases. Our systematic review of papers published in the Proceedings of the ACM
Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal from
2018-2022 indicates that, while there has been progress made on algorithmic
fairness, there is still ample room for growth. Our findings show that only a
small portion (5%) of published papers adheres to modern fairness reporting,
while the overwhelming majority thereof focuses on accuracy or error metrics.
To generalize these results across venues of similar scope, we analyzed recent
proceedings of ACM MobiCom, MobiSys, and SenSys, IEEE Pervasive, and IEEE
Transactions on Mobile Computing Computing, and found no deviation from our
primary result. In light of these findings, our work provides practical
guidelines for the design and development of mobile and wearable technologies
that not only strive for accuracy but also fairness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.15592">Uncovering Bias in Personal Informatics. (arXiv:2303.15592v2 [cs.CY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yfantidou_S/0/1/0/all/0/1">Sofia Yfantidou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sermpezis_P/0/1/0/all/0/1">Pavlos Sermpezis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vakali_A/0/1/0/all/0/1">Athena Vakali</a>, <a href="http://arxiv.org/find/cs/1/au:+Baeza_Yates_R/0/1/0/all/0/1">Ricardo Baeza-Yates</a></p>
<p>Personal informatics (PI) systems, powered by smartphones and wearables,
enable people to lead healthier lifestyles by providing meaningful and
actionable insights that break down barriers between users and their health
information. Today, such systems are used by billions of users for monitoring
not only physical activity and sleep but also vital signs and women's and heart
health, among others. Despite their widespread usage, the processing of
sensitive PI data may suffer from biases, which may entail practical and
ethical implications. In this work, we present the first comprehensive
empirical and analytical study of bias in PI systems, including biases in raw
data and in the entire machine learning life cycle. We use the most detailed
framework to date for exploring the different sources of bias and find that
biases exist both in the data generation and the model learning and
implementation streams. According to our results, the most affected minority
groups are users with health issues, such as diabetes, joint issues, and
hypertension, and female users, whose data biases are propagated or even
amplified by learning models, while intersectional biases can also be observed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.00909">Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v4 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wenqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharan_S/0/1/0/all/0/1">S P Sharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1">Ajay Kumar Jaiswal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kevin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1">Yihan Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dejia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a></p>
<p>For a complicated algorithm, its implementation by a human programmer usually
starts with outlining a rough control flow followed by iterative enrichments,
eventually yielding carefully generated syntactic structures and variables in a
hierarchy. However, state-of-the-art large language models generate codes in a
single pass, without intermediate warm-ups to reflect the structured thought
process of "outline-then-detail". Inspired by the recent success of
chain-of-thought prompting, we propose ChainCoder, a program synthesis language
model that generates Python code progressively, i.e. from coarse to fine in
multiple passes. We first decompose source code into layout frame components
and accessory components via abstract syntax tree parsing to construct a
hierarchical representation. We then reform our prediction target into a
multi-pass objective, each pass generates a subsequence, which is concatenated
in the hierarchy. Finally, a tailored transformer architecture is leveraged to
jointly encode the natural language descriptions and syntactically aligned I/O
data samples. Extensive evaluations show that ChainCoder outperforms
state-of-the-arts, demonstrating that our progressive generation eases the
reasoning procedure and guides the language model to generate higher-quality
solutions. Our codes are available at:
https://github.com/VITA-Group/ChainCoder.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07898">Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus. (arXiv:2305.07898v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Maritan_A/0/1/0/all/0/1">Alessio Maritan</a>, <a href="http://arxiv.org/find/math/1/au:+Sharma_G/0/1/0/all/0/1">Ganesh Sharma</a>, <a href="http://arxiv.org/find/math/1/au:+Schenato_L/0/1/0/all/0/1">Luca Schenato</a>, <a href="http://arxiv.org/find/math/1/au:+Dey_S/0/1/0/all/0/1">Subhrakanti Dey</a></p>
<p>This paper considers the problem of distributed multi-agent learning, where
the global aim is to minimize a sum of local objective (empirical loss)
functions through local optimization and information exchange between
neighbouring nodes. We introduce a Newton-type fully distributed optimization
algorithm, Network-GIANT, which is based on GIANT, a Federated learning
algorithm that relies on a centralized parameter server. The Network-GIANT
algorithm is designed via a combination of gradient-tracking and a Newton-type
iterative algorithm at each node with consensus based averaging of local
gradient and Newton updates. We prove that our algorithm guarantees semi-global
and exponential convergence to the exact solution over the network assuming
strongly convex and smooth loss functions. We provide empirical evidence of the
superior convergence performance of Network-GIANT over other state-of-art
distributed learning algorithms such as Network-DANE and Newton-Raphson
Consensus.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09211">CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1">Momina Liaqat Ali</a>, <a href="http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1">Zunaira Rauf</a>, <a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1">Asifullah Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1">Anabia Sohail</a>, <a href="http://arxiv.org/find/eess/1/au:+Ullah_R/0/1/0/all/0/1">Rafi Ullah</a>, <a href="http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1">Jeonghwan Gwak</a></p>
<p>Transformers, due to their ability to learn long range dependencies, have
overcome the shortcomings of convolutional neural networks (CNNs) for global
perspective learning. Therefore, they have gained the focus of researchers for
several vision related tasks including medical diagnosis. However, their
multi-head attention module only captures global level feature representations,
which is insufficient for medical images. To address this issue, we propose a
Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning
to generate boosted channels and employs both transformers and CNNs to analyse
lymphocytes in histopathological images. The proposed CB HVT comprises five
modules, including a channel generation module, channel exploitation module,
channel merging module, region-aware module, and a detection and segmentation
head, which work together to effectively identify lymphocytes. The channel
generation module uses the idea of channel boosting through transfer learning
to extract diverse channels from different auxiliary learners. In the CB HVT,
these boosted channels are first concatenated and ranked using an attention
mechanism in the channel exploitation module. A fusion block is then utilized
in the channel merging module for a gradual and systematic merging of the
diverse boosted channels to improve the network's learning representations. The
CB HVT also employs a proposal network in its region aware module and a head to
effectively identify objects, even in overlapping regions and with artifacts.
We evaluated the proposed CB HVT on two publicly available datasets for
lymphocyte assessment in histopathological images. The results show that CB HVT
outperformed other state of the art detection models, and has good
generalization ability, demonstrating its value as a tool for pathologists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09446">A Probabilistic Transformation of Distance-Based Outliers. (arXiv:2305.09446v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muhr_D/0/1/0/all/0/1">David Muhr</a>, <a href="http://arxiv.org/find/cs/1/au:+Affenzeller_M/0/1/0/all/0/1">Michael Affenzeller</a>, <a href="http://arxiv.org/find/cs/1/au:+Kung_J/0/1/0/all/0/1">Josef K&#xfc;ng</a></p>
<p>The scores of distance-based outlier detection methods are difficult to
interpret, making it challenging to determine a cut-off threshold between
normal and outlier data points without additional context. We describe a
generic transformation of distance-based outlier scores into interpretable,
probabilistic estimates. The transformation is ranking-stable and increases the
contrast between normal and outlier data points. Determining distance
relationships between data points is necessary to identify the nearest-neighbor
relationships in the data, yet, most of the computed distances are typically
discarded. We show that the distances to other data points can be used to model
distance probability distributions and, subsequently, use the distributions to
turn distance-based outlier scores into outlier probabilities. Our experiments
show that the probabilistic transformation does not impact detection
performance over numerous tabular and image benchmark datasets but results in
interpretable outlier scores with increased contrast between normal and outlier
samples. Our work generalizes to a wide range of distance-based outlier
detection methods, and because existing distance computations are used, it adds
no significant computational overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09946">AdaMSS: Adaptive Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Meng_M/0/1/0/all/0/1">Mingyuan Meng</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu_B/0/1/0/all/0/1">Bingxin Gu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fulham_M/0/1/0/all/0/1">Michael Fulham</a>, <a href="http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1">Shaoli Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1">Dagan Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1">Lei Bi</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Jinman Kim</a></p>
<p>Survival prediction is a major concern for cancer management. Deep survival
models based on deep learning have been widely adopted to perform end-to-end
survival prediction from medical images. Recent deep survival models achieved
promising performance by jointly performing tumor segmentation with survival
prediction, where the models were guided to extract tumor-related information
through Multi-Task Learning (MTL). However, these deep survival models have
difficulties in exploring out-of-tumor prognostic information. In addition,
existing deep survival models are unable to effectively leverage multi-modality
images. Empirically-designed fusion strategies were commonly adopted to fuse
multi-modality information via task-specific manually-designed networks, thus
limiting the adaptability to different scenarios. In this study, we propose an
Adaptive Multi-modality Segmentation-to-Survival model (AdaMSS) for survival
prediction from PET/CT images. Instead of adopting MTL, we propose a novel
Segmentation-to-Survival Learning (SSL) strategy, where our AdaMSS is trained
for tumor segmentation and survival prediction sequentially in two stages. This
strategy enables the AdaMSS to focus on tumor regions in the first stage and
gradually expand its focus to include other prognosis-related regions in the
second stage. We also propose a data-driven strategy to fuse multi-modality
information, which realizes adaptive optimization of fusion strategies based on
training data during training. With the SSL and data-driven fusion strategies,
our AdaMSS is designed as an adaptive model that can self-adapt its focus
regions and fusion strategy for different training stages. Extensive
experiments with two large clinical datasets show that our AdaMSS outperforms
state-of-the-art survival prediction methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12239">Off-Policy Average Reward Actor-Critic with Deterministic Policy Search. (arXiv:2305.12239v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saxena_N/0/1/0/all/0/1">Naman Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Khastigir_S/0/1/0/all/0/1">Subhojyoti Khastigir</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolathaya_S/0/1/0/all/0/1">Shishir Kolathaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1">Shalabh Bhatnagar</a></p>
<p>The average reward criterion is relatively less studied as most existing
works in the Reinforcement Learning literature consider the discounted reward
criterion. There are few recent works that present on-policy average reward
actor-critic algorithms, but average reward off-policy actor-critic is
relatively less explored. In this work, we present both on-policy and
off-policy deterministic policy gradient theorems for the average reward
performance criterion. Using these theorems, we also present an Average Reward
Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first
show asymptotic convergence analysis using the ODE-based method. Subsequently,
we provide a finite time analysis of the resulting stochastic approximation
scheme with linear function approximator and obtain an $\epsilon$-optimal
stationary policy with a sample complexity of $\Omega(\epsilon^{-2.5})$. We
compare the average reward performance of our proposed ARO-DDPG algorithm and
observe better empirical performance compared to state-of-the-art on-policy
average reward actor-critic algorithms over MuJoCo-based environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15851">On sampling determinantal and Pfaffian point processes on a quantum computer. (arXiv:2305.15851v2 [stat.CO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bardenet_R/0/1/0/all/0/1">R&#xe9;mi Bardenet</a>, <a href="http://arxiv.org/find/stat/1/au:+Fanuel_M/0/1/0/all/0/1">Micha&#xeb;l Fanuel</a>, <a href="http://arxiv.org/find/stat/1/au:+Feller_A/0/1/0/all/0/1">Alexandre Feller</a></p>
<p>DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since
then, they have been widely used as models and subsampling tools in statistics
and computer science. Most applications require sampling from a DPP, and given
their quantum origin, it is natural to wonder whether sampling a DPP on a
quantum computer is easier than on a classical one. We focus here on DPPs over
a finite state space, which are distributions over the subsets of
$\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla
sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and
$\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank
of the kernel matrix. A large first part of the current paper consists in
explaining why the state-of-the-art in quantum simulation of fermionic systems
already yields quantum DPP sampling algorithms. We then modify existing quantum
circuits, and discuss their insertion in a full DPP sampling pipeline that
starts from practical kernel specifications. The bottom line is that, with $P$
(classical) parallel processors, we can divide the preprocessing cost by $P$
and build a quantum circuit with $\mathcal{O}(Nr)$ gates that sample a given
DPP, with depth varying from $\mathcal{O}(N)$ to $\mathcal{O}(r\log N)$
depending on qubit-communication constraints on the target machine. We also
connect existing work on the simulation of superconductors to Pfaffian point
processes, which generalize DPPs and would be a natural addition to the machine
learner's toolbox. Finally, the circuits are empirically validated on a
classical simulator and on 5-qubit machines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16165">A Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing. (arXiv:2305.16165v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1">Nischal Ashok Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Wanyong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaewook Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+McNichols_H/0/1/0/all/0/1">Hunter McNichols</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1">Aritra Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1">Andrew Lan</a></p>
<p>In this paper, we take a preliminary step towards solving the problem of
causal discovery in knowledge tracing, i.e., finding the underlying causal
relationship among different skills from real-world student response data. This
problem is important since it can potentially help us understand the causal
relationship between different skills without extensive A/B testing, which can
potentially help educators to design better curricula according to skill
prerequisite information. Specifically, we propose a conceptual solution, a
novel causal gated recurrent unit (GRU) module in a modified deep knowledge
tracing model, which uses i) a learnable permutation matrix for causal ordering
among skills and ii) an optionally learnable lower-triangular matrix for causal
structure among skills. We also detail how to learn the model parameters in an
end-to-end, differentiable way. Our solution placed among the top entries in
Task 3 of the NeurIPS 2022 Challenge on Causal Insights for Learning Paths in
Education. We detail preliminary experiments as evaluated on the challenge's
public leaderboard since the ground truth causal structure has not been
publicly released, making detailed local evaluation impossible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02479">Contagion Effect Estimation Using Proximal Embeddings. (arXiv:2306.02479v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fatemi_Z/0/1/0/all/0/1">Zahra Fatemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheleva_E/0/1/0/all/0/1">Elena Zheleva</a></p>
<p>Contagion effect refers to the causal effect of peers' behavior on the
outcome of an individual in social networks. While prominent methods for
estimating contagion effects in observational studies often assume that there
are no unmeasured confounders, contagion can be confounded due to latent
homophily: nodes in a homophilous network tend to have ties to peers with
similar attributes and can behave similarly without influencing one another.
One way to account for latent homophily is by considering proxies for the
unobserved confounders. However, in the presence of high-dimensional proxies,
proxy-based methods can lead to substantially biased estimation of contagion
effects, as we demonstrate in this paper. To tackle this issue, we introduce
the novel Proximal Embeddings (ProEmb), a framework which integrates
Variational Autoencoders (VAEs) and adversarial networks to generate balanced
low-dimensional representations of high-dimensional proxies for different
treatment groups and identifies contagion effects in the presence of unobserved
network confounders. We empirically show that our method significantly
increases the accuracy of contagion effect estimation in observational network
data compared to state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08617">Multi-class Graph Clustering via Approximated Effective $p$-Resistance. (arXiv:2306.08617v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1">Shota Saito</a>, <a href="http://arxiv.org/find/cs/1/au:+Herbster_M/0/1/0/all/0/1">Mark Herbster</a></p>
<p>This paper develops an approximation to the (effective) $p$-resistance and
applies it to multi-class clustering. Spectral methods based on the graph
Laplacian and its generalization to the graph $p$-Laplacian have been a
backbone of non-euclidean clustering techniques. The advantage of the
$p$-Laplacian is that the parameter $p$ induces a controllable bias on cluster
structure. The drawback of $p$-Laplacian eigenvector based methods is that the
third and higher eigenvectors are difficult to compute. Thus, instead, we are
motivated to use the $p$-resistance induced by the $p$-Laplacian for
clustering. For $p$-resistance, small $p$ biases towards clusters with high
internal connectivity while large $p$ biases towards clusters of small
"extent," that is a preference for smaller shortest-path distances between
vertices in the cluster. However, the $p$-resistance is expensive to compute.
We overcome this by developing an approximation to the $p$-resistance. We prove
upper and lower bounds on this approximation and observe that it is exact when
the graph is a tree. We also provide theoretical justification for the use of
$p$-resistance for clustering. Finally, we provide experiments comparing our
approximated $p$-resistance clustering to other $p$-Laplacian based methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09618">Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions. (arXiv:2306.09618v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khayatkhoei_M/0/1/0/all/0/1">Mahyar Khayatkhoei</a>, <a href="http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1">Wael AbdAlmageed</a></p>
<p>Precision and Recall are two prominent metrics of generative performance,
which were proposed to separately measure the fidelity and diversity of
generative models. Given their central role in comparing and improving
generative models, understanding their limitations are crucially important. To
that end, in this work, we identify a critical flaw in the common approximation
of these metrics using k-nearest-neighbors, namely, that the very
interpretations of fidelity and diversity that are assigned to Precision and
Recall can fail in high dimensions, resulting in very misleading conclusions.
Specifically, we empirically and theoretically show that as the number of
dimensions grows, two model distributions with supports at equal point-wise
distance from the support of the real distribution, can have vastly different
Precision and Recall regardless of their respective distributions, hence an
emergent asymmetry in high dimensions. Based on our theoretical insights, we
then provide simple yet effective modifications to these metrics to construct
symmetric metrics regardless of the number of dimensions. Finally, we provide
experiments on real-world datasets to illustrate that the identified flaw is
not merely a pathological case, and that our proposed metrics are effective in
alleviating its impact.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.10404">The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1">Nishil Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sebastian Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Mannelli_S/0/1/0/all/0/1">Stefano Sarao Mannelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldt_S/0/1/0/all/0/1">Sebastian Goldt</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1">Adrew Saxe</a></p>
<p>Reinforcement learning (RL) algorithms have proven transformative in a range
of domains. To tackle real-world domains, these systems often use neural
networks to learn policies directly from pixels or other high-dimensional
sensory input. By contrast, much theory of RL has focused on discrete state
spaces or worst-case analysis, and fundamental questions remain about the
dynamics of policy learning in high-dimensional settings. Here, we propose a
solvable high-dimensional model of RL that can capture a variety of learning
protocols, and derive its typical dynamics as a set of closed-form ordinary
differential equations (ODEs). We derive optimal schedules for the learning
rates and task difficulty - analogous to annealing schemes and curricula during
training in RL - and show that the model exhibits rich behaviour, including
delayed learning under sparse rewards; a variety of learning regimes depending
on reward baselines; and a speed-accuracy trade-off driven by reward
stringency. Experiments on variants of the Procgen game "Bossfight" and Arcade
Learning Environment game "Pong" also show such a speed-accuracy trade-off in
practice. Together, these results take a step towards closing the gap between
theory and practice in high-dimensional RL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13197">Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lerma_M/0/1/0/all/0/1">Miguel Lerma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1">Mirtha Lucas</a></p>
<p>Gradient based attribution methods for neural networks working as classifiers
use gradients of network scores. Here we discuss the practical differences
between using gradients of pre-softmax scores versus post-softmax scores, and
their respective advantages and disadvantages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14048">H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhenyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1">Ying Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lianmin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1">Ruisi Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zhao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1">Christopher R&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1">Clark Barrett</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Beidi Chen</a></p>
<p>Large Language Models (LLMs), despite their recent impressive
accomplishments, are notably cost-prohibitive to deploy, particularly for
applications involving long-content generation, such as dialogue systems and
story writing. Often, a large amount of transient state information, referred
to as the KV cache, is stored in GPU memory in addition to model parameters,
scaling linearly with the sequence length and batch size. In this paper, we
introduce a novel approach for implementing the KV cache which significantly
reduces its memory footprint. Our approach is based on the noteworthy
observation that a small portion of tokens contributes most of the value when
computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through
a comprehensive investigation, we find that (i) the emergence of H$_2$ is
natural and strongly correlates with the frequent co-occurrence of tokens in
the text, and (ii) removing them results in significant performance
degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O),
a KV cache eviction policy that dynamically retains a balance of recent and
H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular
problem and prove (under mild assumptions) a theoretical guarantee for our
novel eviction algorithm which could help guide future work. We validate the
accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of
tasks. Our implementation of H$_2$O with 20% heavy hitters improves the
throughput over three leading inference systems DeepSpeed Zero-Inference,
Hugging Face Accelerate, and FlexGen by up to 29$\times$, 29$\times$, and
3$\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the
latency by up to 1.9$\times$. The code is available at
https://github.com/FMInference/H2O.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01158">Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning. (arXiv:2307.01158v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oguntola_I/0/1/0/all/0/1">Ini Oguntola</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1">Joseph Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Stepputtis_S/0/1/0/all/0/1">Simon Stepputtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1">Katia Sycara</a></p>
<p>The ability to model the mental states of others is crucial to human social
intelligence, and can offer similar benefits to artificial agents with respect
to the social dynamics induced in multi-agent settings. We present a method of
grounding semantically meaningful, human-interpretable beliefs within policies
modeled by deep networks. We then consider the task of 2nd-order belief
prediction. We propose that ability of each agent to predict the beliefs of the
other agents can be used as an intrinsic reward signal for multi-agent
reinforcement learning. Finally, we present preliminary empirical results in a
mixed cooperative-competitive environment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01646">SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1">Qi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhengyang Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Renjie Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lele Wang</a></p>
<p>Diffusion models based on permutation-equivariant networks can learn
permutation-invariant distributions for graph data. However, in comparison to
their non-invariant counterparts, we have found that these invariant models
encounter greater learning challenges since 1) their effective target
distributions exhibit more modes; 2) their optimal one-step denoising scores
are the score functions of Gaussian mixtures with more components. Motivated by
this analysis, we propose a non-invariant diffusion model, called
$\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message
passing network and utilizes shifted window based self-attention inspired by
SwinTransformers. Further, through systematic ablations, we identify several
critical training and sampling techniques that significantly improve the sample
quality of graph generation. At last, we introduce a simple post-processing
trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably
converts any graph generative model to a permutation-invariant one. Extensive
experiments on synthetic and real-world protein and molecule datasets show that
our SwinGNN achieves state-of-the-art performances. Our code is released at
https://github.com/qiyan98/SwinGNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02486">LongNet: Scaling Transformers to 1,000,000,000 Tokens. (arXiv:2307.02486v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1">Jiayu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Li Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingxing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shaohan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>Scaling sequence length has become a critical demand in the era of large
language models. However, existing methods struggle with either computational
complexity or model expressivity, rendering the maximum sequence length
restricted. To address this issue, we introduce LongNet, a Transformer variant
that can scale sequence length to more than 1 billion tokens, without
sacrificing the performance on shorter sequences. Specifically, we propose
dilated attention, which expands the attentive field exponentially as the
distance grows. LongNet has significant advantages: 1) it has a linear
computation complexity and a logarithm dependency between any two tokens in a
sequence; 2) it can be served as a distributed trainer for extremely long
sequences; 3) its dilated attention is a drop-in replacement for standard
attention, which can be seamlessly integrated with the existing
Transformer-based optimization. Experiments results demonstrate that LongNet
yields strong performance on both long-sequence modeling and general language
tasks. Our work opens up new possibilities for modeling very long sequences,
e.g., treating a whole corpus or even the entire Internet as a sequence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03135">Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yunhao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1">Zhan Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a></p>
<p>Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student's OOD
generalization: (1) by better imitating teacher's visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher's language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Code released at
https://github.com/xuanlinli17/large_vlm_distillation_ood
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03587">BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits. (arXiv:2307.03587v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Werge_N/0/1/0/all/0/1">Nicklas Werge</a>, <a href="http://arxiv.org/find/cs/1/au:+Akgul_A/0/1/0/all/0/1">Abdullah Akg&#xfc;l</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1">Melih Kandemir</a></p>
<p>We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound
(BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary
environments. This unique combination of Bayesian and frequentist principles
enhances adaptability and performance in dynamic settings. The BOF-UCB
algorithm utilizes sequential Bayesian updates to infer the posterior
distribution of the unknown regression parameter, and subsequently employs a
frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing
the expected reward over the posterior distribution. We provide theoretical
guarantees of BOF-UCB's performance and demonstrate its effectiveness in
balancing exploration and exploitation on synthetic datasets and classical
control tasks in a reinforcement learning setting. Our results show that
BOF-UCB outperforms existing methods, making it a promising solution for
sequential decision-making in non-stationary environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04228">Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v2 [physics.geo-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Meles_G/0/1/0/all/0/1">Giovanni Angelo Meles</a>, <a href="http://arxiv.org/find/physics/1/au:+Amaya_M/0/1/0/all/0/1">Macarena Amaya</a>, <a href="http://arxiv.org/find/physics/1/au:+Levy_S/0/1/0/all/0/1">Shiran Levy</a>, <a href="http://arxiv.org/find/physics/1/au:+Marelli_S/0/1/0/all/0/1">Stefano Marelli</a>, <a href="http://arxiv.org/find/physics/1/au:+Linde_N/0/1/0/all/0/1">Niklas Linde</a></p>
<p>Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental
challenges: the accurate characterization of the prior distribution and the
efficient evaluation of the likelihood. In the context of Bayesian studies on
tomography, principal component analysis (PCA) can in some cases facilitate the
straightforward definition of the prior distribution, while simultaneously
enabling the implementation of accurate surrogate models based on polynomial
chaos expansion (PCE) to replace computationally intensive full-physics forward
solvers. When faced with scenarios where PCA does not offer a direct means of
easily defining the prior distribution alternative methods like deep generative
models (e.g., variational autoencoders (VAEs)), can be employed as viable
options. However, accurately producing a surrogate capable of capturing the
intricate non-linear relationship between the latent parameters of a VAE and
the outputs of forward modeling presents a notable challenge. Indeed, while PCE
models provide high accuracy when the input-output relationship can be
effectively approximated by relatively low-degree multivariate polynomials,
this condition is typically unmet when utilizing latent variables derived from
deep generative models. In this contribution, we present a strategy that
combines the excellent reconstruction performances of VAE in terms of prio
representation with the accuracy of PCA-PCE surrogate modeling in the context
of Bayesian ground penetrating radar (GPR) travel-time tomography. Within the
MCMC process, the parametrization of the VAE is leveraged for prior exploration
and sample proposal. Concurrently, modeling is conducted using PCE, which
operates on either globally or locally defined principal components of the VAE
samples under examination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04603">Solvent: A Framework for Protein Folding. (arXiv:2307.04603v3 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Lee_J/0/1/0/all/0/1">Jaemyung Lee</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Han_K/0/1/0/all/0/1">Kyeongtak Han</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kim_J/0/1/0/all/0/1">Jaehoon Kim</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yu_H/0/1/0/all/0/1">Hasun Yu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Lee_Y/0/1/0/all/0/1">Youhan Lee</a></p>
<p>Consistency and reliability are crucial for conducting AI research. Many
famous research fields, such as object detection, have been compared and
validated with solid benchmark frameworks. After AlphaFold2, the protein
folding task has entered a new phase, and many methods are proposed based on
the component of AlphaFold2. The importance of a unified research framework in
protein folding contains implementations and benchmarks to consistently and
fairly compare various approaches. To achieve this, we present Solvent, an
protein folding framework that supports significant components of
state-of-th-arts models in the manner of off-the-shelf interface Solvent
contains different models implemented in a unified codebase and supports
training and evaluation for defined models on the same dataset. We benchmark
well-known algorithms and their components and provide experiments that give
helpful insights into the protein structure modeling field. We hope that
Solvent will increase the reliability and consistency of proposed models and
gives efficiency in both speed and costs, resulting in acceleration on protein
folding modeling research. The code is available at
https://github.com/kakaobrain/solvent, and the project will continue to be
developed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04639">Multimodal brain age estimation using interpretable adaptive population-graph learning. (arXiv:2307.04639v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1">Kyriaki-Margarita Bintsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1">Vasileios Baltatzis</a>, <a href="http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1">Rolandos Alexandros Potamias</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammers_A/0/1/0/all/0/1">Alexander Hammers</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a></p>
<p>Brain age estimation is clinically important as it can provide valuable
information in the context of neurodegenerative diseases such as Alzheimer's.
Population graphs, which include multimodal imaging information of the subjects
along with the relationships among the population, have been used in literature
along with Graph Convolutional Networks (GCNs) and have proved beneficial for a
variety of medical imaging tasks. A population graph is usually static and
constructed manually using non-imaging information. However, graph construction
is not a trivial task and might significantly affect the performance of the
GCN, which is inherently very sensitive to the graph structure. In this work,
we propose a framework that learns a population graph structure optimized for
the downstream task. An attention mechanism assigns weights to a set of imaging
and non-imaging features (phenotypes), which are then used for edge extraction.
The resulting graph is used to train the GCN. The entire pipeline can be
trained end-to-end. Additionally, by visualizing the attention weights that
were the most important for the graph construction, we increase the
interpretability of the graph. We use the UK Biobank, which provides a large
variety of neuroimaging and non-imaging phenotypes, to evaluate our method on
brain age regression and classification. The proposed method outperforms
competing static graph approaches and other state-of-the-art adaptive methods.
We further show that the assigned attention scores indicate that there are both
imaging and non-imaging phenotypes that are informative for brain age
estimation and are in agreement with the relevant literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04838">CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Subramanyam_R/0/1/0/all/0/1">Rakshith Subramanyam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayram_T/0/1/0/all/0/1">T. S. Jayram</a>, <a href="http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1">Rushil Anirudh</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1">Jayaraman J. Thiagarajan</a></p>
<p>In this paper, we explore the potential of Vision-Language Models (VLMs),
specifically CLIP, in predicting visual object relationships, which involves
interpreting visual features from images into language-based relations. Current
state-of-the-art methods use complex graphical models that utilize language
cues and visual features to address this challenge. We hypothesize that the
strong language priors in CLIP embeddings can simplify these graphical models
paving for a simpler approach. We adopt the UVTransE relation prediction
framework, which learns the relation as a translational embedding with subject,
object, and union box embeddings from a scene. We systematically explore the
design of CLIP-based subject, object, and union-box representations within the
UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate
Estimation). CREPE utilizes text-based representations for all three bounding
boxes and introduces a novel contrastive training strategy to automatically
infer the text prompt for union-box. Our approach achieves state-of-the-art
performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual
Genome benchmark, achieving a 15.3\% gain in performance over recent
state-of-the-art at mR@20. This work demonstrates CLIP's effectiveness in
object relation prediction and encourages further research on VLMs in this
challenging domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04988">Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1">Chris Chinenye Emezue</a>, <a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1">Alexandre Drouin</a>, <a href="http://arxiv.org/find/cs/1/au:+Deleu_T/0/1/0/all/0/1">Tristan Deleu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1">Stefan Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>The practical utility of causality in decision-making is widespread and
brought about by the intertwining of causal discovery and causal inference.
Nevertheless, a notable gap exists in the evaluation of causal discovery
methods, where insufficient emphasis is placed on downstream inference. To
address this gap, we evaluate seven established baseline causal discovery
methods including a newly proposed method based on GFlowNets, on the downstream
task of treatment effect estimation. Through the implementation of a
distribution-level evaluation, we offer valuable and unique insights into the
efficacy of these causal discovery methods for treatment effect estimation,
considering both synthetic and real-world scenarios, as well as low-data
scenarios. The results of our study demonstrate that some of the algorithms
studied are able to effectively capture a wide range of useful and diverse ATE
modes, while some tend to learn many low-probability modes which impacts the
(unrelaxed) recall and precision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06333">Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1">Andi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Netanyahu_A/0/1/0/all/0/1">Aviv Netanyahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1">Mark Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1">Tianmin Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bobu_A/0/1/0/all/0/1">Andreea Bobu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1">Julie Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Pulkit Agrawal</a></p>
<p>Policies often fail due to distribution shift -- changes in the state and
reward that occur when a policy is deployed in new environments. Data
augmentation can increase robustness by making the model invariant to
task-irrelevant changes in the agent's observation. However, designers don't
know which concepts are irrelevant a priori, especially when different end
users have different preferences about how the task is performed. We propose an
interactive framework to leverage feedback directly from the user to identify
personalized task-irrelevant concepts. Our key idea is to generate
counterfactual demonstrations that allow users to quickly identify possible
task-relevant and irrelevant concepts. The knowledge of task-irrelevant
concepts is then used to perform data augmentation and thus obtain a policy
adapted to personalized user objectives. We present experiments validating our
framework on discrete and continuous control tasks with real human users. Our
method (1) enables users to better understand agent failure, (2) reduces the
number of demonstrations required for fine-tuning, and (3) aligns the agent to
individual user task preferences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06385">Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_K/0/1/0/all/0/1">Kalyan Ramakrishnan</a></p>
<p>Audio-Visual Event Localization (AVEL) is the task of temporally localizing
and classifying \emph{audio-visual events}, i.e., events simultaneously visible
and audible in a video. In this paper, we solve AVEL in a weakly-supervised
setting, where only video-level event labels (their presence/absence, but not
their locations in time) are available as supervision for training. Our idea is
to use a base model to estimate labels on the training data at a finer temporal
resolution than at the video level and re-train the model with these labels.
I.e., we determine the subset of labels for each \emph{slice} of frames in a
training video by (i) replacing the frames outside the slice with those from a
second video having no overlap in video-level labels, and (ii) feeding this
synthetic video into the base model to extract labels for just the slice in
question. To handle the out-of-distribution nature of our synthetic videos, we
propose an auxiliary objective for the base model that induces more reliable
predictions of the localized event labels as desired. Our three-stage pipeline
outperforms several existing AVEL methods with no architectural changes and
improves performance on a related weakly-supervised task as well.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06608">Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1">Jitao Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1">Qi Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Changsheng Xu</a></p>
<p>Recently, the no-box adversarial attack, in which the attacker lacks access
to the model's architecture, weights, and training data, become the most
practical and challenging attack setup. However, there is an unawareness of the
potential and flexibility inherent in the surrogate model selection process on
no-box setting. Inspired by the burgeoning interest in utilizing foundational
models to address downstream tasks, this paper adopts an innovative idea that
1) recasting adversarial attack as a downstream task. Specifically, image noise
generation to meet the emerging trend and 2) introducing foundational models as
surrogate models. Harnessing the concept of non-robust features, we elaborate
on two guiding principles for surrogate model selection to explain why the
foundational model is an optimal choice for this role. However, paradoxically,
we observe that these foundational models underperform. Analyzing this
unexpected behavior within the feature space, we attribute the lackluster
performance of foundational models (e.g., CLIP) to their significant
representational capacity and, conversely, their lack of discriminative
prowess. To mitigate this issue, we propose the use of a margin-based loss
strategy for the fine-tuning of foundational models on target images. The
experimental results verify that our approach, which employs the basic Fast
Gradient Sign Method (FGSM) attack algorithm, outstrips the performance of
other, more convoluted algorithms. We conclude by advocating for the research
community to consider surrogate models as crucial determinants in the
effectiveness of adversarial attacks in no-box settings. The implications of
our work bear relevance for improving the efficacy of such adversarial attacks
and the overall robustness of AI systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06698">IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thanapalasingam_T/0/1/0/all/0/1">Thiviyan Thanapalasingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Krieken_E/0/1/0/all/0/1">Emile van Krieken</a>, <a href="http://arxiv.org/find/cs/1/au:+Bloem_P/0/1/0/all/0/1">Peter Bloem</a>, <a href="http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1">Paul Groth</a></p>
<p>Knowledge Graph Embedding (KGE) models are used to learn continuous
representations of entities and relations. A key task in the literature is
predicting missing links between entities. However, Knowledge Graphs are not
just sets of links but also have semantics underlying their structure.
Semantics is crucial in several downstream tasks, such as query answering or
reasoning. We introduce the subgraph inference task, where a model has to
generate likely and semantically valid subgraphs. We propose IntelliGraphs, a
set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain
subgraphs with semantics expressed in logical rules for evaluating subgraph
inference. We also present the dataset generator that produced the synthetic
datasets. We designed four novel baseline models, which include three models
based on traditional KGEs. We evaluate their expressiveness and show that these
models cannot capture the semantics. We believe this benchmark will encourage
the development of machine learning models that emphasize semantic
understanding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06975">Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Capogrosso_L/0/1/0/all/0/1">Luigi Capogrosso</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascolini_A/0/1/0/all/0/1">Alessio Mascolini</a>, <a href="http://arxiv.org/find/cs/1/au:+Girella_F/0/1/0/all/0/1">Federico Girella</a>, <a href="http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1">Geri Skenderi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaiardelli_S/0/1/0/all/0/1">Sebastiano Gaiardelli</a>, <a href="http://arxiv.org/find/cs/1/au:+DallOra_N/0/1/0/all/0/1">Nicola Dall&#x27;Ora</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponzio_F/0/1/0/all/0/1">Francesco Ponzio</a>, <a href="http://arxiv.org/find/cs/1/au:+Fraccaroli_E/0/1/0/all/0/1">Enrico Fraccaroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Cataldo_S/0/1/0/all/0/1">Santa Di Cataldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinco_S/0/1/0/all/0/1">Sara Vinco</a>, <a href="http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1">Enrico Macii</a>, <a href="http://arxiv.org/find/cs/1/au:+Fummi_F/0/1/0/all/0/1">Franco Fummi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1">Marco Cristani</a></p>
<p>Industry 4.0 involves the integration of digital technologies, such as IoT,
Big Data, and AI, into manufacturing and industrial processes to increase
efficiency and productivity. As these technologies become more interconnected
and interdependent, Industry 4.0 systems become more complex, which brings the
difficulty of identifying and stopping anomalies that may cause disturbances in
the manufacturing process. This paper aims to propose a diffusion-based model
for real-time anomaly prediction in Industry 4.0 processes. Using a
neuro-symbolic approach, we integrate industrial ontologies in the model,
thereby adding formal knowledge on smart manufacturing. Finally, we propose a
simple yet effective way of distilling diffusion models through Random Fourier
Features for deployment on an embedded system for direct integration into the
manufacturing process. To the best of our knowledge, this approach has never
been explored before.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07873">Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yechao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Shengshan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Leo Yu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Junyu Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minghui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaogeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1">Wei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hai Jin</a></p>
<p>Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08347">M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Che Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Sibo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1">Mengyun Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weitong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Anand Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1">Wenjia Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1">Rossella Arcucci</a></p>
<p>Medical vision-language models enable co-learning and integrating features
from medical imaging and clinical text. However, these models are not easy to
train and the latent representation space can be complex. Here we propose a
novel way for pre-training and regularising medical vision-language models. The
proposed method, named Medical vision-language pre-training with Frozen
language models and Latent spAce Geometry optimization (M-FLAG), leverages a
frozen language model for training stability and efficiency and introduces a
novel orthogonality loss to harmonize the latent space geometry. We demonstrate
the potential of the pre-trained model on three downstream tasks: medical image
classification, segmentation, and object detection. Extensive experiments
across five public datasets demonstrate that M-FLAG significantly outperforms
existing medical vision-language pre-training approaches and reduces the number
of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the
segmentation task while using only 1\% of the RSNA dataset, even outperforming
ImageNet pre-trained models that have been fine-tuned using 100\% of the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08621">Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yutao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Li Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shaohan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shuming Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yuqing Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jilong Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianyong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a></p>
<p>In this work, we propose Retentive Network (RetNet) as a foundation
architecture for large language models, simultaneously achieving training
parallelism, low-cost inference, and good performance. We theoretically derive
the connection between recurrence and attention. Then we propose the retention
mechanism for sequence modeling, which supports three computation paradigms,
i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel
representation allows for training parallelism. The recurrent representation
enables low-cost $O(1)$ inference, which improves decoding throughput, latency,
and GPU memory without sacrificing performance. The chunkwise recurrent
representation facilitates efficient long-sequence modeling with linear
complexity, where each chunk is encoded parallelly while recurrently
summarizing the chunks. Experimental results on language modeling show that
RetNet achieves favorable scaling results, parallel training, low-cost
deployment, and efficient inference. The intriguing properties make RetNet a
strong successor to Transformer for large language models. Code will be
available at https://aka.ms/retnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08913">Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Zeen Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xingzhe Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1">Wenwen Qiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a></p>
<p>In recent years, self-supervised learning (SSL) has emerged as a promising
approach for extracting valuable representations from unlabeled data. One
successful SSL method is contrastive learning, which aims to bring positive
examples closer while pushing negative examples apart. Many current contrastive
learning approaches utilize a parameterized projection head. Through a
combination of empirical analysis and theoretical investigation, we provide
insights into the internal mechanisms of the projection head and its
relationship with the phenomenon of dimensional collapse. Our findings
demonstrate that the projection head enhances the quality of representations by
performing contrastive loss in a projected subspace. Therefore, we propose an
assumption that only a subset of features is necessary when minimizing the
contrastive loss of a mini-batch of data. Theoretical analysis further suggests
that a sparse projection head can enhance generalization, leading us to
introduce SparseHead - a regularization term that effectively constrains the
sparsity of the projection head, and can be seamlessly integrated with any
self-supervised learning (SSL) approaches. Our experimental results validate
the effectiveness of SparseHead, demonstrating its ability to improve the
performance of existing contrastive methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09191">A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Matteucci_F/0/1/0/all/0/1">Federico Matteucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Arzamasov_V/0/1/0/all/0/1">Vadim Arzamasov</a>, <a href="http://arxiv.org/find/cs/1/au:+Boehm_K/0/1/0/all/0/1">Klemens Boehm</a></p>
<p>Categorical encoders transform categorical features into numerical
representations that are indispensable for a wide range of machine learning
models. Existing encoder benchmark studies lack generalizability because of
their limited choice of (1) encoders, (2) experimental factors, and (3)
datasets. Additionally, inconsistencies arise from the adoption of varying
aggregation strategies. This paper is the most comprehensive benchmark of
categorical encoders to date, including an extensive evaluation of 32
configurations of encoders from diverse families, with 36 combinations of
experimental factors, and on 50 datasets. The study shows the profound
influence of dataset selection, experimental factors, and aggregation
strategies on the benchmark's conclusions -- aspects disregarded in previous
encoder benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09458">Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lieberum_T/0/1/0/all/0/1">Tom Lieberum</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahtz_M/0/1/0/all/0/1">Matthew Rahtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kramar_J/0/1/0/all/0/1">J&#xe1;nos Kram&#xe1;r</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1">Neel Nanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1">Geoffrey Irving</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rohin Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikulik_V/0/1/0/all/0/1">Vladimir Mikulik</a></p>
<p>\emph{Circuit analysis} is a promising technique for understanding the
internal mechanisms of language models. However, existing analyses are done in
small models far from the state of the art. To address this, we present a case
study of circuit analysis in the 70B Chinchilla model, aiming to test the
scalability of circuit analysis. In particular, we study multiple-choice
question answering, and investigate Chinchilla's capability to identify the
correct answer \emph{label} given knowledge of the correct answer \emph{text}.
We find that the existing techniques of logit attribution, attention pattern
visualization, and activation patching naturally scale to Chinchilla, allowing
us to identify and categorize a small set of `output nodes' (attention heads
and MLPs).
</p>
<p>We further study the `correct letter' category of attention heads aiming to
understand the semantics of their features, with mixed results. For normal
multiple-choice question answers, we significantly compress the query, key and
value subspaces of the head without loss of performance when operating on the
answer labels for multiple-choice questions, and we show that the query and key
subspaces represent an `Nth item in an enumeration' feature to at least some
extent. However, when we attempt to use this explanation to understand the
heads' behaviour on a more general distribution including randomized answer
labels, we find that it is only a partial explanation, suggesting there is more
to learn about the operation of `correct letter' heads on multiple choice
question answering.
</p>
</p>
</div>

    </div>
    </body>
    