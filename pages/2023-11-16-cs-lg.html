<!DOCTYPE html>
<html>
<head>
<title>2023-11-16-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.07578">A Metacognitive Approach to Out-of-Distribution Detection for Segmentation. (arXiv:2311.07578v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gummadi_M/0/1/0/all/0/1">Meghna Gummadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kent_C/0/1/0/all/0/1">Cassandra Kent</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1">Karl Schmeckpeper</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1">Eric Eaton</a></p>
<p>Despite outstanding semantic scene segmentation in closed-worlds, deep neural
networks segment novel instances poorly, which is required for autonomous
agents acting in an open world. To improve out-of-distribution (OOD) detection
for segmentation, we introduce a metacognitive approach in the form of a
lightweight module that leverages entropy measures, segmentation predictions,
and spatial context to characterize the segmentation model's uncertainty and
detect pixel-wise OOD data in real-time. Additionally, our approach
incorporates a novel method of generating synthetic OOD data in context with
in-distribution data, which we use to fine-tune existing segmentation models
with maximum entropy training. This further improves the metacognitive module's
performance without requiring access to OOD data while enabling compatibility
with established pre-trained models. Our resulting approach can reliably detect
OOD instances in a scene, as shown by state-of-the-art performance on OOD
detection for semantic segmentation benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07579">Relative intrinsic dimensionality is intrinsic to learning. (arXiv:2311.07579v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sutton_O/0/1/0/all/0/1">Oliver J. Sutton</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qinghua Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1">Alexander N. Gorban</a>, <a href="http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1">Ivan Y. Tyukin</a></p>
<p>High dimensional data can have a surprising property: pairs of data points
may be easily separated from each other, or even from arbitrary subsets, with
high probability using just simple linear classifiers. However, this is more of
a rule of thumb than a reliable property as high dimensionality alone is
neither necessary nor sufficient for successful learning. Here, we introduce a
new notion of the intrinsic dimension of a data distribution, which precisely
captures the separability properties of the data. For this intrinsic dimension,
the rule of thumb above becomes a law: high intrinsic dimension guarantees
highly separable data. We extend this notion to that of the relative intrinsic
dimension of two data distributions, which we show provides both upper and
lower bounds on the probability of successfully learning and generalising in a
binary classification problem
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07584">Performance Prediction of Data-Driven Knowledge summarization of High Entropy Alloys (HEAs) literature implementing Natural Language Processing algorithms. (arXiv:2311.07584v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Akshansh Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatti_V/0/1/0/all/0/1">Vijaykumar S Jatti</a>, <a href="http://arxiv.org/find/cs/1/au:+More_V/0/1/0/all/0/1">Vaishnavi More</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Anish Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixit_D/0/1/0/all/0/1">Devarrishi Dixit</a>, <a href="http://arxiv.org/find/cs/1/au:+Sefene_E/0/1/0/all/0/1">Eyob Messele Sefene</a></p>
<p>The ability to interpret spoken language is connected to natural language
processing. It involves teaching the AI how words relate to one another, how
they are meant to be used, and in what settings. The goal of natural language
processing (NLP) is to get a machine intelligence to process words the same way
a human brain does. This enables machine intelligence to interpret, arrange,
and comprehend textual data by processing the natural language. The technology
can comprehend what is communicated, whether it be through speech or writing
because AI pro-cesses language more quickly than humans can. In the present
study, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic
Analysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the
first time for the knowledge summarization purpose of the High Entropy Alloys
(HEAs). The performance prediction of these algorithms is made by using the
BLEU score and ROUGE score. The results showed that the Luhn algorithm has the
highest accuracy score for the knowledge summarization tasks compared to the
other used algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07587">Frontier Language Models are not Robust to Adversarial Arithmetic, or &quot;What do I need to say so you agree 2+2=5?. (arXiv:2311.07587v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freeman_C/0/1/0/all/0/1">C. Daniel Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1">Laura Culp</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1">Aaron Parisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bileschi_M/0/1/0/all/0/1">Maxwell L Bileschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1">Gamaleldin F Elsayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizkowsky_A/0/1/0/all/0/1">Alex Rizkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1">Isabelle Simpson</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1">Alex Alemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1">Azade Nova</a>, <a href="http://arxiv.org/find/cs/1/au:+Adlam_B/0/1/0/all/0/1">Ben Adlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1">Bernd Bohnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1">Gaurav Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1">Izzeddin Gur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaehoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1">JD Co-Reyes</a>, <a href="http://arxiv.org/find/cs/1/au:+Pennington_J/0/1/0/all/0/1">Jeffrey Pennington</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kelvin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1">Kevin Swersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1">Kshiteej Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lechao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rosanne Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1">Noah Constant</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1">Roman Novak</a>, <a href="http://arxiv.org/find/cs/1/au:+Vikram_S/0/1/0/all/0/1">Sharad Vikram</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yundi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1">Noah Fiedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1">Jascha Sohl-Dickstein</a></p>
<p>We introduce and study the problem of adversarial arithmetic, which provides
a simple yet challenging testbed for language model alignment. This problem is
comprised of arithmetic questions posed in natural language, with an arbitrary
adversarial string inserted before the question is complete. Even in the simple
setting of 1-digit addition problems, it is easy to find adversarial prompts
that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and
even to steer models to a particular wrong answer. We additionally provide a
simple algorithm for finding successful attacks by querying those same models,
which we name "prompt inversion rejection sampling" (PIRS). We finally show
that models can be partially hardened against these attacks via reinforcement
learning and via agentic constitutional loops. However, we were not able to
make a language model fully robust against adversarial arithmetic attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07590">Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. (arXiv:2311.07590v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Scheurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1">Mikita Balesni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1">Marius Hobbhahn</a></p>
<p>We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07591">Identification of Books That are Suitable for Middle School Students Using Artificial Neural Networks. (arXiv:2311.07591v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niksarli_A/0/1/0/all/0/1">Alp Niksarli</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorgu_S/0/1/0/all/0/1">Sadik Ozan Gorgu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gencer_E/0/1/0/all/0/1">Ege Gencer</a></p>
<p>Reading right books contributes to children's imagination and brain
development, enhances their language and emotional comprehension abilities, and
strengthens their relationships with others. Building upon the critical role of
reading books in individual development, this paper aims to develop an
algorithm that determines the suitability of books for middle school students
by analyzing their structural and semantic features. Using methods described,
an algorithm will be created that can be utilized by institutions and
individuals responsible for children's education, such as the Ministry of
National Education officials and schools. This algorithm will facilitate the
selection of books to be taught at the middle school level. With the algorithm,
the book selection process for the middle school curriculum can be expedited,
and it will serve as a preliminary reference source for those who evaluate
books by reading them. In this paper, the Python programming language was
employed, utilizing natural language processing methods. Additionally, an
artificial neural network (ANN) was trained using the data which had been
preprocessed to construct an original dataset. To train this network, suitable
books for middle school students were provided by the MEB, Oxford and Cambridge
and with content assessed based on the "R" criterion, and inappropriate books
for middle school students in terms of content were included. This trained
neural network achieved a 90.06% consistency rate in determining the
appropriateness of the test-provided books. Considering the obtained findings,
it can be concluded that the developed software has achieved the desired
objective.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07593">Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification. (arXiv:2311.07593v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1">Reza Esfandiarpoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1">Stephen H. Bach</a></p>
<p>A promising approach for improving the performance of vision-language models
like CLIP for image classification is to extend the class descriptions (i.e.,
prompts) with related attributes, e.g., using brown sparrow instead of sparrow.
However, current zero-shot methods select a subset of attributes regardless of
commonalities between the target classes, potentially providing no useful
information that would have helped to distinguish between them. For instance,
they may use color instead of bill shape to distinguish between sparrows and
wrens, which are both brown. We propose Follow-up Differential Descriptions
(FuDD), a zero-shot approach that tailors the class descriptions to each
dataset and leads to additional attributes that better differentiate the target
classes. FuDD first identifies the ambiguous classes for each image, and then
uses a Large Language Model (LLM) to generate new class descriptions that
differentiate between them. The new class descriptions resolve the initial
ambiguity and help predict the correct label. In our experiments, FuDD
consistently outperforms generic description ensembles and naive LLM-generated
descriptions on 12 datasets. We show that differential descriptions are an
effective tool to resolve class ambiguities, which otherwise significantly
degrade the performance. We also show that high quality natural language class
descriptions produced by FuDD result in comparable performance to few-shot
adaptation methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07595">A Decision Support System for Liver Diseases Prediction: Integrating Batch Processing, Rule-Based Event Detection and SPARQL Query. (arXiv:2311.07595v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1">Ritesh Chandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1">Sadhana Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastogi_S/0/1/0/all/0/1">Satyam Rastogi</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Sonali Agarwal</a></p>
<p>Liver diseases pose a significant global health burden, impacting a
substantial number of individuals and exerting substantial economic and social
consequences. Rising liver problems are considered a fatal disease in many
countries, such as Egypt, Molda, etc. The objective of this study is to
construct a predictive model for liver illness using Basic Formal Ontology
(BFO) and detection rules derived from a decision tree algorithm. Based on
these rules, events are detected through batch processing using the Apache Jena
framework. Based on the event detected, queries can be directly processed using
SPARQL. To make the ontology operational, these Decision Tree (DT) rules are
converted into Semantic Web Rule Language (SWRL). Using this SWRL in the
ontology for predicting different types of liver disease with the help of the
Pellet and Drool inference engines in Protege Tools, a total of 615 records are
taken from different liver diseases. After inferring the rules, the result can
be generated for the patient according to the DT rules, and other
patient-related details along with different precautionary suggestions can be
obtained based on these results. Combining query results of batch processing
and ontology-generated results can give more accurate suggestions for disease
prevention and detection. This work aims to provide a comprehensive approach
that is applicable for liver disease prediction, rich knowledge graph
representation, and smart querying capabilities. The results show that
combining RDF data, SWRL rules, and SPARQL queries for analysing and predicting
liver disease can help medical professionals to learn more about liver diseases
and make a Decision Support System (DSS) for health care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07596">Graph GOSPA metric: a metric to measure the discrepancy between graphs of different sizes. (arXiv:2311.07596v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinhao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcia_Fernandez_A/0/1/0/all/0/1">&#xc1;ngel F. Garc&#xed;a-Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/cs/1/au:+Firth_R/0/1/0/all/0/1">Robert E. Firth</a>, <a href="http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1">Lennart Svensson</a></p>
<p>This paper proposes a metric to measure the dissimilarity between graphs that
may have a different number of nodes. The proposed metric extends the
generalised optimal subpattern assignment (GOSPA) metric, which is a metric for
sets, to graphs. The proposed graph GOSPA metric includes costs associated with
node attribute errors for properly assigned nodes, missed and false nodes and
edge mismatches between graphs. The computation of this metric is based on
finding the optimal assignments between nodes in the two graphs, with the
possibility of leaving some of the nodes unassigned. We also propose a lower
bound for the metric, which is also a metric for graphs and is computable in
polynomial time using linear programming. The metric is first derived for
undirected unweighted graphs and it is then extended to directed and weighted
graphs. The properties of the metric are demonstrated via simulated and
empirical datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07597">Enhancing Actuarial Non-Life Pricing Models via Transformers. (arXiv:2311.07597v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brauer_A/0/1/0/all/0/1">Alexej Brauer</a></p>
<p>Currently, there is a lot of research in the field of neural networks for
non-life insurance pricing. The usual goal is to improve the predictive power
via neural networks while building upon the generalized linear model, which is
the current industry standard. Our paper contributes to this current journey
via novel methods to enhance actuarial non-life models with transformer models
for tabular data. We build here upon the foundation laid out by the combined
actuarial neural network as well as the localGLMnet and enhance those models
via the feature tokenizer transformer. The manuscript demonstrates the
performance of the proposed methods on a real-world claim frequency dataset and
compares them with several benchmark models such as generalized linear models,
feed-forward neural networks, combined actuarial neural networks, LocalGLMnet,
and pure feature tokenizer transformer. The paper shows that the new methods
can achieve better results than the benchmark models while preserving certain
generalized linear model advantages. The paper also discusses the practical
implications and challenges of applying transformer models in actuarial
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07598">Multi-Label Topic Model for Financial Textual Data. (arXiv:2311.07598v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Scherrmann_M/0/1/0/all/0/1">Moritz Scherrmann</a></p>
<p>This paper presents a multi-label topic model for financial texts like ad-hoc
announcements, 8-K filings, finance related news or annual reports. I train the
model on a new financial multi-label database consisting of 3,044 German ad-hoc
announcements that are labeled manually using 20 predefined, economically
motivated topics. The best model achieves a macro F1 score of more than 85%.
Translating the data results in an English version of the model with similar
performance. As application of the model, I investigate differences in stock
market reactions across topics. I find evidence for strong positive or negative
market reactions for some topics, like announcements of new Large Scale
Projects or Bankruptcy Filings, while I do not observe significant price
effects for some other topics. Furthermore, in contrast to previous studies,
the multi-label structure of the model allows to analyze the effects of
co-occurring topics on stock market reactions. For many cases, the reaction to
a specific topic depends heavily on the co-occurrence with other topics. For
example, if allocated capital from a Seasoned Equity Offering (SEO) is used for
restructuring a company in the course of a Bankruptcy Proceeding, the market
reacts positively on average. However, if that capital is used for covering
unexpected, additional costs from the development of new drugs, the SEO implies
negative reactions on average.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07604">Finetuning Text-to-Image Diffusion Models for Fairness. (arXiv:2311.07604v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xudong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Chao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1">Tianyu Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1">Yongkang Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1">Mohan Kankanhalli</a></p>
<p>The rapid adoption of text-to-image diffusion models in society underscores
an urgent need to address their biases. Without interventions, these biases
could propagate a distorted worldview and limit opportunities for minority
groups. In this work, we frame fairness as a distributional alignment problem.
Our solution consists of two main technical contributions: (1) a distributional
alignment loss that steers specific characteristics of the generated images
towards a user-defined target distribution, and (2) biased direct finetuning of
diffusion model's sampling process, which leverages a biased gradient to more
effectively optimize losses defined on the generated images. Empirically, our
method markedly reduces gender, racial, and their intersectional biases for
occupational prompts. Gender bias is significantly reduced even when finetuning
just five soft tokens. Crucially, our method supports diverse perspectives of
fairness beyond absolute equality, which is demonstrated by controlling age to
a $75\%$ young and $25\%$ old distribution while simultaneously debiasing
gender and race. Finally, our method is scalable: it can debias multiple
concepts at once by simply including these prompts in the finetuning data. We
hope our work facilitates the social alignment of T2I generative AI. We will
share code and various debiased diffusion model adaptors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07607">Modeling Choice via Self-Attention. (arXiv:2311.07607v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1">Joohwan Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Andrew A. Li</a></p>
<p>Models of choice are a fundamental input to many now-canonical optimization
problems in the field of Operations Management, including assortment,
inventory, and price optimization. Naturally, accurate estimation of these
models from data is a critical step in the application of these optimization
problems in practice, and so it is perhaps surprising that such choice
estimation has to now been accomplished almost exclusively, both in theory and
in practice, (a) without the use of deep learning in any meaningful way, and
(b) via evaluation on limited data with constantly-changing metrics. This is in
stark contrast to the vast majority of similar learning applications, for which
the practice of machine learning suggests that (a) neural network-based models
are typically state-of-the-art, and (b) strict standardization on evaluation
procedures (datasets, metrics, etc.) is crucial. Thus motivated, we first
propose a choice model that is the first to successfully (both theoretically
and practically) leverage a modern neural network architectural concept
(self-attention). Theoretically, we show that our attention-based choice model
is a low-rank generalization of the Halo Multinomial Logit model, a recent
model that parsimoniously captures irrational choice effects and has seen
empirical success. We prove that whereas the Halo-MNL requires $\Omega(m^2)$
data samples to estimate, where $m$ is the number of products, our model
supports a natural nonconvex estimator (in particular, that which a standard
neural network implementation would apply) which admits a near-optimal
stationary point with $O(m)$ samples. We then establish the first
realistic-scale benchmark for choice estimation on real data and use this
benchmark to run the largest evaluation of existing choice models to date. We
find that the model we propose is dominant over both short-term and long-term
data periods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07608">MuST: Multimodal Spatiotemporal Graph-Transformer for Hospital Readmission Prediction. (arXiv:2311.07608v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1">Yan Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lequan Yu</a></p>
<p>Hospital readmission prediction is considered an essential approach to
decreasing readmission rates, which is a key factor in assessing the quality
and efficacy of a healthcare system. Previous studies have extensively utilized
three primary modalities, namely electronic health records (EHR), medical
images, and clinical notes, to predict hospital readmissions. However, the
majority of these studies did not integrate information from all three
modalities or utilize the spatiotemporal relationships present in the dataset.
This study introduces a novel model called the Multimodal Spatiotemporal
Graph-Transformer (MuST) for predicting hospital readmissions. By employing
Graph Convolution Networks and temporal transformers, we can effectively
capture spatial and temporal dependencies in EHR and chest radiographs. We then
propose a fusion transformer to combine the spatiotemporal features from the
two modalities mentioned above with the features from clinical notes extracted
by a pre-trained, domain-specific transformer. We assess the effectiveness of
our methods using the latest publicly available dataset, MIMIC-IV. The
experimental results indicate that the inclusion of multimodal features in MuST
improves its performance in comparison to unimodal methods. Furthermore, our
proposed pipeline outperforms the current leading methods in the prediction of
hospital readmissions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07613">A Physics-informed Machine Learning-based Control Method for Nonlinear Dynamic Systems with Highly Noisy Measurements. (arXiv:2311.07613v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ma_M/0/1/0/all/0/1">Mason Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1">Jiajie Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Post_C/0/1/0/all/0/1">Chase Post</a>, <a href="http://arxiv.org/find/eess/1/au:+Shi_T/0/1/0/all/0/1">Tony Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Yi_J/0/1/0/all/0/1">Jingang Yi</a>, <a href="http://arxiv.org/find/eess/1/au:+Schmitz_T/0/1/0/all/0/1">Tony Schmitz</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1">Hong Wang</a></p>
<p>This study presents a physics-informed machine learning-based control method
for nonlinear dynamic systems with highly noisy measurements. Existing
data-driven control methods that use machine learning for system identification
cannot effectively cope with highly noisy measurements, resulting in unstable
control performance. To address this challenge, the present study extends
current physics-informed machine learning capabilities for modeling nonlinear
dynamics with control and integrates them into a model predictive control
framework. To demonstrate the capability of the proposed method we test and
validate with two noisy nonlinear dynamic systems: the chaotic Lorenz 3 system,
and turning machine tool. Analysis of the results illustrate that the proposed
method outperforms state-of-the-art benchmarks as measured by both modeling
accuracy and control performance for nonlinear dynamic systems under high-noise
conditions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07614">Application of a Dense Fusion Attention Network in Fault Diagnosis of Centrifugal Fan. (arXiv:2311.07614v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruijun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhixia Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaogang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huijie Wang</a></p>
<p>Although the deep learning recognition model has been widely used in the
condition monitoring of rotating machinery. However, it is still a challenge to
understand the correspondence between the structure and function of the model
and the diagnosis process. Therefore, this paper discusses embedding
distributed attention modules into dense connections instead of traditional
dense cascading operations. It not only decouples the influence of space and
channel on fault feature adaptive recalibration feature weights, but also forms
a fusion attention function. The proposed dense fusion focuses on the
visualization of the network diagnosis process, which increases the
interpretability of model diagnosis. How to continuously and effectively
integrate different functions to enhance the ability to extract fault features
and the ability to resist noise is answered. Centrifugal fan fault data is used
to verify this network. Experimental results show that the network has stronger
diagnostic performance than other advanced fault diagnostic models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07618">Large Language Models&#x27; Understanding of Math: Source Criticism and Extrapolation. (arXiv:2311.07618v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yousefzadeh_R/0/1/0/all/0/1">Roozbeh Yousefzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xuenan Cao</a></p>
<p>It has been suggested that large language models such as GPT-4 have acquired
some form of understanding beyond the correlations among the words in text
including some understanding of mathematics as well. Here, we perform a
critical inquiry into this claim by evaluating the mathematical understanding
of the GPT-4 model. Considering that GPT-4's training set is a secret, it is
not straightforward to evaluate whether the model's correct answers are based
on a mathematical understanding or based on replication of proofs that the
model has seen before. We specifically craft mathematical questions which their
formal proofs are not readily available on the web, proofs that are more likely
not seen by the GPT-4. We see that GPT-4 is unable to solve those problems
despite their simplicity. It is hard to find scientific evidence suggesting
that GPT-4 has acquired an understanding of even basic mathematical concepts. A
straightforward way to find failure modes of GPT-4 in theorem proving is to
craft questions where their formal proofs are not available on the web. Our
finding suggests that GPT-4's ability is to reproduce, rephrase, and polish the
mathematical proofs that it has seen before, and not in grasping mathematical
concepts. We also see that GPT-4's ability to prove mathematical theorems is
continuously expanding over time despite the claim that it is a fixed model. We
suggest that the task of proving mathematical theorems in formal language is
comparable to the methods used in search engines such as Google while
predicting the next word in a sentence may be a misguided approach, a recipe
that often leads to excessive extrapolation and eventual failures. Prompting
the GPT-4 over and over may benefit the GPT-4 and the OpenAI, but we question
whether it is valuable for machine learning or for theorem proving.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07620">EPIM: Efficient Processing-In-Memory Accelerators based on Epitome. (arXiv:2311.07620v1 [cs.AR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chenyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Daquan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhenhua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1">Kurt Keutzer</a></p>
<p>The exploration of Processing-In-Memory (PIM) accelerators has garnered
significant attention within the research community. However, the utilization
of large-scale neural networks on Processing-In-Memory (PIM) accelerators
encounters challenges due to constrained on-chip memory capacity. To tackle
this issue, current works explore model compression algorithms to reduce the
size of Convolutional Neural Networks (CNNs). Most of these algorithms either
aim to represent neural operators with reduced-size parameters (e.g.,
quantization) or search for the best combinations of neural operators (e.g.,
neural architecture search). Designing neural operators to align with PIM
accelerators' specifications is an area that warrants further study. In this
paper, we introduce the Epitome, a lightweight neural operator offering
convolution-like functionality, to craft memory-efficient CNN operators for PIM
accelerators (EPIM). On the software side, we evaluate epitomes' latency and
energy on PIM accelerators and introduce a PIM-aware layer-wise design method
to enhance their hardware efficiency. We apply epitome-aware quantization to
further reduce the size of epitomes. On the hardware side, we modify the
datapath of current PIM accelerators to accommodate epitomes and implement a
feature map reuse technique to reduce computation cost. Experimental results
reveal that our 3-bit quantized EPIM-ResNet50 attains 71.59% top-1 accuracy on
ImageNet, reducing crossbar areas by 30.65 times. EPIM surpasses the
state-of-the-art pruning methods on PIM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07621">To Transformers and Beyond: Large Language Models for the Genome. (arXiv:2311.07621v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Consens_M/0/1/0/all/0/1">Micaela E. Consens</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dufault_C/0/1/0/all/0/1">Cameron Dufault</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wainberg_M/0/1/0/all/0/1">Michael Wainberg</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Forster_D/0/1/0/all/0/1">Duncan Forster</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Karimzadeh_M/0/1/0/all/0/1">Mehran Karimzadeh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Goodarzi_H/0/1/0/all/0/1">Hani Goodarzi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Theis_F/0/1/0/all/0/1">Fabian J. Theis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Moses_A/0/1/0/all/0/1">Alan Moses</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a></p>
<p>In the rapidly evolving landscape of genomics, deep learning has emerged as a
useful tool for tackling complex computational challenges. This review focuses
on the transformative role of Large Language Models (LLMs), which are mostly
based on the transformer architecture, in genomics. Building on the foundation
of traditional convolutional neural networks and recurrent neural networks, we
explore both the strengths and limitations of transformers and other LLMs for
genomics. Additionally, we contemplate the future of genomic modeling beyond
the transformer architecture based on current trends in research. The paper
aims to serve as a guide for computational biologists and computer scientists
interested in LLMs for genomic data. We hope the paper can also serve as an
educational introduction and discussion for biologists to a fundamental shift
in how we will be analyzing genomic data in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07625">Activity Sparsity Complements Weight Sparsity for Efficient RNN Inference. (arXiv:2311.07625v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mukherji_R/0/1/0/all/0/1">Rishav Mukherji</a>, <a href="http://arxiv.org/find/cs/1/au:+Schone_M/0/1/0/all/0/1">Mark Sch&#xf6;ne</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazeer_K/0/1/0/all/0/1">Khaleelulla Khan Nazeer</a>, <a href="http://arxiv.org/find/cs/1/au:+Mayr_C/0/1/0/all/0/1">Christian Mayr</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramoney_A/0/1/0/all/0/1">Anand Subramoney</a></p>
<p>Artificial neural networks open up unprecedented machine learning
capabilities at the cost of ever growing computational requirements.
Sparsifying the parameters, often achieved through weight pruning, has been
identified as a powerful technique to compress the number of model parameters
and reduce the computational operations of neural networks. Yet, sparse
activations, while omnipresent in both biological neural networks and deep
learning systems, have not been fully utilized as a compression technique in
deep learning. Moreover, the interaction between sparse activations and weight
pruning is not fully understood. In this work, we demonstrate that activity
sparsity can compose multiplicatively with parameter sparsity in a recurrent
neural network model based on the GRU that is designed to be activity sparse.
We achieve up to $20\times$ reduction of computation while maintaining
perplexities below $60$ on the Penn Treebank language modeling task. This
magnitude of reduction has not been achieved previously with solely sparsely
connected LSTMs, and the language modeling performance of our model has not
been achieved previously with any sparsely activated recurrent neural networks
or spiking neural networks. Neuromorphic computing devices are especially good
at taking advantage of the dynamic activity sparsity, and our results provide
strong evidence that making deep learning models activity sparse and porting
them to neuromorphic devices can be a viable strategy that does not compromise
on task performance. Our results also drive further convergence of methods from
deep learning and neuromorphic computing for efficient machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07626">Quantum Machine Learning for Remote Sensing: Exploring potential and challenges. (arXiv:2311.07626v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Miroszewski_A/0/1/0/all/0/1">Artur Miroszewski</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nalepa_J/0/1/0/all/0/1">Jakub Nalepa</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Saux_B/0/1/0/all/0/1">Bertrand Le Saux</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Mielczarek_J/0/1/0/all/0/1">Jakub Mielczarek</a></p>
<p>The industry of quantum technologies is rapidly expanding, offering promising
opportunities for various scientific domains. Among these emerging
technologies, Quantum Machine Learning (QML) has attracted considerable
attention due to its potential to revolutionize data processing and analysis.
In this paper, we investigate the application of QML in the field of remote
sensing. It is believed that QML can provide valuable insights for analysis of
data from space. We delve into the common beliefs surrounding the quantum
advantage in QML for remote sensing and highlight the open challenges that need
to be addressed. To shed light on the challenges, we conduct a study focused on
the problem of kernel value concentration, a phenomenon that adversely affects
the runtime of quantum computers. Our findings indicate that while this issue
negatively impacts quantum computer performance, it does not entirely negate
the potential quantum advantage in QML for remote sensing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07627">A Consistent Diffusion-Based Algorithm for Semi-Supervised Graph Learning. (arXiv:2311.07627v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonald_T/0/1/0/all/0/1">Thomas Bonald</a> (IP Paris), <a href="http://arxiv.org/find/cs/1/au:+Lara_N/0/1/0/all/0/1">Nathan de Lara</a> (IP Paris)</p>
<p>The task of semi-supervised classification aims at assigning labels to all
nodes of a graph based on the labels known for a few nodes, called the seeds.
One of the most popular algorithms relies on the principle of heat diffusion,
where the labels of the seeds are spread by thermoconductance and the
temperature of each node at equilibrium is used as a score function for each
label. In this paper, we prove that this algorithm is not consistent unless the
temperatures of the nodes at equilibrium are centered before scoring. This
crucial step does not only make the algorithm provably consistent on a block
model but brings significant performance gains on real graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07630">Cross-modal Generative Model for Visual-Guided Binaural Stereo Generation. (arXiv:2311.07630v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhaojian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yuan Yuan</a></p>
<p>Binaural stereo audio is recorded by imitating the way the human ear receives
sound, which provides people with an immersive listening experience. Existing
approaches leverage autoencoders and directly exploit visual spatial
information to synthesize binaural stereo, resulting in a limited
representation of visual guidance. For the first time, we propose a visually
guided generative adversarial approach for generating binaural stereo audio
from mono audio. Specifically, we develop a Stereo Audio Generation Model
(SAGM), which utilizes shared spatio-temporal visual information to guide the
generator and the discriminator to work separately. The shared visual
information is updated alternately in the generative adversarial stage,
allowing the generator and discriminator to deliver their respective guided
knowledge while visually sharing. The proposed method learns bidirectional
complementary visual information, which facilitates the expression of visual
guidance in generation. In addition, spatial perception is a crucial attribute
of binaural stereo audio, and thus the evaluation of stereo spatial perception
is essential. However, previous metrics failed to measure the spatial
perception of audio. To this end, a metric to measure the spatial perception of
audio is proposed for the first time. The proposed metric is capable of
measuring the magnitude and direction of spatial perception in the temporal
dimension. Further, considering its function, it is feasible to utilize it
instead of demanding user studies to some extent. The proposed method achieves
state-of-the-art performance on 2 datasets and 5 evaluation metrics.
Qualitative experiments and user studies demonstrate that the method generates
space-realistic stereo audio.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07632">ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering. (arXiv:2311.07632v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zecheng Yin</a></p>
<p>Biomedical information graphs are crucial for interaction discovering of
biomedical information in modern age, such as identification of multifarious
molecular interactions and drug discovery, which attracts increasing interests
in biomedicine, bioinformatics, and human healthcare communities. Nowadays,
more and more graph neural networks have been proposed to learn the entities of
biomedical information and precisely reveal biomedical molecule interactions
with state-of-the-art results. These methods remedy the fading of features from
a far distance but suffer from remedying such problem at the expensive cost of
redundant memory and time. In our paper, we propose a novel Residual Message
Graph Convolution Network (ResMGCN) for fast and precise biomedical interaction
prediction in a different idea. Specifically, instead of enhancing the message
from far nodes, ResMGCN aggregates lower-order information with the next round
higher information to guide the node update to obtain a more meaningful node
representation. ResMGCN is able to perceive and preserve various messages from
the previous layer and high-order information in the current layer with least
memory and time cost to obtain informative representations of biomedical
entities. We conduct experiments on four biomedical interaction network
datasets, including protein-protein, drug-drug, drug-target, and gene-disease
interactions, which demonstrates that ResMGCN outperforms previous
state-of-the-art models while achieving superb effectiveness on both storage
and time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07633">Rethinking and Benchmarking Predict-then-Optimize Paradigm for Combinatorial Optimization Problems. (arXiv:2311.07633v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1">Haoyu Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1">Han Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Runzhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a></p>
<p>Numerous web applications rely on solving combinatorial optimization
problems, such as energy cost-aware scheduling, budget allocation on web
advertising, and graph matching on social networks. However, many optimization
problems involve unknown coefficients, and improper predictions of these
factors may lead to inferior decisions which may cause energy wastage,
inefficient resource allocation, inappropriate matching in social networks,
etc. Such a research topic is referred to as "Predict-Then-Optimize (PTO)"
which considers the performance of prediction and decision-making in a unified
system. A noteworthy recent development is the end-to-end methods by directly
optimizing the ultimate decision quality which claims to yield better results
in contrast to the traditional two-stage approach. However, the evaluation
benchmarks in this field are fragmented and the effectiveness of various models
in different scenarios remains unclear, hindering the comprehensive assessment
and fast deployment of these methods. To address these issues, we provide a
comprehensive categorization of current approaches and integrate existing
experimental scenarios to establish a unified benchmark, elucidating the
circumstances under which end-to-end training yields improvements, as well as
the contexts in which it performs ineffectively. We also introduce a new
dataset for the industrial combinatorial advertising problem for inclusive
finance to open-source. We hope the rethinking and benchmarking of PTO could
facilitate more convenient evaluation and deployment, and inspire further
improvements both in the academy and industry within this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07636">Attention-based Multi-task Learning for Base Editor Outcome Prediction. (arXiv:2311.07636v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Mollaysa_A/0/1/0/all/0/1">Amina Mollaysa</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Allam_A/0/1/0/all/0/1">Ahmed Allam</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Krauthamme_M/0/1/0/all/0/1">Michael Krauthamme</a></p>
<p>Human genetic diseases often arise from point mutations, emphasizing the
critical need for precise genome editing techniques. Among these, base editing
stands out as it allows targeted alterations at the single nucleotide level.
However, its clinical application is hindered by low editing efficiency and
unintended mutations, necessitating extensive trial-and-error experimentation
in the laboratory. To speed up this process, we present an attention-based
two-stage machine learning model that learns to predict the likelihood of all
possible editing outcomes for a given genomic target sequence. We further
propose a multi-task learning schema to jointly learn multiple base editors
(i.e. variants) at once. Our model's predictions consistently demonstrated a
strong correlation with the actual experimental results on multiple datasets
and base editor variants. These results provide further validation for the
models' capacity to enhance and accelerate the process of refining base editing
designs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07682">Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion. (arXiv:2311.07682v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaman_K/0/1/0/all/0/1">Kerem Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1">Leshem Choshen</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Shashank Srivastava</a></p>
<p>Model fusion research aims to aggregate the knowledge of multiple models to
enhance performance by combining their weights. In this work, we study the
inverse, investigating whether and how can model fusion interfere and reduce
unwanted knowledge. We delve into the effects of model fusion on the evolution
of learned shortcuts, social biases, and memorization capabilities in
fine-tuned language models. Through several experiments covering text
classification and generation tasks, our analysis highlights that shared
knowledge among models is usually enhanced during model fusion, while unshared
knowledge is usually lost or forgotten. Based on this observation, we
demonstrate the potential of model fusion as a debiasing tool and showcase its
efficacy in addressing privacy concerns associated with language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07687">Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games. (arXiv:2311.07687v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sudhakar_A/0/1/0/all/0/1">Arjun Vaithilingam Sudhakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1">Prasanna Parthasarathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1">Janarthanan Rajendran</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a></p>
<p>Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07692">On The Truthfulness of &#x27;Surprisingly Likely&#x27; Responses of Large Language Models. (arXiv:2311.07692v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1">Naman Goel</a></p>
<p>The surprisingly likely criterion in the seminal work of Prelec (the Bayesian
Truth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,
by rewarding rational agents to maximise the expected information gain with
their answers w.r.t. their probabilistic beliefs. We investigate the relevance
of a similar criterion for responses of LLMs. We hypothesize that if the
surprisingly likely criterion works in LLMs, under certain conditions, the
responses that maximize the reward under this criterion should be more accurate
than the responses that only maximize the posterior probability. Using
benchmarks including the TruthfulQA benchmark and using openly available LLMs:
GPT-2 and LLaMA-2, we show that the method indeed improves the accuracy
significantly (for example, upto 24 percentage points aggregate improvement on
TruthfulQA and upto 70 percentage points improvement on individual categories
of questions).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07693">Matching aggregate posteriors in the variational autoencoder. (arXiv:2311.07693v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Surojit Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Sarang Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Whitaker_R/0/1/0/all/0/1">Ross Whitaker</a></p>
<p>The variational autoencoder (VAE) is a well-studied, deep, latent-variable
model (DLVM) that efficiently optimizes the variational lower bound of the log
marginal data likelihood and has a strong theoretical foundation. However, the
VAE's known failure to match the aggregate posterior often results in
\emph{pockets/holes} in the latent distribution (i.e., a failure to match the
prior) and/or \emph{posterior collapse}, which is associated with a loss of
information in the latent space. This paper addresses these shortcomings in
VAEs by reformulating the objective function associated with VAEs in order to
match the aggregate/marginal posterior distribution to the prior. We use kernel
density estimate (KDE) to model the aggregate posterior in high dimensions. The
proposed method is named the \emph{aggregate variational autoencoder} (AVAE)
and is built on the theoretical framework of the VAE. Empirical evaluation of
the proposed method on multiple benchmark data sets demonstrates the
effectiveness of the AVAE relative to state-of-the-art (SOTA) methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07700">AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising. (arXiv:2311.07700v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangdi Yu</a></p>
<p>Large language models (LLMs) have opened up enormous opportunities while
simultaneously posing ethical dilemmas. One of the major concerns is their
ability to create text that closely mimics human writing, which can lead to
potential misuse, such as academic misconduct, disinformation, and fraud. To
address this problem, we present AuthentiGPT, an efficient classifier that
distinguishes between machine-generated and human-written texts. Under the
assumption that human-written text resides outside the distribution of
machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input
text with artificially added noise, and then semantically compares the denoised
text with the original to determine if the content is machine-generated. With
only one trainable parameter, AuthentiGPT eliminates the need for a large
training dataset, watermarking the LLM's output, or computing the
log-likelihood. Importantly, the detection capability of AuthentiGPT can be
easily adapted to any generative language model. With a 0.918 AUROC score on a
domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other
commercial algorithms, highlighting its potential for detecting
machine-generated text in academic settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07705">Robust and Scalable Hyperdimensional Computing With Brain-Like Neural Adaptations. (arXiv:2311.07705v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1">Mohammad Abdullah Al Faruque</a></p>
<p>The Internet of Things (IoT) has facilitated many applications utilizing
edge-based machine learning (ML) methods to analyze locally collected data.
Unfortunately, popular ML algorithms often require intensive computations
beyond the capabilities of today's IoT devices. Brain-inspired hyperdimensional
computing (HDC) has been introduced to address this issue. However, existing
HDCs use static encoders, requiring extremely high dimensionality and hundreds
of training iterations to achieve reasonable accuracy. This results in a huge
efficiency loss, severely impeding the application of HDCs in IoT systems. We
observed that a main cause is that the encoding module of existing HDCs lacks
the capability to utilize and adapt to information learned during training. In
contrast, neurons in human brains dynamically regenerate all the time and
provide more useful functionalities when learning new information. While the
goal of HDC is to exploit the high-dimensionality of randomly generated base
hypervectors to represent the information as a pattern of neural activity, it
remains challenging for existing HDCs to support a similar behavior as brain
neural regeneration. In this work, we present dynamic HDC learning frameworks
that identify and regenerate undesired dimensions to provide adequate accuracy
with significantly lowered dimensionalities, thereby accelerating both the
training and inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07708">Reinforcement Learning for Solving Stochastic Vehicle Routing Problem. (arXiv:2311.07708v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Iklassov_Z/0/1/0/all/0/1">Zangir Iklassov</a>, <a href="http://arxiv.org/find/cs/1/au:+Sobirov_I/0/1/0/all/0/1">Ikboljon Sobirov</a>, <a href="http://arxiv.org/find/cs/1/au:+Solozabal_R/0/1/0/all/0/1">Ruben Solozabal</a>, <a href="http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1">Martin Takac</a></p>
<p>This study addresses a gap in the utilization of Reinforcement Learning (RL)
and Machine Learning (ML) techniques in solving the Stochastic Vehicle Routing
Problem (SVRP) that involves the challenging task of optimizing vehicle routes
under uncertain conditions. We propose a novel end-to-end framework that
comprehensively addresses the key sources of stochasticity in SVRP and utilizes
an RL agent with a simple yet effective architecture and a tailored training
method. Through comparative analysis, our proposed model demonstrates superior
performance compared to a widely adopted state-of-the-art metaheuristic,
achieving a significant 3.43% reduction in travel costs. Furthermore, the model
exhibits robustness across diverse SVRP settings, highlighting its adaptability
and ability to learn optimal routing strategies in varying environments. The
publicly available implementation of our framework serves as a valuable
resource for future research endeavors aimed at advancing RL-based solutions
for SVRP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07715">PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature. (arXiv:2311.07715v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Jerry Junyang Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yuchen Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinghao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_P/0/1/0/all/0/1">Pranav Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wantian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Grampurohit_S/0/1/0/all/0/1">Sanjeev Grampurohit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramprasad_R/0/1/0/all/0/1">Rampi Ramprasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a></p>
<p>Scientific information extraction (SciIE), which aims to automatically
extract information from scientific literature, is becoming more important than
ever. However, there are no existing SciIE datasets for polymer materials,
which is an important class of materials used ubiquitously in our daily lives.
To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer
materials. POLYIE is curated from 146 full-length polymer scholarly articles,
which are annotated with different named entities (i.e., materials, properties,
values, conditions) as well as their N-ary relations by domain experts. POLYIE
presents several unique challenges due to diverse lexical formats of entities,
ambiguity between entities, and variable-length relations. We evaluate
state-of-the-art named entity extraction and relation extraction models on
POLYIE, analyze their strengths and weaknesses, and highlight some difficult
cases for these models. To the best of our knowledge, POLYIE is the first SciIE
benchmark for polymer materials, and we hope it will lead to more research
efforts from the community on this challenging task. Our code and data are
available on: https://github.com/jerry3027/PolyIE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07723">Generalization Analogies (GENIES): A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1">Joshua Clymer</a>, <a href="http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1">Garrett Baker</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1">Rohan Subramani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sam Wang</a></p>
<p>As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENaralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07726">A Simple Quantum Blockmodeling with Qubits and Permutations. (arXiv:2311.07726v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Daskin_A/0/1/0/all/0/1">Ammar Daskin</a></p>
<p>Blockmodeling of a given problem represented by an $N\times N$ adjacency
matrix can be found by swapping rows and columns of the matrix (i.e.
multiplying matrix from left and right by a permutation matrix). In general,
through performing this task, row and column permutations affect the fitness
value in optimization: For an $N\times N$ matrix, it requires $O(N)$
computations to find (or update) the fitness value of a candidate solution.
</p>
<p>On quantum computers, permutations can be applied in parallel and
efficiently, and their implementations can be as simple as a single qubit
operation (a NOT gate on a qubit) which takes an $O(1)$ time algorithmic step.
In this paper, using permutation matrices, we describe a quantum blockmodeling
for data analysis tasks. In the model, the measurement outcome of a small group
of qubits are mapped to indicate the fitness value. Therefore, we show that it
is possible to find or update the fitness value in $O(log(N))$ time. This lead
us to show that when the number of iterations are less than $log(N)$ time, it
may be possible to reach the same solution exponentially faster on quantum
computers in comparison to classical computers. In addition, since on quantum
circuits the different sequence of permutations can be applied in parallel
(superpositon), the machine learning task in this model can be implemented more
efficiently on quantum computers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07744">Dynamic Local Attention with Hierarchical Patching for Irregular Clinical Time Series. (arXiv:2311.07744v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xingyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaochen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mollaysa_A/0/1/0/all/0/1">Amina Mollaysa</a>, <a href="http://arxiv.org/find/cs/1/au:+Schurch_M/0/1/0/all/0/1">Manuel Sch&#xfc;rch</a>, <a href="http://arxiv.org/find/cs/1/au:+Allam_A/0/1/0/all/0/1">Ahmed Allam</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1">Michael Krauthammer</a></p>
<p>Irregular multivariate time series data is prevalent in the clinical and
healthcare domains. It is characterized by time-wise and feature-wise
irregularities, making it challenging for machine learning methods to work
with. To solve this, we introduce a new model architecture composed of two
modules: (1) DLA, a Dynamic Local Attention mechanism that uses learnable
queries and feature-specific local windows when computing the self-attention
operation. This results in aggregating irregular time steps raw input within
each window to a harmonized regular latent space representation while taking
into account the different features' sampling rates. (2) A hierarchical MLP
mixer that processes the output of DLA through multi-scale patching to leverage
information at various scales for the downstream tasks. Our approach
outperforms state-of-the-art methods on three real-world datasets, including
the latest clinical MIMIC IV dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07750">SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification. (arXiv:2311.07750v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ashraf_S/0/1/0/all/0/1">S.M. Nabil Ashraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1">Md. Adyelullahil Mamun</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1">Hasnat Md. Abdullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1">Md. Golam Rabiul Alam</a></p>
<p>Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the "ChestX-ray14" dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07763">The Disagreement Problem in Faithfulness Metrics. (arXiv:2311.07763v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barr_B/0/1/0/all/0/1">Brian Barr</a>, <a href="http://arxiv.org/find/cs/1/au:+Fatsi_N/0/1/0/all/0/1">Noah Fatsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hancox_Li_L/0/1/0/all/0/1">Leif Hancox-Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Richter_P/0/1/0/all/0/1">Peter Richter</a>, <a href="http://arxiv.org/find/cs/1/au:+Proano_D/0/1/0/all/0/1">Daniel Proano</a>, <a href="http://arxiv.org/find/cs/1/au:+Mok_C/0/1/0/all/0/1">Caleb Mok</a></p>
<p>The field of explainable artificial intelligence (XAI) aims to explain how
black-box machine learning models work. Much of the work centers around the
holy grail of providing post-hoc feature attributions to any model
architecture. While the pace of innovation around novel methods has slowed
down, the question remains of how to choose a method, and how to make it fit
for purpose. Recently, efforts around benchmarking XAI methods have suggested
metrics for that purpose -- but there are many choices. That bounty of choice
still leaves an end user unclear on how to proceed. This paper focuses on
comparing metrics with the aim of measuring faithfulness of local explanations
on tabular classification problems -- and shows that the current metrics don't
agree; leaving users unsure how to choose the most faithful explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07765">FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based Human Activity Recognition. (arXiv:2311.07765v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Isguder_E/0/1/0/all/0/1">Egemen &#x130;&#x15f;g&#xfc;der</a>, <a href="http://arxiv.org/find/cs/1/au:+Incel_O/0/1/0/all/0/1">&#xd6;zlem Durmaz &#x130;ncel</a></p>
<p>Motion sensors integrated into wearable and mobile devices provide valuable
information about the device users. Machine learning and, recently, deep
learning techniques have been used to characterize sensor data. Mostly, a
single task, such as recognition of activities, is targeted, and the data is
processed centrally at a server or in a cloud environment. However, the same
sensor data can be utilized for multiple tasks and distributed machine-learning
techniques can be used without the requirement of the transmission of data to a
centre. This paper explores Federated Transfer Learning in a Multi-Task manner
for both sensor-based human activity recognition and device position
identification tasks. The OpenHAR framework is used to train the models, which
contains ten smaller datasets. The aim is to obtain model(s) applicable for
both tasks in different datasets, which may include only some label types.
Multiple experiments are carried in the Flower federated learning environment
using the DeepConvLSTM architecture. Results are presented for federated and
centralized versions under different parameters and restrictions. By utilizing
transfer learning and training a task-specific and personalized federated
model, we obtained a similar accuracy with training each client individually
and higher accuracy than a fully centralized approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07766">Vision-Language Integration in Multimodal Video Transformers (Partially) Aligns with the Brain. (arXiv:2311.07766v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1">Dota Tianai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1">Mariya Toneva</a></p>
<p>Integrating information from multiple modalities is arguably one of the
essential prerequisites for grounding artificial intelligence systems with an
understanding of the real world. Recent advances in video transformers that
jointly learn from vision, text, and sound over time have made some progress
toward this goal, but the degree to which these models integrate information
from modalities still remains unclear. In this work, we present a promising
approach for probing a pre-trained multimodal video transformer model by
leveraging neuroscientific evidence of multimodal information processing in the
brain. Using brain recordings of participants watching a popular TV show, we
analyze the effects of multi-modal connections and interactions in a
pre-trained multi-modal video transformer on the alignment with uni- and
multi-modal brain regions. We find evidence that vision enhances masked
prediction performance during language processing, providing support that
cross-modal representations in models can benefit individual modalities.
However, we don't find evidence of brain-relevant information captured by the
joint multi-modal transformer representations beyond that captured by all of
the individual modalities. We finally show that the brain alignment of the
pre-trained joint representation can be improved by fine-tuning using a task
that requires vision-language inferences. Overall, our results paint an
optimistic picture of the ability of multi-modal transformers to integrate
vision and language in partially brain-relevant ways but also show that
improving the brain alignment of these models may require new approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07772">In-context Learning and Gradient Descent Revisited. (arXiv:2311.07772v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nathan_T/0/1/0/all/0/1">Tomer Bar Nathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Deutch_G/0/1/0/all/0/1">Gilad Deutch</a>, <a href="http://arxiv.org/find/cs/1/au:+Magar_N/0/1/0/all/0/1">Nadav Magar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1">Guy Dar</a></p>
<p>In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. Recent works
suggest that ICL can be thought of as a gradient descent (GD) based
optimization process. While promising, these results mainly focus on simplified
settings of ICL and provide only a preliminary evaluation of the similarities
between the two methods. In this work, we revisit the comparison between ICL
and GD-based finetuning and study what properties of ICL an equivalent process
must follow. We highlight a major difference in the flow of information between
ICL and standard finetuning. Namely, ICL can only rely on information from
lower layers at every point, while finetuning depends on loss gradients from
deeper layers. We refer to this discrepancy as Layer Causality and show that a
layer causal variant of the finetuning process aligns with ICL on par with
vanilla finetuning and is even better in most cases across relevant metrics. To
the best of our knowledge, this is the first work to discuss this discrepancy
explicitly and suggest a solution that tackles this problem with minimal
changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07784">A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks. (arXiv:2311.07784v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Babakniya_S/0/1/0/all/0/1">Sara Babakniya</a>, <a href="http://arxiv.org/find/cs/1/au:+Fabian_Z/0/1/0/all/0/1">Zalan Fabian</a>, <a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1">Chaoyang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1">Mahdi Soltanolkotabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1">Salman Avestimehr</a></p>
<p>Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
\textbf{federated class incremental learning} that utilizes a generative model
to synthesize samples from past distributions. This data can be later exploited
alongside the training data to mitigate catastrophic forgetting. To preserve
privacy, the generative model is trained on the server using data-free methods
at the end of each task without requesting data from clients. Moreover, our
solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07788">CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion. (arXiv:2311.07788v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Norskov_A/0/1/0/all/0/1">Anders Vestergaard N&#xf8;rskov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zahid_A/0/1/0/all/0/1">Alexander Neergaard Zahid</a>, <a href="http://arxiv.org/find/cs/1/au:+Morup_M/0/1/0/all/0/1">Morten M&#xf8;rup</a></p>
<p>Electroencephalography (EEG) is a prominent non-invasive neuroimaging
technique providing insights into brain function. Unfortunately, EEG data
exhibit a high degree of noise and variability across subjects hampering
generalizable signal extraction. Therefore, a key aim in EEG analysis is to
extract the underlying neural activation (content) as well as to account for
the individual subject variability (style). We hypothesize that the ability to
convert EEG signals between tasks and subjects requires the extraction of
latent representations accounting for content and style. Inspired by recent
advancements in voice conversion technologies, we propose a novel contrastive
split-latent permutation autoencoder (CSLP-AE) framework that directly
optimizes for EEG conversion. Importantly, the latent representations are
guided using contrastive learning to promote the latent splits to explicitly
represent subject (style) and task (content). We contrast CSLP-AE to
conventional supervised, unsupervised (AE), and self-supervised (contrastive
learning) training and find that the proposed approach provides favorable
generalizable characterizations of subject and task. Importantly, the procedure
also enables zero-shot conversion between unseen subjects. While the present
work only considers conversion of EEG, the proposed CSLP-AE provides a general
framework for signal conversion and extraction of content (task activation) and
style (subject variability) components of general interest for the modeling and
analysis of biological signals.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07790">Leveraging Hamilton-Jacobi PDEs with time-dependent Hamiltonians for continual scientific machine learning. (arXiv:2311.07790v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Paula Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1">Tingwei Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1">Zongren Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Darbon_J/0/1/0/all/0/1">J&#xe9;r&#xf4;me Darbon</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a></p>
<p>We address two major challenges in scientific machine learning (SciML):
interpretability and computational efficiency. We increase the interpretability
of certain learning processes by establishing a new theoretical connection
between optimization problems arising from SciML and a generalized Hopf
formula, which represents the viscosity solution to a Hamilton-Jacobi partial
differential equation (HJ PDE) with time-dependent Hamiltonian. Namely, we show
that when we solve certain regularized learning problems with integral-type
losses, we actually solve an optimal control problem and its associated HJ PDE
with time-dependent Hamiltonian. This connection allows us to reinterpret
incremental updates to learned models as the evolution of an associated HJ PDE
and optimal control problem in time, where all of the previous information is
intrinsically encoded in the solution to the HJ PDE. As a result, existing HJ
PDE solvers and optimal control algorithms can be reused to design new
efficient training approaches for SciML that naturally coincide with the
continual learning framework, while avoiding catastrophic forgetting. As a
first exploration of this connection, we consider the special case of linear
regression and leverage our connection to develop a new Riccati-based
methodology for solving these learning problems that is amenable to continual
learning applications. We also provide some corresponding numerical examples
that demonstrate the potential computational and memory advantages our
Riccati-based approach can provide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07797">Explainable History Distillation by Marked Temporal Point Process. (arXiv:2311.07797v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sishun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1">Ke Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiuzhen Zhang</a></p>
<p>Explainability of machine learning models is mandatory when researchers
introduce these commonly believed black boxes to real-world tasks, especially
high-stakes ones. In this paper, we build a machine learning system to
automatically generate explanations of happened events from history by \gls{ca}
based on the \acrfull{tpp}. Specifically, we propose a new task called
\acrfull{ehd}. This task requires a model to distill as few events as possible
from observed history. The target is that the event distribution conditioned on
left events predicts the observed future noticeably worse. We then regard
distilled events as the explanation for the future. To efficiently solve
\acrshort{ehd}, we rewrite the task into a \gls{01ip} and directly estimate the
solution to the program by a model called \acrfull{model}. This work fills the
gap between our task and existing works, which only spot the difference between
factual and counterfactual worlds after applying a predefined modification to
the environment. Experiment results on Retweet and StackOverflow datasets prove
that \acrshort{model} significantly outperforms other \acrshort{ehd} baselines
and can reveal the rationale underpinning real-world processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07798">Probabilistic Physics-integrated Neural Differentiable Modeling for Isothermal Chemical Vapor Infiltration Process. (arXiv:2311.07798v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akhare_D/0/1/0/all/0/1">Deepak Akhare</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeping Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulotty_R/0/1/0/all/0/1">Richard Gulotty</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1">Tengfei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian-Xun Wang</a></p>
<p>Chemical vapor infiltration (CVI) is a widely adopted manufacturing technique
used in producing carbon-carbon and carbon-silicon carbide composites. These
materials are especially valued in the aerospace and automotive industries for
their robust strength and lightweight characteristics. The densification
process during CVI critically influences the final performance, quality, and
consistency of these composite materials. Experimentally optimizing the CVI
processes is challenging due to long experimental time and large optimization
space. To address these challenges, this work takes a modeling-centric
approach. Due to the complexities and limited experimental data of the
isothermal CVI densification process, we have developed a data-driven
predictive model using the physics-integrated neural differentiable (PiNDiff)
modeling framework. An uncertainty quantification feature has been embedded
within the PiNDiff method, bolstering the model's reliability and robustness.
Through comprehensive numerical experiments involving both synthetic and
real-world manufacturing data, the proposed method showcases its capability in
modeling densification during the CVI process. This research highlights the
potential of the PiNDiff framework as an instrumental tool for advancing our
understanding, simulation, and optimization of the CVI manufacturing process,
particularly when faced with sparse data and an incomplete description of the
underlying physics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07821">Statistical Parameterized Physics-Based Machine Learning Digital Twin Models for Laser Powder Bed Fusion Process. (arXiv:2311.07821v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yangfan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mojumder_S/0/1/0/all/0/1">Satyajit Mojumder</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Ye Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Amin_A/0/1/0/all/0/1">Abdullah Al Amin</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jiachen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xiaoyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_G/0/1/0/all/0/1">Gregory J. Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jian Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wing Kam Liu</a></p>
<p>A digital twin (DT) is a virtual representation of physical process, products
and/or systems that requires a high-fidelity computational model for continuous
update through the integration of sensor data and user input. In the context of
laser powder bed fusion (LPBF) additive manufacturing, a digital twin of the
manufacturing process can offer predictions for the produced parts, diagnostics
for manufacturing defects, as well as control capabilities. This paper
introduces a parameterized physics-based digital twin (PPB-DT) for the
statistical predictions of LPBF metal additive manufacturing process. We
accomplish this by creating a high-fidelity computational model that accurately
represents the melt pool phenomena and subsequently calibrating and validating
it through controlled experiments. In PPB-DT, a mechanistic reduced-order
method-driven stochastic calibration process is introduced, which enables the
statistical predictions of the melt pool geometries and the identification of
defects such as lack-of-fusion porosity and surface roughness, specifically for
diagnostic applications. Leveraging data derived from this physics-based model
and experiments, we have trained a machine learning-based digital twin
(PPB-ML-DT) model for predicting, monitoring, and controlling melt pool
geometries. These proposed digital twin models can be employed for predictions,
control, optimization, and quality assurance within the LPBF process,
ultimately expediting product development and certification in LPBF-based metal
additive manufacturing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07833">Toward Efficient and Incremental Spectral Clustering via Parametric Spectral Clustering. (arXiv:2311.07833v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jo-Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hung-Hsuan Chen</a></p>
<p>Spectral clustering is a popular method for effectively clustering
nonlinearly separable data. However, computational limitations, memory
requirements, and the inability to perform incremental learning challenge its
widespread application. To overcome these limitations, this paper introduces a
novel approach called parametric spectral clustering (PSC). By extending the
capabilities of spectral clustering, PSC addresses the challenges associated
with big data and real-time scenarios and enables efficient incremental
clustering with new data points. Experimental evaluations conducted on various
open datasets demonstrate the superiority of PSC in terms of computational
efficiency while achieving clustering quality mostly comparable to standard
spectral clustering. The proposed approach has significant potential for
incremental and real-time data analysis applications, facilitating timely and
accurate clustering in dynamic and evolving datasets. The findings of this
research contribute to the advancement of clustering techniques and open new
avenues for efficient and effective data analysis. We publish the experimental
code at https://github.<a href="/abs/com/1095025">com/1095025</a>18/PSC_BigData.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07841">PEMS: Pre-trained Epidmic Time-series Models. (arXiv:2311.07841v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1">Harshavardhan Kamarthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1">B. Aditya Prakash</a></p>
<p>Providing accurate and reliable predictions about the future of an epidemic
is an important problem for enabling informed public health decisions. Recent
works have shown that leveraging data-driven solutions that utilize advances in
deep learning methods to learn from past data of an epidemic often outperform
traditional mechanistic models. However, in many cases, the past data is sparse
and may not sufficiently capture the underlying dynamics. While there exists a
large amount of data from past epidemics, leveraging prior knowledge from
time-series data of other diseases is a non-trivial challenge. Motivated by the
success of pre-trained models in language and vision tasks, we tackle the
problem of pre-training epidemic time-series models to learn from multiple
datasets from different diseases and epidemics. We introduce Pre-trained
Epidemic Time-Series Models (PEMS) that learn from diverse time-series datasets
of a variety of diseases by formulating pre-training as a set of
self-supervised learning (SSL) tasks. We tackle various important challenges
specific to pre-training for epidemic time-series such as dealing with
heterogeneous dynamics and efficiently capturing useful patterns from multiple
epidemic datasets by carefully designing the SSL tasks to learn important
priors about the epidemic dynamics that can be leveraged for fine-tuning to
multiple downstream tasks. The resultant PEM outperforms previous
state-of-the-art methods in various downstream time-series tasks across
datasets of varying seasonal patterns, geography, and mechanism of contagion
including the novel Covid-19 pandemic unseen in pre-trained data with better
efficiency using smaller fraction of datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07850">Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA. (arXiv:2311.07850v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1">Dhruv Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1">Rajarshi Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1">Sopan Khosla</a>, <a href="http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1">Rashmi Gangadharaiah</a></p>
<p>We present BYOKG, a universal question-answering (QA) system that can operate
on any knowledge graph (KG), requires no human-annotated training data, and can
be ready to use within a day -- attributes that are out-of-scope for current
KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to
comprehend information present in an unseen KG through exploration -- starting
at random nodes, inspecting the labels of adjacent nodes and edges, and
combining them with their prior world knowledge. In BYOKG, exploration
leverages an LLM-backed symbolic agent that generates a diverse set of
query-program exemplars, which are then used to ground a retrieval-augmented
reasoning procedure to predict programs for arbitrary questions. BYOKG is
effective over both small- and large-scale graphs, showing dramatic gains in QA
accuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,
respectively. On GrailQA, we further show that our unsupervised BYOKG
outperforms a supervised in-context learning method, demonstrating the
effectiveness of exploration. Lastly, we find that performance of BYOKG
reliably improves with continued exploration as well as improvements in the
base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1
on a sub-sampled zero-shot split of GrailQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07853">Learning Mutually Informed Representations for Characters and Subwords. (arXiv:2311.07853v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1">Matthew R. Gormley</a></p>
<p>Most pretrained language models rely on subword tokenization, which processes
text as a sequence of subword tokens. However, different granularities of text,
such as characters, subwords, and words, can contain different kinds of
information. Previous studies have shown that incorporating multiple input
granularities improves model generalization, yet very few of them outputs
useful representations for each granularity. In this paper, we introduce the
entanglement model, aiming to combine character and subword language models.
Inspired by vision-language models, our model treats characters and subwords as
separate modalities, and it generates mutually informed representations for
both granularities as output. We evaluate our model on text classification,
named entity recognition, and POS-tagging tasks. Notably, the entanglement
model outperforms its backbone language models, particularly in the presence of
noisy texts and low-resource languages. Furthermore, the entanglement model
even outperforms larger pre-trained models on all English sequence labeling
tasks and classification tasks. Our anonymized code is available at
https://anonymous.4open.science/r/noisy-IE-A673
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07864">Probing clustering in neural network representations. (arXiv:2311.07864v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thao Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a></p>
<p>Neural network representations contain structure beyond what was present in
the training labels. For instance, representations of images that are visually
or semantically similar tend to lie closer to each other than to dissimilar
images, regardless of their labels. Clustering these representations can thus
provide insights into dataset properties as well as the network internals. In
this work, we study how the many design choices involved in neural network
training affect the clusters formed in the hidden representations. To do so, we
establish an evaluation setup based on the BREEDS hierarchy, for the task of
subclass clustering after training models with only superclass information. We
isolate the training dataset and architecture as important factors affecting
clusterability. Datasets with labeled classes consisting of unrelated
subclasses yield much better clusterability than those following a natural
hierarchy. When using pretrained models to cluster representations on
downstream datasets, models pretrained on subclass labels provide better
clusterability than models pretrained on superclass labels, but only when there
is a high degree of domain overlap between the pretraining and downstream data.
Architecturally, we find that normalization strategies affect which layers
yield the best clustering performance, and, surprisingly, Vision Transformers
attain lower subclass clusterability than ResNets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07867">Mixture of Coupled HMMs for Robust Modeling of Multivariate Healthcare Time Series. (arXiv:2311.07867v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Poyraz_O/0/1/0/all/0/1">Onur Poyraz</a>, <a href="http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1">Pekka Marttinen</a></p>
<p>Analysis of multivariate healthcare time series data is inherently
challenging: irregular sampling, noisy and missing values, and heterogeneous
patient groups with different dynamics violating exchangeability. In addition,
interpretability and quantification of uncertainty are critically important.
Here, we propose a novel class of models, a mixture of coupled hidden Markov
models (M-CHMM), and demonstrate how it elegantly overcomes these challenges.
To make the model learning feasible, we derive two algorithms to sample the
sequences of the latent variables in the CHMM: samplers based on (i) particle
filtering and (ii) factorized approximation. Compared to existing inference
methods, our algorithms are computationally tractable, improve mixing, and
allow for likelihood estimation, which is necessary to learn the mixture model.
Experiments on challenging real-world epidemiological and semi-synthetic data
demonstrate the advantages of the M-CHMM: improved data fit, capacity to
efficiently handle missing and noisy measurements, improved prediction
accuracy, and ability to identify interpretable subsets in the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07868">Multi-Signal Reconstruction Using Masked Autoencoder From EEG During Polysomnography. (arXiv:2311.07868v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kweon_Y/0/1/0/all/0/1">Young-Seok Kweon</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_G/0/1/0/all/0/1">Gi-Hwan Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1">Heon-Gyu Kwak</a>, <a href="http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1">Ha-Na Jo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Polysomnography (PSG) is an indispensable diagnostic tool in sleep medicine,
essential for identifying various sleep disorders. By capturing physiological
signals, including EEG, EOG, EMG, and cardiorespiratory metrics, PSG presents a
patient's sleep architecture. However, its dependency on complex equipment and
expertise confines its use to specialized clinical settings. Addressing these
limitations, our study aims to perform PSG by developing a system that requires
only a single EEG measurement. We propose a novel system capable of
reconstructing multi-signal PSG from a single-channel EEG based on a masked
autoencoder. The masked autoencoder was trained and evaluated using the
Sleep-EDF-20 dataset, with mean squared error as the metric for assessing the
similarity between original and reconstructed signals. The model demonstrated
proficiency in reconstructing multi-signal data. Our results present promise
for the development of more accessible and long-term sleep monitoring systems.
This suggests the expansion of PSG's applicability, enabling its use beyond the
confines of clinics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07876">Learning Adversarial Low-rank Markov Decision Processes with Unknown Transition and Full-information Feedback. (arXiv:2311.07876v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Canzhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ruofeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baoxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuezhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuai Li</a></p>
<p>In this work, we study the low-rank MDPs with adversarially changed losses in
the full-information feedback setting. In particular, the unknown transition
probability kernel admits a low-rank matrix decomposition \citep{REPUCB22}, and
the loss functions may change adversarially but are revealed to the learner at
the end of each episode. We propose a policy optimization-based algorithm POLO,
and we prove that it attains the
$\widetilde{O}(K^{\frac{5}{6}}A^{\frac{1}{2}}d\ln(1+M)/(1-\gamma)^2)$ regret
guarantee, where $d$ is rank of the transition kernel (and hence the dimension
of the unknown representations), $A$ is the cardinality of the action space,
$M$ is the cardinality of the model class, and $\gamma$ is the discounted
factor. Notably, our algorithm is oracle-efficient and has a regret guarantee
with no dependence on the size of potentially arbitrarily large state space.
Furthermore, we also prove an $\Omega(\frac{\gamma^2}{1-\gamma} \sqrt{d A K})$
regret lower bound for this problem, showing that low-rank MDPs are
statistically more difficult to learn than linear MDPs in the regret
minimization setting. To the best of our knowledge, we present the first
algorithm that interleaves representation learning, exploration, and
exploitation to achieve the sublinear regret guarantee for RL with nonlinear
function approximation and adversarial losses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07888">RoboSense At Edge: Detecting Slip, Crumple and Shape of the Object in Robotic Hand for Teleoprations. (arXiv:2311.07888v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Padhi_S/0/1/0/all/0/1">Sudev Kumar Padhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1">Mohit Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Giri_D/0/1/0/all/0/1">Debanka Giri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1">Subidh Ali</a></p>
<p>Slip and crumple detection is essential for performing robust manipulation
tasks with a robotic hand (RH) like remote surgery. It has been one of the
challenging problems in the robotics manipulation community. In this work, we
propose a technique based on machine learning (ML) based techniques to detect
the slip, and crumple as well as the shape of an object that is currently held
in the robotic hand. We proposed ML model will detect the slip, crumple, and
shape using the force/torque exerted and the angular positions of the actuators
present in the RH. The proposed model would be integrated into the loop of a
robotic hand(RH) and haptic glove(HG). This would help us to reduce the latency
in case of teleoperation
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07896">Bayesian Conditional Diffusion Models for Versatile Spatiotemporal Turbulence Generation. (arXiv:2311.07896v1 [physics.flu-dyn])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Gao_H/0/1/0/all/0/1">Han Gao</a>, <a href="http://arxiv.org/find/physics/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/physics/1/au:+Fan_X/0/1/0/all/0/1">Xiantao Fan</a>, <a href="http://arxiv.org/find/physics/1/au:+Sun_L/0/1/0/all/0/1">Luning Sun</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_L/0/1/0/all/0/1">Li-Ping Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Duan_L/0/1/0/all/0/1">Lian Duan</a>, <a href="http://arxiv.org/find/physics/1/au:+Wang_J/0/1/0/all/0/1">Jian-Xun Wang</a></p>
<p>Turbulent flows have historically presented formidable challenges to
predictive computational modeling. Traditional numerical simulations often
require vast computational resources, making them infeasible for numerous
engineering applications. As an alternative, deep learning-based surrogate
models have emerged, offering data-drive solutions. However, these are
typically constructed within deterministic settings, leading to shortfall in
capturing the innate chaotic and stochastic behaviors of turbulent dynamics. We
introduce a novel generative framework grounded in probabilistic diffusion
models for versatile generation of spatiotemporal turbulence. Our method
unifies both unconditional and conditional sampling strategies within a
Bayesian framework, which can accommodate diverse conditioning scenarios,
including those with a direct differentiable link between specified conditions
and generated unsteady flow outcomes, and scenarios lacking such explicit
correlations. A notable feature of our approach is the method proposed for
long-span flow sequence generation, which is based on autoregressive
gradient-based conditional sampling, eliminating the need for cumbersome
retraining processes. We showcase the versatile turbulence generation
capability of our framework through a suite of numerical experiments,
including: 1) the synthesis of LES simulated instantaneous flow sequences from
URANS inputs; 2) holistic generation of inhomogeneous, anisotropic wall-bounded
turbulence, whether from given initial conditions, prescribed turbulence
statistics, or entirely from scratch; 3) super-resolved generation of
high-speed turbulent boundary layer flows from low-resolution data across a
range of input resolutions. Collectively, our numerical experiments highlight
the merit and transformative potential of the proposed methods, making a
significant advance in the field of turbulence generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07911">Instruction-Following Evaluation for Large Language Models. (arXiv:2311.07911v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jeffrey Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tianjian Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1">Siddhartha Brahma</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Sujoy Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1">Yi Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Le Hou</a></p>
<p>One core capability of Large Language Models (LLMs) is to follow natural
language instructions. However, the evaluation of such abilities is not
standardized: Human evaluations are expensive, slow, and not objectively
reproducible, while LLM-based auto-evaluation is potentially biased or limited
by the ability of the evaluator LLM. To overcome these issues, we introduce
Instruction-Following Eval (IFEval) for large language models. IFEval is a
straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set
of "verifiable instructions" such as "write in more than 400 words" and
"mention the keyword of AI at least 3 times". We identified 25 types of those
verifiable instructions and constructed around 500 prompts, with each prompt
containing one or more verifiable instructions. We show evaluation results of
two widely available LLMs on the market. Our code and data can be found at
https://github.com/google-research/google-research/tree/master/instruction_following_eval
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07914">Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey. (arXiv:2311.07914v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1">Garima Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1">Tharindu Kumarage</a>, <a href="http://arxiv.org/find/cs/1/au:+Alghami_Z/0/1/0/all/0/1">Zeyad Alghami</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a></p>
<p>The contemporary LLMs are prone to producing hallucinations, stemming mainly
from the knowledge gaps within the models. To address this critical limitation,
researchers employ diverse strategies to augment the LLMs by incorporating
external knowledge, aiming to reduce hallucinations and enhance reasoning
accuracy. Among these strategies, leveraging knowledge graphs as a source of
external information has demonstrated promising results. In this survey, we
conduct a comprehensive review of these knowledge-graph-based knowledge
augmentation techniques in LLMs, focusing on their efficacy in mitigating
hallucinations. We systematically categorize these methods into three
overarching groups, offering both methodological comparisons and empirical
evaluations of their performance. Lastly, the paper explores the challenges
associated with these techniques and outlines potential avenues for future
research in this emerging field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07919">Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models. (arXiv:2311.07919v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chu_Y/0/1/0/all/0/1">Yunfei Chu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1">Jin Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1">Xiaohuan Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1">Qian Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1">Shiliang Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1">Zhijie Yan</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a></p>
<p>Recently, instruction-following audio-language models have received broad
attention for audio interaction with humans. However, the absence of
pre-trained audio models capable of handling diverse audio types and tasks has
hindered progress in this field. Consequently, most existing works have only
been able to support a limited range of interaction capabilities. In this
paper, we develop the Qwen-Audio model and address this limitation by scaling
up audio-language pre-training to cover over 30 tasks and various audio types,
such as human speech, natural sounds, music, and songs, to facilitate universal
audio understanding abilities. However, directly co-training all tasks and
datasets can lead to interference issues, as the textual labels associated with
different datasets exhibit considerable variations due to differences in task
focus, language, granularity of annotation, and text structure. To overcome the
one-to-many interference, we carefully design a multi-task training framework
by conditioning on a sequence of hierarchical tags to the decoder for
encouraging knowledge sharing and avoiding interference through shared and
specified tags respectively. Remarkably, Qwen-Audio achieves impressive
performance across diverse benchmark tasks without requiring any task-specific
fine-tuning, surpassing its counterparts. Building upon the capabilities of
Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from
various audios and text inputs, enabling multi-turn dialogues and supporting
various audio-central scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07928">Towards Improving Robustness Against Common Corruptions in Object Detectors Using Adversarial Contrastive Learning. (arXiv:2311.07928v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a></p>
<p>Neural networks have revolutionized various domains, exhibiting remarkable
accuracy in tasks like natural language processing and computer vision.
However, their vulnerability to slight alterations in input samples poses
challenges, particularly in safety-critical applications like autonomous
driving. Current approaches, such as introducing distortions during training,
fall short in addressing unforeseen corruptions. This paper proposes an
innovative adversarial contrastive learning framework to enhance neural network
robustness simultaneously against adversarial attacks and common corruptions.
By generating instance-wise adversarial examples and optimizing contrastive
loss, our method fosters representations that resist adversarial perturbations
and remain robust in real-world scenarios. Subsequent contrastive learning then
strengthens the similarity between clean samples and their adversarial
counterparts, fostering representations resistant to both adversarial attacks
and common distortions. By focusing on improving performance under adversarial
and real-world conditions, our approach aims to bolster the robustness of
neural networks in safety-critical applications, such as autonomous vehicles
navigating unpredictable weather conditions. We anticipate that this framework
will contribute to advancing the reliability of neural networks in challenging
environments, facilitating their widespread adoption in mission-critical
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07929">Self-supervised Heterogeneous Graph Variational Autoencoders. (arXiv:2311.07929v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yige Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jianxiang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chengcheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiding Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuaiqiang Wang</a></p>
<p>Heterogeneous Information Networks (HINs), which consist of various types of
nodes and edges, have recently demonstrated excellent performance in graph
mining. However, most existing heterogeneous graph neural networks (HGNNs)
ignore the problems of missing attributes, inaccurate attributes and scarce
labels for nodes, which limits their expressiveness. In this paper, we propose
a generative self-supervised model SHAVA to address these issues
simultaneously. Specifically, SHAVA first initializes all the nodes in the
graph with a low-dimensional representation matrix. After that, based on the
variational graph autoencoder framework, SHAVA learns both node-level and
attribute-level embeddings in the encoder, which can provide fine-grained
semantic information to construct node attributes. In the decoder, SHAVA
reconstructs both links and attributes. Instead of directly reconstructing raw
features for attributed nodes, SHAVA generates the initial low-dimensional
representation matrix for all the nodes, based on which raw features of
attributed nodes are further reconstructed to leverage accurate attributes. In
this way, SHAVA can not only complete informative features for non-attributed
nodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct
extensive experiments to show the superiority of SHAVA in tackling HINs with
missing and inaccurate attributes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07939">Discretized Distributed Optimization over Dynamic Digraphs. (arXiv:2311.07939v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Doostmohammadian_M/0/1/0/all/0/1">Mohammadreza Doostmohammadian</a>, <a href="http://arxiv.org/find/math/1/au:+Jiang_W/0/1/0/all/0/1">Wei Jiang</a>, <a href="http://arxiv.org/find/math/1/au:+Liaquat_M/0/1/0/all/0/1">Muwahida Liaquat</a>, <a href="http://arxiv.org/find/math/1/au:+Aghasi_A/0/1/0/all/0/1">Alireza Aghasi</a>, <a href="http://arxiv.org/find/math/1/au:+Zarrabi_H/0/1/0/all/0/1">Houman Zarrabi</a></p>
<p>We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07948">Finding Inductive Loop Invariants using Large Language Models. (arXiv:2311.07948v1 [cs.PL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1">Adharsh Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Senthilnathan_A/0/1/0/all/0/1">Aditya Senthilnathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Saikat Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Deligiannis_P/0/1/0/all/0/1">Pantazis Deligiannis</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1">Shuvendu K. Lahiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Lal_A/0/1/0/all/0/1">Akash Lal</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1">Aseem Rastogi</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1">Subhajit Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Rahul Sharma</a></p>
<p>Loop invariants are fundamental to reasoning about programs with loops. They
establish properties about a given loop's behavior. When they additionally are
inductive, they become useful for the task of formal verification that seeks to
establish strong mathematical guarantees about program's runtime behavior. The
inductiveness ensures that the invariants can be checked locally without
consulting the entire program, thus are indispensable artifacts in a formal
proof of correctness. Finding inductive loop invariants is an undecidable
problem, and despite a long history of research towards practical solutions, it
remains far from a solved problem. This paper investigates the capabilities of
the Large Language Models (LLMs) in offering a new solution towards this old,
yet important problem. To that end, we first curate a dataset of verification
problems on programs with loops. Next, we design a prompt for exploiting LLMs,
obtaining inductive loop invariants, that are checked for correctness using
sound symbolic tools. Finally, we explore the effectiveness of using an
efficient combination of a symbolic tool and an LLM on our dataset and compare
it against a purely symbolic baseline. Our results demonstrate that LLMs can
help improve the state-of-the-art in automated program verification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07951">A Fast and Simple Algorithm for computing the MLE of Amplitude Density Function Parameters. (arXiv:2311.07951v1 [stat.ME])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Teimouri_M/0/1/0/all/0/1">Mahdi Teimouri</a></p>
<p>Over the last decades, the family of $\alpha$-stale distributions has proven
to be useful for modelling in telecommunication systems. Particularly, in the
case of radar applications, finding a fast and accurate estimation for the
amplitude density function parameters appears to be very important. In this
work, the maximum likelihood estimator (MLE) is proposed for parameters of the
amplitude distribution. To do this, the amplitude data are \emph{projected} on
the horizontal and vertical axes using two simple transformations. It is proved
that the \emph{projected} data follow a zero-location symmetric $\alpha$-stale
distribution for which the MLE can be computed quite fast. The average of
computed MLEs based on two \emph{projections} is considered as estimator for
parameters of the amplitude distribution. Performance of the proposed
\emph{projection} method is demonstrated through simulation study and analysis
of two sets of real radar data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07957">Language Models are Better Bug Detector Through Code-Pair Classification. (arXiv:2311.07957v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alrashedy_K/0/1/0/all/0/1">Kamel Alrashedy</a></p>
<p>Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful
models for code generation and understanding. Fine-tuning these models comes
with a high computational cost and requires a large labeled dataset.
Alternatively, in-context learning techniques allow models to learn downstream
tasks with only a few examples. Recently, researchers have shown how in-context
learning performs well in bug detection and repair. In this paper, we propose
code-pair classification task in which both the buggy and non-buggy versions
are given to the model, and the model identifies the buggy ones. We evaluate
our task in real-world dataset of bug detection and two most powerful LLMs. Our
experiments indicate that an LLM can often pick the buggy from the non-buggy
version of the code, and the code-pair classification task is much easier
compared to be given a snippet and deciding if and where a bug exists.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07966">Higher-Order Expander Graph Propagation. (arXiv:2311.07966v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Christie_T/0/1/0/all/0/1">Thomas Christie</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yu He</a></p>
<p>Graph neural networks operate on graph-structured data via exchanging
messages along edges. One limitation of this message passing paradigm is the
over-squashing problem. Over-squashing occurs when messages from a node's
expanded receptive field are compressed into fixed-size vectors, potentially
causing information loss. To address this issue, recent works have explored
using expander graphs, which are highly-connected sparse graphs with low
diameters, to perform message passing. However, current methods on expander
graph propagation only consider pair-wise interactions, ignoring higher-order
structures in complex data. To explore the benefits of capturing these
higher-order correlations while still leveraging expander graphs, we introduce
higher-order expander graph propagation. We propose two methods for
constructing bipartite expanders and evaluate their performance on both
synthetic and real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07967">Comparison of two data fusion approaches for land use classification. (arXiv:2311.07967v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cubaud_M/0/1/0/all/0/1">Martin Cubaud</a> (LaSTIG), <a href="http://arxiv.org/find/cs/1/au:+Bris_A/0/1/0/all/0/1">Arnaud Le Bris</a> (LaSTIG), <a href="http://arxiv.org/find/cs/1/au:+Jolivet_L/0/1/0/all/0/1">Laurence Jolivet</a> (LaSTIG), <a href="http://arxiv.org/find/cs/1/au:+Olteanu_Raimond_A/0/1/0/all/0/1">Ana-Maria Olteanu-Raimond</a> (LaSTIG)</p>
<p>Accurate land use maps, describing the territory from an anthropic
utilisation point of view, are useful tools for land management and planning.
To produce them, the use of optical images alone remains limited. It is
therefore necessary to make use of several heterogeneous sources, each carrying
complementary or contradictory information due to their imperfections or their
different specifications. This study compares two different approaches i.e. a
pre-classification and a post-classification fusion approach for combining
several sources of spatial data in the context of land use classification. The
approaches are applied on authoritative land use data located in the Gers
department in the southwest of France. Pre-classification fusion, while not
explicitly modeling imperfections, has the best final results, reaching an
overall accuracy of 97% and a macro-mean F1 score of 88%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07975">Out-of-Distribution Knowledge Distillation via Confidence Amendment. (arXiv:2311.07975v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhilin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Longbing Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a></p>
<p>Out-of-distribution (OOD) detection is essential in identifying test samples
that deviate from the in-distribution (ID) data upon which a standard network
is trained, ensuring network robustness and reliability. This paper introduces
OOD knowledge distillation, a pioneering learning framework applicable whether
or not training ID data is available, given a standard network. This framework
harnesses OOD-sensitive knowledge from the standard network to craft a binary
classifier adept at distinguishing between ID and OOD samples. To accomplish
this, we introduce Confidence Amendment (CA), an innovative methodology that
transforms an OOD sample into an ID one while progressively amending prediction
confidence derived from the standard network. This approach enables the
simultaneous synthesis of both ID and OOD samples, each accompanied by an
adjusted prediction confidence, thereby facilitating the training of a binary
classifier sensitive to OOD. Theoretical analysis provides bounds on the
generalization error of the binary classifier, demonstrating the pivotal role
of confidence amendment in enhancing OOD sensitivity. Extensive experiments
spanning various datasets and network architectures confirm the efficacy of the
proposed method in detecting OOD samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07978">How good are Large Language Models on African Languages?. (arXiv:2311.07978v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ojo_J/0/1/0/all/0/1">Jessica Ojo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1">Kelechi Ogueji</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1">Pontus Stenetorp</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1">David I. Adelani</a></p>
<p>Recent advancements in natural language processing have led to the
proliferation of large language models (LLMs). These models have been shown to
yield good performance, using in-context learning, even on unseen tasks and
languages. Additionally, they have been widely adopted as
language-model-as-a-service commercial APIs like GPT-4 API. However, their
performance on African languages is largely unknown. We present an analysis of
three popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks
(news topic classification, sentiment classification, machine translation,
question answering, and named entity recognition) across 30 African languages,
spanning different language families and geographical regions. Our results
suggest that all LLMs produce below-par performance on African languages, and
there is a large gap in performance compared to high-resource languages like
English most tasks. We find that GPT-4 has an average or impressive performance
on classification tasks but very poor results on generative tasks like machine
translation. Surprisingly, we find that mT0 had the best overall on
cross-lingual QA, better than the state-of-the-art supervised model (i.e.
fine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the
worst performance due to its limited multilingual capabilities and
English-centric pre-training corpus. In general, our findings present a
call-to-action to ensure African languages are well represented in large
language models, given their growing popularity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08005">Iterative missing value imputation based on feature importance. (arXiv:2311.08005v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Cong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a></p>
<p>Many datasets suffer from missing values due to various reasons,which not
only increases the processing difficulty of related tasks but also reduces the
accuracy of classification. To address this problem, the mainstream approach is
to use missing value imputation to complete the dataset. Existing imputation
methods estimate the missing parts based on the observed values in the original
feature space, and they treat all features as equally important during data
completion, while in fact different features have different importance.
Therefore, we have designed an imputation method that considers feature
importance. This algorithm iteratively performs matrix completion and feature
importance learning, and specifically, matrix completion is based on a filling
loss that incorporates feature importance. Our experimental analysis involves
three types of datasets: synthetic datasets with different noisy features and
missing values, real-world datasets with artificially generated missing values,
and real-world datasets originally containing missing values. The results on
these datasets consistently show that the proposed method outperforms the
existing five imputation algorithms.To the best of our knowledge, this is the
first work that considers feature importance in the imputation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08016">Velocity-Based Channel Charting with Spatial Distribution Map Matching. (arXiv:2311.08016v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Stahlke_M/0/1/0/all/0/1">Maximilian Stahlke</a>, <a href="http://arxiv.org/find/eess/1/au:+Yammine_G/0/1/0/all/0/1">George Yammine</a>, <a href="http://arxiv.org/find/eess/1/au:+Feigl_T/0/1/0/all/0/1">Tobias Feigl</a>, <a href="http://arxiv.org/find/eess/1/au:+Eskofier_B/0/1/0/all/0/1">Bjoern M. Eskofier</a>, <a href="http://arxiv.org/find/eess/1/au:+Mutschler_C/0/1/0/all/0/1">Christopher Mutschler</a></p>
<p>Fingerprint-based localization improves the positioning performance in
challenging, non-line-of-sight (NLoS) dominated indoor environments. However,
fingerprinting models require an expensive life-cycle management including
recording and labeling of radio signals for the initial training and regularly
at environmental changes. Alternatively, channel-charting avoids this labeling
effort as it implicitly associates relative coordinates to the recorded radio
signals. Then, with reference real-world coordinates (positions) we can use
such charts for positioning tasks. However, current channel-charting approaches
lag behind fingerprinting in their positioning accuracy and still require
reference samples for localization, regular data recording and labeling to keep
the models up to date. Hence, we propose a novel framework that does not
require reference positions. We only require information from velocity
information, e.g., from pedestrian dead reckoning or odometry to model the
channel charts, and topological map information, e.g., a building floor plan,
to transform the channel charts into real coordinates. We evaluate our approach
on two different real-world datasets using 5G and distributed
single-input/multiple-output system (SIMO) radio systems. Our experiments show
that even with noisy velocity estimates and coarse map information, we achieve
similar position accuracies
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08022">Two-Stage Predict+Optimize for Mixed Integer Linear Programs with Unknown Parameters in Constraints. (arXiv:2311.08022v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jasper C.H. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jimmy H.M. Lee</a></p>
<p>Consider the setting of constrained optimization, with some parameters
unknown at solving time and requiring prediction from relevant features.
Predict+Optimize is a recent framework for end-to-end training supervised
learning models for such predictions, incorporating information about the
optimization problem in the training process in order to yield better
predictions in terms of the quality of the predicted solution under the true
parameters. Almost all prior works have focused on the special case where the
unknowns appear only in the optimization objective and not the constraints. Hu
et al.~proposed the first adaptation of Predict+Optimize to handle unknowns
appearing in constraints, but the framework has somewhat ad-hoc elements, and
they provided a training algorithm only for covering and packing linear
programs. In this work, we give a new \emph{simpler} and \emph{more powerful}
framework called \emph{Two-Stage Predict+Optimize}, which we believe should be
the canonical framework for the Predict+Optimize setting. We also give a
training algorithm usable for all mixed integer linear programs, vastly
generalizing the applicability of the framework. Experimental results
demonstrate the superior prediction performance of our training framework over
all classical and state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08024">MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with Semi Supervised Learning for Low Dose CT. (arXiv:2311.08024v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Song_T/0/1/0/all/0/1">Tao Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Hou_R/0/1/0/all/0/1">Ruizhi Hou</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_L/0/1/0/all/0/1">Lisong Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1">Lei Xiang</a></p>
<p>Image quality assessment (IQA) plays a critical role in optimizing radiation
dose and developing novel medical imaging techniques in computed tomography
(CT). Traditional IQA methods relying on hand-crafted features have limitations
in summarizing the subjective perceptual experience of image quality. Recent
deep learning-based approaches have demonstrated strong modeling capabilities
and potential for medical IQA, but challenges remain regarding model
generalization and perceptual accuracy. In this work, we propose a multi-scale
distributions regression approach to predict quality scores by constraining the
output distribution, thereby improving model generalization. Furthermore, we
design a dual-branch alignment network to enhance feature extraction
capabilities. Additionally, semi-supervised learning is introduced by utilizing
pseudo-labels for unlabeled data to guide model training. Extensive qualitative
experiments demonstrate the effectiveness of our proposed method for advancing
the state-of-the-art in deep learning-based medical IQA. Code is available at:
https://github.com/zunzhumu/MD-IQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08035">Data-driven building energy efficiency prediction based on envelope heat losses using physics-informed neural networks. (arXiv:2311.08035v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Michalakopoulos_V/0/1/0/all/0/1">Vasilis Michalakopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelekis_S/0/1/0/all/0/1">Sotiris Pelekis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kormpakis_G/0/1/0/all/0/1">Giorgos Kormpakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Karakolis_V/0/1/0/all/0/1">Vagelis Karakolis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouzakitis_S/0/1/0/all/0/1">Spiros Mouzakitis</a>, <a href="http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1">Dimitris Askounis</a></p>
<p>The analytical prediction of building energy performance in residential
buildings based on the heat losses of its individual envelope components is a
challenging task. It is worth noting that this field is still in its infancy,
with relatively limited research conducted in this specific area to date,
especially when it comes for data-driven approaches. In this paper we introduce
a novel physics-informed neural network model for addressing this problem.
Through the employment of unexposed datasets that encompass general building
information, audited characteristics, and heating energy consumption, we feed
the deep learning model with general building information, while the model's
output consists of the structural components and several thermal properties
that are in fact the basic elements of an energy performance certificate (EPC).
On top of this neural network, a function, based on physics equations,
calculates the energy consumption of the building based on heat losses and
enhances the loss function of the deep learning model. This methodology is
tested on a real case study for 256 buildings located in Riga, Latvia. Our
investigation comes up with promising results in terms of prediction accuracy,
paving the way for automated, and data-driven energy efficiency performance
prediction based on basic properties of the building, contrary to exhaustive
energy efficiency audits led by humans, which are the current status quo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08045">Adversarial Preference Optimization. (arXiv:2311.08045v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pengyu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1">Nan Du</a></p>
<p>Human preference alignment is a crucial training step to improve the
interaction quality of large language models (LLMs). Existing aligning methods
depend on manually annotated preference data to guide the LLM optimization
directions. However, in practice, continuously updating LLMs raises a
distribution gap between model-generated samples and human-preferred responses,
which hinders model fine-tuning efficiency. To mitigate this issue, previous
methods require additional preference annotation on generated samples to adapt
the shifted distribution, which consumes a large amount of annotation
resources. Targeting more efficient human preference optimization, we propose
an adversarial preference optimization (APO) framework, where the LLM agent and
the preference model update alternatively via a min-max game. Without
additional annotation, our APO method can make a self-adaption to the
generation distribution gap through the adversarial learning process. In
experiments, we empirically verify the effectiveness of APO in improving LLM's
helpfulness and harmlessness compared with rejection sampling baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08053">Communication-Constrained Bayesian Active Knowledge Distillation. (arXiv:2311.08053v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Croisfelt_V/0/1/0/all/0/1">Victor Croisfelt</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1">Shashi Raj Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1">Osvaldo Simeone</a>, <a href="http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1">Petar Popovski</a></p>
<p>Consider an active learning setting in which a learner has a training set
with few labeled examples and a pool set with many unlabeled inputs, while a
remote teacher has a pre-trained model that is known to perform well for the
learner's task. The learner actively transmits batches of unlabeled inputs to
the teacher through a constrained communication channel for labeling. This
paper addresses the following key questions: (i) Active batch selection: Which
batch of inputs should be sent to the teacher to acquire the most useful
information and thus reduce the number of required communication rounds? (ii)
Batch encoding: How do we encode the batch of inputs for transmission to the
teacher to reduce the communication resources required at each round? We
introduce Communication-Constrained Bayesian Active Knowledge Distillation
(CC-BAKD), a novel protocol that integrates Bayesian active learning with
compression via a linear mix-up mechanism. Bayesian active learning selects the
batch of inputs based on their epistemic uncertainty, addressing the
"confirmation bias" that is known to increase the number of required
communication rounds. Furthermore, the proposed mix-up compression strategy is
integrated with the epistemic uncertainty-based active batch selection process
to reduce the communication overhead per communication round.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08081">Evolutionary-enhanced quantum supervised learning model. (arXiv:2311.08081v1 [quant-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Albino_A/0/1/0/all/0/1">Anton Simen Albino</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Bloot_R/0/1/0/all/0/1">Rodrigo Bloot</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pires_O/0/1/0/all/0/1">Otto M. Pires</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nascimento_E/0/1/0/all/0/1">Erick G. S. Nascimento</a></p>
<p>Quantum supervised learning, utilizing variational circuits, stands out as a
promising technology for NISQ devices due to its efficiency in hardware
resource utilization during the creation of quantum feature maps and the
implementation of hardware-efficient ansatz with trainable parameters. Despite
these advantages, the training of quantum models encounters challenges, notably
the barren plateau phenomenon, leading to stagnation in learning during
optimization iterations. This study proposes an innovative approach: an
evolutionary-enhanced ansatz-free supervised learning model. In contrast to
parametrized circuits, our model employs circuits with variable topology that
evolves through an elitist method, mitigating the barren plateau issue.
Additionally, we introduce a novel concept, the superposition of multi-hot
encodings, facilitating the treatment of multi-classification problems. Our
framework successfully avoids barren plateaus, resulting in enhanced model
accuracy. Comparative analysis with variational quantum classifiers from the
technology's state-of-the-art reveal a substantial improvement in training
efficiency and precision. Furthermore, we conduct tests on a challenging
dataset class, traditionally problematic for conventional kernel machines,
demonstrating a potential alternative path for achieving quantum advantage in
supervised learning for NISQ era.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08083">Solving ARC visual analogies with neural embeddings and vector arithmetic: A generalized method. (arXiv:2311.08083v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thoms_L/0/1/0/all/0/1">Luca H. Thoms</a>, <a href="http://arxiv.org/find/cs/1/au:+Veldkamp_K/0/1/0/all/0/1">Karel A. Veldkamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenbusch_H/0/1/0/all/0/1">Hannes Rosenbusch</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevenson_C/0/1/0/all/0/1">Claire E. Stevenson</a></p>
<p>Analogical reasoning derives information from known relations and generalizes
this information to similar yet unfamiliar situations. One of the first
generalized ways in which deep learning models were able to solve verbal
analogies was through vector arithmetic of word embeddings, essentially
relating words that were mapped to a vector space (e.g., king - man + woman =
__?). In comparison, most attempts to solve visual analogies are still
predominantly task-specific and less generalizable. This project focuses on
visual analogical reasoning and applies the initial generalized mechanism used
to solve verbal analogies to the visual realm. Taking the Abstraction and
Reasoning Corpus (ARC) as an example to investigate visual analogy solving, we
use a variational autoencoder (VAE) to transform ARC items into low-dimensional
latent vectors, analogous to the word embeddings used in the verbal approaches.
Through simple vector arithmetic, underlying rules of ARC items are discovered
and used to solve them. Results indicate that the approach works well on simple
items with fewer dimensions (i.e., few colors used, uniform shapes), similar
input-to-output examples, and high reconstruction accuracy on the VAE.
Predictions on more complex items showed stronger deviations from expected
outputs, although, predictions still often approximated parts of the item's
rule set. Error patterns indicated that the model works as intended. On the
official ARC paradigm, the model achieved a score of 2% (cf. current world
record is 21%) and on ConceptARC it scored 8.8%. Although the methodology
proposed involves basic dimensionality reduction techniques and standard vector
arithmetic, this approach demonstrates promising outcomes on ARC and can easily
be generalized to other abstract visual reasoning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08094">Act-VIT: A Representationally Robust Attention Architecture for Skeleton Based Action Recognition Using Vision Transformer. (arXiv:2311.08094v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karadag_O/0/1/0/all/0/1">Ozge Oztimur Karadag</a></p>
<p>Skeleton-based action recognition receives the attention of many researchers
as it is robust to viewpoint and illumination changes, and its processing is
much more efficient than video frames. With the emergence of deep learning
models, it has become very popular to represent the skeleton data in
pseudo-image form and apply Convolutional Neural Networks for action
recognition. Thereafter, studies concentrated on finding effective methods for
forming pseudo-images. Recently, attention networks, more specifically
transformers have provided promising results in various vision problems. In
this study, the effectiveness of vision transformers for skeleton-based action
recognition is examined and its robustness on the pseudo-image representation
scheme is investigated. To this end, a three-level architecture, Act-VIT is
proposed, which forms a set of pseudo images apply a classifier on each of the
representation and combine their results to find the final action class. The
classifiers of Act-VIT are first realized by CNNs and then by VITs and their
performances are compared. Experimental studies reveal that the vision
transformer is less sensitive to the initial pseudo-image representation
compared to CNN. Nevertheless, even with the vision transformer, the
recognition performance can be further improved by consensus of classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08105">DiLoCo: Distributed Low-Communication Training of Language Models. (arXiv:2311.08105v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1">Arthur Douillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1">Qixuan Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1">Andrei A. Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1">Rachita Chhaparia</a>, <a href="http://arxiv.org/find/cs/1/au:+Donchev_Y/0/1/0/all/0/1">Yani Donchev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1">Adhiguna Kuncoro</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1">Marc&#x27;Aurelio Ranzato</a>, <a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1">Arthur Szlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jiajun Shen</a></p>
<p>Large language models (LLM) have become a critical component in many
applications of machine learning. However, standard approaches to training LLM
require a large number of tightly interconnected accelerators, with devices
exchanging gradients and other intermediate states at each optimization step.
While it is difficult to build and maintain a single computing cluster hosting
many accelerators, it might be easier to find several computing clusters each
hosting a smaller number of devices. In this work, we propose a distributed
optimization algorithm, Distributed Low-Communication (DiLoCo), that enables
training of language models on islands of devices that are poorly connected.
The approach is a variant of federated averaging, where the number of inner
steps is large, the inner optimizer is AdamW, and the outer optimizer is
Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8
workers performs as well as fully synchronous optimization while communicating
500 times less. DiLoCo exhibits great robustness to the data distribution of
each worker. It is also robust to resources becoming unavailable over time, and
vice versa, it can seamlessly leverage resources that become available during
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08113">Understanding learning from EEG data: Combining machine learning and feature engineering based on hidden Markov models and mixed models. (arXiv:2311.08113v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Palma_G/0/1/0/all/0/1">Gabriel Rodrigues Palma</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Thornberry_C/0/1/0/all/0/1">Conor Thornberry</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Commins_S/0/1/0/all/0/1">Se&#xe1;n Commins</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Moral_R/0/1/0/all/0/1">Rafael de Andrade Moral</a></p>
<p>Theta oscillations, ranging from 4-8 Hz, play a significant role in spatial
learning and memory functions during navigation tasks. Frontal theta
oscillations are thought to play an important role in spatial navigation and
memory. Electroencephalography (EEG) datasets are very complex, making any
changes in the neural signal related to behaviour difficult to interpret.
However, multiple analytical methods are available to examine complex data
structure, especially machine learning based techniques. These methods have
shown high classification performance and the combination with feature
engineering enhances the capability of these methods. This paper proposes using
hidden Markov and linear mixed effects models to extract features from EEG
data. Based on the engineered features obtained from frontal theta EEG data
during a spatial navigation task in two key trials (first, last) and between
two conditions (learner and non-learner), we analysed the performance of six
machine learning methods (Polynomial Support Vector Machines, Non-linear
Support Vector Machines, Random Forests, K-Nearest Neighbours, Ridge, and Deep
Neural Networks) on classifying learner and non-learner participants. We also
analysed how different standardisation methods used to pre-process the EEG data
contribute to classification performance. We compared the classification
performance of each trial with data gathered from the same subjects, including
solely coordinate-based features, such as idle time and average speed. We found
that more machine learning methods perform better classification using
coordinate-based data. However, only deep neural networks achieved an area
under the ROC curve higher than 80% using the theta EEG data alone. Our
findings suggest that standardising the theta EEG data and using deep neural
networks enhances the classification of learner and non-learner subjects in a
spatial learning task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08118">Evaluating Neighbor Explainability for Graph Neural Networks. (arXiv:2311.08118v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Llorente_O/0/1/0/all/0/1">Oscar Llorente</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaderna_P/0/1/0/all/0/1">P&#xe9;ter Vaderna</a>, <a href="http://arxiv.org/find/cs/1/au:+Laki_S/0/1/0/all/0/1">S&#xe1;ndor Laki</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotroczo_R/0/1/0/all/0/1">Roland Kotrocz&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Csoma_R/0/1/0/all/0/1">Rita Csoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Szalai_Gindl_J/0/1/0/all/0/1">J&#xe1;nos M&#xe1;rk Szalai-Gindl</a></p>
<p>Explainability in Graph Neural Networks (GNNs) is a new field growing in the
last few years. In this publication we address the problem of determining how
important is each neighbor for the GNN when classifying a node and how to
measure the performance for this specific task. To do this, various known
explainability methods are reformulated to get the neighbor importance and four
new metrics are presented. Our results show that there is almost no difference
between the explanations provided by gradient-based techniques in the GNN
domain. In addition, many explainability techniques failed to identify
important neighbors when GNNs without self-loops are used.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08123">Memory-efficient Stochastic methods for Memory-based Transformers. (arXiv:2311.08123v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vishnu_V/0/1/0/all/0/1">Vishwajit Kumar Vishnu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekhar_C/0/1/0/all/0/1">C. Chandra Sekhar</a></p>
<p>Training Memory-based transformers can require a large amount of memory and
can be quite inefficient. We propose a novel two-phase training mechanism and a
novel regularization technique to improve the training efficiency of
memory-based transformers, which are often used for long-range context
problems. For our experiments, we consider transformer-XL as our baseline model
which is one of memorybased transformer models. We show that our resultant
model, Skip Cross-head TransformerXL, outperforms the baseline on character
level language modeling task with similar parameters and outperforms the
baseline on word level language modelling task with almost 20% fewer
parameters. Our proposed methods do not require any additional memory. We also
demonstrate the effectiveness of our regularization mechanism on BERT which
shows similar performance with reduction in standard deviation of scores of
around 30% on multiple GLUE tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08125">Lite it fly: An All-Deformable-Butterfly Network. (arXiv:2311.08125v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1">Rui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jason Chun Lok Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiajun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1">Binxiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ran_J/0/1/0/all/0/1">Jie Ran</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1">Ngai Wong</a></p>
<p>Most deep neural networks (DNNs) consist fundamentally of convolutional
and/or fully connected layers, wherein the linear transform can be cast as the
product between a filter matrix and a data matrix obtained by arranging feature
tensors into columns. The lately proposed deformable butterfly (DeBut)
decomposes the filter matrix into generalized, butterflylike factors, thus
achieving network compression orthogonal to the traditional ways of pruning or
low-rank decomposition. This work reveals an intimate link between DeBut and a
systematic hierarchy of depthwise and pointwise convolutions, which explains
the empirically good performance of DeBut layers. By developing an automated
DeBut chain generator, we show for the first time the viability of homogenizing
a DNN into all DeBut layers, thus achieving an extreme sparsity and
compression. Various examples and hardware benchmarks verify the advantages of
All-DeBut networks. In particular, we show it is possible to compress a
PointNet to &lt; 5% parameters with &lt; 5% accuracy drop, a record not achievable by
other compression schemes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08141">GMTR: Graph Matching Transformers. (arXiv:2311.08141v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jinpei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaofeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Runzhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a></p>
<p>Vision transformers (ViTs) have recently been used for visual matching beyond
object detection and segmentation. However, the original grid dividing strategy
of ViTs neglects the spatial information of the keypoints, limiting the
sensitivity to local information. Therefore, we propose \textbf{QueryTrans}
(Query Transformer), which adopts a cross-attention module and keypoints-based
center crop strategy for better spatial information extraction. We further
integrate the graph attention module and devise a transformer-based graph
matching approach \textbf{GMTR} (Graph Matching TRansformers) whereby the
combinatorial nature of GM is addressed by a graph transformer neural GM
solver. On standard GM benchmarks, GMTR shows competitive performance against
the SOTA frameworks. Specifically, on Pascal VOC, GMTR achieves
$\mathbf{83.6\%}$ accuracy, $\mathbf{0.9\%}$ higher than the SOTA framework. On
Spair-71k, GMTR shows great potential and outperforms most of the previous
works. Meanwhile, on Pascal VOC, QueryTrans improves the accuracy of NGMv2 from
$80.1\%$ to $\mathbf{83.3\%}$, and BBGM from $79.0\%$ to $\mathbf{84.5\%}$. On
Spair-71k, QueryTrans improves NGMv2 from $80.6\%$ to $\mathbf{82.5\%}$, and
BBGM from $82.1\%$ to $\mathbf{83.9\%}$. Source code will be made publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08148">Cattle Identification Using Muzzle Images and Deep Learning Techniques. (arXiv:2311.08148v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kimani_G/0/1/0/all/0/1">G. N. Kimani</a>, <a href="http://arxiv.org/find/cs/1/au:+Oluwadara_P/0/1/0/all/0/1">P. Oluwadara</a>, <a href="http://arxiv.org/find/cs/1/au:+Fashingabo_P/0/1/0/all/0/1">P. Fashingabo</a>, <a href="http://arxiv.org/find/cs/1/au:+Busogi_M/0/1/0/all/0/1">M. Busogi</a>, <a href="http://arxiv.org/find/cs/1/au:+Luhanga_E/0/1/0/all/0/1">E. Luhanga</a>, <a href="http://arxiv.org/find/cs/1/au:+Sowon_K/0/1/0/all/0/1">K. Sowon</a>, <a href="http://arxiv.org/find/cs/1/au:+Chacha_L/0/1/0/all/0/1">L. Chacha</a> ((1) CyLab-Africa / Upanzi Network, (2) Carnegie Mellon University Africa and (3) Carnegie Mellon University Pittsburgh)</p>
<p>Traditional animal identification methods such as ear-tagging, ear notching,
and branding have been effective but pose risks to the animal and have
scalability issues. Electrical methods offer better tracking and monitoring but
require specialized equipment and are susceptible to attacks. Biometric
identification using time-immutable dermatoglyphic features such as muzzle
prints and iris patterns is a promising solution. This project explores cattle
identification using 4923 muzzle images collected from 268 beef cattle. Two
deep learning classification models are implemented - wide ResNet50 and
VGG16\_BN and image compression is done to lower the image quality and adapt
the models to work for the African context. From the experiments run, a maximum
accuracy of 99.5\% is achieved while using the wide ResNet50 model with a
compression retaining 25\% of the original image. From the study, it is noted
that the time required by the models to train and converge as well as
recognition time are dependent on the machine used to run the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08149">Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes. (arXiv:2311.08149v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Trottet_C/0/1/0/all/0/1">C&#xe9;cile Trottet</a>, <a href="http://arxiv.org/find/cs/1/au:+Schurch_M/0/1/0/all/0/1">Manuel Sch&#xfc;rch</a>, <a href="http://arxiv.org/find/cs/1/au:+Allam_A/0/1/0/all/0/1">Ahmed Allam</a>, <a href="http://arxiv.org/find/cs/1/au:+Barua_I/0/1/0/all/0/1">Imon Barua</a>, <a href="http://arxiv.org/find/cs/1/au:+Petelytska_L/0/1/0/all/0/1">Liubov Petelytska</a>, <a href="http://arxiv.org/find/cs/1/au:+Distler_O/0/1/0/all/0/1">Oliver Distler</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffmann_Vold_A/0/1/0/all/0/1">Anna-Maria Hoffmann-Vold</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1">Michael Krauthammer</a>, the <a href="http://arxiv.org/find/cs/1/au:+collaborators_E/0/1/0/all/0/1">EUSTAR collaborators</a></p>
<p>In this paper, we propose a deep generative time series approach using latent
temporal processes for modeling and holistically analyzing complex disease
trajectories. We aim to find meaningful temporal latent representations of an
underlying generative process that explain the observed disease trajectories in
an interpretable and comprehensive way. To enhance the interpretability of
these latent temporal processes, we develop a semi-supervised approach for
disentangling the latent space using established medical concepts. By combining
the generative approach with medical knowledge, we leverage the ability to
discover novel aspects of the disease while integrating medical concepts into
the model. We show that the learned temporal latent processes can be utilized
for further data analysis and clinical hypothesis testing, including finding
similar patients and clustering the disease into new sub-types. Moreover, our
method enables personalized online monitoring and prediction of multivariate
time series including uncertainty quantification. We demonstrate the
effectiveness of our approach in modeling systemic sclerosis, showcasing the
potential of our machine learning model to capture complex disease trajectories
and acquire new medical knowledge.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08150">The Hyperdimensional Transform for Distributional Modelling, Regression and Classification. (arXiv:2311.08150v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dewulf_P/0/1/0/all/0/1">Pieter Dewulf</a>, <a href="http://arxiv.org/find/cs/1/au:+Baets_B/0/1/0/all/0/1">Bernard De Baets</a>, <a href="http://arxiv.org/find/cs/1/au:+Stock_M/0/1/0/all/0/1">Michiel Stock</a></p>
<p>Hyperdimensional computing (HDC) is an increasingly popular computing
paradigm with immense potential for future intelligent applications. Although
the main ideas already took form in the 1990s, HDC recently gained significant
attention, especially in the field of machine learning and data science. Next
to efficiency, interoperability and explainability, HDC offers attractive
properties for generalization as it can be seen as an attempt to combine
connectionist ideas from neural networks with symbolic aspects. In recent work,
we introduced the hyperdimensional transform, revealing deep theoretical
foundations for representing functions and distributions as high-dimensional
holographic vectors. Here, we present the power of the hyperdimensional
transform to a broad data science audience. We use the hyperdimensional
transform as a theoretical basis and provide insight into state-of-the-art HDC
approaches for machine learning. We show how existing algorithms can be
modified and how this transform can lead to a novel, well-founded toolbox. Next
to the standard regression and classification tasks of machine learning, our
discussion includes various aspects of statistical modelling, such as
representation, learning and deconvolving distributions, sampling, Bayesian
inference, and uncertainty estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08166">MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. (arXiv:2311.08166v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bo Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Buehler_M/0/1/0/all/0/1">Markus J. Buehler</a></p>
<p>Solving mechanics problems using numerical methods requires comprehensive
intelligent capability of retrieving relevant knowledge and theory,
constructing and executing codes, analyzing the results, a task that has thus
far mainly been reserved for humans. While emerging AI methods can provide
effective approaches to solve end-to-end problems, for instance via the use of
deep surrogate models or various data analytics strategies, they often lack
physical intuition since knowledge is baked into the parametric complement
through training, offering less flexibility when it comes to incorporating
mathematical or physical insights. By leveraging diverse capabilities of
multiple dynamically interacting large language models (LLMs), we can overcome
the limitations of conventional approaches and develop a new class of
physics-inspired generative machine learning platform, here referred to as
MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for
elasticity problems, via autonomous collaborations. A two-agent team can
effectively write, execute and self-correct code, in order to apply finite
element methods to solve classical elasticity problems in various flavors
(different boundary conditions, domain geometries, meshes, small/finite
deformation and linear/hyper-elastic constitutive laws, and others). For more
complex tasks, we construct a larger group of agents with enhanced division of
labor among planning, formulating, coding, executing and criticizing the
process and results. The agents mutually correct each other to improve the
overall team-work performance in understanding, formulating and validating the
solution. Our framework shows the potential of synergizing the intelligence of
language models, the reliability of physics-based modeling, and the dynamic
collaborations among diverse agents, opening novel avenues for automation of
solving engineering problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08170">Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach. (arXiv:2311.08170v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marchetti_G/0/1/0/all/0/1">Giovanni Luca Marchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesa_G/0/1/0/all/0/1">Gabriele Cesa</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratik_K/0/1/0/all/0/1">Kumar Pratik</a>, <a href="http://arxiv.org/find/cs/1/au:+Behboodi_A/0/1/0/all/0/1">Arash Behboodi</a></p>
<p>Lattice reduction is a combinatorial optimization problem aimed at finding
the most orthogonal basis in a given lattice. In this work, we address lattice
reduction via deep learning methods. We design a deep neural model outputting
factorized unimodular matrices and train it in a self-supervised manner by
penalizing non-orthogonal lattice bases. We incorporate the symmetries of
lattice reduction into the model by making it invariant and equivariant with
respect to appropriate continuous and discrete groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08182">Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning. (arXiv:2311.08182v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shengguang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Keming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1">Benfeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Junyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1">Qi Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a></p>
<p>Enhancing the instruction-following ability of Large Language Models (LLMs)
primarily demands substantial instruction-tuning datasets. However, the sheer
volume of these imposes a considerable computational burden and annotation
cost. To investigate a label-efficient instruction tuning method that allows
the model itself to actively sample subsets that are equally or even more
effective, we introduce a self-evolving mechanism DiverseEvol. In this process,
a model iteratively augments its training subset to refine its own performance,
without requiring any intervention from humans or more advanced LLMs. The key
to our data sampling technique lies in the enhancement of diversity in the
chosen subsets, as the model selects new data points most distinct from any
existing ones according to its current embedding space. Extensive experiments
across three datasets and benchmarks demonstrate the effectiveness of
DiverseEvol. Our models, trained on less than 8% of the original dataset,
maintain or improve performance compared with finetuning on full data. We also
provide empirical evidence to analyze the importance of diversity in
instruction data and the iterative scheme as opposed to one-time sampling. Our
code is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08190">SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation. (arXiv:2311.08190v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yinuo Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_W/0/1/0/all/0/1">Weimin Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Meng_C/0/1/0/all/0/1">Cai Meng</a>, <a href="http://arxiv.org/find/eess/1/au:+Bai_X/0/1/0/all/0/1">XiangZhi Bai</a></p>
<p>Segment Anything Model (SAM), a vision foundation model trained on
large-scale annotations, has recently continued raising awareness within
medical image segmentation. Despite the impressive capabilities of SAM on
natural scenes, it struggles with performance decline when confronted with
medical images, especially those involving blurry boundaries and highly
irregular regions of low contrast. In this paper, a SAM-based
parameter-efficient fine-tuning method, called SAMIHS, is proposed for
intracranial hemorrhage segmentation, which is a crucial and challenging step
in stroke diagnosis and surgical planning. Distinguished from previous SAM and
SAM-based methods, SAMIHS incorporates parameter-refactoring adapters into
SAM's image encoder and considers the efficient and flexible utilization of
adapters' parameters. Additionally, we employ a combo loss that combines binary
cross-entropy loss and boundary-sensitive loss to enhance SAMIHS's ability to
recognize the boundary regions. Our experimental results on two public datasets
demonstrate the effectiveness of our proposed method. Code is available at
https://github.com/mileswyn/SAMIHS .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08199">Diffusion-based generation of Histopathological Whole Slide Images at a Gigapixel scale. (arXiv:2311.08199v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Harb_R/0/1/0/all/0/1">Robert Harb</a>, <a href="http://arxiv.org/find/eess/1/au:+Pock_T/0/1/0/all/0/1">Thomas Pock</a>, <a href="http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1">Heimo M&#xfc;ller</a></p>
<p>We present a novel diffusion-based approach to generate synthetic
histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel
scale. Synthetic WSIs have many potential applications: They can augment
training datasets to enhance the performance of many computational pathology
applications. They allow the creation of synthesized copies of datasets that
can be shared without violating privacy regulations. Or they can facilitate
learning representations of WSIs without requiring data annotations. Despite
this variety of applications, no existing deep-learning-based method generates
WSIs at their typically high resolutions. Mainly due to the high computational
complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to
tackle image generation of high-resolution WSIs. In this scheme, we increase
the resolution of an initial low-resolution image to a high-resolution WSI.
Particularly, a diffusion model sequentially adds fine details to images and
increases their resolution. In our experiments, we train our method with WSIs
from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also
performed a user study with pathologists. The study results suggest that our
generated WSIs resemble the structure of real WSIs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08202">Federated Skewed Label Learning with Logits Fusion. (arXiv:2311.08202v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Runhan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1">Hao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xuefeng Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Sheng Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Min Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1">Bo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyuan Wu</a></p>
<p>Federated learning (FL) aims to collaboratively train a shared model across
multiple clients without transmitting their local data. Data heterogeneity is a
critical challenge in realistic FL settings, as it causes significant
performance deterioration due to discrepancies in optimization among local
models. In this work, we focus on label distribution skew, a common scenario in
data heterogeneity, where the data label categories are imbalanced on each
client. To address this issue, we propose FedBalance, which corrects the
optimization bias among local models by calibrating their logits. Specifically,
we introduce an extra private weak learner on the client side, which forms an
ensemble model with the local model. By fusing the logits of the two models,
the private weak learner can capture the variance of different data, regardless
of their category. Therefore, the optimization direction of local models can be
improved by increasing the penalty for misclassifying minority classes and
reducing the attention to majority classes, resulting in a better global model.
Extensive experiments show that our method can gain 13\% higher average
accuracy compared with state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08228">Counterfactual Explanation for Regression via Disentanglement in Latent Space. (arXiv:2311.08228v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1">Klaus Broelemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1">Gjergji Kasneci</a></p>
<p>Counterfactual Explanations (CEs) help address the question: How can the
factors that influence the prediction of a predictive model be changed to
achieve a more favorable outcome from a user's perspective? Thus, they bear the
potential to guide the user's interaction with AI systems since they represent
easy-to-understand explanations. To be applicable, CEs need to be realistic and
actionable. In the literature, various methods have been proposed to generate
CEs. However, the majority of research on CEs focuses on classification
problems where questions like ``What should I do to get my rejected loan
approved?" are raised. In practice, answering questions like ``What should I do
to increase my salary?" are of a more regressive nature. In this paper, we
introduce a novel method to generate CEs for a pre-trained regressor by first
disentangling the label-relevant from the label-irrelevant dimensions in the
latent space. CEs are then generated by combining the label-irrelevant
dimensions and the predefined output. The intuition behind this approach is
that the ideal counterfactual search should focus on the label-irrelevant
characteristics of the input and suggest changes toward target-relevant
characteristics. Searching in the latent space could help achieve this goal. We
show that our method maintains the characteristics of the query sample during
the counterfactual search. In various experiments, we demonstrate that the
proposed method is competitive based on different quality measures on image and
tabular datasets in regression problem settings. It efficiently returns results
closer to the original data manifold compared to three state-of-the-art
methods, which is essential for realistic high-dimensional machine learning
applications. Our code will be made available as an open-source package upon
the publication of this work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08252">REST: Retrieval-Based Speculative Decoding. (arXiv:2311.08252v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhenyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zexuan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a></p>
<p>We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
designed to speed up language model generation. The key insight driving the
development of REST is the observation that the process of text generation
often includes certain common phases and patterns. Unlike previous methods that
rely on a draft language model for speculative decoding, REST harnesses the
power of retrieval to generate draft tokens. This method draws from the
reservoir of existing knowledge, retrieving and employing relevant tokens based
on the current context. Its plug-and-play nature allows for seamless
integration and acceleration of any language models, all without necessitating
additional training. When benchmarked on 7B and 13B language models in a
single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on
code or text generation. The code of REST is available at
https://github.com/FasterDecoding/REST.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08271">Mobility-Induced Graph Learning for WiFi Positioning. (arXiv:2311.08271v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kyuwon Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Seung Min Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seong-Lyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1">Seung-Woo Ko</a></p>
<p>A smartphone-based user mobility tracking could be effective in finding
his/her location, while the unpredictable error therein due to low
specification of built-in inertial measurement units (IMUs) rejects its
standalone usage but demands the integration to another positioning technique
like WiFi positioning. This paper aims to propose a novel integration technique
using a graph neural network called Mobility-INduced Graph LEarning (MINGLE),
which is designed based on two types of graphs made by capturing different user
mobility features. Specifically, considering sequential measurement points
(MPs) as nodes, a user's regular mobility pattern allows us to connect neighbor
MPs as edges, called time-driven mobility graph (TMG). Second, a user's
relatively straight transition at a constant pace when moving from one position
to another can be captured by connecting the nodes on each path, called a
direction-driven mobility graph (DMG). Then, we can design graph convolution
network (GCN)-based cross-graph learning, where two different GCN models for
TMG and DMG are jointly trained by feeding different input features created by
WiFi RTTs yet sharing their weights. Besides, the loss function includes a
mobility regularization term such that the differences between adjacent
location estimates should be less variant due to the user's stable moving pace.
Noting that the regularization term does not require ground-truth location,
MINGLE can be designed under semi- and self-supervised learning frameworks. The
proposed MINGLE's effectiveness is extensively verified through field
experiments, showing a better positioning accuracy than benchmarks, say root
mean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- and
semi-supervised learning cases, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08272">Mixed Attention Network for Cross-domain Sequential Recommendation. (arXiv:2311.08272v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guanyu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chen Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jianxin Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1">Yanan Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1">Kun Gai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhiheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1">Depeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a></p>
<p>In modern recommender systems, sequential recommendation leverages
chronological user behaviors to make effective next-item suggestions, which
suffers from data sparsity issues, especially for new users. One promising line
of work is the cross-domain recommendation, which trains models with data
across multiple domains to improve the performance in data-scarce domains.
Recent proposed cross-domain sequential recommendation models such as PiNet and
DASL have a common drawback relying heavily on overlapped users in different
domains, which limits their usage in practical recommender systems. In this
paper, we propose a Mixed Attention Network (MAN) with local and global
attention modules to extract the domain-specific and cross-domain information.
Firstly, we propose a local/global encoding layer to capture the
domain-specific/cross-domain sequential pattern. Then we propose a mixed
attention layer with item similarity attention, sequence-fusion attention, and
group-prototype attention to capture the local/global item similarity, fuse the
local/global item sequence, and extract the user groups across different
domains, respectively. Finally, we propose a local/global prediction layer to
further evolve and combine the domain-specific and cross-domain interests.
Experimental results on two real-world datasets (each with two domains)
demonstrate the superiority of our proposed model. Further study also
illustrates that our proposed method and components are model-agnostic and
effective, respectively. The code and data are available at
https://github.com/Guanyu-Lin/MAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.14350">The convergence of the Stochastic Gradient Descent (SGD) : a self-contained proof. (arXiv:2103.14350v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Turinici_G/0/1/0/all/0/1">Gabrel Turinici</a></p>
<p>We give here a proof of the convergence of the Stochastic Gradient Descent
(SGD) in a self-contained manner.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2104.13881">Large Scale Prediction with Decision Trees. (arXiv:2104.13881v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1">Jason M. Klusowski</a>, <a href="http://arxiv.org/find/stat/1/au:+Tian_P/0/1/0/all/0/1">Peter M. Tian</a></p>
<p>This paper shows that decision trees constructed with Classification and
Regression Trees (CART) and C4.5 methodology are consistent for regression and
classification tasks, even when the number of predictor variables grows
sub-exponentially with the sample size, under natural 0-norm and 1-norm
sparsity constraints. The theory applies to a wide range of models, including
(ordinary or logistic) additive regression models with component functions that
are continuous, of bounded variation, or, more generally, Borel measurable.
Consistency holds for arbitrary joint distributions of the predictor variables,
thereby accommodating continuous, discrete, and/or dependent data. Finally, we
show that these qualitative properties of individual trees are inherited by
Breiman's random forests. A key step in the analysis is the establishment of an
oracle inequality, which allows for a precise characterization of the
goodness-of-fit and complexity tradeoff for a mis-specified model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.06854">A Deep Reinforcement Learning Approach to Marginalized Importance Sampling with the Successor Representation. (arXiv:2106.06854v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1">Scott Fujimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1">David Meger</a>, <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a></p>
<p>Marginalized importance sampling (MIS), which measures the density ratio
between the state-action occupancy of a target policy and that of a sampling
distribution, is a promising approach for off-policy evaluation. However,
current state-of-the-art MIS methods rely on complex optimization tricks and
succeed mostly on simple toy problems. We bridge the gap between MIS and deep
reinforcement learning by observing that the density ratio can be computed from
the successor representation of the target policy. The successor representation
can be trained through deep reinforcement learning methodology and decouples
the reward optimization from the dynamics of the environment, making the
resulting algorithm stable and applicable to high-dimensional domains. We
evaluate the empirical performance of our approach on a variety of challenging
Atari and MuJoCo environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.12945">Low-rank variational Bayes correction to the Laplace method. (arXiv:2111.12945v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Niekerk_J/0/1/0/all/0/1">Janet van Niekerk</a>, <a href="http://arxiv.org/find/stat/1/au:+Rue_H/0/1/0/all/0/1">Haavard Rue</a></p>
<p>Approximate inference methods like the Laplace method, Laplace approximations
and variational methods, amongst others, are popular methods when exact
inference is not feasible due to the complexity of the model or the abundance
of data. In this paper we propose a hybrid approximate method called Low-Rank
Variational Bayes correction (VBC), that uses the Laplace method and
subsequently a Variational Bayes correction in a lower dimension, to the joint
posterior mean. The cost is essentially that of the Laplace method which
ensures scalability of the method, in both model complexity and data size.
Models with fixed and unknown hyperparameters are considered, for simulated and
real examples, for small and large datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.10859">Visualizing the Diversity of Representations Learned by Bayesian Neural Networks. (arXiv:2201.10859v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1">Dennis Grinwald</a>, <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1">Shinichi Nakajima</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a></p>
<p>Explainable Artificial Intelligence (XAI) aims to make learning machines less
opaque, and offers researchers and practitioners various tools to reveal the
decision-making strategies of neural networks. In this work, we investigate how
XAI methods can be used for exploring and visualizing the diversity of feature
representations learned by Bayesian Neural Networks (BNNs). Our goal is to
provide a global understanding of BNNs by making their decision-making
strategies a) visible and tangible through feature visualizations and b)
quantitatively measurable with a distance measure learned by contrastive
learning. Our work provides new insights into the \emph{posterior} distribution
in terms of human-understandable feature information with regard to the
underlying decision making strategies. The main findings of our work are the
following: 1) global XAI methods can be applied to explain the diversity of
decision-making strategies of BNN instances, 2) Monte Carlo dropout with
commonly used Dropout rates exhibit increased diversity in feature
representations compared to the multimodal posterior approximation of
MultiSWAG, 3) the diversity of learned feature representations highly
correlates with the uncertainty estimate for the output and 4) the inter-mode
diversity of the multimodal posterior decreases as the network width increases,
while the intra mode diversity increases. These findings are consistent with
the recent Deep Neural Networks theory, providing additional intuitions about
what the theory implies in terms of humanly understandable concepts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.03359">An efficient semi-supervised quality control system trained using physics-based MRI-artefact generators and adversarial training. (arXiv:2206.03359v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ravi_D/0/1/0/all/0/1">Daniele Ravi</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative), <a href="http://arxiv.org/find/eess/1/au:+Barkhof_F/0/1/0/all/0/1">Frederik Barkhof</a>, <a href="http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1">Daniel C. Alexander</a>, <a href="http://arxiv.org/find/eess/1/au:+Puglisi_L/0/1/0/all/0/1">Lemuel Puglisi</a>, <a href="http://arxiv.org/find/eess/1/au:+Parker_G/0/1/0/all/0/1">Geoffrey JM Parker</a>, <a href="http://arxiv.org/find/eess/1/au:+Eshaghi_A/0/1/0/all/0/1">Arman Eshaghi</a></p>
<p>Large medical imaging data sets are becoming increasingly available, but
ensuring sample quality without significant artefacts is challenging. Existing
methods for identifying imperfections in medical imaging rely on data-intensive
approaches, compounded by a scarcity of artefact-rich scans for training
machine learning models in clinical research. To tackle this problem, we
propose a framework with four main components: 1) artefact generators inspired
by magnetic resonance physics to corrupt brain MRI scans and augment a training
dataset, 2) abstract and engineered features to represent images compactly, 3)
a feature selection process depending on the artefact class to improve
classification, and 4) SVM classifiers to identify artefacts. Our contributions
are threefold: first, physics-based artefact generators produce synthetic brain
MRI scans with controlled artefacts for data augmentation. This will avoid the
labour-intensive collection and labelling process of scans with rare artefacts.
Second, we propose a pool of abstract and engineered image features to identify
9 different artefacts for structural MRI. Finally, we use an artefact-based
feature selection block that, for each class of artefacts, finds the set of
features providing the best classification performance. We performed validation
experiments on a large data set of scans with artificially-generated artefacts,
and in a multiple sclerosis clinical trial where real artefacts were identified
by experts, showing that the proposed pipeline outperforms traditional methods.
In particular, our data augmentation increases performance by up to 12.5
percentage points on accuracy, precision, and recall. The computational
efficiency of our pipeline enables potential real-time deployment, promising
high-throughput clinical applications through automated image-processing
pipelines driven by quality control systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.10397">Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v13 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bingheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhengtian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1">Shupeng Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lin Zhao</a></p>
<p>Estimating and reacting to disturbances is crucial for robust flight control
of quadrotors. Existing estimators typically require significant tuning for a
specific flight scenario or training with extensive ground-truth disturbance
data to achieve satisfactory performance. In this paper, we propose a neural
moving horizon estimator (NeuroMHE) that can automatically tune its key
parameters modeled by a neural network and adapt to different flight scenarios.
We achieve this by deriving the analytical gradients of the MHE estimates with
respect to the MHE weighting matrices, which enables a seamless embedding of
the MHE as a learnable layer into the neural network for highly effective
learning. Interestingly, we show that the gradients can be computed efficiently
using a Kalman filter in a recursive form. Moreover, we develop a model-based
policy gradient algorithm to train NeuroMHE directly from the quadrotor
trajectory tracking error without needing the ground-truth disturbance data.
The effectiveness of NeuroMHE is verified extensively via both simulations and
physical experiments on quadrotors in various challenging flights. Notably,
NeuroMHE outperforms a state-of-the-art neural network-based estimator,
reducing force estimation errors by up to 76.7%, while using a portable neural
network that has only 7.7% of the learnable parameters of the latter. The
proposed method is general and can be applied to robust adaptive control of
other robotic systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08411">DynaConF: Dynamic Forecasting of Non-Stationary Time-Series. (arXiv:2209.08411v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Siqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehrmann_A/0/1/0/all/0/1">Andreas Lehrmann</a></p>
<p>Deep learning has shown impressive results in a variety of time series
forecasting tasks, where modeling the conditional distribution of the future
given the past is the essence. However, when this conditional distribution is
non-stationary, it poses challenges for these models to learn consistently and
to predict accurately. In this work, we propose a new method to model
non-stationary conditional distributions over time by clearly decoupling
stationary conditional distribution modeling from non-stationary dynamics
modeling. Our method is based on a Bayesian dynamic model that can adapt to
conditional distribution changes and a deep conditional distribution model that
handles multivariate time series using a factorized output space. Our
experimental results on synthetic and real-world datasets show that our model
can adapt to non-stationary time series better than state-of-the-art deep
learning solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03461">FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using Style Representations. (arXiv:2210.03461v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1">Ananda Padhmanabhan Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanjana Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1">Pavit Noinongyao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1">Ankush Ganguly</a>, <a href="http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1">Ukrit Watchareeruetai</a>, <a href="http://arxiv.org/find/cs/1/au:+Samacoits_A/0/1/0/all/0/1">Aubin Samacoits</a></p>
<p>In recent years, language-driven artistic style transfer has emerged as a new
type of style transfer technique, eliminating the need for a reference style
image by using natural language descriptions of the style. The first model to
achieve this, called CLIPstyler, has demonstrated impressive stylisation
results. However, its lengthy optimisation procedure at runtime for each query
limits its suitability for many practical applications. In this work, we
present FastCLIPstyler, a generalised text-based image style transfer model
capable of stylising images in a single forward pass for arbitrary text inputs.
Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for
compatibility with resource-constrained devices. Through quantitative and
qualitative comparisons with state-of-the-art approaches, we demonstrate that
our models achieve superior stylisation quality based on measurable metrics
while offering significantly improved runtime efficiency, particularly on edge
devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03475">Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization. (arXiv:2210.03475v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1">Nathan Grinsztajn</a>, <a href="http://arxiv.org/find/cs/1/au:+Furelos_Blanco_D/0/1/0/all/0/1">Daniel Furelos-Blanco</a>, <a href="http://arxiv.org/find/cs/1/au:+Surana_S/0/1/0/all/0/1">Shikha Surana</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonnet_C/0/1/0/all/0/1">Cl&#xe9;ment Bonnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrett_T/0/1/0/all/0/1">Thomas D. Barrett</a></p>
<p>Applying reinforcement learning (RL) to combinatorial optimization problems
is attractive as it removes the need for expert knowledge or pre-solved
instances. However, it is unrealistic to expect an agent to solve these (often
NP-)hard problems in a single shot at inference due to their inherent
complexity. Thus, leading approaches often implement additional search
strategies, from stochastic sampling and beam search to explicit fine-tuning.
In this paper, we argue for the benefits of learning a population of
complementary policies, which can be simultaneously rolled out at inference. To
this end, we introduce Poppy, a simple training procedure for populations.
Instead of relying on a predefined or hand-crafted notion of diversity, Poppy
induces an unsupervised specialization targeted solely at maximizing the
performance of the population. We show that Poppy produces a set of
complementary policies, and obtains state-of-the-art RL results on four popular
NP-hard problems: traveling salesman, capacitated vehicle routing, 0-1
knapsack, and job-shop scheduling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08253">HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1">Jingang Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Faney_T/0/1/0/all/0/1">Thibault Faney</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1">Patrick Gallinari</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousef_S/0/1/0/all/0/1">Soleiman Yousef</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemptinne_J/0/1/0/all/0/1">Jean-Charles de Hemptinne</a></p>
<p>Due to domain shifts, machine learning systems typically struggle to
generalize well to new domains that differ from those of training data, which
is what domain generalization (DG) aims to address. Although a variety of DG
methods have been proposed, most of them fall short in interpretability and
require domain labels, which are not available in many real-world scenarios.
This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture
of Experts (MoE), which does not rely on domain labels and is more
interpretable. MoE proves effective in identifying heterogeneous patterns in
data. For the DG problem, heterogeneity arises exactly from domain shifts. HMOE
employs hypernetworks taking vectors as input to generate the weights of
experts, which promotes knowledge sharing among experts and enables the
exploration of their similarities in a low-dimensional vector space. We
benchmark HMOE against other DG methods under a fair evaluation framework --
DomainBed. Our extensive experiments show that HMOE can effectively separate
mixed-domain data into distinct clusters that are surprisingly more consistent
with human intuition than original domain labels. Using self-learned domain
information, HMOE achieves state-of-the-art results on most datasets and
significantly surpasses other DG methods in average accuracy across all
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10627">EGRC-Net: Embedding-induced Graph Refinement Clustering Network. (arXiv:2211.10627v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhihao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yuheng Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a></p>
<p>Existing graph clustering networks heavily rely on a predefined yet fixed
graph, which can lead to failures when the initial graph fails to accurately
capture the data topology structure of the embedding space. In order to address
this issue, we propose a novel clustering network called Embedding-Induced
Graph Refinement Clustering Network (EGRC-Net), which effectively utilizes the
learned embedding to adaptively refine the initial graph and enhance the
clustering performance. To begin, we leverage both semantic and topological
information by employing a vanilla auto-encoder and a graph convolution
network, respectively, to learn a latent feature representation. Subsequently,
we utilize the local geometric structure within the feature embedding space to
construct an adjacency matrix for the graph. This adjacency matrix is
dynamically fused with the initial one using our proposed fusion architecture.
To train the network in an unsupervised manner, we minimize the Jeffreys
divergence between multiple derived distributions. Additionally, we introduce
an improved approximate personalized propagation of neural predictions to
replace the standard graph convolution network, enabling EGRC-Net to scale
effectively. Through extensive experiments conducted on nine widely-used
benchmark datasets, we demonstrate that our proposed methods consistently
outperform several state-of-the-art approaches. Notably, EGRC-Net achieves an
improvement of more than 11.99\% in Adjusted Rand Index (ARI) over the best
baseline on the DBLP dataset. Furthermore, our scalable approach exhibits a
10.73% gain in ARI while reducing memory usage by 33.73% and decreasing running
time by 19.71%. The code for EGRC-Net will be made publicly available at
\url{https://github.com/ZhihaoPENG-CityU/EGRC-Net}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00768">Simplifying and Understanding State Space Models with Diagonal Linear RNNs. (arXiv:2212.00768v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1">Harsh Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a></p>
<p>Sequence models based on linear state spaces (SSMs) have recently emerged as
a promising choice of architecture for modeling long range dependencies across
various modalities. However, they invariably rely on discretization of a
continuous state space, which complicates their presentation and understanding.
In this work, we dispose of the discretization step, and propose a model based
on vanilla Diagonal Linear RNNs ($\mathrm{DLR}$). We empirically show that,
despite being conceptually much simpler, $\mathrm{DLR}$ is as performant as
previously-proposed SSMs on a variety of tasks and benchmarks including Long
Range Arena and raw speech classification. Moreover, we characterize the
expressivity of SSMs (including $\mathrm{DLR}$) and attention-based models via
a suite of $13$ synthetic sequence-to-sequence tasks involving interactions
over tens of thousands of tokens, ranging from simple operations, such as
shifting an input sequence, to detecting co-dependent visual features over long
spatial ranges in flattened images. We find that while SSMs report near-perfect
performance on tasks that can be modeled via $\textit{few}$ convolutional
kernels, they struggle on tasks requiring $\textit{many}$ such kernels and
especially when the desired sequence manipulation is
$\textit{context-dependent}$. Despite these limitations, $\mathrm{DLR}$ reaches
high performance on two higher-order reasoning tasks $\mathrm{ListOpsSubTrees}$
and $\mathrm{PathfinderSegmentation}\text{-}\mathrm{256}$ with input lengths
$8K$ and $65K$ respectively, and gives encouraging performance on
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{512}$ with input length $262K$
for which attention is not a viable choice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02712">Improved Beam Search for Hallucination Mitigation in Abstractive Summarization. (arXiv:2212.02712v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1">Arvind Krishna Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Visser_E/0/1/0/all/0/1">Erik Visser</a></p>
<p>Advancement in large pretrained language models has significantly improved
their performance for conditional language generation tasks including
summarization albeit with hallucinations. To reduce hallucinations,
conventional methods proposed improving beam search or using a fact checker as
a postprocessing step. In this paper, we investigate the use of the Natural
Language Inference (NLI) entailment metric to detect and prevent hallucinations
in summary generation. We propose an NLI-assisted beam re-ranking mechanism by
computing entailment probability scores between the input context and
summarization model-generated beams during saliency-enhanced greedy decoding.
Moreover, a diversity metric is introduced to compare its effectiveness against
vanilla beam search. Our proposed algorithm significantly outperforms vanilla
beam decoding on XSum and CNN/DM datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04972">MOPRD: A multidisciplinary open peer review dataset. (arXiv:2212.04972v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jialiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhangping Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yidong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaodong Shi</a></p>
<p>Open peer review is a growing trend in academic publications. Public access
to peer review data can benefit both the academic and publishing communities.
It also serves as a great support to studies on review comment generation and
further to the realization of automated scholarly paper review. However, most
of the existing peer review datasets do not provide data that cover the whole
peer review process. Apart from this, their data are not diversified enough as
the data are mainly collected from the field of computer science. These two
drawbacks of the currently available peer review datasets need to be addressed
to unlock more opportunities for related studies. In response, we construct
MOPRD, a multidisciplinary open peer review dataset. This dataset consists of
paper metadata, multiple version manuscripts, review comments, meta-reviews,
author's rebuttal letters, and editorial decisions. Moreover, we propose a
modular guided review comment generation method based on MOPRD. Experiments
show that our method delivers better performance as indicated by both automatic
metrics and human evaluation. We also explore other potential applications of
MOPRD, including meta-review generation, editorial decision prediction, author
rebuttal generation, and scientometric analysis. MOPRD is a strong endorsement
for further studies in peer review-related research and other applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.05949">Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear Contextual Bandits and Markov Decision Processes. (arXiv:2212.05949v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ye_C/0/1/0/all/0/1">Chenlu Ye</a>, <a href="http://arxiv.org/find/stat/1/au:+Xiong_W/0/1/0/all/0/1">Wei Xiong</a>, <a href="http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>Despite the significant interest and progress in reinforcement learning (RL)
problems with adversarial corruption, current works are either confined to the
linear setting or lead to an undesired $\tilde{O}(\sqrt{T}\zeta)$ regret bound,
where $T$ is the number of rounds and $\zeta$ is the total amount of
corruption. In this paper, we consider the contextual bandit with general
function approximation and propose a computationally efficient algorithm to
achieve a regret of $\tilde{O}(\sqrt{T}+\zeta)$. The proposed algorithm relies
on the recently developed uncertainty-weighted least-squares regression from
linear contextual bandit and a new weighted estimator of uncertainty for the
general function class. In contrast to the existing analysis that heavily
relies on the linear structure, we develop a novel technique to control the sum
of weighted uncertainty, thus establishing the final regret bounds. We then
generalize our algorithm to the episodic MDP setting and first achieve an
additive dependence on the corruption level $\zeta$ in the scenario of general
function approximation. Notably, our algorithms achieve regret bounds either
nearly match the performance lower bound or improve the existing methods for
all the corruption levels and in both known and unknown $\zeta$ cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.12989">Improved Kernel Alignment Regret Bound for Online Kernel Learning. (arXiv:2212.12989v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junfan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1">Shizhong Liao</a></p>
<p>In this paper, we improve the kernel alignment regret bound for online kernel
learning in the regime of the Hinge loss function. Previous algorithm achieves
a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational
complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$,
where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an
algorithm whose regret bound and computational complexity are better than
previous results. Our results depend on the decay rate of eigenvalues of the
kernel matrix. If the eigenvalues of the kernel matrix decay exponentially,
then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a
computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a
regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of
$O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and
obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound
which improves the previous $O(1/\sqrt{T})$ bound.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.05525">Understanding Concept Identification as Consistent Data Clustering Across Multiple Feature Spaces. (arXiv:2301.05525v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lanfermann_F/0/1/0/all/0/1">Felix Lanfermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1">Sebastian Schmitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wollstadt_P/0/1/0/all/0/1">Patricia Wollstadt</a></p>
<p>Identifying meaningful concepts in large data sets can provide valuable
insights into engineering design problems. Concept identification aims at
identifying non-overlapping groups of design instances that are similar in a
joint space of all features, but which are also similar when considering only
subsets of features. These subsets usually comprise features that characterize
a design with respect to one specific context, for example, constructive design
parameters, performance values, or operation modes. It is desirable to evaluate
the quality of design concepts by considering several of these feature subsets
in isolation. In particular, meaningful concepts should not only identify
dense, well separated groups of data instances, but also provide
non-overlapping groups of data that persist when considering pre-defined
feature subsets separately. In this work, we propose to view concept
identification as a special form of clustering algorithm with a broad range of
potential applications beyond engineering design. To illustrate the differences
between concept identification and classical clustering algorithms, we apply a
recently proposed concept identification algorithm to two synthetic data sets
and show the differences in identified solutions. In addition, we introduce the
mutual information measure as a metric to evaluate whether solutions return
consistent clusters across relevant subsets. To support the novel understanding
of concept identification, we consider a simulated data set from a
decision-making problem in the energy management domain and show that the
identified clusters are more interpretable with respect to relevant feature
subsets than clusters found by common clustering algorithms and are thus more
suitable to support a decision maker.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.09702">Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v4 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1">Jiaqi Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Reibman_A/0/1/0/all/0/1">Amy R. Reibman</a>, <a href="http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1">Edward J. Delp</a></p>
<p>Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to
learn identity information from labeled images in source domains and apply it
to unlabeled images in a target domain. One major issue with many unsupervised
re-identification methods is that they do not perform well relative to large
domain variations such as illumination, viewpoint, and occlusions. In this
paper, we propose a Synthesis Model Bank (SMB) to deal with illumination
variation in unsupervised person re-ID. The proposed SMB consists of several
convolutional neural networks (CNN) for feature extraction and Mahalanobis
matrices for distance metrics. They are trained using synthetic data with
different illumination conditions such that their synergistic effect makes the
SMB robust against illumination variation. To better quantify the illumination
intensity and improve the quality of synthetic images, we introduce a new 3D
virtual-human dataset for GAN-based image synthesis. From our experiments, the
proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12309">On the Lipschitz Constant of Deep Networks and Double Descent. (arXiv:2301.12309v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gamba_M/0/1/0/all/0/1">Matteo Gamba</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1">Hossein Azizpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1">M&#xe5;rten Bj&#xf6;rkman</a></p>
<p>Existing bounds on the generalization error of deep networks assume some form
of smooth or bounded dependence on the input variable, falling short of
investigating the mechanisms controlling such factors in practice. In this
work, we present an extensive experimental study of the empirical Lipschitz
constant of deep networks undergoing double descent, and highlight
non-monotonic trends strongly correlating with the test error. Building a
connection between parameter-space and input-space gradients for SGD around a
critical point, we isolate two important factors -- namely loss landscape
curvature and distance of parameters from initialization -- respectively
controlling optimization dynamics around a critical point and bounding model
function complexity, even beyond the training data. Our study presents novels
insights on implicit regularization via overparameterization, and effective
model complexity for networks trained in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13737">Self-Consistent Velocity Matching of Probability Flows. (arXiv:2301.13737v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lingxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hurault_S/0/1/0/all/0/1">Samuel Hurault</a>, <a href="http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1">Justin Solomon</a></p>
<p>We present a discretization-free scalable framework for solving a large class
of mass-conserving partial differential equations (PDEs), including the
time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The
main observation is that the time-varying velocity field of the PDE solution
needs to be self-consistent: it must satisfy a fixed-point equation involving
the probability flow characterized by the same velocity field. Instead of
directly minimizing the residual of the fixed-point equation with neural
parameterization, we use an iterative formulation with a biased gradient
estimator that bypasses significant computational obstacles with strong
empirical performance. Compared to existing approaches, our method does not
suffer from temporal or spatial discretization, covers a wider range of PDEs,
and scales to high dimensions. Experimentally, our method recovers analytical
solutions accurately when they are available and achieves superior performance
in high dimensions with less training time compared to alternatives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03683">Linear Partial Monitoring for Sequential Decision-Making: Algorithms, Regret Bounds and Applications. (arXiv:2302.03683v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kirschner_J/0/1/0/all/0/1">Johannes Kirschner</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1">Tor Lattimore</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a></p>
<p>Partial monitoring is an expressive framework for sequential decision-making
with an abundance of applications, including graph-structured and dueling
bandits, dynamic pricing and transductive feedback models. We survey and extend
recent results on the linear formulation of partial monitoring that naturally
generalizes the standard linear bandit setting. The main result is that a
single algorithm, information-directed sampling (IDS), is (nearly) worst-case
rate optimal in all finite-action games. We present a simple and unified
analysis of stochastic partial monitoring, and further extend the model to the
contextual and kernelized setting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06015">A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity. (arXiv:2302.06015v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongkang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sijia Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-yu Chen</a></p>
<p>Vision Transformers (ViTs) with self-attention modules have recently achieved
great empirical success in many vision tasks. Due to non-convex interactions
across layers, however, theoretical learning and generalization analysis is
mostly elusive. Based on a data model characterizing both label-relevant and
label-irrelevant tokens, this paper provides the first theoretical analysis of
training a shallow ViT, i.e., one self-attention layer followed by a two-layer
perceptron, for a classification task. We characterize the sample complexity to
achieve a zero generalization error. Our sample complexity bound is positively
correlated with the inverse of the fraction of label-relevant tokens, the token
noise level, and the initial model error. We also prove that a training process
using stochastic gradient descent (SGD) leads to a sparse attention map, which
is a formal verification of the general intuition about the success of
attention. Moreover, this paper indicates that a proper token sparsification
can improve the test performance by removing label-irrelevant and/or noisy
tokens, including spurious correlations. Empirical experiments on synthetic
data and CIFAR-10 dataset justify our theoretical results and generalize to
deeper ViTs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08942">PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mbacke_S/0/1/0/all/0/1">Sokhna Diarra Mbacke</a>, <a href="http://arxiv.org/find/cs/1/au:+Clerc_F/0/1/0/all/0/1">Florence Clerc</a>, <a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1">Pascal Germain</a></p>
<p>We extend PAC-Bayesian theory to generative models and develop generalization
bounds for models based on the Wasserstein distance and the total variation
distance. Our first result on the Wasserstein distance assumes the instance
space is bounded, while our second result takes advantage of dimensionality
reduction. Our results naturally apply to Wasserstein GANs and Energy-Based
GANs, and our bounds provide new training objectives for these two. Although
our work is mainly theoretical, we perform numerical experiments showing
non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06536">AutoOptLib: Tailoring Metaheuristic Optimizers via Automated Algorithm Design. (arXiv:2303.06536v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1">Bai Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1">Taiwei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xianglong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1">Qiqi Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuhui Shi</a></p>
<p>Metaheuristics are prominent gradient-free optimizers for solving hard
problems that do not meet the rigorous mathematical assumptions of analytical
solvers. The canonical manual optimizer design could be laborious, untraceable
and error-prone, let alone human experts are not always available. This arises
increasing interest and demand in automating the optimizer design process. In
response, this paper proposes AutoOptLib, the first platform for accessible
automated design of metaheuristic optimizers. AutoOptLib leverages computing
resources to conceive, build up, and verify the design choices of the
optimizers. It requires much less labor resources and expertise than manual
design, democratizing satisfactory metaheuristic optimizers to a much broader
range of researchers and practitioners. Furthermore, by fully exploring the
design choices with computing resources, AutoOptLib has the potential to
surpass human experience, subsequently gaining enhanced performance compared
with human problem-solving. To realize the automated design, AutoOptLib
provides 1) a rich library of metaheuristic components for continuous,
discrete, and permutation problems; 2) a flexible algorithm representation for
evolving diverse algorithm structures; 3) different design objectives and
techniques for different optimization scenarios; and 4) a graphic user
interface for accessibility and practicability. AutoOptLib is fully written in
Matlab/Octave; its source code and documentation are available at
https://github.com/qz89/AutoOpt and https://AutoOpt.readthedocs.io/,
respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.10167">Generalized partitioned local depth. (arXiv:2303.10167v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Berenhaut_K/0/1/0/all/0/1">Kenneth S. Berenhaut</a>, <a href="http://arxiv.org/find/stat/1/au:+Foley_J/0/1/0/all/0/1">John D. Foley</a>, <a href="http://arxiv.org/find/stat/1/au:+Lyu_L/0/1/0/all/0/1">Liangdongsheng Lyu</a></p>
<p>In this paper we provide a generalization of the concept of cohesion as
introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National
Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the
technique of partitioned local depth by distilling two key probabilistic
concepts: local relevance and support division. Earlier results are extended
within the new context, and examples of applications to revealing communities
in data with uncertainty are included. The work sheds light on the foundations
of partitioned local depth, and extends the original ideas to enable
probabilistic consideration of uncertain, variable and potentially conflicting
information.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02192">A Diffusion-based Method for Multi-turn Compositional Image Generation. (arXiv:2304.02192v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a></p>
<p>Multi-turn compositional image generation (M-CIG) is a challenging task that
aims to iteratively manipulate a reference image given a modification text.
While most of the existing methods for M-CIG are based on generative
adversarial networks (GANs), recent advances in image generation have
demonstrated the superiority of diffusion models over GANs. In this paper, we
propose a diffusion-based method for M-CIG named conditional denoising
diffusion with image compositional matching (CDD-ICM). We leverage CLIP as the
backbone of image and text encoders, and incorporate a gated fusion mechanism,
originally proposed for question answering, to compositionally fuse the
reference image and the modification text at each turn of M-CIG. We introduce a
conditioning scheme to generate the target image based on the fusion results.
To prioritize the semantic quality of the generated target image, we learn an
auxiliary image compositional match (ICM) objective, along with the conditional
denoising diffusion (CDD) objective in a multi-task learning framework.
Additionally, we also perform ICM guidance and classifier-free guidance to
improve performance. Experimental results show that CDD-ICM achieves
state-of-the-art results on two benchmark datasets for M-CIG, i.e., CoDraw and
i-CLEVR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.06193">Learning Over Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems (Extended Version). (arXiv:2304.06193v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Barbara_N/0/1/0/all/0/1">Nicholas H. Barbara</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1">Ruigang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Manchester_I/0/1/0/all/0/1">Ian R. Manchester</a></p>
<p>This paper presents a policy parameterization for learning-based control on
nonlinear, partially-observed dynamical systems. The parameterization is based
on a nonlinear version of the Youla parameterization and the recently proposed
Recurrent Equilibrium Network (REN) class of models. We prove that the
resulting Youla-REN parameterization automatically satisfies stability
(contraction) and user-tunable robustness (Lipschitz) conditions on the
closed-loop system. This means it can be used for safe learning-based control
with no additional constraints or projections required to enforce stability or
robustness. We test the new policy class in simulation on two reinforcement
learning tasks: 1) magnetic suspension, and 2) inverting a rotary-arm pendulum.
We find that the Youla-REN performs similarly to existing learning-based and
optimal control methods while also ensuring stability and exhibiting improved
robustness to adversarial disturbances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13243">Chip-Chat: Challenges and Opportunities in Conversational Hardware Design. (arXiv:2305.13243v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blocklove_J/0/1/0/all/0/1">Jason Blocklove</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Siddharth Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Karri_R/0/1/0/all/0/1">Ramesh Karri</a>, <a href="http://arxiv.org/find/cs/1/au:+Pearce_H/0/1/0/all/0/1">Hammond Pearce</a></p>
<p>Modern hardware design starts with specifications provided in natural
language. These are then translated by hardware engineers into appropriate
Hardware Description Languages (HDLs) such as Verilog before synthesizing
circuit elements. Automating this translation could reduce sources of human
error from the engineering process. But, it is only recently that artificial
intelligence (AI) has demonstrated capabilities for machine-based end-to-end
design translations. Commercially-available instruction-tuned Large Language
Models (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able to
produce code in a variety of programming languages; but studies examining them
for hardware are still lacking. In this work, we thus explore the challenges
faced and opportunities presented when leveraging these recent advances in LLMs
for hardware design. Given that these `conversational' LLMs perform best when
used interactively, we perform a case study where a hardware engineer
co-architects a novel 8-bit accumulator-based microprocessor architecture with
the LLM according to real-world hardware constraints. We then sent the
processor to tapeout in a Skywater 130nm shuttle, meaning that this `Chip-Chat'
resulted in what we believe to be the world's first wholly-AI-written HDL for
tapeout.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17479">Inferring Causal Effects Under Heterogeneous Peer Influence. (arXiv:2305.17479v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adhikari_S/0/1/0/all/0/1">Shishir Adhikari</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheleva_E/0/1/0/all/0/1">Elena Zheleva</a></p>
<p>Causal inference in networks should account for interference, which occurs
when a unit's outcome is influenced by treatments or outcomes of peers.
Heterogeneous peer influence (HPI) occurs when a unit's outcome is influenced
differently by different peers based on their attributes and relationships, or
when each unit has a different susceptibility to peer influence. Existing
solutions to estimating direct causal effects under interference consider
either homogeneous influence from peers or specific heterogeneous influence
mechanisms (e.g., based on local neighborhood structure). This paper presents a
methodology for estimating individual direct causal effects in the presence of
HPI where the mechanism of influence is not known a priori. We propose a
structural causal model for networks that can capture different possible
assumptions about network structure, interference conditions, and causal
dependence and enables reasoning about identifiability in the presence of HPI.
We find potential heterogeneous contexts using the causal model and propose a
novel graph neural network-based estimator to estimate individual direct causal
effects. We show that state-of-the-art methods for individual direct effect
estimation produce biased results in the presence of HPI, and that our proposed
estimator is robust.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00041">Causal Intervention for Measuring Confidence in Drug-Target Interaction Prediction. (arXiv:2306.00041v2 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Ye_W/0/1/0/all/0/1">Wenting Ye</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1">Chen Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xie_Y/0/1/0/all/0/1">Yang Xie</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_W/0/1/0/all/0/1">Wen Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_H/0/1/0/all/0/1">Hong-Yu Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_B/0/1/0/all/0/1">Bowen Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Cheng_D/0/1/0/all/0/1">Debo Cheng</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Feng_Z/0/1/0/all/0/1">Zaiwen Feng</a></p>
<p>Identifying and discovering drug-target interactions(DTIs) are vital steps in
drug discovery and development. They play a crucial role in assisting
scientists in finding new drugs and accelerating the drug development process.
Recently, knowledge graph and knowledge graph embedding (KGE) models have made
rapid advancements and demonstrated impressive performance in drug discovery.
However, such models lack authenticity and accuracy in drug target
identification, leading to an increased misjudgment rate and reduced drug
development efficiency. To address these issues, we focus on the problem of
drug-target interactions, with knowledge mapping as the core technology.
Specifically, a causal intervention-based confidence measure is employed to
assess the triplet score to improve the accuracy of the drug-target interaction
prediction model. Experimental results demonstrate that the developed
confidence measurement method based on causal intervention can significantly
enhance the accuracy of DTI link prediction, particularly for high-precision
models. The predicted results are more valuable in guiding the design and
development of subsequent drug development experiments, thereby significantly
improving the efficiency of drug development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00183">Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nanda_V/0/1/0/all/0/1">Vedant Nanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Speicher_T/0/1/0/all/0/1">Till Speicher</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P. Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1">Krishna P. Gummadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1">Adrian Weller</a></p>
<p>Representations learned by pre-training a neural network on a large dataset
are increasingly used successfully to perform a variety of downstream tasks. In
this work, we take a closer look at how features are encoded in such
pre-trained representations. We find that learned representations in a given
layer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of
neurons in the layer that is larger than a threshold size shares a large degree
of similarity with the full layer and is able to perform similarly as the whole
layer on a variety of downstream tasks. For example, a linear probe trained on
$20\%$ of randomly picked neurons from the penultimate layer of a ResNet50
pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe
trained on the full layer of neurons for downstream CIFAR10 classification. We
conduct experiments on different neural architectures (including CNNs and
Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a
variety of downstream tasks taken from the VTAB benchmark. We find that the
loss and dataset used during pre-training largely govern the degree of diffuse
redundancy and the "critical mass" of neurons needed often depends on the
downstream task, suggesting that there is a task-inherent
redundancy-performance Pareto frontier. Our findings shed light on the nature
of representations learned by pre-trained deep neural networks and suggest that
entire layers might not be necessary to perform many downstream tasks. We
investigate the potential for exploiting this redundancy to achieve efficient
generalization for downstream tasks and also draw caution to certain possible
unintended consequences. Our code is available at
\url{https://github.com/nvedant07/diffused-redundancy}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05583">Gibbs-Based Information Criteria and the Over-Parameterized Regime. (arXiv:2306.05583v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haobo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1">Yuheng Bu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wornell_G/0/1/0/all/0/1">Gregory W. Wornell</a></p>
<p>Double-descent refers to the unexpected drop in test loss of a learning
algorithm beyond an interpolating threshold with over-parameterization, which
is not predicted by information criteria in their classical forms due to the
limitations in the standard asymptotic approach. We update these analyses using
the information risk minimization framework and provide Akaike Information
Criterion (AIC) and Bayesian Information Criterion (BIC) for models learned by
the Gibbs algorithm. Notably, the penalty terms for the Gibbs-based AIC and BIC
correspond to specific information measures, i.e., symmetrized KL information
and KL divergence. We extend this information-theoretic analysis to
over-parameterized models by providing two different Gibbs-based BICs to
compute the marginal likelihood of random feature models in the regime where
the number of parameters $p$ and the number of samples $n$ tend to infinity,
with $p/n$ fixed. Our experiments demonstrate that the Gibbs-based BIC can
select the high-dimensional model and reveal the mismatch between marginal
likelihood and population risk in the over-parameterized regime, providing new
insights to understand double-descent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06210">Single-Model Attribution of Generative Models Through Final-Layer Inversion. (arXiv:2306.06210v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1">Mike Laszkiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricker_J/0/1/0/all/0/1">Jonas Ricker</a>, <a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1">Johannes Lederer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1">Asja Fischer</a></p>
<p>Recent breakthroughs in generative modeling have sparked interest in
practical single-model attribution. Such methods predict whether a sample was
generated by a specific generator or not, for instance, to prove intellectual
property theft. However, previous works are either limited to the closed-world
setting or require undesirable changes to the generative model. We address
these shortcomings by, first, viewing single-model attribution through the lens
of anomaly detection. Arising from this change of perspective, we propose
FLIPAD, a new approach for single-model attribution in the open-world setting
based on final-layer inversion and anomaly detection. We show that the utilized
final-layer inversion can be reduced to a convex lasso optimization problem,
making our approach theoretically sound and computationally efficient. The
theoretical findings are accompanied by an experimental study demonstrating the
effectiveness of our approach and its flexibility to various domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08230">Unbiased Learning of Deep Generative Models with Structured Discrete Representations. (arXiv:2306.08230v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bendekgey_H/0/1/0/all/0/1">Harry Bendekgey</a>, <a href="http://arxiv.org/find/cs/1/au:+Hope_G/0/1/0/all/0/1">Gabriel Hope</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudderth_E/0/1/0/all/0/1">Erik B. Sudderth</a></p>
<p>By composing graphical models with deep learning architectures, we learn
generative models with the strengths of both frameworks. The structured
variational autoencoder (SVAE) inherits structure and interpretability from
graphical models, and flexible likelihoods for high-dimensional data from deep
learning, but poses substantial optimization challenges. We propose novel
algorithms for learning SVAEs, and are the first to demonstrate the SVAE's
ability to handle multimodal uncertainty when data is missing by incorporating
discrete latent variables. Our memory-efficient implicit differentiation scheme
makes the SVAE tractable to learn via gradient descent, while demonstrating
robustness to incomplete optimization. To more rapidly learn accurate graphical
model parameters, we derive a method for computing natural gradients without
manual derivations, which avoids biases found in prior work. These optimization
innovations enable the first comparisons of the SVAE to state-of-the-art time
series models, where the SVAE performs competitively while learning
interpretable and structured discrete data representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08698">Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v4 [physics.soc-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Braha_D/0/1/0/all/0/1">Dan Braha</a></p>
<p>Phase transitions, characterized by abrupt shifts between macroscopic
patterns of organization, are ubiquitous in complex systems. Despite
considerable research in the physical and natural sciences, the empirical study
of this phenomenon in societal systems is relatively underdeveloped. The goal
of this study is to explore whether the dynamics of collective civil unrest can
be plausibly characterized as a sequence of recurrent phase shifts, with each
phase having measurable and identifiable latent characteristics. Building on
previous efforts to characterize civil unrest as a self-organized critical
system, we introduce a macro-level statistical model of civil unrest and
evaluate its plausibility using a comprehensive dataset of civil unrest events
in 170 countries from 1946 to 2017. Our findings demonstrate that the
macro-level phase model effectively captures the characteristics of civil
unrest data from diverse countries globally and that universal mechanisms may
underlie certain aspects of the dynamics of civil unrest. We also introduce a
scale to quantify a country's long-term unrest per unit of time and show that
civil unrest events tend to cluster geographically, with the magnitude of civil
unrest concentrated in specific regions. Our approach has the potential to
identify and measure phase transitions in various collective human phenomena
beyond civil unrest, contributing to a better understanding of complex social
systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09299">Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization. (arXiv:2306.09299v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Swarnadeep Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1">Peter Hase</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>A hallmark property of explainable AI models is the ability to teach other
agents, communicating knowledge of how to perform a task. While Large Language
Models perform complex reasoning by generating explanations for their
predictions, it is unclear whether they also make good teachers for weaker
agents. To address this, we consider a student-teacher framework between two
LLM agents and study if, when, and how the teacher should intervene with
natural language explanations to improve the student's performance. Since
communication is expensive, we define a budget such that the teacher only
communicates explanations for a fraction of the data, after which the student
should perform well on its own. We decompose the teaching problem along four
axes: (1) if teacher's test time intervention improve student predictions, (2)
when it is worth explaining a data point, (3) how the teacher should
personalize explanations to better teach the student, and (4) if teacher
explanations also improve students on future unexplained data. We first show
that teacher LLMs can indeed intervene on student reasoning to improve their
performance. Next, inspired by the Theory of Mind abilities of effective
teachers, we propose building two few-shot mental models of the student. The
first model defines an Intervention Function that simulates the utility of an
intervention, allowing the teacher to intervene when this utility is the
highest and improving student performance at lower budgets. The second model
enables the teacher to personalize explanations for a particular student and
outperform unpersonalized teachers. We also demonstrate that in multi-turn
interactions, teacher explanations generalize and learning from explained data
improves student performance on future unexplained data. Finally, we verify
that misaligned teachers can lower student performance to random chance by
intentionally misleading them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11797">Towards a robust and reliable deep learning approach for detection of compact binary mergers in gravitational wave data. (arXiv:2306.11797v2 [gr-qc] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/gr-qc/1/au:+Jadhav_S/0/1/0/all/0/1">Shreejit Jadhav</a>, <a href="http://arxiv.org/find/gr-qc/1/au:+Shrivastava_M/0/1/0/all/0/1">Mihir Shrivastava</a>, <a href="http://arxiv.org/find/gr-qc/1/au:+Mitra_S/0/1/0/all/0/1">Sanjit Mitra</a></p>
<p>The ability of deep learning (DL) approaches to learn generalised signal and
noise models, coupled with their fast inference on GPUs, holds great promise
for enhancing gravitational-wave (GW) searches in terms of speed, parameter
space coverage, and search sensitivity. However, the opaque nature of DL models
severely harms their reliability. In this work, we meticulously develop a DL
model stage-wise and work towards improving its robustness and reliability.
First, we address the problems in maintaining the purity of training data by
deriving a new metric that better reflects the visual strength of the 'chirp'
signal features in the data. Using a reduced, smooth representation obtained
through a variational auto-encoder (VAE), we build a classifier to search for
compact binary coalescence (CBC) signals. Our tests on real LIGO data show an
impressive performance of the model. However, upon probing the robustness of
the model through adversarial attacks, its simple failure modes were
identified, underlining how such models can still be highly fragile. As a first
step towards bringing robustness, we retrain the model in a novel framework
involving a generative adversarial network (GAN). Over the course of training,
the model learns to eliminate the primary modes of failure identified by the
adversaries. Although absolute robustness is practically impossible to achieve,
we demonstrate some fundamental improvements earned through such training, like
sparseness and reduced degeneracy in the extracted features at different layers
inside the model. We show that these gains are achieved at practically zero
loss in terms of model performance on real LIGO data before and after GAN
training. Through a direct search on 8.8 days of LIGO data, we recover two
significant CBC events from GWTC-2.1, GW190519_153544 and GW190521_074359. We
also report the search sensitivity obtained from an injection study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13724">Review of compressed embedding layers and their applications for recommender systems. (arXiv:2306.13724v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hajgato_T/0/1/0/all/0/1">Tamas Hajgato</a></p>
<p>We review the literature on trainable, compressed embedding layers and
discuss their applicability for compressing gigantic neural recommender
systems. We also report the results we measured with our compressed embedding
layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14809">Tanimoto Random Features for Scalable Molecular Machine Learning. (arXiv:2306.14809v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tripp_A/0/1/0/all/0/1">Austin Tripp</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacallado_S/0/1/0/all/0/1">Sergio Bacallado</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sukriti Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a></p>
<p>The Tanimoto coefficient is commonly used to measure the similarity between
molecules represented as discrete fingerprints, either as a distance metric or
a positive definite kernel. While many kernel methods can be accelerated using
random feature approximations, at present there is a lack of such
approximations for the Tanimoto kernel. In this paper we propose two kinds of
novel random features to allow this kernel to scale to large datasets, and in
the process discover a novel extension of the kernel to real-valued vectors. We
theoretically characterize these random features, and provide error bounds on
the spectral norm of the Gram matrix. Experimentally, we show that these random
features are effective at approximating the Tanimoto coefficient of real-world
datasets and are useful for molecular property prediction and optimization
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.15794">HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution. (arXiv:2306.15794v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1">Eric Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1">Michael Poli</a>, <a href="http://arxiv.org/find/cs/1/au:+Faizi_M/0/1/0/all/0/1">Marjan Faizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1">Armin Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Birch_Sykes_C/0/1/0/all/0/1">Callum Birch-Sykes</a>, <a href="http://arxiv.org/find/cs/1/au:+Wornow_M/0/1/0/all/0/1">Michael Wornow</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Aman Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabideau_C/0/1/0/all/0/1">Clayton Rabideau</a>, <a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1">Stefano Massaroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>, <a href="http://arxiv.org/find/cs/1/au:+Baccus_S/0/1/0/all/0/1">Stephen A. Baccus</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1">Chris R&#xe9;</a></p>
<p>Genomic (DNA) sequences encode an enormous amount of information for gene
regulation and protein synthesis. Similar to natural language models,
researchers have proposed foundation models in genomics to learn generalizable
features from unlabeled genome data that can then be fine-tuned for downstream
tasks such as identifying regulatory elements. Due to the quadratic scaling of
attention, previous Transformer-based genomic models have used 512 to 4k tokens
as context (&lt;0.001% of the human genome), significantly limiting the modeling
of long-range interactions in DNA. In addition, these methods rely on
tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single
nucleotide resolution where subtle genetic variations can completely alter
protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a
large language model based on implicit convolutions was shown to match
attention in quality while allowing longer context lengths and lower time
complexity. Leveraging Hyena's new long-range capabilities, we present
HyenaDNA, a genomic foundation model pretrained on the human reference genome
with context lengths of up to 1 million tokens at the single nucleotide-level -
an up to 500x increase over previous dense attention-based models. HyenaDNA
scales sub-quadratically in sequence length (training up to 160x faster than
Transformer), uses single nucleotide tokens, and has full global context at
each layer. We explore what longer context enables - including the first use of
in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide
Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets
using a model with orders of magnitude less parameters and pretraining data. On
the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by
+10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00754">ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection. (arXiv:2307.00754v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Minghua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yudong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Ruomeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bowen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shilin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qingwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Anomaly detection in multivariate time series data is of paramount importance
for ensuring the efficient operation of large-scale systems across diverse
domains. However, accurately detecting anomalies in such data poses significant
challenges. Existing approaches, including forecasting and reconstruction-based
methods, struggle to address these challenges effectively. To overcome these
limitations, we propose a novel anomaly detection framework named ImDiffusion,
which combines time series imputation and diffusion models to achieve accurate
and robust anomaly detection. The imputation-based approach employed by
ImDiffusion leverages the information from neighboring values in the time
series, enabling precise modeling of temporal and inter-correlated
dependencies, reducing uncertainty in the data, thereby enhancing the
robustness of the anomaly detection process. ImDiffusion further leverages
diffusion models as time series imputers to accurately capturing complex
dependencies. We leverage the step-by-step denoised outputs generated during
the inference process to serve as valuable signals for anomaly prediction,
resulting in improved accuracy and robustness of the detection process.
</p>
<p>We evaluate the performance of ImDiffusion via extensive experiments on
benchmark datasets. The results demonstrate that our proposed framework
significantly outperforms state-of-the-art approaches in terms of detection
accuracy and timeliness. ImDiffusion is further integrated into the real
production system in Microsoft and observe a remarkable 11.4% increase in
detection F1 score compared to the legacy approach. To the best of our
knowledge, ImDiffusion represents a pioneering approach that combines
imputation-based techniques with time series anomaly detection, while
introducing the novel use of diffusion models to the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06440">No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1">Oscar Key</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1">Piotr Nawrot</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a></p>
<p>The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08684">A Rubik&#x27;s Cube inspired approach to Clifford synthesis. (arXiv:2307.08684v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Bao_N/0/1/0/all/0/1">Ning Bao</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Hartnett_G/0/1/0/all/0/1">Gavin S. Hartnett</a></p>
<p>The problem of decomposing an arbitrary Clifford element into a sequence of
Clifford gates is known as Clifford synthesis. Drawing inspiration from
similarities between this and the famous Rubik's Cube problem, we develop a
machine learning approach for Clifford synthesis based on learning an
approximation to the distance to the identity. This approach is probabilistic
and computationally intensive. However, when a decomposition is successfully
found, it often involves fewer gates than existing synthesis algorithms.
Additionally, our approach is much more flexible than existing algorithms in
that arbitrary gate sets, device topologies, and gate fidelities may
incorporated, thus allowing for the approach to be tailored to a specific
device.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10485">FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao-Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongyang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a></p>
<p>Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating human-like texts, which may potentially
revolutionize the finance industry. However, existing LLMs often fall short in
the financial field, which is mainly attributed to the disparities between
general text data and financial text data. Unfortunately, there is only a
limited number of financial text datasets available, and BloombergGPT, the
first financial LLM (FinLLM), is close-sourced (only the training logs were
released). In light of this, we aim to democratize Internet-scale financial
data for LLMs, which is an open challenge due to diverse data sources, low
signal-to-noise ratio, and high time-validity. To address the challenges, we
introduce an open-sourced and data-centric framework, Financial Generative
Pre-trained Transformer (FinGPT), that automates the collection and curation of
real-time financial data from 34 diverse sources on the Internet, providing
researchers and practitioners with accessible and transparent resources to
develop their FinLLMs. Additionally, we propose a simple yet effective strategy
for fine-tuning FinLLM using the inherent feedback from the market, dubbed
Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank
Adaptation (LoRA, QLoRA) method that enables users to customize their own
FinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several
FinGPT applications, including robo-advisor, sentiment analysis for algorithmic
trading, and low-code development. FinGPT aims to democratize FinLLMs,
stimulate innovation, and unlock new opportunities in open finance. The codes
have been open-sourced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.15807">Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping. (arXiv:2307.15807v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chevtchenko_S/0/1/0/all/0/1">S&#xe9;rgio F. Chevtchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocha_E/0/1/0/all/0/1">Elisson da Silva Rocha</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1">Monalisa Cristina Moura Dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Mota_R/0/1/0/all/0/1">Ricardo Lins Mota</a>, <a href="http://arxiv.org/find/cs/1/au:+Vieira_D/0/1/0/all/0/1">Diego Moura Vieira</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrade_E/0/1/0/all/0/1">Ermeson Carneiro de Andrade</a>, <a href="http://arxiv.org/find/cs/1/au:+Araujo_D/0/1/0/all/0/1">Danilo Ricardo Barbosa de Ara&#xfa;jo</a></p>
<p>Anomaly detection is critical in the smart industry for preventing equipment
failure, reducing downtime, and improving safety. Internet of Things (IoT) has
enabled the collection of large volumes of data from industrial machinery,
providing a rich source of information for Anomaly Detection. However, the
volume and complexity of data generated by the Internet of Things ecosystems
make it difficult for humans to detect anomalies manually. Machine learning
(ML) algorithms can automate anomaly detection in industrial machinery by
analyzing generated data. Besides, each technique has specific strengths and
weaknesses based on the data nature and its corresponding systems. However, the
current systematic mapping studies on Anomaly Detection primarily focus on
addressing network and cybersecurity-related problems, with limited attention
given to the industrial sector. Additionally, these studies do not cover the
challenges involved in using ML for Anomaly Detection in industrial machinery
within the context of the IoT ecosystems. This paper presents a systematic
mapping study on Anomaly Detection for industrial machinery using IoT devices
and ML algorithms to address this gap. The study comprehensively evaluates 84
relevant studies spanning from 2016 to 2023, providing an extensive review of
Anomaly Detection research. Our findings identify the most commonly used
algorithms, preprocessing techniques, and sensor types. Additionally, this
review identifies application areas and points to future challenges and
research opportunities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07336">Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1">Terufumi Morishita</a>, <a href="http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1">Gaku Morio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1">Atsuki Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1">Yasuhiro Sogawa</a></p>
<p>We study a synthetic corpus based approach for language models (LMs) to
acquire logical deductive reasoning ability. The previous studies generated
deduction examples using specific sets of deduction rules. However, these rules
were limited or otherwise arbitrary, limiting the generalizability of acquired
reasoning ability. We rethink this and adopt a well-grounded set of deduction
rules based on formal logic theory, which can derive any other deduction rules
when combined in a multistep way. Then, using the proposed corpora, which we
name FLD (Formal Logic Deduction), we first evaluate and analyze the logical
reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the
problems, suggesting that pure logical reasoning isolated from knowledge is
still challenging for the LLMs, and additional training specialized in logical
reasoning is indeed essential. We next empirically verify that LMs trained on
FLD corpora acquire more generalizable reasoning ability. Furthermore, we
identify the aspects of reasoning ability on which deduction corpora can
enhance LMs and those on which they cannot, and discuss future directions on
each aspect. The released corpora serve both as learning resources and as
challenging benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09345">Denoising diffusion-based MRI to CT image translation enables automated spinal segmentation. (arXiv:2308.09345v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Graf_R/0/1/0/all/0/1">Robert Graf</a>, <a href="http://arxiv.org/find/eess/1/au:+Schmitt_J/0/1/0/all/0/1">Joachim Schmitt</a>, <a href="http://arxiv.org/find/eess/1/au:+Schlaeger_S/0/1/0/all/0/1">Sarah Schlaeger</a>, <a href="http://arxiv.org/find/eess/1/au:+Moller_H/0/1/0/all/0/1">Hendrik Kristian M&#xf6;ller</a>, <a href="http://arxiv.org/find/eess/1/au:+Sideri_Lampretsa_V/0/1/0/all/0/1">Vasiliki Sideri-Lampretsa</a>, <a href="http://arxiv.org/find/eess/1/au:+Sekuboyina_A/0/1/0/all/0/1">Anjany Sekuboyina</a>, <a href="http://arxiv.org/find/eess/1/au:+Krieg_S/0/1/0/all/0/1">Sandro Manuel Krieg</a>, <a href="http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1">Benedikt Wiestler</a>, <a href="http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1">Jan Stefan Kirschke</a></p>
<p>Background: Automated segmentation of spinal MR images plays a vital role
both scientifically and clinically. However, accurately delineating posterior
spine structures presents challenges.
</p>
<p>Methods: This retrospective study, approved by the ethical committee,
involved translating T1w and T2w MR image series into CT images in a total of
n=263 pairs of CT/MR series. Landmark-based registration was performed to align
image pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit
models (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired
translation, SynDiff) image-to-image translation using "peak signal to noise
ratio" (PSNR) as quality measure. A publicly available segmentation network
segmented the synthesized CT datasets, and Dice scores were evaluated on
in-house test sets and the "MRSpineSeg Challenge" volumes. The 2D findings were
extended to 3D Pix2Pix and DDIM.
</p>
<p>Results: 2D paired methods and SynDiff exhibited similar translation
performance and Dice scores on paired data. DDIM image mode achieved the
highest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated
similar Dice scores (0.77). For craniocaudal axis rotations, at least two
landmarks per vertebra were required for registration. The 3D translation
outperformed the 2D approach, resulting in improved Dice scores (0.80) and
anatomically accurate segmentations in a higher resolution than the original MR
image.
</p>
<p>Conclusion: Two landmarks per vertebra registration enabled paired
image-to-image translation from MR to CT and outperformed all unpaired
approaches. The 3D techniques provided anatomically correct segmentations,
avoiding underprediction of small structures like the spinous process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10874">Analyzing Transformer Dynamics as Movement through Embedding Space. (arXiv:2308.10874v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sumeet S. Singh</a></p>
<p>Transformer based language models exhibit intelligent behaviors such as
understanding natural language, recognizing patterns, acquiring knowledge,
reasoning, planning, reflecting and using tools. This paper explores how their
underlying mechanics give rise to intelligent behaviors. Towards that end, we
propose framing Transformer dynamics as movement through embedding space.
Examining Transformers through this perspective reveals key insights,
establishing a Theory of Transformers: 1) Intelligent behaviours map to paths
in Embedding Space which, the Transformer random-walks through during
inferencing. 2) LM training learns a probability distribution over all possible
paths. `Intelligence' is learnt by assigning higher probabilities to paths
representing intelligent behaviors. No learning can take place in-context;
context only narrows the subset of paths sampled during decoding. 5) The
Transformer is a self-mapping composition function, folding a context sequence
into a context-vector such that it's proximity to a token-vector reflects its
co-occurrence and conditioned probability. Thus, the physical arrangement of
vectors in Embedding Space determines path probabilities. 6) Context vectors
are composed by aggregating features of the sequence's tokens via a process we
call the encoding walk. Attention contributes a - potentially redundant -
association-bias to this process. 7) This process is comprised of two principal
operation types: filtering (data independent) and aggregation (data dependent).
This generalization unifies Transformers with other sequence models. Building
upon this foundation, we formalize a popular semantic interpretation of
embeddings into a ``concept-space theory'' and find some evidence of it's
validity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13816">Homological Convolutional Neural Networks. (arXiv:2308.13816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Briola_A/0/1/0/all/0/1">Antonio Briola</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuanrong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartolucci_S/0/1/0/all/0/1">Silvia Bartolucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Aste_T/0/1/0/all/0/1">Tomaso Aste</a></p>
<p>Deep learning methods have demonstrated outstanding performances on
classification and regression tasks on homogeneous data types (e.g., image,
audio, and text data). However, tabular data still pose a challenge, with
classic machine learning approaches being often computationally cheaper and
equally effective than increasingly complex deep learning architectures. The
challenge arises from the fact that, in tabular data, the correlation among
features is weaker than the one from spatial or semantic relationships in
images or natural language, and the dependency structures need to be modeled
without any prior information. In this work, we propose a novel deep learning
architecture that exploits the data structural organization through
topologically constrained network representations to gain relational
information from sparse tabular inputs. The resulting model leverages the power
of convolution and is centered on a limited number of concepts from network
topology to guarantee: (i) a data-centric and deterministic building pipeline;
(ii) a high level of interpretability over the inference process; and (iii) an
adequate room for scalability. We test our model on 18 benchmark datasets
against 5 classic machine learning and 3 deep learning models, demonstrating
that our approach reaches state-of-the-art performances on these challenging
datasets. The code to reproduce all our experiments is provided at
https://github.com/FinancialComputingUCL/HomologicalCNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00305">Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties. (arXiv:2309.00305v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1">Binh Duong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Potapenko_P/0/1/0/all/0/1">Pavlo Potapenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Dermici_A/0/1/0/all/0/1">Aytekin Dermici</a>, <a href="http://arxiv.org/find/cs/1/au:+Govind_K/0/1/0/all/0/1">Kishan Govind</a>, <a href="http://arxiv.org/find/cs/1/au:+Bompas_S/0/1/0/all/0/1">S&#xe9;bastien Bompas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandfeld_S/0/1/0/all/0/1">Stefan Sandfeld</a></p>
<p>Determining, understanding, and predicting the so-called structure-property
relation is an important task in many scientific disciplines, such as
chemistry, biology, meteorology, physics, engineering, and materials science.
Structure refers to the spatial distribution of, e.g., substances, material, or
matter in general, while property is a resulting characteristic that usually
depends in a non-trivial way on spatial details of the structure.
Traditionally, forward simulations models have been used for such tasks.
Recently, several machine learning algorithms have been applied in these
scientific fields to enhance and accelerate simulation models or as surrogate
models. In this work, we develop and investigate the applications of six
machine learning techniques based on two different datasets from the domain of
materials science: data from a two-dimensional Ising model for predicting the
formation of magnetic domains and data representing the evolution of dual-phase
microstructures from the Cahn-Hilliard model. We analyze the accuracy and
robustness of all models and elucidate the reasons for the differences in their
performances. The impact of including domain knowledge through tailored
features is studied, and general recommendations based on the availability and
quality of training data are derived from this.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08652">Quantifying Credit Portfolio sensitivity to asset correlations with interpretable generative neural networks. (arXiv:2309.08652v2 [q-fin.RM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Caprioli_S/0/1/0/all/0/1">Sergio Caprioli</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Cagliero_E/0/1/0/all/0/1">Emanuele Cagliero</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Crupi_R/0/1/0/all/0/1">Riccardo Crupi</a></p>
<p>In this research, we propose a novel approach for the quantification of
credit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the
use of synthetic financial correlation matrices generated with deep learning
models. In previous work Generative Adversarial Networks (GANs) were employed
to demonstrate the generation of plausible correlation matrices, that capture
the essential characteristics observed in empirical correlation matrices
estimated on asset returns. Instead of GANs, we employ Variational Autoencoders
(VAE) to achieve a more interpretable latent space representation. Through our
analysis, we reveal that the VAE latent space can be a useful tool to capture
the crucial factors impacting portfolio diversification, particularly in
relation to credit portfolio sensitivity to asset correlations changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13475">Detecting and Mitigating System-Level Anomalies of Vision-Based Controllers. (arXiv:2309.13475v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aryaman Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_K/0/1/0/all/0/1">Kaustav Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1">Somil Bansal</a></p>
<p>Autonomous systems, such as self-driving cars and drones, have made
significant strides in recent years by leveraging visual inputs and machine
learning for decision-making and control. Despite their impressive performance,
these vision-based controllers can make erroneous predictions when faced with
novel or out-of-distribution inputs. Such errors can cascade to catastrophic
system failures and compromise system safety. In this work, we introduce a
run-time anomaly monitor to detect and mitigate such closed-loop, system-level
failures. Specifically, we leverage a reachability-based framework to
stress-test the vision-based controller offline and mine its system-level
failures. This data is then used to train a classifier that is leveraged online
to flag inputs that might cause system breakdowns. The anomaly detector
highlights issues that transcend individual modules and pertain to the safety
of the overall system. We also design a fallback controller that robustly
handles these detected anomalies to preserve system safety. We validate the
proposed approach on an autonomous aircraft taxiing system that uses a
vision-based controller for taxiing. Our results show the efficacy of the
proposed approach in identifying and handling system-level anomalies,
outperforming methods such as prediction error-based detection, and ensembling,
thereby enhancing the overall safety and robustness of autonomous systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14610">Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1">Kai Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mostafavi_A/0/1/0/all/0/1">Ali Mostafavi</a></p>
<p>Urban flood risk emerges from complex and nonlinear interactions among
multiple features related to flood hazard, flood exposure, and social and
physical vulnerabilities, along with the complex spatial flood dependence
relationships. Existing approaches for characterizing urban flood risk,
however, are primarily based on flood plain maps, focusing on a limited number
of features, primarily hazard and exposure features, without consideration of
feature interactions or the dependence relationships among spatial areas. To
address this gap, this study presents an integrated urban flood-risk rating
model based on a novel unsupervised graph deep learning model (called
FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among
areas and complex and nonlinear interactions among flood hazards and urban
features for specifying emergent flood risk. Using data from multiple
metropolitan statistical areas (MSAs) in the United States, the model
characterizes their flood risk into six distinct city-specific levels. The
model is interpretable and enables feature analysis of areas within each
flood-risk level, allowing for the identification of the three archetypes
shaping the highest flood risk within each MSA. Flood risk is found to be
spatially distributed in a hierarchical structure within each MSA, where the
core city disproportionately bears the highest flood risk. Multiple cities are
found to have high overall flood-risk levels and low spatial inequality,
indicating limited options for balancing urban development and flood-risk
reduction. Relevant flood-risk reduction strategies are discussed considering
ways that the highest flood risk and uneven spatial distribution of flood risk
are formed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00566">Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models. (arXiv:2310.00566v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1">Duanyu Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yongfu Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jimin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qianqian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Weiguang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Lira_A/0/1/0/all/0/1">Alejandro Lopez-Lira</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>In the financial industry, credit scoring is a fundamental element, shaping
access to credit and determining the terms of loans for individuals and
businesses alike. Traditional credit scoring methods, however, often grapple
with challenges such as narrow knowledge scope and isolated evaluation of
credit tasks. Our work posits that Large Language Models (LLMs) have great
potential for credit scoring tasks, with strong generalization ability across
multiple tasks. To systematically explore LLMs for credit scoring, we propose
the first open-source comprehensive framework. We curate a novel benchmark
covering 9 datasets with 14K samples, tailored for credit assessment and a
critical examination of potential biases within LLMs, and the novel instruction
tuning data with over 45k samples. We then propose the first Credit and Risk
Assessment Large Language Model (CALM) by instruction tuning, tailored to the
nuanced demands of various financial risk assessment tasks. We evaluate CALM,
and existing state-of-art (SOTA) open source and close source LLMs on the build
benchmark. Our empirical results illuminate the capability of LLMs to not only
match but surpass conventional models, pointing towards a future where credit
scoring can be more inclusive, comprehensive, and unbiased. We contribute to
the industry's transformation by sharing our pioneering instruction-tuning
datasets, credit and risk assessment LLM, and benchmarks with the research
community and the financial industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05166">A Corrected Expected Improvement Acquisition Function Under Noisy Observations. (arXiv:2310.05166v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Han Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xingchen Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1">Matthew B Blaschko</a></p>
<p>Sequential maximization of expected improvement (EI) is one of the most
widely used policies in Bayesian optimization because of its simplicity and
ability to handle noisy observations. In particular, the improvement function
often uses the best posterior mean as the best incumbent in noisy settings.
However, the uncertainty associated with the incumbent solution is often
neglected in many analytic EI-type methods: a closed-form acquisition function
is derived in the noise-free setting, but then applied to the setting with
noisy observations. To address this limitation, we propose a modification of EI
that corrects its closed-form expression by incorporating the covariance
information provided by the Gaussian Process (GP) model. This acquisition
function specializes to the classical noise-free result, and we argue should
replace that formula in Bayesian optimization software packages, tutorials, and
textbooks. This enhanced acquisition provides good generality for noisy and
noiseless settings. We show that our method achieves a sublinear convergence
rate on the cumulative regret bound under heteroscedastic observation noise.
Our empirical results demonstrate that our proposed acquisition function can
outperform EI in the presence of noisy observations on benchmark functions for
black-box optimization, as well as on parameter search for neural network model
compression.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07793">GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Ruotong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xu Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunpu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a></p>
<p>The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional carefully
designed embedding-based and rule-based models dominate. The question remains
open of whether pre-trained LLMs can understand structured temporal relational
data and replace them as the foundation model for temporal relational
forecasting. Therefore, we bring temporal knowledge forecasting into the
generative setting. However, challenges occur in the huge chasms between
complex temporal graph data structure and sequential natural expressions LLMs
can handle, and between the enormous data sizes of tKGs and heavy computation
costs of finetuning LLMs. To address these challenges, we propose a novel
retrieval augmented generation framework that performs generative forecasting
on tKGs named GenTKG, which combines a temporal logical rule-based retrieval
strategy and lightweight parameter-efficient instruction tuning. Extensive
experiments have shown that GenTKG outperforms conventional methods of temporal
relational forecasting under low computation resources. GenTKG also highlights
remarkable transferability with exceeding performance on unseen datasets
without re-training. Our work reveals the huge potential of LLMs in the tKG
domain and opens a new frontier for generative forecasting on tKGs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07838">Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingyue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a></p>
<p>We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08100">Generative Intrinsic Optimization: Intrinsic Control with Model Learning. (arXiv:2310.08100v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianfei Ma</a></p>
<p>Future sequence represents the outcome after executing the action into the
environment (i.e. the trajectory onwards). When driven by the
information-theoretic concept of mutual information, it seeks maximally
informative consequences. Explicit outcomes may vary across state, return, or
trajectory serving different purposes such as credit assignment or imitation
learning. However, the inherent nature of incorporating intrinsic motivation
with reward maximization is often neglected. In this work, we propose a policy
iteration scheme that seamlessly incorporates the mutual information, ensuring
convergence to the optimal policy. Concurrently, a variational approach is
introduced, which jointly learns the necessary quantity for estimating the
mutual information and the dynamics model, providing a general framework for
incorporating different forms of outcomes of interest. While we mainly focus on
theoretical analysis, our approach opens the possibilities of leveraging
intrinsic control with model learning to enhance sample efficiency and
incorporate uncertainty of the environment into decision-making.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.12570">DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sun_G/0/1/0/all/0/1">Guanqun Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1">Yizhi Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1">Weikun Kong</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1">Zichang Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1">Jianhua Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Racharak_T/0/1/0/all/0/1">Teeradaj Racharak</a>, <a href="http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1">Le-Minh Nguyen</a>, <a href="http://arxiv.org/find/eess/1/au:+Xin_J/0/1/0/all/0/1">Junyi Xin</a></p>
<p>Accurate medical image segmentation is critical for disease quantification
and treatment evaluation. While traditional Unet architectures and their
transformer-integrated variants excel in automated segmentation tasks. However,
they lack the ability to harness the intrinsic position and channel features of
image. Existing models also struggle with parameter efficiency and
computational complexity, often due to the extensive use of Transformers. To
address these issues, this study proposes a novel deep medical image
segmentation framework, called DA-TransUNet, aiming to integrate the
Transformer and dual attention block(DA-Block) into the traditional U-shaped
architecture. Unlike earlier transformer-based U-net models, DA-TransUNet
utilizes Transformers and DA-Block to integrate not only global and local
features, but also image-specific positional and channel features, improving
the performance of medical image segmentation. By incorporating a DA-Block at
the embedding layer and within each skip connection layer, we substantially
enhance feature extraction capabilities and improve the efficiency of the
encoder-decoder structure. DA-TransUNet demonstrates superior performance in
medical image segmentation tasks, consistently outperforming state-of-the-art
techniques across multiple datasets. In summary, DA-TransUNet offers a
significant advancement in medical image segmentation, providing an effective
and powerful alternative to existing techniques. Our architecture stands out
for its ability to improve segmentation accuracy, thereby advancing the field
of automated medical image diagnostics. The codes and parameters of our model
will be publicly available at https://github.com/SUN-1024/DA-TransUnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13810">A Better Match for Drivers and Riders: Reinforcement Learning at Lyft. (arXiv:2310.13810v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azagirre_X/0/1/0/all/0/1">Xabi Azagirre</a>, <a href="http://arxiv.org/find/cs/1/au:+Balwally_A/0/1/0/all/0/1">Akshay Balwally</a>, <a href="http://arxiv.org/find/cs/1/au:+Candeli_G/0/1/0/all/0/1">Guillaume Candeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chamandy_N/0/1/0/all/0/1">Nicholas Chamandy</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Benjamin Han</a>, <a href="http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1">Alona King</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungjun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Loncaric_M/0/1/0/all/0/1">Martin Loncaric</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_S/0/1/0/all/0/1">Sebastien Martin</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasiman_V/0/1/0/all/0/1">Vijay Narasiman</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhiwei/0/1/0/all/0/1">Zhiwei</a> (Tony)Qin, <a href="http://arxiv.org/find/cs/1/au:+Richard_B/0/1/0/all/0/1">Baptiste Richard</a>, <a href="http://arxiv.org/find/cs/1/au:+Smoot_S/0/1/0/all/0/1">Sara Smoot</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_S/0/1/0/all/0/1">Sean Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryzin_G/0/1/0/all/0/1">Garrett van Ryzin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zamoshchin_A/0/1/0/all/0/1">Alex Zamoshchin</a></p>
<p>To better match drivers to riders in our ridesharing application, we revised
Lyft's core matching algorithm. We use a novel online reinforcement learning
approach that estimates the future earnings of drivers in real time and use
this information to find more efficient matches. This change was the first
documented implementation of a ridesharing matching algorithm that can learn
and improve in real time. We evaluated the new approach during weeks of
switchback experimentation in most Lyft markets, and estimated how it benefited
drivers, riders, and the platform. In particular, it enabled our drivers to
serve millions of additional riders each year, leading to more than $30 million
per year in incremental revenue. Lyft rolled out the algorithm globally in
2021.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14421">On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Horenko_I/0/1/0/all/0/1">Illia Horenko</a></p>
<p>Simply-verifiable mathematical conditions for existence, uniqueness and
explicit analytical computation of minimal adversarial paths (MAP) and minimal
adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for
generalized linear models (GLM), and for entropic AI (EAI) are formulated and
proven. Practical computation of MAP and MAD, their comparison and
interpretations for various classes of AI tools (for neuronal networks, boosted
random forests, GLM and EAI) are demonstrated on the common synthetic
benchmarks: on a double Swiss roll spiral and its extensions, as well as on the
two biomedical data problems (for the health insurance claim predictions, and
for the heart attack lethality classification). On biomedical applications it
is demonstrated how MAP provides unique minimal patient-specific
risk-mitigating interventions in the predefined subsets of accessible control
variables.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14550">Corruption-Robust Offline Reinforcement Learning with General Function Approximation. (arXiv:2310.14550v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1">Chenlu Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Rui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a></p>
<p>We investigate the problem of corruption robustness in offline reinforcement
learning (RL) with general function approximation, where an adversary can
corrupt each sample in the offline dataset, and the corruption level
$\zeta\geq0$ quantifies the cumulative corruption amount over $n$ episodes and
$H$ steps. Our goal is to find a policy that is robust to such corruption and
minimizes the suboptimality gap with respect to the optimal policy for the
uncorrupted Markov decision processes (MDPs). Drawing inspiration from the
uncertainty-weighting technique from the robust online RL setting
\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight
iteration procedure to efficiently compute on batched samples and propose a
corruption-robust algorithm for offline RL. Notably, under the assumption of
single policy coverage and the knowledge of $\zeta$, our proposed algorithm
achieves a suboptimality bound that is worsened by an additive factor of
$\mathcal O(\zeta \cdot (\text{CC}(\lambda,\hat{\mathcal F},\mathcal
Z_n^H))^{1/2} (C(\hat{\mathcal F},\mu))^{-1/2} n^{-1})$ due to the corruption.
Here $\text{CC}(\lambda,\hat{\mathcal F},\mathcal Z_n^H)$ is the coverage
coefficient that depends on the regularization parameter $\lambda$, the
confidence set $\hat{\mathcal F}$, and the dataset $\mathcal Z_n^H$, and
$C(\hat{\mathcal F},\mu)$ is a coefficient that depends on $\hat{\mathcal F}$
and the underlying data distribution $\mu$. When specialized to linear MDPs,
the corruption-dependent error term reduces to $\mathcal O(\zeta d n^{-1})$
with $d$ being the dimension of the feature map, which matches the existing
lower bound for corrupted linear MDPs. This suggests that our analysis is tight
in terms of the corruption-dependent term.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15742">Improving Diffusion Models for ECG Imputation with an Augmented Template Prior. (arXiv:2310.15742v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jenkins_A/0/1/0/all/0/1">Alexander Jenkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zehua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_F/0/1/0/all/0/1">Fu Siong Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandic_D/0/1/0/all/0/1">Danilo Mandic</a></p>
<p>Pulsative signals such as the electrocardiogram (ECG) are extensively
collected as part of routine clinical care. However, noisy and poor-quality
recordings are a major issue for signals collected using mobile health systems,
decreasing the signal quality, leading to missing values, and affecting
automated downstream tasks. Recent studies have explored the imputation of
missing values in ECG with probabilistic time-series models. Nevertheless, in
comparison with the deterministic models, their performance is still limited,
as the variations across subjects and heart-beat relationships are not
explicitly considered in the training objective. In this work, to improve the
imputation and forecasting accuracy for ECG with probabilistic models, we
present a template-guided denoising diffusion probabilistic model (DDPM),
PulseDiff, which is conditioned on an informative prior for a range of health
conditions. Specifically, 1) we first extract a subject-level pulsative
template from the observed values to use as an informative prior of the missing
values, which personalises the prior; 2) we then add beat-level stochastic
shift terms to augment the prior, which considers variations in the position
and amplitude of the prior at each beat; 3) we finally design a confidence
score to consider the health condition of the subject, which ensures our prior
is provided safely. Experiments with the PTBXL dataset reveal that PulseDiff
improves the performance of two strong DDPM baseline models, CSDI and
SSSD$^{S4}$, verifying that our method guides the generation of DDPMs while
managing the uncertainty. When combined with SSSD$^{S4}$, PulseDiff outperforms
the leading deterministic model for short-interval missing data and is
comparable for long-interval data loss.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16600">Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Salahub_C/0/1/0/all/0/1">Chris Salahub</a>, <a href="http://arxiv.org/find/stat/1/au:+Oldford_W/0/1/0/all/0/1">Wayne Oldford</a></p>
<p>A common approach to evaluating the significance of a collection of
$p$-values combines them with a pooling function, in particular when the
original data are not available. These pooled $p$-values convert a sample of
$p$-values into a single number which behaves like a univariate $p$-value. To
clarify discussion of these functions, a telescoping series of alternative
hypotheses are introduced that communicate the strength and prevalence of
non-null evidence in the $p$-values before general pooling formulae are
discussed. A pattern noticed in the UMP pooled $p$-value for a particular
alternative motivates the definition and discussion of central and marginal
rejection levels at $\alpha$. It is proven that central rejection is always
greater than or equal to marginal rejection, motivating a quotient to measure
the balance between the two for pooled $p$-values. A combining function based
on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this
quotient and shown to be robust to mis-specified parameters relative to the
UMP. Different powers for different parameter settings motivate a map of
plausible alternatives based on where this pooled $p$-value is minimized.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17784">Data-Centric Financial Large Language Models. (arXiv:2310.17784v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Huaiyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xinyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yijia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wanqing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qing Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Longfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a></p>
<p>Large language models (LLMs) show promise for natural language tasks but
struggle when applied directly to complex domains like finance. LLMs have
difficulty reasoning about and integrating all relevant information. We propose
a data-centric approach to enable LLMs to better handle financial tasks. Our
key insight is that rather than overloading the LLM with everything at once, it
is more effective to preprocess and pre-understand the data. We create a
financial LLM (FLLM) using multitask prompt-based finetuning to achieve data
pre-processing and pre-understanding. However, labeled data is scarce for each
task. To overcome manual annotation costs, we employ abductive augmentation
reasoning (AAR) to automatically generate training data by modifying the pseudo
labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR
substantially outperforms baseline financial LLMs designed for raw text,
achieving state-of-the-art on financial analysis and interpretation tasks. We
also open source a new benchmark for financial analysis and interpretation. Our
methodology provides a promising path to unlock LLMs' potential for complex
real-world domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18784">High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Armacki_A/0/1/0/all/0/1">Aleksandar Armacki</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1">Pranay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1">Gauri Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajovic_D/0/1/0/all/0/1">Dragana Bajovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Jakovetic_D/0/1/0/all/0/1">Dusan Jakovetic</a>, <a href="http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1">Soummya Kar</a></p>
<p>Several recent works have studied the convergence \textit{in high
probability} of stochastic gradient descent (SGD) and its clipped variant.
Compared to vanilla SGD, clipped SGD is practically more stable and has the
additional theoretical benefit of logarithmic dependence on the failure
probability. However, the convergence of other practical nonlinear variants of
SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved
communication efficiency or accelerated convergence is much less understood. In
this work, we study the convergence bounds \textit{in high probability} of a
broad class of nonlinear SGD methods. For strongly convex loss functions with
Lipschitz continuous gradients, we prove a logarithmic dependence on the
failure probability, even when the noise is heavy-tailed. Strictly more general
than the results for clipped SGD, our results hold for any nonlinearity with
bounded (component-wise or joint) outputs, such as clipping, normalization, and
quantization. Further, existing results with heavy-tailed noise assume bounded
$\eta$-th central moments, with $\eta \in (1,2]$. In contrast, our refined
analysis works even for $\eta=1$, strictly relaxing the noise moment
assumptions in the literature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19647">Fast swap regret minimization and applications to approximate correlated equilibria. (arXiv:2310.19647v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Binghui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1">Aviad Rubinstein</a></p>
<p>We give a simple and computationally efficient algorithm that, for any
constant $\varepsilon&gt;0$, obtains $\varepsilon T$-swap regret within only $T =
\mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the
super-linear number of rounds required by the state-of-the-art algorithm, and
resolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an
exponential dependence on $\varepsilon$, but we prove a new, matching lower
bound.
</p>
<p>Our algorithm for swap regret implies faster convergence to
$\varepsilon$-Correlated Equilibrium ($\varepsilon$-CE) in several regimes: For
normal form two-player games with $n$ actions, it implies the first uncoupled
dynamics that converges to the set of $\varepsilon$-CE in polylogarithmic
rounds; a $\mathsf{polylog}(n)$-bit communication protocol for $\varepsilon$-CE
in two-player games (resolving an open problem mentioned by
[Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]); and an
$\tilde{O}(n)$-query algorithm for $\varepsilon$-CE (resolving an open problem
of [Babichenko'2020] and obtaining the first separation between
$\varepsilon$-CE and $\varepsilon$-Nash equilibrium in the query complexity
model).
</p>
<p>For extensive-form games, our algorithm implies a PTAS for $\mathit{normal}$
$\mathit{form}$ $\mathit{correlated}$ $\mathit{equilibria}$, a solution concept
often conjectured to be computationally intractable (e.g. [Stengel-Forges'08,
Fujii'23]).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01797">On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Puheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Huishuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jiang Bian</a></p>
<p>Diffusion models are a class of generative models that serve to establish a
stochastic transport map between an empirically observed, yet unknown, target
distribution and a known prior. Despite their remarkable success in real-world
applications, a theoretical understanding of their generalization capabilities
remains underdeveloped. This work embarks on a comprehensive theoretical
exploration of the generalization attributes of diffusion models. We establish
theoretical estimates of the generalization gap that evolves in tandem with the
training dynamics of score-based diffusion models, suggesting a polynomially
small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$
and the model capacity $m$, evading the curse of dimensionality (i.e., not
exponentially large in the data dimension) when early-stopped. Furthermore, we
extend our quantitative analysis to a data-dependent scenario, wherein target
distributions are portrayed as a succession of densities with progressively
increasing distances between modes. This precisely elucidates the adverse
effect of "modes shift" in ground truths on the model generalization. Moreover,
these estimates are not solely theoretical constructs but have also been
confirmed through numerical simulations. Our findings contribute to the
rigorous understanding of diffusion models' generalization properties and
provide insights that may guide practical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03062">Imaging through multimode fibres with physical prior. (arXiv:2311.03062v2 [physics.optics] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Zhang_C/0/1/0/all/0/1">Chuncheng Zhang</a>, <a href="http://arxiv.org/find/physics/1/au:+Shi_Y/0/1/0/all/0/1">Yingjie Shi</a>, <a href="http://arxiv.org/find/physics/1/au:+Yao_Z/0/1/0/all/0/1">Zheyi Yao</a>, <a href="http://arxiv.org/find/physics/1/au:+Sui_X/0/1/0/all/0/1">Xiubao Sui</a>, <a href="http://arxiv.org/find/physics/1/au:+Chen_Q/0/1/0/all/0/1">Qian Chen</a></p>
<p>Imaging through perturbed multimode fibres based on deep learning has been
widely researched. However, existing methods mainly use target-speckle pairs in
different configurations. It is challenging to reconstruct targets without
trained networks. In this paper, we propose a physics-assisted, unsupervised,
learning-based fibre imaging scheme. The role of the physical prior is to
simplify the mapping relationship between the speckle pattern and the target
image, thereby reducing the computational complexity. The unsupervised network
learns target features according to the optimized direction provided by the
physical prior. Therefore, the reconstruction process of the online learning
only requires a few speckle patterns and unpaired targets. The proposed scheme
also increases the generalization ability of the learning-based method in
perturbed multimode fibres. Our scheme has the potential to extend the
application of multimode fibre imaging.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05046">On the Consistency of Maximum Likelihood Estimation of Probabilistic Principal Component Analysis. (arXiv:2311.05046v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Datta_A/0/1/0/all/0/1">Arghya Datta</a>, <a href="http://arxiv.org/find/stat/1/au:+Chakrabarty_S/0/1/0/all/0/1">Sayak Chakrabarty</a></p>
<p>Probabilistic principal component analysis (PPCA) is currently one of the
most used statistical tools to reduce the ambient dimension of the data. From
multidimensional scaling to the imputation of missing data, PPCA has a broad
spectrum of applications ranging from science and engineering to quantitative
finance.
</p>
<p>Despite this wide applicability in various fields, hardly any theoretical
guarantees exist to justify the soundness of the maximal likelihood (ML)
solution for this model. In fact, it is well known that the maximum likelihood
estimation (MLE) can only recover the true model parameters up to a rotation.
The main obstruction is posed by the inherent identifiability nature of the
PPCA model resulting from the rotational symmetry of the parameterization. To
resolve this ambiguity, we propose a novel approach using quotient topological
spaces and in particular, we show that the maximum likelihood solution is
consistent in an appropriate quotient Euclidean space. Furthermore, our
consistency results encompass a more general class of estimators beyond the
MLE. Strong consistency of the ML estimate and consequently strong covariance
estimation of the PPCA model have also been established under a compactness
assumption.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05511">Anytime-Constrained Reinforcement Learning. (arXiv:2311.05511v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McMahan_J/0/1/0/all/0/1">Jeremy McMahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaojin Zhu</a></p>
<p>We introduce and study constrained Markov Decision Processes (cMDPs) with
anytime constraints. An anytime constraint requires the agent to never violate
its budget at any point in time, almost surely. Although Markovian policies are
no longer sufficient, we show that there exist optimal deterministic policies
augmented with cumulative costs. In fact, we present a fixed-parameter
tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our
reduction yields planning and learning algorithms that are time and
sample-efficient for tabular cMDPs so long as the precision of the costs is
logarithmic in the size of the cMDP. However, we also show that computing
non-trivial approximately optimal policies is NP-hard in general. To circumvent
this bottleneck, we design provable approximation algorithms that efficiently
compute or learn an arbitrarily accurate approximately feasible policy with
optimal value so long as the maximum supported cost is bounded by a polynomial
in the cMDP or the absolute budget. Given our hardness results, our
approximation guarantees are the best possible under worst-case analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05808">Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction. (arXiv:2311.05808v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shanghao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Ning Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1">Yang Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yi Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1">Y.Thomas Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_W/0/1/0/all/0/1">Wenjing Lou</a></p>
<p>Federated learning is known for its capability to safeguard participants'
data privacy. However, recently emerged model inversion attacks (MIAs) have
shown that a malicious parameter server can reconstruct individual users' local
data samples through model updates. The state-of-the-art attacks either rely on
computation-intensive search-based optimization processes to recover each input
batch, making scaling difficult, or they involve the malicious parameter server
adding extra modules before the global model architecture, rendering the
attacks too conspicuous and easily detectable.
</p>
<p>To overcome these limitations, we propose Scale-MIA, a novel MIA capable of
efficiently and accurately recovering training samples of clients from the
aggregated updates, even when the system is under the protection of a robust
secure aggregation protocol. Unlike existing approaches treating models as
black boxes, Scale-MIA recognizes the importance of the intricate architecture
and inner workings of machine learning models. It identifies the latent space
as the critical layer for breaching privacy and decomposes the complex recovery
task into an innovative two-step process to reduce computation complexity. The
first step involves reconstructing the latent space representations (LSRs) from
the aggregated model updates using a closed-form inversion mechanism,
leveraging specially crafted adversarial linear layers. In the second step, the
whole input batches are recovered from the LSRs by feeding them into a
fine-tuned generative decoder.
</p>
<p>We implemented Scale-MIA on multiple commonly used machine learning models
and conducted comprehensive experiments across various settings. The results
demonstrate that Scale-MIA achieves excellent recovery performance on different
datasets, exhibiting high reconstruction rates, accuracy, and attack efficiency
on a larger scale compared to state-of-the-art MIAs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06315">ShipGen: A Diffusion Model for Parametric Ship Hull Generation with Multiple Objectives and Constraints. (arXiv:2311.06315v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagazinski_N/0/1/0/all/0/1">Noah J. Bagazinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1">Faez Ahmed</a></p>
<p>Ship design is a years-long process that requires balancing complex design
trade-offs to create a ship that is efficient and effective. Finding new ways
to improve the ship design process can lead to significant cost savings for
ship building and operation. One promising technology is generative artificial
intelligence, which has been shown to reduce design cycle time and create
novel, high-performing designs. In literature review, generative artificial
intelligence has been shown to generate ship hulls; however, ship design is
particularly difficult as the hull of a ship requires the consideration of many
objectives. This paper presents a study on the generation of parametric ship
hull designs using a parametric diffusion model that considers multiple
objectives and constraints for the hulls. This denoising diffusion
probabilistic model (DDPM) generates the tabular parametric design vectors of a
ship hull for evaluation. In addition to a tabular DDPM, this paper details
adding guidance to improve the quality of generated ship hull designs. By
leveraging classifier guidance, the DDPM produced feasible parametric ship
hulls that maintain the coverage of the initial training dataset of ship hulls
with a 99.5% rate, a 149x improvement over random sampling of the design vector
parameters across the design space. Parametric ship hulls produced with
performance guidance saw an average of 91.4% reduction in wave drag
coefficients and an average of a 47.9x relative increase in the total displaced
volume of the hulls compared to the mean performance of the hulls in the
training dataset. The use of a DDPM to generate parametric ship hulls can
reduce design time by generating high-performing hull designs for future
analysis. These generated hulls have low drag and high volume, which can reduce
the cost of operating a ship and increase its potential to generate revenue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07247">Simultaneous Clutter Detection and Semantic Segmentation of Moving Objects for Automotive Radar Data. (arXiv:2311.07247v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kopp_J/0/1/0/all/0/1">Johannes Kopp</a>, <a href="http://arxiv.org/find/cs/1/au:+Kellner_D/0/1/0/all/0/1">Dominik Kellner</a>, <a href="http://arxiv.org/find/cs/1/au:+Piroli_A/0/1/0/all/0/1">Aldi Piroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1">Vinzenz Dallabetta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1">Klaus Dietmayer</a></p>
<p>The unique properties of radar sensors, such as their robustness to adverse
weather conditions, make them an important part of the environment perception
system of autonomous vehicles. One of the first steps during the processing of
radar point clouds is often the detection of clutter, i.e. erroneous points
that do not correspond to real objects. Another common objective is the
semantic segmentation of moving road users. These two problems are handled
strictly separate from each other in literature. The employed neural networks
are always focused entirely on only one of the tasks. In contrast to this, we
examine ways to solve both tasks at the same time with a single jointly used
model. In addition to a new augmented multi-head architecture, we also devise a
method to represent a network's predictions for the two tasks with only one
output value. This novel approach allows us to solve the tasks simultaneously
with the same inference time as a conventional task-specific model. In an
extensive evaluation, we show that our setup is highly effective and
outperforms every existing network for semantic segmentation on the RadarScenes
dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07344">Missing Value Imputation for Multi-attribute Sensor Data Streams via Message Propagation (Extended Version). (arXiv:2311.07344v2 [cs.DB] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Hua Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1">Christian S. Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1">Varun Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Markl_V/0/1/0/all/0/1">Volker Markl</a></p>
<p>Sensor data streams occur widely in various real-time applications in the
context of the Internet of Things (IoT). However, sensor data streams feature
missing values due to factors such as sensor failures, communication errors, or
depleted batteries. Missing values can compromise the quality of real-time
analytics tasks and downstream applications. Existing imputation methods either
make strong assumptions about streams or have low efficiency. In this study, we
aim to accurately and efficiently impute missing values in data streams that
satisfy only general characteristics in order to benefit real-time applications
more widely. First, we propose a message propagation imputation network (MPIN)
that is able to recover the missing values of data instances in a time window.
We give a theoretical analysis of why MPIN is effective. Second, we present a
continuous imputation framework that consists of data update and model update
mechanisms to enable MPIN to perform continuous imputation both effectively and
efficiently. Extensive experiments on multiple real datasets show that MPIN can
outperform the existing data imputers by wide margins and that the continuous
imputation framework is efficient and accurate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07439">Investigating Multi-Pivot Ensembling with Massively Multilingual Machine Translation Models. (arXiv:2311.07439v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1">Alireza Mohammadshahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1">Jannis Vamvas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1">Rico Sennrich</a></p>
<p>Massively multilingual machine translation models allow for the translation
of a large number of languages with a single model, but have limited
performance on low- and very-low-resource translation directions. Pivoting via
high-resource languages remains a strong strategy for low-resource directions,
and in this paper we revisit ways of pivoting through multiple languages.
Previous work has used a simple averaging of probability distributions from
multiple paths, but we find that this performs worse than using a single pivot,
and exacerbates the hallucination problem because the same hallucinations can
be probable across different paths. As an alternative, we propose MaxEns, a
combination strategy that is biased towards the most confident predictions,
hypothesising that confident predictions are less prone to be hallucinations.
We evaluate different strategies on the FLORES benchmark for 20 low-resource
language directions, demonstrating that MaxEns improves translation quality for
low-resource languages while reducing hallucination in translations, compared
to both direct translation and an averaging approach. On average, multi-pivot
strategies still lag behind using English as a single pivot language, raising
the question of how to identify the best pivoting strategy for a given
translation direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07534">Unsupervised Musical Object Discovery from Audio. (arXiv:2311.07534v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gha_J/0/1/0/all/0/1">Joonsu Gha</a>, <a href="http://arxiv.org/find/cs/1/au:+Herrmann_V/0/1/0/all/0/1">Vincent Herrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1">Benjamin Grewe</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1">J&#xfc;rgen Schmidhuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_A/0/1/0/all/0/1">Anand Gopalakrishnan</a></p>
<p>Current object-centric learning models such as the popular SlotAttention
architecture allow for unsupervised visual scene decomposition. Our novel
MusicSlots method adapts SlotAttention to the audio domain, to achieve
unsupervised music decomposition. Since concepts of opacity and occlusion in
vision have no auditory analogues, the softmax normalization of alpha masks in
the decoders of visual object-centric models is not well-suited for decomposing
audio objects. MusicSlots overcomes this problem. We introduce a
spectrogram-based multi-object music dataset tailored to evaluate
object-centric learning on western tonal music. MusicSlots achieves good
performance on unsupervised note discovery and outperforms several established
baselines on supervised note property prediction tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01994">Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution. (arXiv:2304.01994v2 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1">Brian Moser</a>, <a href="http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1">Stanislav Frolov</a>, <a href="http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1">Federico Raue</a>, <a href="http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1">Sebastian Palacio</a>, <a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1">Andreas Dengel</a></p>
<p>This paper presents a novel Diffusion-Wavelet (DiWa) approach for
Single-Image Super-Resolution (SISR). It leverages the strengths of Denoising
Diffusion Probabilistic Models (DDPMs) and Discrete Wavelet Transformation
(DWT). By enabling DDPMs to operate in the DWT domain, our DDPM models
effectively hallucinate high-frequency information for super-resolved images on
the wavelet spectrum, resulting in high-quality and detailed reconstructions in
image space. Quantitatively, we outperform state-of-the-art diffusion-based
SISR methods, namely SR3 and SRDiff, regarding PSNR, SSIM, and LPIPS on both
face (8x scaling) and general (4x scaling) SR benchmarks. Meanwhile, using DWT
enabled us to use fewer parameters than the compared models: 92M parameters
instead of 550M compared to SR3 and 9.3M instead of 12M compared to SRDiff.
Additionally, our method outperforms other state-of-the-art generative methods
on classical general SR datasets while saving inference time. Finally, our work
highlights its potential for various applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06794">CL-Flow:Strengthening the Normalizing Flows by Contrastive Learning for Better Anomaly Detection. (arXiv:2311.06794v1 [cs.IR] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shunfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yueyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haichi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_C/0/1/0/all/0/1">Chenyang Bi</a></p>
<p>In the anomaly detection field, the scarcity of anomalous samples has
directed the current research emphasis towards unsupervised anomaly detection.
While these unsupervised anomaly detection methods offer convenience, they also
overlook the crucial prior information embedded within anomalous samples.
Moreover, among numerous deep learning methods, supervised methods generally
exhibit superior performance compared to unsupervised methods. Considering the
reasons mentioned above, we propose a self-supervised anomaly detection
approach that combines contrastive learning with 2D-Flow to achieve more
precise detection outcomes and expedited inference processes. On one hand, we
introduce a novel approach to anomaly synthesis, yielding anomalous samples in
accordance with authentic industrial scenarios, alongside their surrogate
annotations. On the other hand, having obtained a substantial number of
anomalous samples, we enhance the 2D-Flow framework by incorporating
contrastive learning, leveraging diverse proxy tasks to fine-tune the network.
Our approach enables the network to learn more precise mapping relationships
from self-generated labels while retaining the lightweight characteristics of
the 2D-Flow. Compared to mainstream unsupervised approaches, our
self-supervised method demonstrates superior detection accuracy, fewer
additional model parameters, and faster inference speed. Furthermore, the
entire training and inference process is end-to-end. Our approach showcases new
state-of-the-art results, achieving a performance of 99.6\% in image-level
AUROC on the MVTecAD dataset and 96.8\% in image-level AUROC on the BTAD
dataset.
</p>
</p>
</div>

    </div>
    </body>
    