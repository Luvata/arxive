<!DOCTYPE html>
<html>
<head>
<title>2025-05-15-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    .copy-button {
        background-color: #007BFF;
        color: #fff;
        border: none;
        padding: 5px 10px;
        cursor: pointer;
        margin-top: 10px;
    }
    .copy-button:active {
        background-color: #0056b3;
    }
    .copy-message {
        display: none;
        background-color: #4CAF50;
        color: #fff;
        padding: 5px;
        margin-top: 10px;
        border-radius: 3px;
    }
    </style>
    <script>
    function copyToClipboard(text, buttonId) {
        const el = document.createElement('textarea');
        el.value = text;
        document.body.appendChild(el);
        el.select();
        document.execCommand('copy');
        document.body.removeChild(el);
        
        const messageElement = document.getElementById('copy-message-' + buttonId);
        messageElement.innerHTML = 'Copied "' + text + '" to clipboard';
        messageElement.style.display = 'block';
        setTimeout(function() {
            messageElement.style.display = 'none';
        }, 3000);
    }
    </script>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08792">A Preliminary Framework for Intersectionality in ML Pipelines</a></h1>
<p><b>Authors:</b> Michelle Nashla Turcios, Alicia E. Boyd, Angela D. R. Smith, Brittany Johnson</p>
<p>Abstract: Machine learning (ML) has become a go-to solution for improving how we use, experience, and interact with technology (and the world around us). Unfortunately, studies have repeatedly shown that machine learning technologies may not provide adequate support for societal identities and experiences. Intersectionality is a sociological framework that provides a mechanism for explicitly considering complex social identities, focusing on social justice and power. While the framework of intersectionality can support the development of technologies that acknowledge and support all members of society, it has been adopted and adapted in ways that are not always true to its foundations, thereby weakening its potential for impact. To support the appropriate adoption and use of intersectionality for more equitable technological outcomes, we amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and Collins (three C's), to create a socially relevant preliminary framework in developing machine-learning solutions. We use this framework to evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08792', 0)">Copy Link</button>
<div id="copy-message-0" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08793">Onboard Optimization and Learning: A Survey</a></h1>
<p><b>Authors:</b> Monirul Islam Pavel, Siyi Hu, Mahardhika Pratama, Ryszard Kowalczyk</p>
<p>Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time data processing, decision-making, and adaptive model training directly on resource-constrained devices without relying on centralized servers. This paradigm is crucial for applications demanding low latency, enhanced privacy, and energy efficiency. However, onboard learning faces challenges such as limited computational resources, high inference costs, and security vulnerabilities. This survey explores a comprehensive range of methodologies that address these challenges, focusing on techniques that optimize model efficiency, accelerate inference, and support collaborative learning across distributed devices. Approaches for reducing model complexity, improving inference speed, and ensuring privacy-preserving computation are examined alongside emerging strategies that enhance scalability and adaptability in dynamic environments. By bridging advancements in hardware-software co-design, model compression, and decentralized learning, this survey provides insights into the current state of onboard learning to enable robust, efficient, and secure AI deployment at the edge.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08793', 1)">Copy Link</button>
<div id="copy-message-1" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08795">The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures</a></h1>
<p><b>Authors:</b> Andres Anabalon, Hugo Garces, Julio Oliva, Jose Cifuentes</p>
<p>Abstract: We show that there is a fast algorithm that embeds hierarchical structures in three-dimensional Minkowski spacetime. The correlation of data ends up purely encoded in the causal structure. Our model relies solely on oriented token pairs -- local hierarchical signals -- with no access to global symbolic structure. We apply our method to the corpus of \textit{WordNet}. We provide a perfect embedding of the mammal sub-tree including ambiguities (more than one hierarchy per node) in such a way that the hierarchical structures get completely codified in the geometry and exactly reproduce the ground-truth. We extend this to a perfect embedding of the maximal unambiguous subset of the \textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We introduce a novel retrieval mechanism in which causality, not distance, governs hierarchical access. Our results seem to indicate that all discrete data has a perfect geometrical representation that is three-dimensional. The resulting embeddings are nearly conformally invariant, indicating deep connections with general relativity and field theory. These results suggest that concepts, categories, and their interrelations, namely hierarchical meaning itself, is geometric.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08795', 2)">Copy Link</button>
<div id="copy-message-2" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08803">Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models</a></h1>
<p><b>Authors:</b> Zizhao Hu, Mohammad Rostami, Jesse Thomason</p>
<p>Abstract: Recent research has highlighted the risk of generative model collapse, where performance progressively degrades when continually trained on self-generated data. However, existing exploration on model collapse is limited to single, unimodal models, limiting our understanding in more realistic scenarios, such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving. We expand the synthetic data training and model collapse study to multi-modal vision-language generative systems, such as vision-language models (VLMs) and text-to-image diffusion models, as well as recursive generate-train loops with multiple models. We find that model collapse, previously observed in single-modality generative models, exhibits distinct characteristics in the multi-modal context, such as improved vision-language alignment and increased variance in VLM image-captioning task. Additionally, we find that general approaches such as increased decoding budgets, greater model diversity, and relabeling with frozen models can effectively mitigate model collapse. Our findings provide initial insights and practical guidelines for reducing the risk of model collapse in self-improving multi-agent AI systems and curating robust multi-modal synthetic datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08803', 3)">Copy Link</button>
<div id="copy-message-3" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08823">An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits</a></h1>
<p><b>Authors:</b> Cody Steinmetz, Gavin Childress, Aaron Herbst, Gavin Jones, Jasdeep Singh, Eli Vang, Keagan Weinstock</p>
<p>Abstract: Large language models (LLMs) have transformed natural-language processing, yet their scale makes real-world deployment costly. Post-training quantization reduces memory and computation but often degrades accuracy, while quantization-aware training can recover performance at the cost of extra training. Pushing quantization to the ternary (2-bit) regime yields even larger savings but is notoriously unstable. Building on recent work showing that a bias-free, RMS-normalized Transformer with straight-through estimation can reach 1.58-bit precision, we demonstrate that simply inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule stably fine-tunes full-precision checkpoints into ternary LLMs. Our approach matches or surpasses more elaborate knowledge-distillation pipelines on standard language-modeling benchmarks without adding model complexity. These results indicate that careful normalization alone can close much of the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08823', 4)">Copy Link</button>
<div id="copy-message-4" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08827">Self Rewarding Self Improving</a></h1>
<p><b>Authors:</b> Toby Simonds, Kevin Lopez, Akira Yoshiyama, Dominique Garmier</p>
<p>Abstract: We demonstrate that large language models can effectively self-improve through self-judging without requiring reference solutions, leveraging the inherent asymmetry between generating and verifying solutions. Our experiments on Countdown puzzles and MIT Integration Bee problems show that models can provide reliable reward signals without ground truth answers, enabling reinforcement learning in domains previously not possible. By implementing self-judging, we achieve significant performance gains maintaining alignment with formal verification. When combined with synthetic question generation, we establish a complete self-improvement loop where models generate practice problems, solve them, and evaluate their own performance-achieving an 8% improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on integration tasks. Our findings demonstrate that LLM judges can provide effective reward signals for training models, unlocking many reinforcement learning environments previously limited by the difficulty of creating programmatic rewards. This suggests a potential paradigm shift toward AI systems that continuously improve through self-directed learning rather than human-guided training, potentially accelerating progress in domains with scarce training data or complex evaluation requirements.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08827', 5)">Copy Link</button>
<div id="copy-message-5" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08829">Aggregating Concepts of Accuracy and Fairness in Prediction Algorithms</a></h1>
<p><b>Authors:</b> David Kinney</p>
<p>Abstract: An algorithm that outputs predictions about the state of the world will almost always be designed with the implicit or explicit goal of outputting accurate predictions (i.e., predictions that are likely to be true). In addition, the rise of increasingly powerful predictive algorithms brought about by the recent revolution in artificial intelligence has led to an emphasis on building predictive algorithms that are fair, in the sense that their predictions do not systematically evince bias or bring about harm to certain individuals or groups. This state of affairs presents two conceptual challenges. First, the goals of accuracy and fairness can sometimes be in tension, and there are no obvious normative guidelines for managing the trade-offs between these two desiderata when they arise. Second, there are many distinct ways of measuring both the accuracy and fairness of a predictive algorithm; here too, there are no obvious guidelines on how to aggregate our preferences for predictive algorithms that satisfy disparate measures of fairness and accuracy to various extents. The goal of this paper is to address these challenges by arguing that there are good reasons for using a linear combination of accuracy and fairness metrics to measure the all-things-considered value of a predictive algorithm for agents who care about both accuracy and fairness. My argument depends crucially on a classic result in the preference aggregation literature due to Harsanyi. After making this formal argument, I apply my result to an analysis of accuracy-fairness trade-offs using the COMPAS dataset compiled by Angwin et al.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08829', 6)">Copy Link</button>
<div id="copy-message-6" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08846">Evaluating Simplification Algorithms for Interpretability of Time Series Classification</a></h1>
<p><b>Authors:</b> Felix Marti-Perez, Brigt H{\aa}vardstun, C\`esar Ferri, Carlos Monserrat, Jan Arne Telle</p>
<p>Abstract: In this work, we introduce metrics to evaluate the use of simplified time series in the context of interpretability of a TSC - a Time Series Classifier. Such simplifications are important because time series data, in contrast to text and image data, are not intuitively understandable to humans. These metrics are related to the complexity of the simplifications - how many segments they contain - and to their loyalty - how likely they are to maintain the classification of the original time series. We employ these metrics to evaluate four distinct simplification algorithms, across several TSC algorithms and across datasets of varying characteristics, from seasonal or stationary to short or long. Our findings suggest that using simplifications for interpretability of TSC is much better than using the original time series, particularly when the time series are seasonal, non-stationary and/or with low entropy.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08846', 7)">Copy Link</button>
<div id="copy-message-7" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08915">An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models</a></h1>
<p><b>Authors:</b> Jialin Mao, Itay Griniasty, Yan Sun, Mark K. Transtrum, James P. Sethna, Pratik Chaudhari</p>
<p>Abstract: Recent experiments have shown that training trajectories of multiple deep neural networks with different architectures, optimization algorithms, hyper-parameter settings, and regularization methods evolve on a remarkably low-dimensional "hyper-ribbon-like" manifold in the space of probability distributions. Inspired by the similarities in the training trajectories of deep networks and linear networks, we analytically characterize this phenomenon for the latter. We show, using tools in dynamical systems theory, that the geometry of this low-dimensional manifold is controlled by (i) the decay rate of the eigenvalues of the input correlation matrix of the training data, (ii) the relative scale of the ground-truth output to the weights at the beginning of training, and (iii) the number of steps of gradient descent. By analytically computing and bounding the contributions of these quantities, we characterize phase boundaries of the region where hyper-ribbons are to be expected. We also extend our analysis to kernel machines and linear models that are trained with stochastic gradient descent.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08915', 8)">Copy Link</button>
<div id="copy-message-8" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08940">NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach</a></h1>
<p><b>Authors:</b> Jeremie Blanchard, Lisa Casino, Jordan Gierschendorf</p>
<p>Abstract: The characterization of exoplanetary atmospheres through spectral analysis is a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration with the European Space Agency's (ESA) Ariel mission, provided an opportunity to explore machine learning techniques for extracting atmospheric compositions from simulated spectral data. In this work, we focus on a data-centric business approach, prioritizing generalization over competition-specific optimization. We briefly outline multiple experimental axes, including feature extraction, signal transformation, and heteroskedastic uncertainty modeling. Our experiments demonstrate that uncertainty estimation plays a crucial role in the Gaussian Log-Likelihood (GLL) score, impacting performance by several percentage points. Despite improving the GLL score by 11%, our results highlight the inherent limitations of tabular modeling and feature engineering for this task, as well as the constraints of a business-driven approach within a Kaggle-style competition framework. Our findings emphasize the trade-offs between model simplicity, interpretability, and generalization in astrophysical data analysis.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08940', 9)">Copy Link</button>
<div id="copy-message-9" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08941">ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers</a></h1>
<p><b>Authors:</b> Gavin Hull, Alex Bihlo</p>
<p>Abstract: Predicting the future citation rates of academic papers is an important step toward the automation of research evaluation and the acceleration of scientific progress. We present $\textbf{ForeCite}$, a simple but powerful framework to append pre-trained causal language models with a linear head for average monthly citation rate prediction. Adapting transformers for regression tasks, ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of 900K+ biomedical papers published between 2000 and 2024, a 27-point improvement over the previous state-of-the-art. Comprehensive scaling-law analysis reveals consistent gains across model sizes and data volumes, while temporal holdout experiments confirm practical robustness. Gradient-based saliency heatmaps suggest a potentially undue reliance on titles and abstract texts. These results establish a new state-of-the-art in forecasting the long-term influence of academic research and lay the groundwork for the automated, high-fidelity evaluation of scientific contributions.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08941', 10)">Copy Link</button>
<div id="copy-message-10" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08964">GPML: Graph Processing for Machine Learning</a></h1>
<p><b>Authors:</b> Majed Jaber, Julien Michel, Nicolas Boutry, Pierre Parrend</p>
<p>Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in dynamic networks involves advanced cyber-threat detectors. The GPML (Graph Processing for Machine Learning) library addresses this need by transforming raw network traffic traces into graph representations, enabling advanced insights into network behaviors. The library provides tools to detect anomalies in interaction and community shifts in dynamic networks. GPML supports community and spectral metrics extraction, enhancing both real-time detection and historical forensics analysis. This library supports modern cybersecurity challenges with a robust, graph-based approach.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08964', 11)">Copy Link</button>
<div id="copy-message-11" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08977">SaFARi: State-Space Models for Frame-Agnostic Representation</a></h1>
<p><b>Authors:</b> Hossein Babaei, Mel White, Sina Alemohammad, Richard G. Baraniuk</p>
<p>Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online function approximation, and as the backbone of machine learning models for long-range dependent data. However, to date, only a few polynomial bases have been explored for this purpose, and the state-of-the-art implementations were built upon the best of a few limited options. In this paper, we present a generalized method for building an SSM with any frame or basis, rather than being restricted to polynomials. This framework encompasses the approach known as HiPPO, but also permits an infinite diversity of other possible "species" within the SSM architecture. We dub this approach SaFARi: SSMs for Frame-Agnostic Representation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08977', 12)">Copy Link</button>
<div id="copy-message-12" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.08982">Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret</a></h1>
<p><b>Authors:</b> Jiachen Qian, Yang Zheng</p>
<p>Abstract: We consider the problem of online prediction for an unknown, non-explosive linear stochastic system. With a known system model, the optimal predictor is the celebrated Kalman filter. In the case of unknown systems, existing approaches based on recursive least squares and its variants may suffer from degraded performance due to the highly imbalanced nature of the regression model. This imbalance can easily lead to overfitting and thus degrade prediction accuracy. We tackle this problem by injecting an inductive bias into the regression model via {exponential forgetting}. While exponential forgetting is a common wisdom in online learning, it is typically used for re-weighting data. In contrast, our approach focuses on balancing the regression model. This achieves a better trade-off between {regression} and {regularization errors}, and simultaneously reduces the {accumulation error}. With new proof techniques, we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$ is the number of observations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08982', 13)">Copy Link</button>
<div id="copy-message-13" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09003">Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition</a></h1>
<p><b>Authors:</b> Zeki Doruk Erden, Donia Gasmi, Boi Faltings</p>
<p>Abstract: Continual learning for reinforcement learning agents remains a significant challenge, particularly in preserving and leveraging existing information without an external signal to indicate changes in tasks or environments. In this study, we explore the effectiveness of autoencoders in detecting new tasks and matching observed environments to previously encountered ones. Our approach integrates policy optimization with familiarity autoencoders within an end-to-end continual learning system. This system can recognize and learn new tasks or environments while preserving knowledge from earlier experiences and can selectively retrieve relevant knowledge when re-encountering a known environment. Initial results demonstrate successful continual learning without external signals to indicate task changes or reencounters, showing promise for this methodology.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09003', 14)">Copy Link</button>
<div id="copy-message-14" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09011">Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer</a></h1>
<p><b>Authors:</b> Antonio Candito, Matthew D Blackledge, Richard Holbrey, Nuria Porta, Ana Ribeiro, Fabio Zugni, Luca D'Erme, Francesca Castagnoli, Alina Dragan, Ricardo Donners, Christina Messiou, Nina Tunariu, Dow-Mu Koh</p>
<p>Abstract: We developed an AI-driven software solution to quantify metastatic bone disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised Residual U-Net model generating a skeleton probability map to isolate bone; (ii) a statistical framework for WB-DWI intensity normalisation, obtaining a signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional neural network that processes outputs from (i) and (ii) to generate a mask of suspected bone lesions, characterised by higher b900 signal intensity due to restricted water diffusion. This mask is applied to the gADC map to extract TDV and gADC statistics. We tested the tool using expert-defined metastatic bone disease delineations on 66 datasets, assessed repeatability of imaging biomarkers (N=10), and compared software-based response assessment with a construct reference standard based on clinical, laboratory and imaging assessments (N=118). Dice score between manual and automated delineations was 0.6 for lesions within pelvis and spine, with an average surface distance of 2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC were below 9% and 5%, respectively. Repeatability analysis showed coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing response to treatment compared to the construct reference standard. Computation time generating a mask averaged 90 seconds per scan. Our software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, thus providing potentially useful measurements for clinical decision-making in APC patients.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09011', 15)">Copy Link</button>
<div id="copy-message-15" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09017">DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update</a></h1>
<p><b>Authors:</b> Bizhan Alipour Pijan, Serdar Bozdag</p>
<p>Abstract: Most of the dynamic graph representation learning methods involve dividing a dynamic graph into discrete snapshots to capture the evolving behavior of nodes over time. Existing methods primarily capture only local or global structures of each node within a snapshot using message-passing and random walk-based methods. Then, they utilize sequence-based models (e.g., transformers) to encode the temporal evolution of node embeddings, and meta-learning techniques to update the model parameters. However, these approaches have two limitations. First, they neglect the extraction of global and local information simultaneously in each snapshot. Second, they fail to consider the model's performance in the current snapshot during parameter updates, resulting in a lack of temporal dependency management. Recently, HiPPO (High-order Polynomial Projection Operators) algorithm has gained attention for their ability to optimize and preserve sequence history in State Space Model (SSM). To address the aforementioned limitations in dynamic graph representation learning, we propose a novel method called Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. We then integrate the local and global features using a cross-attention mechanism. Additionally, we incorporate an SSM based on HiPPO algorithm to account for long-term dependencies when updating model parameters, ensuring that model performance in each snapshot informs subsequent updates. Experiments on five public datasets show that our method outperforms existing baseline and state-of-the-art (SOTA) methods in 17 out of 20 cases.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09017', 16)">Copy Link</button>
<div id="copy-message-16" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09022">Block-Biased Mamba for Long-Range Sequence Processing</a></h1>
<p><b>Authors:</b> Annan Yu, N. Benjamin Erichson</p>
<p>Abstract: Mamba extends earlier state space models (SSMs) by introducing input-dependent dynamics, and has demonstrated strong empirical performance across a range of domains, including language modeling, computer vision, and foundation models. However, a surprising weakness remains: despite being built on architectures designed for long-range dependencies, Mamba performs poorly on long-range sequential tasks. Understanding and addressing this gap is important for improving Mamba's universality and versatility. In this work, we analyze Mamba's limitations through three perspectives: expressiveness, inductive bias, and training stability. Our theoretical results show how Mamba falls short in each of these aspects compared to earlier SSMs such as S4D. To address these issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias. We prove that these changes equip the model with a better-suited inductive bias and improve its expressiveness and stability. Empirically, $\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks while maintaining Mamba's performance on language modeling benchmarks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09022', 17)">Copy Link</button>
<div id="copy-message-17" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09063">Single-shot prediction of parametric partial differential equations</a></h1>
<p><b>Authors:</b> Khalid Rafiq, Wenjing Liao, Aditya G. Nair</p>
<p>Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot forecasting of nonlinear parametric partial differential equations (PDEs), eliminating the need for iterative time-stepping while maintaining high accuracy and stability. Flexi-VAE incorporates a neural propagator that advances latent representations forward in time, aligning latent evolution with physical state reconstruction in a variational autoencoder setting. We evaluate two propagation strategies, the Direct Concatenation Propagator (DCP) and the Positional Encoding Propagator (PEP), and demonstrate, through representation-theoretic analysis, that DCP offers superior long-term generalization by fostering disentangled and physically meaningful latent spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal that propagated latent states reside in regions of lower decoder sensitivity and more stable local geometry than those derived via direct encoding, enhancing robustness for long-horizon predictions. We validate Flexi-VAE on canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D advection-diffusion equation, achieving accurate forecasts across wide parametric ranges. The model delivers over 50x CPU and 90x GPU speedups compared to autoencoder-LSTM baselines for large temporal shifts. These results position Flexi-VAE as a scalable and interpretable surrogate modeling tool for accelerating high-fidelity simulations in computational fluid dynamics (CFD) and other parametric PDE-driven applications, with extensibility to higher-dimensional and more complex systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09063', 18)">Copy Link</button>
<div id="copy-message-18" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09076">AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation</a></h1>
<p><b>Authors:</b> Berkay Guler, Hamid Jafarkhani</p>
<p>Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division Multiplexing (OFDM) systems often suffer from performance degradation under fast-fading channels and low-SNR scenarios. To address these limitations, we introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model specifically designed to enhance channel estimation in challenging environments. Our approach employs convolutional layers that exploit locality bias to capture strong correlations between neighboring channel elements, combined with a transformer encoder that applies the global Attention mechanism to channel patches. This approach effectively models both long-range dependencies and spectro-temporal interactions within single OFDM frames. We further augment the model's adaptability by integrating nonlinear representations of available channel statistics SNR, delay spread, and Doppler shift as priors. A residual connection is employed to merge global features from the transformer with local features from early convolutional processing, followed by final convolutional layers to refine the hierarchical channel representation. Despite its compact architecture, AdaFortiTran achieves up to 6 dB reduction in mean squared error (MSE) compared to state-of-the-art models. Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB), and delay spreads (50-300 ns), it demonstrates superior robustness in high-mobility environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09076', 19)">Copy Link</button>
<div id="copy-message-19" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09085">Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision</a></h1>
<p><b>Authors:</b> Jiaxuan Chen, Yu Qi, Yueming Wang, Gang Pan</p>
<p>Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale language models, have demonstrated remarkable capabilities in image and natural language understanding. Although scaling up model parameters with increasing volume of training data has progressively improved DNN capabilities, achieving complex cognitive abilities - such as understanding abstract concepts, reasoning, and adapting to novel scenarios, which are intrinsic to human cognition - remains a major challenge. In this study, we show that brain-in-the-loop supervised learning, utilizing a small set of brain signals, can effectively transfer human conceptual structures to DNNs, significantly enhancing their comprehension of abstract and even unseen concepts. Experimental results further indicate that the enhanced cognitive capabilities lead to substantial performance gains in challenging tasks, including few-shot/zero-shot learning and out-of-distribution recognition, while also yielding highly interpretable concept representations. These findings highlight that human-in-the-loop supervision can effectively augment the complex cognitive abilities of large models, offering a promising pathway toward developing more human-like cognitive abilities in artificial systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09085', 20)">Copy Link</button>
<div id="copy-message-20" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09089">Generating time-consistent dynamics with discriminator-guided image diffusion models</a></h1>
<p><b>Authors:</b> Philipp Hess, Maximilian Gelbrecht, Christof Sch\"otz, Michael Aich, Yu Huang, Shangshang Yang, Niklas Boers</p>
<p>Abstract: Realistic temporal dynamics are crucial for many video generation, processing and modelling applications, e.g. in computational fluid dynamics, weather prediction, or long-term climate simulations. Video diffusion models (VDMs) are the current state-of-the-art method for generating highly realistic dynamics. However, training VDMs from scratch can be challenging and requires large computational resources, limiting their wider application. Here, we propose a time-consistency discriminator that enables pretrained image diffusion models to generate realistic spatiotemporal dynamics. The discriminator guides the sampling inference process and does not require extensions or finetuning of the image diffusion model. We compare our approach against a VDM trained from scratch on an idealized turbulence simulation and a real-world global precipitation dataset. Our approach performs equally well in terms of temporal consistency, shows improved uncertainty calibration and lower biases compared to the VDM, and achieves stable centennial-scale climate simulations at daily time steps.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09089', 21)">Copy Link</button>
<div id="copy-message-21" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09106">Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network</a></h1>
<p><b>Authors:</b> Ya Liu, Kai Yang, Yu Zhu, Keying Yang, Haibo Zhao</p>
<p>Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a core element in the 6G networks. However, traditional centralized and synchronous optimization algorithms are unsuitable for SAGIN due to infrastructureless and time-varying environments. This paper aims to develop a novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and non-smooth decentralized federated bilevel learning over SAGIN. The proposed algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle bilevel learning problems in time-varying networks asynchronously, thereby averting stragglers from impeding the overall training speed. We provide a theoretical analysis of the iteration complexity, communication complexity, and computational complexity of Argus. Its effectiveness is further demonstrated through numerical experiments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09106', 22)">Copy Link</button>
<div id="copy-message-22" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09113">Sequential Treatment Effect Estimation with Unmeasured Confounders</a></h1>
<p><b>Authors:</b> Yingrong Wang, Anpeng Wu, Baohong Li, Ziyang Xiao, Ruoxuan Xiong, Qing Han, Kun Kuang</p>
<p>Abstract: This paper studies the cumulative causal effects of sequential treatments in the presence of unmeasured confounders. It is a critical issue in sequential decision-making scenarios where treatment decisions and outcomes dynamically evolve over time. Advanced causal methods apply transformer as a backbone to model such time sequences, which shows superiority in capturing long time dependence and periodic patterns via attention mechanism. However, even they control the observed confounding, these estimators still suffer from unmeasured confounders, which influence both treatment assignments and outcomes. How to adjust the latent confounding bias in sequential treatment effect estimation remains an open challenge. Therefore, we propose a novel Decomposing Sequential Instrumental Variable framework for CounterFactual Regression (DSIV-CFR), relying on a common negative control assumption. Specifically, an instrumental variable (IV) is a special negative control exposure, while the previous outcome serves as a negative control outcome. This allows us to recover the IVs latent in observation variables and estimate sequential treatment effects via a generalized moment condition. We conducted experiments on 4 datasets and achieved significant performance in one- and multi-step prediction, supported by which we can identify optimal treatments for dynamic systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09113', 23)">Copy Link</button>
<div id="copy-message-23" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09131">Fair Clustering via Alignment</a></h1>
<p><b>Authors:</b> Kunwoong Kim, Jihu Lee, Sangchul Park, Yongdai Kim</p>
<p>Abstract: Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice. To resolve these limitations, we propose a new fair clustering algorithm based on a novel decomposition of the fair K-means clustering objective function. The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space. A key advantage of FCA is that it theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice. Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09131', 24)">Copy Link</button>
<div id="copy-message-24" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09134">Scaling Gaussian Process Regression with Full Derivative Observations</a></h1>
<p><b>Authors:</b> Daniel Huang</p>
<p>Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict full derivative observations called DSoftKI. It extends SoftKI, a method that approximates a kernel via softmax interpolation from learned interpolation point locations, to the setting with derivatives. DSoftKI enhances SoftKI's interpolation scheme to incorporate the directional orientation of interpolation points relative to the data. This enables the construction of a scalable approximate kernel, including its first and second-order derivatives, through interpolation. We evaluate DSoftKI on a synthetic function benchmark and high-dimensional molecular force field prediction (100-1000 dimensions), demonstrating that DSoftKI is accurate and can scale to larger datasets with full derivative observations than previously possible.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09134', 25)">Copy Link</button>
<div id="copy-message-25" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09160">A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning</a></h1>
<p><b>Authors:</b> Berkay Guler, Giovanni Geraci, Hamid Jafarkhani</p>
<p>Abstract: Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications. Aiming to fill this gap, we first propose WiMAE (Wireless Masked Autoencoder), a transformer-based encoder-decoder foundation model pretrained on a realistic open-source multi-antenna wireless channel dataset. Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by incorporating a contrastive learning objective alongside the reconstruction task in a unified multi-task framework. By warm-starting from pretrained WiMAE weights and generating positive pairs via noise injection, the contrastive component enables the model to capture both structural and discriminative features, enhancing representation quality beyond what reconstruction alone can achieve. Through extensive evaluation on unseen scenarios, we demonstrate the effectiveness of both approaches across multiple downstream tasks, with ContraWiMAE showing further improvements in linear separability and adaptability in diverse wireless environments. Comparative evaluations against a state-of-the-art wireless channel foundation model confirm the superior performance and data efficiency of our models, highlighting their potential as powerful baselines for future research in self-supervised wireless channel representation learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09160', 26)">Copy Link</button>
<div id="copy-message-26" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09174">Quotient Complex Transformer (QCformer) for Perovskite Data Analysis</a></h1>
<p><b>Authors:</b> Xinyu You, Xiang Liu, Chuan-Shen Hu, Kelin Xia, Tze Chien Sum</p>
<p>Abstract: The discovery of novel functional materials is crucial in addressing the challenges of sustainable energy generation and climate change. Hybrid organic-inorganic perovskites (HOIPs) have gained attention for their exceptional optoelectronic properties in photovoltaics. Recently, geometric deep learning, particularly graph neural networks (GNNs), has shown strong potential in predicting material properties and guiding material design. However, traditional GNNs often struggle to capture the periodic structures and higher-order interactions prevalent in such systems. To address these limitations, we propose a novel representation based on quotient complexes (QCs) and introduce the Quotient Complex Transformer (QCformer) for material property prediction. A material structure is modeled as a quotient complex, which encodes both pairwise and many-body interactions via simplices of varying dimensions and captures material periodicity through a quotient operation. Our model leverages higher-order features defined on simplices and processes them using a simplex-based Transformer module. We pretrain QCformer on benchmark datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP datasets. The results show that QCformer outperforms state-of-the-art models in HOIP property prediction, demonstrating its effectiveness. The quotient complex representation and QCformer model together contribute a powerful new tool for predictive modeling of perovskite materials.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09174', 27)">Copy Link</button>
<div id="copy-message-27" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09175">Optimizing Urban Critical Green Space Development Using Machine Learning</a></h1>
<p><b>Authors:</b> Mohammad Ganjirad, Mahmoud Reza Delavar, Hossein Bagheri, Mohammad Mehdi Azizi</p>
<p>Abstract: This paper presents a novel framework for prioritizing urban green space development in Tehran using diverse socio-economic, environmental, and sensitivity indices. The indices were derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the Weather Research & Forecasting (WRF) model. The WRF model was used to estimate the air temperature at a 1 km resolution due to insufficient meteorological stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C, respectively. After data preparation, several machine learning models were used for binary vegetation cover classification including XGBoost, LightGBM, Random Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94% in Overall Accuracy, Recall, and F1-score. Then, the probability of areas lacking vegetation cover was assessed using socio-economic, environmental and sensitivity indices. This resulted in the RF generating an urban green space development prioritization map. Feature Importance Analysis revealed that the most significant indices were nightly land surface temperature (LST) and sensitive population. Finally, the framework performance was validated through microclimate simulation to assess the critical areas after and before the green space development by green roofs. The simulation demonstrated reducing air temperature by up to 0.67{\deg}C after utilizing the green roof technology in critical areas. As a result, this framework provides a valuable tool for urban planners to develop green spaces.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09175', 28)">Copy Link</button>
<div id="copy-message-28" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09214">The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks</a></h1>
<p><b>Authors:</b> Zhonghao Lyu, Ming Xiao, Jie Xu, Mikael Skoglund, Marco Di Renzo</p>
<p>Abstract: The growing demand for large artificial intelligence model (LAIM) services is driving a paradigm shift from traditional cloud-based inference to edge-based inference for low-latency, privacy-preserving applications. In particular, edge-device co-inference, which partitions LAIMs between edge devices and servers, has emerged as a promising strategy for resource-efficient LAIM execution in wireless networks. In this paper, we investigate a pruning-aware LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned into on-device and on-server sub-models for deployment. For analysis, we first prove that the LAIM output distortion is upper bounded by its parameter distortion. Then, we derive a lower bound on parameter distortion via rate-distortion theory, analytically capturing the relationship between pruning ratio and co-inference performance. Next, based on the analytical results, we formulate an LAIM co-inference distortion bound minimization problem by jointly optimizing the pruning ratio, transmit power, and computation frequency under system latency, energy, and available resource constraints. Moreover, we propose an efficient algorithm to tackle the considered highly non-convex problem. Finally, extensive simulations demonstrate the effectiveness of the proposed design. In particular, model parameter distortion is shown to provide a reliable bound on output distortion. Also, the proposed joint pruning ratio and resource management design achieves superior performance in balancing trade-offs among inference performance, system latency, and energy consumption compared with benchmark schemes, such as fully on-device and on-server inference. Moreover, the split point is shown to play a critical role in system performance optimization under heterogeneous and resource-limited edge environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09214', 29)">Copy Link</button>
<div id="copy-message-29" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09218">Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods</a></h1>
<p><b>Authors:</b> Alexander Tyurin, Danil Sivtsov</p>
<p>Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing distributed SGD methods. The central idea is to represent each method as a weighted directed tree, referred to as a computation tree. Leveraging this representation, we introduce a general theoretical result that reduces convergence analysis to studying the geometry of these trees. This perspective yields a purely graph-based interpretation of optimization dynamics, offering a new and intuitive foundation for method development. Using Birch SGD, we design eight new methods and analyze them alongside previously known ones, with at least six of the new methods shown to have optimal computational time complexity. Our research leads to two key insights: (i) all methods share the same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} + \frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree distance" along the main branch of a tree; and (ii) different methods exhibit different trade-offs-for example, some update iterates more frequently, improving practical performance, while others are more communication-efficient or focus on other aspects. Birch SGD serves as a unifying framework for navigating these trade-offs. We believe these results provide a unified foundation for understanding, analyzing, and designing efficient asynchronous and parallel optimization methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09218', 30)">Copy Link</button>
<div id="copy-message-30" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09239">Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories</a></h1>
<p><b>Authors:</b> Faruk Alpay</p>
<p>Abstract: The Information Bottleneck (IB) method frequently suffers from unstable optimization, characterized by abrupt representation shifts near critical points of the IB trade-off parameter, beta. In this paper, I introduce a novel approach to achieve stable and convex IB optimization through symbolic continuation and entropy-regularized trajectories. I analytically prove convexity and uniqueness of the IB solution path when an entropy regularization term is included, and demonstrate how this stabilizes representation learning across a wide range of \b{eta} values. Additionally, I provide extensive sensitivity analyses around critical points (beta) with statistically robust uncertainty quantification (95% confidence intervals). The open-source implementation, experimental results, and reproducibility framework included in this work offer a clear path for practical deployment and future extension of my proposed method.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09239', 31)">Copy Link</button>
<div id="copy-message-31" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09284">Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations</a></h1>
<p><b>Authors:</b> Panqi Chen, Yifan Sun, Lei Cheng, Yang Yang, Weichang Li, Yang Liu, Weiqing Liu, Jiang Bian, Shikai Fang</p>
<p>Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations presents a fundamental challenge in scientific research. Recently, diffusion-based generative modeling shows promising potential for physical simulation. However, current approaches typically operate on on-grid data with preset spatiotemporal resolution, but struggle with the sparsely observed and continuous nature of real-world physical dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in Functional Tucker space, a novel framework that generates full-field evolution of physical dynamics from irregular sparse observations. SDIFT leverages the functional Tucker model as the latent space representer with proven universal approximation property, and represents observations as latent functions and Tucker core sequences. We then construct a sequential diffusion model with temporally augmented UNet in the functional Tucker space, denoising noise drawn from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior Sampling mechanism, enabling conditional generation of the entire sequence guided by observations at limited time steps. We validate SDIFT on three physical systems spanning astronomical (supernova explosions, light-year scale), environmental (ocean sound speed fields, kilometer scale), and molecular (organic liquid, millimeter scale) domains, demonstrating significant improvements in both reconstruction accuracy and computational efficiency compared to state-of-the-art approaches.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09284', 32)">Copy Link</button>
<div id="copy-message-32" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09287">Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features</a></h1>
<p><b>Authors:</b> Shunsuke Yoneda, Valdemar \v{S}v\'abensk\'y, Gen Li, Daisuke Deguchi, Atsushi Shimada</p>
<p>Abstract: Digital textbooks are widely used in various educational contexts, such as university courses and online lectures. Such textbooks yield learning log data that have been used in numerous educational data mining (EDM) studies for student behavior analysis and performance prediction. However, these studies have faced challenges in integrating confidential data, such as academic records and learning logs, across schools due to privacy concerns. Consequently, analyses are often conducted with data limited to a single school, which makes developing high-performing and generalizable models difficult. This study proposes a method that combines federated learning and differential features to address these issues. Federated learning enables model training without centralizing data, thereby preserving student privacy. Differential features, which utilize relative values instead of absolute values, enhance model performance and generalizability. To evaluate the proposed method, a model for predicting at-risk students was trained using data from 1,136 students across 12 courses conducted over 4 years, and validated on hold-out test data from 5 other courses. Experimental results demonstrated that the proposed method addresses privacy concerns while achieving performance comparable to that of models trained via centralized learning in terms of Top-n precision, nDCG, and PR-AUC. Furthermore, using differential features improved prediction performance across all evaluation datasets compared to non-differential approaches. The trained models were also applicable for early prediction, achieving high performance in detecting at-risk students in earlier stages of the semester within the validation datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09287', 33)">Copy Link</button>
<div id="copy-message-33" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09294">On the Learning with Augmented Class via Forests</a></h1>
<p><b>Authors:</b> Fan Xu, Wuyang Chen, Wei Gao</p>
<p>Abstract: Decision trees and forests have achieved successes in various real applications, most working with all testing classes known in training data. In this work, we focus on learning with augmented class via forests, where an augmented class may appear in testing data yet not in training data. We incorporate information of augmented class into trees' splitting, i.e., a new splitting criterion, called augmented Gini impurity, is introduced to exploit some unlabeled data from testing distribution. We then develop the approach named Learning with Augmented Class via Forests (LACForest), which constructs shallow forests based on the augmented Gini impurity and then splits forests with pseudo-labeled augmented instances for better performance. We also develop deep neural forests with a novel optimization objective based on our augmented Gini impurity, so as to utilize the representation power of neural networks for forests. Theoretically, we present the convergence analysis for augmented Gini impurity, and finally conduct experiments to verify the effectiveness of our approaches. The code is available at https://github.com/nju-xuf/LACForest/.</p>
<p>URLs: <a href="https://github.com/nju-xuf/LACForest/.">https://github.com/nju-xuf/LACForest/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09294, https://github.com/nju-xuf/LACForest/.', 34)">Copy Link</button>
<div id="copy-message-34" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09308">Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model</a></h1>
<p><b>Authors:</b> George Andriopoulos, Soyuj Jung Basnet, Juan Guevara, Li Guo, Keith Ross</p>
<p>Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that enables closed-form approximations for minimal training loss and related performance measures in deep neural networks (DNNs). This paper leverages the UFM to provide qualitative insights into neural multivariate regression, a critical task in imitation learning, robotics, and reinforcement learning. Specifically, we address two key questions: (1) How do multi-task models compare to multiple single-task models in terms of training performance? (2) Can whitening and normalizing regression targets improve training performance? The UFM theory predicts that multi-task models achieve strictly smaller training MSE than multiple single-task models when the same or stronger regularization is applied to the latter, and our empirical results confirm these findings. Regarding whitening and normalizing regression targets, the UFM theory predicts that they reduce training MSE when the average variance across the target dimensions is less than one, and our empirical results once again confirm these findings. These findings highlight the UFM as a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09308', 35)">Copy Link</button>
<div id="copy-message-35" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09331">MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks</a></h1>
<p><b>Authors:</b> Cunlai Pu, Fangrui Wu, Rajput Ramiz Sharafat, Guangzhao Dai, Xiangbo Shu</p>
<p>Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs) aims to predict the potential formation of future links between UAVs. In adversarial environments where the route information of UAVs is unavailable, predicting future links must rely solely on the observed historical topological information of UANETs. However, the highly dynamic and sparse nature of UANET topologies presents substantial challenges in effectively capturing meaningful structural and temporal patterns for accurate link prediction. Most existing link prediction methods focus on temporal dynamics at a single structural scale while neglecting the effects of sparsity, resulting in insufficient information capture and limited applicability to UANETs. In this paper, we propose a multi-scale structural-temporal link prediction model (MUST) for UANETs. Specifically, we first employ graph attention networks (GATs) to capture structural features at multiple levels, including the individual UAV level, the UAV community level, and the overall network level. Then, we use long short-term memory (LSTM) networks to learn the temporal dynamics of these multi-scale structural features. Additionally, we address the impact of sparsity by introducing a sophisticated loss function during model optimization. We validate the performance of MUST using several UANET datasets generated through simulations. Extensive experimental results demonstrate that MUST achieves state-of-the-art link prediction performance in highly dynamic and sparse UANETs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09331', 36)">Copy Link</button>
<div id="copy-message-36" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09344">GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks</a></h1>
<p><b>Authors:</b> Gabriel Cort\^es, Nuno Louren\c{c}o, Paolo Romano, Penousal Machado</p>
<p>Abstract: Determining the performance of a Deep Neural Network during Neural Architecture Search processes is essential for identifying optimal architectures and hyperparameters. Traditionally, this process requires training and evaluation of each network, which is time-consuming and resource-intensive. Zero-cost proxies estimate performance without training, serving as an alternative to traditional training. However, recent proxies often lack generalization across diverse scenarios and provide only relative rankings rather than predicted accuracies. To address these limitations, we propose GreenFactory, an ensemble of zero-cost proxies that leverages a random forest regressor to combine multiple predictors' strengths and directly predict model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust results across multiple datasets. Specifically, GreenFactory achieves high Kendall correlations on NATS-Bench-SSS, indicating substantial agreement between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120, showcasing its reliability in both search spaces.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09344', 37)">Copy Link</button>
<div id="copy-message-37" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09354">Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning</a></h1>
<p><b>Authors:</b> Guangtai Wang, Chi-Man Vong, Jintao Huang</p>
<p>Abstract: Diminishing the impact of false-positive labels is critical for conducting disambiguation in partial label learning. However, the existing disambiguation strategies mainly focus on exploiting the characteristics of individual partial label instances while neglecting the strong supervision information of clean samples randomly lying in the datasets. In this work, we show that clean samples can be collected to offer guidance and enhance the confidence of the most possible candidates. Motivated by the manner of the differentiable count loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new calibration strategy called CleanSE. Specifically, we attribute the most reliable candidates with higher significance under the assumption that for each clean sample, if its label is one of the candidates of its nearest neighbor in the representation space, it is more likely to be the ground truth of its neighbor. Moreover, clean samples offer help in characterizing the sample distributions by restricting the label counts of each label to a specific interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL datasets showed this calibration strategy can be applied to most of the state-of-the-art PLL methods as well as enhance their performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09354', 38)">Copy Link</button>
<div id="copy-message-38" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09361">Efficient Mixed Precision Quantization in Graph Neural Networks</a></h1>
<p><b>Authors:</b> Samir Moustafa, Nils M. Kriege, Wilfried N. Gansterer</p>
<p>Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale graph applications. However, the computational demands of GNNs necessitate the development of efficient methods to accelerate inference. Mixed precision quantization emerges as a promising solution to enhance the efficiency of GNN architectures without compromising prediction performance. Compared to conventional deep learning architectures, GNN layers contain a wider set of components that can be quantized, including message passing functions, aggregation functions, update functions, the inputs, learnable parameters, and outputs of these functions. In this paper, we introduce a theorem for efficient quantized message passing to aggregate integer messages. It guarantees numerical equality of the aggregated messages using integer values with respect to those obtained with full (FP32) precision. Based on this theorem, we introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which flexibly selects effective integer bit-widths for all components within GNN layers. Our approach systematically navigates the wide set of possible bit-width combinations, addressing the challenge of optimizing efficiency while aiming at maintaining comparable prediction performance. MixQ-GNN integrates with existing GNN quantization methods, utilizing their graph structure advantages to achieve higher prediction performance. On average, MixQ-GNN achieved reductions in bit operations of 5.5x for node classification and 5.1x for graph classification compared to architectures represented in FP32 precision.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09361', 39)">Copy Link</button>
<div id="copy-message-39" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09366">Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks</a></h1>
<p><b>Authors:</b> SeyedMojtaba Mohasel, Alireza Afzal Aghaei, Corey Pew</p>
<p>Abstract: Objective: This paper investigates the potential of learnable activation functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a lower-limb prosthesis. In addition, user-specific vs. pooled training data is evaluated to improve machine learning (ML) and Deep Learning (DL) performance for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected from five individuals with lower-limb amputation performing turning tasks in a laboratory setting. Ability to classify an upcoming turn was evaluated for Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models) assessed the effectiveness of learnable activation functions. Models were trained separately on user-specific and pooled data to evaluate the impact of training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield significant improvement compared to MLP and CNN, respectively. Training on user-specific data yielded superior results compared to pooled data for ML models ($p < 0.05$). In contrast, no significant difference was observed between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may demonstrate distinct advantages in datasets involving more complex tasks and larger volumes. In addition, pooled training showed comparable performance to user-specific training in DL models, indicating that model training for prosthesis control can utilize data from multiple participants.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09366', 40)">Copy Link</button>
<div id="copy-message-40" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09427">SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation</a></h1>
<p><b>Authors:</b> Achref Doula, Max M\"uhlh\"auser, Alejandro Sanchez Guinea</p>
<p>Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77\% and collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven path planning more safer.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09427', 41)">Copy Link</button>
<div id="copy-message-41" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09432">Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses</a></h1>
<p><b>Authors:</b> Yuzhou Cao, Han Bao, Lei Feng, Bo An</p>
<p>Abstract: Surrogate regret bounds, also known as excess risk bounds, bridge the gap between the convergence rates of surrogate and target losses, with linear bounds favorable for their lossless regret transfer. While convex smooth surrogate losses are appealing in particular due to the efficient estimation and optimization, the existence of a trade-off between the smoothness and linear regret bound has been believed in the community. That being said, the better optimization and estimation properties of convex smooth surrogate losses may inevitably deteriorate after undergoing the regret transfer onto a target loss. We overcome this dilemma for arbitrary discrete target losses by constructing a convex smooth surrogate loss, which entails a linear surrogate regret bound composed with a tailored prediction link. The construction is based on Fenchel-Young losses generated by the convolutional negentropy, which are equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk. Consequently, the infimal convolution enables us to derive a smooth loss while maintaining the surrogate regret bound linear. We additionally benefit from the infimal convolution to have a consistent estimator of the underlying class probability. Our results are overall a novel demonstration of how convex analysis penetrates into optimization and statistical efficiency in risk minimization.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09432', 42)">Copy Link</button>
<div id="copy-message-42" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09436">CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios</a></h1>
<p><b>Authors:</b> Raghav Garg, Kapil Sharma, Karan Gupta</p>
<p>Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09436', 43)">Copy Link</button>
<div id="copy-message-43" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09458">Variational Rank Reduction Autoencoder</a></h1>
<p><b>Authors:</b> Jad Mounayer, Alicia Tierz, Jerome Tomezyk, Chady Ghnatios, Francisco Chinesta</p>
<p>Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a regularization on the latent space by applying a truncated SVD. While this regularization makes Autoencoders more powerful, using them for generative purposes is counter-intuitive due to their deterministic nature. On the other hand, Variational Autoencoders (VAEs) are well known for their generative abilities by learning a probabilistic latent space. In this paper, we present Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the advantages of both RRAEs and VAEs. Our claims and results show that when carefully sampling the latent space of RRAEs and further regularizing with the Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs and VAEs. Additionally, we show that the regularization induced by the SVD not only makes VRRAEs better generators than VAEs, but also reduces the possibility of posterior collapse. Our results include a synthetic dataset of a small size that showcases the robustness of VRRAEs against collapse, and three real-world datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to outperform both VAEs and RRAEs on many random generation and interpolation tasks based on the FID score.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09458', 44)">Copy Link</button>
<div id="copy-message-44" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09486">Preserving Plasticity in Continual Learning with Adaptive Linearity Injection</a></h1>
<p><b>Authors:</b> Seyed Roozbeh Razavi Rohani, Khashayar Khajavi, Wesley Chung, Mo Chen, Sharan Vaswani</p>
<p>Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a model's capacity to incrementally learn and has been identified as a key obstacle to learning in non-stationary problem settings. Recent work has shown that deep linear networks tend to be resilient towards loss of plasticity. Motivated by this observation, we propose Adaptive Linearization (AdaLin), a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss. Unlike prior methods that rely on regularization or periodic resets, AdaLin equips every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow. This adaptive modulation ensures sufficient gradient signal and sustains continual learning without introducing additional hyperparameters or requiring explicit task boundaries. When used with conventional activation functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in mitigating plasticity loss in off-policy reinforcement learning agents. We perform a systematic set of ablations that show that neuron-level adaptation is crucial for good performance and analyze a number of metrics in the network that might be correlated to loss of plasticity.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09486', 45)">Copy Link</button>
<div id="copy-message-45" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09500">Layered Unlearning for Adversarial Relearning</a></h1>
<p><b>Authors:</b> Timothy Qian, Vinith Suriyakumar, Ashia Wilson, Dylan Hadfield-Menell</p>
<p>Abstract: Our goal is to understand how post-training methods, such as fine-tuning, alignment, and unlearning, modify language model behavior and representations. We are particularly interested in the brittle nature of these modifications that makes them easy to bypass through prompt engineering or relearning. Recent results suggest that post-training induces shallow context-dependent ``circuits'' that suppress specific response patterns. This could be one explanation for the brittleness of post-training. To test this hypothesis, we design an unlearning algorithm, Layered Unlearning (LU), that creates distinct inhibitory mechanisms for a growing subset of the data. By unlearning the first $i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU limits the ability of relearning on a subset of data to recover the full dataset. We evaluate LU through a combination of synthetic and large language model (LLM) experiments. We find that LU improves robustness to adversarial relearning for several different unlearning methods. Our results contribute to the state-of-the-art of machine unlearning and provide insight into the effect of post-training updates.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09500', 46)">Copy Link</button>
<div id="copy-message-46" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09503">Towards Fair In-Context Learning with Tabular Foundation Models</a></h1>
<p><b>Authors:</b> Patrik Kenfack, Samira Ebrahimi Kahou, Ulrich A\"ivodji</p>
<p>Abstract: Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context. This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods. However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL. The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias. Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions. The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL.</p>
<p>URLs: <a href="https://github.com/patrikken/Fair-TabICL.">https://github.com/patrikken/Fair-TabICL.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09503, https://github.com/patrikken/Fair-TabICL.', 47)">Copy Link</button>
<div id="copy-message-47" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09572">SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures</a></h1>
<p><b>Authors:</b> Julian Kranz, Davide Gallon, Steffen Dereich, Arnulf Jentzen</p>
<p>Abstract: We study gradient flows for loss landscapes of fully connected feed forward neural networks with commonly used continuously differentiable activation functions such as the logistic, hyperbolic tangent, softplus or GELU function. We prove that the gradient flow either converges to a critical point or diverges to infinity while the loss converges to an asymptotic critical value. Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the loss value of any gradient flow initialized at most $\varepsilon$ above the optimal level converges to it. For polynomial target functions and sufficiently big architecture and data set, we prove that the optimal loss value is zero and can only be realized asymptotically. From this setting, we deduce our main result that any gradient flow with sufficiently good initialization diverges to infinity. Our proof heavily relies on the geometry of o-minimal structures. We confirm these theoretical findings with numerical experiments and extend our investigation to real-world scenarios, where we observe an analogous behavior.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09572', 48)">Copy Link</button>
<div id="copy-message-48" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09586">Rhomboid Tiling for Geometric Graph Deep Learning</a></h1>
<p><b>Authors:</b> Yipeng Zhang, Longlong Li, Kelin Xia</p>
<p>Abstract: Graph Neural Networks (GNNs) have proven effective for learning from graph-structured data through their neighborhood-based message passing framework. Many hierarchical graph clustering pooling methods modify this framework by introducing clustering-based strategies, enabling the construction of more expressive and powerful models. However, all of these message passing framework heavily rely on the connectivity structure of graphs, limiting their ability to capture the rich geometric features inherent in geometric graphs. To address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering method based on the rhomboid tiling structure, which performs clustering by leveraging the complex geometric information of the data and effectively extracts its higher-order geometric structures. Moreover, we design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks. The proposed model demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09586', 49)">Copy Link</button>
<div id="copy-message-49" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09593">Online Isolation Forest</a></h1>
<p><b>Authors:</b> Filippo Leveni, Guilherme Weigert Cassales, Bernhard Pfahringer, Albert Bifet, Giacomo Boracchi</p>
<p>Abstract: The anomaly detection literature is abundant with offline methods, which require repeated access to data in memory, and impose impractical assumptions when applied to a streaming context. Existing online anomaly detection methods also generally fail to address these constraints, resorting to periodic retraining to adapt to the online context. We propose Online-iForest, a novel method explicitly designed for streaming conditions that seamlessly tracks the data generating process as it evolves over time. Experimental validation on real-world datasets demonstrated that Online-iForest is on par with online alternatives and closely rivals state-of-the-art offline anomaly detection techniques that undergo periodic retraining. Notably, Online-iForest consistently outperforms all competitors in terms of efficiency, making it a promising solution in applications where fast identification of anomalies is of primary importance such as cybersecurity, fraud and fault detection.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09593', 50)">Copy Link</button>
<div id="copy-message-50" class="copy-message"></div>
</div>
<div class="article">
<h1> new <a href="https://arxiv.org/abs/2505.09602">Adversarial Suffix Filtering: a Defense Pipeline for LLMs</a></h1>
<p><b>Authors:</b> David Khachaturov, Robert Mullins</p>
<p>Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems and public-facing environments, yet they remain susceptible to jailbreak vulnerabilities that may undermine their security and trustworthiness. Adversarial suffixes are considered to be the current state-of-the-art jailbreak, consistently outperforming simpler methods and frequently succeeding even in black-box settings. Existing defenses rely on access to the internal architecture of models limiting diverse deployment, increase memory and computation footprints dramatically, or can be bypassed with simple prompt engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$ (ASF), a lightweight novel model-agnostic defensive pipeline designed to protect LLMs against adversarial suffix attacks. ASF functions as an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes in prompts, effectively neutralizing malicious injections. We demonstrate that ASF provides comprehensive defense capabilities across both black-box and white-box attack settings, reducing the attack efficacy of state-of-the-art adversarial suffix generation methods to below 4%, while only minimally affecting the target model's capabilities in non-adversarial scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09602', 51)">Copy Link</button>
<div id="copy-message-51" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.07363">Equilibrium Propagation for Learning in Lagrangian Dynamical Systems</a></h1>
<p><b>Authors:</b> Serge Massar</p>
<p>Abstract: We propose a method for training dynamical systems governed by Lagrangian mechanics using Equilibrium Propagation. Our approach extends Equilibrium Propagation -- initially developed for energy-based models -- to dynamical trajectories by leveraging the principle of action extremization. Training is achieved by gently nudging trajectories toward desired targets and measuring how the variables conjugate to the parameters to be trained respond. This method is particularly suited to systems with periodic boundary conditions or fixed initial and final states, enabling efficient parameter updates without requiring explicit backpropagation through time. In the case of periodic boundary conditions, this approach yields the semiclassical limit of Quantum Equilibrium Propagation. Applications to systems with dissipation are also discussed.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.07363', 52)">Copy Link</button>
<div id="copy-message-52" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08801">OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions</a></h1>
<p><b>Authors:</b> Md. Sakib Hassan Chowdhury, Md. Hafiz Ahamed, Bishowjit Paul, Sarafat Hussain Abhi, Abu Bakar Siddique, Md. Robius Sany</p>
<p>Abstract: Gait recognition, known for its ability to identify individuals from a distance, has gained significant attention in recent times due to its non-intrusive verification. While video-based gait identification systems perform well on large public datasets, their performance drops when applied to real-world, unconstrained gait data due to various factors. Among these, uncontrolled outdoor environments, non-overlapping camera views, varying illumination, and computational efficiency are core challenges in gait-based authentication. Currently, no dataset addresses all these challenges simultaneously. In this paper, we propose an OptiGait-LGBM model capable of recognizing person re-identification under these constraints using a skeletal model approach, which helps mitigate inconsistencies in a person's appearance. The model constructs a dataset from landmark positions, minimizing memory usage by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to represent uncontrolled gait sequences in complex outdoor environments. The process involves extracting skeletal joint landmarks, generating numerical datasets, and developing an OptiGait-LGBM gait classification model. Our aim is to address the aforementioned challenges with minimal computational cost compared to existing methods. A comparative analysis with ensemble techniques such as Random Forest and CatBoost demonstrates that the proposed approach outperforms them in terms of accuracy, memory usage, and training time. This method provides a novel, low-cost, and memory-efficient video-based gait recognition solution for real-world scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08801', 53)">Copy Link</button>
<div id="copy-message-53" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08804">TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis</a></h1>
<p><b>Authors:</b> Longtian Wang, Xiaofei Xie, Tianlin Li, Yuhan Zhi, Chao Shen</p>
<p>Abstract: Text-to-image (T2I) models have significantly advanced in producing high-quality images. However, such models have the ability to generate images containing not-safe-for-work (NSFW) content, such as pornography, violence, political content, and discrimination. To mitigate the risk of generating NSFW content, refusal mechanisms, i.e., safety checkers, have been developed to check potential NSFW content. Adversarial prompting techniques have been developed to evaluate the robustness of the refusal mechanisms. The key challenge remains to subtly modify the prompt in a way that preserves its sensitive nature while bypassing the refusal mechanisms. In this paper, we introduce TokenProber, a method designed for sensitivity-aware differential testing, aimed at evaluating the robustness of the refusal mechanisms in T2I models by generating adversarial prompts. Our approach is based on the key observation that adversarial prompts often succeed by exploiting discrepancies in how T2I models and safety checkers interpret sensitive content. Thus, we conduct a fine-grained analysis of the impact of specific words within prompts, distinguishing between dirty words that are essential for NSFW content generation and discrepant words that highlight the different sensitivity assessments between T2I models and safety checkers. Through the sensitivity-aware mutation, TokenProber generates adversarial prompts, striking a balance between maintaining NSFW content generation and evading detection. Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I models, using 324 NSFW prompts, demonstrates its superior effectiveness in bypassing safety filters compared to existing methods (e.g., 54%+ increase on average), highlighting TokenProber's ability to uncover robustness issues in the existing refusal mechanisms.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08804', 54)">Copy Link</button>
<div id="copy-message-54" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08814">Towards Understanding Deep Learning Model in Image Recognition via Coverage Test</a></h1>
<p><b>Authors:</b> Wenkai Li, Xiaoqi Li, Yingjie Mao, Yishun Wang</p>
<p>Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial intelligence, and their security-related testing has been a prominent research focus. By inputting test cases, the behavior of models is examined for anomalies, and coverage metrics are utilized to determine the extent of neurons covered by these test cases. With the widespread application and advancement of DNNs, different types of neural behaviors have garnered attention, leading to the emergence of various coverage metrics for neural networks. However, there is currently a lack of empirical research on these coverage metrics, specifically in analyzing the relationships and patterns between model depth, configuration information, and neural network coverage. This paper aims to investigate the relationships and patterns of four coverage metrics: primary functionality, boundary, hierarchy, and structural coverage. A series of empirical experiments were conducted, selecting LeNet, VGG, and ResNet as different DNN architectures, along with 10 models of varying depths ranging from 5 to 54 layers, to compare and study the relationships between different depths, configuration information, and various neural network coverage metrics. Additionally, an investigation was carried out on the relationships between modified decision/condition coverage and dataset size. Finally, three potential future directions are proposed to further contribute to the security testing of DNN Models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08814', 55)">Copy Link</button>
<div id="copy-message-55" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08816">Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems</a></h1>
<p><b>Authors:</b> Ippokratis Koukoulis, Ilias Syrigos, Thanasis Korakis</p>
<p>Abstract: As the digital landscape becomes more interconnected, the frequency and severity of zero-day attacks, have significantly increased, leading to an urgent need for innovative Intrusion Detection Systems (IDS). Machine Learning-based IDS that learn from the network traffic characteristics and can discern attack patterns from benign traffic offer an advanced solution to traditional signature-based IDS. However, they heavily rely on labeled datasets, and their ability to generalize when encountering unseen traffic patterns remains a challenge. This paper proposes a novel self-supervised contrastive learning approach based on transformer encoders, specifically tailored for generalizable intrusion detection on raw packet sequences. Our proposed learning scheme employs a packet-level data augmentation strategy combined with a transformer-based architecture to extract and generate meaningful representations of traffic flows. Unlike traditional methods reliant on handcrafted statistical features (NetFlow), our approach automatically learns comprehensive packet sequence representations, significantly enhancing performance in anomaly identification tasks and supervised learning for intrusion detection. Our transformer-based framework exhibits better performance in comparison to existing NetFlow self-supervised methods. Specifically, we achieve up to a 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. Moreover, our model provides a strong baseline for supervised intrusion detection with limited labeled data, exhibiting an improvement over self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated on the same dataset. Additionally, we show the adaptability of our pretrained model when fine-tuned across different datasets, demonstrating strong performance even when lacking benign data from the target domain.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08816', 56)">Copy Link</button>
<div id="copy-message-56" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08817">Towards SFW sampling for diffusion models via external conditioning</a></h1>
<p><b>Authors:</b> Camilo Carvajal Reyes, Joaqu\'in Fontbona, Felipe Tobar</p>
<p>Abstract: Score-based generative models (SBM), also known as diffusion models, are the de facto state of the art for image synthesis. Despite their unparalleled performance, SBMs have recently been in the spotlight for being tricked into creating not-safe-for-work (NSFW) content, such as violent images and non-consensual nudity. Current approaches that prevent unsafe generation are based on the models' own knowledge, and the majority of them require fine-tuning. This article explores the use of external sources for ensuring safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional Trajectory Correction step that guides the samples away from undesired regions in the ambient space using multimodal models as the source of conditioning. Furthermore, using Contrastive Language Image Pre-training (CLIP), our method admits user-defined NSFW classes, which can vary in different settings. Our experiments on the text-to-image SBM Stable Diffusion validate that the proposed SFW sampler effectively reduces the generation of explicit content while being competitive with other fine-tuning-based approaches, as assessed via independent NSFW detectors. Moreover, we evaluate the impact of the SFW sampler on image quality and show that the proposed correction scheme comes at a minor cost with negligible effect on samples not needing correction. Our study confirms the suitability of the SFW sampler towards aligned SBM models and the potential of using model-agnostic conditioning for the prevention of unwanted images.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08817', 57)">Copy Link</button>
<div id="copy-message-57" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08818">Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare</a></h1>
<p><b>Authors:</b> Amara Tariq, Rimita Lahiri, Charles Kahn, Imon Banerjee</p>
<p>Abstract: The intricate and multifaceted nature of vision language model (VLM) development, adaptation, and application necessitates the establishment of clear and standardized reporting protocols, particularly within the high-stakes context of healthcare. Defining these reporting standards is inherently challenging due to the diverse nature of studies involving VLMs, which vary significantly from the development of all new VLMs or finetuning for domain alignment to off-the-shelf use of VLM for targeted diagnosis and prediction tasks. In this position paper, we argue that traditional machine learning reporting standards and evaluation guidelines must be restructured to accommodate multiphase VLM studies; it also has to be organized for intuitive understanding of developers while maintaining rigorous standards for reproducibility. To facilitate community adoption, we propose a categorization framework for VLM studies and outline corresponding reporting standards that comprehensively address performance evaluation, data reporting protocols, and recommendations for manuscript composition. These guidelines are organized according to the proposed categorization scheme. Lastly, we present a checklist that consolidates reporting standards, offering a standardized tool to ensure consistency and quality in the publication of VLM-related research.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08818', 58)">Copy Link</button>
<div id="copy-message-58" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08819">Thoughts on Objectives of Sparse and Hierarchical Masked Image Model</a></h1>
<p><b>Authors:</b> Asahi Miyazaki, Tsuyoshi Okita</p>
<p>Abstract: Masked image modeling is one of the most poplular objectives of training. Recently, the SparK model has been proposed with superior performance among self-supervised learning models. This paper proposes a new mask pattern for this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the effect of the mask pattern used for image masking in pre-training on performance.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08819', 59)">Copy Link</button>
<div id="copy-message-59" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08822">The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics</a></h1>
<p><b>Authors:</b> Yuhao Wang (Jack), Kailai Wang (Jack), Songhua Hu (Jack),  Yunpeng (Jack),  Zhang, Gino Lim, Pengyu Zhu</p>
<p>Abstract: The rapid evolution of the transportation cybersecurity ecosystem, encompassing cybersecurity, automotive, and transportation and logistics sectors, will lead to the formation of distinct spatial clusters and visitor flow patterns across the US. This study examines the spatiotemporal dynamics of visitor flows, analyzing how socioeconomic factors shape industry clustering and workforce distribution within these evolving sectors. To model and predict visitor flow patterns, we develop a BiTransGCN framework, integrating an attention-based Transformer architecture with a Graph Convolutional Network backbone. By integrating AI-enabled forecasting techniques with spatial analysis, this study improves our ability to track, interpret, and anticipate changes in industry clustering and mobility trends, thereby supporting strategic planning for a secure and resilient transportation network. It offers a data-driven foundation for economic planning, workforce development, and targeted investments in the transportation cybersecurity ecosystem.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08822', 60)">Copy Link</button>
<div id="copy-message-60" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08833">Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models</a></h1>
<p><b>Authors:</b> Qingyi Wang, Yuebing Liang, Yunhan Zheng, Kaiyuan Xu, Jinhua Zhao, Shenhao Wang</p>
<p>Abstract: Generative AI offers new opportunities for automating urban planning by creating site-specific urban layouts and enabling flexible design exploration. However, existing approaches often struggle to produce realistic and practical designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion model, extended with ControlNet, to generate high-fidelity satellite imagery conditioned on land use descriptions, infrastructure, and natural environments. To overcome data availability limitations, we spatially link satellite imagery with structured land use and constraint information from OpenStreetMap. Using data from three major U.S. cities, we demonstrate that the proposed diffusion model generates realistic and diverse urban landscapes by varying land-use configurations, road networks, and water bodies, facilitating cross-city learning and design diversity. We also systematically evaluate the impacts of varying language prompts and control imagery on the quality of satellite imagery generation. Our model achieves high FID and KID scores and demonstrates robustness across diverse urban contexts. Qualitative assessments from urban planners and the general public show that generated images align closely with design descriptions and constraints, and are often preferred over real images. This work establishes a benchmark for controlled urban imagery generation and highlights the potential of generative AI as a tool for enhancing planning workflows and public engagement.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08833', 61)">Copy Link</button>
<div id="copy-message-61" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08837">Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning</a></h1>
<p><b>Authors:</b> Muhammad Saqib, Dipkumar Mehta, Fnu Yashu, Shubham Malhotra</p>
<p>Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is complex and dynamic. Static security policies have become inadequate as threats evolve and cloud resources exhibit elasticity [1]. This paper addresses the limitations of static policies by proposing a security policy management framework that uses reinforcement learning (RL) to adapt dynamically. Specifically, we employ deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, enabling the learning and continuous adjustment of controls such as firewall rules and Identity and Access Management (IAM) policies. The proposed RL based solution leverages cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies, maximizing threat mitigation, and compliance while minimizing resource impact. Experimental results demonstrate that our adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and substantially reducing incident detection and response times by 58%. In addition, it maintains high conformity with security requirements and efficient resource usage. These findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08837', 62)">Copy Link</button>
<div id="copy-message-62" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08845">Validation of Conformal Prediction in Cervical Atypia Classification</a></h1>
<p><b>Authors:</b> Misgina Tsighe Hagos, Antti Suutala, Dmitrii Bychkov, Hakan K\"uc\"ukel, Joar von Bahr, Milda Poceviciute, Johan Lundin, Nina Linder, Claes Lundstr\"om</p>
<p>Abstract: Deep learning based cervical cancer classification can potentially increase access to screening in low-resource regions. However, deep learning models are often overconfident and do not reliably reflect diagnostic uncertainty. Moreover, they are typically optimized to generate maximum-likelihood predictions, which fail to convey uncertainty or ambiguity in their results. Such challenges can be addressed using conformal prediction, a model-agnostic framework for generating prediction sets that contain likely classes for trained deep-learning models. The size of these prediction sets indicates model uncertainty, contracting as model confidence increases. However, existing conformal prediction evaluation primarily focuses on whether the prediction set includes or covers the true class, often overlooking the presence of extraneous classes. We argue that prediction sets should be truthful and valuable to end users, ensuring that the listed likely classes align with human expectations rather than being overly relaxed and including false positives or unlikely classes. In this study, we comprehensively validate conformal prediction sets using expert annotation sets collected from multiple annotators. We evaluate three conformal prediction approaches applied to three deep-learning models trained for cervical atypia classification. Our expert annotation-based analysis reveals that conventional coverage-based evaluations overestimate performance and that current conformal prediction methods often produce prediction sets that are not well aligned with human labels. Additionally, we explore the capabilities of the conformal prediction methods in identifying ambiguous and out-of-distribution data.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08845', 63)">Copy Link</button>
<div id="copy-message-63" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08849">Improved Algorithms for Differentially Private Language Model Alignment</a></h1>
<p><b>Authors:</b> Keyu Chen, Hao Tang, Qinglin Liu, Yizhao Xu</p>
<p>Abstract: Language model alignment is crucial for ensuring that large language models (LLMs) align with human preferences, yet it often involves sensitive user data, raising significant privacy concerns. While prior work has integrated differential privacy (DP) with alignment techniques, their performance remains limited. In this paper, we propose novel algorithms for privacy-preserving alignment and rigorously analyze their effectiveness across varying privacy budgets and models. Our framework can be deployed on two celebrated alignment techniques, namely direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF). Through systematic experiments on large-scale language models, we demonstrate that our approach achieves state-of-the-art performance. Notably, one of our algorithms, DP-AdamW, combined with DPO, surpasses existing methods, improving alignment quality by up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further investigate the interplay between privacy guarantees, alignment efficacy, and computational demands, providing practical guidelines for optimizing these trade-offs.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08849', 64)">Copy Link</button>
<div id="copy-message-64" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08886">Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images</a></h1>
<p><b>Authors:</b> Hamideh Khaleghpour, Brett McKinney</p>
<p>Abstract: The rising incidence of skin cancer, coupled with limited public awareness and a shortfall in clinical expertise, underscores an urgent need for advanced diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool in this domain, particularly for distinguishing malignant from benign skin lesions. Leveraging publicly available datasets of skin lesions, researchers have been developing AI-based diagnostic solutions. However, the integration of such computer systems in clinical settings is still nascent. This study aims to bridge this gap by employing a fusion of image processing techniques and machine learning algorithms, specifically neuro-fuzzy and colonial competition approaches. Applied to dermoscopic images from the ISIC database, our method achieved a notable accuracy of 94% on a dataset of 560 images. These results underscore the potential of our approach in aiding clinicians in the early detection of melanoma, thereby contributing significantly to skin cancer diagnostics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08886', 65)">Copy Link</button>
<div id="copy-message-65" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08899">Bounding Neyman-Pearson Region with $f$-Divergences</a></h1>
<p><b>Authors:</b> Andrew Mullhaupt, Cheng Peng</p>
<p>Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of points whose coordinates represent the false positive rate and false negative rate of some test. The lower boundary of this region is given by the Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the optimal ROC curve. We establish a novel lower bound for the boundary in terms of any $f$-divergence. Since the bound generated by hockey-stick $f$-divergences characterizes the Neyman-Pearson boundary, this bound is best possible. In the case of KL divergence, this bound improves Pinsker's inequality. Furthermore, we obtain a closed-form refined upper bound for the Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally, we present methods for constructing pairs of distributions that can approximately or exactly realize any given Neyman-Pearson boundary.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08899', 66)">Copy Link</button>
<div id="copy-message-66" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08908">Statistical Decision Theory with Counterfactual Loss</a></h1>
<p><b>Authors:</b> Benedikt Koch, Kosuke Imai</p>
<p>Abstract: Classical statistical decision theory evaluates treatment choices based solely on observed outcomes. However, by ignoring counterfactual outcomes, it cannot assess the quality of decisions relative to feasible alternatives. For example, the quality of a physician's decision may depend not only on patient survival, but also on whether a less invasive treatment could have produced a similar result. To address this limitation, we extend standard decision theory to incorporate counterfactual losses--criteria that evaluate decisions using all potential outcomes. The central challenge in this generalization is identification: because only one potential outcome is observed for each unit, the associated risk under a counterfactual loss is generally not identifiable. We show that under the assumption of strong ignorability, a counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes. Moreover, we demonstrate that additive counterfactual losses can yield treatment recommendations that differ from those based on standard loss functions, provided that the decision problem involves more than two treatment options.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08908', 67)">Copy Link</button>
<div id="copy-message-67" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08909">Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems</a></h1>
<p><b>Authors:</b> Deliang Wei, Peng Chen, Haobo Xu, Jiale Yao, Fang Li, Tieyong Zeng</p>
<p>Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results in imaging problems. They typically require strong convexity or smoothness of the fidelity term and a (residual) non-expansive denoiser for convergence. These assumptions, however, are violated in Poisson inverse problems, and non-expansiveness can hinder denoising performance. To address these challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be (residual) expansive, leading to improved denoising. By leveraging the generalized Helmholtz decomposition, we introduce a novel training strategy that combines Hamiltonian regularization to promote conservativeness and spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser is a proximal operator of a weakly convex function, enabling a restoration model with an implicit weakly convex prior. The global convergence of PnP methods to a stationary point of this restoration model is established. Extensive experimental results demonstrate that our approach outperforms closely related methods in both visual quality and quantitative metrics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08909', 68)">Copy Link</button>
<div id="copy-message-68" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08961">Differentiable Channel Selection in Self-Attention For Person Re-Identification</a></h1>
<p><b>Authors:</b> Yancheng Wang, Nebojsa Jojic, Yingzhen Yang</p>
<p>Abstract: In this paper, we propose a novel attention module termed the Differentiable Channel Selection Attention module, or the DCS-Attention module. In contrast with conventional self-attention, the DCS-Attention module features selection of informative channels in the computation of the attention weights. The selection of the feature channels is performed in a differentiable manner, enabling seamless integration with DNN training. Our DCS-Attention is compatible with either fixed neural network backbones or learnable backbones with Differentiable Neural Architecture Search (DNAS), leading to DCS with Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our DCS-Attention is motivated by the principle of Information Bottleneck (IB), and a novel variational upper bound for the IB loss, which can be optimized by SGD, is derived and incorporated into the training loss of the networks with the DCS-Attention modules. In this manner, a neural network with DCS-Attention modules is capable of selecting the most informative channels for feature extraction so that it enjoys state-of-the-art performance for the Re-ID task. Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention in learning discriminative features critical to identifying person identities. The code of our work is available at https://github.com/Statistical-Deep-Learning/DCS-Attention.</p>
<p>URLs: <a href="https://github.com/Statistical-Deep-Learning/DCS-Attention.">https://github.com/Statistical-Deep-Learning/DCS-Attention.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08961, https://github.com/Statistical-Deep-Learning/DCS-Attention.', 69)">Copy Link</button>
<div id="copy-message-69" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08971">Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training</a></h1>
<p><b>Authors:</b> Yangyi Chen, Hao Peng, Tong Zhang, Heng Ji</p>
<p>Abstract: In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08971', 70)">Copy Link</button>
<div id="copy-message-70" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08986">ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation</a></h1>
<p><b>Authors:</b> Amirreza Davar, Zhengtong Xu, Siavash Mahmoudi, Pouya Sohrabipour, Chaitanya Pallerla, Yu She, Wan Shou, Philip Crandall, Dongyi Wang</p>
<p>Abstract: Automated poultry processing lines still rely on humans to lift slippery, easily bruised carcasses onto a shackle conveyor. Deformability, anatomical variance, and strict hygiene rules make conventional suction and scripted motions unreliable. We present ChicGrasp, an end--to--end hardware--software co-design for this task. An independently actuated dual-jaw pneumatic gripper clamps both chicken legs, while a conditional diffusion-policy controller, trained from only 50 multi--view teleoperation demonstrations (RGB + proprioception), plans 5 DoF end--effector motion, which includes jaw commands in one shot. On individually presented raw broiler carcasses, our system achieves a 40.6\% grasp--and--lift success rate and completes the pick to shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning (IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be open-source. ChicGrasp shows that imitation learning can bridge the gap between rigid hardware and variable bio--products, offering a reproducible benchmark and a public dataset for researchers in agricultural engineering and robot learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08986', 71)">Copy Link</button>
<div id="copy-message-71" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.08995">Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning</a></h1>
<p><b>Authors:</b> Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael R\"uegsegger</p>
<p>Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning framework for analyzing simulated air combat scenarios involving heterogeneous agents. The objective is to identify effective Courses of Action that lead to mission success within preset simulations, thereby enabling the exploration of real-world defense scenarios at low cost and in a safe-to-fail setting. Applying deep Reinforcement Learning in this context poses specific challenges, such as complex flight dynamics, the exponential size of the state and action spaces in multi-agent systems, and the capability to integrate real-time control of individual units with look-ahead planning. To address these challenges, the decision-making process is split into two levels of abstraction: low-level policies control individual units, while a high-level commander policy issues macro commands aligned with the overall mission targets. This hierarchical structure facilitates the training process by exploiting policy symmetries of individual agents and by separating control from command tasks. The low-level policies are trained for individual combat control in a curriculum of increasing complexity. The high-level commander is then trained on mission targets given pre-trained control policies. The empirical validation confirms the advantages of the proposed framework.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08995', 72)">Copy Link</button>
<div id="copy-message-72" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09004">Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features</a></h1>
<p><b>Authors:</b> Monica Welfert, Nathan Stromberg, Mario Diaz, Lalitha Sankar</p>
<p>Abstract: We propose an adversarial evaluation framework for sensitive feature inference based on minimum mean-squared error (MMSE) estimation with a finite sample size and linear predictive models. Our approach establishes theoretical lower bounds on the true MMSE of inferring sensitive features from noisy observations of other correlated features. These bounds are expressed in terms of the empirical MMSE under a restricted hypothesis class and a non-negative error term. The error term captures both the estimation error due to finite number of samples and the approximation error from using a restricted hypothesis class. For linear predictive models, we derive closed-form bounds, which are order optimal in terms of the noise variance, on the approximation error for several classes of relationships between the sensitive and non-sensitive features, including linear mappings, binary symmetric channels, and class-conditional multi-variate Gaussian distributions. We also present a new lower bound that relies on the MSE computed on a hold-out validation dataset of the MMSE estimator learned on finite-samples and a restricted hypothesis class. Through empirical evaluation, we demonstrate that our framework serves as an effective tool for MMSE-based adversarial evaluation of sensitive feature inference that balances theoretical guarantees with practical efficiency.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09004', 73)">Copy Link</button>
<div id="copy-message-73" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09018">Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction</a></h1>
<p><b>Authors:</b> Adarsh Kumar</p>
<p>Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet accurately estimating caloric intake remains a major challenge. While continuous glucose monitors (CGMs) offer valuable physiological data, they often fall short in capturing the full nutritional profile of meals due to inter-individual and meal-specific variability. In this work, we introduce a multimodal deep learning framework that jointly leverages CGM time-series data, Demographic/Microbiome, and pre-meal food images to enhance caloric estimation. Our model utilizes attention based encoding and a convolutional feature extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome data followed by a late fusion strategy for joint reasoning. We evaluate our approach on a curated dataset of over 40 participants, incorporating synchronized CGM, Demographic and Microbiome data and meal photographs with standardized caloric labels. Our model achieves a Root Mean Squared Relative Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These findings demonstrate the potential of multimodal sensing to improve automated dietary assessment tools for chronic disease management.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09018', 74)">Copy Link</button>
<div id="copy-message-74" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09024">Automated Meta Prompt Engineering for Alignment with the Theory of Mind</a></h1>
<p><b>Authors:</b> Aaron Baughman, Rahul Agarwal, Eduardo Morales, Gozde Akay</p>
<p>Abstract: We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09024', 75)">Copy Link</button>
<div id="copy-message-75" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09026">Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes</a></h1>
<p><b>Authors:</b> Domniki Ladopoulou, Dat Minh Hong, Petros Dellaportas</p>
<p>Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining grid stability and enabling efficient integration of renewable energy sources. Gaussian Process (GP) models offer a principled framework for quantifying uncertainty; however, conventional approaches rely on stationary kernels, which are inadequate for modeling the inherently non-stationary nature of wind speed and power output. We propose a non-stationary GP framework that incorporates the generalized spectral mixture (GSM) kernel, enabling the model to capture time-varying patterns and heteroscedastic behaviors in wind speed and wind power data. We evaluate the performance of the proposed model on real-world SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons. Compared to standard radial basis function and spectral mixture kernels, the GSM-based model outperforms, particularly in short-term forecasts. These results highlight the necessity of modeling non-stationarity in wind power forecasting and demonstrate the practical value of non-stationary GP models in operational settings.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09026', 76)">Copy Link</button>
<div id="copy-message-76" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09029">Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control</a></h1>
<p><b>Authors:</b> Hazim Alzorgan, Abolfazl Razi</p>
<p>Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient (TD3), depend on basic noise-based exploration, which can result in less than optimal policy convergence. In this study, we introduce Monte Carlo Beam Search (MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts with TD3 to improve exploration and action selection. MCBS produces several candidate actions around the policy's output and assesses them through short-horizon rollouts, enabling the agent to make better-informed choices. We test MCBS across various continuous-control benchmarks, including HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency and performance compared to standard TD3 and other baseline methods like SAC, PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy learning through structured look-ahead search while ensuring computational efficiency. Additionally, we offer a detailed analysis of crucial hyperparameters, such as beam width and rollout depth, and explore adaptive strategies to optimize MCBS for complex control tasks. Our method shows a higher convergence rate across different environments compared to TD3, SAC, PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward within around 200 thousand timesteps compared to 400 thousand timesteps for the second-best method.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09029', 77)">Copy Link</button>
<div id="copy-message-77" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09040">RT-cache: Efficient Robot Trajectory Retrieval System</a></h1>
<p><b>Authors:</b> Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani</p>
<p>Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that accelerates real-world robot inference by leveraging big-data retrieval and learning from experience. While modern Vision-Language-Action (VLA) models can handle diverse robotic tasks, they often incur high per-step inference costs, resulting in significant latency, sometimes minutes per task. In contrast, RT-cache stores a large-scale Memory of previously successful robot trajectories and retrieves relevant multistep motion snippets, drastically reducing inference overhead. By integrating a Memory Builder with a Trajectory Retrieval, we develop an efficient retrieval process that remains tractable even for extremely large datasets. RT-cache flexibly accumulates real-world experiences and replays them whenever the current scene matches past states, adapting quickly to new or unseen environments with only a few additional samples. Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks both faster and more successfully than a baseline lacking retrieval, suggesting a practical, data-driven solution for real-time manipulation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09040', 78)">Copy Link</button>
<div id="copy-message-78" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09062">Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models</a></h1>
<p><b>Authors:</b> Junda Zhao, Yuliang Song, Eldan Cohen</p>
<p>Abstract: Recent advancements in source code summarization have leveraged transformer-based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing methods often focus on generating a single high-quality summary for a given source code, neglecting scenarios where the generated summary might be inadequate and alternative options are needed. In this paper, we introduce Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, allowing the user to choose the most suitable one for the given source code. Our method integrates a Conditional Variational Autoencoder (CVAE) framework as a modular component into pre-trained models, enabling us to model the distribution of observed target summaries and sample continuous embeddings to be used as prefixes to steer the generation of diverse outputs during decoding. Importantly, we construct our method in a parameter-efficient manner, eliminating the need for expensive model retraining, especially when using LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset of generated summaries, optimizing both the diversity and the accuracy of the options presented to users. We present extensive experimental evaluations using widely used datasets and current state-of-the-art pre-trained code summarization models to demonstrate the effectiveness of our approach and its adaptability across models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09062', 79)">Copy Link</button>
<div id="copy-message-79" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09075">Risk Bounds For Distributional Regression</a></h1>
<p><b>Authors:</b> Carlos Misael Madrid Padilla, Oscar Hernan Madrid Padilla, Sabyasachi Chatterjee</p>
<p>Abstract: This work examines risk bounds for nonparametric distributional regression estimators. For convex-constrained distributional regression, general upper bounds are established for the continuous ranked probability score (CRPS) and the worst-case mean squared error (MSE) across the domain. These theoretical results are applied to isotonic and trend filtering distributional regression, yielding convergence rates consistent with those for mean estimation. Furthermore, a general upper bound is derived for distributional regression under non-convex constraints, with a specific application to neural network-based estimators. Comprehensive experiments on both simulated and real data validate the theoretical contributions, demonstrating their practical effectiveness.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09075', 80)">Copy Link</button>
<div id="copy-message-80" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09087">A Comparative Review of RNA Language Models</a></h1>
<p><b>Authors:</b> He Wang, Yikun Zhang, Jie Chen, Jian Zhan, Yaoqi Zhou</p>
<p>Abstract: Given usefulness of protein language models (LMs) in structure and functional inference, RNA LMs have received increased attentions in the last few years. However, these RNA models are often not compared against the same standard. Here, we divided RNA LMs into three classes (pretrained on multiple RNA types (especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein LMs as controls in zero-shot prediction of RNA secondary structure and functional classification. Results shows that the models doing well on secondary structure prediction often perform worse in function classification or vice versa, suggesting that more balanced unsupervised training is needed.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09087', 81)">Copy Link</button>
<div id="copy-message-81" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09091">DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis</a></h1>
<p><b>Authors:</b> Zeeshan Ahmad, Shudi Bao, Meng Chen</p>
<p>Abstract: In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences. However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation. To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation. This innovative approach enhances the model's ability to capture and reproduce intricate audio patterns. In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio. Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality. We trained two versions of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M parameters). For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability. Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09091', 82)">Copy Link</button>
<div id="copy-message-82" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09098">Statistical Mean Estimation with Coded Relayed Observations</a></h1>
<p><b>Authors:</b> Yan Hao Ling, Zhouhao Yang, Jonathan Scarlett</p>
<p>Abstract: We consider a problem of statistical mean estimation in which the samples are not observed directly, but are instead observed by a relay (``teacher'') that transmits information through a memoryless channel to the decoder (``student''), who then produces the final estimate. We consider the minimax estimation error in the large deviations regime, and establish achievable error exponents that are tight in broad regimes of the estimation accuracy and channel quality. In contrast, two natural baseline methods are shown to yield strictly suboptimal error exponents. We initially focus on Bernoulli sources and binary symmetric channels, and then generalize to sub-Gaussian and heavy-tailed settings along with arbitrary discrete memoryless channels.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09098', 83)">Copy Link</button>
<div id="copy-message-83" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09099">Imitation Learning for Adaptive Control of a Virtual Soft Exoglove</a></h1>
<p><b>Authors:</b> Shirui Lyu, Vittorio Caggiano, Matteo Leonetti, Dario Farina, Letizia Gionfrida</p>
<p>Abstract: The use of wearable robots has been widely adopted in rehabilitation training for patients with hand motor impairments. However, the uniqueness of patients' muscle loss is often overlooked. Leveraging reinforcement learning and a biologically accurate musculoskeletal model in simulation, we propose a customized wearable robotic controller that is able to address specific muscle deficits and to provide compensation for hand-object manipulation tasks. Video data of a same subject performing human grasping tasks is used to train a manipulation model through learning from demonstration. This manipulation model is subsequently fine-tuned to perform object-specific interaction tasks. The muscle forces in the musculoskeletal manipulation model are then weakened to simulate neurological motor impairments, which are later compensated by the actuation of a virtual wearable robotics glove. Results shows that integrating the virtual wearable robotic glove provides shared assistance to support the hand manipulator with weakened muscle forces. The learned exoglove controller achieved an average of 90.5\% of the original manipulation proficiency.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09099', 84)">Copy Link</button>
<div id="copy-message-84" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09110">Toward Malicious Clients Detection in Federated Learning</a></h1>
<p><b>Authors:</b> Zhihao Dou, Jiaqi Wang, Wei Sun, Zhuqing Liu, Minghong Fang</p>
<p>Abstract: Federated learning (FL) enables multiple clients to collaboratively train a global machine learning model without sharing their raw data. However, the decentralized nature of FL introduces vulnerabilities, particularly to poisoning attacks, where malicious clients manipulate their local models to disrupt the training process. While Byzantine-robust aggregation rules have been developed to mitigate such attacks, they remain inadequate against more advanced threats. In response, recent advancements have focused on FL detection techniques to identify potentially malicious participants. Unfortunately, these methods often misclassify numerous benign clients as threats or rely on unrealistic assumptions about the server's capabilities. In this paper, we propose a novel algorithm, SafeFL, specifically designed to accurately identify malicious clients in FL. The SafeFL approach involves the server collecting a series of global models to generate a synthetic dataset, which is then used to distinguish between malicious and benign models based on their behavior. Extensive testing demonstrates that SafeFL outperforms existing methods, offering superior efficiency and accuracy in detecting malicious clients.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09110', 85)">Copy Link</button>
<div id="copy-message-85" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09114">Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer</a></h1>
<p><b>Authors:</b> Minh Hoang Nguyen, Linh Le Pham Van, Thommen George Karimpanal, Sunil Gupta, Hung Le</p>
<p>Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains. However, DT requires high-quality, comprehensive data to perform optimally. In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance. To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning. CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios. Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches. Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications. These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agents' performance and generalization capabilities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09114', 86)">Copy Link</button>
<div id="copy-message-86" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09142">ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor</a></h1>
<p><b>Authors:</b> Seungbeom Choi, Jeonghoe Goo, Eunjoo Jeon, Mingyu Yang, Minsung Jang</p>
<p>Abstract: We propose ELIS, a serving system for Large Language Models (LLMs) featuring an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to efficiently manage inference tasks with the shortest remaining tokens. Current LLM serving systems often employ a first-come-first-served scheduling strategy, which can lead to the "head-of-line blocking" problem. To overcome this limitation, it is necessary to predict LLM inference times and apply a shortest job first scheduling strategy. However, due to the auto-regressive nature of LLMs, predicting the inference latency is challenging. ELIS addresses this challenge by training a response length predictor for LLMs using the BGE model, an encoder-based state-of-the-art model. Additionally, we have devised the ISRTF scheduling strategy, an optimization of shortest remaining time first tailored to existing LLM iteration batching. To evaluate our work in an industrial setting, we simulate streams of requests based on our study of real-world user LLM serving trace records. Furthermore, we implemented ELIS as a cloud-native scheduler system on Kubernetes to evaluate its performance in production environments. Our experimental results demonstrate that ISRTF reduces the average job completion time by up to 19.6%.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09142', 87)">Copy Link</button>
<div id="copy-message-87" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09161">Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures</a></h1>
<p><b>Authors:</b> Yu Xin, Peng Liu, Zhuohang Xie, Wenhui Mi, Pengyue Gao, Hong Jian Zhao, Jian Lv, Yanchao Wang, Yanming Ma</p>
<p>Abstract: Even though thermodynamic energy-based crystal structure prediction (CSP) has revolutionized materials discovery, the energy-driven CSP approaches often struggle to identify experimentally realizable metastable materials synthesized through kinetically controlled pathways, creating a critical gap between theoretical predictions and experimental synthesis. Here, we propose a synthesizability-driven CSP framework that integrates symmetry-guided structure derivation with a Wyckoff encode-based machine-learning model, allowing for the efficient localization of subspaces likely to yield highly synthesizable structures. Within the identified promising subspaces, a structure-based synthesizability evaluation model, fine-tuned using recently synthesized structures to enhance predictive accuracy, is employed in conjunction with ab initio calculations to systematically identify synthesizable candidates. The framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn, Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting synthesizable structures. Notably, 92,310 structures are filtered from the 554,054 candidates predicted by GNoME, exhibiting great potential for promising synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X = Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$ candidates exhibit high synthesizability, presenting viable candidates for experimental realization and potentially associated with experimentally observed temperature-induced phase transitions. This work establishes a data-driven paradigm for machine-learning-assisted inorganic materials synthesis, highlighting its potential to bridge the gap between computational predictions and experimental realization while unlocking new opportunities for the targeted discovery of novel functional materials.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09161', 88)">Copy Link</button>
<div id="copy-message-88" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09167">Online Learning of Neural Networks</a></h1>
<p><b>Authors:</b> Amit Daniely, Idan Mehalel, Elchanan Mossel</p>
<p>Abstract: We study online learning of feedforward neural networks with the sign activation function that implement functions from the unit ball in $\mathbb{R}^d$ to a finite label set $\{1, \ldots, Y\}$.
  First, we characterize a margin condition that is sufficient and in some cases necessary for online learnability of a neural network: Every neuron in the first hidden layer classifies all instances with some margin $\gamma$ bounded away from zero. Quantitatively, we prove that for any net, the optimal mistake bound is at most approximately $\mathtt{TS}(d,\gamma)$, which is the $(d,\gamma)$-totally-separable-packing number, a more restricted variation of the standard $(d,\gamma)$-packing number. We complement this result by constructing a net on which any learner makes $\mathtt{TS}(d,\gamma)$ many mistakes. We also give a quantitative lower bound of approximately $\mathtt{TS}(d,\gamma) \geq \max\{1/(\gamma \sqrt{d})^d, d\}$ when $\gamma \geq 1/2$, implying that for some nets and input sequences every learner will err for $\exp(d)$ many times, and that a dimension-free mistake bound is almost always impossible.
  To remedy this inevitable dependence on $d$, it is natural to seek additional natural restrictions to be placed on the network, so that the dependence on $d$ is removed. We study two such restrictions. The first is the multi-index model, in which the function computed by the net depends only on $k \ll d$ orthonormal directions. We prove a mistake bound of approximately $(1.5/\gamma)^{k + 2}$ in this model. The second is the extended margin assumption. In this setting, we assume that all neurons (in all layers) in the network classify every ingoing input from previous layer with margin $\gamma$ bounded away from zero. In this model, we prove a mistake bound of approximately $(\log Y)/ \gamma^{O(L)}$, where L is the depth of the network.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09167', 89)">Copy Link</button>
<div id="copy-message-89" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09203">InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials</a></h1>
<p><b>Authors:</b> Xiao-Qi Han, Peng-Jie Guo, Ze-Feng Gao, Hao Sun, Zhong-Yi Lu</p>
<p>Abstract: Developing inverse design methods for functional materials with specific properties is critical to advancing fields like renewable energy, catalysis, energy storage, and carbon capture. Generative models based on diffusion principles can directly produce new materials that meet performance constraints, thereby significantly accelerating the material design process. However, existing methods for generating and predicting crystal structures often remain limited by low success rates. In this work, we propose a novel inverse material design generative framework called InvDesFlow-AL, which is based on active learning strategies. This framework can iteratively optimize the material generation process to gradually guide it towards desired performance characteristics. In terms of crystal structure prediction, the InvDesFlow-AL model achieves an RMSE of 0.0423 {\AA}, representing an 32.96% improvement in performance compared to exsisting generative models. Additionally, InvDesFlow-AL has been successfully validated in the design of low-formation-energy and low-Ehull materials. It can systematically generate materials with progressively lower formation energies while continuously expanding the exploration across diverse chemical spaces. These results fully demonstrate the effectiveness of the proposed active learning-driven generative model in accelerating material discovery and inverse design. To further prove the effectiveness of this method, we took the search for BCS superconductors under ambient pressure as an example explored by InvDesFlow-AL. As a result, we successfully identified Li\(_2\)AuH\(_6\) as a conventional BCS superconductor with an ultra-high transition temperature of 140 K. This discovery provides strong empirical support for the application of inverse design in materials science.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09203', 90)">Copy Link</button>
<div id="copy-message-90" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09229">Optimal Transport-Based Domain Adaptation for Rotated Linear Regression</a></h1>
<p><b>Authors:</b> Brian Britos (AMU), Mathias Bourel (UDELAR)</p>
<p>Abstract: Optimal Transport (OT) has proven effective for domain adaptation (DA) by aligning distributions across domains with differing statistical properties. Building on the approach of Courty et al. (2016), who mapped source data to the target domain for improved model transfer, we focus on a supervised DA problem involving linear regression models under rotational shifts. This ongoing work considers cases where source and target domains are related by a rotation-common in applications like sensor calibration or image orientation. We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the optimal transport map recovers the underlying rotation. Based on this, we propose an algorithm that combines K-means clustering, OT, and singular value decomposition (SVD) to estimate the rotation angle and adapt the regression model. This method is particularly effective when the target domain is sparsely sampled, leveraging abundant source data for improved generalization. Our contributions offer both theoretical and practical insights into OT-based model adaptation under geometric transformations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09229', 91)">Copy Link</button>
<div id="copy-message-91" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09262">EDBench: Large-Scale Electron Density Data for Molecular Modeling</a></h1>
<p><b>Authors:</b> Hongxin Xiang, Ke Li, Mingquan Liu, Zhixiang Cheng, Bin Yao, Wenjie Du, Jun Xia, Li Zeng, Xin Jin, Xiangxiang Zeng</p>
<p>Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on the learning of atoms, molecules, and simple quantum chemical properties (such as energy and force), but ignore the importance of electron density (ED) $\rho(r)$ in accurately understanding molecular force fields (MFFs). ED describes the probability of finding electrons at specific locations around atoms or molecules, which uniquely determines all ground state properties (such as energy, molecular structure, etc.) of interactive multi-particle systems according to the Hohenberg-Kohn theorem. However, the calculation of ED relies on the time-consuming first-principles density functional theory (DFT) which leads to the lack of large-scale ED data and limits its application in MLFFs. In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED designed to advance learning-based research at the electronic scale. Built upon the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million molecules. To comprehensively evaluate the ability of models to understand and utilize electronic information, we design a suite of ED-centric benchmark tasks spanning prediction, retrieval, and generation. Our evaluation on several state-of-the-art methods demonstrates that learning from EDBench is not only feasible but also achieves high accuracy. Moreover, we show that learning-based method can efficiently calculate ED with comparable precision while significantly reducing the computational cost relative to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09262', 92)">Copy Link</button>
<div id="copy-message-92" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09266">Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques</a></h1>
<p><b>Authors:</b> Lirand\"e Pira, Airin Antony, Nayanthara Prathap, Daniel Peace, Jacquiline Romero</p>
<p>Abstract: Photonic chip design has seen significant advancements with the adoption of inverse design methodologies, offering flexibility and efficiency in optimizing device performance. However, the black-box nature of the optimization approaches, such as those used in inverse design in order to minimize a loss function or maximize coupling efficiency, poses challenges in understanding the outputs. This challenge is prevalent in machine learning-based optimization methods, which can suffer from the same lack of transparency. To this end, interpretability techniques address the opacity of optimization models. In this work, we apply interpretability techniques from machine learning, with the aim of gaining understanding of inverse design optimization used in designing photonic components, specifically two-mode multiplexers. We base our methodology on the widespread interpretability technique known as local interpretable model-agnostic explanations, or LIME. As a result, LIME-informed insights point us to more effective initial conditions, directly improving device performance. This demonstrates that interpretability methods can do more than explain models -- they can actively guide and enhance the inverse-designed photonic components. Our results demonstrate the ability of interpretable techniques to reveal underlying patterns in the inverse design process, leading to the development of better-performing components.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09266', 93)">Copy Link</button>
<div id="copy-message-93" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09295">Toward Fair Federated Learning under Demographic Disparities and Data Imbalance</a></h1>
<p><b>Authors:</b> Qiming Wu, Siqi Li, Doudou Zhou, Nan Liu</p>
<p>Abstract: Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect. We propose FedIDA (Fed erated Learning for Imbalance and D isparity A wareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling. FedIDA supports multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm. We provide theoretical analysis establishing fairness improvement bounds using Lipschitz continuity and concentration inequalities, and show that FedIDA reduces the variance of fairness metrics across test sets. Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare. The source code is available on GitHub.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09295', 94)">Copy Link</button>
<div id="copy-message-94" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09304">Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning</a></h1>
<p><b>Authors:</b> Luciano Sebastian Martinez-Rau, Quynh Nguyen Phuong Vu, Yuxuan Zhang, Bengt Oelmann, Sebastian Bader</p>
<p>Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling efficient and intuitive audio interaction. However, standard KWS systems deployed on embedded devices often suffer performance degradation under real-world operating conditions. Resilient KWS systems address this issue by enabling dynamic adaptation, with applications such as adding or replacing keywords, adjusting to specific users, and improving noise robustness. However, deploying resilient, standalone KWS systems with low latency on resource-constrained devices remains challenging due to limited memory and computational resources. This study proposes a low computational approach for continuous noise adaptation of pretrained neural networks used for KWS classification, requiring only 1-shot learning and one epoch. The proposed method was assessed using two pretrained models and three real-world noise sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted models consistently outperformed the pretrained models across all scenarios, especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to 46.0%. These results highlight the efficacy of the proposed methodology while being lightweight enough for deployment on resource-constrained devices.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09304', 95)">Copy Link</button>
<div id="copy-message-95" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09306">Predicting butterfly species presence from satellite imagery using soft contrastive regularisation</a></h1>
<p><b>Authors:</b> Thijs L van der Plas, Stephen Law, Michael JO Pocock</p>
<p>Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled interest in remote sensing data, due to its widespread availability and extensive coverage. Traditionally, the application of remote sensing to biodiversity research has focused on mapping and monitoring habitats, but with increasing availability of large-scale citizen-science wildlife observation data, recent methods have started to explore predicting multi-species presence directly from satellite images. This paper presents a new data set for predicting butterfly species presence from satellite data in the United Kingdom. We experimentally optimise a Resnet-based model to predict multi-species presence from 4-band satellite images, and find that this model especially outperforms the mean rate baseline for locations with high species biodiversity. To improve performance, we develop a soft, supervised contrastive regularisation loss that is tailored to probabilistic labels (such as species-presence data), and demonstrate that this improves prediction accuracy. In summary, our new data set and contrastive regularisation method contribute to the open challenge of accurately predicting species biodiversity from remote sensing data, which is key for efficient biodiversity monitoring.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09306', 96)">Copy Link</button>
<div id="copy-message-96" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09313">Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach</a></h1>
<p><b>Authors:</b> Qiangqiang Liu, Qian Huang, Frank Fan, Haishan Wu, Xueyan Tang</p>
<p>Abstract: Sybil attacks pose a significant security threat to blockchain ecosystems, particularly in token airdrop events. This paper proposes a novel sybil address identification method based on subgraph feature extraction lightGBM. The method first constructs a two-layer deep transaction subgraph for each address, then extracts key event operation features according to the lifecycle of sybil addresses, including the time of first transaction, first gas acquisition, participation in airdrop activities, and last transaction. These temporal features effectively capture the consistency of sybil address behavior operations. Additionally, the method extracts amount and network structure features, comprehensively describing address behavior patterns and network topology through feature propagation and fusion. Experiments conducted on a dataset containing 193,701 addresses (including 23,240 sybil addresses) show that this method outperforms existing approaches in terms of precision, recall, F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of this study can be further applied to broader blockchain security areas such as transaction manipulation identification and token liquidity risk assessment, contributing to the construction of a more secure and fair blockchain ecosystem.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09313', 97)">Copy Link</button>
<div id="copy-message-97" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09315">TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving</a></h1>
<p><b>Authors:</b> Xuefeng Jiang, Yuan Ma, Pengxiang Li, Leimeng Xu, Xin Wen, Kun Zhan, Zhongpu Xia, Peng Jia, XianPeng Lang, Sheng Sun</p>
<p>Abstract: In recent years, diffusion model has shown its potential across diverse domains from vision generation to language modeling. Transferring its capabilities to modern autonomous driving systems has also emerged as a promising direction.In this work, we propose TransDiffuser, an encoder-decoder based generative trajectory planning model for end-to-end autonomous driving. The encoded scene information serves as the multi-modal conditional input of the denoising decoder. To tackle the mode collapse dilemma in generating high-quality diverse trajectories, we introduce a simple yet effective multi-modal representation decorrelation optimization mechanism during the training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark, surpassing previous state-of-the-art methods without any anchor-based prior trajectories.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09315', 98)">Copy Link</button>
<div id="copy-message-98" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09324">Neural Video Compression using 2D Gaussian Splatting</a></h1>
<p><b>Authors:</b> Lakshya Gupta, Imran N. Junejo</p>
<p>Abstract: The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09324', 99)">Copy Link</button>
<div id="copy-message-99" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09326">Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks</a></h1>
<p><b>Authors:</b> Vincent Abbott, Kotaro Kamiya, Gerard Glowacki, Yu Atsumi, Gioele Zardini, Yoshihiro Maruyama</p>
<p>Abstract: How do we enable artificial intelligence models to improve themselves? This is central to exponentially improving generalized artificial intelligence models, which can improve their own architecture to handle new problem domains in an efficient manner that leverages the latest hardware. However, current automated compilation methods are poor, and efficient algorithms require years of human development. In this paper, we use neural circuit diagrams, based in category theory, to prove a general theorem related to deep learning algorithms, guide the development of a novel attention algorithm catered to the domain of gene regulatory networks, and produce a corresponding efficient kernel. The algorithm we propose, spherical attention, shows that neural circuit diagrams enable a principled and systematic method for reasoning about deep learning architectures and providing high-performance code. By replacing SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special function unit bottleneck of standard attention while retaining the streaming property essential to high-performance. Our diagrammatically derived \textit{FlashSign} kernel achieves comparable performance to the state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and $3.6\times$ the performance of PyTorch. Overall, this investigation shows neural circuit diagrams' suitability as a high-level framework for the automated development of efficient, novel artificial intelligence architectures.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09326', 100)">Copy Link</button>
<div id="copy-message-100" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09342">Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems</a></h1>
<p><b>Authors:</b> Mostafa Jafari, Alireza Shameli-Sendi</p>
<p>Abstract: Machine learning is a key tool for Android malware detection, effectively identifying malicious patterns in apps. However, ML-based detectors are vulnerable to evasion attacks, where small, crafted changes bypass detection. Despite progress in adversarial defenses, the lack of comprehensive evaluation frameworks in binary-constrained domains limits understanding of their robustness. We introduce two key contributions. First, Prioritized Binary Rounding, a technique to convert continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Second, the sigma-binary attack, a novel adversarial method for binary domains, designed to achieve attack goals with minimal feature changes. Experiments on the Malscan dataset show that sigma-binary outperforms existing attacks and exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant brittleness, with attack success rates exceeding 90% using fewer than 10 feature modifications and reaching 100% with just 20. Adversarially trained defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small budgets but remains vulnerable to unrestricted perturbations, with attack success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates strong robustness against state-of-the-art gradient-based adversarial attacks by maintaining an attack success rate below 16.55%, the sigma-binary attack significantly outperforms these methods, achieving a 94.56% success rate under unrestricted perturbations. These findings highlight the critical need for precise method like sigma-binary to expose hidden vulnerabilities in existing defenses and support the development of more resilient malware detection systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09342', 101)">Copy Link</button>
<div id="copy-message-101" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09358">Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis</a></h1>
<p><b>Authors:</b> Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, Konrad Schindler</p>
<p>Abstract: The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io</p>
<p>URLs: <a href="https://marigoldcomputervision.github.io">https://marigoldcomputervision.github.io</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09358, https://marigoldcomputervision.github.io', 102)">Copy Link</button>
<div id="copy-message-102" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09364">Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch</a></h1>
<p><b>Authors:</b> Michael Benigni, Maurizio Ferrari Dacrema, Dietmar Jannach</p>
<p>Abstract: Countless new machine learning models are published every year and are reported to significantly advance the state-of-the-art in \emph{top-n} recommendation. However, earlier reproducibility studies indicate that progress in this area may be quite limited. Specifically, various widespread methodological issues, e.g., comparisons with untuned baseline models, have led to an \emph{illusion of progress}. In this work, our goal is to examine whether these problems persist in today's research. To this end, we aim to reproduce the latest advancements reported from applying modern Denoising Diffusion Probabilistic Models to recommender systems, focusing on four models published at the top-ranked SIGIR conference in 2023 and 2024. Our findings are concerning, revealing persistent methodological problems. Alarmingly, through experiments, we find that the latest recommendation techniques based on diffusion models, despite their computational complexity and substantial carbon footprint, are consistently outperformed by simpler existing models. Furthermore, we identify key mismatches between the characteristics of diffusion models and those of the traditional \emph{top-n} recommendation task, raising doubts about their suitability for recommendation. We also note that, in the papers we analyze, the generative capabilities of these models are constrained to a minimum. Overall, our results and continued methodological issues call for greater scientific rigor and a disruptive change in the research and publication culture in this area.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09364', 103)">Copy Link</button>
<div id="copy-message-103" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09365">ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections</a></h1>
<p><b>Authors:</b> H. T. R\"udisser, G. Nguyen, J. Le Lou\"edec, C. M\"ostl</p>
<p>Abstract: Interplanetary coronal mass ejections (ICMEs) are major drivers of space weather disturbances, posing risks to both technological infrastructure and human activities. Automatic detection of ICMEs in solar wind in situ data is essential for early warning systems. While several methods have been proposed to identify these structures in time series data, robust real-time detection remains a significant challenge. In this work, we present ARCANE - the first framework explicitly designed for early ICME detection in streaming solar wind data under realistic operational constraints, enabling event identification without requiring observation of the full structure. Our approach evaluates the strengths and limitations of detection models by comparing a machine learning-based method to a threshold-based baseline. The ResUNet++ model, previously validated on science data, significantly outperforms the baseline, particularly in detecting high-impact events, while retaining solid performance on lower-impact cases. Notably, we find that using real-time solar wind (RTSW) data instead of high-resolution science data leads to only minimal performance degradation. Despite the challenges of operational settings, our detection pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5% of the event's duration while only seeing a minimal amount of data. As more data becomes available, the performance increases significantly. These results mark a substantial step forward in automated space weather monitoring and lay the groundwork for enhanced real-time forecasting capabilities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09365', 104)">Copy Link</button>
<div id="copy-message-104" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09368">RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo</a></h1>
<p><b>Authors:</b> Jenny Schmalfuss, Victor Oei, Lukas Mehl, Madlen Bartsch, Shashank Agnihotri, Margret Keuper, Andr\'es Bruhn</p>
<p>Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision algorithms generally focus on model accuracy rather than robustness to image corruptions like noise or rain. Hence, the resilience of models to such real-world perturbations is largely unquantified. To address this, we present RobustSpring, a comprehensive dataset and benchmark for evaluating robustness to image corruptions for optical flow, scene flow, and stereo models. RobustSpring applies 20 different image corruptions, including noise, blur, color changes, quality degradations, and weather distortions, in a time-, stereo-, and depth-consistent manner to the high-resolution Spring dataset, creating a suite of 20,000 corrupted images that reflect challenging conditions. RobustSpring enables comparisons of model robustness via a new corruption robustness metric. Integration with the Spring benchmark enables public two-axis evaluations of both accuracy and robustness. We benchmark a curated selection of initial models, observing that accurate models are not necessarily robust and that robustness varies widely by corruption type. RobustSpring is a new computer vision benchmark that treats robustness as a first-class citizen to foster models that combine accuracy with resilience. It will be available at https://spring-benchmark.org.</p>
<p>URLs: <a href="https://spring-benchmark.org.">https://spring-benchmark.org.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09368, https://spring-benchmark.org.', 105)">Copy Link</button>
<div id="copy-message-105" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09371">TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search</a></h1>
<p><b>Authors:</b> Akash Kundu, Stefano Mangini</p>
<p>Abstract: Variational quantum algorithms hold the promise to address meaningful quantum problems already on noisy intermediate-scale quantum hardware, but they face the challenge of designing quantum circuits that both solve the target problem and comply with device limitations. Quantum architecture search (QAS) automates this design process, with reinforcement learning (RL) emerging as a promising approach. Yet, RL-based QAS methods encounter significant scalability issues, as computational and training costs grow rapidly with the number of qubits, circuit depth, and noise, severely impacting performance. To address these challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that combines tensor network (TN) methods with RL for designing quantum circuits. By warm-starting the architecture search with a matrix product state approximation of the target solution, TensorRL-QAS effectively narrows the search space to physically meaningful circuits, accelerating convergence to the desired solution. Tested on several quantum chemistry problems of up to 12-qubit, TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth compared to baseline methods, while maintaining or surpassing chemical accuracy. It reduces function evaluations by up to 100-fold, accelerates training episodes by up to $98\%$, and achieves up to $50\%$ success probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline approaches. Robustness and versatility are demonstrated both in the noiseless and noisy scenarios, where we report a simulation of up to 8-qubit. These advancements establish TensorRL-QAS as a promising candidate for a scalable and efficient quantum circuit discovery protocol on near-term quantum hardware.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09371', 106)">Copy Link</button>
<div id="copy-message-106" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09380">Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform</a></h1>
<p><b>Authors:</b> Qinghui Liu, Jon Nesvold, Hanna Raaum, Elakkyen Murugesu, Martin R{\o}vang, Bradley J Maclntosh, Atle Bj{\o}rnerud, Karoline Skogen</p>
<p>Abstract: Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models. We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway (site-1) with suspected traumatic brain injury (TBI) or patients with suspected stroke (site-2). We assessed ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining. Performance metrics included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873). Model refinement stages were associated with notable gains, highlighting the value of real-time radiologist feedback.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09380', 107)">Copy Link</button>
<div id="copy-message-107" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09395">Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting</a></h1>
<p><b>Authors:</b> Chen-Yu Liu, Kuan-Cheng Chen, Yi-Chien Chen, Samuel Yen-Chi Chen, Wei-Hao Huang, Wei-Jia Huang, Yen-Jui Chang</p>
<p>Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but remains computationally demanding due to the complexity of atmospheric dynamics and the resource requirements of deep learning models. Quantum-Train (QT), a hybrid quantum-classical framework that leverages quantum neural networks (QNNs) to generate trainable parameters exclusively during training, eliminating the need for quantum hardware at inference time. Building on QT's success across multiple domains, including image classification, reinforcement learning, flood prediction, and large language model (LLM) fine-tuning, we introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA enables parameter-efficient training while maintaining predictive accuracy. This work represents the first application of quantum machine learning (QML) to large-scale typhoon trajectory prediction, offering a scalable and energy-efficient approach to climate modeling. Our results demonstrate that QPA significantly reduces the number of trainable parameters while preserving performance, making high-performance forecasting more accessible and sustainable through hybrid quantum-classical learning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09395', 108)">Copy Link</button>
<div id="copy-message-108" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09425">Independent Component Analysis by Robust Distance Correlation</a></h1>
<p><b>Authors:</b> Sarah Leyder, Jakob Raymaekers, Peter J. Rousseeuw, Tom Van Deuren, Tim Verdonck</p>
<p>Abstract: Independent component analysis (ICA) is a powerful tool for decomposing a multivariate signal or distribution into fully independent sources, not just uncorrelated ones. Unfortunately, most approaches to ICA are not robust against outliers. Here we propose a robust ICA method called RICA, which estimates the components by minimizing a robust measure of dependence between multivariate random variables. The dependence measure used is the distance correlation (dCor). In order to make it more robust we first apply a new transformation called the bowl transform, which is bounded, one-to-one, continuous, and maps far outliers to points close to the origin. This preserves the crucial property that a zero dCor implies independence. RICA estimates the independent sources sequentially, by looking for the component that has the smallest dCor with the remainder. RICA is strongly consistent and has the usual parametric rate of convergence. Its robustness is investigated by a simulation study, in which it generally outperforms its competitors. The method is illustrated on three applications, including the well-known cocktail party problem.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09425', 109)">Copy Link</button>
<div id="copy-message-109" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09430">Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU</a></h1>
<p><b>Authors:</b> Yutong Hu, Pinhao Song, Kehan Wen, Renaud Detry</p>
<p>Abstract: We present a method for training multi-task vision-language robotic diffusion policies that reduces training time and memory usage by an order of magnitude. This improvement arises from a previously underexplored distinction between action diffusion and the image diffusion techniques that inspired it: image generation targets are high-dimensional, while robot actions lie in a much lower-dimensional space. Meanwhile, the vision-language conditions for action generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this asymmetry by introducing Level-2 minibatching, which pairs multiple noised action samples with each vision-language condition, instead of the conventional one-to-one sampling strategy. To support this batching scheme, we introduce architectural adaptations to the diffusion transformer that prevent information leakage across samples while maintaining full conditioning access. In RLBench simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art multi-task diffusion policies, while using only 5\% of the training time and 7\% of the memory. Real-world experiments further validate that Mini-Diffuser preserves the key strengths of diffusion-based policies, including the ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs. Code available at github.com/utomm/mini-diffuse-actor.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09430', 110)">Copy Link</button>
<div id="copy-message-110" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09456">Quantum state-agnostic work extraction (almost) without dissipation</a></h1>
<p><b>Authors:</b> Josep Lumbreras, Ruo Cheng Huang, Yanglin Hu, Mile Gu, Marco Tomamichel</p>
<p>Abstract: We investigate work extraction protocols designed to transfer the maximum possible energy to a battery using sequential access to $N$ copies of an unknown pure qubit state. The core challenge is designing interactions to optimally balance two competing goals: charging of the battery optimally using the qubit in hand, and acquiring more information by qubit to improve energy harvesting in subsequent rounds. Here, we leverage exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies achieving energy dissipation that scales only poly-logarithmically in $N$. This represents an exponential improvement over current protocols based on full state tomography.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09456', 111)">Copy Link</button>
<div id="copy-message-111" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09471">Fairness-aware Bayes optimal functional classification</a></h1>
<p><b>Authors:</b> Xiaoyu Hu, Gengyu Xue, Zhenhua Lin, Yi Yu</p>
<p>Abstract: Algorithmic fairness has become a central topic in machine learning, and mitigating disparities across different subpopulations has emerged as a rapidly growing research area. In this paper, we systematically study the classification of functional data under fairness constraints, ensuring the disparity level of the classifier is controlled below a pre-specified threshold. We propose a unified framework for fairness-aware functional classification, tackling an infinite-dimensional functional space, addressing key challenges from the absence of density ratios and intractability of posterior probabilities, and discussing unique phenomena in functional classification. We further design a post-processing algorithm, Fair Functional Linear Discriminant Analysis classifier (Fair-FLDA), which targets at homoscedastic Gaussian processes and achieves fairness via group-wise thresholding. Under weak structural assumptions on eigenspace, theoretical guarantees on fairness and excess risk controls are established. As a byproduct, our results cover the excess risk control of the standard FLDA as a special case, which, to the best of our knowledge, is first time seen. Our theoretical findings are complemented by extensive numerical experiments on synthetic and real datasets, highlighting the practicality of our designed algorithm.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09471', 112)">Copy Link</button>
<div id="copy-message-112" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09496">Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data</a></h1>
<p><b>Authors:</b> Rui Miao, Babak Shahbaba, Annie Qu</p>
<p>Abstract: Offline reinforcement learning (RL) aims to find optimal policies in dynamic environments in order to maximize the expected total rewards by leveraging pre-collected data. Learning from heterogeneous data is one of the fundamental challenges in offline RL. Traditional methods focus on learning an optimal policy for all individuals with pre-collected data from a single episode or homogeneous batch episodes, and thus, may result in a suboptimal policy for a heterogeneous population. In this paper, we propose an individualized offline policy optimization framework for heterogeneous time-stationary Markov decision processes (MDPs). The proposed heterogeneous model with individual latent variables enables us to efficiently estimate the individual Q-functions, and our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm guarantees a fast rate on the average regret under a weak partial coverage assumption on behavior policies. In addition, our simulation studies and a real data application demonstrate the superior numerical performance of the proposed method compared with existing methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09496', 113)">Copy Link</button>
<div id="copy-message-113" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09506">Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders</a></h1>
<p><b>Authors:</b> Mar\'ia Alejandra Hern\'andez, Oscar Rodriguez, Dae-Jin Lee</p>
<p>Abstract: Several approaches have been developed to capture the complexity and nonlinearity of human growth. One widely used is the Super Imposition by Translation and Rotation (SITAR) model, which has become popular in studies of adolescent growth. SITAR is a shape-invariant mixed-effects model that represents the shared growth pattern of a population using a natural cubic spline mean curve while incorporating three subject-specific random effects -- timing, size, and growth intensity -- to account for variations among individuals. In this work, we introduce a supervised deep learning framework based on an autoencoder architecture that integrates a deep neural network (neural network) with a B-spline model to estimate the SITAR model. In this approach, the encoder estimates the random effects for each individual, while the decoder performs a fitting based on B-splines similar to the classic SITAR model. We refer to this method as the Deep-SITAR model. This innovative approach enables the prediction of the random effects of new individuals entering a population without requiring a full model re-estimation. As a result, Deep-SITAR offers a powerful approach to predicting growth trajectories, combining the flexibility and efficiency of deep learning with the interpretability of traditional mixed-effects models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09506', 114)">Copy Link</button>
<div id="copy-message-114" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09516">Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios</a></h1>
<p><b>Authors:</b> Siyi Wang, Alexandre Leblanc, Paul D. McNicholas</p>
<p>Abstract: Cluster analysis, or clustering, plays a crucial role across numerous scientific and engineering domains. Despite the wealth of clustering methods proposed over the past decades, each method is typically designed for specific scenarios and presents certain limitations in practical applications. In this paper, we propose depth-based local center clustering (DLCC). This novel method makes use of data depth, which is known to produce a center-outward ordering of sample points in a multivariate space. However, data depth typically fails to capture the multimodal characteristics of {data}, something of the utmost importance in the context of clustering. To overcome this, DLCC makes use of a local version of data depth that is based on subsets of {data}. From this, local centers can be identified as well as clusters of varying shapes. Furthermore, we propose a new internal metric based on density-based clustering to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a flexible clustering approach that seems to overcome some limitations of traditional clustering methods, thereby enhancing data analysis capabilities across a wide range of application scenarios.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09516', 115)">Copy Link</button>
<div id="copy-message-115" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09518">\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs</a></h1>
<p><b>Authors:</b> Maris F. L. Galesloot, Roman Andriushchenko, Milan \v{C}e\v{s}ka, Sebastian Junges, Nils Jansen</p>
<p>Abstract: Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its POMDPs. We compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09518', 116)">Copy Link</button>
<div id="copy-message-116" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09529">Contactless Cardiac Pulse Monitoring Using Event Cameras</a></h1>
<p><b>Authors:</b> Mohamed Moustafa, Joseph Lemley, Peter Corcoran</p>
<p>Abstract: Time event cameras are a novel technology for recording scene information at extremely low latency and with low power consumption. Event cameras output a stream of events that encapsulate pixel-level light intensity changes within the scene, capturing information with a higher dynamic range and temporal resolution than traditional cameras. This study investigates the contact-free reconstruction of an individual's cardiac pulse signal from time event recording of their face using a supervised convolutional neural network (CNN) model. An end-to-end model is trained to extract the cardiac signal from a two-dimensional representation of the event stream, with model performance evaluated based on the accuracy of the calculated heart rate. The experimental results confirm that physiological cardiac information in the facial region is effectively preserved within the event stream, showcasing the potential of this novel sensor for remote heart rate monitoring. The model trained on event frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm) compared to the RMSE of 2.92 bpm achieved by the baseline model trained on standard camera frames. Furthermore, models trained on event frames generated at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an RMSE of 2.54 and 2.13 bpm, respectively.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09529', 117)">Copy Link</button>
<div id="copy-message-117" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09546">Distilling Realizable Students from Unrealizable Teachers</a></h1>
<p><b>Authors:</b> Yujin Kim, Nathaniel Chin, Arnav Vasudev, Sanjiban Choudhury</p>
<p>Abstract: We study policy distillation under privileged information, where a student policy with only partial observations must learn from a teacher with full-state access. A key challenge is information asymmetry: the student cannot directly access the teacher's state space, leading to distributional shifts and policy degradation. Existing approaches either modify the teacher to produce realizable but sub-optimal demonstrations or rely on the student to explore missing information independently, both of which are inefficient. Our key insight is that the student should strategically interact with the teacher --querying only when necessary and resetting from recovery states --to stay on a recoverable path within its own observation space. We introduce two methods: (i) an imitation learning approach that adaptively determines when the student should query the teacher for corrections, and (ii) a reinforcement learning approach that selects where to initialize training for efficient exploration. We validate our methods in both simulated and real-world robotic tasks, demonstrating significant improvements over standard teacher-student baselines in training efficiency and final performance. The project website is available at : https://portal-cornell.github.io/CritiQ_ReTRy/</p>
<p>URLs: <a href="https://portal-cornell.github.io/CritiQ_ReTRy/">https://portal-cornell.github.io/CritiQ_ReTRy/</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09546, https://portal-cornell.github.io/CritiQ_ReTRy/', 118)">Copy Link</button>
<div id="copy-message-118" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09552">Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods</a></h1>
<p><b>Authors:</b> Pascal K\"undig, Fabio Sigrist</p>
<p>Abstract: Mixed effects models are widely used for modeling data with hierarchically grouped structures and high-cardinality categorical predictor variables. However, for high-dimensional crossed random effects, current standard computations relying on Cholesky decompositions can become prohibitively slow. In this work, we present novel Krylov subspace-based methods that address several existing computational bottlenecks. Among other things, we theoretically analyze and empirically evaluate various preconditioners for the conjugate gradient and stochastic Lanczos quadrature methods, derive new convergence results, and develop computationally efficient methods for calculating predictive variances. Extensive experiments using simulated and real-world data sets show that our proposed methods scale much better than Cholesky-based computations, for instance, achieving a runtime reduction of approximately two orders of magnitudes for both estimation and prediction. Moreover, our software implementation is up to 10'000 times faster and more stable than state-of-the-art implementations such as lme4 and glmmTMB when using default settings. Our methods are implemented in the free C++ software library GPBoost with high-level Python and R packages.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09552', 119)">Copy Link</button>
<div id="copy-message-119" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09558">WavReward: Spoken Dialogue Models With Generalist Reward Evaluators</a></h1>
<p><b>Authors:</b> Shengpeng Ji, Tianle Liang, Yangzhuo Li, Jialong Zuo, Minghui Fang, Jinzheng He, Yifu Chen, Zhengqing Liu, Ziyue Jiang, Xize Cheng, Siqi Zheng, Jin Xu, Junyang Lin, Zhou Zhao</p>
<p>Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.</p>
<p>URLs: <a href="https://github.com/jishengpeng/WavReward">https://github.com/jishengpeng/WavReward</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09558, https://github.com/jishengpeng/WavReward', 120)">Copy Link</button>
<div id="copy-message-120" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09561">Learning Long-Context Diffusion Policies via Past-Token Prediction</a></h1>
<p><b>Authors:</b> Marcel Torne, Andy Tang, Yuejiang Liu, Chelsea Finn</p>
<p>Abstract: Reasoning over long sequences of observations and actions is essential for many robotic tasks. Yet, learning effective long-context policies from demonstrations remains challenging. As context length increases, training becomes increasingly expensive due to rising memory demands, and policy performance often degrades as a result of spurious correlations. Recent methods typically sidestep these issues by truncating context length, discarding historical information that may be critical for subsequent decisions. In this paper, we propose an alternative approach that explicitly regularizes the retention of past information. We first revisit the copycat problem in imitation learning and identify an opposite challenge in recent diffusion policies: rather than over-relying on prior actions, they often fail to capture essential dependencies between past and future actions. To address this, we introduce Past-Token Prediction (PTP), an auxiliary task in which the policy learns to predict past action tokens alongside future ones. This regularization significantly improves temporal modeling in the policy head, with minimal reliance on visual representations. Building on this observation, we further introduce a multistage training strategy: pre-train the visual encoder with short contexts, and fine-tune the policy head using cached long-context embeddings. This strategy preserves the benefits of PTP while greatly reducing memory and computational overhead. Finally, we extend PTP into a self-verification mechanism at test time, enabling the policy to score and select candidates consistent with past actions during inference. Experiments across four real-world and six simulated tasks demonstrate that our proposed method improves the performance of long-context diffusion policies by 3x and accelerates policy training by more than 10x.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09561', 121)">Copy Link</button>
<div id="copy-message-121" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09603">DataMIL: Selecting Data for Robot Imitation Learning with Datamodels</a></h1>
<p><b>Authors:</b> Shivin Dass, Alaa Khaddaj, Logan Engstrom, Aleksander Madry, Andrew Ilyas, Roberto Mart\'in-Mart\'in</p>
<p>Abstract: Recently, the robotics community has amassed ever larger and more diverse datasets to train generalist robot policies. However, while these policies achieve strong mean performance across a variety of tasks, they often underperform on individual, specialized tasks and require further tuning on newly acquired task-specific data. Combining task-specific data with carefully curated subsets of large prior datasets via co-training can produce better specialized policies, but selecting data naively may actually harm downstream performance. To address this, we introduce DataMIL, a policy-driven data selection framework built on the datamodels paradigm that reasons about data selection in an end-to-end manner, using the policy itself to identify which data points will most improve performance. Unlike standard practices that filter data using human notions of quality (e.g., based on semantic or visual similarity), DataMIL directly optimizes data selection for task success, allowing us to select data that enhance the policy while dropping data that degrade it. To avoid performing expensive rollouts in the environment during selection, we use a novel surrogate loss function on task-specific data, allowing us to use DataMIL in the real world without degrading performance. We validate our approach on a suite of more than 60 simulation and real-world manipulation tasks - most notably showing successful data selection from the Open X-Embodiment datasets-demonstrating consistent gains in success rates and superior performance over multiple baselines. Our results underscore the importance of end-to-end, performance-aware data selection for unlocking the potential of large prior datasets in robotics. More information at https://robin-lab.cs.utexas.edu/datamodels4imitation/</p>
<p>URLs: <a href="https://robin-lab.cs.utexas.edu/datamodels4imitation/">https://robin-lab.cs.utexas.edu/datamodels4imitation/</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09603, https://robin-lab.cs.utexas.edu/datamodels4imitation/', 122)">Copy Link</button>
<div id="copy-message-122" class="copy-message"></div>
</div>
<div class="article">
<h1> cross <a href="https://arxiv.org/abs/2505.09612">Adaptively-weighted Nearest Neighbors for Matrix Completion</a></h1>
<p><b>Authors:</b> Tathagata Sadhukhan, Manit Paul, Raaz Dwivedi</p>
<p>Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted nearest neighbor method for performing matrix completion. Nearest neighbor (NN) methods are widely used in missing data problems across multiple disciplines such as in recommender systems and for performing counterfactual inference in panel data settings. Prior works have shown that in addition to being very intuitive and easy to implement, NN methods enjoy nice theoretical guarantees. However, the performance of majority of the NN methods rely on the appropriate choice of the radii and the weights assigned to each member in the nearest neighbor set and despite several works on nearest neighbor methods in the past two decades, there does not exist a systematic approach of choosing the radii and the weights without relying on methods like cross-validation. AWNN addresses this challenge by judiciously balancing the bias variance trade off inherent in weighted nearest-neighbor regression. We provide theoretical guarantees for the proposed method under minimal assumptions and support the theory via synthetic experiments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.09612', 123)">Copy Link</button>
<div id="copy-message-123" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2404.11269">DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series</a></h1>
<p><b>Authors:</b> Zahra Zamanzadeh Darban, Yiyuan Yang, Geoffrey I. Webb, Charu C. Aggarwal, Qingsong Wen, Shirui Pan, Mahsa Salehi</p>
<p>Abstract: In time series anomaly detection (TSAD), the scarcity of labeled data poses a challenge to the development of accurate models. Unsupervised domain adaptation (UDA) offers a solution by leveraging labeled data from a related domain to detect anomalies in an unlabeled target domain. However, existing UDA methods assume consistent anomalous classes across domains. To address this limitation, we propose a novel Domain Adaptation Contrastive learning model for Anomaly Detection in multivariate time series (DACAD), combining UDA with contrastive learning. DACAD utilizes an anomaly injection mechanism that enhances generalization across unseen anomalous classes, improving adaptability and robustness. Additionally, our model employs supervised contrastive loss for the source domain and self-supervised contrastive triplet loss for the target domain, ensuring comprehensive feature representation learning and domain-invariant feature extraction. Finally, an effective Center-based Entropy Classifier (CEC) accurately learns normal boundaries in the source domain. Extensive evaluations on multiple real-world datasets and a synthetic dataset highlight DACAD's superior performance in transferring knowledge across domains and mitigating the challenge of limited labeled data in TSAD.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2404.11269', 124)">Copy Link</button>
<div id="copy-message-124" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2405.13806">A General Graph Spectral Wavelet Convolution via Chebyshev Order Decomposition</a></h1>
<p><b>Authors:</b> Nian Liu, Xiaoxin He, Thomas Laurent, Francesco Di Giovanni, Michael M. Bronstein, Xavier Bresson</p>
<p>Abstract: Spectral graph convolution, an important tool of data filtering on graphs, relies on two essential decisions: selecting spectral bases for signal transformation and parameterizing the kernel for frequency analysis. While recent techniques mainly focus on standard Fourier transform and vector-valued spectral functions, they fall short in flexibility to model signal distributions over large spatial ranges, and capacity of spectral function. In this paper, we present a novel wavelet-based graph convolution network, namely WaveGC, which integrates multi-resolution spectral bases and a matrix-valued filter kernel. Theoretically, we establish that WaveGC can effectively capture and decouple short-range and long-range information, providing superior filtering flexibility, surpassing existing graph wavelet neural networks. To instantiate WaveGC, we introduce a novel technique for learning general graph wavelets by separately combining odd and even terms of Chebyshev polynomials. This approach strictly satisfies wavelet admissibility criteria. Our numerical experiments showcase the consistent improvements in both short-range and long-range tasks. This underscores the effectiveness of the proposed model in handling different scenarios. Our code is available at https://github.com/liun-online/WaveGC.</p>
<p>URLs: <a href="https://github.com/liun-online/WaveGC.">https://github.com/liun-online/WaveGC.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.13806, https://github.com/liun-online/WaveGC.', 125)">Copy Link</button>
<div id="copy-message-125" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2406.13041">Accelerated Stochastic Min-Max Optimization Based on Bias-corrected Momentum</a></h1>
<p><b>Authors:</b> Haoyuan Cai, Sulaiman A. Alghunaim, Ali H. Sayed</p>
<p>Abstract: Lower-bound analyses for nonconvex strongly-concave minimax optimization problems have shown that stochastic first-order algorithms require at least $\mathcal{O}(\varepsilon^{-4})$ oracle complexity to find an $\varepsilon$-stationary point. Some works indicate that this complexity can be improved to $\mathcal{O}(\varepsilon^{-3})$ when the loss gradient is Lipschitz continuous. The question of achieving enhanced convergence rates under distinct conditions, remains unresolved. In this work, we address this question for optimization problems that are nonconvex in the minimization variable and strongly concave or Polyak-Lojasiewicz (PL) in the maximization variable. We introduce novel bias-corrected momentum algorithms utilizing efficient Hessian-vector products. We establish convergence conditions and demonstrate a lower iteration complexity of $\mathcal{O}(\varepsilon^{-3})$ for the proposed algorithms. The effectiveness of the method is validated through applications to robust logistic regression using real-world datasets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2406.13041', 126)">Copy Link</button>
<div id="copy-message-126" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2408.10368">Deep-MacroFin: Informed Equilibrium Neural Network for Continuous Time Economic Models</a></h1>
<p><b>Authors:</b> Yuntao Wu, Jiayuan Guo, Goutham Gopalakrishna, Zissis Poulos</p>
<p>Abstract: In this paper, we present Deep-MacroFin, a comprehensive framework designed to solve partial differential equations, with a particular focus on models in continuous time economics. This framework leverages deep learning methodologies, including Multi-Layer Perceptrons and the newly developed Kolmogorov-Arnold Networks. It is optimized using economic information encapsulated by Hamilton-Jacobi-Bellman (HJB) equations and coupled algebraic equations. The application of neural networks holds the promise of accurately resolving high-dimensional problems with fewer computational demands and limitations compared to other numerical methods. This framework can be readily adapted for systems of partial differential equations in high dimensions. Importantly, it offers a more efficient (5$\times$ less CUDA memory and 40$\times$ fewer FLOPs in 100D problems) and user-friendly implementation than existing libraries. We also incorporate a time-stepping scheme to enhance training stability for nonlinear HJB equations, enabling the solution of 50D economic models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2408.10368', 127)">Copy Link</button>
<div id="copy-message-127" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2409.17632">Least Squares and Marginal Log-Likelihood Model Predictive Control using Normalizing Flows</a></h1>
<p><b>Authors:</b> Eike Cramer</p>
<p>Abstract: Real-world (bio)chemical processes often exhibit stochastic dynamics with non-trivial correlations and state-dependent fluctuations. Model predictive control (MPC) often must consider these fluctuations to achieve reliable performance. However, most process models simply add stationary noise terms to a deterministic prediction. This work proposes using conditional normalizing flows as discrete-time models to learn stochastic dynamics. Normalizing flows learn the probability density function (PDF) of the states explicitly, given prior states and control inputs. In addition to standard least squares (LSQ) objectives, this work derives a marginal log-likelihood (MLL) objective based on the explicit PDF and Markov chain simulations. In a reactor study, the normalizing flow MPC reduces the setpoint error in open and closed-loop cases to half that of a nominal controller. Furthermore, the chance constraints lead to fewer constraint violations than the nominal controller. The MLL objective yields slightly more stable results than the LSQ, particularly for small scenario sets.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.17632', 128)">Copy Link</button>
<div id="copy-message-128" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.02247">Theoretical Insights into Fine-Tuning Attention Mechanism: Generalization and Optimization</a></h1>
<p><b>Authors:</b> Xinhao Yao, Hongjin Qian, Xiaolin Hu, Gengze Xu, Wei Liu, Jian Luan, Bin Wang, Yong Liu</p>
<p>Abstract: Large Language Models (LLMs), built on Transformer architectures, exhibit remarkable generalization across a wide range of tasks. However, fine-tuning these models for specific tasks remains resource-intensive due to their extensive parameterization. In this paper, we explore two remarkable phenomena related to the attention mechanism during the fine-tuning of LLMs (where $\mathbf{W}_q$, $\mathbf{W}_k$, and $\mathbf{W}_v$ denote the weights of the query, key, and value layers, respectively). The first phenomenon, termed "Unequal Importance of Attention Matrices", highlights the impact of fine-tuning different weight matrices. It shows that optimizing the $\mathbf{W}_v$ matrix yields significantly better performance than optimizing the $\mathbf{W}_k$ matrix. Fine-tuning only the $\mathbf{W}_q$ and $\mathbf{W}_v$ matrices is computationally efficient while delivering results comparable to, or even better than fine-tuning all three matrices ($\mathbf{W}_q$, $\mathbf{W}_k$, and $\mathbf{W}_v$). The second phenomenon,"Attention Matrices with Customized Learning Rate Lead to Better Convergence", emphasizes the importance of assigning distinct learning rates to these matrices. Specifically, a higher learning rate for the $\mathbf{W}_v$ matrix compared to $\mathbf{W}_q$ and $\mathbf{W}_k$ accelerates convergence and improves performance. Building on these insights, we propose a new strategy that improves fine-tuning efficiency in terms of both storage and time. Experimental results on benchmark datasets validate the effectiveness of this approach, supporting our theoretical findings. Our analysis lays the theoretical groundwork for configuring and improving algorithms in LLMs fine-tuning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.02247', 129)">Copy Link</button>
<div id="copy-message-129" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.08007">Time Can Invalidate Algorithmic Recourse</a></h1>
<p><b>Authors:</b> Giovanni De Toni, Stefano Teso, Bruno Lepri, Andrea Passerini</p>
<p>Abstract: Algorithmic Recourse (AR) aims to provide users with actionable steps to overturn unfavourable decisions made by machine learning predictors. However, these actions often take time to implement (e.g., getting a degree can take years), and their effects may vary as the world evolves. Thus, it is natural to ask for recourse that remains valid in a dynamic environment. In this paper, we study the robustness of algorithmic recourse over time by casting the problem through the lens of causality. We demonstrate theoretically and empirically that (even robust) causal AR methods can fail over time, except in the -- unlikely -- case that the world is stationary. Even more critically, unless the world is fully deterministic, counterfactual AR cannot be solved optimally. To account for this, we propose a simple yet effective algorithm for temporal AR that explicitly accounts for time under the assumption of having access to an estimator approximating the stochastic process. Our simulations on synthetic and realistic datasets show how considering time produces more resilient solutions to potential trends in the data distribution.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.08007', 130)">Copy Link</button>
<div id="copy-message-130" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2410.17075">Combinatorial Logistic Bandits</a></h1>
<p><b>Authors:</b> Xutong Liu, Xiangxiang Dai, Xuchuang Wang, Mohammad Hajiesmaili, John C. S. Lui</p>
<p>Abstract: We introduce a novel framework called combinatorial logistic bandits (CLogB), where in each round, a subset of base arms (called the super arm) is selected, with the outcome of each base arm being binary and its expectation following a logistic parametric model. The feedback is governed by a general arm triggering process. Our study covers CLogB with reward functions satisfying two smoothness conditions, capturing application scenarios such as online content delivery, online learning to rank, and dynamic channel allocation. We first propose a simple yet efficient algorithm, CLogUCB, utilizing a variance-agnostic exploration bonus. Under the 1-norm triggering probability modulated (TPM) smoothness condition, CLogUCB achieves a regret bound of $\tilde{O}(d\sqrt{\kappa KT})$, where $\tilde{O}$ ignores logarithmic factors, $d$ is the dimension of the feature vector, $\kappa$ represents the nonlinearity of the logistic model, and $K$ is the maximum number of base arms a super arm can trigger. This result improves on prior work by a factor of $\tilde{O}(\sqrt{\kappa})$. We then enhance CLogUCB with a variance-adaptive version, VA-CLogUCB, which attains a regret bound of $\tilde{O}(d\sqrt{KT})$ under the same 1-norm TPM condition, improving another $\tilde{O}(\sqrt{\kappa})$ factor. VA-CLogUCB shows even greater promise under the stronger triggering probability and variance modulated (TPVM) condition, achieving a leading $\tilde{O}(d\sqrt{T})$ regret, thus removing the additional dependency on the action-size $K$. Furthermore, we enhance the computational efficiency of VA-CLogUCB by eliminating the nonconvex optimization process when the context feature map is time-invariant while maintaining the tight $\tilde{O}(d\sqrt{T})$ regret. Finally, experiments on synthetic and real-world datasets demonstrate the superior performance of our algorithms compared to benchmark algorithms.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.17075', 131)">Copy Link</button>
<div id="copy-message-131" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.09009">A physics-informed transformer neural operator for learning generalized solutions of initial boundary value problems</a></h1>
<p><b>Authors:</b> Sumanth Kumar Boya, Deepak Subramani</p>
<p>Abstract: Initial boundary value problems arise commonly in applications with engineering and natural systems governed by nonlinear partial differential equations (PDEs). Operator learning is an emerging field for solving these equations by using a neural network to learn a map between infinite dimensional input and output function spaces. These neural operators are trained using a combination of data (observations or simulations) and PDE-residuals (physics-loss). A major drawback of existing neural approaches is the requirement to retrain with new initial/boundary conditions, and the necessity for a large amount of simulation data for training. We develop a physics-informed transformer neural operator (named PINTO) that efficiently generalizes to unseen initial and boundary conditions, trained in a simulation-free setting using only physics loss. The main innovation lies in our new iterative kernel integral operator units, implemented using cross-attention, to transform the PDE solution's domain points into an initial/boundary condition-aware representation vector, enabling efficient learning of the solution function for new scenarios. The PINTO architecture is applied to simulate the solutions of important equations used in engineering applications: advection, Burgers, and steady and unsteady Navier-Stokes equations (three flow scenarios). For these five test cases, we show that the relative errors during testing under challenging conditions of unseen initial/boundary conditions are only one-fifth to one-third of other leading physics informed operator learning methods. Moreover, our PINTO model is able to accurately solve the advection and Burgers equations at time steps that are not included in the training collocation points. The code is available at https://github.com/quest-lab-iisc/PINTO</p>
<p>URLs: <a href="https://github.com/quest-lab-iisc/PINTO">https://github.com/quest-lab-iisc/PINTO</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.09009, https://github.com/quest-lab-iisc/PINTO', 132)">Copy Link</button>
<div id="copy-message-132" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2412.21061">BridgePure: Limited Protection Leakage Can Break Black-Box Data Protection</a></h1>
<p><b>Authors:</b> Yihan Wang, Yiwei Lu, Xiao-Shan Gao, Gautam Kamath, Yaoliang Yu</p>
<p>Abstract: Availability attacks, or unlearnable examples, are defensive techniques that allow data owners to modify their datasets in ways that prevent unauthorized machine learning models from learning effectively while maintaining the data's intended functionality. It has led to the release of popular black-box tools (e.g., APIs) for users to upload personal data and receive protected counterparts. In this work, we show that such black-box protections can be substantially compromised if a small set of unprotected in-distribution data is available. Specifically, we propose a novel threat model of protection leakage, where an adversary can (1) easily acquire (unprotected, protected) pairs by querying the black-box protections with a small unprotected dataset; and (2) train a diffusion bridge model to build a mapping between unprotected and protected data. This mapping, termed BridgePure, can effectively remove the protection from any previously unseen data within the same distribution. BridgePure demonstrates superior purification performance on classification and style mimicry tasks, exposing critical vulnerabilities in black-box data protection. We suggest that practitioners implement multi-level countermeasures to mitigate such risks.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.21061', 133)">Copy Link</button>
<div id="copy-message-133" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.13223">A Comprehensive Social Bias Audit of Contrastive Vision Language Models</a></h1>
<p><b>Authors:</b> Zahraa Al Sahili, Ioannis Patras, Matthew Purver</p>
<p>Abstract: In the domain of text-to-image generative models, biases inherent in training datasets often propagate into generated content, posing significant ethical challenges, particularly in socially sensitive contexts. We introduce FairCoT, a novel framework that enhances fairness in text-to-image models through Chain-of-Thought (CoT) reasoning within multimodal generative large language models. FairCoT employs iterative CoT refinement to systematically mitigate biases, and dynamically adjusts textual prompts in real time, ensuring diverse and equitable representation in generated images. By integrating iterative reasoning processes, FairCoT addresses the limitations of zero-shot CoT in sensitive scenarios, balancing creativity with ethical responsibility. Experimental evaluations across popular text-to-image systems--including DALL-E and various Stable Diffusion variants--demonstrate that FairCoT significantly enhances fairness and diversity without sacrificing image quality or semantic fidelity. By combining robust reasoning, lightweight deployment, and extensibility to multiple models, FairCoT represents a promising step toward more socially responsible and transparent AI-driven content generation.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.13223', 134)">Copy Link</button>
<div id="copy-message-134" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.13786">Handling Missing Data in Downstream Tasks With Distribution-Preserving Guarantees</a></h1>
<p><b>Authors:</b> Rahul Bordoloi, Cl\'emence R\'eda, Saptarshi Bej, Olaf Wolkenhauer</p>
<p>Abstract: Missing feature values are a significant hurdle for downstream machine-learning tasks such as classification. However, imputation methods for classification might be time-consuming for high-dimensional data, and offer few theoretical guarantees on the preservation of the data distribution and imputation quality, especially for not-missing-at-random mechanisms. First, we propose an imputation approach named F3I based on the iterative improvement of a K-nearest neighbor imputation, where neighbor-specific weights are learned through the optimization of a novel concave, differentiable objective function related to the preservation of the data distribution on non-missing values. F3I can then be chained to and jointly trained with any classifier architecture. Second, we provide a theoretical analysis of imputation quality and data distribution preservation by F3I for several types of missing mechanisms. Finally, we demonstrate the superior performance of F3I on several imputation and classification tasks, with applications to drug repurposing and handwritten-digit recognition data.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.13786', 135)">Copy Link</button>
<div id="copy-message-135" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2501.16393">Improving Network Threat Detection by Knowledge Graph, Large Language Model, and Imbalanced Learning</a></h1>
<p><b>Authors:</b> Lili Zhang, Quanyan Zhu, Herman Ray, Ying Xie</p>
<p>Abstract: Network threat detection has been challenging due to the complexities of attack activities and the limitation of historical threat data to learn from. To help enhance the existing practices of using analytics, machine learning, and artificial intelligence methods to detect the network threats, we propose an integrated modelling framework, where Knowledge Graph is used to analyze the users' activity patterns, Imbalanced Learning techniques are used to prune and weigh Knowledge Graph, and LLM is used to retrieve and interpret the users' activities from Knowledge Graph. The proposed framework is applied to Agile Threat Detection through Online Sequential Learning. The preliminary results show the improved threat capture rate by 3%-4% and the increased interpretabilities of risk predictions based on the users' activities.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2501.16393', 136)">Copy Link</button>
<div id="copy-message-136" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.00497">Convolutional Fourier Analysis Network (CFAN): A Unified Time-Frequency Approach for ECG Classification</a></h1>
<p><b>Authors:</b> Sam Jeong, Hae Yong Kim</p>
<p>Abstract: Machine learning has revolutionized biomedical signal analysis, particularly in electrocardiogram (ECG) classification. While convolutional neural networks (CNNs) excel at automatic feature extraction, the optimal integration of time- and frequency-domain information remains unresolved. This study introduces the Convolutional Fourier Analysis Network (CFAN), a novel architecture that unifies time-frequency analysis by embedding Fourier principles directly into CNN layers. We evaluate CFAN against four benchmarks - spectrogram-based 2D CNN (SPECT); 1D CNN (CNN1D); Fourier-based 1D CNN (FFT1D); and CNN1D with integrated Fourier Analysis Network (CNN1D-FAN) - across three ECG tasks: arrhythmia classification (MIT-BIH), identity recognition (ECG-ID), and apnea detection (Apnea-ECG). CFAN achieved state-of-the-art performance, surpassing all competing methods with accuracies of 98.95% (MIT-BIH), 96.83% (ECG-ID), and 95.01% (Apnea-ECG). Notably, on ECG-ID and Apnea-ECG, CFAN demonstrated statistically significant improvements over the second-best method (CNN1D-FAN, $p \leq 0.02$), further validating its superior performance. Key innovations include CONV-FAN blocks that combine sine, cosine and GELU activations in convolutional layers to capture periodic features and joint time-frequency learning without spectrogram conversion. Our results highlight CFAN's potential for broader biomedical and signal classification applications.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.00497', 137)">Copy Link</button>
<div id="copy-message-137" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.01391">Learning Traffic Anomalies from Generative Models on Real-Time Observations</a></h1>
<p><b>Authors:</b> Fotis I. Giasemis, Alexandros Sopasakis</p>
<p>Abstract: Accurate detection of traffic anomalies is crucial for effective urban traffic management and congestion mitigation. We use the Spatiotemporal Generative Adversarial Network (STGAN) framework combining Graph Neural Networks and Long Short-Term Memory networks to capture complex spatial and temporal dependencies in traffic data. We apply STGAN to real-time, minute-by-minute observations from 42 traffic cameras across Gothenburg, Sweden, collected over several months in 2020. The images are processed to compute a flow metric representing vehicle density, which serves as input for the model. Training is conducted on data from April to November 2020, and validation is performed on a separate dataset from November 14 to 23, 2020. Our results demonstrate that the model effectively detects traffic anomalies with high precision and low false positive rates. The detected anomalies include camera signal interruptions, visual artifacts, and extreme weather conditions affecting traffic flow.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.01391', 138)">Copy Link</button>
<div id="copy-message-138" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.04405">FAS: Fast ANN-SNN Conversion for Spiking Large Language Models</a></h1>
<p><b>Authors:</b> Long Chen, Xiaotian Song, Andy Song, BaDong Chen, Jiancheng Lv, Yanan Sun</p>
<p>Abstract: Spiking Large Language Models have been shown as a good alternative to LLMs in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct training and ANN-SNN conversion, often suffer from performance degradation and relatively high computational costs. To address these issues, we propose a novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking LLMs in two stages. The first stage employs a full-parameter fine-tuning of pre-trained models, so it does not need any direct training from scratch. The second stage introduces a coarse-to-fine calibration method to reduce conversion errors and improve accuracy. Experiments on both language and vision-language tasks across four different scales of LLMs demonstrate that FAS can achieve state-of-the-art performance yet with significantly reduced inference latency and computational costs. Notably, FAS only takes eight timesteps to achieve an accuracy of 3\% higher than that of the OPT-7B model, while reducing energy consumption by 96.63\%. The source code is available at https://github.com/lc783/FAS</p>
<p>URLs: <a href="https://github.com/lc783/FAS">https://github.com/lc783/FAS</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.04405, https://github.com/lc783/FAS', 139)">Copy Link</button>
<div id="copy-message-139" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.08975">Graph-structured Small Molecule Drug Discovery Through Deep Learning: Progress, Challenges, and Opportunities</a></h1>
<p><b>Authors:</b> Kun Li, Yida Xiong, Hongzhi Zhang, Xiantao Cai, Jia Wu, Bo Du, Wenbin Hu</p>
<p>Abstract: Due to their excellent drug-like and pharmacokinetic properties, small molecule drugs are widely used to treat various diseases, making them a critical component of drug discovery. In recent years, with the rapid development of deep learning (DL) techniques, DL-based small molecule drug discovery methods have achieved excellent performance in prediction accuracy, speed, and complex molecular relationship modeling compared to traditional machine learning approaches. These advancements enhance drug screening efficiency and optimization and provide more precise and effective solutions for various drug discovery tasks. Contributing to this field's development, this paper aims to systematically summarize and generalize the recent key tasks and representative techniques in graph-structured small molecule drug discovery in recent years. Specifically, we provide an overview of the major tasks in small molecule drug discovery and their interrelationships. Next, we analyze the six core tasks, summarizing the related methods, commonly used datasets, and technological development trends. Finally, we discuss key challenges, such as interpretability and out-of-distribution generalization, and offer our insights into future research directions for small molecule drug discovery.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.08975', 140)">Copy Link</button>
<div id="copy-message-140" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.15507">Activation Steering in Neural Theorem Provers</a></h1>
<p><b>Authors:</b> Shashank Kirtania</p>
<p>Abstract: Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.15507', 141)">Copy Link</button>
<div id="copy-message-141" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2502.15823">InductionBench: LLMs Fail in the Simplest Complexity Class</a></h1>
<p><b>Authors:</b> Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang</p>
<p>Abstract: Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.</p>
<p>URLs: <a href="https://github.com/Wenyueh/inductive_reasoning_benchmark.">https://github.com/Wenyueh/inductive_reasoning_benchmark.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.15823, https://github.com/Wenyueh/inductive_reasoning_benchmark.', 142)">Copy Link</button>
<div id="copy-message-142" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2503.12662">TuneNSearch: a hybrid transfer learning and local search approach for solving vehicle routing problems</a></h1>
<p><b>Authors:</b> Arthur Corr\^ea, Crist\'ov\~ao Silva, Liming Xu, Alexandra Brintrup, Samuel Moniz</p>
<p>Abstract: This paper introduces TuneNSearch, a hybrid transfer learning and local search approach for addressing different variants of vehicle routing problems (VRP). Recently, multi-task learning has gained much attention for solving VRP variants. However, this adaptability often compromises the performance of the models. To address this challenge, we first pre-train a reinforcement learning model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it to different variants. By leveraging the complexity of the multi-depot VRP, the pre-trained model learns richer node representations and gains more transferable knowledge compared to models trained on simpler routing problems, such as the traveling salesman problem. TuneNSearch employs, in the first stage, a Transformer-based architecture, augmented with a residual edge-graph attention network to capture the impact of edge distances and residual connections between layers. This architecture allows for a more precise capture of graph-structured data, improving the encoding of VRP's features. After inference, our model is also coupled with a second stage composed of a local search algorithm, which yields substantial performance gains with minimal computational overhead added. Results show that TuneNSearch outperforms many existing state-of-the-art models trained for each VRP variant, requiring only one-fifth of the training epochs. Our approach demonstrates strong generalization, achieving high performance across different tasks, distributions and problem sizes, thus addressing a long-standing gap in the literature.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.12662', 143)">Copy Link</button>
<div id="copy-message-143" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2504.02692">GPTAQ: Efficient Finetuning-Free Quantization for Asymmetric Calibration</a></h1>
<p><b>Authors:</b> Yuhang Li, Ruokai Yin, Donghyun Lee, Shiting Xiao, Priyadarshini Panda</p>
<p>Abstract: We introduce GPTAQ, a novel finetuning-free quantization method for compressing large-scale transformer architectures. Unlike the previous GPTQ method, which independently calibrates each layer, we always match the quantized layer's output to the exact output in the full-precision model, resulting in a scheme that we call asymmetric calibration. Such a scheme can effectively reduce the quantization error accumulated in previous layers. We analyze this problem using optimal brain compression to derive a close-formed solution. The new solution explicitly minimizes the quantization error as well as the accumulated asymmetry error. Furthermore, we utilize various techniques to parallelize the solution calculation, including channel parallelization, neuron decomposition, and Cholesky reformulation for matrix fusion. As a result, GPTAQ is easy to implement, simply using 20 more lines of code than GPTQ but improving its performance under low-bit quantization. Remarkably, on a single GPU, we quantize a 405B language transformer as well as EVA-02, the rank first vision transformer that achieves 90% pretraining Imagenet accuracy. Code is available at Github.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.02692', 144)">Copy Link</button>
<div id="copy-message-144" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2504.09192">Towards More Efficient, Robust, Instance-adaptive, and Generalizable Sequential Decision making</a></h1>
<p><b>Authors:</b> Zhiyong Wang</p>
<p>Abstract: The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer networks, video analytics, and large language models (LLMs). Sequential decision-making methods, such as bandits and RL, have demonstrated remarkable success - ranging from outperforming human players in complex games like Atari and Go to advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these successes, many established algorithms rely on idealized models that can fail under model misspecifications or adversarial perturbations, particularly in settings where accurate prior knowledge of the underlying model class is unavailable or where malicious users operate within dynamic systems. These challenges are pervasive in real-world applications, where robust and adaptive solutions are critical. Furthermore, while worst-case guarantees provide theoretical reliability, they often fail to capture instance-dependent performance, which can lead to more efficient and practical solutions. Another key challenge lies in generalizing to new, unseen environments, a crucial requirement for deploying these methods in dynamic and unpredictable settings. To address these limitations, my research aims to develop more efficient, robust, instance-adaptive, and generalizable sequential decision-making algorithms for both reinforcement learning and bandits. Towards this end, I focus on developing more efficient, robust, instance-adaptive, and generalizable for both general reinforcement learning (RL) and bandits.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.09192', 145)">Copy Link</button>
<div id="copy-message-145" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2504.10612">Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling</a></h1>
<p><b>Authors:</b> Michal Balcerak, Tamaz Amiranashvili, Antonio Terpin, Suprosanna Shit, Sebastian Kaltenbach, Petros Koumoutsakos, Bjoern Menze</p>
<p>Abstract: The most widely used generative models map noise and data distributions by matching flows or scores. However, they struggle to incorporate partial observations and additional priors--something energy-based models (EBMs) handle elegantly by simply adding corresponding scalar energy terms. We address this issue by proposing Energy Matching, a framework that endows flow-based approaches with the flexibility of EBMs. Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in terms of fidelity, while retaining simulation-free training of transport-based approaches away from the data manifold. Furthermore, we leverage the method's flexibility to introduce an interaction energy that supports diverse mode exploration, which we demonstrate in a controlled protein-generation setting. Our approach focuses on learning a scalar potential energy--without time-conditioning, auxiliary generators, or additional networks--which marks a significant departure from recent EBM methods. We believe that this simplified framework significantly advances EBMs capabilities and paves the way for their wider adoption in generative modeling across diverse domains.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.10612', 146)">Copy Link</button>
<div id="copy-message-146" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2504.11383">Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition</a></h1>
<p><b>Authors:</b> Wei Wang, Maryam Hakimzadeh, Haihui Ruan, Somdatta Goswami</p>
<p>Abstract: Numerical solvers for PDEs face challenges in balancing computational cost and accuracy, particularly for multiscale and dynamical systems. Neural operators (NOs) can significantly speed up simulations; however, they face challenges such as error accumulation for dynamical systems and limited generalization in multiphysics problems. This work introduces a novel hybrid framework that integrates PI-NO with finite element method (FE) through domain decomposition and leverages numerical analysis for time marching. The core innovation lies in efficient coupling FE and NO subdomains via a Schwarz alternating method: regions with complex, nonlinear, or high-gradient behavior are resolved using a pretrained NO, while the remainder is handled by conventional FE. To address the challenges of dynamic systems, we embed a time-stepping scheme directly into the NO architecture, substantially reducing long-term error propagation. Also, an adaptive subdomain evolution strategy enables the ML resolved region to expand dynamically, capturing emerging fine scale features without remeshing. The framework efficacy has been validated across a range of problems, spanning static, quasi-static, and dynamic regimes (e.g., linear elasticity, hyperelasticity, and elastodynamics), demonstrating accelerated convergence (up to 20% improvement in convergence compared to conventional FE coupling) while preserving solution fidelity with error margins consistently below 1%. Our study shows that our hybrid solver: (1) maintains solution continuity across subdomain interfaces, (2) reduces computational costs by eliminating fine mesh requirements, (3) mitigates error accumulation in time dependent simulations, and (4) enables automatic adaptation to evolving physical phenomena. This work bridges the gap between numerical methods and AI-driven surrogates, offering a scalable pathway for high-fidelity multiscale simulations.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.11383', 147)">Copy Link</button>
<div id="copy-message-147" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.00887">Rethinking Time Encoding via Learnable Transformation Functions</a></h1>
<p><b>Authors:</b> Xi Chen, Yateng Tang, Jiarong Xu, Jiawei Zhang, Siwei Zhang, Sijia Peng, Xuehao Zheng, Yun Xiong</p>
<p>Abstract: Effectively modeling time information and incorporating it into applications or models involving chronologically occurring events is crucial. Real-world scenarios often involve diverse and complex time patterns, which pose significant challenges for time encoding methods. While previous methods focus on capturing time patterns, many rely on specific inductive biases, such as using trigonometric functions to model periodicity. This narrow focus on single-pattern modeling makes them less effective in handling the diversity and complexities of real-world time patterns. In this paper, we investigate to improve the existing commonly used time encoding methods and introduce Learnable Transformation-based Generalized Time Encoding (LeTE). We propose using deep function learning techniques to parameterize non-linear transformations in time encoding, making them learnable and capable of modeling generalized time patterns, including diverse and complex temporal dynamics. By enabling learnable transformations, LeTE encompasses previous methods as specific cases and allows seamless integration into a wide range of tasks. Through extensive experiments across diverse domains, we demonstrate the versatility and effectiveness of LeTE.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.00887', 148)">Copy Link</button>
<div id="copy-message-148" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.01618">Don&#x27;t be lazy: CompleteP enables compute-efficient deep transformers</a></h1>
<p><b>Authors:</b> Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, Joel Hestness</p>
<p>Abstract: We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34% compute efficiency improvements over the prior state-of-the-art.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.01618', 149)">Copy Link</button>
<div id="copy-message-149" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.06795">Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery</a></h1>
<p><b>Authors:</b> Abhijit Gupta</p>
<p>Abstract: Commodity price volatility creates economic challenges, necessitating accurate multi-horizon forecasting. Predicting prices for commodities like copper and crude oil is complicated by diverse interacting factors (macroeconomic, supply/demand, geopolitical, etc.). Current models often lack transparency, limiting strategic use. This paper presents a Regularized Sparse Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon commodity price prediction and discovery of interpretable latent market drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series. Crucially, L1 regularization ($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity, promoting parsimonious explanations of market dynamics through learned factors representing underlying drivers (e.g., demand, supply shocks). Drawing from energy-based models and sparse coding, the RSAE optimizes predictive accuracy while learning sparse representations. Evaluated on historical Copper and Crude Oil data with numerous indicators, our findings indicate the RSAE offers competitive multi-horizon forecasting accuracy and data-driven insights into price dynamics via its interpretable latent space, a key advantage over traditional black-box approaches.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.06795', 150)">Copy Link</button>
<div id="copy-message-150" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.07086">Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design</a></h1>
<p><b>Authors:</b> Tong Chen, Yinuo Zhang, Sophia Tang, Pranam Chatterjee</p>
<p>Abstract: Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.07086', 151)">Copy Link</button>
<div id="copy-message-151" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.07961">Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement</a></h1>
<p><b>Authors:</b> Xuechen Zhang, Zijian Huang, Chenshun Ni, Ziyang Xiong, Jiasi Chen, Samet Oymak</p>
<p>Abstract: Recent research enhances language model reasoning by scaling test-time compute via longer chain-of-thought traces. This often improves accuracy but also introduces redundancy and high computational cost, especially for small language models distilled with supervised fine-tuning (SFT). In this work, we propose new algorithms to improve token-efficient reasoning with small-scale models by effectively trading off accuracy and computation. We first show that the post-SFT model fails to determine the optimal stopping point of the reasoning process, resulting in verbose and repetitive outputs. Verbosity also significantly varies across wrong vs correct responses. To address these issues, we propose two solutions: (1) Temperature scaling (TS) to control the stopping point for the thinking phase and thereby trace length, and (2) TLDR: a length-regularized reinforcement learning method based on GRPO that facilitates multi-level trace length control (e.g. short, medium, long reasoning). Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and OlympiadBench, demonstrate that TS is highly effective compared to s1's budget forcing approach and TLDR significantly improves token efficiency by about 50% with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also facilitates flexible control over the response length, offering a practical and effective solution for token-efficient reasoning in small models. Ultimately, our work reveals the importance of stopping time control, highlights shortcomings of pure SFT, and provides effective algorithmic recipes.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.07961', 152)">Copy Link</button>
<div id="copy-message-152" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.08320">SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness</a></h1>
<p><b>Authors:</b> Yoonhyuk Choi, Chong-Kwon Kim</p>
<p>Abstract: We introduce SpecSphere, the first dual-pass spectral-spatial GNN that certifies every prediction against both $\ell\_{0}$ edge flips and $\ell\_{\infty}$ feature perturbations, adapts to the full homophily-heterophily spectrum, and surpasses the expressive power of 1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a Chebyshev-polynomial spectral branch with an attention-gated spatial branch and fuses their representations through a lightweight MLP trained in a cooperative-adversarial min-max game. We further establish (i) a uniform Chebyshev approximation theorem, (ii) minimax-optimal risk across the homophily-heterophily spectrum, (iii) closed-form robustness certificates, and (iv) universal approximation strictly beyond 1-WL. SpecSphere achieves state-of-the-art node-classification accuracy and delivers tighter certified robustness guarantees on real-world benchmarks. These results demonstrate that high expressivity, heterophily adaptation, and provable robustness can coexist within a single, scalable architecture.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08320', 153)">Copy Link</button>
<div id="copy-message-153" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.08550">OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain</a></h1>
<p><b>Authors:</b> Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi</p>
<p>Abstract: This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-based multivariate time series forecasting model that operates in an $\mathbf{o}$rthogonally transformed domain. Recent forecasting models typically adopt the temporal forecast (TF) paradigm, which directly encode and decode time series in the time domain. However, the entangled step-wise dependencies in series data can hinder the performance of TF. To address this, some forecasters conduct encoding and decoding in the transformed domain using fixed, dataset-independent bases (e.g., sine and cosine signals in the Fourier transform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptive transformation based on an orthogonal matrix that diagonalizes the series' temporal Pearson correlation matrix. This approach enables more effective encoding and decoding in the decorrelated feature domain and can serve as a plug-in module to enhance existing forecasters. To enhance the representation learning for multivariate time series, we introduce a customized linear layer, $\mathbf{NormLin}$, which employs a normalized weight matrix to capture multivariate dependencies. Empirically, the NormLin module shows a surprising performance advantage over multi-head self-attention, while requiring nearly half the FLOPs. Extensive experiments on 24 benchmarks and 140 forecasting tasks demonstrate that OLinear consistently achieves state-of-the-art performance with high efficiency. Notably, as a plug-in replacement for self-attention, the NormLin module consistently enhances Transformer-based forecasters. The code and datasets are available at https://anonymous.4open.science/r/OLinear</p>
<p>URLs: <a href="https://anonymous.4open.science/r/OLinear">https://anonymous.4open.science/r/OLinear</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08550, https://anonymous.4open.science/r/OLinear', 154)">Copy Link</button>
<div id="copy-message-154" class="copy-message"></div>
</div>
<div class="article">
<h1> replace <a href="https://arxiv.org/abs/2505.08740">Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations</a></h1>
<p><b>Authors:</b> Abdolmehdi Behroozi, Chaopeng Shen and, Daniel Kifer</p>
<p>Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are fundamental in science and engineering. While deep learning frameworks such as the Fourier Neural Operator (FNO) can efficiently approximate solutions, they struggle with inverse problems, sensitivity estimation (du/dp), and concept drift. We address these limitations by introducing a sensitivity-based regularization strategy, called Sensitivity-Constrained Fourier Neural Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths and consistently outperforms standard FNO and FNO with physics-informed regularization. It improves performance in parameter inversion tasks, scales to high-dimensional parameter spaces (tested with up to 82 parameters), and reduces both data and training requirements. These gains are achieved with a modest increase in training time (30% to 130% per epoch) and generalize across various types of differential equations and neural operators. Code and selected experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators</p>
<p>URLs: <a href="https://github.com/AMBehroozi/SC_Neural_Operators">https://github.com/AMBehroozi/SC_Neural_Operators</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08740, https://github.com/AMBehroozi/SC_Neural_Operators', 155)">Copy Link</button>
<div id="copy-message-155" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2110.15907">Learning to Be Cautious</a></h1>
<p><b>Authors:</b> Montaser Mohammedalamen, Dustin Morrill, Alexander Sieusahai, Yash Satsangi, Michael Bowling</p>
<p>Abstract: A key challenge in the field of reinforcement learning is to develop agents that behave cautiously in novel situations. It is generally impossible to anticipate all situations that an autonomous system may face or what behavior would best avoid bad outcomes. An agent that can learn to be cautious would overcome this challenge by discovering for itself when and how to behave cautiously. In contrast, current approaches typically embed task-specific safety information or explicit cautious behaviors into the system, which is error-prone and imposes extra burdens on practitioners. In this paper, we present both a sequence of tasks where cautious behavior becomes increasingly non-obvious, as well as an algorithm to demonstrate that it is possible for a system to learn to be cautious. The essential features of our algorithm are that it characterizes reward function uncertainty without task-specific safety information and uses this uncertainty to construct a robust policy. Specifically, we construct robust policies with a k-of-N counterfactual regret minimization (CFR) subroutine given learned reward function uncertainty represented by a neural network ensemble. These policies exhibit caution in each of our tasks without any task-specific safety tuning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2110.15907', 156)">Copy Link</button>
<div id="copy-message-156" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2204.03139">DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects</a></h1>
<p><b>Authors:</b> Priya Sundaresan, Rika Antonova, Jeannette Bohg</p>
<p>Abstract: Research in manipulation of deformable objects is typically conducted on a limited range of scenarios, because handling each scenario on hardware takes significant effort. Realistic simulators with support for various types of deformations and interactions have the potential to speed up experimentation with novel tasks and algorithms. However, for highly deformable objects it is challenging to align the output of a simulator with the behavior of real objects. Manual tuning is not intuitive, hence automated methods are needed. We view this alignment problem as a joint perception-inference challenge and demonstrate how to use recent neural network architectures to successfully perform simulation parameter inference from real point clouds. We analyze the performance of various architectures, comparing their data and training requirements. Furthermore, we propose to leverage differentiable point cloud sampling and differentiable simulation to significantly reduce the time to achieve the alignment. We employ an efficient way to propagate gradients from point clouds to simulated meshes and further through to the physical simulation parameters, such as mass and stiffness. Experiments with highly deformable objects show that our method can achieve comparable or better alignment with real object behavior, while reducing the time needed to achieve this by more than an order of magnitude. Videos and supplementary material are available at https://diffcloud.github.io.</p>
<p>URLs: <a href="https://diffcloud.github.io.">https://diffcloud.github.io.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2204.03139, https://diffcloud.github.io.', 157)">Copy Link</button>
<div id="copy-message-157" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2303.17496">Data-driven multiscale modeling for correcting dynamical systems</a></h1>
<p><b>Authors:</b> Karl Otness, Laure Zanna, Joan Bruna</p>
<p>Abstract: We propose a multiscale approach for predicting quantities in dynamical systems which is explicitly structured to extract information in both fine-to-coarse and coarse-to-fine directions. We envision this method being generally applicable to problems with significant self-similarity or in which the prediction task is challenging and where stability of a learned model's impact on the target dynamical system is important. We evaluate our approach on a climate subgrid parameterization task in which our multiscale networks correct chaotic underlying models to reflect the contributions of unresolved, fine-scale dynamics.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2303.17496', 158)">Copy Link</button>
<div id="copy-message-158" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2402.00045">Detecting Multimedia Generated by Large AI Models: A Survey</a></h1>
<p><b>Authors:</b> Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu</p>
<p>Abstract: The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.</p>
<p>URLs: <a href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.">https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.00045, https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.', 159)">Copy Link</button>
<div id="copy-message-159" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2402.06919">TREET: TRansfer Entropy Estimation via Transformers</a></h1>
<p><b>Authors:</b> Omer Luxembourg, Dor Tsur, Haim Permuter</p>
<p>Abstract: Transfer entropy (TE) is an information theoretic measure that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel attention-based approach for estimating TE for stationary processes. The proposed approach employs Donsker-Varadhan representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods on a dedicated estimation benchmark. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma, and use it to estimate the capacity of communication channels with memory, which is a canonical optimization problem in information theory. We further demonstrate how an optimized TREET can be used to estimate underlying densities, providing experimental results. Finally, we apply TREET to feature analysis of patients with Apnea, demonstrating its applicability to real-world physiological data. Our work, applied with state-of-the-art deep learning methods, opens a new door for communication problems which are yet to be solved.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2402.06919', 160)">Copy Link</button>
<div id="copy-message-160" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2403.13952">Considerations in the use of ML interaction potentials for free energy calculations</a></h1>
<p><b>Authors:</b> Orlando A. Mendible, Jonathan K. Whitmer, Yamil J. Col\'on</p>
<p>Abstract: Machine learning force fields (MLFFs) promise to accurately describe the potential energy surface of molecules at the ab initio level of theory with improved computational efficiency. Within MLFFs, equivariant graph neural networks (EQNNs) have shown great promise in accuracy and performance and are the focus of this work. The capability of EQNNs to recover free energy surfaces (FES) remains to be thoroughly investigated. In this work, we investigate the impact of collective variables (CVs) distribution within the training data on the accuracy of EQNNs predicting the FES of butane and alanine dipeptide (ADP). A generalizable workflow is presented in which training configurations are generated with classical molecular dynamics simulations, and energies and forces are obtained with ab initio calculations. We evaluate how bond and angle constraints in the training data influence the accuracy of EQNN force fields in reproducing the FES of the molecules at both classical and ab initio levels of theory. Results indicate that the model's accuracy is unaffected by the distribution of sampled CVs during training, given that the training data includes configurations from characteristic regions of the system's FES. However, when the training data is obtained from classical simulations, the EQNN struggles to extrapolate the free energy for configurations with high free energy. In contrast, models trained with the same configurations on ab initio data show improved extrapolation accuracy. The findings underscore the difficulties in creating a comprehensive training dataset for EQNNs to predict FESs and highlight the importance of prior knowledge of the system's FES.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2403.13952', 161)">Copy Link</button>
<div id="copy-message-161" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2404.02171">Optimal navigation of magnetic artificial microswimmers in blood capillaries with deep reinforcement learning</a></h1>
<p><b>Authors:</b> Lucas Amoudruz, Sergey Litvinov, Petros Koumoutsakos</p>
<p>Abstract: Biomedical applications such as targeted drug delivery, microsurgery, and sensing rely on reaching precise areas within the body in a minimally invasive way. Artificial bacterial flagella (ABFs) have emerged as potential tools for this task by navigating through the circulatory system with the help of external magnetic fields. While their swimming characteristics are well understood in simple settings, their controlled navigation through realistic capillary networks remains a significant challenge due to the complexity of blood flow and the high computational cost of detailed simulations. We address this challenge by conducting numerical simulations of ABFs in retinal capillaries, propelled by an external magnetic field. The simulations are based on a validated blood model that predicts the dynamics of individual red blood cells and their hydrodynamic interactions with ABFs. The magnetic field follows a control policy that brings the ABF to a prescribed target. The control policy is learned with an actor-critic, off-policy reinforcement learning algorithm coupled with a reduced-order model of the system. We show that the same policy robustly guides the ABF to a prescribed target in both the reduced-order model and the fine-grained blood simulations. This approach is suitable for designing robust control policies for personalized medicine at moderate computational cost.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2404.02171', 162)">Copy Link</button>
<div id="copy-message-162" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2405.17955">Efficient Prior Calibration From Indirect Data</a></h1>
<p><b>Authors:</b> O. Deniz Akyildiz, Mark Girolami, Andrew M. Stuart, Arnaud Vadeboncoeur</p>
<p>Abstract: Bayesian inversion is central to the quantification of uncertainty within problems arising from numerous applications in science and engineering. To formulate the approach, four ingredients are required: a forward model mapping the unknown parameter to an element of a solution space, often the solution space for a differential equation; an observation operator mapping an element of the solution space to the data space; a noise model describing how noise pollutes the observations; and a prior model describing knowledge about the unknown parameter before the data is acquired. This paper is concerned with learning the prior model from data; in particular, learning the prior from multiple realizations of indirect data obtained through the noisy observation process. The prior is represented, using a generative model, as the pushforward of a Gaussian in a latent space; the pushforward map is learned by minimizing an appropriate loss function. A metric that is well-defined under empirical approximation is used to define the loss function for the pushforward map to make an implementable methodology. Furthermore, an efficient residual-based neural operator approximation of the forward model is proposed and it is shown that this may be learned concurrently with the pushforward map, using a bilevel optimization formulation of the problem; this use of neural operator approximation has the potential to make prior learning from indirect data more computationally efficient, especially when the observation process is expensive, non-smooth or not known. The ideas are illustrated with the Darcy flow inverse problem of finding permeability from piezometric head measurements.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2405.17955', 163)">Copy Link</button>
<div id="copy-message-163" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2407.21062">Hybrid Heuristic Algorithms for Adiabatic Quantum Machine Learning Models</a></h1>
<p><b>Authors:</b> Bahram Alidaee, Haibo Wang, Lutfu Sua, Wade Liu</p>
<p>Abstract: Numerous established machine learning models and various neural network architectures can be restructured as Quadratic Unconstrained Binary Optimization (QUBO) problems. A significant challenge in Adiabatic Quantum Machine Learning (AQML) is the computational demand of the training phase. To mitigate this, approximation techniques inspired by quantum annealing, like Simulated Annealing and Multiple Start Tabu Search (MSTS), have been employed to expedite QUBO-based AQML training. This paper introduces a novel hybrid algorithm that incorporates an "r-flip" strategy. This strategy is aimed at solving large-scale QUBO problems more effectively, offering better solution quality and lower computational costs compared to existing MSTS methods. The r-flip approach has practical applications in diverse fields, including cross-docking, supply chain management, machine scheduling, and fraud detection. The paper details extensive computational experiments comparing this r-flip enhanced hybrid heuristic against a standard MSTS approach. These tests utilize both standard benchmark problems and three particularly large QUBO instances. The results indicate that the r-flip enhanced method consistently produces high-quality solutions efficiently, operating within practical time constraints.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2407.21062', 164)">Copy Link</button>
<div id="copy-message-164" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2409.02668">Introduction to Machine Learning</a></h1>
<p><b>Authors:</b> Laurent Younes</p>
<p>Abstract: This book introduces the mathematical foundations and techniques that lead to the development and analysis of many of the algorithms that are used in machine learning. It starts with an introductory chapter that describes notation used throughout the book and serve at a reminder of basic concepts in calculus, linear algebra and probability and also introduces some measure theoretic terminology, which can be used as a reading guide for the sections that use these tools. The introductory chapters also provide background material on matrix analysis and optimization. The latter chapter provides theoretical support to many algorithms that are used in the book, including stochastic gradient descent, proximal methods, etc. After discussing basic concepts for statistical prediction, the book includes an introduction to reproducing kernel theory and Hilbert space techniques, which are used in many places, before addressing the description of various algorithms for supervised statistical learning, including linear methods, support vector machines, decision trees, boosting, or neural networks. The subject then switches to generative methods, starting with a chapter that presents sampling methods and an introduction to the theory of Markov chains. The following chapter describe the theory of graphical models, an introduction to variational methods for models with latent variables, and to deep-learning based generative models. The next chapters focus on unsupervised learning methods, for clustering, factor analysis and manifold learning. The final chapter of the book is theory-oriented and discusses concentration inequalities and generalization bounds.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.02668', 165)">Copy Link</button>
<div id="copy-message-165" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2409.15511">Bayesian computation with generative diffusion models by Multilevel Monte Carlo</a></h1>
<p><b>Authors:</b> Abdul-Lateef Haji-Ali, Marcelo Pereyra, Luke Shaw, Konstantinos Zygalakis</p>
<p>Abstract: Generative diffusion models have recently emerged as a powerful strategy to perform stochastic sampling in Bayesian inverse problems, delivering remarkably accurate solutions for a wide range of challenging applications. However, diffusion models often require a large number of neural function evaluations per sample in order to deliver accurate posterior samples. As a result, using diffusion models as stochastic samplers for Monte Carlo integration in Bayesian computation can be highly computationally expensive, particularly in applications that require a substantial number of Monte Carlo samples for conducting uncertainty quantification analyses. This cost is especially high in large-scale inverse problems such as computational imaging, which rely on large neural networks that are expensive to evaluate. With quantitative imaging applications in mind, this paper presents a Multilevel Monte Carlo strategy that significantly reduces the cost of Bayesian computation with diffusion models. This is achieved by exploiting cost-accuracy trade-offs inherent to diffusion models to carefully couple models of different levels of accuracy in a manner that significantly reduces the overall cost of the calculation, without reducing the final accuracy. The proposed approach achieves a $4\times$-to-$8\times$ reduction in computational cost w.r.t. standard techniques across three benchmark imaging problems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.15511', 166)">Copy Link</button>
<div id="copy-message-166" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2409.18872">Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks</a></h1>
<p><b>Authors:</b> Richard Osuala, Smriti Joshi, Apostolia Tsirikoglou, Lidia Garrucho, Walter H. L. Pinaya, Daniel M. Lang, Julia A. Schnabel, Oliver Diaz, Karim Lekadir</p>
<p>Abstract: This paper presents a method for virtual contrast enhancement in breast MRI, offering a promising non-invasive alternative to traditional contrast agent-based DCE-MRI acquisition. Using a conditional generative adversarial network, we predict DCE-MRI images, including jointly-generated sequences of multiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs, enabling tumor localization and characterization without the associated health risks. Furthermore, we qualitatively and quantitatively evaluate the synthetic DCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe), assessing their utility in a tumor segmentation downstream task, and conclude with an analysis of the temporal patterns in multi-sequence DCE-MRI generation. Our approach demonstrates promising results in generating realistic and useful DCE-MRI sequences, highlighting the potential of virtual contrast enhancement for improving breast cancer diagnosis and treatment, particularly for patients where contrast agent administration is contraindicated.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2409.18872', 167)">Copy Link</button>
<div id="copy-message-167" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.23238">Full-waveform earthquake source inversion using simulation-based inference</a></h1>
<p><b>Authors:</b> A. A. Saoulis, D. Piras, A. Spurio Mancini, B. Joachimi, A. M. G. Ferreira</p>
<p>Abstract: This paper presents a novel framework for full-waveform seismic source inversion using simulation-based inference (SBI). Traditional probabilistic approaches often rely on simplifying assumptions about data errors, which we show can lead to inaccurate uncertainty quantification. SBI addresses this limitation by building an empirical probabilistic model of the data errors using machine learning models, known as neural density estimators, which can then be integrated into the Bayesian inference framework. We apply the SBI framework to point-source moment tensor inversions as well as joint moment tensor and time-location inversions. We construct a range of synthetic examples to explore the quality of the SBI solutions, as well as to compare the SBI results with standard Gaussian likelihood-based Bayesian inversions. We then demonstrate that under real seismic noise, common Gaussian likelihood assumptions for treating full-waveform data yield overconfident posterior distributions that underestimate the moment tensor component uncertainties by up to a factor of 3. We contrast this with SBI, which produces well-calibrated posteriors that generally agree with the true seismic source parameters, and offers an order-of-magnitude reduction in the number of simulations required to perform inference compared to standard Monte Carlo techniques. Finally, we apply our methodology to a pair of moderate magnitude earthquakes in the North Atlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean bottom seismometer array as well as by regional land stations in the Azores, comparing full moment tensor and source-time location posteriors between SBI and a Gaussian likelihood approach. We find that our adaptation of SBI can be directly applied to real earthquake sources to efficiently produce high quality posterior distributions that significantly improve upon Gaussian likelihood approaches.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.23238', 168)">Copy Link</button>
<div id="copy-message-168" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2410.23854">Reflecting Topology Consistency and Abnormality via Learnable Attentions for Airway Labeling</a></h1>
<p><b>Authors:</b> Chenyu Li, Minghui Zhang, Chuyan Zhang, Yun Gu</p>
<p>Abstract: Accurate airway anatomical labeling is crucial for clinicians to identify and navigate complex bronchial structures during bronchoscopy. Automatic airway anatomical labeling is challenging due to significant individual variability and anatomical variations. Previous methods are prone to generate inconsistent predictions, which is harmful for preoperative planning and intraoperative navigation. This paper aims to address these challenges by proposing a novel method that enhances topological consistency and improves the detection of abnormal airway branches. We propose a novel approach incorporating two modules: the Soft Subtree Consistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module constructs a soft subtree to capture clinically relevant topological relationships, allowing for flexible feature aggregation within and across subtrees. The ABS module facilitates the interaction between node features and prototypes to distinguish abnormal branches, preventing the erroneous aggregation of features between normal and abnormal nodes. Evaluated on a challenging dataset characterized by severe airway distortion and atrophy, our method achieves superior performance compared to state-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the segmental level and an 83.7% accuracy at the subsegmental level, representing a 1.4% increase in subsegmental accuracy and a 3.1% increase in topological consistency. Notably, the method demonstrates reliable performance in cases with disease-induced airway deformities, ensuring consistent and accurate labeling.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2410.23854', 169)">Copy Link</button>
<div id="copy-message-169" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.04867">Think Smart, Act SMARL! Analyzing Probabilistic Logic Shields for Multi-Agent Reinforcement Learning</a></h1>
<p><b>Authors:</b> Satchit Chatterji, Erman Acar</p>
<p>Abstract: Safe reinforcement learning (RL) is crucial for real-world applications, and multi-agent interactions introduce additional safety challenges. While Probabilistic Logic Shields (PLS) has been a powerful proposal to enforce safety in single-agent RL, their generalizability to multi-agent settings remains unexplored. In this paper, we address this gap by conducting extensive analyses of PLS within decentralized, multi-agent environments, and in doing so, propose Shielded Multi-Agent Reinforcement Learning (SMARL) as a general framework for steering MARL towards norm-compliant outcomes. Our key contributions are: (1) a novel Probabilistic Logic Temporal Difference (PLTD) update for shielded, independent Q-learning, which incorporates probabilistic constraints directly into the value update process; (2) a probabilistic logic policy gradient method for shielded PPO with formal safety guarantees for MARL; and (3) comprehensive evaluation across symmetric and asymmetrically shielded $n$-player game-theoretic benchmarks, demonstrating fewer constraint violations and significantly better cooperation under normative constraints. These results position SMARL as an effective mechanism for equilibrium selection, paving the way toward safer, socially aligned multi-agent systems.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.04867', 170)">Copy Link</button>
<div id="copy-message-170" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2411.11681">PSPO*: An Effective Process-supervised Policy Optimization for Reasoning Alignment</a></h1>
<p><b>Authors:</b> Jiawei Li, Xinyue Liang, Junlong Zhang, Yizhe Yang, Chong Feng, Yang Gao</p>
<p>Abstract: Process supervision enhances the performance of large language models in reasoning tasks by providing feedback at each step of chain-of-thought reasoning. However, due to the lack of effective process supervision methods, even advanced large language models are prone to logical errors and redundant reasoning. We claim that the effectiveness of process supervision significantly depends on both the accuracy and the length of reasoning chains. Moreover, we identify that these factors exhibit a nonlinear relationship with the overall reward score of the reasoning process. Inspired by these insights, we propose a novel process supervision paradigm, PSPO*, which systematically outlines the workflow from reward model training to policy optimization, and highlights the importance of nonlinear rewards in process supervision. Based on PSPO*, we develop the PSPO-WRS, which considers the number of reasoning steps in determining reward scores and utilizes an adjusted Weibull distribution for nonlinear reward shaping. Experimental results on six mathematical reasoning datasets demonstrate that PSPO-WRS consistently outperforms current mainstream models.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2411.11681', 171)">Copy Link</button>
<div id="copy-message-171" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2412.01297">Morphological-Symmetry-Equivariant Heterogeneous Graph Neural Network for Robotic Dynamics Learning</a></h1>
<p><b>Authors:</b> Fengze Xie, Sizhe Wei, Yue Song, Yisong Yue, Lu Gan</p>
<p>Abstract: We present a morphological-symmetry-equivariant heterogeneous graph neural network, namely MS-HGNN, for robotic dynamics learning, that integrates robotic kinematic structures and morphological symmetries into a single graph network. These structural priors are embedded into the learning architecture as constraints, ensuring high generalizability, sample and model efficiency. The proposed MS-HGNN is a versatile and general architecture that is applicable to various multi-body dynamic systems and a wide range of dynamics learning problems. We formally prove the morphological-symmetry-equivariant property of our MS-HGNN and validate its effectiveness across multiple quadruped robot learning problems using both real-world and simulated data. Our code is made publicly available at https://github.com/lunarlab-gatech/MorphSym-HGNN/.</p>
<p>URLs: <a href="https://github.com/lunarlab-gatech/MorphSym-HGNN/.">https://github.com/lunarlab-gatech/MorphSym-HGNN/.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2412.01297, https://github.com/lunarlab-gatech/MorphSym-HGNN/.', 172)">Copy Link</button>
<div id="copy-message-172" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.07409">MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification</a></h1>
<p><b>Authors:</b> Anh-Tien Nguyen, Duy Minh Ho Nguyen, Nghiem Tuong Diep, Trung Quoc Nguyen, Nhat Ho, Jacqueline Michelle Metsch, Miriam Cindy Maurer, Daniel Sonntag, Hanibal Bohnenberger, Anne-Christin Hauschild</p>
<p>Abstract: Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model's ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this MGPATH.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.07409', 173)">Copy Link</button>
<div id="copy-message-173" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.09650">Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples</a></h1>
<p><b>Authors:</b> Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu</p>
<p>Abstract: The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.</p>
<p>URLs: <a href="https://github.com/glorgao/SelectiveDPO.">https://github.com/glorgao/SelectiveDPO.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.09650, https://github.com/glorgao/SelectiveDPO.', 174)">Copy Link</button>
<div id="copy-message-174" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.11331">Transfer Learning of CATE with Kernel Ridge Regression</a></h1>
<p><b>Authors:</b> Seok-Jin Kim, Hongjie Liu, Molei Liu, Kaizheng Wang</p>
<p>Abstract: The proliferation of data has sparked significant interest in leveraging findings from one study to estimate treatment effects in a different target population without direct outcome observations. However, the transfer learning process is frequently hindered by substantial covariate shift and limited overlap between (i) the source and target populations, as well as (ii) the treatment and control groups within the source. We propose a novel method for overlap-adaptive transfer learning of conditional average treatment effect (CATE) using kernel ridge regression (KRR). Our approach involves partitioning the labeled source data into two subsets. The first one is used to train candidate CATE models based on regression adjustment and pseudo-outcomes. An optimal model is then selected using the second subset and unlabeled target data, employing another pseudo-outcome-based strategy. We provide a theoretical justification for our method through sharp non-asymptotic MSE bounds, highlighting its adaptivity to both weak overlaps and the complexity of CATE function. Extensive numerical studies confirm that our method achieves superior finite-sample efficiency and adaptability. We conclude by demonstrating the effectiveness of our approach using a 401(k) eligibility dataset.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.11331', 175)">Copy Link</button>
<div id="copy-message-175" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.12877">Pushing the Limits of the Reactive Affine Shaker Algorithm to Higher Dimensions</a></h1>
<p><b>Authors:</b> Roberto Battiti, Mauro Brunato</p>
<p>Abstract: Bayesian Optimization (BO) for the minimization of expensive functions of continuous variables uses all the knowledge acquired from previous samples (${\boldsymbol x}_i$ and $f({\boldsymbol x}_i)$ values) to build a surrogate model based on Gaussian processes. The surrogate is then exploited to define the next point to sample, through a careful balance of exploration and exploitation. Initially intended for low-dimensional spaces, BO has recently been modified and used also for very large-dimensional spaces (up to about one thousand dimensions).
  In this paper we consider a much simpler algorithm, called "Reactive Affine Shaker" (RAS). The next sample is always generated with a uniform probability distribution inside a parallelepiped (the "box"). At each iteration, the form of the box is adapted during the search through an affine transformation, based only on the point $\boldsymbol x$ position and on the success or failure in improving the function. The function values are therefore not used directly to modify the search area and to generate the next sample. The entire dimensionality is kept (no active subspaces).
  Despite its extreme simplicity and its use of only stochastic local search, surprisingly the produced results are comparable to and not too far from the state-of-the-art results of high-dimensional versions of BO, although with some more function evaluations.
  An ablation study and an analysis of probability distribution of directions (improving steps and prevailing box orientation) in very large-dimensional spaces are conducted to understand more about the behavior of RAS and to assess the relative importance of the algorithmic building blocks for the final results.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.12877', 176)">Copy Link</button>
<div id="copy-message-176" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.18760">Learning Autonomy: Off-Road Navigation Enhanced by Human Input</a></h1>
<p><b>Authors:</b> Akhil Nagariya, Dimitar Filev, Srikanth Saripalli, Gaurav Pandey</p>
<p>Abstract: In the area of autonomous driving, navigating off-road terrains presents a unique set of challenges, from unpredictable surfaces like grass and dirt to unexpected obstacles such as bushes and puddles. In this work, we present a novel learning-based local planner that addresses these challenges by directly capturing human driving nuances from real-world demonstrations using only a monocular camera. The key features of our planner are its ability to navigate in challenging off-road environments with various terrain types and its fast learning capabilities. By utilizing minimal human demonstration data (5-10 mins), it quickly learns to navigate in a wide array of off-road conditions. The local planner significantly reduces the real world data required to learn human driving preferences. This allows the planner to apply learned behaviors to real-world scenarios without the need for manual fine-tuning, demonstrating quick adjustment and adaptability in off-road autonomous driving technology.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.18760', 177)">Copy Link</button>
<div id="copy-message-177" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2502.19086">Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood</a></h1>
<p><b>Authors:</b> Stefano Damato, Dario Azzimonti, Giorgio Corani</p>
<p>Abstract: We adopt Gaussian Processes (GPs) as latent functions for probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function and marginalizes it out when making predictions. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which has both a point mass at zero and heavy tails, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2502.19086', 178)">Copy Link</button>
<div id="copy-message-178" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.16371">Reinforcement Learning-based Heuristics to Guide Domain-Independent Dynamic Programming</a></h1>
<p><b>Authors:</b> Minori Narita, Ryo Kuroiwa, J. Christopher Beck</p>
<p>Abstract: Domain-Independent Dynamic Programming (DIDP) is a state-space search paradigm based on dynamic programming for combinatorial optimization. In its current implementation, DIDP guides the search using user-defined dual bounds. Reinforcement learning (RL) is increasingly being applied to combinatorial optimization problems and shares several key structures with DP, being represented by the Bellman equation and state-based transition systems. We propose using reinforcement learning to obtain a heuristic function to guide the search in DIDP. We develop two RL-based guidance approaches: value-based guidance using Deep Q-Networks and policy-based guidance using Proximal Policy Optimization. Our experiments indicate that RL-based guidance significantly outperforms standard DIDP and problem-specific greedy heuristics with the same number of node expansions. Further, despite longer node evaluation times, RL guidance achieves better run-time performance than standard DIDP on three of four benchmark domains.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.16371', 179)">Copy Link</button>
<div id="copy-message-179" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.18599">Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization</a></h1>
<p><b>Authors:</b> Minsu Kim, Seongmin Hong, RyeoWook Ko, Soongyu Choi, Hunjong Lee, Junsoo Kim, Joo-Young Kim, Jongse Park</p>
<p>Abstract: Modern Large Language Model serving system batches multiple requests to achieve high throughput, while batching attention operations is challenging, rendering memory bandwidth a critical bottleneck. The community relies on high-end GPUs with multiple high-bandwidth memory channels. Unfortunately, HBM's high bandwidth often comes at the expense of limited memory capacity, which reduces core utilization and increases costs. Recent advancements enabling longer contexts for LLMs have substantially increased the key-value cache size, further intensifying the pressures on memory capacity. The literature has explored KV cache quantization techniques, which commonly use low bitwidth for most values, selectively using higher bitwidth for outlier values. While this approach helps achieve high accuracy and low bitwidth simultaneously, it comes with the limitation that cost for online outlier detection is excessively high, negating the advantages. We propose Oaken, an acceleration solution that achieves high accuracy and high performance simultaneously through co-designing algorithm and hardware. To effectively find a sweet spot in the accuracy-performance trade-off space of KV cache quantization, Oaken employs an online-offline hybrid approach, setting outlier thresholds offline, which are then used to determine the quantization scale online. To translate the proposed algorithmic technique into tangible performance gains, Oaken also comes with custom quantization engines and memory management units that can be integrated with any LLM accelerators. We built an Oaken accelerator on top of an LLM accelerator, LPU, and conducted a comprehensive evaluation. Our experiments show that for a batch size of 256, Oaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU, incurring a minimal accuracy loss of only 0.54\% on average, compared to state-of-the-art KV cache quantization techniques.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.18599', 180)">Copy Link</button>
<div id="copy-message-180" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2503.18938">AdaWorld: Learning Adaptable World Models with Latent Actions</a></h1>
<p><b>Authors:</b> Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan</p>
<p>Abstract: World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2503.18938', 181)">Copy Link</button>
<div id="copy-message-181" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.05009">Deconstructing Jazz Piano Style Using Machine Learning</a></h1>
<p><b>Authors:</b> Huw Cheston, Reuben Bance, Peter M. C. Harrison</p>
<p>Abstract: Artistic style has been studied for centuries, and recent advances in machine learning create new possibilities for understanding it computationally. However, ensuring that machine-learning models produce insights aligned with the interests of practitioners and critics remains a significant challenge. Here, we focus on musical style, which benefits from a rich theoretical and mathematical analysis tradition. We train a variety of supervised-learning models to identify 20 iconic jazz musicians across a carefully curated dataset of 84 hours of recordings, and interpret their decision-making processes. Our models include a novel multi-input architecture that enables four musical domains (melody, harmony, rhythm, and dynamics) to be analysed separately. These models enable us to address fundamental questions in music theory and also advance the state-of-the-art in music performer identification (94% accuracy across 20 classes). We release open-source implementations of our models and an accompanying web application for exploring musical styles.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.05009', 182)">Copy Link</button>
<div id="copy-message-182" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.05235">IAEmu: Learning Galaxy Intrinsic Alignment Correlations</a></h1>
<p><b>Authors:</b> Sneh Pandya, Yuanyuan Yang, Nicholas Van Alfen, Jonathan Blazek, Robin Walters</p>
<p>Abstract: The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing analyses, arise from correlations in galaxy shapes driven by tidal interactions and galaxy formation processes. Accurate IA modeling is essential for robust cosmological inference, but current approaches rely on perturbative methods that break down on nonlinear scales or on expensive simulations. We introduce IAEmu, a neural network-based emulator that predicts the galaxy position-position ($\xi$), position-orientation ($\omega$), and orientation-orientation ($\eta$) correlation functions and their uncertainties using mock catalogs based on the halo occupation distribution (HOD) framework. Compared to simulations, IAEmu achieves ~3% average error for $\xi$ and ~5% for $\omega$, while capturing the stochasticity of $\eta$ without overfitting. The emulator provides both aleatoric and epistemic uncertainties, helping identify regions where predictions may be less reliable. We also demonstrate generalization to non-HOD alignment signals by fitting to IllustrisTNG hydrodynamical simulation data. As a fully differentiable neural network, IAEmu enables $\sim$10,000$\times$ speed-ups in mapping HOD parameters to correlation functions on GPUs, compared to CPU-based simulations. This acceleration facilitates inverse modeling via gradient-based sampling, making IAEmu a powerful surrogate model for galaxy bias and IA studies with direct applications to Stage IV weak lensing surveys.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.05235', 183)">Copy Link</button>
<div id="copy-message-183" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.06566">Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure</a></h1>
<p><b>Authors:</b> Minshuo Chen, Renyuan Xu, Yumin Xu, Ruixun Zhang</p>
<p>Abstract: Financial scenario simulation is essential for risk management and portfolio optimization, yet it remains challenging especially in high-dimensional and small data settings common in finance. We propose a diffusion factor model that integrates latent factor structure into generative diffusion processes, bridging econometrics with modern generative AI to address the challenges of the curse of dimensionality and data scarcity in financial simulation. By exploiting the low-dimensional factor structure inherent in asset returns, we decompose the score function--a key component in diffusion models--using time-varying orthogonal projections, and this decomposition is incorporated into the design of neural network architectures. We derive rigorous statistical guarantees, establishing nonasymptotic error bounds for both score estimation at O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4} n^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather than the number of assets d, surpassing the dimension-dependent limits in the classical nonparametric statistics literature and making the framework viable for markets with thousands of assets. Numerical studies confirm superior performance in latent subspace recovery under small data regimes. Empirical analysis demonstrates the economic significance of our framework in constructing mean-variance optimal portfolios and factor portfolios. This work presents the first theoretical integration of factor structure with diffusion models, offering a principled approach for high-dimensional financial simulation with limited data. Our code is available at https://github.com/xymmmm00/diffusion_factor_model.</p>
<p>URLs: <a href="https://github.com/xymmmm00/diffusion_factor_model.">https://github.com/xymmmm00/diffusion_factor_model.</a></p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.06566, https://github.com/xymmmm00/diffusion_factor_model.', 184)">Copy Link</button>
<div id="copy-message-184" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.11436">Shifting Work Patterns with Generative AI</a></h1>
<p><b>Authors:</b> Eleanor Wiske Dillon, Sonia Jaffe, Nicole Immorlica, Christopher T. Stanton</p>
<p>Abstract: We present evidence on how generative AI changes the work patterns of knowledge workers using data from a 6-month-long, cross-industry, randomized field experiment. Half of the 7,137 workers in the study received access to a generative AI tool integrated into the applications they already used for emails, document creation, and meetings. We find that access to the AI tool during the first year of its release primarily impacted behaviors that workers could change independently and not behaviors that require coordination to change: workers who used the tool in more than half of the sample weeks spent 3.6 fewer hours, or 31% less time on email each week (intent to treat estimate is 1.3 hours) and completed documents moderately faster, but did not significantly change time spent in meetings.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.11436', 185)">Copy Link</button>
<div id="copy-message-185" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.17938">Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G</a></h1>
<p><b>Authors:</b> Raza Ul Mustafa, Sesha Dassanayake, Noman Ashraf</p>
<p>Abstract: The Quality of Experience (QoE) is the users satisfaction while streaming a video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube reflects the smooth streaming session without any buffering and quality shift events. One of the most important factors nowadays affecting QoE of YouTube is frequent shifts from higher to lower resolutions and vice versa. These shifts ensure a smooth streaming session; however, it might get a lower mean opinion score. For instance, dropping from 1080p to 480p during a video can preserve continuity but might reduce the viewers enjoyment. Over time, OTT platforms are looking for alternative ways to boost user experience instead of relying on traditional Quality of Service (QoS) metrics such as bandwidth, latency, and throughput. As a result, we look into the relationship between quality shifting in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our findings state that these channel metrics positively correlate with shifts. Thus, in real-time, OTT can only rely on them to predict video streaming sessions into lower- and higher-resolution categories, thus providing more resources to improve user experience. Using traditional Machine Learning (ML) classifiers, we achieved an accuracy of 77-percent, while using only RSRP, RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency networks promise enhanced streaming capabilities, the proposed methodology can be used to improve OTT services.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.17938', 186)">Copy Link</button>
<div id="copy-message-186" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2504.21227">Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets</a></h1>
<p><b>Authors:</b> Omid Halimi Milani, Amanda Nikho, Lauren Mills, Marouane Tliba, Ahmet Enis Cetin, Mohammed H. Elnagar</p>
<p>Abstract: Deep learning models have great potential in medical imaging, including orthodontics and skeletal maturity assessment. However, applying a model to data different from its training set can lead to unreliable predictions that may impact patient care. To address this, we propose a comprehensive verification framework that evaluates model suitability through multiple complementary strategies. First, we introduce a Gradient Attention Map (GAM)-based approach that analyzes attention patterns using Grad-CAM and compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance. Second, we extend verification to early convolutional feature maps, capturing structural mis-alignments missed by attention alone. Finally, we incorporate an additional garbage class into the classification model to explicitly reject out-of-distribution inputs. Experimental results demonstrate that these combined methods effectively identify unsuitable models and inputs, promoting safer and more reliable deployment of deep learning in medical imaging.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2504.21227', 187)">Copy Link</button>
<div id="copy-message-187" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2505.00949">Llama-Nemotron: Efficient Reasoning Models</a></h1>
<p><b>Authors:</b> Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung</p>
<p>Abstract: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.00949', 188)">Copy Link</button>
<div id="copy-message-188" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2505.01742">Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs</a></h1>
<p><b>Authors:</b> Yu Mao, Jingzong Li, Jun Wang, Hong Xu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue</p>
<p>Abstract: Neural image compression, necessary in various machine-to-machine communication scenarios, suffers from its heavy encode-decode structures and inflexibility in switching between different compression levels. Consequently, it raises significant challenges in applying the neural image compression to edge devices that are developed for powerful servers with high computational and storage capacities. We take a step to solve the challenges by proposing a new transformer-based edge-compute-free image coding framework called Easz. Easz shifts the computational overhead to the server, and hence avoids the heavy encoding and model switching overhead on the edge. Easz utilizes a patch-erase algorithm to selectively remove image contents using a conditional uniform-based sampler. The erased pixels are reconstructed on the receiver side through a transformer-based framework. To further reduce the computational overhead on the receiver, we then introduce a lightweight transformer-based reconstruction structure to reduce the reconstruction load on the receiver side. Extensive evaluations conducted on a real-world testbed demonstrate multiple advantages of Easz over existing compression approaches, in terms of adaptability to different compression levels, computational efficiency, and image reconstruction quality.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.01742', 189)">Copy Link</button>
<div id="copy-message-189" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2505.06977">CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging</a></h1>
<p><b>Authors:</b> Wenju Sun, Qingyong Li, Yangli-ao Geng, Boyang Li</p>
<p>Abstract: Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.06977', 190)">Copy Link</button>
<div id="copy-message-190" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2505.08037">TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation</a></h1>
<p><b>Authors:</b> Yutong Liu, Feng Xiao, Ziyue Zhang, Yongbin Yu, Cheng Huang, Fan Gao, Xiangxiang Wang, Ma-bao Ban, Manping Fan, Thupten Tsering, Cheng Huang, Gadeng Luosang, Renzeng Duojie, Nyima Tashi</p>
<p>Abstract: Multi-level Tibetan spelling correction addresses errors at both the character and syllable levels within a unified model. Existing methods focus mainly on single-level correction and lack effective integration of both levels. Moreover, there are no open-source datasets or augmentation methods tailored for this task in Tibetan. To tackle this, we propose a data augmentation approach using unlabeled text to generate multi-level corruptions, and introduce TiSpell, a semi-masked model capable of correcting both character- and syllable-level errors. Although syllable-level correction is more challenging due to its reliance on global context, our semi-masked strategy simplifies this process. We synthesize nine types of corruptions on clean sentences to create a robust training set. Experiments on both simulated and real-world data demonstrate that TiSpell, trained on our dataset, outperforms baseline models and matches the performance of state-of-the-art approaches, confirming its effectiveness.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08037', 191)">Copy Link</button>
<div id="copy-message-191" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2505.08175">Fast Text-to-Audio Generation with Adversarial Post-Training</a></h1>
<p><b>Authors:</b> Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, Jordi Pons</p>
<p>Abstract: Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating $\approx$12s of 44.1kHz stereo audio in $\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest text-to-audio model to our knowledge.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08175', 192)">Copy Link</button>
<div id="copy-message-192" class="copy-message"></div>
</div>
<div class="article">
<h1> replace-cross <a href="https://arxiv.org/abs/2505.08435">Hakim: Farsi Text Embedding Model</a></h1>
<p><b>Authors:</b> Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman</p>
<p>Abstract: Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding.</p>
<button class="copy-button" onclick="copyToClipboard('https://arxiv.org/abs/2505.08435', 193)">Copy Link</button>
<div id="copy-message-193" class="copy-message"></div>
</div>

    </div>
    </body>
    